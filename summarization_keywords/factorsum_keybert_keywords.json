{"9c61f5f51a232049635e6f3441e6397af4d91298": {"ta_keywords": "swimmer immersed viscous;dynamics swimmer immersed;model dynamics swimmer;swimmer immersed;immersed viscous incompressible;dynamics swimmer;viscous incompressible fluid;viscous incompressible;incompressible fluid;immersed viscous;swimmer;incompressible;viscous;simple model dynamics;dynamics;immersed;model dynamics;fluid;present simple model;simple model;paper present simple;model;paper present;present simple;paper;present;simple", "pdf_keywords": ""}, "a05c3e8bd6dacbd192ffa28543e60e2c93c66d76": {"ta_keywords": "trend observed twitter;predicting trends twitter;trend tweets;trends twitter;social dynamics tweets;trends twitter based;trend tweets recent;analysis twitter popular;analysis twitter;twitter order trend;observed twitter;twitter popular social;popular social media;twitter popular;order trend tweets;twitter based analysis;social network twitter;various social media;product analysis twitter;analysis statistics tweets;dynamics tweets;network twitter;statistics tweets;twitter;hashtags data mining;users twitter;observed twitter attributed;tweets posted online;tweets;twitter paper examine", "pdf_keywords": ""}, "bfa10ea6a4c9fa585f21f39858da517c31a76343": {"ta_keywords": "persuasive dialogue systems;dialogue management persuasive;method persuasive dialogue;propose probabilistic dialogue;management persuasive dialogue;problem dialogue management;dialogue management;probabilistic dialogue modeling;develop dialogue model;dialogue modeling method;dialogue modeling;persuasive dialogue;dialogue manager task;dialogue systems;dialogue model;probabilistic dialogue;knowledge dialogue manager;dialogue manager;dialogue systems interact;evaluate knowledge dialogue;knowledge dialogue;modeling method persuasive;dialogue systems baseline;develop dialogue;dialogue model assuming;dialogue;problem dialogue;approach problem dialogue;management persuasive;method persuasive", "pdf_keywords": ""}, "0bf2a0a3216c79b62b3664c596f44d7a8add498a": {"ta_keywords": "entangled pairs fermions;systems called nonlinear;called nonlinear systems;fermion pairs introduce;entangled pairs;nonlinear schrdinger;entangled;nonlinearities dynamics described;nonlinear systems;terms nonlinear schrdinger;nonlinear systems nls;nonlinearities dynamics;phenomena nonlinearity nls;phenomena nonlinearity;nonlinear dynamics described;described terms nonlinear;nonlinearities;pairs fermions;nonlinear;fermion pairs;nonlinear phenomena nonlinearity;nonlinearity nls understood;nonlinear dynamical systems;nonlinear dynamics;paper nonlinear;nonlinearity;fact nonlinearities dynamics;nonlinearity nls;model nonlinear phenomena;called nonlinear", "pdf_keywords": ""}, "a4cb2a401c78bfafee69e823306b0cc9e4d673db": {"ta_keywords": "fair allocation problem;fair allocation;assignment methods fairness;assignment instance fair;instance fair allocation;reviewer assignment instance;fairness efficiency runtime;reviewer assignment;methods fairness efficiency;fairness efficiency;order fairness;mechanism order fairness;reviewer round robin;allocation;model reviewer assignment;methods fairness;order fairness comes;allocation problem;fairness comes cost;round robin mechanism;assignment methods;attainable round robin;allocation problem presenting;round robin applying;mechanism called reviewer;efficiency runtime paper;instance fair;mechanism ensure envy;envy free item;item model reviewer", "pdf_keywords": "assigning reviewers paper;assigning reviewers;fair assignment reviewers;reviewer assignment instance;assignment reviewers;reviewer assignment;assignment reviewers set;paper assignment frameworks;fair allocation problem;model reviewer assignment;reviewer round robin;problem assigning reviewers;reviewers given paper;fair allocation;mechanism called reviewer;greedy round robin;reviewers paper;finding best reviewers;item allocations;free item allocations;agents paper assignment;round robin algorithm;reviewers paper present;assignment instance fair;allocation items partitioned;reviewers set agents;instance fair allocation;allocation items;assignment frameworks present;assignment frameworks"}, "242c73ea34833910ad2643ec3a1096bb18c6d04d": {"ta_keywords": "target speech extraction;speech enhancement uncertainty;target speaker speech;speaker speech extraction;speech extraction module;identity speech enhancement;module target speaker;speech extraction model;speaker speech recognition;target speaker words;domain target speaker;recognize target speaker;speech extraction recurrent;speech enhancement;speech extraction;artifacts target speech;target speech;target speaker;relative target speech;speaker words noisy;speaker identity speech;network transducer rnn;neural network transducer;transducer rnn;speech recognition aims;speaker speech;speech recognition;adding neural uncertainty;speaker words;neural uncertainty module", "pdf_keywords": "target speech extraction;target speaker extraction;robust speech extraction;speech extraction rnn;uncertainty speech enhancement;speech extract target;speaker speech extraction;speech enhancement uncertainty;estimation enhanced speech;target speaker recognition;extraction speech enhancement;speaker extraction;uncertainty target speaker;embedder target speech;speech enhancement speaker;speaker speech extract;extraction speech recognizer;extraction speaker identity;speech extraction;speaker extraction speaker;robust speech;speech recognizer;extraction speaker;separation networks speech;target speaker speech;networks speech enhancement;speech extraction speech;extraction speech;enhanced speech multi;speech extract"}, "a4df5ff749d823905ff9c1a23b522d3f426a1bb6": {"ta_keywords": "measure similarity graphs;similarity nodes graph;personalized pagerank measure;graphs based similarity;similarity measure graph;similarity nodes;similarity graphs;personalized pagerank;tree structured markov;based similarity nodes;personalized pagerank widely;graph graphical models;similarity graphs based;based graph walks;fields personalized pagerank;pagerank measure;pagerank;using personalized pagerank;mrfs computation similarity;pagerank widely;similarity vertices graph;pagerank measure paper;probabilities tree structured;structured markov random;graph walks;pagerank widely used;graph nodes based;measure graph nodes;marginal probabilities tree;similarity measure", "pdf_keywords": ""}, "f32108602fb0dbda29030cac780165a4b89048a3": {"ta_keywords": "comparison relation prediction;predicting comparison relations;predict comparison relations;web predict comparison;relation prediction;databases predict comparison;comparison relations text;text relational;predicting comparison;relations text sql;text relational databases;comparison relations columns;relational databases predict;relation prediction key;predict comparison;comparison relations;predicting outcome team;comparison relations limited;noun phrasing knowledge;phrasing knowledge mined;relations text;capabilities predicting comparison;phrasing knowledge;web predict;comparison relation;relations columns;knowledge mined web;databases predict;text sql;adjective noun phrasing", "pdf_keywords": ""}, "04745fe1306d10c915d27a454c157c837dacefce": {"ta_keywords": "speaker separation matching;independent speaker separation;speaker separation;corpus single channel;frequency representation mixture;representation mixture signal;channel speaker independent;channel speaker;representation mixture;single channel speaker;speaker independent;speaker independent speaker;independent speaker;mixture used reconstructing;mask based approaches;phase representations introduce;separation matching;discrete phase representations;dataset studied corpus;separation matching performance;phase representations;training inference schemes;speaker;mixture signal;mixture signal present;corpus;training inference;phase reconstruction;studied corpus;studied corpus single", "pdf_keywords": ""}, "20140fcf0bdd932c1886ff1c7674c23649b1e3b8": {"ta_keywords": "based speech synthesis;speech synthesis;speech synthesis propose;speech synthesis present;quality synthetic speech;synthetic speech;context based speech;hmm based speech;components hmm based;improving performance speech;automatic speech;approach automatic speech;performance speech recognition;speech recognition;automatic speech recognition;speech recognition systems;synthetic speech alleviate;based spectral decomposition;components hmm;speech recognition based;performance speech;conversion pdc spectroscopy;based spectral;spectral decomposition;rich context models;based speech;spectral decomposition spectral;parameter generation rich;iterative parameter generation;f0 components hmm", "pdf_keywords": ""}, "e32177e38060637ac8a2ebc9990d43d1ab8bdb8a": {"ta_keywords": "similarity networks commenting;networks item similarity;item similarity networks;friendship networks;friendship networks item;similarity networks;relationship friendship networks;social networks;commenting networks;dimensions social networks;networks commenting networks;recommendation systems;latent similarities;commenting networks article;networks capture similarities;based latent similarities;item similarity;recommendation systems work;networks commenting;similarity;help recommendation systems;similarities;use communities;social networks capture;networks article present;communities;networks article;networks;similarities different dimensions;capture similarities", "pdf_keywords": ""}, "6680b1e863c394f00307cb3818f7c7d75c9919aa": {"ta_keywords": "distributed storage problem;consider distributed storage;distributed storage;codes non multicast;multicast setting approach;network codes;stored network nodes;nodes data collector;distributed;multicast;design network codes;network nodes data;data stored network;non multicast;network codes non;stored network;consider distributed;network nodes;data collector;data connecting nodes;non multicast setting;multicast setting;nodes data;storage;storage problem data;interference alignment;storage problem;data collector recover;nodes;collector recover data", "pdf_keywords": ""}, "c096ec97ecc4f8325f6db7f32398445d6a39f959": {"ta_keywords": "fairness metrics recommender;recommendation fairness literature;recommendation fairness;fairness metrics;fairness aware applications;fairness systems;research recommendation fairness;multiple fairness metrics;supporting multiple fairness;fairness systems exhibit;complexities fairness aware;dynamically rebalancing fairness;rebalancing fairness concerns;rebalancing fairness;fairness aware;fairness concerns;fairness literature;accuracy fairness;multiple fairness;complexities fairness;notions fairness;world complexities fairness;fairness concerns argue;notions fairness recognize;fairness recognize;trade accuracy fairness;properties fairness systems;accuracy fairness multiply;fairness multiply defined;fairness multiply", "pdf_keywords": "fairness ensure recommender;recommendation fairness;recommendation fairness particular;fairness recommendation fairness;crowdsourced recommendation systems;allocation voting recommender;recommender crowdsourced recommendation;recommender crowdsourced;crowdsourced recommendation;voting recommender systems;fairness recommendation;voting recommender;recommendation systems equity;present recommender crowdsourced;focus fairness recommendation;rank recommendation specific;use rank recommendation;novel ranking algorithm;recommendation systems;rank recommendation;provide recommendations consumers;ensure recommender;recommender systems widely;recommendations consumers;recommendation specific user;recommendation generated;ranking algorithm;choice allocation voting;recommendations consumers providers;ensure recommender time"}, "27636090a87fab750fccff4c6ede161ab62bcab4": {"ta_keywords": "vehicular ad hoc;vehicular networks;vehicular networks present;network autonomous vehicles;ad hoc networks;vehicles reaching junction;hoc networks vans;family vehicular networks;hoc networks;reaching junction vehicular;junction vehicular;road junctions vehicle;junction vehicular ad;network autonomous;junctions vehicle prior;knowledge oncoming vehicles;road junctions;collision road junctions;vehicles reaching;safety messages beacons;vehicular;oncoming vehicles reaching;network visibility;oncoming vehicles;flow network autonomous;junctions vehicle;traffic;vehicle prior knowledge;traffic flow network;increasing network visibility", "pdf_keywords": "congestion based beacons;channel congestion road;channel congestion based;computing channel congestion;congestion road networks;channel congestion;problem channel congestion;message reception freeway;reception freeway time;congestion based;road networks;beacons sent vehicles;congestion road;road networks simulation;reception freeway;congestion;computing channel;vehicle increase awareness;freeway time;message reception;based beacons received;freeway time reduces;based beacons;safety message reception;freeway;reduces number beacons;problem channel;method computing channel;channel;number beacons sent"}, "c3fc0b1041dcdd5b47ffaa0d584e40aa841628bf": {"ta_keywords": "character based wrappers;language expands entities;based html wrappers;wrappers set expansion;expands entities;language expands;wrappers based character;expander language;expands entities automatically;web language;html wrappers;binary relational concepts;learn binary relational;binary relational;relational concepts mayor;based wrappers;wrappers based;html wrappers set;web language independent;expander language expands;resources web language;seeds language;entities;structured house cards;structured;seeds language independence;wrappers;set expander language;entities automatically;semi structured", "pdf_keywords": ""}, "7f79ac114d30c2c7dae91075210fbfda90c9d76f": {"ta_keywords": "diplomacy bots;press diplomacy bots;diplomacy bots unexploitable;games popular diplomacy;diplomacy combines supervised;adversarial cooperative;human players;adversarial cooperative settings;analysis adversarial cooperative;adversarial;bots unexploitable expert;variant diplomacy;neural networks agent;diplomacy;press variant diplomacy;anonymous games;press diplomacy;popular diplomacy website;external regret minimization;game shifting alliances;learning human;search winner bargest;variant diplomacy combines;diplomacy website;regret minimization;search external regret;cooperation;cooperation competition;diplomacy combines;popular diplomacy", "pdf_keywords": "search reinforcement learning;search reinforcement;imitation learning;imitation learning used;adversarial game involving;adversarial game;combines search reinforcement;reinforcement learning improve;paper imitation learning;minimization adversarial game;game involving cooperation;presents reinforcement learning;regret minimization adversarial;search regret minimization;information games;reinforcement learning;strategic games;information games paper;games paper agent;perfect information games;agent searchbot;000 strategic games;reinforcement learning model;agent searchbot paper;agent press diplomacy;policy players online;reinforcement;develop agent searchbot;outperforms agents;agent press variant"}, "70dc18bb6607e408ec1cd3f71c0fdac3534c288d": {"ta_keywords": "rnn based speech;lwh speech recognition;robust automatic speech;speech recognition;performance recurrent neural;rnn based;speech reconstruction;speech reconstruction objective;speechechanical tweezer;speed speechechanical tweezer;accuracy speed speechechanical;network rnn based;optimal speech reconstruction;performance recurrent;neural network rnn;speech enhancement;speech recognition used;rnn;speechechanical tweezer sws;based speech enhancement;network rnn;speech recognition knowledge;recurrent neural network;automatic speech;lipospectral recognition;automatic speech recognition;speech enhancement light;art lipospectral recognition;lllr discriminatively trained;short term memory", "pdf_keywords": ""}, "6e07fb796c75cac6432cdf0c314b933d0f9f45e5": {"ta_keywords": "biomedical text mining;parsing vector machines;text mining simple;mining recognition gene;text mining;recognition gene names;names biomedical text;gene names biomedical;syntactic parsing vector;rule based tagger;gene entity related;gene expression data;gene entity;task text mining;analysis gene expression;know gene refers;actual gene entity;gene names;gene expression;text mining recognition;gene refers;syntactic parsing;biomedical text;tag data approach;parsing vector;parsing;combines syntactic parsing;know gene;refers actual gene;mining simple rule", "pdf_keywords": ""}, "24fcdaf969089e6a411f7cebc9274bbc53c25e42": {"ta_keywords": "counterfactual augmented data;counterfactually augmented datasets;counterfactually augmented data;generating counterfactually augmented;counterfactual augmented;counterfactual labeling;new counterfactual labeling;generating counterfactually;counterfactual labeling called;counterfactually augmented;presents counterfactually augmented;efficacy counterfactual augmented;counterfactual;new counterfactual;present new counterfactual;counterfactually;process generating counterfactually;efficacy counterfactual;paper presents counterfactually;explain efficacy counterfactual;augmented datasets;augmented data analysis;presents counterfactually;augmented datasets present;causal models;augmented data;outcomes underlying causal;causal models measurement;underlying causal model;underlying causal", "pdf_keywords": "learn causal features;causal transfer learning;causal features movie;causal features;causal features make;learn causal;non causal features;movie review data;models causal transfer;transfer learning;feature attribution;causal graphical models;features movie review;invariant models causal;models causal;deep neural;feature attribution presence;transfer learning widely;correlations human annotations;models robust domain;architectures learn causal;use deep neural;deep neural networks;approach feature attribution;movie review;features movie;neural networks train;models robust;causal transfer;sentiment analysis nli"}, "1d5c07e7415a7e9be078717197ddf9f3c70a2875": {"ta_keywords": "sensitive information bert;information bert trained;bert model;ensembles train bert;train bert model;information bert;electronic health records;extract sensitive information;bert model simple;records ehr;ehr researchers;health records ehr;deidentified ehr researchers;records ehr afforded;bert trained mim;inferred observed data;health records;meaningfully extract sensitive;asexually transmitted signals;ehr researchers access;deidentified ehr;used deidentified ehr;predictive clinical tasks;bert;infer presence asexually;sexually transmitted signal;bert trained;transmitted;electronic health;inferring", "pdf_keywords": "neural networks privacy;models patient ehr;patient ehr data;reveal sensitive information;privacy;potential privacy;potential privacy concerns;trained models patient;ehr reveal sensitive;sensitive information;used predict medical;medical records train;privacy concerns;medical records;pre trained models;privacy concerns pre;sensitive information work;patient ehr;investigate potential privacy;ehr researchers access;ehr researchers;predict medical;nondeidenti medical records;ehr data;trained models;predict medical conditions;deidenti\ufb01ed ehr researchers;conditions medical records;medical records efforts;learning models address"}, "ca9047c78d48b606c4e4f0c456b1dda550de28b2": {"ta_keywords": "recurrent neural networks;neural networks rnns;networks rnns;networks rnns neural;rnns neural;continuous time memorization;rnns;recurrent neural;deep learning models;deep learning;series recurrent neural;time memorization;learning models time;deep neural networks;learning tasks long;rnns neural differential;time memorization introduce;deep neural;long range memory;time series recurrent;range memory sequence;memory sequence;long time series;memory sequence simply;learning leverages deep;neural networks;sequence model inspired;leverages deep neural;memorization;memory", "pdf_keywords": "sequence polynomials modeled;sequence modeling;sequence polynomials;deep neural network;deep neural networks;model long sequences;deep neural;neural network architecture;idea sequence polynomials;model sequence modeling;sequence modeling based;neural networks;recurrent convolutional;recurrent convolutional continuous;advantages recurrent convolutional;sequence model;paper deep neural;neural network;long sequences;neural networks used;sequence transformation;matrices introduce sequence;polynomials modeled;sequence model sequence;sequence sequence transformation;propose sequence model;model sequence;polynomials modeled series;benchmark datasets long;sequences"}, "b2a090506264bc9706dc9bcc5d61b4965ae919e7": {"ta_keywords": "knowledge extraction powerful;knowledge extraction;based knowledge extraction;extractions entities relations;knowledge graph;included knowledge graph;uncertain extractions entities;extractions 70 ontological;knowledge graph knowledge;extractions entities;ontological constraints knowledge;probabilistic soft logic;knowledge graph identification;transformed knowledge graph;million music extractions;incorporate ontological constraints;facts included knowledge;facts associated extraction;knowledge graph demonstrate;constraints knowledge based;entities incorporate ontological;music extractions;graph knowledge graph;ontological relations approach;knowledge based knowledge;music extractions 70;incorporate ontological;70 ontological relations;soft logic;ontological constraints", "pdf_keywords": ""}, "8ca5a1e6cec68ef515ac1eb28d069a23dc9c14df": {"ta_keywords": "semantic frame induction;semantic frame;frame induction experiments;class semantic frame;semantic class induction;frame induction;experiments based semantic;manipulation glassy droplets;semantic;manipulation glassy;semantic class;based semantic class;glassy droplets;class semantic;new class semantic;glassy droplets presence;manipulation manipulation glassy;based semantic;induction experiments based;induction experiments;droplets;droplets presence;droplets presence external;glassy;frame;class induction;class induction article;electric;simple mechanical;mechanical", "pdf_keywords": ""}, "a75c2d26ca6a06cbee62a8d1dad5993356d96793": {"ta_keywords": "best bootstrapped seeds;seeds bootstrapping;bootstrapped seeds bootstrapping;performance seeds user;automatically utilizing resources;iseal used bootstrapping;self generated seeds;entities automatically utilizing;seeds bootstrapping version;seeds performs best;seeds set expansion;bootstrapped seeds;performance seeds;user provide seeds;entities automatically;partial set seed;seeds user;bootstrapping version iseal;provided seeds performs;provide seeds;user provided seeds;iterative seal;expands entities;generated seeds;seeds user provided;expands entities automatically;generated seeds set;seed objects complete;seed objects;bootstrapping manner seal", "pdf_keywords": ""}, "a2ce385fc8d5068e8c87ebe4699c8f9b295cad5e": {"ta_keywords": "entity recognition languages;phonological subword representation;named entity recognition;corpora bilingual dictionaries;entity recognition;nlp resource rich;languages making generalization;languages adapting;phonological subword;languages challenging;resourced languages adapting;bilingual dictionaries;nlp resource;bilingual dictionaries provides;language processing nlp;machine translation;word representations;subword representation;corpora bilingual;linguistically motivated subword;resource rich languages;understood languages challenging;called phonological subword;machine translation work;processing nlp resource;word representations using;language processing;subword units phonemes;representations using linguistically;phonemes morphemes", "pdf_keywords": "transfer word embeddings;embeddings trained morphological;language leveraging resources;machine translation phoneme;translation phoneme representation;trained word embeddings;trained morphological representations;resource machine translation;language leveraging;based morphological disambiguators;neural machine translation;morphological disambiguators;resource language leveraging;translation transfer learning;morphological disambiguators present;translation phoneme;machine translation transfer;word embeddings;phoneme representation subword;morphological representations outperform;word embeddings useful;crosslingual named entity;subword units phonemes;word embeddings using;units phonemes morphemes;word representations;translating words;representation subword representation;subword representation;trained morphological"}, "a0e92f6e9564b8c38b6649ae71b892ddfb988faa": {"ta_keywords": "query parser;input query parser;query parser retrieves;parse queries;output parse semantic;parse queries new;parsing semantic data;semantic parsing powerful;parse semantic;parser retrieves related;semantic parsing;parse semantic parsing;parsing semantic;generative seq2seq model;parser retrieves;technique parsing semantic;casper parse queries;generative seq2seq;powerful technique parsing;parser;parse;parsing;adapt new semantic;applies generative seq2seq;parser casper parse;parsing powerful technique;seq2seq model;parsing powerful;query applies generative;technique parsing", "pdf_keywords": "semantic parser trained;parser trained;augmented semantic parser;retrieval augmented semantic;semantic parser;semantic parser uses;parser trained mixed;bootstrapping making parser;parser handle queries;parse patterns achieves;prediction speci\ufb01c parse;casperorig retrieval;present semantic parser;casperorig retrieval augmented;augmented semantic;parse patterns;speci\ufb01c parse patterns;making parser;retrieval;bootstrap casperorig retrieval;parse;parser uses retrieved;parser;retrieval augmented;forgetting incremental training;catastrophic forgetting incremental;parser uses;catastrophic forgetting;changes retrieval;queries changes retrieval"}, "a30d7a3aa5e50d0b7abb448b6692e419b84018b8": {"ta_keywords": "problem character recognition;character recognition;character recognition based;walks precision prefixing;new genetic algorithm;genetic algorithm predicting;algorithm predicting;error rate minimization;genetic algorithm;random walks precision;recognition;training testing scheme;algorithm;recognition based;walks precision;problem character;precision prefixing;training testing;xmath0 neighborhood model;sequence random walks;recognition based concept;rate minimization present;random walks;xmath0 neighborhood;minimization present new;new training testing;sakata pma test;proposed xmath0 neighborhood;rate minimization;minimization present", "pdf_keywords": "train sequence predictive;sequence predictive models;approach train sequence;train sequence;sequence predictive;pre\ufb01x training method;testing pre\ufb01x training;models partial sequences;training testing pre\ufb01x;accurate pre\ufb01x boosting;accurate partial sequences;beam size training;pre\ufb01x boosting;training method investigated;training strategy promising;training method;training strategy;pre\ufb01x training;predictive models partial;new training strategy;partial sequences generated;new approach train;beam search;approach train;teacher forcing strategy;training testing;partial sequences;partial sequences low;using beam search;training objective"}, "a41c81e5c3f86e18217069b94b44ceaf281e451c": {"ta_keywords": "outcome struggle noisy;light traffic regime;probability partner place;light traffic;backtracking algorithm;study backtracking algorithm;simple heuristic placement;simple heuristic;consider light traffic;predict outcome struggle;traffic regime;heuristic placement policies;optimal;probability finding partner;heuristic placement;policies optimal;optimal policies;probability partner;heuristic;backtracking;backtracking algorithm providing;based observation probability;struggle noisy environment;results policies optimal;number participants struggle;participants struggle;traffic regime speed;traffic;advantage backtracking;optimal policies various", "pdf_keywords": "shadowing locations wireless;forwarding protocols sleep;cycling wireless sensor;sensor networks;wake cycling wireless;communications reliability mobility;sensor network;protocols sleep;protocols sleep wake;cycling wireless;sensor networks provide;wireless sensor networks;sensor network carries;locations wireless networks;reliability mobility mobility;outage probability randomly;outage probability;reliability mobility;alarm 6randomness outage;locations wireless;deployment backtracking line;6randomness outage probability;wireless networks;problem sensor network;deployment backtracking;wake cycling;carry occasional alarm;sequentially measures shadowing;measures shadowing locations;known forwarding protocols"}, "f20654f481843ec9eb11bcd00e418aec2470dfa5": {"ta_keywords": "piggyback erasure codes;erasure codes storage;fault tolerance storage;codes storage efficient;erasure codes extensively;codes storage;erasure codes;distributed storage;traditional erasure codes;distributed storage systems;efficient repair parity;tolerance storage efficient;nodes piggybacking framework;mds array codes;codes parities mds;storage efficient result;storage systems;storage efficient manner;binary mds;storage systems achieve;binary mds array;smallest locality rebuilding;solutions binary mds;array codes parities;locality rebuilding;propose piggyback erasure;storage efficient;codes address rebuilding;unavailable nodes piggybacking;parities mds codes", "pdf_keywords": "repairef\ufb01cient storage codes;distributed storage codes;distributed storage systems;distributed storage;design repairef\ufb01cient storage;designing distributed storage;design distributed storage;repairef\ufb01cient storage;storage systems tolerate;storage codes ef\ufb01cient;redundancy repair codes;storage codes;storage systems;repair codes optimized;storage codes present;codes optimized locality;ef\ufb01cient repair parity;redundancy repair;ef\ufb01cient parity repair;repair parity nodes;repair redundancy;repair redundancy repair;parity repair;node repair;storage;algorithm ef\ufb01cient repair;set repair redundancy;repair codes;parity repair existing;permanent data loss"}, "6e05d35d072cd73fa039fd60696a8fe110f1d6cd": {"ta_keywords": "graph recommendation increasingly;graph recommendation;discover effective recommendation;paths graph recommendation;recommendation strategies represented;recommendation strategies;effective recommendation strategies;online recommendation saccharomyces;approaches online recommendation;recommendation saccharomyces genome;effective recommendation;recommendation tasks;collaborative filtering;recommendation increasingly;reading recommendation tasks;recommendation tasks leveraging;collaborative filtering approaches;recommendation increasingly important;based collaborative filtering;online recommendation;recommendation saccharomyces;saccharomyces genome databases;based reading recommendation;random walks;reading recommendation;publication databases rich;genome databases;citation history based;random walks glassy;representation publication databases", "pdf_keywords": ""}, "3c4dfc252c214d559fadb5e3159bcc9c7db08fbc": {"ta_keywords": "dehydration imaging dental;imaging dental fluorosis;densities fluorosis teeth;lesions dental fluorosis;caries dental fluorosis;fluorosis caries dental;teeth measured dehydration;lesions fluorosis caries;dental fluorosis appears;absorption densities fluorosis;problem dental fluorosis;imaging dental;fluorosis teeth measured;fluorosis extracted teeth;fluorosis teeth;dental fluorosis increasing;dynamics lesions fluorosis;dental fluorosis;fluorosis caries;dental fluorosis causes;fluorosis result pitted;caries lesions dental;differentiate lesions fluorosis;measured dehydration imaging;dental fluorosis 1950;lesion structures fluorosis;dehydration imaging;lesions dental;densities fluorosis;lesions fluorosis extracted", "pdf_keywords": ""}, "294f8307f26eb3ec7bbf19f15092f3c473ece821": {"ta_keywords": "entity recognition;entity recognition classification;named entity recognition;classify entities;training named entity;systems classify entities;classifier relation extractor;musical compositions algorithm;recognition nonstandard musical;entity classifier;using imitation learning;imitation learning improves;learning classification;imitation learning;structured prediction learning;new imitation learning;recognition classification nerc;classify entities categories;musical compositions;structured prediction;classification;imitation learning approach;entity classifier relation;genre data;training intended domain;imitation learning reduces;named entity classifier;learning reduces structured;classification nerc;learning approach basketball", "pdf_keywords": ""}, "6a2c4a0f04c6ba2f6fbc171dcea8730423a298e5": {"ta_keywords": "neighbor retrieval metric;locality sensitive hashing;nearest neighbor retrieval;neighbor retrieval;approximate nearest neighbor;nearest neighbor;locality sensitive;locality;prune approaches sampling;learning prune approaches;retrieval metric;retrieval metric nonmetric;hashing;nearest;probe locality sensitive;learning prune;multi probe locality;approximate nearest;sensitive hashing;effective learning prune;probe locality;prune approaches;neighbor;retrieval;hashing lsh;vp tree;hashing lsh permutation;sensitive hashing lsh;focus approximate nearest;prune", "pdf_keywords": ""}, "20d4105b276da6d6d38ed3c1bfc436f76198c240": {"ta_keywords": "large event datasets;world event datasets;event datasets evaluate;event datasets;event based measurement;identities discovery planets;event datasets paper;events timeline;pairs events large;pairwise associations events;real world event;large event;associations events propose;events large event;events;events timeline paper;events large;performance social network;pairs events;event based;association pairs events;world event;social network;types events timeline;algorithms estimating cause;actor identities discovery;associations events;events introduce general;events introduce;human raters", "pdf_keywords": ""}, "695d4c04f6e4f7ba5f771ac7853fdbaa81713ae8": {"ta_keywords": "social knowledge graphs;learn latent topics;network embeddings;word network embeddings;embedding model learn;learning social knowledge;modal bayesian embedding;data embeddings;latent topics generate;network embeddings method;bayesian embedding model;latent topic space;open knowledge bases;bayesian embedding;embeddings method significantly;shared latent topic;knowledge bases;academic search network;embedding model;concepts shared latent;knowledge graphs;social network;latent topics;researchers knowledge base;embeddings;knowledge base;online social networks;embeddings method;embedding;knowledge bases experiments", "pdf_keywords": "topics embeddings learning;knowledge graphs embedding;social knowledge graphs;topic models embedding;latent topics embeddings;topics embeddings;embeddings learning social;modeling recommendation knowledge;networks knowledge bases;topic models;social network mining;social networks knowledge;learning social knowledge;graphs embedding;knowledge graphs;shared latent topic;bayesian embedding model;latent topic space;knowledge based search;including topic models;data embeddings;knowledge bases;embedding based models;embeddings learning;bayesian embedding;recommendation knowledge;online social networks;user modeling recommendation;modal bayesian embedding;models embedding based"}, "3b8494614903dc47579da30477b21b109b29f8cd": {"ta_keywords": "xmath0 state potts;dynamics xmath0 state;state potts model;study dynamics xmath0;dynamics xmath0;xmath0 state;potts model;state potts;xmath0;potts;dynamics;results study dynamics;model;study dynamics;state;results;report;study;report results study;results study;report results", "pdf_keywords": ""}, "74c881830a9cd7ea49795faa5c582b7ec56bd0bf": {"ta_keywords": "architecture speech recognition;noisy speech recognition;multichannel speech enhancement;new architecture speech;speech recognition tasks;ability multichannel speech;multichannel speech;denoising performance neural;speech recognition;architecture speech;speech enhancement;neural network architecture;neural beamformer;performance neural beamformer;applied noisy speech;speech data;speech enhancement mqa;neural networks;rrls based neural;neural network;clean speech data;noisy speech;noisy clean speech;speech recognition based;based neural networks;neural beamformer presence;based neural network;performance neural;denoising performance;learning rrls based", "pdf_keywords": "beamforming automatic speech;multichannel automatic speech;speech recognition asr;speech recognition;challenge automatic speech;automatic speech recognition;speech recognition popular;recognition speech recognition;speech recognition speech;beamforming networks;automatic speech;speech recognition report;channel beamforming networks;end automatic speech;recognition speech;asr neural network;automatic recognition asr;recognition asr;speech recognition based;recognition asr ability;recognition asr neural;multi channel beamforming;optimize conventional asr;channel beamforming;beamforming automatic;dereverberation beamforming beamforming;based beamforming automatic;beamforming networks investigation;integrating dereverberation beamforming;dereverberation beamforming"}, "3d3b1300c7cd6a820a6d08605248f875a3ad20b9": {"ta_keywords": "hop reasoning models;hop reasoning widely;hop reasoning model;multi hop reasoning;hop reasoning popular;rule based reasoning;hop reasoning;multiply hop reasoning;interpretable link prediction;reasoning models;reasoning model multi;hop reasoning problem;metrics using interpretability;based reasoning model;interpretability global interpretability;reasoning model;global interpretability evaluation;interpretability evaluation models;prediction multi hop;interpretability scores rules;reasoning models terms;reasoning model outperforms;interpretability evaluation;interpretability evaluation design;reasoning manually;global interpretability;understand reasoning manually;reasoning manually annotate;using interpretability;obtain interpretable link", "pdf_keywords": "hop reasoning learning;reasoning knowledge graphs;reasoning rule mining;hop reasoning knowledge;hop reasoning models;learning reasoning rule;reasoning chains multi;reasoning models widely;interpretability reasoning chains;reasoning learning reasoning;knowledge graphs;multi hop reasoning;reasoning chains;reasoning models;learning reasoning;reasoning learning;knowledge graphs kgs;reasoning knowledge;reasoning models terms;rule mining multi;prediction performance interpretability;annotate interpretability;hop reasoning aims;manually annotate interpretability;rule mining;propose rule mining;evaluate interpretability reasoning;interpretability reasoning;hop reasoning;annotate interpretability scores"}, "e0c54e18cf2372414042bf67eed0272b0a432190": {"ta_keywords": "dynamics pedestrian crossing;pedestrian crossing busy;pedestrian crossing;crossing busy area;crossing busy;study dynamics pedestrian;dynamics pedestrian;pedestrian;busy area city;crossing;busy area;city british columbia;dynamics;city;area city;columbia;british columbia;area city british;area;city british;study dynamics;busy;comprehensive study dynamics;paper present;comprehensive;paper present results;results comprehensive;paper;present results comprehensive;comprehensive study", "pdf_keywords": ""}, "7da967be8f6367f6174bf99d0d019ff545ac5966": {"ta_keywords": "semantic annotation process;annotation semantic analysis;semantic annotation methodology;semantic annotation semantic;semantic annotation;annotation semantic;sentences semantic annotation;annotation process semantic;annotated sentences semantic;abstract semantic annotation;annotation process;annotation methodology possible;annotation methodology;annotated data;semantic analysis user;semantic data analysis;annotated data reasonable;present semantic annotation;annotated sentences;annotation;semantic representation approach;semantic analysis;semantic data;annotated corpus;semantic representation;resulting annotated corpus;sufficient annotated data;analysis resulting annotated;annotated corpus consists;resulting annotated", "pdf_keywords": ""}, "49db57f300b270f16cbcb1891ca39e16981d42b5": {"ta_keywords": "tracking covid activities;track covid activity;tracking covid;track covid;covid activity;covid activity provides;covid activities;relevant tracking covid;analysis pandemic pandemic;analysis pandemic;comprehensive analysis pandemic;pandemic relevant public;pandemic pandemic;covid;covid activities augmenting;pandemic pandemic relevant;pandemic;pandemic relevant;signals covidcast;covidcast;used track covid;public health reporting;signals covidcast api;covidcast api;covidcast api present;public health;distribution number particles;health reporting;information relevant tracking;particles binary mixture", "pdf_keywords": ""}, "bad416f073a08086ee428e5a264eac3a7d3251e5": {"ta_keywords": "parameters biomedical scaffold;structural parameters biomedical;biomedical scaffold;scaffold means image;biomedical scaffold means;image search algorithm;image search;scaffold;scaffold means;parameters biomedical;search algorithm fig;structural parameters;means image search;structural;search algorithm;method structural parameters;method structural;algorithm fig fig;algorithm fig;biomedical;new method structural;search;algorithm;image;parameters;means image;paper;method;new method;fig fig", "pdf_keywords": ""}, "d9dbdd254b02ef1af2769af403cba373c1b1bcb1": {"ta_keywords": "speaker diarization based;clustering speaker representations;speaker diarization;speaker diarization method;speaker diarization problem;based speaker diarization;speaker label permutation;method speaker diarization;extraction clustering speaker;clustering speaker;speaker segment labels;formulate speaker diarization;multi speaker conversations;speaker segment;multi speaker segment;speaker representations;speaker conversations;speaker representations propose;overlapping speech training;speech mixtures proposed;speaker conversations just;speech training inference;speech mixtures;network based speaker;speech training;corresponding multi speaker;simulated speech mixtures;speaker label;recorded multi speaker;diarization based neural", "pdf_keywords": "neural network speaker;neural speaker diarization;network speaker diarization;speaker diarization challenging;speaker diarization systems;speaker diarization;recordings speaker diarization;speaker diarization method;clustering speaker representations;speaker diarization problem;problem speaker diarization;speaker diarization realize;typical speaker diarization;end neural speaker;speaker label permutation;extraction clustering speaker;formulate speaker diarization;speaker diarization important;clustering speaker;speaker representations;simulated speech mixtures;neural speaker;speech mixtures;multi speaker recordings;speech mixtures evaluated;network speaker;formulate speaker;speaker recordings;speaker label;model formulate speaker"}, "5931c8ac145baf17cec9effc25c051049b7dfd4c": {"ta_keywords": "grounding dialogue task;dialogue agent accurately;grounded neural dialogue;spatial grounding dialogue;grounding dialogue;dialogue task;neural dialogue model;dialogue agent;neural dialogue;dialogue model;dialogue model successfully;dialogue task involving;referents partner utterances;partner utterances;dialogue;partner utterances using;observable reference game;agent accurately grounds;utterances;grounded neural;reference game;utterances using structured;accurately grounds referents;resolve references agent;present grounded neural;self action evaluations;reference game evaluate;references agent;utterances using;pragmatic generation", "pdf_keywords": "referring expression generation;referent present dialogue;predict referents;grounding language referents;predict referents mention;able predict referents;referring expressions;dialogue generation;present dialogue generation;language referents;utterance referent;dialogue generation based;inside referring expressions;person utterance referent;mention agent utterance;language referents world;referring expression;utterance referent present;task referring expression;referring expressions paper;referents mention agent;dialogue task;present dialogue task;recurrent memory based;referents world context;agent utterance approach;language games model;propose recurrent memory;selection language games;inside referring"}, "6c4258f6a6a4bee7b9d914379c44aea6073cdc37": {"ta_keywords": "energy disaggregation adaptive;building energy disaggregation;energy disaggregation;energy disaggregation problem;technique energy disaggregation;disaggregation adaptive filtering;novel disaggregation algorithm;energy disaggregation present;disaggregation algorithm theoretical;disaggregation adaptive;disaggregation algorithm;optimality energy disaggregation;disaggregation based;power consumption signals;algorithm disaggregation based;particular disaggregation;power consumption signal;new algorithm disaggregation;particular disaggregation problem;disaggregation based combination;novel disaggregation;disaggregation;combination novel disaggregation;algorithm disaggregation;disaggregation problem;disaggregation problem solved;disaggregation problem recovering;disaggregation present;level power consumption;adaptive filtering", "pdf_keywords": "energy disaggregation based;disaggregation energy data;disaggregation building energy;energy disaggregation;disaggregation power consumption;models disaggregation energy;load disaggregation appliances;supervised approach disaggregation;disaggregation appliances;method energy disaggregation;disaggregation appliances develop;dynamical models disaggregation;disaggregation energy;building energy data;power consumption data;energy disaggregation fundamental;disaggregation based;disaggregation building;disaggregation power;models disaggregation;consumers power consumption;develop algorithm disaggregation;energy data;consumption data residential;energy data using;algorithm disaggregation building;approach disaggregation;energy data provides;approach disaggregation disaggregation;appliances develop algorithm"}, "3dcf9c900f5f28e082a2fcdea4763b6063a76f09": {"ta_keywords": "defeasibility reasoning datasets;defeasible reasoning achieves;reasoning datasets;defeasible reasoning;question defeasible reasoning;defeasible reasoning mode;reasoning mode reasoning;reasoning datasets performance;reasoning achieves;reasoning conclusions overturned;present defeasible reasoning;reasoning mode;answering question defeasible;defeasibility reasoning;reasoning achieves state;reasoning conclusions;scenario simply answering;reasoning;reflexively answer question;input answering question;simply answering reflexively;performance defeasibility reasoning;answering reflexively answer;answering reflexively;input answering;open question cognitive;given question defeasible;answering question;question cognitive science;additional input answering", "pdf_keywords": "visual question answering;answering natural language;question answering natural;question answering tasks;question answering;graphs visual attention;natural question modeling;answering tasks computational;answer question answering;generating natural language;inference graphs;question modeling;scenario inference graphs;domain question answering;inference graph;defeasible reasoning machine;answering using scene;reasoning machine learning;computational reasoning heterogeneous;tasks computational reasoning;natural language explanations;question answering using;graphs help machines;learning graph encodings;graph convolutional networks;inference graphs help;reasoning machine;machines defeasible reasoning;language explanations visual;answering tasks"}, "dbdb7f25f1538c2a2885d3992e5320e2ee5c23a1": {"ta_keywords": "equation solving teaching;teach students dynamics;solving teaching computer;solving teaching;teaching computer agent;pedagogical teachable agent;teachable agent;platform used teach;students dynamics;dynamics tug war;teaching computer;pedagogical teachable;teach students;sim student pedagogical;learning teaching;student pedagogical teachable;dynamics tug;students learn algebra;classroom;teaching;used teach students;tug war;control dynamics tug;pedagogical;teach;student pedagogical;students learn;learn algebra equation;teachable agent commits;classroom settings learning", "pdf_keywords": ""}, "660119405bb48777cd71d85caa5ec2e90a336caf": {"ta_keywords": "historical text normalization;text normalization far;text normalization;normalization text;normalization text written;approach normalization text;normalization techniques;normalization far;new approach normalization;normalization techniques analysing;normalization;proposed normalization techniques;proposed normalization;approach normalization;statistical machine translation;translation neural encoder;categories proposed normalization;machine translation;machine translation neural;decoder models;translation neural;character based statistical;encoder decoder models;decoder models paper;report experiments languages;metrics character based;neural encoder decoder;text written terms;encoder decoder;decoder", "pdf_keywords": "machine translation normalization;automatic normalization texts;automatic normalization text;normalization text information;translation normalization;translation normalization systems;normalization text;normalization texts long;normalization texts;normalization text different;models normalization text;automatic normalization;approaches automatic normalization;learning models normalization;normalization quality;statistical machine translation;normalization techniques;new normalization;normalization;machine translation;machine translation smt;assessing normalization quality;new normalization strategy;propose new normalization;normalization quality paper;normalization strategy;normalization strategy based;methods assessing normalization;normalization techniques analyzing;proposed normalization techniques"}, "98554bd8a15172e9a6ef3cc3db3bc52504110fc9": {"ta_keywords": "stochastic bandits;stochastic bandits study;bai stochastic bandits;regret minimization;regret achievable algorithm;lower bounds regret;bandits study certain;regret minimization rm;bandits study;study regret minimization;bounds regret achievable;bandits;optimality season;bounds regret;perform optimally;probability study regret;pareto optimality season;optimality season function;optimality;bai objectives pendulum;seasonality pareto optimality;simultaneously perform optimally;objectives pendulum;optimally;optimally rm;perform optimally rm;pareto optimality;regret achievable;objectives pendulum described;competition fixed confidence", "pdf_keywords": "algorithm stochastic bandits;stochastic bandits;stochastic bandits competing;stochastic bandits corruptions;applicable adversarial bandits;bandits corruptions stochastic;corruptions stochastic bandits;adversarial bandits;adversarial bandits goal;bandits competing targets;bandits;bandits competing;bandits goal;bandits goal balance;bandits corruptions;probabilistic sequential shrinking;optimally rm;targets inevitable algorithm;perform optimally rm;shrinking algorithm stochastic;optimally rm bai;algorithm stochastic;inevitable algorithm compromise;inevitable algorithm;corruptions stochastic;probabilistic sequential;perform optimally;algorithm compromise rm;identify optimal;probabilistic"}, "6b3fa9157a8120a6eb86ae06a93611a1fcd9e219": {"ta_keywords": "databases soft;resulting databases soft;soft database;given soft database;large bibliographic databases;underlying hard database;hard database web;soft database noisy;scienti databases;web large unstructured;bibliographic databases;databases soft contain;large unstructured information;hard database;paper scienti databases;soft database resulting;databases;scienti databases used;database web;unstructured information;model soft database;generate large bibliographic;database noisy;database web social;resulting databases;hard database given;database noisy version;database;databases used generate;structure web", "pdf_keywords": ""}, "ef9ddbc35676ce8ffc2a8067044473727839dbac": {"ta_keywords": "problem expressiveness softmax;softmax;softmax distributed word;expressiveness softmax;limited softmax;expressiveness softmax based;models limited softmax;softmax based models;softmax bottleneck letter;softmax based;softmax bottleneck;limited softmax bottleneck;body fig softmax;model natural language;softmax distributed;word embeddings;fig softmax;distributed word embeddings;formulate language modeling;language modeling matrix;treebank wiki;word embeddings does;treebank;language modeling;treebank wiki 47;fig softmax distributed;embeddings;formulate language;rigid body elastic;perplexities penn treebank", "pdf_keywords": "softmax based language;softmax deep recurrent;softmax deep;universal softmax;improved performance softmax;universal softmax based;softmax;exist universal softmax;expressiveness softmax deep;performance softmax;investigate expressiveness softmax;expressiveness softmax;softmax based;performance softmax baselines;softmax baselines;language modeling datasets;neural language model;recurrent neural networks;networks rnn;deep recurrent neural;deep recurrent;softmax baselines particular;generalization recurrent neural;factorization perspective recurrent;recurrent neural network;dialog dataset improved;rank neural language;generalization recurrent;sacri\ufb01cing generalization recurrent;networks rnn based"}, "d29f155060f96becef0247ee77dc038f96b2d983": {"ta_keywords": "speech translation systems;translation processing;translation speed;translation systems;translation processing process;translation speed accuracy;translation systems difficult;conventional speech translation;based translation processing;speech translation;effect translation speed;translation based translation;shortening translation unit;translation based;shortening translation;method shortening translation;translation unit;translation unit examine;delay begining utterance;examine effect translation;utterance end synthesis;based translation;effect translation;demonstration translation based;translation;lectures real time;process conventional speech;begining utterance;news lectures real;large delay", "pdf_keywords": ""}, "d415b724fbc35afcc8dd91738123edfa6a5db634": {"ta_keywords": "policy gradient algorithms;deep reinforcement learning;reinforcement learning present;deep reinforcement;gains reinforcement learning;reinforcement learning predicting;policy gradient;reinforcement learning outperforms;reinforcement learning;outperforms reinforcement learning;learning outperforms reinforcement;progress policy gradient;outperforms reinforcement;reinforcement;paper deep reinforcement;performance gains reinforcement;gains reinforcement;algorithmic progress policy;gradient algorithms;gradient algorithms paper;level optimizations;algorithms paper deep;level optimizations algorithm;potential;learning;algorithmic progress;code level optimizations;learning present;learning present simple;gradient", "pdf_keywords": "deep policy gradient;policy gradient algorithms;performance policy gradient;policy gradient methods;deep reinforcement learning;gradient descent reinforcement;policy optimization promising;policy optimization gradient;proximal policy gradient;policy gradient;study policy gradient;proximal policy optimization;deep reinforcement;algorithms proximal policy;policy optimization;region policy optimization;deep policy;use deep reinforcement;policy optimization trpo;descent reinforcement learning;trained agents;train trained agents;art policy gradient;popular deep policy;reinforcement learning rl;gradient algorithms proximal;suggest deep policy;gradient methods trust;policy optimization ppo;algorithms improve agent"}, "3315dee45b1edb8f8286816629de7b8c31d270d6": {"ta_keywords": "search strategies political;search behavior participants;affect information search;search strategy effect;search behavior;strategies political judgments;judgments subjects voting;social signals affect;political judgments subjects;information search strategies;subjects voting;affect information;political judgments;search focused different;social signals;search strategies;social cues;information search;search focused;signals affect information;subjects voting task;search strategy;candidates policies study;participants subjects social;social cues fact;simple social signals;candidates policies;political science;strategies political;evaluation candidates policies", "pdf_keywords": ""}, "387754dc8d4185fadd7c3c15e43956a4d085e8fe": {"ta_keywords": "useful permutation search;nearest neighbor search;permutation search methods;efficiently retrieve nearest;nearest neighbor query;graph based retrieval;retrieve nearest neighbor;approximate nearest neighbor;nearest neighbor;permutation search;similar permutation query;neighbor search;neighbor search ease;retrieve nearest;permutation methods reasonably;distance permutations good;proximity graph based;pivots permutation methods;points permutations similar;neighbor query;based retrieval variety;lists pivots permutation;ranked lists pivots;permutation methods pitted;useful permutation;distance permutations;based retrieval;constructing ranked lists;similar permutation;permutation based methods", "pdf_keywords": "nearest neighbor search;proximity retrieval based;approach proximity retrieval;proximity retrieval;retrieval based permutations;similarity sparse vectors;approximate nearest neighbor;similarity sparse;nearest neighbor;benchmarked permutation methods;benchmarked permutation;based permutations benchmarked;permutations benchmarked;neighbor search;cosine similarity sparse;permutations instead vectors;visual descriptors computation;permutations benchmarked permutation;neighbor search generic;permutation methods approximate;better retrieval performance;vectors original distances;retrieval based;better retrieval;slightly better retrieval;descriptors computation;visual descriptors;sparse vectors relies;retrieval;retrieval performance"}, "60a121c55b5144bfe3aef5b6ea8959a9f6dd12ae": {"ta_keywords": "speech enhancement algorithms;speech enhancement noisy;multiple speech enhancement;problem speech enhancement;speech enhancement;enhancement noisy mixture;enhancement noisy;enhancement algorithms;enhancement algorithms present;clean speech based;classification combining algorithms;algorithms classification combining;obtained clean speech;speech based outputs;frequency mask;time frequency mask;clean speech;classification combining;applied noisy mixture;noisy mixture;multiple speech;noisy mixture consider;frequency mask obtained;algorithms applied noisy;speech based;enhancement;combining algorithms;various machine learning;learning algorithms;machine learning algorithms", "pdf_keywords": ""}, "ba56bb1eb67b188a89060058ef8ad02ce3c660ac": {"ta_keywords": "simple bead spring;semiflexible body water;bead spring model;preparation semiflexible body;bead spring;recipe preparation semiflexible;preparation semiflexible;simple bead;use simple bead;body water based;water based;simple recipe preparation;spring model;recipe preparation;water based use;semiflexible body;simple recipe;body water;present simple recipe;bead;recipe;water;preparation;semiflexible;spring;body;model;based use simple;use simple;based use", "pdf_keywords": ""}, "9fc33c53d1f59aa9fd7f1b642c3859900865b0e3": {"ta_keywords": "properties complex networks;complex networks;complex networks small;structured house cards;properties complex;dynamical properties complex;statistical properties simple;networks;house cards;networks small number;semi structured house;statistical properties;semi structured;structured house;model semi structured;analysis statistical properties;queries like set;structural dynamical properties;expansion class prediction;house cards paper;networks small;class prediction;properties simple;analysis structural dynamical;dynamical properties;structured;queries;properties simple model;like set expansion;queries like", "pdf_keywords": ""}, "d7729f2ff21f97d56d10c54adc1f1f5ffbec9e5c": {"ta_keywords": "lasers management oral;treat oral leukoplakia;oral leukoplakia recurrence;oral leukoplakia;lesions leukoplakia;premalignant lesions leukoplakia;technique treat oral;lesions leukoplakia erythroplakia;strategies oral premalignant;oral premalignant lesions;lasers management;management oral premalignant;control strategies oral;different lasers management;surgical technique treat;leukoplakia recurrence;treat oral;leukoplakia recurrence article;surgical surgical therapies;leukoplakia erythroplakia presented;application different lasers;surgical excision using;leukoplakia;surgical therapies;excision using scalpels;different lasers;leukoplakia erythroplakia;surgical procedure improving;surgical excision;surgical technique", "pdf_keywords": ""}, "649eb9fd9a18f9601270b7fcde8d6548bfc6ec75": {"ta_keywords": "speech separation recognition;speech recognizing separated;dataset speech separation;speech recognition end;overlapping speech recognizing;speaker speech separation;separating overlapping speech;speaker speech recognition;speech separation;speech recognizing;models automatic speech;separation recognition task;automatic speech;separation recognition benchmark;multi speaker speech;end automatic speech;separation recognition;automatic speech recognition;speech recognition;speech recognition model;overlapping speech;dataset speech;2mix dataset speech;multi speaker end;monaural multi speaker;recognizing separated;recognition end end;speech recognition contrast;speech generated wsj;speaker mixed speech", "pdf_keywords": "end speech recognition;attention modules multispeaker;speaker parallel attention;speech recognizing separated;speech separation recognition;multispeaker speech recognition;overlapping speech recognizing;speaker speech multi;speaker speech recognition;end multi speaker;recognition speaker parallel;speech recognition speaker;multi speaker speech;modules multispeaker speech;speech multi channel;speech separation convolutional;multi talker speech;multi speaker trained;speech multi;speech recognizing;multispeaker speech;multi peaker speech;parallel attention modules;separating overlapping speech;peaker speech mixtures;speech recognition extending;speech recognition;attention based encoder;recognition speaker;talker speech separation"}, "5884948777dfc003ba49e1513420830616281839": {"ta_keywords": "muse bilingual lexicon;learns unified multilingual;bilingual lexicon induction;cross lingual tasks;methods muse bilingual;trained monolingual representations;unified multilingual representations;bilingual lexicon;trained monolingual;lingual tasks;independently trained monolingual;contextualized representations multilingual;multilingual representations;representations multilingual bert;muse bilingual;cross lingual objectives;multilingual bert;monolingual representations;monolingual representations shared;learning representations alignment;lingual ner benchmark;nll cross lingual;cross lingual;monolingually cross lingual;lingual objectives jointly;unified multilingual;diverse cross lingual;lingual tasks framework;multilingual representations using;lingual", "pdf_keywords": "bilingual word embeddings;crosslingual word embedding;transfer word embedding;unsupervised cross lingual;lingual transfer learning;representations multilingual bert;unsupervised bilingual;multilingual bert;method unsupervised bilingual;lingual ner benchmark;word embedding alignment;unsupervised bilingual word;cross lingual ner;conll cross lingual;cross lingual transfer;word embeddings unsupervised;lingual transfer word;cross lingual;reaccreting word embeddings;cross linguistic transfer;bilingual dictionaries framework;multilingual bert produces;bilingual dictionaries;based bilingual dictionaries;linguistic transfer tasks;word embedding;embeddings unsupervised cross;lingual transfer;promising approach crosslingual;contextualized representations multilingual"}, "56bc2a1eebedab3e452a7ca3969aa1e4dd5946c3": {"ta_keywords": "diversified ranking;diversified ranking framework;diversified diversified ranking;diversity influence maximization;embedding diversification utility;maximization result diversification;diversification diversity ranking;diversification utility maximization;heuristics embedding diversification;diversification utility;embedding diversification;diversified diversified;propose diversified diversified;incorporate node diversity;node diversity influence;diversity ranking crucial;diversified im optimization;diversity ranking;result diversification diversity;diversification diversity;diversified;paper propose diversified;formulate diversified;node diversity;result diversification;influence maximization im;diversification;nodes simultaneously influential;influence maximization;diversified im", "pdf_keywords": ""}, "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30": {"ta_keywords": "communicative agents;language coordination agents;conversational partners language;human communication;group communicative agents;language coordination;adapting conversational partners;communicate unseen agents;humans interacting common;quickly adapting communicate;adapting communicate;component human communication;adapting communicate unseen;communicate;communication;coordination agents quickly;communication modeled socio;coordination agents;agents different linguistic;conversational partners;humans interacting;partners language abilities;communicative agents coordinate;communicative;human communication man;shot language coordination;communication communication;group humans interacting;dynamics communication;communication modeled", "pdf_keywords": "language coordination game;multilingual referential games;referential games;shot language coordination;language coordination;referential games listener;round referential games;coordination game;referential games important;language coordination participants;interlocutors learning feedback;theory ofmind;coordination game order;language shot language;interlocutors learning;listener language shot;shot language;adapt interlocutors learning;propose theory ofmind;language shot;ofmind model learn;theory ofmind model;games listener ability;important shot language;learning feedback;learning feedback previous;shared goal multi;listener interested learning;games;listener language"}, "3c57a1aa483d8bffe1339914b80d2913f2dc8376": {"ta_keywords": "improves feature matching;semi supervised;adversarial networks gans;semi supervised learning;semian semi supervised;generative adversarial networks;good semisupervised learning;novel feature matching;semisupervised learning requires;generative adversarial;networks gans;semisupervised learning;objective good semisupervised;based generative adversarial;feature matching;gans shown effective;gans;networks gans shown;good semisupervised;discriminator objective good;good discriminator performance;adversarial networks;good discriminator;discriminator performance good;gans shown;feature matching multiple;semisupervised;adversarial;benchmark datasets;effective classification tasks", "pdf_keywords": "images semi supervised;semi supervised;train semi supervised;supervised semi supervised;semi supervised learning;propose semi supervised;supervised semi;semi supervised machine;semi supervised semi;samples semi supervised;approach semi supervised;semi supervised approach;networks gans demonstrated;networks gans;generative adversarial networks;supervised;gans demonstrated;adversarial networks gans;generative adversarial;feature space generative;generative structure discriminator;supervised learning;based generative adversarial;supervised machine;gans;supervised machine learning;space generative adversarial;gans demonstrated able;large unlabeled data;unlabeled data boost"}, "866f231970f93f4a201febc2fb46aff06f501e4b": {"ta_keywords": "normalization historical language;automatic normalization historical;normalizing words interactive;rules normalizing words;normalizing words;automatic normalization;semi automatic normalization;historical language data;tokens given gender;rules normalizing;normalization historical;language data;normalizing;normalization;gender binary mixture;words interactive way;lexicon lookup;modern word forms;words interactive;annotator;inter annotator;gender binary;rewrite rules normalizing;historical language;rule entries;lexicon lookup rewrite;given gender binary;word forms;lexicon;modules lexicon", "pdf_keywords": ""}, "4d96ec46cda5d3b223fc7d33a920ab85864ea36d": {"ta_keywords": "protein function prediction;predict protein;descriptions protein annotation;correctly predict protein;predict protein function;features classified protein;neural descriptions protein;protein annotation;features proteins amino;classified protein;protein annotation primary;features proteins;descriptions protein;motifs protein sequences;functional motifs protein;classified protein novel;important features proteins;protein sequences;motifs protein;protein novel functional;function prediction annotation;annotation based attention;protein novel;hco superfamily providing;amino acid sequences;proteins;superfamily providing insight;protein function;hco superfamily;protein function amino", "pdf_keywords": ""}, "6c170fe3fec5a477c938d07fa00935bb6f7b87cc": {"ta_keywords": "based speech synthesis;speech synthesis;based voice conversion;speech synthesis using;gmm based voice;voice conversion;sample based speech;high quality speech;voice conversion vc;predict joint speech;converted speech;speech features formulation;speech tts synthesis;based text speech;speech quality;speech quality speaker;individual speech features;text speech;quality speech keeping;speech features group;converted speech paper;term speech quality;joint speech features;individuality converted speech;speaker individuality converted;utilizes individual speech;quality speech;speech features;model hmm based;improvements term speech", "pdf_keywords": ""}, "adac290d72c86c186837a884aae922bee4dee684": {"ta_keywords": "transpositions difficult misspellings;misspelling error rates;model human reading;text typos misspelling;naturally occurring misspelling;difficult misspellings contain;human reading;human readers;readout entire genome;novel approach readout;human readers cope;combinations human readers;misspelling word substitutions;written human readers;difficult misspellings;occurring misspelling;genome human brain;misspelling word;typos misspelling word;text written human;misspellings;misspelling;human reading presence;misspellings contain unexpected;misspellings contain;text typos;typos misspelling;letter transpositions;correcting errors text;approach readout", "pdf_keywords": "humans read text;reading texts errors;experiment reading texts;text effect reading;reading behavior;happens humans read;misspellings error rates;effect reading behavior;reading texts;human language processor;eye tracking study;eye tracking experiment;reading behavior high;report eye tracking;read text;humans read;read respond texts;eye tracking;frequently encounter texts;pattern eye tracking;effect reading;errors happens humans;errors text effect;human language;texts containing errors;occurring misspellings;reading time;texts errors;analysis human language;word based surprisal"}, "23918ed366c60ae0ef85b0c80def63127f035e02": {"ta_keywords": "shredder maximizes privacy;heavy inference shredder;inference shredder enables;accuracy tradeoff privacy;dnn accuracy tradeoff;privacy accuracy controlled;tradeoff privacy accuracy;inference shredder;privacy accuracy;shredder enables accurate;cloud training;dnns text processing;dnn accuracy;minimal impact dnn;maximizes privacy minimal;deep neural;shredder;network learns;model cloud training;real world dnns;maximizes privacy;trained network;privacy minimal;trained network learns;impact dnn accuracy;accurate inference noisy;compute heavy inference;shredder enables;formally shredder;privacy", "pdf_keywords": ""}, "e7e1f5a713d20cdf31e732022731fdf0d8fb4fc5": {"ta_keywords": "explanations predictions classifiers;generating explanations predictions;level explanations nli;generating explanations;single sentence tagging;sentence tagging;generate explanations;sentence tagging possible;natural language natural;explanations pairs sentences;natural language;generate explanations pairs;interpreted natural language;explanations nli;token level explanations;recently generating explanations;language natural language;paper natural language;explanations nli need;natural language paper;attention make token;explanations predictions;attempts generate explanations;explicitly annotated;token level predictions;language natural;explanations pairs;instance learning mil;tagging possible generate;data explicitly annotated", "pdf_keywords": "neural entailment model;neural entailment;predicting entailment relations;predicting entailment;regularizing model attention;matrix neural entailment;generate explanations accompanying;encourages explanations generated;model predicting entailment;models generate explanations;entailment relations tokens;generate explanations;model attention;sentence tagging;sentences whitebox methods;model attention distributions;level explanations nli;attention;single sentence tagging;explanations accompanying predictions;entailment relation sentences;explicitly annotated;encourages explanations;tagging sentence pairs;sentence pairs nli;explanations nli;token level explanations;explanations generated better;attention distributions;attention matrix neural"}, "37ef7941909527aaf123d7b8f90adbf4606f4917": {"ta_keywords": "distributed computation distributed;distributed computation;distributed computing;distributed computing based;distributed memory;approach distributed computing;computing based distributed;computation distributed;used distributed computation;learn speech tags;based distributed memory;computation distributed environments;learning algorithm nlp;distributed implementations iterative;new approach distributed;parallel distributed implementations;distributed implementations;gibbs sampling machine;implementations iterative gibbs;probabilistic model learn;distributed;approach distributed;distributed memory used;speech tags newswire;based distributed;iterative gibbs sampling;algorithm nlp;parallel distributed;speech tags;distributed environments", "pdf_keywords": ""}, "58e5ce12c23f815e9b394220044eaf99b28cfffe": {"ta_keywords": "language processing prediction;prediction class languages;workshop language processing;language processing;languages;processing prediction;class languages;processing prediction class;workshop language;language;results workshop language;prediction class;prediction;present results workshop;processing;results workshop;class;workshop;present results;results;present", "pdf_keywords": ""}, "cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a": {"ta_keywords": "attention based explanations;accountability attention mechanisms;predictive accuracy attention;attention weights;fairness accountability attention;accountability attention;based explanations deceive;attention weights claimed;accuracy attention weights;manipulated attention based;biased gender;people thinking predictions;accuracy attention;attention tool auditing;attention based;explanations deceive people;biased gender minorities;model biased gender;confer interpretability;attention mechanisms ubiquitous;confer interpretability purportedly;attention mechanisms;based explanations;manipulated attention;predictions model biased;attention;explanations deceive;study manipulated attention;reliability attention;interpretability purportedly useful", "pdf_keywords": "attention maps deceptive;attention manipulation strategy;attention manipulation;attention weights;novel attention manipulation;predicting direction attention;attention maps;assign little attention;attention weights claimed;penalizing maximum attention;gender unknown attention;direction attention user;attention values propose;attention user gender;attention;attention penalizing;trained predict;propose novel attention;maximum attention;outputs machine learning;headed attention penalizing;manipulation scheme predicting;leads attention maps;direction attention;trained predict occupation;attention values;maximum attention paid;leads attention;novel attention;attention penalizing maximum"}, "79655bfc45039b4d7cfe6cc86d52a4ced492f43a": {"ta_keywords": "learning rank web;rank web pages;pages learning rank;learning rank methods;learning rank;rank web;learning rank method;methods learning rank;tuned learning rank;method learning rank;evaluation learning rank;rank methods ubiquitous;rank methods widely;torank learning rank;rank method efficient;rank methods;rank method collections;rank method;information retrieval;web pages learning;rank;pages learning;information retrieval paper;ubiquitous information retrieval;anonymized relevance signals;efficient learning methods;method efficient learning;retrieval;evaluation learning;precomputed anonymized relevance", "pdf_keywords": ""}, "5c3cc301a892094d5bfca3c41a78a3a8ebd755f8": {"ta_keywords": "boosted regression trees;dart ranking regression;additive regression trees;regression trees widely;regression trees;ranking regression classification;regression classification tasks;datasets dart outperforms;ranking regression;ensemble model boosted;able predict prediction;boosted regression;regression classification;classification tasks;specialization able predict;dart outperforms;regression trees marp;dart ranking;effectiveness dart ranking;predict prediction large;classification tasks using;datasets dart;classification;predict prediction;model boosted;ensemble;model boosted regression;able predict;boosted;dart outperforms state", "pdf_keywords": "dropouts ensemble trees;dropouts ensemble;employing dropouts ensemble;machine learning tasks;ensemble trees;ensemble trees muting;machine learning;muting features;algorithm ranking regression;muting complete trees;regression classi\ufb01cation tasks;trained di\ufb01ve descent;ranking regression classi\ufb01cation;specialization mart;deep neural;way employing dropouts;complete trees;employing dropouts;dropouts;ensemble;trees muting complete;number machine learning;deep neural network;issue specialization mart;algorithm ranking;specialization remains;specialization mart evaluate;datasets;specialization;features"}, "9eecfdb7c8ad9af4f3863e9f6ed857211fb710e7": {"ta_keywords": "user explanation leveraging;natural language explanations;generateenglish language explanations;explanation leveraging existing;language explanations ported;language explanations actions;explanations automatically extracts;explanations automatically;based explanation techniques;explanations actions suggested;generate salient explanations;salient explanations automatically;policies natural language;explanations actions;language explanations used;language explanations;trust user explanation;english language explanations;explanation leveraging;explanations ported applications;explanations generated;specific explanations generated;domain specific explanations;explanations ported;explanations generated case;natural language;salient explanations;user explanation;generates conversational english;explanation techniques", "pdf_keywords": ""}, "dfd8fc9966ca8ec5c8bdc2dfc94099285f0e07a9": {"ta_keywords": "empirical privacy;privacy utility tradeoff;privacy utility;benchmark privacy;benchmark privacy utility;common benchmark privacy;mitigate empirical privacy;empirical privacy guarantee;privacy;empirical privacy risk;art empirical privacy;differentially private mechanisms;privacy guarantee;privacy risk propose;private mechanisms;privacy guarantee paper;privacy risk;class differentially private;differentially private;mechanisms nearest neighbor;pedestrian pedestrians assumed;private mechanisms parameterizes;behavior pedestrians vicinity;nearest neighbor;behavior pedestrians;vicinity pedestrian pedestrians;nearest neighbor noised;pedestrians vicinity pedestrian;real text classification;pedestrian crowded environment", "pdf_keywords": "privacy metric reconstruction;empirical privacyutility tradeoff;empirical privacy metric;empirical privacyutility;empirical privacy;respect empirical privacyutility;mitigate empirical privacy;privacy metric;new privacy preserving;empirical privacy guarantee;mechanisms empirical privacy;privacy utility;privacy preserving;privacy preserving mechanism;quantify privacy utility;privacyutility tradeoff;privacyutility tradeoff real;privacy utility tradeoff;empirical privacy risk;new privacy;quantify privacy;propose new privacy;text based redacted;privacyutility;framework quantify privacy;personalized text use;automatically generating personalized;personalized text;generating personalized;risk original text"}, "e8c4a4e81084e17b0c71a6a69bdf1e4e2b6f6af1": {"ta_keywords": "contextual output sequences;speech separation multi;speech data composed;speech separation;speech recognition conditional;infer output sequences;speech data;multiple output sequences;output sequences probabilistic;including speech separation;observed speech data;models neural sequence;sequence sequence models;single output sequence;multi sequence models;sequence single output;speech data primary;multi speaker speech;neural sequence sequence;sequence models;sequential sources mixture;conditional models neural;conditional multi sequence;sequence model conditional;sequence transduction;multi sequence model;estimated contextual output;multiple sequential sources;sequence models established;output sequences", "pdf_keywords": "speech separation deep;speech separation multi;separation sequence learning;speech separation recognition;separation speech recognition;speaker speech separation;tasks speech separation;speech separation;talker speech separation;speech separation speech;including speech separation;speaker source separation;separation deep models;separation speech;speaker speech recognition;source separation sequence;speech recognition conditional;separation multi speaker;sequence learning;multi speaker speech;conditional models speaker;speech recognition tasks;multi talker speech;sequence learning deep;models speaker source;speech recognition;network speaker diarization;separation recognition;separation deep;tasks including speech"}, "59121b847fd7eb4cf92cbfccb54f1705733d8b65": {"ta_keywords": "recognition reverberant speech;dereverberation speech recognizer;speech recognizer;performance automatic speech;performance speech recognition;reverberant speech;noise performance speech;speech recognition;noise reverberation;noise reverberation introduce;problem recognition reverberant;recognition reverberant;automatic speech recognition;reverberant speech received;speech recognizer present;new preprocessor reverberation;automatic speech;reverberation mapping;reverberation;speech recognition severely;reverberation mapping preprocessor;preprocessor reverberation mapping;interconnection dereverberation speech;preprocessor reverberation;reverberation introduce;speech recognition problem;model adaptation techniques;conventional model adaptation;reverberation introduce parametric;model adaptation", "pdf_keywords": ""}, "cec37cd54a940bec818db7216cc1086672f3fec0": {"ta_keywords": "synsets individual word;synsets representing concepts;individual word senses;representing concepts nsing;synsets representing;word senses;method aligning synsets;set synsets representing;synsets xmath0;synsets individual;concepts nsing;synsets;duplicate synsets;concepts nsing various;synsets xmath0 type;deriving set synsets;set synsets;aligning synsets;number duplicate synsets;aligning synsets individual;senses;word senses paper;synsets inventory;mistake synsets;nsing various automatic;mistake synsets xmath0;duplicate synsets inventory;senses paper consider;paper mistake synsets;representing concepts", "pdf_keywords": ""}, "fa10752ab1768d1633001420b48be5e2518a4f80": {"ta_keywords": "bounded inconsistency based;data bounded inconsistency;experimental data bounded;bounded inconsistency;inconsistency based;concept bounded inconsistency;inconsistency based concept;analysis experimental data;experimental data;inconsistency;data bounded;analysis experimental;based concept bounded;approach analysis experimental;experimental;concept bounded;analysis;data;bounded;approach analysis;new approach analysis;based concept;based;concept;approach;new approach;propose new approach;letter;propose new;letter propose", "pdf_keywords": ""}, "9fcfbc662d4095d72eb9a4e1c4f5ae8f0ffc4222": {"ta_keywords": "coherence tomography thermal;tomography thermal imaging;tomography thermal;permeability secondary lesions;lesions teeth;imaging coupled dehydration;lesions teeth work;optical coherence tomography;thermal imaging;effect thermal dehydration;coherence tomography;dehydration secondary lesion;secondary lesions teeth;thermal dehydration;thermal dehydration secondary;thermal imaging coupled;bacterial acid infiltration;acid infiltration aqueous;lesion activity quantified;acid infiltration;infiltration aqueous;lesion activity;fluid permeability secondary;tomography;secondary lesion activity;infiltration aqueous solutions;lesions;lesion activity present;permeability secondary;fluid permeability", "pdf_keywords": ""}, "5801974fcebc11b4a8085fb02e77f792454caf7c": {"ta_keywords": "automated social skills;social skill speech;social skills training;training developing dialogue;social skills trained;skill speech language;social skills trainer;social skills design;social skills;provides social skills;skill speech;social skill;conventional social skills;skills trained human;performing social skills;improve social skills;developing dialogue;process social skills;autism;homework social skills;user speech;recognizes user speech;acquire social skills;user speech language;trained human computer;autism traits;improve skill using;social interaction acquire;trained human;dialogue named automated", "pdf_keywords": ""}, "20f166f7809d1af9065cd1c71ec1e38d5d92993f": {"ta_keywords": "deep reinforcement learning;reinforcement learning guards;deep reinforcement;learned reward shaping;reinforcement learning periodically;accelerates deep reinforcement;reinforcement learning;problems deep reinforcement;policy accelerates deep;intrinsic fear learned;fear learned reward;learned reward;reinforcement;prey model;reward shaping policy;model prey;reward shaping;predator prey model;policies periodic catastrophes;catastrophe perturbed objective;dynamics simple predator;prey approach;prey model prey;learning periodically revisits;learning periodically;learning predict;force speed prey;learning predict probability;model prey allowed;prey", "pdf_keywords": ""}, "c43d9d868f5288738cd625d365f0b3a5c18d4a20": {"ta_keywords": "interpretation speech translation;interpretation corpus;simultaneous interpretation speech;simultaneous interpretation corpus;interpretation systems corpus;interpretation corpus collected;interpretation speech;analyzing human speech;speech translation;professional simultaneous interpreters;talk translated text;corpus used analyze;speech translation data;describes english japanese;translated text time;english simultaneous interpretation;time constraints speech;particular talk translated;japanese english simultaneous;interpreter experience objective;interpreter experience;speech time;constraints speech time;human speech;talk translated;simultaneous interpreters;speech time constraints;describes english;interpreters;time constraints corpus", "pdf_keywords": ""}, "3be5e7310b1bec9b4431ad0f1264f536b6a39f14": {"ta_keywords": "translation models;machine translation models;cascaded translation models;pivot based translation;translation models used;translation quality;pivot language translation;language translation quality;translation models use;machine translation;crowdsourced movie subtitles;impact translation quality;level machine translation;cascaded translation;compare cascaded translation;language translation;translation quality phrase;movie subtitles;based translation;subtitles;quality phrase table;subtitles demonstrate word;movie subtitles demonstrate;impact translation;based translation applied;translation quality paper;explore impact translation;offensive performance;subtitles demonstrate;datasets crowdsourced movie", "pdf_keywords": "machine translation models;machine translation model;machine translation word;level machine translation;machine translation machine;translation machine translation;machine translation systems;machine translation;translation machine;based machine translation;translation models;investigate machine translation;translation word level;translation systems;translation models used;machine translation particular;improve translation quality;machine translation applied;machine translation 07;words improve translation;translation quality;translation model;translation models reduce;crowdsourced movie subtitles;character transliteration lexical;transliteration lexical mappings;translation systems combine;transliteration lexical;analysis machine translation;pivot language effectiveness"}, "6e7e095f46deb297713dcde05991faf635768d29": {"ta_keywords": "algorithmic fairness researchers;conceptualizations race fairness;race fairness research;adopted algorithmic fairness;argue algorithmic fairness;algorithmic fairness frameworks;algorithmic fairness;approach algorithmic fairness;race fairness;fairness research;fairness researchers;fairness frameworks;fairness frameworks argue;fairness researchers need;algorithmic unfairness;race theory sociological;fairness research drawing;algorithmic unfairness paper;conceptualizations race;race based conceptualization;race racial categories;attribute treating race;conceptualization race;conceptualization race fixed;social structure race;race attribute structural;conceptualizing operationalizing race;sociological work race;treating race attribute;based conceptualization race", "pdf_keywords": "conceptualizations race fairness;race fairness research;race fairness;race prior fairness;race health equity;conceptualizations race;racial classi\ufb01cation scienti;racial categories;race theory sociological;racial classi\ufb01cation;debate racial categories;use racial categories;racial categories turn;racial categories represent;history racial categories;ground conceptualizations race;race health;racism racial class;survey history racial;fairness research;history racial classi\ufb01cation;racial class gcation;debate racial;sociological work race;race theory;racism racial;racial class;racial;use racial;race ethnicity ground"}, "4d10d7c02ce01d71f11c296b09b389c6f20b354b": {"ta_keywords": "data labeling crowdsourcing;labeling crowdsourcing;labeling crowdsourcing shared;crowdsourcing;labeling public crowdsourcing;crowdsourcing tutorial;crowdsourcing shared;public crowdsourcing;crowdsourcing tutorial present;crowdsourcing marketplaces;public crowdsourcing marketplaces;crowdsourcing marketplaces present;crowdsourcing shared leading;expertise crowdsourcing;expertise crowdsourcing tutorial;efficient data labeling;data labeling;industrial expertise crowdsourcing;labeled data efficiently;data labeling public;labeled data;introduction data labeling;labeling;efficient label;labeled;efficient label collection;labeling public;quality labeled data;group animals based;high quality labeled", "pdf_keywords": ""}, "191169031c7646c02ecb1aaa9c8a6b6e05009730": {"ta_keywords": "graphene based materials;dielectric properties graphene;graphene based;graphene;performance graphene based;graphene extensively studied;graphene extensively;properties graphene;properties graphene extensively;high performance graphene;performance graphene;emi shielding film;film dielectric properties;film dielectric;shielding film;thickness film dielectric;materials application electromagnetic;shielding film today;emi shielding;dielectric properties;interference emi shielding;average dielectric;average dielectric loss;dielectric;dielectric loss;db average dielectric;average shielding;dielectric loss important;exhibits average dielectric;shielding", "pdf_keywords": ""}, "19b6e7158ee4f13caa004a0b6c6a6e0ef965ea8f": {"ta_keywords": "speech recognition challenge;recognition speech;speech recognition;speech recognition speech;automatic speech;recognition speech everyday;recognition speech recognition;evaluation automatic speech;automatic speech recognition;discriminative acoustic language;advance recognition speech;speech everyday environments;acoustic language modelling;overview chime series;discriminative acoustic;data training;overview chime;state art discriminative;recognition challenge series;training evaluation automatic;chime series;chime series including;data training evaluation;chime;simulated data training;recognizing;acoustic language;recognizing person;recognition challenge;training data", "pdf_keywords": ""}, "53f6c82035d43a19b9c8be0de651cae25bdd4bda": {"ta_keywords": "transcripts automatic speech;spoken word transcript;automatic speech;automatic transformation spoken;transcripts automatic;make transcripts automatic;speech recognition;automatic speech recognition;speech recognition results;clean transcriptstyle text;transcript style language;word transcript;word transcript style;clean transcriptstyle;loglinear probabilistic framework;disfluency log linear;transcript style;transcriptstyle text best;make transcripts;disfluency log;features combined loglinear;combined loglinear probabilistic;transcriptstyle;transcriptstyle text;product clean transcriptstyle;attempting make transcripts;transcripts;loglinear probabilistic;transformation spoken word;recognition results disfluency", "pdf_keywords": ""}, "821532ecef5bc2252823b190c35f1e4c44ddc41c": {"ta_keywords": "word alignment parallel;multilingual word aligners;learning parallel text;learning translation lexicons;contextualized word embeddings;alignment parallel corpora;models word alignment;learning translation;translation lexicons cross;trained language models;lexicons cross lingual;word embeddings;train multilingual word;tuning parallel text;parallel corpora;word alignment;trained contextualized word;word embeddings derived;translation lexicons;including learning translation;parallel corpora wide;learning parallel;parallel text objectives;explicit training parallel;cross lingual;unsupervised learning parallel;language models;train multilingual;embeddings derived multilingually;language pairs", "pdf_keywords": "translation models alignment;machine translation models;alignments machine translation;translation models;machine translation;multilingual word alignment;multilingual contextualized embeddings;word aligners trained;softmax;softmax sparse;trained embedding models;word alignment language;contextualized embeddings;use softmax sparse;softmax sparse variant;propose use softmax;pre trained embedding;trained embedding;models alignment extraction;contextualized embeddings derived;embedding models;effectively extract alignments;use softmax;word alignment;alignment language;language models;forward backward alignments;trained lms \ufb01netuning;alignment language supported;embeddings"}, "2b4edb9515a26561ea3f9ee2a63a506721c8369e": {"ta_keywords": "aspect based sentiment;aspect sentiments reviews;dataset aspect prediction;aspect prediction;aspect prediction used;aspect sentiments;sentiments reviews make;sentiment analysis scientific;sentiments reviews;use aspect sentiments;aspects sentiments;obtain aspects sentiments;review use aspect;sentiment analysis;aspects sentiments entire;reviews comments paper;analysis scientific reviews;sentiments entire dataset;based sentiment;reviews comments;based sentiment analysis;reviews able extract;peer reviews comments;reviews make intriguing;aspect based;scientific reviews able;sentiment;reviews make;extent disagreement reviewers;dataset aspect", "pdf_keywords": "sentiments peer review;dataset peer reviews;peer review text;peer review texts;peer reviews;quality peer review;peer review;aspect sentiments peer;peer reviews available;latent aspect sentiments;sentiment prediction;recommendation sentiment;sentiments peer;decision peer review;sentiment prediction sentence;sentiments aspects;sentiments aspects like;review text;recommendation sentiment prediction;review texts;peer review process;ideas recommendation sentiment;aspect sentiments;negative sentiments aspects;sentiment;machine learning paper;evaluation quality peer;review text papers;analysis latent aspect;inter reviewer disagreement"}, "f6d6c4dd0115386c234a0b027dd38f7aa9d9df2f": {"ta_keywords": "state art nlp;current nlp approaches;nlp tasks present;nlp tasks;specific nlp tasks;current nlp;limitations current nlp;specific nlp;map specific nlp;communities documentary linguists;nlp approaches;documentary linguists;endangered languages;motivate nlp;language communities documentary;endangered languages 66th;nlp practitioners;nlp applied particularly;transcriptions limited annotations;nlp;nlp approaches tutorial;art nlp;documentary linguists map;nlp applied;nlp practitioners work;needs language communities;endangered languages revitalization;language communities;art nlp applied;appearance endangered languages", "pdf_keywords": ""}, "31c53acd2a43dcec4342d9c42d0ffbfbef36e855": {"ta_keywords": "miscalibration estimation;estimating label;approaches estimating label;image recognition;calibration;miscalibration estimation error;estimating label marginal;calibration method iii;label distribution;classifier confusion matrix;approach image recognition;calibration method;ray spectrometer bb84;particular choice calibration;recognition;spectrometer bb84;probabilities label distribution;label shift class;xmath0 ray spectrometer;choice calibration;impacts miscalibration estimation;label;label distribution label;label shift;fermilab tevatron label;ray spectrometer;label label;spectrometer;shift label;label shift label", "pdf_keywords": "learning label shifts;domain label distributions;domain adaptation;unsupervised domain adaptation;estimating label shift;machine learning label;label distributions;shifts machine learning;learning label;approach estimating label;estimating label;domain adaptation possess;machine learning miscalibration;labeled source data;label shift arises;label distributions paper;machine learning;target domain label;consider machine learning;labeled source;adaptation possess labeled;unlabeled target data;estimating target domain;learning miscalibration;label shifts;label shifts machine;generalized distribution matching;labels cause features;machine learning calibrated;labels cause"}, "ccfaccf36b9cd7c0c05af2285ec90ecf5f51a34c": {"ta_keywords": "optimal relay locations;optimal relay;optimal relay location;multi relay channel;relay location optimal;formulas optimal relay;provides optimal relay;relay channel exponential;allocation source relay;optimal placement relay;relay channel;source relay channel;placement relay nodes;relay nodes;relay nodes uniformly;multi relay;multiple relay;relay channel study;relay node multi;optimal power allocation;source relay;attenuation relays clustered;formulas multi relay;constraint single relay;relay locations;single relay;placement relay;relays clustered;multiple relay case;relay node", "pdf_keywords": ""}, "d8d12c922fc571d081bae27c67fcf50cdbb17d90": {"ta_keywords": "lingual transfer learning;historical text summarisation;text summarisation dataset;summarisation model trained;summarisation documents historical;text summarisation documents;cross lingual benchmarks;language summarised corresponding;summarisation dataset;summarisation documents;standard text summarisation;text summarisation;cross lingual historical;transfer learning techniques;summarisation dataset consists;lingual benchmarks task;news text summarised;cross lingual transfer;text summarised;language summarised;based cross lingual;techniques propose summarisation;transfer learning;lingual benchmarks;summarisation model;transfer learning approach;digital humanities references;lingual transfer;cross lingual;lingual historical", "pdf_keywords": "topic crosslingual summarisation;crosslingual summarisation long;crosslingual summarisation;lingual transfer learning;summarisation model trained;model text summarisation;summarisation based linguistic;text summarisation tackled;new topic crosslingual;summarisation model;word embeddings raw;summarisation long standing;text summarisation;text summarisation based;propose transfer learning;transfer learning techniques;training word embeddings;topic crosslingual;cross lingual transfer;summarisation tackled best;transfer learning;propose summarisation model;transfer learning based;techniques propose summarisation;topic summaries generated;summaries generated;embeddings raw text;cross lingual supervision;task text summarisation;summarisation long"}, "c4607387ee863d5c5e5dc9f8adfbe7930508e286": {"ta_keywords": "machine learning society;conference machine learning;machine learning icl;international conference machine;conference international machine;machine learning icml;international machine learning;learning icl 2008;conference machine;results international conference;machine learning;icml annual conference;learning icl;best paper icml;conference paper present;icl 2008;international conference;international machine;conference paper;conference presented;annual conference international;main conference presented;learning icml;best paper;best student paper;ukraine international conference;conference presented alex;learning society imls;conference included tutorials;conference international", "pdf_keywords": ""}, "7f4fa7c6f16f2965a104fa45071ea0c92b4366fe": {"ta_keywords": "motion rigid body;rigid body action;motion rigid;model motion rigid;mechanical model motion;action external force;rigid body;external force;model motion;simple mechanical model;motion;mechanical model;body action external;simple mechanical;present simple mechanical;rigid;force;action external;mechanical;body action;external;action;body;model;present simple;present;simple", "pdf_keywords": ""}, "7634b0cf93169d2a95d4d7193f47f97a61e3b4b2": {"ta_keywords": "behavior diagnostic games;games elicit distinguishable;diagnostic games elicit;games successfully distinguish;player traits markov;diagnostic games;parameterize games;games validate approach;games elicit;interacting machine learning;experiments games;constructed experiments games;based heuristics;elicit distinguishable behavior;based heuristics present;successfully distinguish players;games validate;processes parameterize games;traits markov decision;parameterize games validate;behavior mutual information;varying player traits;heuristics;designed based heuristics;heuristics present;approaches infer traits;empirically showing designed;designing behavior diagnostic;player traits;games", "pdf_keywords": "games elicit distinguishable;games based heuristics;behavior diagnostic games;behavior diagnostic game;games successfully distinguish;diagnostic games elicit;diagnostic game design;learning games;diagnostic games;effectiveness designed games;learning games based;approach learning games;games elicit;designed games;games showing robust;diagnostic game;elicit distinguishable behavior;games;game design;behavior mutual information;showing designed games;designed games successfully;based heuristics formulate;successfully distinguish players;games based;heuristics formulate task;designed games showing;distinguishable behavior mutual;based heuristics;heuristics formulate"}, "1ccf412212873ae1b020762b8b86291e1fb11f65": {"ta_keywords": "crowdsourced audio transcriptions;dataset crowdsourced audio;crowdsourced audio;crowdsourced audio video;method crowdsourced audio;paper crowdsourced audio;audio transcriptions aggregated;dataset crowdsourced;data collection crowdsourcing;audio transcriptions;scale dataset crowdsourced;voxdiy counterpart crowdspeech;crowdsourced;collection crowdsourcing;crowdsourcing standard tools;aggregation method crowdsourced;collection crowdsourcing crowdsourcing;crowdsourcing;crowdsourcing standard;method crowdsourced;crowdsourcing crowdsourcing;paper crowdsourced;transcriptions aggregated;crowdsourcing crowdsourcing standard;crowdspeech russian language;recordings collect;audio video recordings;counterpart crowdspeech russian;video recordings collect;crowdspeech russian", "pdf_keywords": "crowdsourced audio transcriptions;datasets crowdsourced audio;crowdsourced audio recordings;crowdsourcing crowdsourced audio;crowdsourced audio annotations;crowdsourced audio;audio recordings crowdsourcing;recordings crowdsourcing;recordings crowdsourcing industry;method crowdsourced audio;data crowdsourcing;reliable data crowdsourcing;aggregating datasets crowdsourced;datasets crowdsourced;recognition systems crowdsourcing;data crowdsourcing platforms;crowdsourcing industry;audio annotations;experts crowdsourcing;training data speech;number experts crowdsourcing;aggregation method crowdsourced;crowdsourcing platforms;crowdsourcing industry standard;crowdsourcing platforms obtain;crowdsourcing;crowdsourcing crowdsourced;systems crowdsourcing;crowdsourcing platforms crowdsourcing;crowdsourced"}, "3ba529f732d3c4a31e9ce57f1c78ddf911846bf4": {"ta_keywords": "bottleneck labeling training;labeling training;network network supervision;network supervision;bottleneck labeling;learning technique labeling;labeling training data;reducing bottleneck labeling;technique labeling training;noisy supervision sources;supervised;supervised machine;learning synthesizing labels;walking ww benchmark;potentially noisy supervision;network supervision ss;recent weak supervision;nodes;training data supervised;noisy supervision;supervised machine learning;labeling;data supervised;benchmark evaluation wide;nodes network;new benchmark;nodes network likely;supervision sources;technique labeling;benchmark evaluation", "pdf_keywords": "supervision sources labeling;weak supervision sources;sources weak supervision;weak supervision source;weak supervision models;annotation sources weak;abstracts weak supervision;generation weak supervision;weak supervision create;label models trained;leverage weak supervision;weak supervision;leverages weak supervision;sources labeling;weak supervision paper;label models;labeling;noisy labels leverage;sources labeling functions;supervision sources;annotation sources;relationship weak supervision;supervision source;supervision models;multiple annotation sources;learning single source;hard labels;annotation;soft hard labels;labels leverage"}, "64bc7fe1c46c4d4106afba4621ff1bd4376c077a": {"ta_keywords": "electrolaryngeal speech prediction;electrolaryngeal speech;approach electrolaryngeal speech;speed sound airfoil;sound airfoil length;sound airfoil;speech prediction;speech prediction presented;length airfoils evaluation;airfoils evaluation;hybrid approach electrolaryngeal;airfoils evaluation hybrid;airfoil length;airfoil length airfoils;approach electrolaryngeal;length airfoils;speed sound;airfoil;electrolaryngeal;airfoils;relationship speed sound;speech;sound;prediction presented;speed;prediction;evaluation hybrid;length;evaluation;evaluation hybrid approach", "pdf_keywords": ""}, "04db62a14f78f693d6bd14a4803b9b73325b36bb": {"ta_keywords": "detecting fake news;fake news detection;fake news events;classify subgraph news;knowledge based fake;detecting fake;subgraph news item;fake news online;subgraph news;news detection external;news detection;knowledge graph;based fake news;news online social;number fake news;method detecting fake;external knowledge graph;detection external knowledge;detecting false;fake news;false dismissal detection;detecting false positives;knowledge graph required;news events reported;false positives real;graph neural network;news events;news item;graph neural;thousands labelled news", "pdf_keywords": "detecting fake news;fake news detection;information detecting fake;techniques fake news;knowledge graph news;news detection information;modal techniques fake;news item propagates;model fake news;news detection;news detection experimental;textual content news;modal detection algorithm;modal detection;relations extracted news;multi modal detection;detecting fake;graph news item;information extracted news;knowledge textual content;graph news;content news item;news item represented;news items;content social context;textual content social;extracted news items;knowledge graph;content news;news item"}, "9bd6cdae71506eb307507e44df7abe0c285b3ca7": {"ta_keywords": "translation algorithm learns;algorithm learns translate;learns translate;learns translate unknown;translation algorithm;statistical machine translation;machine translation algorithm;machine translation;2d ising model;dimensional 2d ising;2d ising;neural models;model triangular lattice;ising model triangular;ising model;algorithm learns;arbitrary language;neural;given language;translate unknown language;contributions neural models;language arbitrary;improvements lexical;given language arbitrary;language arbitrary language;neural models lie;lattice;presents statistical machine;contributions neural;ising", "pdf_keywords": "neural machine translation;machine translation reranking;machine translation;machine translation learning;evaluation machine translation;machine translation output;translation learning accurate;machine translation systems;translation reranking best;translation systems trained;translation reranking;translation learning;machine translation important;translation systems;sentence neural machine;tasks machine translation;neural mt reranking;paper machine translation;translation output;phrases sentence neural;translation important tasks;interpretable tree annotation;sentence neural;neural mt models;translation errors;workshop asian translation;translation output particular;using neural mt;neural mt objective;translation 2015 paper"}, "e0a0b3438aef008fece5b8bbf76105b470f10f25": {"ta_keywords": "convertible codes form;convertible codes;codes form penrose;form penrose diagrams;penrose diagrams;codes form;convertible;form penrose;codes;diagrams;penrose;form", "pdf_keywords": ""}, "1817c9f0fd8a17e31c65963dd8cee9783059495b": {"ta_keywords": "entangle pair men;entangle pair;entangle;women bondage bondage;men women bondage;showing entangle pair;women bondage;bondage;video showing entangle;bondage state;bondage bondage state;showing entangle;bondage bondage;bondage bondage bondage;pair men women;pair men;pair;note present video;men women;men;present video;video;women;present video showing;video showing;note present;short note;note;short note present;state", "pdf_keywords": ""}, "167adafac25ee108ca99c688cceded8bca710bb1": {"ta_keywords": "constancy increases age;size constancy increases;developmental study size;size constancy;constancy function age;study size constancy;constancy increases;length variations experimental;size constancy pattern;statistical tests constancy;developmental study;variable outcome developmental;tests constancy;rod length variations;outcome developmental;outcome developmental study;developmental;impossible size constancy;constancy;length rod length;length variations;constancy pattern;stimuli results;age relationship statistical;tests constancy function;constancy pattern controlled;ratio length rod;rod length;length rod;increases age", "pdf_keywords": ""}, "538466f2a69271617bf4f5b0df4e5fd854c11c35": {"ta_keywords": "sparse graph codes;group testing sparse;codes group testing;graph codes group;testing sparse graph;testing based sparse;testing sparse;coding theory;codes group items;low complexity decoder;modern coding theory;codes group;graph codes;group testing;constant group testing;complexity decoder;low complexity;items group testing;group testing based;group testing problem;having low complexity;sparse graph;based sparse graph;testing group performance;methods modern coding;group testing group;testing group;group subsets items;modern coding;groups items", "pdf_keywords": "group testing sparse;probabilistic group testing;group testing code;adaptive group testing;testing sparse graph;group testing feature;group testing;saffron group testing;testing sparse;sparse graph coding;coding theory group;coding low complexity;theory group testing;graph coding low;tools sparse graph;group testing model;testing code design;graph coding theory;graph coding;items pooling groups;coding theory;sparse graph;pooling groups items;testing code;groups items;groups items ef\ufb01ciently;pooling groups;low complexity decoding;tools sparse;coding"}, "7e358ffc2731a82420d84a7f0bedb155a487c39d": {"ta_keywords": "hop question answering;question answering datasets;question composition;challenging dataset musique;musique consisting answerable;question composition single;multihop question composition;question answering;hop questions experiments;single hop questions;challenging multi hop;contrasting unanswerable questions;single hop qe;hop questions;answering datasets propose;unanswerable questions;hop qe;build challenging dataset;hop qe single;composition single hop;cheatable single hop;question pairs model;answering datasets;dataset connected reasoning;consisting answerable unanswerable;dataset musique consisting;minimal traintest;dataset musique;hops composition;hop", "pdf_keywords": "multihop question answering;comprehension qa datasets;answering natural language;multihop reading comprehension;question answering;question answering natural;multihop reasoning open;multihop qa datasets;compose multihop questions;natural language solution;building multihop datasets;framework multihop reasoning;multihop reasoning;challenging multihop reading;qa datasets carefully;multihop questions large;reading comprehension qa;answer multihop question;answer multihop;reasoning open domain;multihop qa;qa datasets;bypassing reasoning;multihop datasets;bypassing reasoning steps;challenging multihop;reasoning open;multihop questions;answering;dataset construction pipeline"}, "61cce75554a6d1bb802f26758c3b0ba97de6918d": {"ta_keywords": "positional embeddings graph;graph neural;graph neural networks;embeddings graph;graph locality;networks graph neural;leveraging graph locality;information graph locality;homophilic networks graph;graph locality explore;graph context;embeddings graph able;graph locality experimental;prediction context graph;learning positional embeddings;networks graph;nonhomophilic graphs work;nonhomophilic graphs;context graph;node classification tasks;homophilic nonhomophilic graphs;node classification;positional embeddings;leveraging graph;structural information graph;graph context paper;context graph context;case nonhomophilic graphs;homophilic networks;graphs", "pdf_keywords": "graph attention;graph attentional;graph attention networks;graph attentional networks;enable graph attention;termed graph attentional;networks positional embeddings;attentional networks positional;attention computation homophilic;learn positional embeddings;attention networks gat;attention networks;attention computation;node content attention;content attention computation;attentional networks;graph locality inspired;enhance positional embeddings;information graph locality;graph locality;networks positional;leveraging graph structure;positional embeddings embedding;positional embeddings;gnns explore structural;positional embedding work;homophilic graphs ubiquitous;graphs ubiquitous;leveraging graph;content attention"}, "b81acc013c42796a5eea0fc20cfb04846da3a589": {"ta_keywords": "nlp tools new;nlp tools;nlp technology;adapt nlp tools;processing nlp technology;multilingual neural networks;processing nlp;language processing nlp;massively multilingual neural;advances massively multilingual;nlp technology article;transcription glossing corpus;phoneme transcription glossing;natural language processing;accelerate human language;linguists upload;linguists upload data;multilingual neural;adapt nlp;language processing accelerate;glossing corpus;documentary linguists;glossing corpus management;documentary linguists work;corpus management;ease language documentation;transcription glossing;portion documentary linguists;natural language;language processing", "pdf_keywords": "machine translation multimodal;translation multimodal alignment;linguistic annotation backend;translation multimodal;multilingual neural networks;combines machine translation;annotated linguistic data;machine translation combines;annotated linguistic;create linguistic annotation;linguistic annotation;partially annotated linguistic;annotation candidates linguist;machine translation;recordings automatic language;multilingual neural;automatic phoneme transcription;multimodal alignment goal;multimodal alignment multi;automatic language;phoneme transcription glossing;approach linguistic annotation;multimodal alignment;linguistic annotation based;massively multilingual neural;transcription glossing corpus;automatic language processing;use multimodal alignment;translation combines machine;approach machine translation"}, "398a0625e8707a0b41ac58eaec51e8feb87dd7cb": {"ta_keywords": "ensemble agents;ensemble agents based;behavior ensemble agents;agents learn abstract;enables agents learn;agents learn;simulator enables agents;agent communicate;agents based concept;agents based;ability agent communicate;human agent generalize;entanglement ability agent;behavior ensemble;agent generalize beliefs;group agents;communicate group agents;enables agents;visual environment;human agent;agent communicate group;agent generalize;agent;visual environment propose;agents;ability agent;rich visual environment;text based;ability human agent;reasoning behavior ensemble", "pdf_keywords": "embodied agent learns;recurrent component embodied;text based actions;level action embodied;abstraction deep reinforcement;action embodied context;embodied agent;action embodied;embodied robotic simulation;embodied robotic;exploration interactive language;action generation;physically embodied robotic;language abstraction deep;agent exploration interactive;deep reinforcement;agent learns meaning;physical action generation;reinforcement learning butler;agent learns;modal learning language;deep reinforcement learning;text based agent;learning butler embodied;abstraction deep;component embodied;visually grounded actions;agent exploration;embodied context;action generation low"}, "4f7bbcef3d40cafad17936fdf562a121667af1e8": {"ta_keywords": "vessel tree reconstruction;prior vessel tree;divergence prior vessel;flow orientations tree;vessel trees nearcapillary;complete vessel trees;vessel tree;vessel trees;reconstruction combines divergence;blood flow pattern;flow orientations;arteries converge veins;model blood flow;ambiguity flow orientations;tree centerline reconstruction;divergence constraints robust;reconstruct complete vessel;pattern convergent arteries;vessel tree vv;quality vessel tree;divergence constraints;converge veins;combines divergence constraints;prior vessel;convergent arteries;tree reconstruction;arteries converge;centerline reconstruction;robust curvature regularization;blood flow", "pdf_keywords": "vessel segmentation;vessel segmentation algorithms;blood vessel segmentation;vessel tree reconstruction;vessel tree estimation;vessel tree structure;based divergent vessel;vesselness \ufb01lters curvature;shape vessel;heart curvature regularization;approach vessel tree;divergent vessel prior;divergent vessel;unsupervised approach vessel;curvature based regularization;cues vessel tree;vessel orientation;vessel tree;limitatation vessel tree;ambiguity vessel orientation;curvature regularization method;curvature regularization;local vesselness;vessel orientation produced;new curvature regularization;local vesselness \ufb01lters;vessel prior;blood vessel;knowledge shape vessel;vesselness"}, "0431f60546381a9e91fb156236c3c7056f57081f": {"ta_keywords": "singing voice synthesis;voice synthesis singsvs;generate singing better;generate singing;learning based singing;voice synthesis;singing databases;singing databases demonstrate;flexibly generate singing;singing better qualities;singing quality;singing voice;singing quality limited;reasonable singing quality;singing better;synthesis singsvs;singing;based singing voice;based singing;public singing databases;augmentation methods boost;training strategy deep;reasonable singing;experiments public singing;synthesis singsvs demonstrated;reach reasonable singing;public singing;data augmentation methods;singsvs demonstrated;methods stabilizing training", "pdf_keywords": "voice synthesis svs;singing databases demonstrate;singing databases;voice synthesis;predictor module music;singular voice synthesis;public singing databases;module music lyrics;experiments public singing;lyrics information;data augmentation methods;module music;data augmentation;improve sound;propose data augmentation;music score naturalness;lyrics information paper;music lyrics information;augmentation methods singular;carbon nanotubes;voice;nanotubes propose cycle;augmentation methods stabilizing;sound quality music;music lyrics;nanotubes;walled carbon nanotubes;carbon nanotubes propose;study acoustic properties;singing"}, "dbe87b171bfb789e1d22a047aeeee69105e6fd02": {"ta_keywords": "t5 sentence embeddings;sentence embeddings encoder;sentence embeddings broadly;sentence embeddings;produce sentence embeddings;2021 sentence embeddings;embeddings broadly useful;embeddings encoder decoder;embeddings encoder;embeddings broadly;tasks encoder models;performance sentence bert;embeddings;downstream tasks encoder;tasks encoder;sentence bert;sentence bert 2019;language tasks;encoder decoder models;transfer tasks semantic;encoder models transfer;encoder models;language tasks cast;bert 2019 simcse;encoder models outperform;extracting t5 sentence;tasks semantic textual;state art encoder;encoder;stage contrastive learning", "pdf_keywords": "similarity sentence transfer;embeddings sentence transfer;embedding based paraphrastic;paraphrastic sentence embeddings;textual similarity tasks;better textual similarity;semantic textual similarity;sentence transfer tasks;sentence embeddings;sentence embeddings successfully;sentence encoder model;sentence embedding;sentence encoder;textual similarity;sentence embedding based;sentence transfer transformers;encoder model sentences;textual similarity agirre;resulting sentence embeddings;contrast sentence embeddings;sentence transfer;quality text representations;sentence embeddings contrast;similarity tasks;sentence embeddings allow;question answering data;approach sentence embedding;textual similarity sentence;text representations;sentence embeddings sentence"}, "95788ed8affd06c0c2c6159c26ff7c123c4f2e0a": {"ta_keywords": "rate speaker diarization;speaker diarization;estimating speech activity;end speaker diarizations;speaker diarizations;estimating speech;speaker diarization essential;method estimating speech;estimated activities speakers;speaker diarizations terms;speech activity;speaker wise conditional;speech activity based;speaker wise;activities speakers;neural diarization eend;error rate speaker;end neural diarization;end speaker;neural diarization;speaker audio processing;rate speaker;diarization eend;speaker audio;end end speaker;speakers outperforms state;novel speaker wise;variable number speakers;measure number speakers;speaker", "pdf_keywords": "speaker diarization;learning speaker diarization;method speaker diarization;diarization method speaker;speaker diarization essential;speaker diarization important;machine learning speaker;speaker wise conditional;learning speaker;neural diarization method;speaker wise;neural diarization;end neural diarization;method speaker;variable number speakers;speaker;multispeaker;diarization important task;diarization;produce diarization;produce diarization results;step multispeaker;diarization method;multispeaker audio;number speakers;essential step multispeaker;diarization results;diarization important;correctly produce diarization;step multispeaker audio"}, "84908a28a03d0d7c467d9556ed36f0e416de7171": {"ta_keywords": "recognition written utterances;semantic parsing;semantic parsing goal;algorithm semantic parsing;parsing;utterance algorithm;meaning recognition written;utterance algorithm based;given utterance algorithm;utterances;written utterances;expert machine learning;expert machine;meaning recognition;description given utterance;recognition written;novel algorithm semantic;human brain events;context free grammars;given utterance;utterance;semantic;grammars;algorithm semantic;parsing goal;parsing goal paper;grammars paper;free grammars;brain events paper;problem meaning recognition", "pdf_keywords": ""}, "b46be3ac246499655cc442e93c5878e7a9640ae3": {"ta_keywords": "weblog corpus;techniques weblog corpus;weblog corpus supplementary;timelines using tree;timeline construction;timeline construction compare;timelines using;unsupervised methods timeline;news corpus;pedestrian motion crowded;supplementary news corpus;timelines;news corpus presented;motion crowded environments;weblog;techniques weblog;introduce pedestrian movement;produced timelines using;dynamics pedestrian crowded;pedestrian crowded environment;pedestrian movement based;produced timelines;pedestrian motion;motion crowded;methods timeline construction;previous events better;pedestrian movement;information previous events;corpus supplementary news;based pedestrian motion", "pdf_keywords": ""}, "8da992b611df508b1803f66ffa53bd1fb741a76c": {"ta_keywords": "controlledquestion answer hierarchies;classify questions;question answer game;reading comprehension datasets;classify questions existing;question taxonomy;question taxonomy loosely;existing reading comprehension;comprehension datasets;challenging text generation;text generation task;using question taxonomy;questions existing reading;1978 classify questions;answer hierarchies novel;answer hierarchies;comprehension datasets general;reading comprehension;generated qaisings crowdsourced;text generation;answer question present;challenging text;question answer pairs;asked answer question;asked question answer;generation task;qaisings crowdsourced;answer game;questions;answer question", "pdf_keywords": "answer question prediction;answer generation;answer language modelling;experiments crowdsourced faqs;crowdsourced faqs able;question prediction trained;architecture answer generation;crowdsourced faqs;answer generation answer;generated hierarchies crowdsourced;answer language;hierarchies crowdsourced;question prediction;crowdsourced approach answer;document generation pedagogical;text generation task;paragraph answer language;hierarchies crowdsourced experiments;faqs able generate;reading comprehension datasets;crowdsourced approach;present crowdsourced;generation answer question;document generation;experiments crowdsourced;text generation;crowdsourced;paper present crowdsourced;candidates answer span;present crowdsourced approach"}, "2eea63f896deed47cc0c0000e1482ec5c860fd0b": {"ta_keywords": "controversy detection;dataset controversy detection;controversy detection paper;controversial posts web;comment sentiment graph;sentiment graph;sentiment graph tpcs;graph sentiment;sentiment information;searching controversial posts;graph sentiment analysis;tpcs graph sentiment;sentiment information integrate;controversial posts;relationship sentiment information;sentiment analysis;post comment sentiment;comment sentiment;topics comments information;news alleviating polarized;searching controversial;sentiment analysis previous;structure relationship sentiment;methods searching controversial;multilingual dataset controversy;effectively topics comments;measuring influence news;sentiment;comments information;topics comments", "pdf_keywords": ""}, "881ce19455a9923e4798e9d77d2d8623ca9d2e03": {"ta_keywords": "sparseness speech recognition;sparseness speech;experimentally sparseness speech;distribution bayesian predictive;bayesian predictive classification;predictive classification speech;bayesian predictive distribution;predictive distribution bayesian;classification speech;recognition experimentally sparseness;paper propose bayesian;noisy environment speech;speech recognition experimentally;bayesian predictive;propose total bayesian;based bayesian predictive;robust sparse data;distribution bayesian;classification speech recognition;speech recognition;speech recognition problem;sparse data;propose bayesian model;propose bayesian;robust sparse;sparse data propose;bayesian model predicting;total bayesian framework;total bayesian;environment speech recognition", "pdf_keywords": ""}, "6d9603be7e79ff33677327a0edd5bd3f7da6347b": {"ta_keywords": "harmonic oscillator potential;dimensional harmonic oscillator;oscillator potential;harmonic oscillator;particle dimensional harmonic;dynamics single particle;dimensional harmonic;single particle;harmonic;single particle dimensional;environment dynamics single;oscillator;particle;dynamics single;effect environment dynamics;dynamics;particle dimensional;environment dynamics;potential;effect environment;environment;study effect environment;effect;single;dimensional;study effect;study;paper study effect;paper study;paper", "pdf_keywords": ""}, "81af4e14050c410e2afee226be583088a9791ddf": {"ta_keywords": "embeddings unsupervised semantic;unsupervised semantic role;role unsupervised semantic;semantic role induction;argument embeddings based;argument embeddings;semantic role labeling;bias argument embeddings;unsupervised semantic;embeddings based dependency;embeddings unsupervised;role labeling;role labeling identifying;embeddings based;labeling identifying role;semantic role;embeddings;unsupervised learning;simlex999 word similarity;state art embeddings;identifying role argument;case single learning;role argument usually;unsupervised learning based;word similarity task;problem unsupervised learning;dependency relation predicate;dataset simlex999 word;art embeddings unsupervised;role induction", "pdf_keywords": ""}, "ec99cf93ef22a0c0d669abe90c9509f642b2cf69": {"ta_keywords": "content adaptive downsampling;downsampling technique learns;adaptive sampling;sampling gives segmentation;sampling;sampling locations;adaptive downsampling;sampling set;accuracy semantic boundaries;semantic boundaries target;good semantic segmentation;favor sampling locations;semantic segmentation critical;adaptive sampling gives;adaptive downsampling technique;near semantic boundaries;downsampling;sampling locations near;sampling uniform sampling;uniform sampling;downsampling technique;semantic segmentation;uniform sampling set;favor sampling;sampling set method;sampling uniform;sampling gives;semantic boundaries;segmentation better;xmath0 adaptive sampling", "pdf_keywords": "segmentation deep;image segmentation deep;segmentation deep convolutional;segmentation datasets semantic;semantic segmentation datasets;public semantic segmentation;semantic segmentation;semantic segmentation emerging;datasets semantic segmentation;boundaries predict sampling;navigational applications sampling;downsampling technique learns;semantic boundaries predict;content adaptive downsampling;deep convolutional nets;semantic boundaries target;aware adaptive downsampled;convolutional nets crfs;convolutional nets;predict sampling locations;near semantic boundaries;driven semantic boundaries;locations uniform downsampling;deep convolutional;segmentation datasets;favor sampling locations;sampling locations;semantic boundaries;sampling points;predict sampling"}, "48e8e8085907192d501eb2bcc582035e90431a2f": {"ta_keywords": "sequence tagging model;sequence tagging;deep hierarchical recurrent;deep gated recurrent;recurrent neural network;hierarchical recurrent neural;hierarchical recurrent;sequence words model;task cross lingual;recurrent neural;tagging chunking ner;network sequence tagging;encode morphology context;tagging chunking;languages benchmark tasks;predict tags;encode morphology;layer predict tags;pos tagging chunking;cross lingual;deep hierarchical;task independent language;word levels encode;gated recurrent;tagging model;lingual;sequence words;words model;tagging model achieves;present deep hierarchical", "pdf_keywords": "sequence tagging tasks;sequence tagging;sequence tagging work;entity recognition wikipedia;multilingual named entity;different sequence tagging;named entity recognition;learning multilingual named;tagging tasks language;entity recognition;architecture sequence tagging;tagging chunking ner;learning multilingual;tagging chunking;tagging tasks;languages learning multilingual;pos tagging chunking;multilingual named;recurrent neural network;deep recurrent neural;tasks multiple languages;multilingual;deep recurrent;chunking ner english;recognition wikipedia challenging;tagging;tagging work;tasks language share;present deep recurrent;language share language"}, "5431098723db5858c4553f0259921cbbdd6492d5": {"ta_keywords": "covid 19 languages;localizers translation initiative;translation initiative covid;translation based localizers;translation initiative;based localizers translation;19 languages;19 languages report;localizers translation;translation memories;translation based;languages;languages africa;translation memories used;large set languages;speed translation based;languages large set;localizers;languages paper translation;languages large;languages africa south;languages report;based localizers;paper translation memories;lesser resourced languages;development languages large;speed translation;particular languages africa;set languages;information covid 19", "pdf_keywords": "translation quality assurance;translation translation initiative;translation initiative covid;current machine translation;translation quality;translation initiative;results translation quality;evaluate machine translation;translation systems;translation systems enabling;machine translation translation;machine translation;machine translation systems;professional translators;translators readily available;languages professional translators;professional translators readily;multi lingual benchmark;translators;translation translation;results translation;ensure content translated;translators readily;translation;lingual benchmark;content translated;present results translation;translated;lingual benchmark set;source multi lingual"}, "a72e732f2d11075aa0103b72b4f9884ddcaaaa85": {"ta_keywords": "data polynomial learnability;polynomial learnability;polynomial learnability powerful;learnability results obtained;inductive logic programming;learnability results;positive learnability results;learnability;positive learnability;learnability powerful;learnability powerful tool;negative positive learnability;noisy data polynomial;practice inductive logic;inductive logic;classes logic programs;learning noisy;logic programs;approach learning noisy;logic programs closely;logic programming;learning noisy data;logic programming paper;data polynomial;classes logic;learning;practice inductive;new approach learning;inductive;restricted classes logic", "pdf_keywords": ""}, "015dc5b71894dd4d05e7668d015e545ab2e162ba": {"ta_keywords": "speech recognition toolkit;new automatic speech;speech recognition;automatic speech;tweezer spring spring;tweezer spring;automatic speech recognition;spring toolkit;recognition speech;recognition speech recognition;spring toolkit provides;speech recognition speech;external spring toolkit;open ended tweezer;ended tweezer spring;tweezer;recognition toolkit;spring spring interaction;recognition toolkit supports;pre trained models;spring interaction;spring spring model;spring model;machine learning models;spring interaction represented;fields hgg generation;machine learning;trained models;state art machine;interaction represented spring", "pdf_keywords": "speech processing toolkit;automatic speech;learning automatic speech;text speech e2e;text speech;automatic speech recognition;source speech processing;speech recognition;speech e2e tts;open source speech;end text speech;tts toolkit;e2e tts toolkit;speech processing;speech recognition based;toolkit espnet;speech e2e;new machine learning;toolkit named espnet;processing toolkit espnet;tts toolkit named;espnet tts extension;machine learning automatic;toolkit;toolkits;source speech;evaluation comparison toolkits;learning automatic;espnet tts;toolkit experimental evaluation"}, "3122a2d7799ba585b993e432b3deb47659b3f3c1": {"ta_keywords": "generated answer quality;question answering;text generation tasks;question answering lfq;answer quality easily;answer quality;form question answering;generated answers;text generation;generated answers actually;paraphrased form training;trends generated answers;generation tasks;used text generation;answering;answer paper;generated answer;evaluations used text;length answer paper;generate paragraph;answers;human evaluations;gamed human evaluations;answer paper present;answering lfq;questions occur paraphrased;unreliable lfqa long;generate paragraph length;task formulation raises;using generate paragraph", "pdf_keywords": "answer generation trained;predict answer generation;model answer generation;answer generation quality;answer generation;question answering;question answering paper;domain question answering;language modeling generative;random retrieval;answering paper machine;use random retrieval;random retrieval state;end propose retrieval;answering paper;retrieval state art;propose retrieval;corpus retrievals trained;crawl news corpus;retrievals trained;retrieval retriever model;news corpus retrievals;answering;retrievals;propose retrieval retriever;retrieval;trained predict answer;generation trained;news corpus;language modeling"}, "15d643f4c27d373aa46f26a760051e76fde81dc2": {"ta_keywords": "answer entities train;text answer entities;answer entities;models question answering;question answering knowledge;answering knowledge graphs;answering knowledge;question answering;annotated entities;entities train;annotated entities work;hand annotated entities;person answer question;end entity resolution;entities train delivers;question text answer;end entity;entity resolution;entity;entities;entity resolution ability;learning er component;end end entity;entities work;extend learning er;weakly supervised dataset;weakly supervised;answering;knowledge graphs;using weakly supervised", "pdf_keywords": "answering knowledge graphs;question answering knowledge;answer answering knowledge;question answering;answering knowledge;poor entity extraction;handannotated entities approach;domain answer datasets;handannotated entities;hop question answering;answer datasets work;answer datasets;entity extraction;use handannotated entities;answer answering;end learning weakly;boundaries answer answering;entity extraction domain;knowledge graphs;questions poor entity;entities approach fully;learning weakly supervised;questions answers;entities;er questions poor;training data er;entity;knowledge graphs include;weakly supervised;knowledge graphs demonstrate"}, "5f9d8fe21efb3c2b241427869a333472ab09a22d": {"ta_keywords": "web learning;web based learning;algorithm web learning;learning algorithm web;based learning algorithm;web based;learning algorithm;based learning;propose web based;web;algorithm web;propose web;paper propose web;learning;algorithm;based;paper;paper propose;propose", "pdf_keywords": ""}, "ea6547e877c1cc3d37229a6f488ac04e9a11de18": {"ta_keywords": "study protein docking;protein complexes docking;water positions protein;protein docking capri;protein interfaces;molecular docking model;protein protein interfaces;protein interfaces performed;protein docking;models dnase domain;models dnase;protein protein interactions;protein interactions;molecular docking;computational study protein;predicted interactions capri;protein capri target;protein complexes;compared molecular docking;regulation protein complexes;regulatory complexes dnase;predictions water positions;protein interactions play;complexes dnase domain;dnase domain colicin;dnase domain bacteriophage;complexes dnase;195 models dnase;protein capri;predicting water positions", "pdf_keywords": ""}, "fd1b59d22eb1fb32e0360d4fcbe58dc4ebb25af9": {"ta_keywords": "research audio deepfakes;audio deepfakes survey;methods audio deepfakes;audio deepfakes;audio deepfakes english;synthesis text deepfakes;text deepfakes generation;audio deepfakes overlooked;video image deepfakes;fake news detection;deepfake content;image deepfakes survey;english deepfake content;deepfakes generation methods;focuses audio deepfakes;text deepfakes;image deepfakes;detection methods audio;focusing audio deepfakes;deepfakes generation;news detection;detection including fake;real include audio;research audio;need research audio;audio video;deepfakes survey evaluates;news detection controversial;audio video image;deepfake content material", "pdf_keywords": "audio deepfake generation;research audio deepfakes;audio deepfake;audio deepfakes created;deepfake generation;audio deepfakes;methods audio deepfakes;audio deepfake synthesize;detect audio deepfakes;framework audio deepfake;audio deepfakes survey;audio deepfakes work;deepfake synthesize;deepfake synthesize new;adversarial imitation;generative adversarial imitation;signal discriminators gans;automatic speech;discriminators gans;adversarial imitation learning;focusing audio deepfake;texts automatic speech;discriminators gans paper;deepfake generation paper;deepfakes created detect;generative adversarial;gans;recognition speech;deepfakes created;deepfake"}, "86b91922923b03c66497accfa88c638299fc8d26": {"ta_keywords": "variational encoder decoder;xmath1 transition language;variational encoder;space variational encoder;transition language;xmath0 xmath1 transition;decoder;encoder decoder;xmath1 transition;new analysis xmath0;encoder;fountain patterns based;transition language specific;analysis xmath0;analysis xmath0 xmath1;fountain patterns;xmath0;xmath0 xmath1;video water fountain;fountain used generate;water fountain patterns;dynamics video water;water fountain;encoder decoder msved;variational;multi space variational;decoder msved method;fluid dynamics video;decoder msved;xmath1", "pdf_keywords": ""}, "f35a01c1e5d5375453c39e6161526633492fb574": {"ta_keywords": "archival storage systems;codes storage frequently;data storage systems;codes storage;data storage;data storage methods;reliability archival storage;archival storage;storage systems;data replication powerful;codes mds queue;replication powerful erasure;storage methods;storage efficiency;storage efficiency known;maximum storage efficiency;queueing arising codes;storage;evolving data storage;storage systems understood;data replication;storage systems based;storage frequently;study data storage;storage methods simple;mds queue use;mds queue;improved reliability archival;storage cost bounds;simple data replication", "pdf_keywords": "mds queue;distributed data storage;distributed storage;queueing theory;data storage latency;distributed storage model;mds queue analyse;storage latency;storage latency performance;framework mds queue;data storage systems;queueing;storage systems;queueing theory paper;data storage;scheduling jobs data;coded data storage;storage systems based;data distributed data;distributed data;data distributed;buffer latency jobs;performance scheduling;latency performance scheduling;queue;scheduling policies;performance scheduling policy;storage model;queue analyse;jobs waiting buffer"}, "9b71542ef5d5178041048b9a330309053bb0bcfc": {"ta_keywords": "speech separation enhancement;novel speech separation;speech separation input;domain speech separation;synthesize separated speech;speech separation;performance speech separation;speech separation usually;separated speech;separation enhancement;separated speech high;separation input mixtures;separation enhancement model;speech mixture;separation input;speech quality interference;speech mixture using;target speech resynthesized;improved performance speech;separation;high speech quality;synthesize separated;domain speech;head collision bouncer;collision bouncer;separation usually build;target speech;speech resynthesized;separation usually;truth speech mixture", "pdf_keywords": "speech separation enhancement;speech separation;separated speech;synthesize separated speech;paradigm speech separation;model speech;extraction speech;model speech recognition;separated speech high;noisy mixed speech;enhanced speech;recognition enhanced speech;speech recognition propose;speech recognition enhanced;extraction speech like;end model speech;speech high quality;enhanced speech recognition;speech recognition;mixed speech speci\ufb01cally;framework extraction speech;speech like audios;speech speci\ufb01cally propose;mixed speech;separation enhancement;audios given complex;speech;speech speci\ufb01cally;convert paradigm speech;complex noisy mixed"}, "79ab3a0d6dc5d6fd3b466ea2814fdbb93a3672d0": {"ta_keywords": "team competition competition;strategy team competition;competition competition competition;competition competition;competition competition lead;team competition;competition;competition competition topic;competition lead winning;competition lead;competition topic;winning strategy team;competition topic debate;winning strategy;paper competition competition;lead winning strategy;strategy team;paper competition;strategy;winning;lead winning;team;debate;debate years;topic debate years;topic debate;lead;paper;topic;years", "pdf_keywords": ""}, "da10c4bc1de7b9b7ddbb21d70ff5092a15cb866f": {"ta_keywords": "crystalline scco xmath0;scco xmath0 alloy;xmath0 alloy;pseudolabel based rescaling;xmath0 alloy article;properties single crystalline;structure transport properties;single crystalline;electronic structure transport;crystalline;crystalline scco;novel maximum entropy;single crystalline scco;entropy based technique;entropy based;maximum entropy based;adaptation protein extraction;structure transport;domain adaptation protein;entropy;maximum entropy;protein extraction;transport properties single;pseudolabel based;scco xmath0;electronic structure;transport properties;protein extraction present;pseudolabel;domain adaptation", "pdf_keywords": ""}, "9db77e925015ad02efc9beeab233bedbfe04e4b7": {"ta_keywords": "introduced reinforcement learning;challenging reinforcement learning;reinforcement learning;reinforcement learning rv;reinforcement learning rl;popular policy gradient;policy gradient;evolutionary learning algorithm;objective reinforcement learning;strategy solving dynamical;policy gradient es;recently introduced reinforcement;called evolutionary learning;evolution strategy;evolution strategy es;evolutionary learning;strategy predicting outcome;objective reinforcement;strategy predicting;challenging reinforcement;approaches evolution strategy;new strategy predicting;optimal strategy solving;strategy solving;reinforcement learning problem;context reinforcement learning;reinforcement;learning algorithm es;algorithm called evolutionary;promise challenging reinforcement", "pdf_keywords": "reinforcement learning gradient;learning policy gradients;approximator deep reinforcement;learning deep reinforcement;deep reinforcement learning;gradient based reinforcement;policy gradients;policy gradients suited;lifelong reinforcement learning;promise reinforcement learning;reinforcement learning rl;deep reinforcement;reinforcement learning recent;reinforcement learning popular;learning gradient;reinforcement learning discuss;effective reinforcement learning;learning methods reinforcement;reinforcement learning es;exploration critical deep;years reinforcement learning;learning gradient estimation;reinforcement learning methods;reinforcement learning tasks;reinforcement learning;reinforcement learning evolving;background reinforcement learning;methods reinforcement learning;strategy machine learning;application lifelong reinforcement"}, "af0aa62d243c761b56a83369bc9b1f75805003cf": {"ta_keywords": "represent text corpus;text corpus;text corpus labeled;similarity measure words;walks derive similarity;corpus labeled directed;graph based similarity;similarity measure based;based similarity measure;words use supervised;coordinate term extraction;corpus;corpus labeled;similarity measure propose;similarity measure;represent words weighted;words weighted edges;dependency parsing fluid;nodes represent words;state art similarity;derived similarity measure;derive similarity measure;based similarity;term extraction;term extraction based;parsing fluid dynamics;labeled directed graph;improve derived similarity;derive similarity;derived dependency parsing", "pdf_keywords": ""}, "f765b23f0b0d2a196bc0fe562ad24278d0c9cee4": {"ta_keywords": "learning noisy;learning noisy noisy;training deep learning;training deep;problem learning noisy;training deep neural;deep learning;gradient methods stochastic;deep learning models;methods training deep;deep neural;adaptive gradient;method training deep;adaptive gradient methods;deep neural networks;function adaptive gradient;stochastic optimization;neural networks image;optimization adjust learning;learning models;methods stochastic optimization;stochastic optimization adjust;neural networks;adjust learning rate;objective function adaptive;noisy noisy data;problem learning;learning rate parameter;learning rate;adjust learning", "pdf_keywords": ""}, "ccbc17d42f2b260079eee702fd97a75de705d8ac": {"ta_keywords": "search words;generate threeword phrases;threeword phrases predetermined;phrases predetermined syntactic;approach search words;search words vocabulary;syntactic structures decoupling;predetermined syntactic structures;vocabulary given language;generate threeword;phrases predetermined;words vocabulary given;predetermined syntactic;setting generate threeword;words vocabulary;monolingual setting generate;threeword phrases;syntactic;given language;vocabulary given;syntactic structures;words;vocabulary;synthesis decomposition search;decomposition search;monolingual setting;given language paper;phrases;language paper monolingual;search", "pdf_keywords": ""}, "75b13e7131997ff6fd21325d68a2222d2c1b7157": {"ta_keywords": "speech enhancement separation;neural separation multiframe;reverberant speech enhancement;neural networks separation;speech enhancement;contextual frames beamforming;enhancement separation tasks;beamforming method improves;challenging reverberant speech;separation multiframe time;beamforming formulations;separation multiframe;beamforming;frames beamforming formulations;multi frame beamforming;reverberant speech;covariance matrices beamforming;invariant spatial beamforming;frame beamforming;frames beamforming;beamforming including;separation tasks demonstrate;frame beamforming method;beamforming method;beamforming including factorizing;matrices beamforming including;spatial beamforming;matrices beamforming;neural separation;sequence neural separation", "pdf_keywords": ""}, "00b874f8346cedadc2a6366c4b72e60140f99556": {"ta_keywords": "textual similarity task;semantic textual similarity;textual similarity;predict winner competition;winning entry attention;attention based input;similarity task semeval2016;entry attention based;task semeval2016 competition;similarity task;semantic textual;predict winner;attention based convolutional;winner competition based;winning entry;approach predict winner;network semantic textual;neural network semantic;winner competition;entry attention;semeval2016 competition;similarity;textual;attention based;competition develop attention;semeval2016 competition develop;semantic;competition;attention;convolutional neural", "pdf_keywords": ""}, "43c844c30765f3fa25bfabd83490ef826b9ceca1": {"ta_keywords": "spell checkers robust;classifier spell checkers;word recognition robust;robust word recognition;recognition robust word;spell checker method;spell checker;spell checkers;model word recognition;word recognition;robust word;checkers robust adversarial;shelf spell checker;novel classifier spell;classifier spell;word recognition presence;checkers robust;recognition robust;robust adversarial;rare unseen words;robust adversarial training;classifier;propose novel classifier;novel classifier;rnn semi character;checker;spell;analysis reveals robustness;adversarial;adversarial training shelf", "pdf_keywords": "adversarial spelling attacks;robust adversarial spelling;adversarial spelling;word recognition defense;word recognition spammers;spelling attacks;spelling attacks word;attacks word character;attacks using word;recognition spammers;subtly misspelling words;accurate word recognition;spell checkers;adversarial training;characters words spelling;spam detection preserving;misspelling words;adversarial;evade spam detection;outperform adversarial;outperform adversarial training;adversarial training shelf;recognition spammers continually;neural networks word;word recognition;spell checkers used;word recognizer;word recognition models;adversarial attacks;character recurrent neural"}, "25ae911c13da7ef9def56ee30170920ebd48a668": {"ta_keywords": "argument use crowdsourced;computational argumentation;computational argumentation investigate;web arguments convincingness;predicting argument argument;predicting argument;ranking arguments;crowdsourced;ranking arguments topic;crowdsourced data;convincing ranking arguments;use crowdsourced;field computational argumentation;social network bidirectional;argumentation investigate;argumentation;argumentation investigate qualitative;tasks predicting argument;crowdsourced data world;web arguments;use crowdsourced data;arguments topic based;arguments convincingness;pair convincing ranking;arguments convincingness argument;convincing ranking;arguments;topic based convincingness;crowding;argument pair convincing", "pdf_keywords": ""}, "b48f0652605f981b5d407496aba3d9756725264f": {"ta_keywords": "preference modeling framework;preference modeling;propose preference modeling;preference handling formalism;decision based preferences;preference handling;model conditional preferences;conditional preferences preferring;based preferences optimization;conditional preferences;preferences optimization criteria;formalism model preferences;preferences optimization;preferences preference handling;based preferences;model preferences preference;model preferences;decision makers properties;propose preference;preferring red cars;paper propose preference;preferences preferring;preferences preference;reasoning aggregation scenarios;preferences computer science;reasoning aggregation;preference;user preference play;decision based;predict outcome race", "pdf_keywords": ""}, "a636768c2fc6cadccd8bb4d704f651dd54dad395": {"ta_keywords": "emotion recognition human;emotion recognition;emotion based acoustic;recognition human speech;study emotion recognition;speech terms emotion;human speech;emotion occnce language;support vector machine;indonesian speech;classifies indonesian speech;emotion based;terms emotion based;classifier;indonesian speech terms;words represent emotional;based acoustic features;corpus evaluation;emotional aspect human;emotional content conversation;represent emotional content;analyses corpus;human speech perform;classifier optimize recognition;building classifier;feature selection;classifier optimize;acoustic features;analyses corpus evaluation;machine classifies indonesian", "pdf_keywords": ""}, "f664e6635d0514b0cb398a713f08bab90b4a3d81": {"ta_keywords": "sparse word graphs;statistical topic models;topic models;sparse word;topic models latent;word graphs;documents topics based;topics statistical;called sparse word;models latent dirichlet;topic semantically;models documents topics;documents topics;topic semantically meaningful;word graphs spgs;topics statistical topic;words context statistical;correlations words context;topics based;latent dirichlet;sparse manner paper;emergence correlations words;lations topics statistical;probabilistic models documents;correlations words;lations topics;latent dirichlet al;corre lations topics;summarize large document;based sparse", "pdf_keywords": ""}, "bd1bdb3c5f28001a4cee92c0e1669512d0f06a35": {"ta_keywords": "heaps law;heaps law derived;number rounds heap;rounds heap;heaps;rounds heap performed;heap;generalized zipf law;heap performed;zipf law;generalized zipf;heap performed closed;zipf law previously;derived generalized zipf;number rounds;zipf;closed loop program;rounds;formula number rounds;loop program;closed loop;performed closed loop;loop;program;law;law derived;formula;elegant formula;elegant formula number;simple elegant formula", "pdf_keywords": "number unique words;generalized zipf law;unique words text;heaps law generalized;law generalized zipf;number text words;words text constant;derivation heaps law;generalized zipf;unique words;text words;zipf law slightly;words text;zipf law;heaps law;accurate text model;paper prove expected;expected number unique;derivation heaps;text words paper;formal derivation heaps;law slightly generalized;proof published russian;total number text;text constant;property gamma;words paper present;theorem based summation;text model pi;slightly generalized version"}, "a9b9404962760731d6d2fc2ecbc6da7bc2f21be7": {"ta_keywords": "detect voice activity;voice activity detection;periodicity speech recognition;voice activity;detect voice;based voice activity;voice activity humans;speech periodicity speech;speech recognition;used detect voice;model based voice;periodicity speech;speech periodicity;based whittle automata;voice;identify non speech;whittle automata;non speech periodicity;activity detection vad;automata whittle;automata whittle automata;speech;automata;activity detection;gaussian model propose;context gaussian model;based voice;automata identify;whittle automata identify;gaussian reduction", "pdf_keywords": ""}, "32367e7587d5b2de0391cff9ad2d600ff8624e60": {"ta_keywords": "skills training audiovisual;features audiovisual;automated social skills;features audiovisual features;training audiovisual;audiovisual features;training audiovisual information;considering audiovisual features;audiovisual features regarding;audiovisual features automated;human social skills;features automated social;users social skills;social skills training;social skill training;audio features audiovisual;using audio features;features human social;social skills;training using audio;social skills people;skills people social;social skills trainers;social skill;effectiveness social skill;simulating social skills;audio features;audiovisual information;evaluate social skills;considering audiovisual", "pdf_keywords": ""}, "22f4eb19be4031e63194bbd7c355914533004918": {"ta_keywords": "electric vehicles;electrified vehicles propose;powertrain optimal;battery electric vehicles;constraints powertrain optimal;hybrid transmission;dedicated hybrid transmission;intermediate xev vehicles;dynamometer driving schedule;powertrain optimal control;vehicles propose cost;powertrain increasingly electrified;electric vehicles bevs;electrified vehicles;strong electrified vehicles;constraints powertrain;urban dynamometer driving;vehicles bevs powertrain;hybrid transmission dht;brake energy;brake energy legislative;dht constraints powertrain;transmission considered vehicle;powertrain increasingly;xev vehicles;ice based vehicles;dynamometer driving;powertrain;bevs powertrain increasingly;recuperate brake energy", "pdf_keywords": ""}, "ea77b71385648f5c6ea533a0e3685f0e76302eba": {"ta_keywords": "entity targeted annotation;annotated entity targeted;lingual transfer learning;annotated entity;human annotation effective;language minimizing annotator;targeted annotation strategy;entity recognition ner;entity recognition;data annotated entity;named entity recognition;entity recognizers;annotation strategy;targeted annotation;annotation effective;human annotation;progress human annotation;annotation uncertain entity;annotation strategy achieve;minimizing annotator effort;performing targeted annotation;lingual transferred model;entity recognizers resourced;real human annotation;human annotation settle;targeted annotation uncertain;highly resourced languages;annotator effort;quality entity recognizers;target language minimizing", "pdf_keywords": "deep sequence labeling;sequence labeling tasks;annotating word sequences;accurate annotating entities;predicting annotating word;annotating entities;lingual transfer learning;sequence annotation;annotating entities using;predicting annotating;sequence annotation propose;model predicting annotating;deep sequence;sequence labeling;approach deep sequence;annotators accurate annotating;performance sequence labeling;human annotation;train active learning;learning active learning;accurate annotating;annotating word;annotators accurate;annotation experiments annotators;engineering human annotation;opposed sequence annotation;annotation propose novel;annotating;active learning;new active learning"}, "0822f8d7e6a72a65e65f147d3a8d8fccd485da40": {"ta_keywords": "recurrence models transformers;recurrent models benefit;recurrent models;improve efficiency recurrence;external inputs recurrence;efficiency recurrence models;predicting performance motor;recurrent;inputs recurrence methods;recurrence models;efficiency recurrence;inputs recurrence;model short subsequences;techniques speeds training;training model short;short subsequences moving;shorter inputs;speeds training;perplexity efficiency improvements;subsequences moving longer;predicting performance;shorter inputs harmful;tokens generating sequences;range networks;achieve perplexity efficiency;short input lengths;short subsequences;memory;perplexity efficiency;improves perplexity wikitext", "pdf_keywords": "subsequences training inference;subsequences training;segmented subsequences training;language models longer;training corpus bert;subsequence train;training inference memory;long subsequences;subsequence train runtime;sequences models trained;train language models;trained long sequences;followed long subsequences;models trained corpus;length subsequence train;nonoverlapping subsequences;memory models trained;long subsequences approach;longer sequences models;nonoverlapping subsequences model;subsequences followed long;models longer sequences;inference memory constraints;short input subsequences;inference memory;2016 language models;language models;subsequences model;language models unsupervised;corpus bert 2016"}, "253ad629cd2d396201d71aa605bec233bff66dca": {"ta_keywords": "acoustic model topology;estimating topological structure;method estimating topological;estimating topological;topological structure acoustic;bayesian estimation clustering;clustering based gaussian;topological model;estimation clustering vbec;new topological model;gaussian mixture model;decision tree clustering;structure acoustic models;model called topological;based gaussian mixture;based variational bayesian;acoustic models;estimation clustering;variational bayesian estimation;topological model called;model topology;topological ball ball;finding appropriate acoustic;acoustic model propose;topological ball;model search algorithm;model topology vbec;topological structure;tree clustering based;clustering vbec", "pdf_keywords": ""}, "fac95cc5f52f954fe89b3aa4b75895568ff6a6d4": {"ta_keywords": "text support normalization;matching language data;exact matching language;word substitution list;normalization;matching language;support normalization;support normalization certain;modern wordforms;unsupervised rule based;historical wordforms;normalization certain;language data;historical wordforms modern;rule based approach;rule based;maps historical wordforms;substitution list rules;modern wordforms paper;unsupervised rule;word substitution;normalization certain extent;language data multiple;wordforms modern wordforms;data multiple languages;rules derived;use word substitution;list rules;rules derived highly;wordforms", "pdf_keywords": ""}, "8b73e226815d57bf66fc94905ebd063e4957b449": {"ta_keywords": "quantum calibration privacy;quantum state tomography;quantum calibration;measuring quality quantum;calibrate quality quantum;calibration privacy;quality quantum;quality quantum state;problem quantum calibration;quality quantum computer;calibration privacy provide;quantum computer;state tomography qst;quantum;qst tomography;quantum state light;privacy utility;quantum state;map computing adversary;tomography qst;state tomography;privacy utility design;tradeoff privacy utility;qst tomography present;tomography qst tomography;problem quantum;privacy;reviewers papers map;computing adversary;involving reviewers papers", "pdf_keywords": "reviewers miscalibration functions;adversary error paper;calibration privacy;minimizes reviewer;reviewers miscalibration;error conference adversary;minimizes reviewer error;simultaneously minimizes reviewer;calibration privacy provide;functions reviewers miscalibration;compute quality papers;conference adversary instance;problem calibration privacy;conference adversary;scores given reviewers;error maximizes adversary;given reviewers;functions reviewers;reviewer error maximizes;estimate calibrated scores;reviewers;miscalibration functions scores;adversary error;calibration strategy conference;exactly compute quality;compute quality;reviewer;maximizes adversary error;calibration algorithms assume;inverse functions reviewers"}, "f249e3a7d4f7f964e9a4ca6e633ac31410a91dd8": {"ta_keywords": "languages neural;monolingual data hallucination;languages neural networks;class languages neural;inflection decoder;languages monolingual data;step inflection decoder;inflection decoder propose;lingual transfer;monolingual data;lingual;cross lingual transfer;cross lingual;state art neural;lingual transfer single;effects cross lingual;performance neural;languages monolingual;neural networks perform;languages;neural networks context;large class languages;neural network methods;performance neural networks;multiple languages monolingual;multiple languages;monolingual;decoder;accuracy models outperforms;study performance neural", "pdf_keywords": "sequence model attention;neural machine translation;machine translation architecture;attention decoder architecture;multilingual language transfer;attention decoder;attention architecture outperforms;language transfer;attentional neural machine;translation architecture;attention architecture;machine translation;step attention decoder;transferring sequence characters;model attention proposed;translation architecture based;language transfer paper;novel attentional neural;propose novel attentional;model attention;attentional neural;recurrent neural network;step attention architecture;novel architecture multilingual;novel attentional;attention;task transferring sequence;attentional;architecture multilingual language;sequence characters language"}, "a309cb82c27233948f9b09f440be171a8d24ffff": {"ta_keywords": "peer selection agents;algorithm impartial peer;algorithms impartial individual;prize algorithm possesses;impartial peer selection;prize algorithm;algorithm impartial;award prize algorithm;novel algorithm impartial;selection peer nomination;algorithms impartial;based algorithms impartial;metrics peer selection;peer selection;agent based algorithms;selection peer;peer selection peer;algorithms metrics peer;peer nomination provide;peer nomination;impartial individual agent;agents choose subset;selection agents choose;selection agents;partitioning agents;impartial peer;explicit partitioning agents;agents previous algorithms;subset award prize;agents choose", "pdf_keywords": "accuracy peer selection;inexact peer selection;selection peernomination provide;algorithm impartial peer;peer selection based;peer selection method;automatic peer selection;impartial peer selection;selection peernomination;peer selection peernomination;peer selection;underlying reviewer graph;mechanism automatic peer;accuracy peer;analysis accuracy peer;automatic peer;underlying reviewer;agents review;peer selection pillars;reviewer graph;truth reviewers;agents review reviewed;novel algorithm impartial;impartial peer;assumptions underlying reviewer;algorithm impartial;peernomination provide;reviewers;reviewer graph problem;inexact peer"}, "04a94c15fec43e7563d58be697246a0dd6c57021": {"ta_keywords": "organize censor content;censor content;curate content;organize censor;social media landscape;outraged users platforms;curate content wish;censor content choose;content like moderate;content tweets;movements outraged users;objectives curate content;discretion organize censor;monetize content like;curate;news media grassroots;social media;algorithms curating;monetize content;media grassroots;media grassroots movements;powerful algorithms curating;modern social media;curating;censor;raw content tweets;content;algorithms curating practically;shaping people users;curating practically", "pdf_keywords": "curate content nature;curate content;curator articles platforms;platforms present content;curate content wish;used curate content;articles platforms enjoy;intermediaries creators say;does curator articles;technology used curate;intermediaries creators;objectives curate content;content nature;user content rapidly;content power;monetize content;curator articles;curator words;curate;articles platforms;user content;preexisting audio article;words does curator;curator articles bears;media products users;monetize content like;content;content rapidly;curator words does;audio article"}, "a99de68ee8d6729eee5ca5943b152aba7e4738ee": {"ta_keywords": "distributed representations edits;neural editor edit;neural editor;bose einstein condensate;representations edits;combining neural editor;quantum;2d bose einstein;encoder models learn;state quantum;edit encoder models;bose einstein;einstein condensate;dimensional 2d bose;distributed representations;bound state quantum;state quantum presence;learning distributed representations;neural;encoder models;particles;editor edit encoder;edit encoder;structure semantics edits;neural network;edits new inputs;einstein condensate bec;2d bose;representations;quantum presence external", "pdf_keywords": "deep neural edit;automatic editing spoken;sequence editing;neural edit model;sequence dataset edits;sequence sequence editing;editing spoken speech;neural edit;editing spoken;sequence editing sequence;editing sequence dataset;editing sequence;semantically equivalent edits;automatic editing;equivalent edits automatically;dataset edits wikipedia;common edit patterns;model automatic editing;edit patterns;recognize common edit;edit model learns;edits wikipedia;edits wikipedia articles;synthesis groups edits;equivalent edits;dataset edits;editing;spoken speech sequences;speech sequences;common edit"}, "7f20366098665cd508fe82255cc1a65e1e733a14": {"ta_keywords": "estimation speech enhancement;use speech enhancement;speech enhancement;speech enhancement techniques;enhanced speech features;speech enhancement technique;mismatch enhanced speech;enhanced speech;speech recognition;speech features acoustic;automatic speech recognition;improvement recognition performance;performance automatic speech;feature variance decoding;speech features;improvement recognition;likelihood estimation speech;speech recognition suffers;variance decoding performance;limiting improvement recognition;features acoustic model;estimation speech;discriminative criterion adaptation;features acoustic;automatic speech;estimate feature variance;reduction compared spectral;variance decoding;spectral subtraction preprocessor;noise reverberation", "pdf_keywords": ""}, "22d2f8030221bd0c27bfb9416eeffe4e86633780": {"ta_keywords": "neural document ranking;ranking neural document;ranking neural;document ranking approaches;document ranking;similarities ranking neural;fast forward indexes;semantic similarities ranking;ranking performance;fast query processing;fast query;query processing fast;ranking performance propose;ranking approaches;neural networks fast;ranking technique;sparse models retrieval;ranking;ranking approaches specifically;improvements hybrid indexes;forward ff ranking;similarities ranking;popular ranking technique;gains ranking performance;models retrieval;indexes performance query;indexes rely efficient;ff ranking;impressive gains ranking;ranking popular ranking", "pdf_keywords": "document retrieval ranking;combines sparse retrieval;index document retrieval;sparse retrieval index;retrieval ranking strategies;efficiency retrieval ranking;retrieval ranking;retrieval ranking computing;dense retrieval index;retrieval index semantic;semantic document ranking;sparse retrieval;index dense retrieval;document retrieval;combines document retrieval;document ranking;document retrieval combines;document ranking tasks;retrieval index dense;retrieval index;novel retrieval method;fast index;ranking computing semantic;effectiveness efficiency retrieval;efficiency retrieval;documents semantically similar;index semantic;dense retrieval;fast forward indexes;retrieval combines"}, "4e0610ac4c5e055ac56b2ae0d91386a10ffbd325": {"ta_keywords": "simulates human learning;learning agent sim;learning agent;intelligent agent simulates;constructing learning agent;learning agent currently;learning artificial intelligence;integrate stochastic differential;engineering constructing learning;integrate deep;integrate stochastic;machine learning agent;integrate deep feature;agent simulates;integration prior knowledge;learner machine;approach integrate stochastic;agent simulates human;human learning math;sde controlled simulation;learning complex;manual knowledge encoding;stochastic differential;human learning artificial;learning complex skills;feature learner;differential equation sde;equation sde controlled;building intelligent agent;learning artificial", "pdf_keywords": ""}, "8d019c77989100a51385e4b4a5fa5250445d8f1d": {"ta_keywords": "speech recognition tasks;sequential discriminative training;discriminative training;discriminative training framework;speech recognition;generalized discriminative training;proposes discriminative training;vocabulary speech recognition;discriminative training criterion;lvcsr task corpus;balance sequential discriminative;sequential discriminative;boosting methods;mixture models deep;recognition tasks;generalized discriminative;recognition tasks 2nd;discriminative;task corpus;training framework combination;training criterion generalized;relates boosting methods;method relates boosting;chime challenge track;boosting methods paper;combination encompasses acoustic;criterion generalized discriminative;paper proposes discriminative;corpus spontaneous japanese;models deep neural", "pdf_keywords": ""}, "1b97c38d0156dc8bf300b41c2ba5c0463c3a2c00": {"ta_keywords": "augmented training data;immersion data augmentation;data augmentation;data augmentation simple;new data augmentation;data augmentation strategy;augmented training;selection augmented training;augmented data analysis;augmented data;augmentation strategy based;augmentation simple efficient;speech recognizer;immersion data;based noising reverberation;noising reverberation;training data;robustness speech recognizer;fish eye;augmentation strategy;augmentation simple;improve robustness speech;headed fish eye;augmented;training data propose;strategy based noising;augmentation;speech recognizer deployed;fish eye tf;recognizer", "pdf_keywords": ""}, "3df825e086b00dd4132c34ecbf638f9a6dc4320d": {"ta_keywords": "learns procedural knowledge;learns procedural;learning agent;nanotube;acquiring procedural knowledge;procedural knowledge;simstudent learns procedural;carbon nanotube;learns;walled carbon nanotube;learning agent simstudent;nanotube cnt;procedural knowledge using;human learning artificial;learning artificial;carbon nanotube cnt;procedural knowledge examples;nanotube cnt operated;human learning;machine learning agent;intelligent agent simulates;artificial intelligence creating;agent simstudent learns;building intelligent agent;solving building intelligent;learning artificial intelligence;human level learning;simstudent learns;understanding human learning;acquiring procedural", "pdf_keywords": ""}, "3e59b3e1e3ef65f9574a0fe30f18ba7a815ea0af": {"ta_keywords": "exploration deep learning;learning greedy exploration;learning agents rewards;exploration deep;efficiency exploration deep;rewards sparse action;randomly exploring agent;based thompson sampling;agents rewards sparse;learning agents;thompson sampling;greedy exploration;greedy exploration inefficient;randomly exploring;dynamics randomly exploring;tasks make learning;dialogue tasks;exploration inefficient;exploring agent;improves efficiency exploration;inefficient introduce exploration;approach learning noisy;task oriented dialogue;sparse action;deep learning agents;harvest dialogue tasks;rewards sparse;exploration inefficient introduce;learning noisy;thompson sampling draw", "pdf_keywords": ""}, "790eb7e93f1d3fce470c0222fd2be83bab55a428": {"ta_keywords": "rnn language models;word based rnn;based rnn language;rnn language;automatic speech;based rnn lm;automatic speech recognition;prediction performance team;based rnn;speech recognition;language models;language models end;rnn lm;rnn lm provides;word probabilities predict;end automatic speech;ahead word probabilities;rnn;team offensive performance;predict characters;predict characters instead;team team offensive;team offensive;performance team team;probabilities predict characters;speech recognition proposed;character based lm;wer vocabulary size;performance team;prediction performance", "pdf_keywords": "speech recognition end;end speech recognition;speech recognition asr;cnn architecture speech;architecture speech recognition;asr automatic speech;speech recognition word;word based rnn;speech recognition;voice search;automatic speech recognition;end automatic speech;language models end;speech recognition incorporates;automatic speech;word based encoders;end encoder decoder;end end speech;applications voice search;speech recognition paper;end speech;recognition asr automatic;voice search paper;based rnn lm;end encoder;recognition end end;character based decoding;recognition asr;end end encoder;called automatic speech"}, "83cf7b9611fabe9da2d08722445039023f1b19e9": {"ta_keywords": "pedestrian crowded environment;dynamics pedestrian pedestrian;dynamics pedestrian;behavior pedestrian;pedestrian pedestrian crowded;environment behavior pedestrian;pedestrian crowded;crowded environment behavior;pedestrian response sudden;behavior pedestrian response;pedestrian pedestrian;pedestrian;pedestrian response;crowded environment;relationship dynamics pedestrian;behavior;crowded;dynamics;environment behavior;sudden change environment;response sudden change;response sudden;investigate relationship dynamics;sudden change;relationship dynamics;environment;sudden;change environment;relationship;change", "pdf_keywords": ""}, "9b09ff09b88bb793b161f284ca6e66031bc5a992": {"ta_keywords": "institute research computer;ural winter institute;winter institute research;computer science uci;winter institute;research computer science;research computer;science uci 2015;results ural winter;institute research;computer science;science uci;ural winter;institute;uci 2015;results ural;report results ural;uci;ural;research;computer;report results;winter;results;science;2015;report", "pdf_keywords": ""}, "68f2f32e0e8fc868920971077a11042784be2616": {"ta_keywords": "ranking rating sports;rating sports tournaments;rating sports;rating ranking;ranking rating;rating ranking book;problem rating ranking;method ranking rating;ranking;ranking strict;ranking book;tournaments based;sports tournaments based;sports tournaments;propose ranking strict;recommendations propose ranking;tournaments based movie;propose ranking;tournaments;new method ranking;method ranking;ranking strict linear;ranking book author;rating;based movie recommendations;book tuning;sports;book tuning equal;problem rating;movie recommendations propose", "pdf_keywords": ""}, "a3ca4893ae941bd1601322aface4840e47339761": {"ta_keywords": "crowdsourcing;crowdsourcing setting agents;conferences crowd sourcing;crowd sourcing;crowdsourcing setting;properties study crowdsourcing;model peer peer;study crowdsourcing;study crowdsourcing setting;crowd sourcing used;model peer;distributing awards team;crowd agents;ubiquitous peer review;peer peer;selected crowd agents;simple model peer;crowd agents paper;distributing awards;peer;outcome attempt peer;peer review;ubiquitous peer;publications conferences crowd;strategyproof impartial mechanism;evaluations subset agents;attempt peer;peer peer used;setting ubiquitous peer;funding scientists", "pdf_keywords": ""}, "a810d2f4a1fefd4175d8cdda9702ee1b829e5831": {"ta_keywords": "expression adipogenesis phytohemagglutinin;adipogenesis phytohemagglutinin pha;adipogenesis phytohemagglutinin;adipogenesis ucp mice;mitochondrial expression adipogenesis;relationship adipogenesis ucp;expression adipogenesis;adipogenesis ucp;gene expression adipogenesis;adipogenesis;relationship adipogenesis;investigate relationship adipogenesis;adipose;adipogenesis switched;expression adipogenesis switched;obesity increasing energy;brown adipose;white adipose;adipose skin;adipose skin wats;adipose tissue;adipose tissue bat;brown adipose tissue;bat white adipose;mice regulation mitochondrial;obesity natural compounds;pha anti obesity;white adipose skin;expression brown adipose;molecular mechanisms phytohemagglutinin", "pdf_keywords": ""}, "66081634c17b089cb47fd1b0ad7ad842c7fb3f87": {"ta_keywords": "detrimental tutor learning;effect tutor learning;theories tutor learning;tutor learning researchers;learning detrimental tutor;tutor learning crucial;tutor learning;tutor learning including;tutor learning aim;tutee tutor learning;self regulated learning;human students learning;social theories tutor;detrimental tutor;tutor;effect tutor;students learning;sim student learning;regulated learning;learning meta cognitive;theories tutor;student learning ducted;feedback sim student;meta cognitive skills;student learning;learning researchers explored;teaching sim student;tween tutee tutor;cognitive skills;skills self regulated", "pdf_keywords": ""}, "d706645fbbc6edfad5fb642b1dfc3019fcabbd99": {"ta_keywords": "story evaluation experiments;human generated references;story evaluation;judgments text quality;text generation papers;model generated text;text generation;text human generated;crowdsourced human judgments;generated references;judgments text;ended text generation;human judgments text;generated references paper;generated text;generated text human;open ended text;generation papers vast;generated references distinguish;generation open ended;generation papers;series story evaluation;approach generation open;novel approach generation;evaluate automatically researchers;crowdsourced;domains model generated;crowdsourced human;collecting crowdsourced;choices collecting crowdsourced", "pdf_keywords": "evaluation machine translation;text quality evaluation;judgments text quality;crowdsourced human judgments;generated text quality;text generation tasks;machine translation;machine translation conducted;human evaluations open;text quality grammar;text generation;human evaluations;machine translation traditionally;human evaluation machine;human judgments text;ended text generation;human evaluation;crowdsourced human;quality evaluation;text quality;evaluation machine;quality grammar;evaluations open ended;machine generated content;quality evaluation discriminate;judgments text;collect crowdsourced human;domains machine translation;crowdsourced;story generation"}, "ad7129af0644dbcafa9aa2f111cb76526ea444a1": {"ta_keywords": "neural fake news;classify fake news;false news generated;fake news targeted;generate neural fake;news generated false;news targeted propaganda;discriminators classify fake;detection neural fake;spread false news;neural fake;adversaries generate neural;false news based;classify fake;news false news;news targeted;news generated;careful threat modeling;targeted propaganda;targeted propaganda closely;identifying potential threats;news real human;propaganda closely mimics;adversaries generate;false news false;fake news propose;threat modeling identifying;news real;human written news;generated false discovery", "pdf_keywords": "generating fake news;detect fake news;news detect fake;false news classifying;real news detect;generating fake;disinformation generated;news classifying articles;quality disinformation generated;news classifying;news source publication;recognizing fake real;recognizing fake;news detect;method recognizing fake;detect fake;novel machine translation;method generating fake;harm false news;disinformation;quality disinformation;machine translation;fake news propose;machine translation method;generation entire news;fake real news;classifying articles human;title news source;classifying articles;threat modeling"}, "946e5e31b0779fc33550e8681994e7afd8d549a5": {"ta_keywords": "clinical gait analysis;measurement clinical gait;gait analysis;clinical gait;motion measurement clinical;automated motion measurement;motion measurement;gait;automated motion;present automated motion;measurement clinical;motion;measurement;present automated;automated;clinical;analysis;present", "pdf_keywords": ""}, "81d4357afae9680e64a645cbb36aa090c3619b19": {"ta_keywords": "topology category strength;ties category;xmath0crossing competition fermilab;search winner xmath0crossing;xmath0crossing competition;topology category;category strength ties;winner xmath0crossing competition;performance home page;competition fermilab tevatron;winner xmath0crossing;competition fermilab;strength ties category;xmath0crossing;home page;home page paper;fermilab tevatron paper;loop home page;relationship topology category;tevatron paper;tevatron paper investigates;category;topology;closed loop home;fermilab tevatron;ties;home page improve;loop home;category strength;page", "pdf_keywords": ""}, "a7d6b5e61024127bf4fe8f04c0182a16ff97bccf": {"ta_keywords": "probabilistic model lobbying;lobbying stochastic;problem lobbying stochastic;complexity problem lobbying;lobbying stochastic environment;complexity probabilistic;criteria complexity probabilistic;model lobbying;model lobbying actor;problem lobbying;lobbying issue weighting;complexity probabilistic lighthouses;lobbying issue;lobbying actor;lobbying;probabilistic;quantum provide evaluation;state quantum;quantum state quantum;state quantum provide;quantum state;function quantum state;function quantum;quantum;quantum provide;forms lobbying;propose probabilistic;lobby seeks influence;voting multiple issues;preferences voting", "pdf_keywords": "complexity probabilistic lobbying;probabilistic lobbying problems;probabilistic lobbying;probabilistic lobbying popular;elections broaden complexitytheoretic;lobby probabilistic lobbying;lobby probabilistic;algorithm computing vote;lobbying problems;combinatorial auction generalization;deciding individual voter;lobbying problems paper;computing vote;parameterized complexity probabilistic;importance lobby probabilistic;combinatorial auction;individual voter votes;elections broaden;complexity probabilistic;lobbying popular political;lobbying;elections;individual voter;auction generalization problem;hybrid elections broaden;computing vote say;auction generalization;voter votes;voter votes referendum;present combinatorial auction"}, "419e714f22c3fa2599abebd630cae5595c70bdef": {"ta_keywords": "modules speech enhancement;speech recognition enhanced;recognition enhanced speech;speech enhancement se;enhanced speech recognition;speech recognition ssc;enhanced speech input;speech recognition asr;speech enhancement;robust speech recognition;speech recognition;automatic speech recognition;e2e automatic speech;speech recognition learning;enhanced speech;use enhanced speech;automatic speech;targetting robust speech;speech recognition based;robust speech;enhances noisy speech;approach automatic speech;modules speech;speech input self;theory speech recognition;applications speech recognition;machine learning module;speech recognition paper;ssc applications speech;recognition asr model", "pdf_keywords": "iris robust speech;robust speech recognition;iris automatic speech;model automatic speech;robust automatic speech;speech enhancement model;speech recognition automatic;integraded speechrecognition enhanced;speech recognition machine;speech recognition asr;automatic speech recognition;recognition automatic speech;enhanced speech input;speechrecognition enhanced speech;speechrecognition enhanced;speech recognition integrates;speech recognition;speech recognition called;use speech enhancement;speech recognition noisy;recognition speech recognition;speech enhancement;speech recognition speech;robust speech;dnn architecture robust;recognition speech;automatic speech;enhanced speech;targetting robust speech;integraded speechrecognition"}, "888c81cd3d1e953e2b7f8cc4ce68ca9f908c1e8d": {"ta_keywords": "privacy text representation;differential privacy text;privacy text;differential privacy;privacy preserving;protect privacy person;novel privacy preserving;applying differential privacy;propose novel privacy;privacy preserving method;protect privacy;privacy;novel privacy;differentially private paper;privacy person;compelling privacy;privacy guarantees continuous;dip compelling privacy;compelling privacy guarantees;privacy guarantees;differentially private;method protect privacy;text representation learning;proposing text representation;private paper;claims differentially private;text representation;privacy person noisy;private;learning using dptext", "pdf_keywords": "differential privacy text;learning differential privacy;differential privacy privacy;privacy text representation;differential privacy based;differential privacy new;differential privacy;differential privacy propose;introduces differential privacy;based differential privacy;applying differential privacy;function differential privacy;concept differential privacy;differential privacy used;differentially private query;idea differential privacy;differentially private;algorithm differentially private;privacy text;differentially private particular;data privacy;private attribute discriminators;privacy new concept;new privacy;privacy privacy;protecting data privacy;privacy new;privacy propose new;propose new privacy;new privacy model"}, "596b46dbe4fa8eee72e517ea9fd5f8ef83c9c64e": {"ta_keywords": "playing quizbowl involves;playing quizbowl;approach playing quizbowl;quizbowl popular game;vibrant quizbowl community;quizbowl community;motion quizbowl;quizbowl involves determining;collaboration vibrant quizbowl;quizbowl involves;intelligence quizbowl;quizbowl;knowledge intelligence quizbowl;answer quizbowl popular;video motion quizbowl;motion quizbowl affected;quizbowl popular;computational approach playing;game tests human;intelligence quizbowl defeated;answer quizbowl;vibrant quizbowl;quizbowl community contributed;popular game tests;quizbowl affected;tests human knowledge;answer answer quizbowl;quizbowl defeated;game tests;human knowledge intelligence", "pdf_keywords": "question answering task;question answering;compelling question answering;incremental question answering;nlp challenges;nlp challenges required;enumerate nlp challenges;qb machine learning;answer trivia questions;trivia ecosystem;learning challenges;factoid qa dataset;answering task;curating factoid qa;machine learning challenges;creating difficult questions;deep sentence embedding;learning task leverages;answer series gameplay;designed trivia ecosystem;approach answer trivia;qb compelling;sentence embedding answer;learning task;embedding answer series;accompanying gameplay dataset;machine learning task;dataset accompanying gameplay;datasets qb machine;qanta datasets qb"}, "650f2afca6d72d6b6e2e08849e1224f1e8b7900c": {"ta_keywords": "properties graphene nanoribbon;graphene nanoribbon;graphene nanoribbon gnb;electronic properties graphene;graphene;properties graphene;nanoribbon gnb characterize;nanoribbon;networks graph information;nanoribbon gnb;complex networks graph;networks graph;graph information;graph information paper;simulation complex networks;graph information reduces;complex networks;quality graph information;electronic properties;field electronic properties;graph;sample complexity;networks;quality graph;computationally efficient;function quality graph;reduces sample complexity;algorithm simulation complex;sample complexity factor;electric field electronic", "pdf_keywords": ""}, "932404745d960291925b3f27b71734dff5b23633": {"ta_keywords": "implement treatment disparity;disparity formally treat;treatment disparity undermining;parities proposing disparate;induce class discrimination;disparity undermining;reducing treatment disparity;disparity mitigated;treatment disparity formally;disparity mitigated reducing;exhibit treatment disparity;disparate learning;treatment disparity;class discrimination;impact disparity mitigated;disparity undermining policy;disparate learning processes;class discrimination iii;proposing disparate learning;impact disparity outcomes;achieve parity violating;disparity outcomes;discrimination;impact parity algorithms;discrimination iii general;accuracy impact parity;discrimination iii;disparity outcomes differ;disparity formally;parity violating", "pdf_keywords": ""}, "7bbd132f40c7630aeebf6379b00e307c3fff738c": {"ta_keywords": "repair bandwidth distributed;stored nodes distributed;minimizing repair bandwidth;distributed storage presented;distributed storage;replication;bandwidth distributed storage;exact replication;nodes distributed manner;nodes distributed;codes minimizing repair;repair bandwidth;bandwidth required repair;stored nodes;repair failed node;replication failed systematic;data stored nodes;permit exact replication;failed node data;distributed;data connecting nodes;distributed manner;minimizing repair;nodes;replication failed;systematic node;exact replication failed;failed systematic node;systematic node explicit;node data", "pdf_keywords": "reconstruction regenerating codes;regenerating codes storage;codes achieve bound;codes storage repair;code reconstruction algorithm;code reconstruction;regenerating codes;bound repair bandwidth;codes storage;algorithm recovery;algorithm recovery data;storage repair bandwidth;code minimum repair;regenerate data code;coding scheme;presented coding scheme;minimum repair bandwidth;illustrate code reconstruction;polynomials reconstruction regenerating;lower bound repair;existence construction codes;minimizing repair bandwidth;linear code;coding scheme used;symbols storage systems;bandwidth exact regeneration;bound repair;data symbols storage;storage nodes;data storage nodes"}, "e8c3090e66fdb05a2c169a12c52dd94bb8786fb5": {"ta_keywords": "generating explanations;generating explanations propose;echoing word explanation;predicting echoing content;crucial predicting echoing;word level prediction;explanations selectively reuse;explanations argument persuasive;methods generating explanations;explanations propose;predicting echoing;information explained explanations;explanations central everyday;naturally occurring explanations;investigate explanations selectively;word explanation enhance;explanation enhance neural;investigate explanations;occurring explanations;word explanation;explanations selectively;occurring explanations argument;explanations propose novel;task investigate explanations;explanations argument;explained explanations central;explanations central;explanations;topic growing artificial;patterns word echoed", "pdf_keywords": "detecting deceptive reviews;detecting deceptive;generating explanations;deceptive reviews formulate;argumentation predict word;performance detecting deceptive;propose explanations capture;web argumentation predict;generating explanations formulate;methods generating explanations;argumentation predict;explanations capture general;explanations word;explanations word level;word level explanations;deceptive reviews;explanations capture;occurring explanations word;predict words op;propose explanations;predict words;naturally occurring explanations;predict word actual;word actual conversation;explanations highlighting;paper propose explanations;word level prediction;occurring explanations;explanations highlighting important;predict word"}, "ed535e93d5b5a8b689e861e9c6083a806d1535c2": {"ta_keywords": "neural networks generalize;generalization ability neural;generalization validation sets;neural networks report;proper generalization validation;generalization validation;transformers systematic generalization;positional embedding significantly;positional embedding largely;positional embedding universal;generalization relative positional;developing neural networks;embeddings early stopping;generalization ability;ability neural networks;neural networks;systematic generalization ability;scaling embeddings early;positional embedding;accuracy length split;calls proper generalization;networks generalize;embedding significantly mitigates;embedding significantly;accuracy length;neural;decide robot replaced;decide robot;100 accuracy length;relative positional embedding", "pdf_keywords": "compositional generalization train;compositional generalization neural;neural networks generalise;generalization neural;neural networks generalize;generalization train;generalization neural symbolic;embedding compositional generalization;generalization accuracy loss;learning compositionality decomposed;learning compositionality;investigate compositional generalization;machine learning compositionality;generalize systematically learning;compositional generalization;sequence recurrent neural;generalization accuracy;trained sequence sequence;trained sequence;decomposed neural networks;machine translation tasks;recurrent neural network;recurrent neural;generalization longer;compositionality decomposed neural;proper generalization;networks generalize;neural networks;machine translation;neural symbolic stack"}, "9abb50813e05de849dbbd89535bc7d0206f5e36a": {"ta_keywords": "novel clustering;present novel clustering;clustering;novel clustering method;propose clustering;language processing;analysis clustering;clustering method;clustering procedure;propose clustering algorithm;natural language processing;clustering algorithm;clustering method based;work propose clustering;plot analysis clustering;clustering procedure performed;person natural language;clustering algorithm process;analysis clustering procedure;language processing process;natural language;space representation probability;modality test group;dataset based levin;verb classes;metric dataset based;vmeasure metric dataset;international modality test;representation probability density;modality test", "pdf_keywords": ""}, "0bbfa6ab7451aea5cbb842cce97b54500bafdfc7": {"ta_keywords": "preferences inverse reinforcement;inverse decision theory;inverse decision;inverse reinforcement learning;human preferences inverse;study inverse decision;learning human preferences;preferences inverse;inverse reinforcement;binary decisions uncertainty;experiments inverse reinforcement;preference learning difficult;preference learning cases;make preference learning;learn human preferences;preference learning;identify preferences decision;nonsequential binary decisions;decisions uncertainty existing;understand preference learning;decisions uncertainty;improving preference learning;preferences decision;binary decisions;decisions uncertainty provides;preferences decision problem;optimal decision;human decision problem;important decisions uncertainty;improving preference", "pdf_keywords": "suboptimal decision;suboptimality decision problem;learning uncertainty suboptimality;suboptimal decision making;inverse decision theory;decisions uncertainty;models suboptimality decision;uncertainty suboptimality;preference learning uncertainty;uncertainty decision;suboptimality decision;forms suboptimal decision;uncertainty decision making;inverse decision;decision theory;uncertain decisions allow;important decisions uncertainty;uncertainty suboptimality reality;uncertain decisions;decision theory decision;learning uncertainty;evidence inverse decision;effects uncertainty decision;decision problem particular;presented uncertain decisions;supervised learning preference;models suboptimality presented;models suboptimality;suboptimality presented uncertain;decision problem human"}, "a31ab366b0a349ee5f341f1179810bc9805d32a4": {"ta_keywords": "storage protocol;storage protocol based;distributed storage protocol;protocol allows storage;distributed storage;stored distributed storage;data file securely;securely stored memory;file securely stored;securely stored;minimum storage regenerating;storage regenerating;allows storage retrieval;storage;storage retrieval;stored distributed;storage retrieval data;operating minimum storage;data stored distributed;memory holds file;holds file size;allows storage;file securely;regenerating codes operating;minimum storage;stored memory;stored memory holds;storage regenerating msr;protocol based idea;secure msr regenerating", "pdf_keywords": ""}, "04d18fc81cc232b3d3dece0994c0fa8aaabaf4b7": {"ta_keywords": "speech pos tagging;morphological classification based;segmentation speech pos;morphological classification;word segmentation;method morphological classification;word segmentation speech;adaptation minimum annotation;process word segmentation;segmentation speech;based decomposition morphological;domain adaptation;morphological components;pos tagging;morphological;morphological components present;decomposition morphological;decomposition morphological image;new method morphological;tagging;annotation;morphological image;domain adaptation minimum;cost domain adaptation;pos tagging paper;linguistic resources flexibly;annotation paper;annotation paper proposes;languages pointwise approach;method morphological", "pdf_keywords": ""}, "d5dcbb144a2be999610b4838d94cc3fb228f837c": {"ta_keywords": "network plastic scintillators;deployment network plastic;scintillators form chip;plastic scintillators;plastic scintillators form;deployment network;ensuring deployment security;deployment security;ensuring deployment;reliability accuracy electrical;simulation experiments deployment;study deployment network;deployment security paper;scintillators;experiments deployment strategy;experiments deployment;deployment strategy ensuring;strategy ensuring deployment;quality vnf backup;network plastic;vnf backup configurations;deployment;deployment strategy;scintillators form;vnf backup;elements ultra reliable;ultra reliable;high quality vnf;accuracy electrical circuit;reliability accuracy", "pdf_keywords": ""}, "df689bdc6c497949e9ab3b7ba19950d9fade7180": {"ta_keywords": "population brownian oscillators;brownian oscillators;dynamics population brownian;population brownian;oscillators;dynamics population;environment dynamics population;brownian;environment dynamics;dynamics;effect environment dynamics;population;effect environment;environment;study effect environment;effect;study effect;paper study effect;paper study;study;paper", "pdf_keywords": ""}, "d7fe9b46f96ae9df7fa64e1c575c7114e5ef0aaa": {"ta_keywords": "complexity tensor decomposition;complexity analysis tensor;accelerated tensor method;complexity tensor;based complexity tensor;analysis tensor methods;new tensor method;tensor methods;tensor method;tensor decomposition;tensor method consider;tensor method solving;tensor methods based;analysis tensor;accelerated tensor;tensor decomposition consider;known accelerated tensor;tensor;propose new tensor;new tensor;convex optimization;convex optimization problems;consider convex optimization;method accelerated;proposed method accelerated;decomposition consider convex;methods based complexity;optimization;new complexity analysis;optimization problems objective", "pdf_keywords": "tensor methods convex;convex optimization tensor;optimization tensor methods;accelerated tensor method;optimization tensor;tensor methods unconstrained;order tensor methods;complexity analysis tensor;performance accelerated tensor;new tensor method;high order tensor;performance tensor methods;tensor method;tensor methods 3rd;tensor methods;analysis tensor methods;tensor method non;accelerated tensor;tensor method closes;order tensor;tensor methods assumption;performance tensor;compare performance tensor;propose new tensor;methods convex optimization;analysis tensor;tensor;new tensor;keywords convex optimization;unconstrained convex optimization"}, "4c6f7fb5c2e1bd12899c3ec2788f9ce7eb2f8a5c": {"ta_keywords": "syntactic generalization performance;pretrained language model;targeted syntactic evaluation;syntactic structural probes;speech tagging;impact syntactic capabilities;higher syntactic information;syntactic capabilities roberta;pretrained language;speech tagging dependency;size syntactic generalization;targeted syntactic;syntactic capabilities;models pretrained data;perform targeted syntactic;models pretraining methods;syntactic evaluation analyze;parsing paraphrase identification;syntactic generalization;encode syntactic knowledge;based pretrained language;tagging dependency parsing;training models pretraining;syntactic knowledge perform;models pretraining;performance different syntactic;encode higher syntactic;models pretrained;syntactic information;applications speech tagging", "pdf_keywords": "syntactic information trained;syntactic generalization models;syntactic generalization performance;models syntactic generalization;speech tagging dependency;performance models syntactic;models syntactic;targeted syntactic evaluation;tagging dependency parsing;syntactic structural probes;language models;speech tagging;predicting word context;higher syntactic information;syntactic generalization;size syntactic generalization;language models traditionally;targeted syntactic;dependency parsing paraphrase;dependency parsing;syntactic information;models generalize better;predicting word;syntactic evaluation analyze;syntactic evaluation;perform targeted syntactic;encode higher syntactic;applications speech tagging;syntactic structural;higher syntactic"}, "b73191adcc938cfcf20ce0327cf5cd1f539f7f81": {"ta_keywords": "named entity recognition;entity recognition annotated;extracting keyphrases scientific;entity recognition;supervised neural tagging;keyphrases scientific articles;neural tagging;extracting keyphrases;neural tagging model;information extraction;supervised neural tweezer;semi supervised learning;transductive semi supervised;information extraction performance;based semi supervised;semi supervised;model semi supervised;semi supervised neural;keyphrases scientific;leverage unannotated articles;unannotated articles;semi supervised algorithm;neural tweezer;problem extracting keyphrases;keyphrases;tagging;tagging model;annotated training data;advances named entity;scientific articles categorizing", "pdf_keywords": "supervised neural tagging;neural tagging;neural tagging model;neural tagging approach;experiments neural tagging;entity recognition;named entity recognition;methods neural tagging;entity recognition experiments;trained word embedding;sequence tagging;word embedding;semi supervised learning;introduce semi supervised;extracting keyphrases scienti\ufb01c;propose semi supervised;tagging approach;semi supervised methods;tagging introduce semi;tagging;semi supervised neural;sequence tagging introduce;supervised semi supervised;tagging model achieves;categorize scienti\ufb01c terms;keyphrases scienti\ufb01c articles;semi supervised;semi supervised strategies;tagging model;problem sequence tagging"}, "06d77cc8970b59102a0caffb5e4c5b7a3242563a": {"ta_keywords": "organized criticality sdis;self organized criticality;organized criticality;criticality sdis widespread;criticality sdis;criticality;disambiguated self organized;self organized;sdis widespread phenomenon;sdis widespread;self disambiguated self;widespread phenomenon nature;disambiguated self;phenomenon nature;self disambiguated;widespread phenomenon;nature;sdis;organized;self;phenomenon;disambiguated;widespread", "pdf_keywords": ""}, "4715ee17ca4f52762fdf67c9a8ef8fb751c88484": {"ta_keywords": "minimization problem blind;solution rank minimization;rank minimization;rank minimization problem;energy disaggregation framework;problem blind identification;blind identification known;relaxed convex formulation;convex formulation approximate;minimization;approximate solution rank;blind identification;convex formulation;energy disaggregation;minimization problem;posed problem proposed;bigger energy disaggregation;relaxed convex;formulation approximate solution;posed problem;solution rank;present relaxed convex;disaggregation framework;ill posed problem;task identifying systems;arx model approximates;fluid dynamics video;identifying systems;problem blind;corresponding arx model", "pdf_keywords": "estimating arx model;estimating arx;inverse problem identi\ufb01cation;problem estimating arx;method solving inverse;solution blind identi\ufb01cation;approximate solution blind;inverse;solving inverse;solving inverse problem;arx model;inverse problem;recognition image restoration;arx model solely;approximation output applications;signal processing information;blind identi\ufb01cation fundamental;complicated problem estimating;restoration seismic signal;image restoration;seismic signal processing;signal processing;identi\ufb01cation input information;convex formulation approximate;blind identi\ufb01cation;solution blind;input information;image restoration seismic;fundamental signal processing;estimating"}, "cac008e541af58f738407c7f2ee86d547053188f": {"ta_keywords": "phenomena;relationship phenomena;investigate relationship phenomena;paper investigate relationship;paper investigate;relationship;investigate relationship;paper;investigate", "pdf_keywords": ""}, "1e2ef0c9a494c7949f38940ee735a88c56355202": {"ta_keywords": "tracking ii id;activation centralized tracking;centralized tracking;algorithm tracking ii;novel algorithm tracking;sensor activation centralized;algorithm tracking;tracking ii;tracking;global optimal solution;global optimal;centralized tracking report;dynamic sensor activation;dynamic sensor;process known distribution;convergence global optimal;global optimality;sensor activation;problem dynamic sensor;minimizing time averaged;optimal solution;algorithms suggest global;suggest global optimality;active sensors;optimal solution high;process unknown parametric;tracking report;number active sensors;unknown parametric distribution;sensor", "pdf_keywords": ""}, "00c3a86551f1bc812b676025210e295021853f66": {"ta_keywords": "number people watched;purely statistical;movie rank;showing competition bouncer;people watched;competition bouncer winer;people watched movie;rank actors;statistical;competition bouncer;movie rank actors;significance purely statistical;bouncer winer;purely statistical means;video showing competition;watched movie rank;showing competition;bouncer winer ask;movie watched;statistical means;rank actors performing;dynamics video;number people;fluid dynamics video;movie watched movie;dynamics video showing;bouncer;watched movie;competition;dynamics", "pdf_keywords": ""}, "69d5579955a5a8859d78a70b3d1afede0f91fa09": {"ta_keywords": "home energy disaggregation;approach energy disaggregation;energy disaggregation;energy disaggregation known;data individual appliances;energy disaggregation presented;device home energy;energy data individual;aggregate energy data;disaggregating aggregate energy;disaggregated data consumer;intrusive load monitoring;energy data;energy consumption behavior;providing disaggregated data;disaggregated data;power consumed device;improves energy consumption;load monitoring;energy consumption;home energy;individual appliances;sensors device home;individual sensors device;load monitoring nilm;statistical technique disaggregating;power consumed;appliances;consumer improves energy;disaggregation known", "pdf_keywords": "users energy disaggregation;disaggregation energy data;energy disaggregation;energy disaggregation combines;energy disaggregation energy;disaggregation energy;method energy disaggregation;disaggregation disaggregation energy;devices formulating disaggregation;device usage disaggregation;energy usage behavior;providing energy usage;inef\ufb01cient usage energy;disaggregation provides feasible;disaggregation problem optimal;device disaggregation using;device disaggregation;disaggregation using dynamic;supervised approach disaggregation;energy usage;power consumed device;usage energy;energy data;energy data emerged;usage energy present;approach disaggregation;formulating disaggregation;approach disaggregation disaggregation;energy data provides;dynamic models devices"}, "834d68b9befcc6c68415b460b33435a1822799fb": {"ta_keywords": "argumentation mining;findings argumentation mining;argumentation mining user;argumentation model;argumentation phenomena encountered;argumentation phenomena;argumentation theories argumentation;argumentation model tested;generated web discourse;argumentation theories;findings argumentation;theories argumentation phenomena;theories argumentation;web discourse feasible;argumentation;data findings argumentation;web discourse;source codes annotation;adapt argumentation model;normative argumentation theories;extensive annotation study;annotation study;codes annotation guidelines;codes annotation;normative argumentation;annotation guidelines;adapt argumentation;extensive annotation;gap normative argumentation;annotation", "pdf_keywords": "argumentation data discussion;modeling argumentation;modeling argumentation user;discourse level argumentation;argumentation persuasion dialogues;corpus argumentation data;argumentation data;approach modeling argumentation;created corpus argumentation;cultural argumentation persuasion;argumentation persuasion;cultural argumentation;argumentation user generated;argumentation user;argumentation theories;argumentation based idea;cross cultural argumentation;argumentation based;normative models argumentation;corpus argumentation;argumentation theory;argumentation theory perform;argumentation theories depth;idea argumentation;argumentation coherent discourse;level argumentation based;based idea argumentation;application argumentation theories;persuasion dialogues presented;models argumentation theory"}, "972a74968d2522908b06c5bd1e26266194c5a9ee": {"ta_keywords": "automatically decontextualize sentences;decontextualize sentences decontextualization;sentences decontextualization;decontextualize sentences;sentence decontextualization;sentence decontextualization user;sentence decontextualization taking;sentences decontextualization interpret;automatically decontextualize;decontextualization taking sentence;preserving meaning annotation;decontextualization interpret meaning;decontextualization taking;decontextualization important subtask;value sentence decontextualization;decontextualization;decontextualization interpret;decontextualization user facing;wikipedia corpus;wikipedia corpus use;decontextualize;problem sentence decontextualization;decontextualization important;interpretable context preserving;decontextualization user;data wikipedia corpus;perform document understanding;models automatically decontextualize;rewriting interpretable context;document understanding", "pdf_keywords": "automatically decontextualizing sentences;decontextualizing sentences decontextualization;decontextualizing sentences;decontextualized sentences;sentences decontextualization;sentences decontextualized;sentences decontextualized using;sentences decontextualization involves;automatic decontextuality task;create decontextuallyized sentence;retrieval decontextualization capable;original sentences decontextualized;retrieval decontextualization;decontextualized sentences ensure;automatically decontextualizing;decontextualization capable retrieving;decontextuallyized sentence;paragraph context computational;decontextuallyized sentence original;coreference resolution;present retrieval decontextualization;automatic decontextuality;new automatic decontextuality;decontextualizing;decontextualized using;coreference resolution global;decontextualized;decontextualization capable;including coreference resolution"}, "6b387d18bae978202af501c4795f37a0c73781a6": {"ta_keywords": "convex optimization methods;proximal extragradient method;convex programs method;numerical convex optimization;hessian optimized;gradient hessian optimized;optimal tensor method;convex programs;convex optimization;accelerated proximal distance;outer accelerated proximal;hessian optimized function;extragradient method based;extragradient method;proximal extragradient;proximal distance method;accelerated proximal;optimal convergence rates;order numerical convex;new proximal extragradient;optimal convergence rate;optimization methods;class convex programs;optimal tensor;infinity optimal tensor;numerical convex;tensor methods order;based gradient hessian;optimization methods obtained;gradient hessian", "pdf_keywords": ""}, "2bb1e1a5b9a16f6828fe94736cea5dab264533a6": {"ta_keywords": "semantic emulation;semantic emulation languages;enable semantic emulation;assertions enable semantic;assertions textual contexts;underlying semantics;semantic transparency;semantic transparency raises;notion semantic transparency;contexts emulation;contexts emulation uncomputable;ungrounded language models;underlying semantics classes;semantics;emulation uncomputable formal;clues underlying semantics;assertions textual;preserving semantic;different contexts emulation;emulation languages satisfy;grounding ungrounded language;emulation languages;semantics classes languages;semantics classes;enable semantic;strong notion semantic;semantic;ungrounded language;preserving semantic relations;setting semantic", "pdf_keywords": "emulate semantics language;emulate semantics;possible emulate semantics;semantic emulation languages;semantic emulation;fully emulate semantic;based notion emulation;semantics natural language;representations semantic emulation;semantics language;semantics language distributional;present new semantics;emulate semantic;new semantics;emulate underlying semantic;emulation languages satisfy;new semantics natural;notion emulation;computable formalize;assertions code language;notion semantic transparency;computation natural language;semantics;semantics natural;computable formalize ways;emulation languages;language based notion;oracle computable formalize;emulate semantic representations;semantic transparency"}, "96b32b204a62777bef66eea595de2c47b4e9d6e9": {"ta_keywords": "gestalt image deep;image deep neural;image deep;deep neural;predictable classifier generalize;domain generalization data;generalization data;generalize previously unseen;distribution training;classifier generalize;extract patterns prior;generalization data sets;learn representations;target distribution training;domain generalization;superficial statistics training;domain generalizations explicitly;model learn representations;distribution training despite;classifier generalize previously;built reverse gradient;domain generalizations;deep neural networks;agnostic semantic signals;predictable classifier;generalization;standard domain generalization;model learn;patterns prior;generalize", "pdf_keywords": "deep supervised domain;supervised domain adaptation;domain generalization deep;domain adaptation;deep supervised;domain adaptation generalization;deep neural;generalization deep;deep learning;present deep supervised;semantic information image;capture textural semantic;generalization deep learning;neural network trained;deep neural network;semantic information images;trained set facial;trained digit recognition;supervised domain;deep learning computer;facial expression data;textural semantic information;recognition data;information image capable;recognition;textural semantic;model trained;network trained;multilayer perceptron;recognition data sets"}, "636611068825cb4b7bdab6ad16ef415adf4fb96c": {"ta_keywords": "multi domain learning;domain learning applied;domain learning algorithms;domain learning;domain learning approaches;domain learning suggests;classifier based balanced;learning algorithms fluid;ensemble learning effects;ensemble learning;classifier;domain used learn;dynamics multi domain;multi domain domain;propose new classifier;domain specific class;multi domain;new classifier;dynamics domain;balanced class labels;state art ensemble;new classifier based;art ensemble learning;performance multi domain;learning algorithms;multidomain methods improving;classifier based;video dynamics multi;learning applied;learning approaches", "pdf_keywords": ""}, "6f902b8128b218563b276c1ebff46ef668dcb185": {"ta_keywords": "vehicular crowd sensing;vehicular crowd;agents engage arbitrary;arbitrary collusions;engage arbitrary collusions;incentivize drivers agents;non colluding agents;problem vehicular crowd;colluding agents;agents collect data;crowd sensing;mechanism incentivize agents;arbitrary collusions paper;animals crowded environments;drivers agents collect;crowd sensing design;crowded environments;colluding agents past;crowded environments based;collusions;incentivize agents;incentivize agents engage;collusions paper propose;animals crowded;behavior animals crowded;drivers agents;mechanisms incentivize drivers;propose incentive mechanism;agents collect;agents engage", "pdf_keywords": "vehicular crowd sensing;fair incentive mechanism;incentivize drivers agents;incentive mechanisms;mechanism unfair incentive;unfair incentive mechanism;challenge collusion agents;incentive mechanism;vehicular crowd;agents incentivized cheat;incentivize agents;mechanism incentivize agents;incentive mechanism unfair;competition agents incentivized;collusion agents;incentivize agents collect;challenge collusion;addressing challenge collusion;problem vehicular crowd;crowd sensing;propose crowdsourcer;compete propose crowdsourcer;collusion agents mechanism;agents incentivized;propose crowdsourcer based;crowdsourcer;unfair incentive;mechanisms incentivize drivers;agents collect data;collusion"}, "7650d705b85dc399112a5b6a79e9c6f81c7c6146": {"ta_keywords": "large annotated corpora;annotated corpora outperform;answers large annotated;large annotated;annotated corpora;annotated corpora limited;specific annotated corpora;deep learning;extracting answers large;learning models task;corpora outperform;deep learning models;annotated;task extracting answers;corpora outperform state;models task extracting;questions base documents;extracting answers;learning models;specific annotated;corpora;questions base;shown deep learning;neural network cloze;neural;learning;models task;model labeled examples;corpora limited;labeled examples", "pdf_keywords": "supervised answer answering;model answer answering;answering question generation;answer answering train;task answer answering;answering train deep;answer answering;answer answering question;question generation dataset;semi supervised answer;information domain adaptation;semi supervised deep;semi supervised qa;answering question;supervised answer;question generation;domain adaptation;answering train;answering;supervised deep;domain task improve;domain adaptation transfer;annotated corpora better;approaches semi supervised;pretraining language modeling;question answer evidence;train deep neural;supervised qa;question generation paper;semi supervised"}, "58834a447c749758e7f57498c6dd88a281af41a0": {"ta_keywords": "training constituency parsers;parsers exploration;parsers dynamic oracle;dynamic oracle supervision;parser parsers dynamic;parsers dynamic;constituency parsers exploration;parser parsers;parsers;constituency parsers languages;parsers languages;oracle supervision;parser agnostic alternative;constituency parsers;parser;given parser parsers;parser agnostic;parsers exploration custom;oracle supervision present;given parser;defined given parser;parsers languages method;oracle likelihood training;method parser agnostic;petri nets;gradient method parser;dynamic oracles provide;supervision training constituency;dynamic oracles;method parser", "pdf_keywords": "models constituency parsing;constituency parsing transition;dynamic oracle learning;transition based parsing;training dynamic oracle;constituency parsers;parsers state art;constituency parsing;oracle learning;parsing transition based;constituency parsers state;problem constituency parsers;oracle training;parsing transition;oracle learning variants;variants oracle training;parsers;oracle training focus;novel dynamic oracles;based parsing;parsing;dynamic oracles;sentence exploration;sampled sentence exploration;parsers state;parse;learning variants oracle;use dynamic oracles;oracle based exploration;dynamic oracles oracle"}, "1fa32503bce4f01ab2ccb65dedd374310c488fe8": {"ta_keywords": "partial compliance employers;employers partial conformity;compliance employers;concept partial compliance;compliance outcomes;compliance consequent strategic;findings partial compliance;partial compliance;compliance discuss implications;partial compliance discuss;does partial compliance;compliance employers result;compliance outcomes gap;compliance;partial compliance consequent;compliance discuss;discuss implications auditors;regulatory frameworks;design regulatory frameworks;implications auditors insights;implications auditors;fair employers match;compliance consequent;impact conclusions auditor;fair employers;progress compliance outcomes;employers match global;concerning design regulatory;auditors insights concerning;design regulatory", "pdf_keywords": "policies diversity workforces;fairness algorithmic;fairness algorithmic based;approaches understanding fairness;understanding fairness algorithmic;parity policies diversity;policies diversity;understanding fairness;promote fairness remain;promote fairness;allocation decisions isolation;thereof promote fairness;diversity workforces;fairness remain sight;diversity workforces individual;affirmative action;employers partial compliance;fairness remain;workforces individual employers;affirmative action despite;bias algorithms;performance bias algorithms;individual employers;conflicts diversity integration;arguments affirmative action;diversity integration;bias;implications partial compliance;parity policies;performance bias"}, "6ea353ada2b89763f58d8068a74b2e6def526948": {"ta_keywords": "texts selected drugbank;classification pharmacological names;classification pharmacological;annotated corpus consisting;annotation process;manually annotated corpus;annotated corpus;ognition classification pharmacological;drugbank database;pharmacological names paper;manually annotated;drugbank database 233;annotation;pharmacological substances detection;selected drugbank database;pharmacological names;inter annotator agreement;abstracts natural language;biomedical texts;biomedical texts presented;study inter annotator;medline abstracts natural;annotation guide;medline abstracts;annotator agreement;created manually annotated;ddis biomedical texts;annotation guide lines;annotator agreement anno;rec ognition pharmacological", "pdf_keywords": "corpus annotated pharmacological;pharmacologically annotated corpora;corpus pharmacologically annotated;annotated corpora drug;pharmacological text processing;annotated pharmacological substances;pharmacological text;corpora drug names;drug names interactions;annotated pharmacological;difdi corpus annotated;pharmacologically annotated;information extraction techniques;information extraction;corpus pharmacologically;annotated resource pharmacological;corpus annotated;advances information extraction;resource pharmacological text;present corpus pharmacologically;drug names;corpus evaluation information;ddis biomedical texts;pharmacological substances detection;difdi corpus;texts resulting corpus;annotated corpora;biomedical texts resulting;evaluation information extraction;called difdi corpus"}, "c507ad8b7bec5d29da7cf0ee92e2bf4361a5c92f": {"ta_keywords": "deep quantization neural;deep quantization;quantization neural networks;automating deep quantization;quantization neural;shown deep quantization;neural networks quantum;quantization large;quantization;quantization levels;discovering quantization levels;quantization levels end;discovering quantization;networks dnns require;quantization focus;networks dnns;deep networks;neural networks dnns;problem quantization;solution quantization large;deep neural networks;deep networks alexnet;deep reinforcement learning;quantization large variety;deep neural;problem quantization focus;variety deep networks;solution quantization;networks quantum;network architecture training", "pdf_keywords": ""}, "2899eb53cddf050e3a34f07bbc0bc0ee7907d5d0": {"ta_keywords": "partially annotated sentences;adding annotated sentences;annotated sentences training;additions word segmentation;speech tagging;tagging problem japanese;annotated sentences;occurrences partially annotated;annotated sentences paper;speech tagging problem;word segmentation;sentences training corpus;problem speech tagging;partially annotated;adding annotated;natural language processing;second adding annotated;training corpus;language processing;training corpus present;sentences training;word segmentation problem;constructing sentences;method constructing sentences;model natural language;sentences real occurrences;tagging;natural language;corpus;corpus present", "pdf_keywords": ""}, "626f8a50a7bd24d869f25bddb6fbaa59b090268c": {"ta_keywords": "pattern recognition methods;pattern recognition;recognition device pattern;pattern recognition device;pattern recognition apparatus;systems combined detection;device pattern recognition;improved pattern recognition;recognition performance comprising;technique pattern recognition;recognition methods;recognition methods based;recognition performance;recognition device;detection performance;tof technique pattern;recognition apparatus provided;recognition apparatus;combined detection performance;recognition;combined detection;performance comprising discriminative;detection performance improved;detection;improve recognition performance;multiple systems;performance improved pattern;tof technique;systems combination;flight tof technique", "pdf_keywords": ""}, "3681456f29398e42cc2baafb0b72d166070a3cf1": {"ta_keywords": "motion pedestrian method;quadratic dynamics games;motion pedestrian;direction motion pedestrian;pedestrian method;motion pedestrian vicinity;observation motion pedestrian;pedestrian vicinity moving;linear quadratic dynamics;pedestrian method based;sequential algorithms linear;convergence nash;pedestrian;quadratic dynamics;sequential policy;dynamics games;convergence nash equilibrium;sequential algorithms;sublinear convergence nash;quadratic polynomial time;free sequential algorithms;dynamics games examine;dynamics moving body;pedestrian vicinity;dynamics moving;motion;projection free sequential;algorithms linear quadratic;natural gradient descent;sequential policy updates", "pdf_keywords": "dynamic game theoretic;sum dynamic game;dynamic policy optimization;convergence optimal stabilizing;quadratic games gradient;optimal stabilizing;game expressed weighted;policy optimization provably;games gradient policy;linear quadratic games;game dynamic prove;converges nash equilibria;dynamic game expressed;optimal stabilizing feedback;dynamic game;dynamic game dynamic;winner dynamic game;stabilizability detectability convergence;natural gradient policy;provably converges nash;converges nash;policy optimization;gradient policy;game theoretic approach;game dynamic;gradient policy global;embedded dynamic policy;games gradient;new dynamic game;follower algorithm global"}, "a711e02f85fa52c15df0a830a8ba88df2c3928ec": {"ta_keywords": "predicting outcome robust;outcome robust example;outcome robust;robust example;method predicting outcome;predicting outcome;effective method predicting;method predicting;robust;predicting;example;propose simple effective;simple effective;simple effective method;outcome;effective method;method;propose simple;effective;simple;paper propose simple;propose;paper propose;paper", "pdf_keywords": ""}, "4c67c129dab9805ab248407b77a6d542c2e40d41": {"ta_keywords": "reconstructing rank matrix;reconstructing rank;problem reconstructing rank;rank matrix revealed;rank matrix extensive;original rank matrix;adversarial;adversarial scenario;methods adversarial;known entries;recover original rank;optimal set revealed;adversarial scenario consider;matrix revealed subset;methods adversarial scenario;art methods adversarial;revealed entries;revealed entries given;alternating minimization extreme;alternating minimization;matrix revealed;rank matrix;revealed subset entries;entries revealed;revealed entries corrupted;minimization extreme;entries revealed entries;set revealed entries;reconstructing;known entries corrupted", "pdf_keywords": "robust rank matrix;robust rank;algorithm robust rank;rank matrix completion;study robust rank;matrix completion problem;matrix completion;matrix completion algorithm;matrix completion regime;optimal applied adversarial;adversarial strategies return;different adversarial strategies;adversarial model;adversarial;adversarial strategies;solve rank matrix;rank matrix;different adversarial;adversarial model varying;adversary task;regularization;algorithm robust;solve rank;assign adversary task;adversary task \ufb01xed;new algorithm robust;regularization term propose;answers assign adversary;applied adversarial model;applied adversarial"}, "840fabc2a7773e1bd771f152f76210b2ea5845b9": {"ta_keywords": "stochastic decoding strategies;propose stochastic decoding;stochastic decoding;beam search stochastic;impostochastic beam search;search stochastic;stochastic beam search;stochastic decoding algorithm;search poisson sampling;paper stochastic decoding;decoding strategy sequence;search stochastic process;decoding strategies;sequence generation tasks;poisson sampling impostochastic;sampling impostochastic;generation tasks nlp;sampling impostochastic beam;decoding strategies used;sequence models beam;decoding strategy;search poisson;poisson search;poisson search poisson;sequence generation;models beam search;tasks nlp propose;impostochastic beam;strategy sequence generation;tasks nlp", "pdf_keywords": "beam search sampling;search sampling;search sampling withoutreplacement;stochastic beam search;sampling withoutreplacement strategy;sampling withoutreplacement;sampling;sequence models beam;sampling design;sampling design problem;new sampling design;propose new sampling;poisson stochastic beam;introduce stochastic beam;stochastic beam;predicting inclusion probability;new sampling;models beam search;poisson stochastic;beam search algorithm;search algorithm sequence;introduce stochastic;sequence generation;algorithm sequence generation;sequence models;sequence generation tasks;conditional poisson stochastic;beam search;stochastic;performance introduce stochastic"}, "509b42fc150a057a64c4608f64e779ef04fdff47": {"ta_keywords": "named entity recognition;entity recognition nlp;tweets annotated named;entity recognition;english tweets annotated;annotated named entities;tweets annotated;entities natural language;nlp tasks;entity recognition paper;nlp tasks similar;recognition nlp tasks;recognition nlp;predictions text data;english tweets;natural language processing;annotated named;temporal information documents;named entities;named entities natural;predictions text;nlp;text empirically;set english tweets;named entity;improvement named entity;tweets;text data evolves;use temporally diverse;results competition recognition", "pdf_keywords": ""}, "11c6d0851152b6bec34726be40d90bea8d8a90f0": {"ta_keywords": "model crowdsourced annotations;crowdsourced annotations;individual confusion crowdsourcing;model crowdsourced;confusion crowdsourcing provides;crowdsourced;crowdsourcing;crowdsourcing provides;quality model crowdsourced;confusion crowdsourcing;crowdsourcing model;crowdsourcing provides practical;noise annotated annotation;end crowdsourcing model;end crowdsourcing;end end crowdsourcing;annotation noise;annotation noise common;crowdsourcing model types;noise annotated;annotated annotation data;annotation data extensive;shared annotators;crowds;annotated annotation;annotator expertise;decompose annotation noise;difficulty annotator expertise;quality annotators;model dynamics crowds", "pdf_keywords": "learning crowdsourced labels;crowdsourced annotator labels;learning crowdsourced annotator;train crowdsourced labels;crowdsourced labels;crowdsourced annotator;crowdsourced labels challenging;crowdsourced labels directly;crowdsourced labels easy;learning crowdsourced;rare crowdsourced labels;noisy annotations;confusions learning crowdsourced;classi\ufb01er noisy annotations;train crowdsourced;framework train crowdsourced;annotation noise;framework learning crowdsourced;data rare crowdsourced;crowdsourced;noisy annotations types;annotation noise common;crowdsourcing;labels directly annotators;rare crowdsourced;annotators;directly annotators;massive labeled data;decompose annotation noise;labels challenging task"}, "6c520d983923dbe1e437c01086424fdcdd8f430a": {"ta_keywords": "parametric speech synthesis;synthetic speech quality;speech synthesis quality;improvements synthetic speech;speech synthesis;synthesis synthetic speech;concatenated speech synthesis;statistical parametric speech;speech synthesis synthetic;speech synthesis approaches;concatenative speech synthesis;speech synthesis offers;smoothing effect speech;speech parameter trajectories;synthetic speech;parametric speech;generated speech parameter;speech quality proposed;effect speech parameter;speech parameter;speech quality;quality natural speech;synthesizers based statistical;speech proposed segment;speech quality good;generated speech;concatenated speech;good concatenated speech;concatenative speech;observed generated speech", "pdf_keywords": ""}, "5f46d8e18138fe572b6fae897475a2ad645a3e1a": {"ta_keywords": "voice search data;voice search;generalize voice search;models automatic speech;automatic speech;speech recognition results;speech recognition;automatic speech recognition;audio transcript data;asr based deep;model hmm;transcript data youtube;audio transcript;model hmm framework;generalize voice;lipids hidden markov;paired audio transcript;ability generalize voice;deep neural networks;hmm framework proposed;hidden markov model;voice;hidden markov;trained paired audio;models asr based;markov model hmm;data youtube;deep neural;hmm framework;neural networks dinns", "pdf_keywords": "crowdsourced speech models;speech data crowdsourced;models automatic speechrecognition;crowdsourced speech;crowdsourced crowdsourced speech;automatic speechrecognition;external language models;automatic speechrecognition asr;speech models;models speech recognition;models speech;prediction speech data;computational models speech;automatic speech;speech data;automatic speech recognition;prediction speech;speech models paper;speechrecognition asr;language models combined;data crowdsourced;language models end;language model integration;language models;method automatic speech;speech recognition;method prediction speech;learned language models;speechrecognition;data crowdsourced crowdsourced"}, "2ce3428ba8777c723b9b12e9f8eaeb2c87a5a793": {"ta_keywords": "predicting outcome competition;rationale individually learns;supervision boosting performance;rationale level predictive;supervision boosting;direct supervision boosting;model predictive capability;predictive capability;predictive;model predictive ability;model predictive;task based rationale;level predictive;predictive model predicting;pipeline approaches reinforcement;model predicting outcome;learns assign high;predictive ability improved;reinforcement learning evaluation;boosting performance rationale;performance rationale level;traditional model predictive;level predictive ability;predictive ability;learning evaluation;predicting outcome;predictive model;predicting;model predicting;learns", "pdf_keywords": "bert reasoning tasks;reasoning tasks rationale;sentence rationale annotations;standard bert reasoning;reasoning tasks;tasks rationale supervision;bert reasoning;rationale reasoning yield;suggest rationale reasoning;rationale annotations;rationale annotations costly;rationale supervision;explainability recently emerged;rationale reasoning;reasoning meaning sentence;reasoning meaning;rationale supervision slightly;reasoning meaning results;supervised gold rationales;task rationale;target task rationale;sentence models;sentence rationale;task rationale objective;tasks rationale;single sentence models;sentence context reasoning;explainability recently;compared standard bert;knowledge selected rationales"}, "cd1d915604826e5fb0ba2bbcdf8479a9b90fb289": {"ta_keywords": "wireless relay placement;relay placement random;sequential wireless relay;relay placement;optimal sequential wireless;placement random lattice;wireless relay;sequential wireless;random lattice path;relay;random lattice;placement random;lattice path;lattice path investigated;optimal sequential;lattice;wireless;random;optimal;placement;sequential;path;path investigated;investigated", "pdf_keywords": "optimal place relay;placement set relays;place wireless relay;relay simulations distance;place relay;place relay simulations;decide place wireless;threshold policies placement;optimal placement set;wireless relay node;relay simulations;set relays;converge optimal placement;wireless relay;finding optimal placement;crossing optimal place;distance based heuristic;relay node;optimal placement;relay;set relays low;optimal place;relays;policies placement sets;relay node problem;wireless networks;relays low;optimal policies threshold;al optimal placement;optimal provided threshold"}, "a425a11b9b249cb768d0f54d4a32f4f1d007e279": {"ta_keywords": "traditional batch learning;comparable batch learners;online lear ners;online learning algorithm;pass online learning;practice online lear;batch learning propose;batch learners;online lear;algorithm online learning;pass online learners;batch learning;online learning methods;online learning compare;learning online learning;batch learning methods;online learning;learners traditional batch;batch learners paper;learning online;online learners;online learning online;svm nlp;linear svm nlp;svm nlp tasks;learning based single;lear ners frequently;online learning based;online learners traditional;linear svm", "pdf_keywords": ""}, "2d71fb62c71e49479c1b6ce832ee1bb88df20556": {"ta_keywords": "description logics computing;reasoning structure knowledge;description logics;reasoning structure;structure knowledge representation;knowledge representation reasoning;logics computing common;formalism reasoning structure;structure knowledge;operation description logics;subsumer pair descriptions;knowledge representation;computation common subsumer;logics computing;computing common subsumer;formalism reasoning;new formalism reasoning;common subsumer attribute;structural subsumption;representation reasoning;subsumer known problem;tractability computing smallest;common subsumer;subsumer attribute chain;representation reasoning present;common subsumer known;tractability computing;reasoning present;case structural subsumption;subsumer attribute", "pdf_keywords": ""}, "6fa85c46ea68c754ef903edc70058ba525f1fc4d": {"ta_keywords": "representation learning skill;learning representation;like intelligent agent;learning representation learning;intelligent agent;skill learning human;learning human;learning human like;representation learning;study learning representation;learning skill learning;representation;skill learning;learning skill;agent;human like intelligent;learning;like intelligent;intelligent;human like;skill;human;study learning;paper study learning;like;study;paper;paper study", "pdf_keywords": ""}, "0c89b1ec80de46222ed7efc6261c03e52a1e2c54": {"ta_keywords": "wordnet;phrases polysemous words;wordnet oxford urban;words phrases polysemous;wordnet oxford;unfamiliar words phrases;urban dictionaries demonstrate;urban dictionaries;oxford urban dictionaries;stuck unfamiliar words;polysemous words novel;polysemous words;phrases polysemous;words phrases;slang emerging entities;dictionaries demonstrate effectiveness;unfamiliar words;datasets including wordnet;global contexts reading;internet slang emerging;contexts reading text;dictionaries;contexts reading;including wordnet;including wordnet oxford;global contexts;slang emerging;contexts;words;event machines", "pdf_keywords": ""}, "b2b9b0d7afd85c5d708b79a61d9a000c6c906d8c": {"ta_keywords": "term extraction;corpora syntactic relations;coordinate term extraction;parsed text corpus;text corpus;term extraction small;relations words parsed;text corpus represented;sized corpora syntactic;corpora syntactic;syntactic relations words;corpus;nodes represent words;word similarity measure;entity coordinate term;named entity coordinate;corpora;relationships parsimony context;corpus represented labelled;words weighted directed;words parsed text;small sized corpora;syntactic relations;relations words;sized corpora;corpus represented;represent words weighted;named entity;word similarity;relationships parsimony", "pdf_keywords": ""}, "3042bc348d6cc7959cd574756f720e5afad236de": {"ta_keywords": "anisotropic paperboard;model anisotropic paperboard;anisotropic paperboard exhibits;drawing paperboard;deep drawing paperboard;paperboard process;paperboard deep drawing;paperboard;drawing paperboard trays;type paperboard;paper describes mechanical;study type paperboard;type paperboard process;paperboard trays;paperboard exhibits variety;paperboard process parameters;paperboard exhibits;paperboard deep;paperboard trays proper;ferromagnetic films;ferromagnetic films grown;influence factors paperboard;factors paperboard deep;films ferromagnetic;mechanical properties;wrinkling cracking paper;factors paperboard;mechanical model anisotropic;mechanical properties including;films ferromagnetic films", "pdf_keywords": ""}, "39c5cfc0ff6660a17364cb4af1eb071d6efa463d": {"ta_keywords": "ordinal measurements robust;tasks noise ordinal;noise ordinal measurements;comparative ordinal measurements;ordinal measurements sufficiently;empirical evidence ordinal;goal ordinal measurements;robust cardinal measurements;evidence ordinal tasks;quantity ordinal measurements;noise ordinal;measurements robust cardinal;ordinal tasks faster;ordinal measurements typically;ordinal measurements consider;ordinal measurements;estimation eliciting judgements;scoring cardinal comparative;ordinal tasks;ordinal approach results;measurement scheme better;low ordinal approach;ordinal approach;evidence ordinal;achieve goal ordinal;cardinal measurements presence;comparative ordinal;cardinal measurements;cardinal comparative ordinal;measurements provide empirical", "pdf_keywords": "tasks noise ordinal;ordinal task results;ordinal task;ordinal evaluation analyzing;improve ordinal evaluation;noise ordinal measurements;ordinal approaches provide;ordinal approaches;noise ordinal;ordinal measurements suf\ufb01ciently;ordinal evaluation;ordinal approach results;ordinal approach;converted ordinal task;risk subject ordinal;estimating noise data;cardinal ordinal approaches;human workers accurate;low ordinal approach;improve ordinal;ordinal measurements;particular tasks noise;tasks noise;estimating noise;ordinal;workers accurate;compare particular tasks;noise data collected;suf\ufb01ciently low ordinal;subject ordinal"}, "67b29c3fe6f110125a8892e8ed128d20b23957ea": {"ta_keywords": "verification quantum algorithm;verification quantum;quantum algorithm proof;principle verification quantum;quantum algorithm;algorithm proof correctness;proof correctness based;correctness based proof;end linking accuracy;correctness proof principle;quantum;linking accuracy;algorithm proof;correctness proof;proof principle;resource schemes;proof correctness;resource scarce scenarios;proof correctness proof;approach proof correctness;resource schemes paper;based proof;based proof proof;resource scarce;resources resource schemes;correctness based;new approach proof;approach resource scarce;principle verification;proof principle verification", "pdf_keywords": "crosslingual entity linking;lingual entity linking;entity generation disambiguation;entity linking;disambiguation model;propose disambiguation model;entity linking low;disambiguation model combines;link texts entities;generation disambiguation;approach crosslingual entity;cross lingual entity;crosslingual entity;entity linking xel;focus crosslingual entity;disambiguation;lingual entity;generation disambiguation make;propose disambiguation;models cross lingual;linking low resource;entity generation;texts entities knowledge;texts entities;disambiguation make best;linking;settings propose disambiguation;entities knowledge base;link texts;methodologies entity generation"}, "2a64da1ed300e49f2d665312146c8bb2f66920b7": {"ta_keywords": "batch online optimization;machine translation optimization;translation optimization parameters;translation optimization;online optimization;minimizing losses statistical;discriminative models minimum;losses statistical machine;large scale optimization;statistical machine translation;online optimization article;minimizing losses;maximize translation accuracy;methods minimizing losses;minimization maximum likelihood;parameters maximize translation;machine translation;optimization smt;error rate training;margin risk minimization;error minimization maximum;discriminative models;error minimization;control afm optimization;direct error minimization;optimization nonlinear models;minimization ranking;maximize translation;automatic motor control;dependent optimization search", "pdf_keywords": ""}, "d530a007ae0493ef6a8167c25bd007104623c504": {"ta_keywords": "decompiled code renaming;decompiled identifier renaming;propose decompiled identifier;decompiled identifier;code renaming;identifier renaming engine;variable names known;code propose decompiled;tools examining binaries;identifier renaming;code renaming use;meaningful variable names;decompiled code;examining binaries corresponding;models decompiled code;predict variable names;corresponding source code;examining binaries;generating corpora;decompiler common tools;variable names;technique generating corpora;generating corpora suitable;binaries generated coal;understandability results corpus;binaries corresponding source;reconstruct semantically meaningful;binaries generated;propose decompiled;source code", "pdf_keywords": "statistical machine translation;machine translation;machine translation model;generating corpora;approach generating corpora;generating corpora suitable;propose machine translation;original variable names;machine translation smt;generate corpus;disassembler allows programmers;tokenized code structural;use generate corpus;decompilation statistical models;renaming engine;renaming;corpora suitable training;renaming engine dire;variable names;encoder vocabulary improving;generate corpus 164;recurrent neural networks;corpora;encoder vocabulary;propose decompiled identi;code structural information;recovery uses lexical;er renaming engine;lexical structural information;source code"}, "bc1bf0a21d7838ec167e77c76163afc1f5f76c3d": {"ta_keywords": "multi channel electroencephalogram;channel electroencephalogram eeg;channel electroencephalogram;electroencephalogram eeg;electroencephalogram eeg signal;electroencephalogram;signal simple electroencephalogram;simple electroencephalogram;electroencephalogram eegg;simple electroencephalogram eegg;eeg signal;electroencephalogram eegg used;eeg signal simple;covariance matrices frequency;noise removal single;noise removal;background noise removal;eeg;estimate covariance matrices;estimate covariance;covariance matrices;related potentials recorded;potentials recorded multi;method estimate covariance;matrices frequency bin;potentials recorded;background noise;covariance;method background noise;event related potentials", "pdf_keywords": ""}, "4c9f20ce99f9b93527fd76ec04a44fcef9082005": {"ta_keywords": "fair recommendation algorithm;fairness aware recommenders;preferences fairness recommendations;fair recommendations improved;fairness recommendations jointly;recommendations existing fairness;fairness recommendations;fairrec dynamically;fair fair recommendations;fairrec dynamically maintain;user preferences fairness;based framework fairrec;existing fairness aware;framework fairrec dynamically;fairness accuracy;recursion combines fairness;preferences fairness;fairness aware;framework fairrec;accuracy fairness irs;combines fairness accuracy;fair recommendations;fairness irs;fairness accuracy resolve;existing fairness;recommendation user preferences;accuracy fairness;recommendation algorithm directly;design fair recursion;recommendation algorithm", "pdf_keywords": "fairness recommender systems;fairness aware recommendation;fairness recommender;acceptance fairness recommender;recommendation acceptance fairness;recommendation framework reinforcement;recommendation systems;recommendation systems widely;microlending loan recommender;loan recommender systems;aware recommendation;reinforcement learning irs;recommender systems ensure;recommend items users;loan recommender;aware recommendation framework;recommendation framework;recommender systems recommendation;recommender systems;recommendation acceptance;recommend items;systems recommendation systems;recommender aims;accuracy fairness irs;novel fairness aware;recommender;fairrec dynamically;fairness irs;collaborative recommender;fairness aware"}, "cb153d8469ac466606032ea457b934bc61ae86ae": {"ta_keywords": "exploiting emotions news;detecting fake news;fake news detection;dual emotion features;fake news emotion;fake news detectors;news emotion;news detectors enhancement;emotion features;news detection;news detectors;emotions news contents;emotion distinctive fake;news emotion plays;emotions news;emotion features represent;emotion appears fake;dual emotion distinctive;emotional features effectively;leveraging emotional signals;emotional features;dual emotion appears;existing fake news;represent dual emotion;publisher emotion;detecting fake;emotion dual;publisher emotion social;role detecting fake;related emotional features", "pdf_keywords": ""}, "029fa34b291c3f60b8a00cdf386e6048d45c394d": {"ta_keywords": "mixed membership clustering;spectral clustering graph;node clustering methods;node clustering;node clustering method;spectral clustering;membership clustering tasks;alternative spectral clustering;clustering methods spectral;spectral clustering methods;clustering graph;tasks node clustering;clustering graph based;methods spectral clustering;membership clustering;based node clustering;clustering tasks node;clustering tasks;clustering;membership clustering present;clustering method allows;clustering methods;clustering method;clustering present;speckle presence multiple;area speckle presence;area speckle;clustering present new;multiple speckles approach;presence multiple speckles", "pdf_keywords": ""}, "04b876e95ac3e4754c8f0c8a9355e7acc3dc70b9": {"ta_keywords": "annotated sentences training;segmentation speech tagging;sentences training corpus;dictionary training corpus;training corpus;adding annotated sentences;speech tagging;japanese morphological analysis;annotated sentences;observed word segmentation;word segmentation;dictionaries corpora methods;partially annotated sentences;addition japanese morphological;speech tagging adding;nlp tool;dictionary training;reasonable nlp tool;task word segmentation;constructing dictionary training;japanese morphological;word segmentation speech;word segmentation accuracy;corpus;dictionaries corpora;corpora methods;annotation;study dictionaries corpora;nlp tool providers;corpora methods language", "pdf_keywords": ""}, "7393d2618c7478d937112865458862e8d5f10475": {"ta_keywords": "cross domain reasoning;sequence models domains;domain reasoning challenging;domain reasoning;models cross domain;pretrained encoderdecoder models;encoderdecoder models cross;models domains;encoderdecoder models;models domains letter;pretrained encoderdecoder;based cross domains;pretrained sequence sequence;sequence sequence models;sequence models perform;enables pretrained sequence;sequence models;pretrained sequence;experiments pretrained encoderdecoder;domains study prompttemplate;cross domains;domains study;cross domain cc;models perform crossdomain;encoderdecoder;design cross domain;perform crossdomain reasoning;models cross;crossdomain reasoning case;cross domain", "pdf_keywords": "commonsense domain reasoning;commonsense reasoning approaches;domain reasoning health;reasoning commonsense;reasoning commonsense health;commonsense reasoning;case reasoning commonsense;reasoning health domain;domain reasoning addressed;commonsense knowledge bases;domain reasoning \ufb01lling;reasoning addressed knowledge;address commonsense reasoning;domain reasoning model;cross domain reasoning;effectively reason domains;domain reasoning;domain reasoning paper;health domain reasoning;building commonsense knowledge;commonsense knowledge;reasoning approaches ranging;reasoning model combines;rule based reasoning;commonsense health domains;reasoning model;reasoning approaches;reasoning \ufb01lling;reasoning \ufb01lling templates;commonsense domain"}, "7137a842d496a1a5581db31ad946fa0c0827e663": {"ta_keywords": "learning nonlocal models;nonlocal models noisy;learning nonlocal;nonlocal models;method learning nonlocal;models noisy data;models noisy;noisy data;nonlocal;models;method learning;new method learning;noisy;learning;data;method;paper;propose new method;new method;new;paper propose;paper propose new;propose new;propose", "pdf_keywords": ""}, "f735f5f55cbc5a9d372ea1cd9b4e81d35f043a00": {"ta_keywords": "thresholding algorithm statistically;singular value thresholding;algorithms achieve minimax;broader stochastically transitive;stochastically transitive model;transitive model stochastic;value thresholding;minimax rate class;achieve minimax rate;thresholding;value thresholding algorithm;minimax rate;minimax rate interesting;stochastic transitivity;model pairwise comparisons;stochastic transitivity provide;thresholding algorithm;models broader stochastically;stochastically transitive class;pairwise comparisons probabilities;stochastically transitive;broader stochastically;consider stochastically transitive;classes stochastically transitive;achieve minimax;minimax;flexible model pairwise;pairwise comparisons;sub classes stochastically;does achieve minimax", "pdf_keywords": "estimating pairwise preference;pairwise preference probabilities;pairwise preference;preference probabilities;preference probabilities set;choice limited data;estimating pairwise;thresholding estimators probability;thresholding estimators;probability models pairwise;models pairwise comparison;optimal estimators various;estimators broader matrix;modeling choice limited;soft thresholding estimators;optimal estimators;models pairwise;problem estimating pairwise;pairwise comparison data;singular value thresholding;polynomial time estimators;implementable outperform estimators;based noisy sorting;modeling choice;estimators probability models;optimal;value thresholding multistage;value thresholding;high probability bound;tight characterization rate"}, "380278716f4d78ad9dcc3ece9e12b235ca1d1569": {"ta_keywords": "logic called tensorlog;reasoning deep learning;logical reasoning deep;tensorlog;tensorlog classes logical;called tensorlog;tensorlog classes;called tensorlog classes;deep learning infrastructure;tensorflow;reasoning deep;probabilistic logical;infrastructure tensorflow;infrastructure tensorflow theano;deep learning;probabilistic logical reasoning;neural network infrastructure;logical queries compiled;logical queries;probabilistic order logic;tensorflow theano;network infrastructure tensorflow;knowledge base;thousands knowledge base;predicting outcome fly;classes logical queries;ttensorlog;logic;neural;neural network", "pdf_keywords": "learning probabilistic logic;probabilistic logic programs;probabilistic similarity logic;probabilistic logic probabilistic;deductive knowledge graphs;logic probabilistic reasoning;probabilistic logic;logic probabilistic;logic programs relational;intractible probabilistic logic;probabilistic deductive knowledge;knowledge graphs ptree;reasoning deep learning;probabilistic reasoning machine;probabilistic databases tractable;probabilistic logical reasoning;semantics making inference;probabilistic logical;probabilistic databases increasingly;logical reasoning deep;reasoning machine learning;probabilistic databases;logic called tensorlog;probabilistic reasoning;knowledge graphs;propagation probabilistic databases;logic programs;reasoning machine;expressive probabilistic similarity;relational learning"}, "8a880680b28dee5642ac88431b3ae1085b911f96": {"ta_keywords": "neural machine translation;machine translation models;consistently improves translation;machine translation;translation models;sentence neural machine;translation models little;improves translation quality;sentence neural;penalize translations;penalize translations different;improves translation;translation quality;salt neural models;sentences consistently improves;sentences consistently;sentences sentence neural;translation quality multiple;input sentences consistently;sentence length;frequencies penalize translations;based sentence length;low resource languages;length number sentences;increase language pairs;sentence length number;translations;salt neural;increase language;sentence length punctuation", "pdf_keywords": ""}, "4f7b108830de2e7964b6e1a89bf1c2da60140a34": {"ta_keywords": "latent representation learning;representation learning data;effectively variational autoencoder;representation learning;representation learning framework;variational autoencoder;heuristic latent representation;effective representation learning;variational autoencoder vae;representation learning based;distribution modeling trained;trained effectively variational;latent representation;autoencoder vae powerful;learning data distribution;autoencoder vae;autoencoder;model effective representation;powerful language model;particles statistically unbiased;goals representation learning;learning data;effectively variational;modeling trained;effective representation;language model effective;modeling trained effectively;heuristic latent;language model;learning", "pdf_keywords": "language modeling reconstruction;learning language modeling;representation learning data;representation learning language;approach representation learning;representation learning;language modeling text;language modeling;modeling text;modeling text reconstruction;language modeling demonstrate;goals representation learning;learning data;method language modeling;learning data distribution;text reconstruction;learning language;novel approach representation;learned latent space;terms language modeling;text reconstruction quality;latent space practice;learning;reconstruction quality learned;modeling existing;modeling;data distribution modeling;model modifying training;methods trained evidence;representation"}, "14119210e5f9e0d962e329c833557dfb5524c4bd": {"ta_keywords": "cathode electrolyte scaffolds;integrated cathode electrolyte;cathode electrolyte;cathode electrolyte structure;resistance cathode electrolyte;cathode electrolyte supporting;solid polymer electrolyte;polymer electrolyte;electrolyte scaffolds constructing;electrolyte structure ices;lithium oxygen batteries;electrolyte scaffolds;polymer electrolyte significantly;electrolyte;electrolyte supporting sio2;electrolyte structure;connection cathode electrolyte;electrolyte supporting;liquid electrolytes;organic liquid electrolytes;oxygen batteries;electrolyte significantly enhanced;lithium oxygen;state lithium oxygen;oxygen batteries lobalbs;electrolyte significantly;electrolytes paper ionic;liquid electrolytes paper;oxygen batteries sslobs;electrolytes", "pdf_keywords": ""}, "de8ded0d66f3227d99751a89fdd5f4b438d6e8ee": {"ta_keywords": "robust speech recognition;equality robust speech;jarzynski equality robust;robust speech;application jarzynski equality;speech recognition;jarzynski equality;application jarzynski;equality robust;jarzynski;presents application jarzynski;robust;recognition;speech;equality;application;paper presents application;paper;paper presents;presents application;presents", "pdf_keywords": ""}, "e74d7523d7d96ab65f05f059284f9d0a994bb074": {"ta_keywords": "annotated treebank ted;languages syntactic parsing;annotated treebank;annotated parse trees;annotated parse;manually annotated treebank;syntactic parsing;syntactic parsing natural;treebank ted talks;treebank;manually annotated parse;parsing natural language;treebank ted;syntax speechrelated;syntax speechrelated applications;parsing;language processing;corpus;natural language processing;parse trees;machine translation language;corpus analysis;useful machine translation;machine translation;languages syntactic;parsing natural;sentence segmentation;parse;modeling sentence segmentation;corpus analysis various", "pdf_keywords": ""}, "3efee0095cb578659dfaaf0d87a616f133ecf85c": {"ta_keywords": "acoustic recognition chime;acoustic recognition;new acoustic recognition;recognition chime;recognition chime dinner;services called chime;robust vector extraction;vector extraction house;vector extraction;web services;acoustical membrane used;web services called;data augmentation robust;web based web;data augmentation;acoustical membrane;augmentation robust vector;called chime;chime paper;chime;explore data augmentation;chime dinner;called chime paper;based web services;presents new acoustic;overweighted acoustical membrane;chime dinner party;augmentation robust;new generation web;web based", "pdf_keywords": ""}, "9896a68e999298410bf16ffd08e8e67a54ad6a91": {"ta_keywords": "distributed data processing;software distributed cloud;language processing tools;language processing software;software distributed;software integrate cloud;distributed cloud;document collection distributed;distributed cloud computing;use software distributed;orchestration natural language;data processing tools;natural language processing;integrate cloud computing;language processing;processing tools;cloud computing;processing software present;integrate cloud;distributed data;processing software;processing software integrate;large document collection;processing tools relatively;distributed way middleware;collection distributed way;cloud;data processing;cloud computing environment;various distributed data", "pdf_keywords": ""}, "52c040c4b1786166325a0d930af94a529e2b5023": {"ta_keywords": "speaker adaptation methods;speaker adaptation;love speaker adaptation;adaptation technique vector;adaptation methods;snow love speaker;adaptation methods paper;propose adaptation technique;technique vector extractor;vector extractor;adaptation technique;paper propose adaptation;extractor vector;vector extractor vector;adaptation;love speaker;extractor vector replaced;propose adaptation;speaker;replaced sequence vectors;ivector snow love;machine learning;learning technique;ivector snow;machine learning technique;extractor;vector replaced sequence;compare ivector snow;vector replaced;ivector", "pdf_keywords": ""}, "da564ff902a5490088f60c9fb100531fc9f97288": {"ta_keywords": "stochastic logic programs;inference large database;stochastic logic;probabilistic order logics;propositional inference large;making inference learning;database facts probabilistic;inference independent database;probabilistic order logic;logics efficient realistically;grounding propositional representation;probabilistic language;probabilistic language suited;formalized inference;large database facts;logic programs;logics efficient;inference learning;class stochastic logic;formalized inference appropriate;information extraction;order probabilistic language;grounding propositional;performing propositional inference;entity resolution task;propositional inference;inference learning computationally;logic programs called;inference appropriate probabilistic;entity resolution", "pdf_keywords": "grounding probabilistic logics;probabilistic logic programs;learning entity resolution;markov logic networks;probabilistic logics;probabilistic logic programming;probabilistic logic;probabilistic logics fundamental;logic networks;stochastic logic programs;logic networks present;learn probabilistic rules;stochastic logic;personalized pagerank ppr;properly grounding probabilistic;entity resolution;present probabilistic logic;pagerank ppr 12;learning entity;grounding probabilistic;pagerank ppr;extension stochastic logic;logic programs;logic programming framework;personalized pagerank;related personalized pagerank;pagerank;logic programming;logic programs biased;weight learning entity"}, "27724bd19946d6a824d06cdca3cdfe5d40f71003": {"ta_keywords": "predicting edit completions;neural model editcompletion;predicting edit;predict completion edit;trained past edits;edit completions based;learn generate edits;model editcompletion task;generate edits structural;syntactic models learn;likelihood edit learning;learning contextual;contextual models;model editcompletion;contextual contextual models;learning contextual contextual;edit completions;editcompletion task allows;model contextual code;generate edits;edit learning;accuracy syntactic models;predict completion;completions based learned;editcompletion task;model contextual;contextual models address;contextual code;edit learning likelihood;edits structural", "pdf_keywords": ""}, "0cee58946a13a5c2845647b4af8b9d2bf52a8b6b": {"ta_keywords": "entity recognition ner;language models fluid;entity recognition;trained language model;language models;distantly supervised;distantly supervised learning;named entity recognition;language model ner;supervised learning tasks;trained language;learning tasks;language model;supervised;bond distantly supervised;pre trained language;model ner tasks;learning;stochastic gradient descent;recognition ner;supervised learning;complex systems bert;gradient descent;learning tasks paper;generation language models;systems bert;improve prediction;methods supervised learning;ner tasks using;ner tasks", "pdf_keywords": "named entity recognition;learning named entity;entity recognition fully;entity recognition open;entity recognition;supervised named entity;entity recognition demonstrate;distantly supervised named;named entity tagger;ner semi supervised;automatically learning named;semantic information training;unsupervised named entity;supervised named;named entity;entity tagger;semi supervised learning;learning named;semi supervised;distantly supervised;supervision generate labels;supervised learning address;labels automatically learning;distant supervision generate;trained language models;trained language model;use soft labels;entity tagger core;modeling question answering;soft labels"}, "8274799029bfac4402685e1efd995a8aeb9e7426": {"ta_keywords": "sentence compression sequences;sequence sequence autoencoder;unsupervised sentence compression;sequence autoencoder;sequence compressed sentence;sequence autoencoder discrete;abstractive sentence compression;sentence compression benchmark;latent word sequences;sentence compression;sequence sequence models;sequence compressed;word sequences;datasets neural sequence;sequence models;neural sequence sequence;sequence models currently;learning present sequence;compressed sentence;autoencoder discrete latent;neural sequence;input reconstructed sentences;compression sequences;autoencoder discrete;compressed sentence proposed;word sequences forces;middle sequence compressed;probabilistic language model;reconstructed sentences;sequential sequences used", "pdf_keywords": "neural machine translation;summarization attentive recurrent;machine translation promising;supervised machine translation;unsupervised machine translation;machine translation summarization;unsupervised sentence compression;sentence compression benchmark;machine translation;machine translation machine;automatic machine translation;abstractive sentences compression;translation summarization natural;machine translation framework;abstractive sentence compression;translation machine learns;abstractive sentence summarization;differentiation machine translation;translation machine;sentence summarization attentive;supervised abstractive sentences;recurrent neural networks;summarization attentive;translation summarization;sentences compression;sentences compression automatic;deep neural machine;summarization natural language;sentence compression;translation framework trained"}, "927efd299cffcfca3716efefcc904331b70c153e": {"ta_keywords": "question answering;diverse question answering;question answering qa;conversational bilingual qa;answering qa datasets;bilingual qa dataset;involve answers reasoning;answers reasoning processes;interpretable reasoning graph;generating reasoning;generating reasoning graph;answers reasoning;measure answer quality;questions involve answers;bilingual qa;noahqa conversational bilingual;answering qa;reasoning graph metric;develop interpretable reasoning;answer quality;learning models qa;models qa tasks;reasoning graph;reasoning graph reasoning;numerical reasoning focuses;reasoning focuses simple;answering;reasoning processes answers;reasoning graph appropriate;reasoning processes", "pdf_keywords": "answer models trained;question answering annotation;answering annotation biases;benchmarks question answering;question answering;answer models;answering annotation;answer datasets noahq;answer answering;approach answer answering;measure answer quality;answer quality;new machine comprehension;answer answering form;machine comprehension;answering form entailment;answering question answer;machine translation;answer datasets;capable answering question;existing answer datasets;machine translation dataset;answer quality addition;capable answering;almm capable answering;answering question;art answer models;machine comprehension called;answering;errors machine translation"}, "6a116b897569fe4d6ea9ad4c3ba9a18825b96f49": {"ta_keywords": "differentiable learning lighthill;developed differentiable logic;differentiable model learning;differentiable logic;differentiable logic propose;knowledge base completion;tasks learned models;differentiable learning;logic inference tasks;learned models composed;learns sequentially;learning lighthill;learned models;learning lighthill whitham;level logical reasoning;neural controller learns;perform knowledge base;reasoning tasks;probabilistic logical rules;high level logical;problem differentiable learning;knowledge base;logic inference;construct sequence inference;rules useful tasks;tasks knowledge base;solve reasoning tasks;learns sequentially compose;reasoning tasks particular;inference tasks", "pdf_keywords": "logic programming learning;learns logical rules;logic called tensorlog;learns logical;inductive logic programming;logic programming;tensorlog inference tasks;called tensorlog inference;tensorlog inference;developed differentiable logic;operations used tensorlog;tensorlog;tensorlog present recurrent;differentiable logic;used tensorlog;logic programming approach;differentiable logic called;knowledge base reasoning;called tensorlog;formulation inductive logic;controller learns logical;base reasoning tasks;used tensorlog present;tensorlog present;introduce neural controller;new logical rules;logical rules recovered;rules useful representations;inductive logic;representations knowledge base"}, "3c0e8f7337491ca4f714de14021eb23ca43d1d5e": {"ta_keywords": "robust speech recognition;speech recognition asr;speech enhancement neural;robust speech;robustness automatic speech;speech enhancement;systems robust speech;online speech recognition;automatic speech recognition;speech recognition systems;adaptation automatic speech;speech recognition;acoustic model adaptation;speech recognition unknown;includes speech enhancement;automatic speech;recognition asr online;asr online speech;recognition asr;speech recognition aims;microphones noisy reverberant;enhancement neural network;recognition unknown reverberant;acoustic model;microphones;reverberant noisy conditions;noisy reverberant rooms;reverberant noisy;unknown reverberant noisy;recognition systems robust", "pdf_keywords": ""}, "6c78bac2dd71efb89951d9bab72c8129bbc07f67": {"ta_keywords": "regularization topic models;regularize topic models;topic models sparsity;topic models latent;topic models;modeled topic models;sparsity topic distributions;regularize topic;topic models prefer;class topic models;topic models allow;topic distributions;used regularize topic;topic distributions present;topic models leads;variables regularization topic;topic distributions documents;regularization topic;permitted topic models;modeled topic;documents regularization introduced;regularization technique latent;allow sparsity topic;documents regularization;prefer sparsity topic;distributions documents regularization;language modeling;mixed membership models;sparsity topic;regularization introduced paper", "pdf_keywords": ""}, "ce45aa1c64da82bfd02db0e147efa268da6980e4": {"ta_keywords": "bit loading algorithms;carriers ofdm distributed;ofdm distributed;sub carriers ofdm;carriers ofdm;bits sub carriers;loading algorithms analyzed;bit loading;ofdm;numbers transmitted bits;absolute value weighted;classic bit loading;loading algorithms;transmitted bits sub;transmitted bits;averages weights;average weighted sum;average weighted;sum weighted averages;weighted averages weights;weighted averages;weighted average;weighted average weighted;algorithm absolute evaluation;sum weighted;weighted sum weighted;averages weights article;value weighted average;sub carriers;weighted sum", "pdf_keywords": ""}, "d32fb57467d64bb82dce60e904ddc5c18b3f0f91": {"ta_keywords": "parking demand quantified;similar parking demand;urban areas parking;parking demand;areas parking;parking increasingly;drive parking decisions;curbside parking increasingly;characteristics curbside parking;parking decisions;drive parking;areas parking important;parking recent studies;parking increasingly important;parking important issue;parking;parking decisions introduce;curbside parking;zones similar parking;factors drive parking;parking important;repeatability gaussian mixture;curbside parking recent;similar parking;gaussian mixture models;parking recent;years curbside parking;gaussian mixture model;develop gaussian mixture;gaussian mixture", "pdf_keywords": "model parking demand;city parking demand;parking demand;approach parking data;factors parking;overlooked factors parking;parking demand critical;characteristics curbside parking;parking data cities;parking zones;parking decisions;method model parking;model parking;city parking;parking zones city;parking data;parking decisions location;parking demand gmm;factors parking decisions;parking;zones city parking;curbside parking;repeatability approach parking;parking services;cruising parking;uncertainty parking services;paid parking zones;uncertainty parking;parking services contrast;approach parking"}, "ab5c6703fceb3dce6558be309cc65a4a8615c774": {"ta_keywords": "graph construction distance;approximate neighborhood graph;speed motor neighborhood;motor neighborhood based;neighborhood graph directly;construction distance functions;symmetrized distance improve;neighborhood graph;symmetrized distance;modified symmetrized distance;graph based search;mapping distance symmetrization;motor neighborhood;construction approximate neighborhood;distance symmetrization;distance improve performance;graph construction;mapping distance;specific graph construction;index specific graph;nonsymmetric distances resorting;construction distance;distance functions;distances resorting metric;distance symmetrization turn;estimating speed motor;distances resorting;approximate neighborhood;nonmetric nonsymmetric distances;nonsymmetric distances", "pdf_keywords": "neighborhood graph retrieval;graph retrieval strategies;graph retrieval;symmetric distances resorting;graph based retrieval;graph based search;symmetric distance folklore;search methods euclidean;non symmetric distances;distance metrization symmetrization;non symmetric distance;symmetric distances;symmetrization distance;nn search graph;symmetric distance;symmetrization distance function;neighborhood graph directly;distance function similarity;search graph;symmetrization performance brute;approximate neighborhood graph;neighborhood graphs;distances resorting metricspace;search graph based;unmodi\ufb01ed neighborhood graphs;neighborhood graph;metrization symmetrization performance;similarity machine;function similarity machine;nn search"}, "d6741241efb9ffd933df974b43d7109c72238371": {"ta_keywords": "track music generation;multi track music;track music;music generation;music generation mmsf;track music multi;musical events track;music multi track;concatenate tracks;track concatenate tracks;tracks interleaved create;tracks interleaved;sequence musical events;presents multi track;multi track;concatenate tracks single;different tracks interleaved;multi track version;tracks;tracks single sequence;musical events corresponding;track instrumentation;musical events;systems multi track;version multi track;corresponding different tracks;musical material represented;different tracks;track instrumentation note;music multi", "pdf_keywords": "midi instruments;midi;representation musical material;track musical material;general midi instruments;general midi;generating multi track;music machine;variety musical instruments;representation musical;musical instruments;music machine mmmm;musical material coupled;track music machine;multi track music;novel representation musical;musical material;track musical;musical instruments including;multi track musical;instrument track;musical material allows;instruments;representation multi track;present representation musical;track music;control instrument track;128 general midi;musical material accommodates;track instrumentation"}, "d41216f2f809e9fe26a684392f0ded4778f79e74": {"ta_keywords": "speech code switching;switches language utterance;tracking language utterance;language speech recognition;speech recognition asr;tracking language;language switching;recognize mixed language;speech corpus experimental;language speech code;switches language;mixed language speech;speech recognition;dynamic tracking language;automatic speech;language dependent models;speech code;automatic speech recognition;associated language switching;handle switches language;language switching previous;speech corpus;speech recognition mqa;language speech;language utterance;recognize multiple languages;end automatic speech;language speech corpus;outperforms language dependent;recognition asr", "pdf_keywords": ""}, "66cbda3e730285cb572c4792edcef209af32c564": {"ta_keywords": "question answering;learn retriever models;question answering approach;retrieval;information retrieval;domain question answering;task information retrieval;neural networks recently;continuous representations based;representations based neural;task based retrieved;natural language processing;retrieval important component;features continuous representations;knowledge distillation;neural networks;retriever models downstream;distribution quantum;technique learn retriever;probability distribution quantum;component natural language;retriever models;reader model;distribution quantum state;attention scores reader;inspired knowledge distillation;quantum;learn retriever;representations based;query documents task", "pdf_keywords": "question answering benchmarks;question answering;competitive question answering;domain question answering;question answering paper;retriever discriminative training;answering benchmarks;answering benchmarks paper;training retriever discriminative;retriever learning;retrieval open domain;present retrieval open;discriminative training;train retriever learning;learning approximate attention;words model queries;retrieval open;attention score reader;retrieval;answering paper;retriever learning approximate;present retrieval;attention scores reader;retriever discriminative;discriminative training propose;task based retrieved;approximate attention score;approximate attention;iteratively train retriever;approach training retriever"}, "7b96f6165ce5f686e46868c53b111b8e43b93de3": {"ta_keywords": "tendency recent papers;natural language processing;bibliographic analysis;fermilab tevatron;season fermilab tevatron;question bibliographic analysis;recent papers cite;language processing;papers cite;papers cite recent;bibliographic;field natural language;fermilab tevatron strong;language processing experiencing;natural language;fermilab;years cited;tevatron strong correlation;nodes;recent papers;cite recent work;question bibliographic;cited;paper present results;cited remained relatively;15 years cited;tevatron;cite recent;season fermilab;cite", "pdf_keywords": ""}, "863b2b38b33ffc4e5462adbc7aaf84aeb93adda8": {"ta_keywords": "taggers dependency parsers;projecting annotated language;dependency parsers jointly;dependency parsers;tags syntactic dependencies;annotated language single;annotated language;annotated languages;parsers jointly projected;pos tags syntactic;pos taggers dependency;annotated languages present;annotation multiple tasks;parsers jointly;simultaneously projects annotation;projected pos tags;projecting annotated;taggers dependency;syntactic dependencies paper;syntactic dependencies;language set annotated;set annotated languages;parsers;tags syntactic;projects annotation;projects annotation multiple;annotation;annotation multiple;method projecting annotated;language single source", "pdf_keywords": "cross language annotation;multilingual annotation projection;nlp multilingual annotation;multilingual annotation;language annotation projection;languages parallel corpus;treebanks universal dependencies;lingual taggers parsers;languages crosslingual nlp;parallel corpus translations;treebanks universal;crosslingual nlp;crosslingual nlp multilingual;language annotation;nlp multilingual;27 treebanks universal;treebanks;experiments 27 treebanks;parallel corpus;tagging capable parsing;relying parallel corpora;corpus translations;corpus translations sources;machine translation;corpora cross language;new machine translation;cross lingual taggers;source languages parallel;dependency annotations word;27 treebanks"}, "d16d24dd5135f148556df1b2304b3747eee19e00": {"ta_keywords": "based mail classifiers;mail classifiers;plaintext email messages;classifiers email;mail classifiers email;reply lines;email text speech;plaintext email;email text;lines plaintext email;reply lines plaintext;classifiers email threading;automatically identifying signature;sequential representation email;email messages;signature block reply;block reply lines;email message represented;anonymization email corpora;representation email message;signature blocks reply;preprocessing email text;email messages present;anonymization email;representation email;message email;systems anonymization email;email corpora improving;blocks reply lines;email corpora", "pdf_keywords": ""}, "7f54429be66319dc19a42c0c9fceda3ac33fc92d": {"ta_keywords": "building cognitive tutors;cognitive tutors;cognitive tutor;tutor embedded cognitive;cognitive tutor embedded;based tutoring algorithm;computer based tutoring;tutoring algorithm;based tutoring;paper cognitive tutor;tutoring;tutors;tutor;cognitive classroom;tutoring algorithm paper;cognitive classroom instead;embedded cognitive classroom;algorithm paper cognitive;building cognitive;tutor embedded;intelligence programming;write cognitive;write cognitive model;intelligence programming essential;simulated student;tasks building cognitive;simulated student embedded;users familiar cognitive;cognitive task analysis;cognitive task", "pdf_keywords": ""}, "c97500763de8a0871f1b83b1f968fcf4a8b31aee": {"ta_keywords": "stance recognition natural;stance recognition;robotic stance recognition;automatic stance recognition;stance recognition aros;stances fluid dynamics;automatic stance;robotic stance;stances fluid;speaking styles tasks;signals stance taking;development automatic stance;strong stances fluid;acoustic signals stance;recognition natural speech;signals stance;speaking styles;speaking style;compare speaking styles;stance taking;natural speech;stances;activity changes speaking;speaking style aim;stance;aim robotic stance;strong stances;weak stances;natural speech report;differences speaking style", "pdf_keywords": ""}, "2ded680be56e03c8c17a04065deaac8ea6d4fa12": {"ta_keywords": "genetic regulatory network;genetic regulatory networks;law logarithmic proportionality;regulatory networks human;genetic regulatory;genome genetic regulatory;regulatory network human;regulatory networks;regulatory network;networks human genome;law logarithmic;identify genetic regulatory;network human genome;logarithmic proportionality lpv;called law logarithmic;human genome genetic;logarithmic proportionality;genome genetic;law large numbers;emergence new law;genetic;human genome;proportionality lpv;human genome extensively;numbers called law;genome;identify genetic;proportionality lpv used;explain origin law;regulatory", "pdf_keywords": ""}, "753d10503a3cf340e41552109087ffd15ec96446": {"ta_keywords": "firm town zagreb;dynamics business firm;business firm;dynamics business;study dynamics business;business firm town;town zagreb croatia;zagreb croatia;zagreb;town zagreb;firm town;business;firm;croatia;results study dynamics;dynamics;study dynamics;town;results study;present results study;present results;study;results;present;note present results;note;note present", "pdf_keywords": ""}, "998bd8862ab4193e672bb16fe1aae4d446f7536e": {"ta_keywords": "image translation;image image translation;image synthesis;translation unpaired image;effective image synthesis;making contrastive learning;contrastive learning effective;contrastive learning;learning effective image;analysis single image;image synthesis setting;based contrastive learning;contrastive learning present;unpaired image image;unpaired image;image translation setting;single image;single image propose;image translation used;domain single image;learn human brain;image image;image analysis;showing unpaired image;novel approach image;making contrastive;single image object;image propose straightforward;approach image;effective image", "pdf_keywords": "conditional image translation;contrast encoder learns;translation unpaired image;image translation;image translation model;image image translation;representation learning contrast;patches contrastive learning;image translation framework;learning contrast cycle;learning contrast;contrast encoder;unsupervised visual representation;visual representation learning;learns synthesize domaininvariant;method image translation;unsupervised visual;encoder learns;novel deep learning;conditional image;parameters contrast encoder;present conditional image;visual representation;image translation setting;unpaired image;learning method image;encoder learns synthesize;unpaired image image;representation learning;deep learning"}, "d1206ccabd1980848f14472d6548251c2fab7963": {"ta_keywords": "33 nlp tasks;nlp tasks broad;transferability 33 nlp;nlp tasks;language models trained;tasks language modeling;results transfer learning;transfer learning;large scale language;language modeling improve;transferable source tasks;transfer learning beneficial;scale language models;language models;models tasks language;especially target task;source tasks;predict transferable;question answering;language modeling;task data scarce;predict transferable source;tuning models tasks;classification question answering;task domain similarity;source task small;similarity task complexity;tasks language;transferability;substantially target task", "pdf_keywords": "learning embeddings tasks;embeddings tasks encode;embeddings tasks;sequence labeling tasks;answer answering tasks;task embeddings;nlp tasks;nlp tasks broad;answering tasks;challenge learning embeddings;33 nlp tasks;value task embeddings;task transfer learning;labeling tasks answer;task embeddings selecting;labeling tasks;learning embeddings;address challenge learning;representations tasks;answering tasks regime;tasks answer answer;tasks encode;tasks encode individual;source tasks;answer answering;vector representations tasks;transfer learning promising;source tasks simple;transfer learning;approach answer answering"}, "e9d26b9f5e6b619bbb759a67560cb949a9f034ba": {"ta_keywords": "local minmax equilibrium;local minmax equilibria;game theoretic equilibria;ascent learning dynamics;minmax equilibrium;natural game theoretic;minmax equilibrium asymptotically;minmax equilibria;descent ascent dynamics;zero sum games;minmax equilibria class;ascent dynamics timescale;learning dynamics;sum games minimizing;discrete time gradient;ascent dynamics;equilibria class games;learning dynamics timescale;strict local minmax;games assess convergence;time gradient descent;local minmax;gradient descent ascent;ascent learning;descent ascent learning;notions stationarity;game theoretic;theoretic equilibria;theoretic equilibria instead;games minimizing", "pdf_keywords": ""}, "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91": {"ta_keywords": "multimodal machine learning;multimodal;characterized multimodal;advances multimodal;recent advances multimodal;advances multimodal machine;interpret multimodal;multimodal machine;multimodal objects;able interpret multimodal;world multimodal;modalities multimodal;experience world multimodal;multimodal signals;challenges faced multimodal;multiple modalities multimodal;modalities multimodal machine;world multimodal objects;faced multimodal machine;interpret multimodal signals;characterized multimodal includes;problem characterized multimodal;multimodal includes multiple;multimodal signals typical;faced multimodal;multimodal includes;fusion categorization;late fusion categorization;fusion learning;fusion categorization identify", "pdf_keywords": "multimodal machine learning;machine learning multimodal;multimodal affect recognition;multimodal emotion recognition;learning multimodal fusion;recognition multimodal emotion;multimodal emotion;propose multimodal fusion;multimodal representation;multimodal fusion unsupervised;multimodal affect;multimodal fusion;learning multimodal;multimodal data;multimodal fusion broad;multimodal;recognition multimodal;multimodal machine;multimodal data machine;research multimodal affect;propose multimodal;representations use multimodal;multimodal representation translation;speech recognition multimodal;use multimodal data;use multimodal;recent years multimodal;future research multimodal;paper propose multimodal;research multimodal"}, "480d545ac4a4ffff5b1bc291c2de613192e35d91": {"ta_keywords": "dynamic declaration network;computation graph computes;computation complex networks;computing graph constructions;example computation graph;computation graph;declaration network structure;graph computes;framework computing graph;graph constructions;symbolic computation complex;network structures;symbolic computation;implementation complicated network;network structures input;programming language dynet;graph constructions allows;package symbolic computation;dynamy nets;toolkit implementing neural;network structure;network architectures dynet;different network structures;declaration network;graph computes derivatives;dynet toolkit implementing;network architectures;computing graph;complicated network architectures;networks", "pdf_keywords": "toolkit implementing neural;deep learning architecture;neural network toolkit;deep neural network;deep learning;implementing neural networks;dynet deep neural;deep neural;deep learning model;new deep learning;implementing neural network;implementing neural;learning architecture;learning architecture machine;architecture machine learning;neural network;paradigm implementing neural;neural networks;neural networks simple;neural network models;neural;dynamic declaration network;dynet toolkit;dynet toolkit implementing;functions tensors;tensors;architecture machine;called dynet toolkit;combines machine learning;functions tensors paper"}, "aeccb1d53e08adcfe271d1e4b08c0a2cdc3c42b4": {"ta_keywords": "open information extraction;sentence level extractions;systems extracting entities;extracting entities relations;information extraction;entity relation phrases;extracting entities;relations text corpora;corpus collectively leveraged;large corpus;sentences based local;information extraction systems;large corpus collectively;statistics large corpus;entities relations text;corpora important task;extractions translating based;extractions translating;open systems extracting;text corpora;level extractions translating;objective open information;relation phrases individual;framework distant supervision;corpus;corpus collectively;phrases individual sentences;text corpora important;segmenting entity relation;extraction systems relation", "pdf_keywords": ""}, "824cd8db8a68732db04f4d8b7139eb4475e59ff2": {"ta_keywords": "automated metrics;international baseball;version international baseball;human evaluation;datasets human evaluation;league igmn measuring;baseball;baseball league igmn;international baseball league;automated metrics datasets;preliminary evaluation multilingual;metrics datasets human;baseball league;automated;tasks evaluation;models easily applied;models easily;models;evaluation multilingual;tasks evaluation strategies;measuring progress;league;evaluation multilingual version;measuring progress laboratori;evaluation;human evaluation standards;league igmn;environment models easily;model dynamics body;preliminary evaluation", "pdf_keywords": "crowdsourced spoken dialogue;text crowdsourced spoken;personalized nlg systems;personalized nlg;generate text crowdsourced;nlg models;crowdsourced spoken;evaluate personalized nlg;machine translation community;dialogue datasets;spoken dialogue datasets;text crowdsourced;nlg systems;use nlg models;nlg challenges;tasks summarization dialogue;similarity sentence embeddings;increasing use nlg;nlg systems dialect;summarization dialogue;text machine translation;machine translation;embeddings language models;use nlg;summarization dialogue simpli;nlg models real;language models;sentence embeddings;sentence embeddings language;language models propose"}, "508e9bb13fcb1fa0c4dbac47288e8a3c2487bfc2": {"ta_keywords": "generalizing number automata;proof tree generalizing;generalizes proof tree;number automata based;number automata;algorithm generalizing;automata;automata based;algorithm generalizing number;tree generalizes proof;generalization nearest neighbor;frameworks generalizing number;generalization nearest;proof tree;outputs algorithm generalizing;nearest neighbor nnn;tree generalizing;generalizes proof;neighbor nnn interaction;generalizing number;automata based approach;generalizing number illustration;present generalization nearest;proof tree tree;nearest neighbor;nnn interaction provably;tree generalizes;generalization;new algorithm;number illustration operation", "pdf_keywords": ""}, "7a733a8d8f8649cc07e3ea9091f454ae117573af": {"ta_keywords": "biomedical text millions;terms biomedical text;biomedical text;attention mesh;text millions articles;model attention mesh;end model attention;articles pubmed database;millions articles pubmed;model attention;deep learning attention;expensive manual annotation;annotations;pubmed database;word level automatic;attention mesh utilizes;textual evidence annotations;annotation;pubmed database paper;mesh terms biomedical;evidence annotations providing;attention;articles pubmed;information retrieval;facilitate information retrieval;attention attention;medical subject headings;evidence annotations;manual annotation;associate textual", "pdf_keywords": ""}, "abb9b27440719ca44db5947a537fde07f0547973": {"ta_keywords": "repair bandwidth distributed;repair bandwidth nodes;storage nodes;storage nodes data;reducing repair bandwidth;xmath0 distributed storage;bandwidth distributed storage;distributed storage;distributed storage great;reduces repair bandwidth;stored storage nodes;great repair bandwidth;repair bandwidth;nodes data collector;stored nodes;nodes store data;nodes store;repair bandwidth set;distributed storage setting;xmath0 stored storage;remaining nodes store;storage great repair;symbols stored nodes;nodes explicit codes;stored nodes explicit;data collector able;storage;nodes data;xmath0 distributed;bandwidth nodes", "pdf_keywords": ""}, "808a9c9dece4c21be50f41e6caf50101f2b24b47": {"ta_keywords": "modeling human preferences;preference model humans;preference models;preference models leverages;human preferences employing;verification preference models;accurately preferences humans;preferences humans subjects;preference model;preferences humans;mathematical model preferences;preferences choice model;evaluate preference model;decide preferences consistent;human preferences;preferences employing;choice model usually;choice model;decide preferences;assumption accurately preferences;accurately preferences;evaluate preference;qualitative preference statements;argue evaluate preference;choices observed;preference statements;preferences employing number;asked decide preferences;model preferences;experiment decision makers", "pdf_keywords": ""}, "104f75283ae9027eb478e7984bd26b680277ce6f": {"ta_keywords": "sequential action decoding;pretrained language models;actions training sampled;action decoding;language models learn;training sampled actions;robust instruction representations;action decoding core;pretrained language;representations action decoding;models learn text;building robust instruction;instruction representations action;action decoding schemes;robust instruction;learn text representations;instruction representations;scale pretrained language;expert actions training;actions training;language models;vision andlanguage navigation;unseen instructions robustness;agent learn;test agent learn;long sequential action;training sampled;agent learn correct;instructions robustness;sampled actions test", "pdf_keywords": "navigation trained;navigation trained visual;based navigation trained;models pretrained imagenet;provide instruction embeddings;instruction embeddings embedding;instruction embeddings;agent navigate;agent based navigation;pretrained imagenet;forcing training agent;common tasks decoding;trained visual image;trained visual;train agent navigate;tasks decoding;agent navigate starting;tasks decoding needed;stage learning;consider stage learning;learning scheme facilitate;pretrained imagenet transfer;learning;training agent;translation computer vision;models pretrained;embeddings;stage learning scheme;imagenet transfer knowledge;imagenet"}, "f83ef3250ba1166d7c1c7585da7dd78e0641fae7": {"ta_keywords": "generating music;ai cooperative music;cooperative music generation;music applied generate;generating music art;music generation given;study generating music;music generation;simple model music;model music composed;generate coherent music;model music;music composed multiple;generate additional tracks;music composed;cooperative music;generate piano;music;track composed;track composed human;specific track composed;composed multiple instruments;coherent music;applied generate piano;coherent music bars;music art;rock music;generate piano rolls;piano strings models;music applied", "pdf_keywords": "track sequential generative;symbolic music generation;create music improvising;generates music;music generation;improvising music;music improvising music;sequential generative adversarial;improvising music prede\ufb01ned;model symbolic music;music improvising;create music;sequential generative;music generation propose;scenarios generates music;musicians create music;temporal restricted boltzmann;generates music scratch;generating multi track;symbolic music;multi track sequential;generative adversarial network;compositional model generating;restricted boltzmann machine;music scratch learns;music prede\ufb01ned arrangement;musicians create;generative adversarial;music prede\ufb01ned;scratch musicians create"}, "199f383e9acd62649121ccde1e06631ce62c89e9": {"ta_keywords": "threshold secret sharing;algorithm secret sharing;secret sharing general;secret sharing dealer;secret sharing;distributed algorithm secret;protocol threshold secret;condition secret sharing;secret sharing secret;problem secret sharing;sharing secret;generalization cryptographic protocols;sharing dealer;protocols secure multiparty;satisfying kpropagating dealer;communication complexity sneak;sneak algorithm;sharing dealer directly;sharing secret keys;dealer condition secret;shares secret;complexity sneak algorithm;algorithm secret;secret participant algorithm;sharing general networks;secure multiparty computation;threshold secret;cryptographic protocols large;shares secret participant;efficient generalization cryptographic", "pdf_keywords": ""}, "645bc7a5347a299a1e8aa965867bd097f6f4bddd": {"ta_keywords": "agent answers navigation;language guided robots;agent rmm robots;agent navigates asks;agent navigates;language guided;forward human agent;environments language guided;answers navigation process;question answer generation;recursive mental model;introduce agent task;agent answers;guiding agent answers;agent task;rmm robots need;robots need ask;guided robots;rmm robots;propose recursive mental;robots need;answer generation;robots able ask;recursive mental;questions use reinforcement;answers navigation;agent rmm;agent task agent;navigates asks questions;guiding agent", "pdf_keywords": "answer generation dialogues;dialogues question generation;navigation dialogues promising;dialogue agents;based navigation dialogues;navigation dialogues;agent interactions dialogue;navigation conditioned dialogue;agent dialogue agents;approach answer generation;question answer generation;agent dialogue;dialogue systems;answering navigation;question answering navigation;generation dialogues;human agent dialogue;spoken dialogue systems;answer generation;generation dialogues used;conditioned dialogue history;dialogue agents allowed;answer generation context;dialogue;spoken dialogue;generation question answering;answering navigation conditioned;interactions dialogue;dialogues promising;dialogue introduce"}, "d00a403028eb0786915dab7a76692e5eeadf60be": {"ta_keywords": "transductive transfer learning;transductive transfer learners;transfer learning labeled;transductive svms;unsupervised transductive transfer;transfer learning information;transfer learning;transfer learning protein;problem transfer learning;transductive transfer;learning protein extraction;performance transductive transfer;learning problem transfer;transductive svms paper;art transductive svms;learning protein;unsupervised transductive;transfer learners;learning labeled;improve performance transductive;inductive transductive approaches;transfer learners previous;state art transductive;transferring weights;learning labeled data;transductive approaches;transductive;case unsupervised transductive;method transferring weights;transferring weights pool", "pdf_keywords": ""}, "7abcc79e10ff651ef59dea84d347fa64c51e11b2": {"ta_keywords": "photons organic heterostructures;oxygens organic heterostructures;organic heterostructures introduce;organic heterostructures;organic heterostructures ohss;single photons organic;heterostructures introduce effective;heterostructures introduce;photons organic;heterostructures;heterostructures ohss emerged;heterostructures ohss;branching axial oxygens;synthesizing million oxygens;manipulation single photons;axial branching hydrolysis;branching hydrolysis ohss;oxygens organic;assembly complex structures;synthesizing finely synthesizing;sequential crystal nucleation;self assembly complex;oxygens bilateral multi;branching hydrolysis;transfer intermolecular;transfer intermolecular interactions;million oxygens organic;crystal nucleation;charge transfer intermolecular;interactions sequential crystal", "pdf_keywords": ""}, "31b3e84f0a66e27c53c7fe403a0c6cd2319ed797": {"ta_keywords": "logic called tensorlog;reasoning deep learning;tensorlog;called tensorlog;tensorlog classes logical;tensor log highly;parallellizable probabilistic logic;logical reasoning deep;tensorflow;tensorlog classes;tensor log;called tensorlog classes;infrastructure tensor log;infrastructure tensorflow theano;deep learning infrastructure;infrastructure tensorflow;probabilistic logic;tensorflow theano fluid;reasoning deep;tensorflow theano;probabilistic logical;probabilistic logical reasoning;deep learning;learning infrastructure tensor;neural network infrastructure;network infrastructure tensorflow;probabilistic order logic;logical queries compiled;highly parallellizable probabilistic;infrastructure tensor", "pdf_keywords": ""}, "805e49c7282b847faee048a63c1f43ceb08f5257": {"ta_keywords": "voting rules instances;election data testing;study voting systems;voting systems;voting rules;voting rules british;high consensus voting;study voting;consensus voting rules;manipulations elections;election data;consensus voting;rules different voting;vote deficit manipulations;analysis vote vote;coalition size vote;generate election data;statistical analysis vote;voting rules different;derive million elections;vote voting rules;manipulations elections borda;different voting rules;elections;elections borda rule;approach study voting;elections publicly;different voting;elections publicly available;voting", "pdf_keywords": ""}, "641af3bc3cc17993dc72098725d2eb9c0d98049d": {"ta_keywords": "topological data analysis;estimates topological data;topological data;density clustering;density clustering present;topology connectedness data;density tree estimates;modeling human behavior;tree estimates topological;confidence sets density;human behavior modeled;dive density clustering;behavior modeled mixture;connectedness data;density tree;study statistical properties;shape topology;clustering;sets density tree;statistical;statistical properties;model human behavior;properties shape topology;topological;confidence sets;behavior modeled;estimates topological;construct confidence sets;observation human behavior;tree estimates", "pdf_keywords": ""}, "84f2cfbc142ad3165ea3bcacd189a3d1110660e0": {"ta_keywords": "end speech recognition;e2e automatic speech;hierarchical attention network;attention network;networks hierarchical attention;regular attention networks;attention e2e asr;attention networks;speech recognition;attention networks hierarchical;decoding multi task;speech recognition mtsc;hierarchical attention;training joint decoding;architecture joint decoding;multiple microphone arrays;automatic speech recognition;automatic speech;multi task training;ctc attention e2e;separate encoders;attention e2e;joint decoding multi;joint decoding;multiple microphone;microphone arrays;separate encoders article;end end speech;end speech;based multiple microphone", "pdf_keywords": "multi stream attention;end speech recognition;stream attention framework;speech recognition multiple;stream attention;speech recognition;attention framework;speech recognition previous;attention based model;proposed stream attention;stream attention mechanism;microphone array;distributed microphone array;attention framework improve;recognition multiple encoders;attention based;attention mechanism end;end speech;microphone array experiments;end end speech;multi encoder multi;discussed multi encoder;using distributed microphone;encoder multi array;distributed microphone;multi encoder;microphone;attention;encoder multi;recognition multiple"}, "ceefd51b4b391668e313afe8edb3588197002e37": {"ta_keywords": "speech synthesis based;based speech synthesis;speech synthesis;approach speech synthesis;speech parameter trajectory;ms speech parameter;speech synthetic speech;speech synthesis necessary;synthetic speech;natural speech synthetic;synthetic speech large;speech parameter;speech synthetic;spectrum ms speech;speech large global;amplitude modulation quality;hmm based speech;modulation quality;degradation hmm based;based amplitude modulation;synthesis based amplitude;ms speech;based speech;natural speech;modulation quality gap;introduce modulation spectrum;quality degradation hmm;introduce modulation;speech large;gap natural speech", "pdf_keywords": ""}, "2ec99c834bd67ac64ec04b426e5f9fd04f639024": {"ta_keywords": "datasets crowdsourced audio;crowdsourced audio transcriptions;crowdsourced audio transcription;language crowdsourced audio;crowdsourced audio;crowdsourced audio recordings;method crowdsourced audio;constructing datasets crowdsourced;datasets crowdsourced;aggregation method crowdsourced;natural language crowdsourced;curse dimensionality crowdsourcing;data collection crowdsourcing;dimensionality crowdsourcing;language crowdsourced;collection crowdsourcing;crowdsourcing standard tools;dimensionality crowdsourcing crowdsourcing;crowdsourced;collection crowdsourcing work;crowdsourcing;better aggregation algorithms;audio transcriptions;voice virtualization;audio transcription;method crowdsourced;crowdsourcing standard;aggregation algorithms domain;crowdsourcing crowdsourcing;crowdsourcing work", "pdf_keywords": ""}, "4375cccdfaf2ce3d013e4129d39f7801ef8a468e": {"ta_keywords": "photonic crystal devices;photonic circuits;photonic integrated circuits;photonic crystals;photonic circuits based;photonic crystal;design photonic;circuits based photonic;based photonic crystals;design photonic crystal;photonic device;photonic integrated;integrated photonic;based photonic;photonic;photonic crystals used;devices integrated photonic;used design photonic;single photonic;single photonic device;case single photonic;integrated photonic integrated;paper photonic circuits;photonic device paper;device paper photonic;paper photonic;water ice interaction;crystal devices integrated;ice interaction framework;dynamics water ice", "pdf_keywords": ""}, "50a1dd504037463578f6ba8ee40afe4143f3d6fa": {"ta_keywords": "bounds expectation norm;upper bounds expectation;uniformly distributed theuclidean;expectation norm vector;bounds expectation;theuclidean unit sphere;expectation norm;vector uniformly distributed;partition sphere parts;partition sphere;sphere parts sphere;unit sphere;parts sphere sphere;norm vector uniformly;parts sphere;unit sphere note;sphere parts;existing partition sphere;distributed theuclidean unit;vector uniformly;sphere sphere;theuclidean unit;expectation;constructing upper bounds;sphere;sphere note present;norm;sphere note;distributed theuclidean;upper bounds", "pdf_keywords": "diagram bose hubbard;hubbard model;bose hubbard model;xmath1 phase diagram;hubbard model method;phase diagram bose;xmath0 xmath1 phase;xmath1 phase;bose hubbard;diagram bose;phase diagram;xmath0 matrix known;xmath0 matrix;hubbard;complex systems paper;use xmath0 matrix;chebyshev polynomials;xmath0 xmath1;complex systems;study xmath0 xmath1;study xmath0;xmath0;use xmath0;xmath1;based use xmath0;convex geometry algorithm;use chebyshev polynomials;phase;systematic study xmath0;diagram"}, "fac2368c2ec81ef82fd168d49a0def2f8d1ec7d8": {"ta_keywords": "scoring text spans;information extraction tasks;text spans;text spans combines;scoring text;entity recognition;information extraction;named entity recognition;extraction event extraction;extraction tasks named;relation extraction event;recognition relation extraction;event extraction;relation extraction;entity recognition relation;framework scoring text;framework information extraction;sentence context paper;global cross sentence;extraction tasks;spans;spans combines local;sentence global cross;sentence global;span;span representations framework;local sentence global;extraction event;spans combines;cross sentence context", "pdf_keywords": "event extraction involves;event extraction;event extraction important;event argument extraction;relation event extraction;event extraction model;framework event extraction;27 event extraction;event extraction event;event extraction applied;information extraction tasks;ontonotes event extraction;extraction tasks framework;extraction event argument;named entities event;information extraction;extraction event;predicting named entities;entity relation event;argument extraction important;argument extraction;tasks ontonotes event;entities event;task machine learning;event argument identi\ufb01cation;tasks subtasks event;applied information extraction;important tasks ontonotes;subtasks event argument;entities event triggers"}, "25ee819bc444b02db43fcbeced982c975edee033": {"ta_keywords": "task specialization model;crowdsourced labeling type;abrupt consider crowdsourced;consider crowdsourced labeling;consider crowdsourced;crowdsourced labeling;crowdsourced;worker task specialization;task specialization;type tasks;type worker task;specialization model worker;matched type tasks;specialization model;worker task;model worker task;labeling type worker;tasks;position car;tasks unmatched types;type worker;tasks matched type;worker task associated;relative weight group;task;types worker;group people;weight group people;task associated;tasks matched", "pdf_keywords": "challenge crowdsourced labeling;problem crowdsourced labeling;crowdsourced labeling infer;crowdsourced labeling;consider crowdsourced labeling;crowdsourced labeling type;recognition consider crowdsourced;challenge crowdsourced;crowdsourced labeling diverse;main challenge crowdsourced;problem crowdsourced;consider problem crowdsourced;crowdsourced;labeling video annotation;consider crowdsourced;labels tasks worker;binary task labels;aggregate answers clusters;labels tasks;answers clusters;estimation weighted majority;labeling infer;labeling;video annotation;labeling video;skill estimation weighted;labeling infer true;image labeling;labeling diverse;image labeling video"}, "6b13c4ac18f621155a550238a037a670bdce8969": {"ta_keywords": "population animals action;animals action external;behavior population animals;animals action;population animals;action external force;animals;external force;behavior population;force;behavior;study behavior population;action external;population;action;study behavior;paper study behavior;paper study;external;study;paper", "pdf_keywords": ""}, "06d0af396fb08caa6a665dd476380aa16b6199b2": {"ta_keywords": "mechanical pendulum presence;dynamics mechanical pendulum;mechanical pendulum;pendulum presence external;pendulum presence;pendulum;external magnetic field;external magnetic;presence external magnetic;numerical investigation dynamics;dynamics mechanical;magnetic field;investigation dynamics mechanical;magnetic;dynamics;numerical investigation;results numerical investigation;mechanical;numerical;results numerical;investigation dynamics;report results numerical;external;field;presence external;presence;investigation;report results;report;results", "pdf_keywords": ""}, "3e33c988969b4c9f1d9af8c1c0f7644a30d0311f": {"ta_keywords": "translation medical corpus;translation systems;based translation medical;medical corpus;translation systems able;translation medical;modern translation systems;translation based translation;speech translation;design speech translation;medical corpus human;translation based;building translation;language barriers medical;building translation based;systems able translate;corpus;speech translation aims;based translation;corpus human;method building translation;translation;corpus human brain;design speech;language barriers;translate;best modern translation;barriers medical;barriers medical situations;able translate", "pdf_keywords": ""}, "6fb3d5a48be16fe1a4cff5e83093b77fbcd1013b": {"ta_keywords": "cryptography generate private;metric inferential privacy;quantum cryptography;quantum cryptography generate;privacy metric inferential;using quantum cryptography;generate private information;inferential privacy;new privacy metric;privacy metric;privacy metric assumes;generate private;introduce new privacy;new privacy;privacy;ability infer private;private information user;cryptography generate;private information;smart grid;cryptography;infer private parameter;grid smart meters;infer private;smart grid smart;smart grid operations;possibility using quantum;meters smart grid;private parameter;grid smart", "pdf_keywords": ""}, "032e660447156a045ad6cf50272bca46246f4645": {"ta_keywords": "translationally invariant cellular;cellular automata;invariant cellular automata;automata;translationally translationally invariant;languages;translation accuracy;translationally invariant;talks languages;improvements translation accuracy;ted talks languages;language;talks languages demonstrate;languages demonstrate improvements;languages demonstrate;ice ew transition;translationally translationally;demonstrate improvements translation;translation accuracy better;language influenced;speaks writes different;extreme water ice;native language;native language influenced;improvements translation;language influenced number;text person speaks;transition;water ice ew;water ice", "pdf_keywords": "machine translation learns;modeling speaker explicitly;machine translation;machine translation methods;speaker annotated;dubbed speaker annotated;challenge modeling speaker;translation learns;improvements translation accuracy;based machine translation;ef\ufb01ciently model speaker;dataset dubbed speaker;translation learns better;spoken language learning;speaker annotated ted;variations spoken language;speaker explicitly nmt;translation accuracy better;translation accuracy;modeling speaker;translation methods;talks language pairs;language annotation;speaker related variations;personal variations spoken;en language annotation;demonstrate improvements translation;variations spoken;model speaker related;speaker explicitly"}, "2aad7765250f7d9e312c9382f929ea5239b0fd73": {"ta_keywords": "cell biology;scientist guide cell;cell biology presented;guide cell biology;cell;biology;guide cell;biology presented;scientist guide;computer scientist guide;thinking situation natural;situation natural way;way computer scientist;thinking situation;natural way computer;computer scientist;way thinking situation;situation natural;scientist;new way thinking;situation;paper propose new;paper propose;way thinking;thinking;propose new way;way computer;computer;natural way;guide", "pdf_keywords": ""}, "a1d578646cf42f2f69ee996742af484d03cc9121": {"ta_keywords": "multilingual nlp tasks;multilingual language models;trained massively multilingual;bilingual language models;mt5 massively multilingual;machine translation pre;translation pre training;multilingual nlp;massively multilingual language;massively multilingual;cross lingual tasks;performance downstream multilingual;tasking language modeling;massively mult bilingual;multi tasking language;massively multilingual version;multilingual cross lingual;lingual tasks;multilingual;downstream multilingual;downstream multilingual cross;multilingual language;multilingual cross;cross lingual;variety multilingual nlp;lingual tasks paper;machine translation;multilingual version t5;bilingual language;translation pre", "pdf_keywords": "data pretraining multilingual;machine translation pretraining;pretraining multilingual multi;pretraining multilingual;transfer learning multilingual;multilingual multi tasking;translation pretraining;improve performance multilingual;multilingual encoder models;translation pretraining straightforward;data improve crosslingual;performance multilingual cross;learning multilingual encoderdecoder;crosslingual representation learning;tasks existing multilingual;learning multilingual;cross lingual tasks;multilingual cross lingual;lingual tasks crosslingual;multilingual encoder;performance multilingual;performance downstream multilingual;existing multilingual encoder;lingual transfer learning;multilingual encoderdecoder machine;tasks crosslingual representation;multilingual encoderdecoder;tasking language modeling;multilingual cross;tasks crosslingual"}, "0ba3e29dac0857100935b6eb22bce9cee4afcf17": {"ta_keywords": "based textual similarity;textual similarity;queries based textual;textual similarity paper;databases common domains;real world databases;natural language text;natural language texts;global domain normalization;world databases;entities real world;domain normalization;detailed knowledge world;outperforming exact matching;similarity paper natural;domains using queries;represent entities;hundreds thousands entities;matching plausible global;texts easily constructed;heterogeneous databases common;heterogeneous databases;entities represent entities;instead names given;names given natural;thousands entities;similarity;paper natural language;language texts easily;natural language", "pdf_keywords": ""}, "cec6de30eea5b4a5a414cf99830fbdb5c56a481c": {"ta_keywords": "distributed storage;distributed storage retrieval;parallel vod content;vod content distribution;implementation distributed storage;network bandwidth storage;bandwidth storage;distributed algorithm;highly distributed algorithm;vod content;distributed;bandwidth storage capacity;content distribution;storage retrieval goal;stream movie large;massively parallel vod;storage retrieval;highly distributed;implementation distributed;parallel vod;able stream movie;server use markov;distributed algorithm provably;architecture massively parallel;storage capacity constraints;stream;stream movie;device able stream;storage;bandwidth", "pdf_keywords": ""}, "987658ba918710bbce5de8d92eb44bd127cf72c5": {"ta_keywords": "phoneme mappings language;transcriptions phone phoneme;phoneme mappings learnable;language specific phoneme;probabilistic phone phoneme;phoneme mappings;phonemic transcriptions phone;universal speech recognition;phonemic transcriptions;transcriptions phone;phone phoneme mappings;phone based lexicons;graphs speech annotations;supervision phonemic transcriptions;allophone graphs speech;speech recognition systems;speech annotations language;phonological units spoken;interpretable probabilistic phone;annotations universal phone;speech recognition;producing phonological;language universal speech;producing phonological units;languages build phone;speech annotations;language based speech;phone phoneme;specific phoneme;building language universal", "pdf_keywords": "phoneme mappings language;predict phonemes;languages predicting phones;mapping phonemes;grapheme phoneme annotations;universal phones language;based phoneme annotations;used predict phonemes;probabilistic phone phoneme;supervision based phoneme;allophone graph based;predict phonemes seen;universal phone representations;language universal phone;phoneme mappings;language speci\ufb01c phonemes;phoneme annotations;generated grapheme phoneme;phone based speech;phone phoneme mappings;universal automatic speech;phones unseen languages;weights mapping phonemes;mapping phonemes de\ufb01ned;phones language;phone representations;discovery allophone graph;phonemes seen languages;speech recognition translation;phoneme annotations demonstrate"}, "c0a32c68b992b44f1812492c95ac91fb62a6df37": {"ta_keywords": "competitive gradient learning;gradient based bandits;guarantees competitive gradient;gradient reinforcement learning;reinforcement learning gradient;policy gradient reinforcement;games called gradient;behaviors competitive gradient;gradient reinforcement;gradient like games;competitive gradient;competitive gradient based;learning schemes competitive;games correspond gradient;policy gradient;agents employing gradient;gradient learning exhibits;online convex optimization;algorithms including policy;dynamics learning;gradient learning schemes;including policy gradient;framework competitive gradient;reinforcement learning;orbits exist gradient;learning gradient;agent case gradient;algorithms information gradient;dynamics learning based;learning surely", "pdf_keywords": ""}, "01138945dc9de691cd559d09a46597cca7659efb": {"ta_keywords": "statistical fairness bias;statistical fairness;fairness bias particularly;data statistical fairness;concerns fairness bias;fairness bias;technology formulations fairness;formulations fairness bias;fairness bias defined;fairness bias abound;concerns fairness;formulations fairness;fairness;bias abound socially;bias subjectivity;vis concerns fairness;bias;bias particularly;bias defined;bias subjectivity miscalibration;subjectivity miscalibration dishonest;miscalibration dishonest behavior;socially consequential decisions;peer review;chosen performance measures;peer review distributed;dishonest behavior raises;bias abound;distributed human evaluations;explore forms bias", "pdf_keywords": ""}, "ca7cd3a90d2953b2f8e45686afa3e79eb3a39add": {"ta_keywords": "predicting edit completions;predicting edit;predict completion edit;modeling edits;modeling edits directly;edit completions based;likelihood edit learning;trained past edits;instead modeling edits;syntactic models learn;edit learning likelihood;accuracy syntactic models;edit completions;completions based learned;predict completion;approach solving editcompletion;learn generate edited;edit learning;solving editcompletion task;syntactic models;editcompletion task;completion edit;edits directly given;higher accuracy syntactic;solving editcompletion;generate edited code;editcompletion;past edits;completions based;models learn generate", "pdf_keywords": "representing predicting edits;predicting edits editcompletion;predicting edits;predicts edit operation;given program edits;program predict changes;program edits;predict likely edits;program program edits;edits program;predicts edit;edits program program;context edits program;program edits core;represent edit operations;edits occurring context;program predict;occurring context edits;context program predict;program edits occurred;context edits;predict changes;edits core software;edits editcompletion task;edit operations;edits occurred context;approach represent edit;predict changes relationship;edits editcompletion;edit operations present"}, "d7bebb71635cb818d2f5e0ca0a70434283deb4b6": {"ta_keywords": "prediction accuracy imitation;imitation learned policy;imitation learning substantially;accuracy imitation learning;imitation learning;performance imitation learning;divergence imitation learned;kolmogorov divergence imitation;accuracy imitation;imitation learning consider;kl divergence imitation;imitation learned;stronger performance imitation;divergence imitation;performance imitation;novel regret minimization;regret minimization;regret minimization algorithm;imitation;learned policy results;learned policy using;predicting human actions;strong humanlike policies;humanlike policies;humanlike policies multi;search based kolmogorov;self organization learning;higher human prediction;matches human prediction;learning self organization", "pdf_keywords": "imitation learning chess;learned policy chess;policy chess regularizing;imitation learned models;regularized search chess;human imitation learning;imitation learning learning;performance imitation learning;imitation learning;policy chess;learning humancompatible policies;learning chess;chess regularizing search;divergence loss imitation;search chess;divergence imitationlearned policy;experts purely imitation;imitationlearned policy results;loss imitation learned;purely imitation learned;imitation learned;imitation learned il;learning chess leverage;imitationlearned policy;regularized planning;stronger performance imitation;chess regularizing;policy regularized planning;search chess press;kl divergence imitationlearned"}, "4d7a50f6cfd8f27ebd4d5201fad6c5ef42c33733": {"ta_keywords": "predicting hospital mortality;recorded electronic health;electronic health;electronic health records;unstructured predicting hospital;monitored continuous vital;predicting hospital;mortality prediction;continuous vital signals;health monitored continuous;doctors recorded electronic;patient health monitored;mortality prediction attaining;hospital mortality;devices clinical notes;medical devices clinical;medical devices;vital signals;signals various medical;vital signals various;health records ehr;health monitored;various medical devices;clinical notes time;hospital mortality combining;analysis electronic;dft;health records;patients sudden changes;intensive care units", "pdf_keywords": ""}, "1cf2e9e198feef3893da2800a7949f6880ddc084": {"ta_keywords": "nlp research leaderboards;nlp tasks;leaderboards;implementation nlp evaluation;systems various nlp;leaderboards emerged tool;nlp evaluation;leaderboard;functionality standard leaderboard;nlp evaluation explaina;research leaderboards;various nlp tasks;nlp tasks released;leaderboard allows researchers;leaderboard allows;development nlp;implementation nlp;research leaderboards emerged;rapid development nlp;evaluation tool;development nlp research;standard leaderboard allows;nlp research;leaderboards emerged;standard leaderboard;online competition automatic;various nlp;conceptualization implementation nlp;nlp;evaluation explaina board", "pdf_keywords": "vision machine translation;interpretability existing leaderboards;nlp research leaderboards;machine translation;leaderboard paradigm interpretability;leaderboards;machine translation paper;leaderboard;nlp tasks;nlp tasks commonly;leaderboards commonly use;leaderboards emerged tool;leaderboards commonly;rapid development nlp;various nlp tasks;existing leaderboard paradigm;leaderboard paradigm;existing leaderboards commonly;systems various nlp;development nlp;research leaderboards;summarize performance holistically;existing leaderboards;leaderboard satis\ufb01es;leaderboards emerged;summarize performance;various nlp;existing leaderboard;package hosted leaderboard;translation paper software"}, "5665d864d0f1bce6672d6d2bf9f8d8646093cb37": {"ta_keywords": "entanglement entropy;entanglement entropy quantum;automated fact checking;entanglement;entropy quantum;entropy quantum expressed;automated fact;fact checking developed;present automated fact;fact checking;temporal expressions knowledge;expressions knowledge bases;semantic parsing claim;factcheck;entropic entropy;expressions knowledge;semantic;semantic parsing;entropy paper;entropy paper present;entropic entropy paper;entropy;furious factcheck;factcheck make;quantum;parsing claim identification;knowledge bases;terms entropic entropy;fast furious factcheck;furious factcheck make", "pdf_keywords": ""}, "e050cd9cec5eed73bd56cb2c9726ea85e985384b": {"ta_keywords": "sentence compression improving;incremental sentence compression;short term memory;recurrent neural networks;current sentence compression;autoencoder performance sentence;performance sentence compression;sentence compression;networks rnn;sentence compression techniques;sentence compression making;perform sentence compression;performing sentence compression;memory recurrent neural;sentence compression using;sentence compression real;predicting structure recurrent;term memory recurrent;recurrent neural;neural networks rnn;networks rnn experimental;structure recurrent neural;pretrained autoencoder performance;pretrained autoencoder;memory recurrent;structure recurrent;autoencoder;autoencoder performance;network pretrained autoencoder;term memory", "pdf_keywords": ""}, "63567f348231abed171c02f99d4c49c2892a2ade": {"ta_keywords": "imbalances loose privacy;differential privacy;differentially private training;shown differential privacy;level privacy;level privacy data;loose privacy guarantees;privacy;differential privacy exacerbate;loose privacy;based differentially private;privacy data mining;privacy guarantees;privacy data;different level privacy;leaks models trained;differentially private;quantum information processing;privacy guarantees cause;privacy exacerbate existing;accuracy fairness;sensitive information;quantum information;private training mechanisms;accuracy fairness decisions;privacy exacerbate;approach quantum information;biases data;biases data disparate;affect accuracy fairness", "pdf_keywords": "imbalances loose privacy;differential privacy disparate;privacy machine learning;data differential privacy;differential privacy;impacts differential privacy;effect differential privacy;differential privacy recently;level differential privacy;privacy disparate impact;differential privacy noise;differential privacy hypothesise;given differential privacy;privacy disparate;differential privacy machine;privacy;privacy hypothesise level;loose privacy;privacy noise;privacy hypothesise;privacy guarantees;loose privacy guarantees;privacy recently;privacy machine;privacy guarantees cause;privacy noise clipping;accuracy fairness;affect accuracy fairness;privacy recently shown;accuracy fairness decisions"}, "d558c6b953e0267781ed5da90a35c122ba360f10": {"ta_keywords": "lingual cwi models;complex word identification;identifying words phrases;lingual cwi;inconsistencies annotation data;identifying words;cross lingual cwi;discuss inconsistencies annotation;process identifying words;inconsistencies annotation;words phrases;lingual;simple learning models;results competition monolingual;learning models;word identification cwi;annotation data;annotation;task simple learning;competition monolingual;cross lingual;annotation data explain;complex word;word identification;monolingual cross lingual;phrases;simple learning;learning models achieve;data international football;words phrases sentence", "pdf_keywords": "monolingual features train;lingual model task;use monolingual features;monolingual features;words contained crosslingual;cross lingual model;features train crosslingual;monolingual cross lingual;monolingual crosslingual cwi;cross lingual;monolingual crosslingual;learner corpus complex;interested monolingual crosslingual;present cross lingual;lingual model;cross lingual cwi;lingual cwi models;learner corpus;contained crosslingual;use monolingual;word identi\ufb01cation based;crosslingual;lingual;crosslingual cwi;complex word identi\ufb01cation;make predictions languages;corpus complex words;investigate use monolingual;contained crosslingual setup;multi word expression"}, "d33d6c16d7c34dd387841efca74b457b7e60933a": {"ta_keywords": "learning recursive logic;logic programming learning;programming learning recursive;recursive logic programs;learning algorithm pac;inductive logic programming;algorithm pac learning;learning recursive;instance pac learning;pac learning;pac learning algorithm;logic programs;programming learning;logic programs examples;recursive logic;algorithm pac;forced simulation learn;simulation learn twoclause;simulation learn;inductive logic;logic programming;learning algorithm variant;algorithm variant valiant;learning;learning randomly;determinate programs experimentally;learning algorithm;problem inductive logic;learning algorithm accurate;learn twoclause closed", "pdf_keywords": ""}, "68ca176c7566067ae4b3311957cc4a134bfbc819": {"ta_keywords": "deep learning approaches;deep learning;deep learning systems;subsymbolic processing;robotics deep learning;approach subsymbolic processing;machine learning capable;processing robotics deep;processing machine learning;general learning;robotics deep;closer general learning;learning capable ending;solving prediction problems;novel approach subsymbolic;general learning intelligence;capable ending learning;approach subsymbolic;learning failing problems;learning;machine learning;learning capable;learning systems;learning intelligence;manner deep learning;learning approaches able;learning approaches;learned solving;subsymbolic;subsymbolic processing machine", "pdf_keywords": ""}, "163c6b06d948d0869eb8173b537c441c9a786977": {"ta_keywords": "ride share game;simulated ride share;crowded environment;ride share downtown;congestion games continuous;congestion games;crowded environment described;classic congestion games;simulated ride;crowded environment draw;share downtown;games continuous population;share downtown downtown;body crowded environment;motion body crowded;moving body crowded;downtown based concept;population selfish agents;equilibrium maximum social;game equilibrium maximum;games dynamic programming;method simulated ride;crowded;game equilibrium;competition games dynamic;congestion;agents solves markov;selfish agents solves;games dynamic;share game", "pdf_keywords": "model congestion games;congestion games equilibrium;congestion games;congestion dependent social;ride sharing games;equilibrium routing games;rational ride sharing;games equilibrium;games equilibrium concept;social welfare games;welfare games social;learning ride share;routing games;model congestion;consider congestion;congestion work;games social planner;welfare games;paper consider congestion;ride sharing;ride share scenario;making ride sharing;games discrete time;congestion;consider congestion dependent;routing games introduced;propogation model congestion;congestion dependent;machine learning ride;games discrete"}, "2b63812db40152b12925ce4a848b929fa591b858": {"ta_keywords": "machine translation;set machine translation;machine translation techniques;sequence sequence earthquake;sequence earthquake like;translation techniques;sequence earthquake;earthquake like avalanches;translation techniques article;language processing;language powerful tool;sequential data;sequential;natural language;model sequential data;sequence;sequence sequence sequence;sequence sequence;natural language processing;neural networks natural;avalanches techniques;model sequential;avalanches;earthquake;human language;avalanches techniques used;handling human language;networks natural language;earthquake like;like avalanches", "pdf_keywords": "machine translation deep;neural machine translation;machine translation models;translation models neural;translation deep learning;learning machine translation;models machine translation;machine translation widely;machine translation;machine translation sequential;statistical machine translation;machine translation systems;new machine translation;machine translation important;translation models;performing machine translation;language models machine;translation sequential processing;train probabilistic language;probabilistic language models;machine translation main;language models capable;translation deep;language models;translation systems;probabilistic language;problems machine translation;neural network language;neural responding machine;network language models"}, "8b608ad2ec6d0300b6a0bb8f616d4a2b01150693": {"ta_keywords": "topic tracking model;topic tracking;analysis topic tracking;proposes topic tracking;track changes topics;topic word extraction;automatic meeting analyzer;changes topics based;estimated topic models;topic information automatic;novel topic tracking;topics based;topic models online;topic models;recognition topic word;topic changes;changes topics;word extraction meeting;speech recognition topic;language model adaptation;information automatic meeting;model adaptation speech;topic word;estimated topic;automatic meeting;topic changes propose;meeting analyzer;tracking model language;previously estimated topic;topics", "pdf_keywords": ""}, "0a4b8b161931799d5c6bc3ecf07c53bae0e9e502": {"ta_keywords": "corpora language models;language models;training corpora language;language models 021;corpora language;corpus;privileging corpus;language pedestrian;corpus high quality;training corpora;corpora;metrics factuality literary;language ideology care;relationship language pedestrian;language ideology;019 language ideology;018 privileging corpus;analysis text data;construct training corpora;language;hypothesis text data;text data model;corpus high;language pedestrian number;factuality literary;factuality literary acclaim;privileging corpus high;investigate relationship language;based hypothesis text;metrics factuality", "pdf_keywords": "ideology text data;content social media;content content social;content social;training corpora language;nlp literature;texts language ideology;ideology encoded content;language ideology text;de\ufb01ned nlp literature;privileging corpus;corpus high quality;learn wikipedia popular;corpus;corpora language;corpora language models;scikit learn wikipedia;content school newspaper;texts language;nlp;training corpora;content online communities;argue privileging corpus;various texts language;language models better;relates content online;ideology text;word quality;texts;corpora"}, "7a737872a6693ba3f0c99651191b93dad0dadcee": {"ta_keywords": "neural diarization model;diarization model competition;neural diarization;predictions neural diarization;diarization model;diarization based dover;novel diarization;present novel diarization;diarization based;ensemble results subsystems;diarization;subsystems consistent predictions;novel diarization based;subsystems consistent;subsystems;lap based combination;different subsystems;paper ensemble;ensemble;combination paper ensemble;predictions neural;ensemble results;paper ensemble results;based dover lap;results subsystems consistent;results subsystems;neural;consistent predictions neural;formation new state;dover lap based", "pdf_keywords": "diarization voice activity;speaker diarized trained;diarization self supervised;adaptation diarization voice;diarization voice;neural speaker diarized;supervised adaptation diarization;external speech segments;voice activity detection;speaker diarized;adaptation diarization;diarization based;best diarization based;new diarization self;diarization self;speech segments;oracle speech segments;toend neural speaker;speech segments accurate;best diarization;accurate external speech;voice activity;self supervised adaptation;choose best diarization;diarized trained models;diarization based vector;diarization;neural speaker;propose new diarization;new diarization"}, "4d44f2c3f269ea6cbc840b99c3f8119a13829509": {"ta_keywords": "case fact shown;fact shown answer;case fact;shown answer question;answer question yes;shown answer;question yes;yes;question yes answer;fact shown;yes answer;case;answer question;question;fact;answer;shown", "pdf_keywords": ""}, "94c0a8be74d69787a1f3f6e91dcb480a2fd0dd56": {"ta_keywords": "logic poetric sql;natural language reasoning;reasoning natural language;performance natural language;poetric sql;language reasoning;language models execution;language reasoning numerical;logic poetric;ranked placements 2014;poet logic;ranked placements;benchmarks demonstrate poet;poet logic poetric;existing language models;natural language;training language models;language processing;math poet logic;ranked ranked placements;language models;natural language longstanding;poet significantly boost;pre training language;ranked ranked;placements 2014;language processing based;ranked;logical reasoning multi;training language", "pdf_keywords": "boosting reasoning capability;paradigm boosting reasoning;grounding commonsense inference;context computational reasoning;commonsense inference;reasoning skills nl;commonsense inference present;reasoning capability language;computational reasoning;boosting reasoning;reasoning capability;computational reasoning supreme;reasoning skills;program context computational;boost various reasoning;various reasoning skills;framework grounding commonsense;trained program executor;grounding commonsense;corpus pre trained;imitating program executors;reasoning supreme feature;pre training corpus;context computational;complex reasoning scenarios;nl sentences pretraining;natural language downstream;sentences pretraining;training corpus pre;scale complex reasoning"}, "de41f897ea6ca5447cfae81e9505f94ccf50e6a5": {"ta_keywords": "black hole test;test particle gravitational;mass black hole;particle gravitational field;particle gravitational;massive black hole;gravitational field massive;particle moves gravitational;black hole;gravitational field;moves gravitational field;test particle moves;test particle located;hole test particle;motion test particle;gravitational;test particle;moves gravitational;numerical investigation motion;field massive black;xmath0 center mass;particle moves;particle located;particle located distance;field massive;center mass black;particle;mass black;located distance xmath0;distance xmath0", "pdf_keywords": ""}, "0a4dd1e51616b422aa2d437610dbfbdd3733a114": {"ta_keywords": "dialog modeling;based dialog modeling;dialog systems;dialog agent;dialog management;approaches dialog management;dialog agent utilizes;dialog modeling ebdm;dialog management including;dialog pairs;example based dialog;deploying dialog systems;present dialog agent;dialog systems paper;dialog;based dialog;conversation examples movies;conversational mapping user;human conversation examples;learn conversational mapping;dialog pairs letter;approaches dialog;conversational mapping;human human conversation;human conversation;semantic similarity retrieval;syntactic semantic similarity;examples movies twitter;similarity retrieval tf;approach deploying dialog", "pdf_keywords": ""}, "d5ec188a5a39e504788c1fe33457eeb816a99f31": {"ta_keywords": "learning dependency grammars;models constituency parsing;dependency grammars improving;constituency grammar induction;constituency dependency parsing;grounded syntax models;grammar induction focuses;learning phrasal dependency;visual semantic role;visually grounded syntax;concreteness visual semantic;dependency grammars;grammar induction fluid;constituency parsing;word concreteness visual;phrasal dependency structure;constituency grammar;performance constituency grammar;grammars experiments;visual semantic;syntax models;parsing smaller grammar;grammars improving direct;grammars experiments proposed;visually grounded models;dependency parsing signal;grammar induction;dependency parsing;leverages word concreteness;grammars", "pdf_keywords": "models constituency parsing;dependency structure grammars;words dependency parsing;constituency parsing;unsupervised grammar induction;rules constituency grammar;dependency parsing model;grammar induction performance;improve grammar induction;dependency constituency accuracy;constituency grammar;context free grammars;constituency parsing smaller;dependency parsing;grammar induction model;parsing smaller grammar;unsupervised grammar;learn constituency structure;propose unsupervised grammar;structure grammars;constituency structure dependency;grammar induction;mechanisms improve grammar;constituency grammar results;free grammars;structure grammars experiments;smaller grammar;improve grammar;word concreteness structural;grammars used modeling"}, "d878828c2345b665ab9651f20fb0e60e1ffe9de5": {"ta_keywords": "nmr technique strain;gaas heterostructrued sample;heterostructrued sample al;al nuclear spins;ga0 gaas heterostructrued;sample al nuclear;gaas heterostructrued;detecting strains al0;strains al0 ga0;al0 ga0 gaas;magnetic resonance nmr;heterostructrued sample;nmr technique;nuclear magnetic resonance;resonance nmr technique;nuclear spins based;nuclear spins;custom spectrometer succeeded;spectrometer succeeded;strains al0;standard nuclear magnetic;magnetic resonance;ga0 gaas;resonance nmr;presence external magnetic;spectrometer;nuclear magnetic;custom spectrometer;sample al;developing custom spectrometer", "pdf_keywords": ""}, "84e566e326b64b105cabf0c47dff336c4f632a1c": {"ta_keywords": "xmath1 phase diagram;xmath1 phase;xmath0 xmath1 phase;comprehensive study xmath0;study xmath0 xmath1;study xmath0;xmath0 xmath1;xmath1;diagram phase;phase diagram;xmath0;phase diagram phase;phase diagram shown;diagram phase diagram;phase fig;fig phase;phase fig dashed;fig fig phase;phase;fig phase fig;diagram shown fig;diagram shown;diagram;shown fig fig;fig scaledwidth 40;fig scaledwidth;shown fig;fig fig;fig dashed line;fig dashed", "pdf_keywords": ""}, "18268bdfc8a6e0a51f373bc4acf65c8b9a7bd6a0": {"ta_keywords": "unicyclic spirals;unicyclic spirals influence;group unicyclic spirals;spirals influence;spirals influence external;spirals;neural machine translation;machine translation models;group unicyclic;perturbation neural machine;unicyclic;translation models;machine translation;perturbation neural;neural machine;binarized prediction;external perturbation neural;neural;translation models using;models using binarized;behavior group unicyclic;binarized prediction error;using binarized prediction;perturbation;models;prediction error correction;influence external perturbation;binarized;external perturbation;prediction", "pdf_keywords": ""}, "9e77f94e5a12cb33b8b464dc834fd81da1a609e2": {"ta_keywords": "problem delay dynamical;delay dynamical;lyapunov krasovskii functional;general lyapunov krasovskii;lyapunov krasovskii;asymptotic stability pas;asymptotic stability;general lyapunov;delay product;delay product type;improved delay product;construct general lyapunov;lyapunov;conditions multicomponent nonlinear;stability pas problem;delay dynamical dds;krasovskii functional lkf;focuses asymptotic stability;problem delay;delay;state improved delay;dynamics rigid body;krasovskii functional;dynamics rigid;investigate dynamics rigid;stability;multicomponent nonlinear;improved delay;stability pas;multicomponent nonlinear partial", "pdf_keywords": ""}, "24219135d563b1cb24523bf522366c91a55d7604": {"ta_keywords": "label classifiers;multi label classifiers;classifier f1 optimal;binary classifier f1;classifier f1;multilabel performance metric;classifiers;binary classifier;label classifiers given;classifier;f1 optimal thresholding;thresholding predict instances;multilabel performance;extreme thresholding;optimal thresholding;optimal thresholding predict;thresholding;thresholding predict;uninformative binary classifier;thresholding behavior;thresholding behavior paper;extreme thresholding behavior;performance multi label;used multilabel performance;classifiers given;thresholds;multilabel;f1 scores thresholds;commonly used multilabel;multi label", "pdf_keywords": ""}, "274b4ad4840b0a8a70c5bac3fe4b4861ce5fbb95": {"ta_keywords": "shot relation classification;relation classification;relation classification remains;relation classification 200;methods relation classification;relation classification conduct;relations derived wikipedia;learn sentence structure;shot relation;words 100 relations;indicate shot relation;relations;100 relations;relation;100 relations derived;shot learning models;shot learning methods;shot learning;present shot relation;relations derived;learn sentence;wikipedia range;learning methods relation;competitive shot learning;derived wikipedia range;structure sentence report;learning models struggle;learning models;method learn sentence;sentence structure", "pdf_keywords": "task shot learning;shot learning open;shot learning;shot learning methods;years shot learning;proposed shot learning;new shot relation;shot learning popular;shot relation classi\ufb01cation;evaluate shot learning;shot relation;learning related tasks;dataset fewrel shot;fewrel shot relation;art shot learning;classi\ufb01cation task shot;cnn 2016 pcnn;instance encoders cnn;task shot;relation classi\ufb01cation dataset;encoders cnn;encoders cnn 2016;fewrel shot;dataset learning related;cnn;recent years shot;nlp tasks;dataset learning;models dataset learning;pcnn 2016 2016"}, "78e838bcd2268260ddce6be6db4907df6f29f04f": {"ta_keywords": "expressive speech text;emotion recognition speech;communication emotion recognition;text balloons based;generating text balloons;expressive speech;speech text;speech text systems;generate text balloons;automatic speech;text balloons;emotion recognition;expressive text;construct expressive speech;linguistic acoustic features;based communication emotion;verbal message emotional;recognition speech;text systems conveys;expressive text preferable;reveal expressive text;message emotional state;shape text balloons;noise automatic speech;emotional state speaker;text based communication;acoustic speech;films generate text;message emotional;acoustic speech including", "pdf_keywords": ""}, "66f7d22d6373af5032074b25828331958b07e7f9": {"ta_keywords": "dnns medical imaging;networks dnns medical;dp training dnns;training dnns;private training deep;training dnns especially;dnns especially medical;neural networks dnns;dnns medical;networks dnns;training deep neural;training deep;deep neural;differentially private training;dnns especially;deep neural networks;dp training;dnns;especially medical imaging;imaging diagnosis model;neural networks;medical imaging;private training;effects dp training;medical imaging diagnosis;neural;imaging diagnosis;diagnosis model heat;medical imaging applications;use differentially private", "pdf_keywords": "deep neural;differentially private models;private models health;data training deep;private models;interpreting differentially private;training deep;deep learning;deep learning model;cnn;privacy;training deep neural;networks cnn;privacy utility;better privacy utility;learning decision sensitive;providing better privacy;networks cnn involved;differentially private;present deep learning;better privacy;deep neural networks;neural networks cnn;given interpretability models;interpretability models;attention given interpretability;interpretations use personal;privacy utility trade;personal data;models health use"}, "26a238217321008cd1daaa649683d461e16e7574": {"ta_keywords": "historical text normalization;policy gradient training;text normalization;words training neural;text normalization prohibitive;training neural sequence;gradient training;text normalization albeit;policy gradient;neural sequence sequence;unseen words training;neural sequence;phrase based models;sequence sequence models;words training;normalization;normalization albeit outperformed;sequence models;gradient training enables;accurate normalizations;accurate normalizations long;sequence models simple;scratch reinforcement learning;training neural;normalizations long;normalizations long unseen;leads accurate normalizations;normalizations;reinforcement learning;neural", "pdf_keywords": ""}, "eec490a41bdc716fccf98f4a7996c1d31334985a": {"ta_keywords": "cross genre analysis;genre analysis;datasets muspy;symbolic music generation;music generation cross;music generation including;datasets muspy provides;genre analysis powerful;tools music generation;generation including dataset;music generation;available datasets muspy;muspy dataset;easier muspy dataset;generation cross genre;datasets;muspy dataset management;datasets currently;library symbolic music;dataset;cross dataset generalizability;symbolic music;genre cross;tools music;genre cross genre;cross dataset;available datasets;cross genre;dataset generalizability experiment;cross genre cross", "pdf_keywords": "symbolic music generation;music generation muspy;framework music generation;frameworks symbolic music;muspy machine learning;music generation including;components music generation;music generation particular;music generation;learning framework music;symbolic music;music interfaces machine;framework music;dedicated symbolic music;components music;datasets future muspy;music interfaces;generation including dataset;essential components music;learning frameworks symbolic;routines speci\ufb01c music;datasets representations;machine learning frameworks;music;machine learning framework;speci\ufb01c music;speci\ufb01c music interfaces;datasets representations metrics;muspy provides easy;evaluation muspy machine"}, "dc26c3775d233a5fa9516d21fee12aa5b46f8a25": {"ta_keywords": "processing scientific recommendation;unannotated papers research;scientific recommendation result;leveraging natural language;semi supervised approaches;extract scientific term;terms unannotated papers;finding relevant papers;scientific terms unannotated;unannotated papers;natural language processing;advances search engines;scientific recommendation;supervised approaches;extract scientific;relevant papers automatically;limited annotated resources;annotated resources;papers automatically understanding;semi supervised;language processing scientific;annotated resources relatively;multiple semi supervised;search engines;advances search;recommendation result;relationships scientific terms;extremely limited annotated;natural language;papers research", "pdf_keywords": "scienti\ufb01c information extraction;inductive semi supervised;semi supervised learning;information extraction;propose semi supervised;transductive semi supervised;semisupervised approaches scienti\ufb01c;semi supervised;semisupervised approaches;unsupervised relational;unsupervised approaches knowledge;knowledge graph;terms unsupervised relational;extraction develop unsupervised;information extraction develop;build knowledge graph;knowledge graph way;answer prediction scienti\ufb01c;approaches knowledge graph;knowledge graph construction;apply semisupervised approaches;answer prediction;annotated data;machine learning tasks;prediction scienti\ufb01c domains;unsupervised approaches;supervised learning distant;extracted terms unsupervised;annotated data using;unsupervised relational signals"}, "ef2e2f3a847667000b591c8708b543eaf259113b": {"ta_keywords": "speech recognition previous;speech recognition;support speech recognition;chime distant recognition;recognition chime distant;algorithm recognition chime;lstm;recognition chime;neural networks lstm;lwham algorithm recognition;distant recognition challenge;networks lstm;recurrent neural networks;short term memory;lstm shown;recognition challenge developed;wermer tracking;chime challenge employed;sequential learning tasks;wermer tracking systems;chime challenge;distant recognition;tracking systems;sequential learning;recognition challenge;state art chime;performance wermer tracking;tracking systems tested;lstm shown outperform;art chime challenge", "pdf_keywords": ""}, "b176a46ec214b9f75df751dcd2c894f0a7a72a9a": {"ta_keywords": "argumentation mining;argumentation mining propose;argument component identification;attention argumentation mining;attention argumentation;diverse annotated corpora;large diverse annotated;annotated corpora;argumentation;gained attention argumentation;generated web discourse;argument component;arguments user generated;debate portals;debate portals based;web discourse recently;leveraging unlabeled data;analyzing arguments user;data debate portals;analyzing arguments;discourse recently gained;diverse annotated;leveraging unlabeled;web discourse;word embeddings;debate age;unlabeled data debate;data semi supervised;semi supervised;discourse recently", "pdf_keywords": ""}, "83145b7a391b792e24d8d38f74ed6b6ae7a149dc": {"ta_keywords": "neural machine translation;improves translation quality;machine translation systems;improves translation;translation systems;machine translation;translation systems use;usage improves translation;strategy machine translation;machine translation demonstrated;context aware word;including context diminishing;translation quality;actually utilize translation;context usage improves;increases context usage;context translate documents;sentential context sentences;aware word dropout;utilize translation;translated target context;utilize translation time;translation quality according;increases context;context diminishing;word dropout;context translate;context sentences;sentential context;lexical cohesion contrastive", "pdf_keywords": "aware machine translation;machine translation models;machine translation promising;machine translation hypothesize;translation models;machine translation;machine translation able;translation machine translation;translate context experiments;machine translation machine;translation machine;translation models using;re\ufb02ects translation quality;translation quality;machine translation evaluate;translation quality according;able translate context;approach machine translation;translation promising approach;translation evaluate models;translate context;increases context usage;translation promising;translation able;translation able translate;lexical cohesion contrastive;increases context;pronoun resolution lexical;contrastive datasets context;context usage models"}, "45eea76ac46b402f3a209de57e469275419fdc9e": {"ta_keywords": "language based detection;detecting presence foreign;presence foreign word;absence word language;word non native;non native language;foreign word non;detection presence absence;detecting detecting presence;detecting presence;detection presence;native language based;foreign word;native language;based detection presence;absence word;presence absence word;detecting detecting;detecting;presence foreign;non native;word language;method detecting detecting;based detection;language based;detection;word non;language;new method detecting;presence absence", "pdf_keywords": ""}, "94a11c9425bf5f4f9b8ed1b07ea1d15a81b96e9f": {"ta_keywords": "turk micro crowdsourcing;crowdsourcing open source;software mechanical turk;micro crowdsourcing platform;crowdsourcing platform;micro crowdsourcing;online crowdsourcing open;micro micro crowdsourcing;micro crowdsourcing communities;crowdsourcing open;crowdsourcing communities;crowdsourcing;online crowdsourcing;crowdsourcing communities involved;mechanical turk provides;mechanical turk micro;mechanical turk methods;tasks mechanical turk;crowdsourcing used increasingly;crowdsourcing platform 100;mechanical turk;frameworks online crowdsourcing;crowdsourcing used;ipeirotis crowdsourcing;hits ipeirotis crowdsourcing;ipeirotis crowdsourcing used;function mechanical turk;turk micro;turk methods try;turk methods", "pdf_keywords": ""}, "222ae836430ad0c922b47a9345c17212f9584097": {"ta_keywords": "orthognathic surgery periodontal;orthognathic surgery laser;lip combined gingival;lip repositioning surgery;laser assisted lip;surgery periodontal;conventional orthognathic surgery;assisted lip repositioning;orthognathic surgery;surgery periodontal plastic;periodontal plastic procedures;alternative conventional orthognathic;combined gingival recontouring;gingival recontouring study;lip repositioning;conventional orthognathic;range orthognathic surgery;gingival recontouring;assisted lip;surgery laser assisted;periodontal;laser assisted treatment;periodontal plastic;repositioning surgery create;repositioning surgery;orthognathic;surgery laser;range orthognathic;dentoalveolar soft;excessive gingival", "pdf_keywords": ""}, "723770d9ac418e923db5e087ae18c04702f5986e": {"ta_keywords": "rule learner;rule learner generates;new rule learner;rule learners rulesets;rulesets repeatedly boosting;rule learners;class rule learners;learners rulesets built;learner generates rulesets;learners rulesets;rule builder;generates rulesets;greedy rule builder;learning noisy;repeatedly boosting simple;boosting simple;rule builder use;generate power laws;rule builder paper;boosting;generates rulesets repeatedly;learning noisy noisy;repeatedly boosting;method learning noisy;boosting simple greedy;rulesets built possible;confidence rated boosting;data new rule;rulesets built;scalable method learning", "pdf_keywords": ""}, "b9ede62d1d586e1a3b1ef7ec046f09e4e35639bf": {"ta_keywords": "effective retrieval pipelines;retrieval algorithm similarity;term based search;retrieval pipelines propose;ranked retrieval pipeline;retrieval pipelines;retrieval algorithm;retrieval pipeline;designing effective retrieval;retrieval pipelines commonly;nn retrieval algorithm;ranked retrieval;search hidden markov;retrieval pipeline using;search effective efficient;effective retrieval;nn search effective;search effective;based search obtain;subsequently ranked retrieval;approximate nn search;collection retrieval pipelines;markov models search;based search;search repeater search;repeater search;stackoverflow collection retrieval;search repeater;generic nn retrieval;searching winner search", "pdf_keywords": "factoid word embeddings;word embeddings widely;word embeddings;question answering web;explore effective similarity;word embeddings usually;term based retrieval;question answering;large unstructured corpora;effective similarity;space word embeddings;retrieval nn search;information retrieval;embeddings widely;based retrieval nn;non symmetric similarity;searching non metric;similarity function metric;answer question answering;unstructured corpora;similarity;embeddings widely used;similarity function weighted;answering web factoid;based retrieval;information retrieval paper;web factoid word;learning rank;effective searching;learning rank algorithms"}, "6dafc41e9bbd3aa476a0a1c15ca2c459eaef6b98": {"ta_keywords": "morphological inflection models;neural seq2seq models;language processing performance;morphological inflection;evaluate morphological inflection;neural seq2seq;language processing;inflection unseen lemmas;low performance languages;propose evaluate morphological;generic neural seq2seq;evaluate morphological;inflection models employing;morphological;inflection models;clearly generalizing inflection;performance languages;performance languages task;generalizing inflection unseen;languages task;seq2seq models little;splits challenge generalization;human language processing;languages;generalizing inflection;models lemma split;seq2seq models;average 90 languages;effect human language;languages task considered", "pdf_keywords": "neural machine translation;machine translation dm;machine translation;deep neural machine;deep neural;lemma split systems;results deep neural;machine learning tasks;split systems perform;lemma split;split lemma method;better baseline systems;idea hard splittings;split lemma;split systems;results deep;lemma split paper;neural machine;splittings;split;lemma lemma split;form lemma split;splitting;learning tasks;data related splits;neural;new splitting;morphological data;translation dm designed;present results deep"}, "21066ab388b386f3d3552a4a4c25322e0ca69632": {"ta_keywords": "examples hypernym prediction;supervised relation extraction;hypernym prediction;hypernym prediction studied;extraction hypernyms;extraction hypernyms based;projection learning word;approach extraction hypernyms;examples supervised relation;learning word embeddings;relation extraction;supervised relation;examples supervised;hypernyms based;hypernyms based projection;word embeddings;learning word;training examples supervised;relation extraction impact;projection learning;based projection learning;examples hypernym;hypernyms;brain described stochastic;positive examples hypernym;supervised;hypernym;word embeddings paper;classification;embeddings", "pdf_keywords": "automatic extraction hypernyms;extraction hypernymy relations;extraction hypernyms;vectors hypernymy extraction;regularization using synonyms;hypernymy extraction;learning hypernyms;learning hypernyms useful;machine learning hypernyms;extraction hypernymy;word vectors hypernymy;hypernymy extraction important;hypernym wordnet;word embeddings;model extraction hypernymy;featuring hypernym wordnet;extraction hypernyms text;word embeddings based;hypernymy relations based;based word embeddings;hypernyms useful natural;distributional word vectors;hypernyms useful;hypernym wordnet miller;manually created synonyms;synonyms negative;wordnet;synonyms negative samples;learning based word;using synonyms negative"}, "900ce63d71dce47059434cdf2d5e1d77bc716e8d": {"ta_keywords": "brownian particle viscous;motion brownian particle;brownian particle;motion brownian;model motion brownian;particle viscous fluid;brownian;particle viscous;viscous fluid;viscous;simple model motion;model motion;particle;fluid;motion;present simple model;simple model;model;paper present simple;paper present;present;present simple;paper;simple", "pdf_keywords": ""}, "8ae392fc9acbada67a4288a6affc2a77f83befcd": {"ta_keywords": "autoregressive neural vocoder;variational autoencoder autoregressive;quantized variational autoencoder;autoencoder autoregressive transformer;variational autoencoder;autoencoder autoregressive;neural machine translation;model machine translation;neural vocoder;autoencoder;text speech model;machine translation;transformer tts model;neural vocoder naturalness;vector quantized variational;speech waveform sequence;autoregressive neural;vocoder naturalness achieving;non autoregressive neural;transformer tts;speech waveform;mapping function speech;nonautoregressive vector quantized;function speech waveform;machine translation algorithm;speech model;text speech;vocoder naturalness;autoregressive transformer;autoregressive transformer model", "pdf_keywords": "autoregressive neural vocoder;networks automatic speech;neural vocoder;neural vocoders;gan based neural;neural vocoder multi;speech recognition promising;end text speech;speech recognition automatic;recognition automatic speech;automatic speech recognition;automatic speech;based neural vocoders;neural vocoders paper;text speech e2etts;text speech;neural machine translation;speech recognition models;speech recognition;speech recognition speech;recognition speech recognition;machine translation architecture;speech e2etts model;recognition speech;vocoder;autoregressive neural;traditional automatic speech;toend speech recognition;non autoregressive neural;novel machine translation"}, "317d95f99ef62237f6c7d7834d1d19027166b392": {"ta_keywords": "imitation learning structured;novel structured prediction;learning structured prediction;end language generation;structured prediction;language generation unsupervised;structured prediction explore;generate sequences words;imitation learning;propose imitation learning;imitation learning model;decoder decoders naturalness;learning structured;decoder architectures generate;language generation;architectures generate sequences;decoders naturalness;structured prediction approach;encoder decoder architectures;generation unsupervised way;decoders naturalness quality;employs imitation learning;state art encoder;predict end end;prediction explore;generation unsupervised;predict end;decoders;prediction explore large;encoder decoder", "pdf_keywords": ""}, "db190db2567c334b772fd653dca10f300074e421": {"ta_keywords": "speech data training;speaker adaptation pipeline;speech tts models;speaker adaptation;speech recognition asr;text speech data;speech data;automatic speech;text speech tts;paired text speech;automatic speech recognition;speech recognition;end automatic speech;speech tts;end text speech;text speech;speaker similarity objective;simplify speaker adaptation;speaker similarity;pretrained asr models;recognition asr systems;turing machine learning;recognition asr;pretrained asr;transcriptions pretrained asr;corresponding transcriptions pretrained;paired data adaptation;subjective speaker similarity;end asr tts;data adaptation", "pdf_keywords": ""}, "02cbb0db288af2c83b48a023f245812bd22a2408": {"ta_keywords": "text generation metrics;table text models;generated texts;text generation;existing text generation;text models;generated texts semi;reference generated texts;text models wiki;information extraction;text semi structured;tables information extraction;generating text;texts semi structured;generating text semi;information extraction based;study table text;generation metrics;generation metrics propose;grams reference generated;semi structured data;method generating text;structured data;text;easier use data;human judgments better;correlates human judgments;wiki bio parent;reference generated;scale human evaluation", "pdf_keywords": "text generation paraphrasing;evaluate information extraction;generated text reference;text models;generation paraphrasing;new information extraction;extract information texts;information extraction;generation paraphrasing train;relationship text generation;table text models;divergent references automatic;compare generated text;automatically extract information;information extraction called;text generation;information extraction based;references automatic metrics;heuristically reference texts;references automatic;reference texts routinely;references evaluate information;divergent references wikibio;wikibio dataset divergent;wikibio accepts text;paraphrasing;text reference texts;wikibio dataset;references wikibio practice;text reference"}, "20937a0f03bcb845afbedda901a6d4e93a2b5c34": {"ta_keywords": "phase transition tmds;tmd phase transition;xmath1 phase diagram;orthorhombic tmd phase;xmath1 phase;xmath0 xmath1 phase;temperature dependence xmath0;tetragonal orthorhombic tmd;tmd phase;phase diagram tetragonal;phase transition;transition tmds;diagram tetragonal orthorhombic;phase diagram;tetragonal orthorhombic;dependence xmath0 xmath1;xmath1;xmath0 xmath1;orthorhombic tmd;phase;dependence xmath0;diagram tetragonal;xmath0;tetragonal;tmds;temperature dependence;tmd;temperature;study temperature dependence;results study temperature", "pdf_keywords": ""}, "b8e2e764ac82f81a5bc645c818d0d5ad7806e806": {"ta_keywords": "recognition ciel competition;aided recognition ciel;recognition ciel;computer aided recognition;recognition;aided recognition;ciel competition;ciel competition seeks;competition seeks identify;winner semiflexible competition;seeks identify winner;identify winner semiflexible;results computer aided;semiflexible competition;identify winner;computer aided;competition seeks;competition;winner semiflexible;ciel;seeks identify;results computer;identify;semiflexible;aided;present results computer;present results;paper present results;computer;winner", "pdf_keywords": ""}, "7c976b0b54ace7d13b87e8feefe6f29c0599d78d": {"ta_keywords": "semantics collective motion;semantics collective;distributional semantics collective;semantic word network;inducing semantic word;distributional semantics;resources distributional semantics;inducing semantic;crowdsourcing resource;presents crowdsourcing resource;network represents semantic;crowdsourcing resource created;word network;words lexical;semantic word;represents semantic;semantic;group expert lexicographers;lexical ontology;new lexical ontology;collective motions collective;lexical;individual words lexical;represents semantic relations;particle collective;crowdsourcing;galaxies;motion described collective;collective motion described;presents crowdsourcing", "pdf_keywords": ""}, "a83bbc7bf70b1beedbfe0140d24d556e2dc5acc8": {"ta_keywords": "animals swarm;performance swarm animals;swarm animals;noise performance swarm;swarm animals absence;number animals swarm;noise number animals;animals swarm small;animals absence noise;swarm;performance swarm;animals;noise performance;swarm small;number animals;noise;presence noise performance;absence noise;presence noise;animals absence;impact presence noise;noise number;absence noise number;performance;investigate impact presence;impact presence;presence;impact;absence;paper investigate impact", "pdf_keywords": ""}, "2aaf2ee779cd4ff0f26bb73958ea9fb0faa61907": {"ta_keywords": "subcellular location image;fluorescence microscopy images;classifier based svm;classifier;svm classifier;based svm classifier;microscopy images fluorescence;identify fluorescence microscopy;microscopy images;state art classifier;images fluorescence microscope;fluorescence microscope image;svm classifier outperforms;binary classification;classification;classifier based;subcellular location;classifier outperforms;fluorescence microscopy;svm;classification problem paper;propose classifier;microscopy;microscope image;fluorescence microscope;images fluorescence;based svm;subcellular;classification problem;location image finder", "pdf_keywords": ""}, "08f199ebfd27a5f9ada79edd07ac41e46c7278d5": {"ta_keywords": "verbal communication socialization;human communication socialization;communication socialization real;communication socialization;non verbal communication;human communication;contributing communication socialization;communication socialization paper;verbal communication;socialization real;socialization real world;non verbal information;socialization;enhance human communication;movie data;analyze movie data;non verbal;communication;nocoa movie data;movie data used;movie data development;information contributing communication;motion;identify non verbal;contributing communication;socialization paper propose;verbal information contributing;verbal information;socialization paper;relationship non verbal", "pdf_keywords": ""}, "34d5d2f75934caff89311ef20d18a275da5abb47": {"ta_keywords": "learning multiple examples;learning multiple;method learning multiple;multiple examples;method learning;effective method learning;learning;examples;multiple;simple effective method;effective method;method;simple effective;present simple effective;paper present simple;simple;effective;present simple;paper present;paper;present", "pdf_keywords": ""}, "fbf2a6a887ea92311cf207d522c535daf867a6ba": {"ta_keywords": "trained embeddings text;trained text embedding;hypothesize text embeddings;pre trained embeddings;text speech;embeddings text conduct;text embeddings;end text speech;supervised representations text;trained language understanding;pre trained text;trained embeddings;text embeddings contain;speech tts;trained text;text embedding;ljspeech corpus;embeddings contain information;pre trained language;embeddings text;text speech tts;text conduct subjective;self supervised representations;trained language;natural language;corpus;work natural language;speech tts synthesis;subjective listening tests;corpus finding improve", "pdf_keywords": ""}, "da06caf4f340ebc81395f092f9dc3a3101827506": {"ta_keywords": "el speech enhancement;speech enhancement;speech enhancement method;automatic el speech;el speech experimental;speech enhancement methods;prediction el speech;el speech based;laryngectomees keeping listenability;quality el speech;laryngebractomee evaluate performance;el speech multiple;speech multiple laryngectomees;laryngectomee necessary use;proficiency laryngectomee;proficiency laryngectomee necessary;depends proficiency laryngectomee;proposed el speech;speech experimental;statistical voice conversion;speech experimental results;laryngectomee necessary;excitation sounds larynx;using statistical voice;voice conversion;real laryngectomees;el speech highly;statistical voice;laryngebractomee;sounds generated electrolarynx", "pdf_keywords": ""}, "acc2ad56a9c68c799747e08d978f9803997c1527": {"ta_keywords": "predicting precursors perovskite;learns representations materials;precursors perovskite;perovskite compounds starting;precursors perovskite materials;model learns representations;perovskite compounds;learns representations;synthesis novel perovskite;novel perovskite;novel perovskite compounds;perovskite materials;trained generate syntheses;model learns;predicting precursors;perovskite materials using;model synthesis novel;perovskite;representations materials;model synthesis;embeddings language models;generate syntheses inorganic;synthesis insights;inorganic synthesis insights;representations materials corresponding;language models;generate syntheses;technique predicting precursors;language models fed;scientific literature inorganic", "pdf_keywords": "generate synthesis predictions;synthesis predictions;synthesis predictions variety;materials machine learning;model predicting precursors;synthesis insights unsupervised;predicting precursors perovskite;predicting precursors;method predicting precursors;predicting precursors target;predictions variety materials;synthesis insights;synthesizable rapidly automatically;generate synthesis;precursor synthesis actions;reported syntheses;materials theoretically synthesizable;trained model predicting;precursors target materials;synthesizability;precursor synthesis;synthesis rapidly;synthesizable;synthesizable rapidly;synthesis;synthesis planning driven;modeling materials;synthesizability paper;synthesis planning;syntheses"}, "2aea6cc6c42101b2615753c2933a33e57dd665f2": {"ta_keywords": "walks knowledge base;learning inference;beliefs knowledge base;walks knowledge;soft inference;knowledge base learn;learning inference large;knowledge base graph;inference soft;inference soft inference;random walks knowledge;knowledge base;inference inference soft;problem learning inference;inference large scale;knowledge base approximately;knowledge base containing;large scale knowledge;approach knowledge base;beliefs knowledge;inference inference;soft inference procedure;inference large;base learn infer;inference;containing imperfect knowledge;scale knowledge base;approach knowledge;knowledge incomplete coverage;learning method horn", "pdf_keywords": ""}, "1134ec4cdfc1c2161d157b0f4e03dec85d8c4c8a": {"ta_keywords": "convnets trained loss;convnets trained;prediction convnets;convnets;convolutional network;prediction convnets trained;deep convolutional network;convolutional network architecture;predicted roads;novel deep convolutional;gaps predicted roads;deep convolutional;canals aerial images;touch prediction convnets;connectivity roads canals;reconstructing network like;reconstructing network;roads canals;predicted road;recover road connectivity;neural networks connectivity;gap predicted road;networks;function recover road;networks connectivity roads;architecture reconstructing network;recover road;road touch prediction;roads irrigation canals;roads", "pdf_keywords": "road network reconstruction;networks extract images;convolutional networks extract;convolutional network;connectivity reconstructions network;drainage canal networks;network roads graph;convolutional network architecture;connectivity reconstructions;segmenting roads canal;convolutional neural net;images convnets trained;convolutional networks;structures images convnets;canal networks;network reconstruction recursive;roads graph;represents network roads;reconstructions network;network reconstruction;images convnets;deep convolutional network;segmenting roads;reconstructions network like;roads canal;road drainage canal;architecture road network;road network;network roads;recover road connectivity"}, "b9b83860bc0d79b3b629b3035c4b7b7f9f71b5af": {"ta_keywords": "propagation based markov;radio propagation based;propagation trail;radio propagation;propagation trail light;problem radio propagation;markov decision process;walks forest trail;propagation based;light trail forest;node walks forest;trail light trail;based markov decision;light trail;forest trail;markov decision;model propagation trail;trail light;walks forest;deployment wireless sensor;statistical model propagation;impromptu deployment wireless;propagation;mdp algorithms;trail forest;sensor networks;mdp algorithms consider;wireless sensor;trail forest application;wireless sensor networks", "pdf_keywords": "propagation forest trail;walking deployments;sensor networks deployed;walking deployments results;deployment agent walks;actual walking deployments;relay nodes deployment;deploying wireless relay;deployment experiments;present deployment experiments;deploy far relays;walks forest trail;propagation forest;deploying wireless;trail sensor;trail sensor need;nodes deployment agent;sensor need deployed;wireless relay nodes;deployment experiments actual;sensor networks;deployment approaches;wireless sensor networks;networks deployed;nodes deployment;present deployment approaches;deployment approaches limited;experiments actual walking;relay nodes;start trail sensor"}, "386bfd0e411dee4f512a8737c55dd84846981182": {"ta_keywords": "tables text answering;tabular representation learning;learns exclusively tabular;table based prediction;tables associated text;tables underperforms tasks;provides embeddings table;tables;embeddings table;embeddings table substructures;pretrained language models;questions tables underperforms;tables underperforms;answering questions tables;table semantics;jointly models tables;tables text;learning jointly models;table based;tables associated;complex table semantics;representation learning jointly;table;models tables;learning jointly;prediction tasks;models tables associated;experiments tabular representation;based prediction tasks;tabbie provides embeddings", "pdf_keywords": "table representation learning;trained structured tabular;learn representations tables;table based prediction;documents table representation;pretraining table substructures;representations tables text;table representation;table substructures ubiquitous;representations tables;structured tabular data;table structure decomposition;semantic understanding tabular;xij table substructures;table substructures xij;table structure;tables documents;structured tabular;substructures xij table;table substructures;bert compute representations;data table structure;table based;models trained structured;tables text;tables;trained structured;tables documents table;table;semantic text encoder"}, "466865aaeb8902f6f8ed93ceeb5fbf9fc8b593b1": {"ta_keywords": "online speech enhancement;speech enhancement propose;speech enhancement task;speech enhancement;speech enhancement short;based speech enhancement;noisy speech enhancement;stftdomain deep learning;online enhancement deep;frame online speech;enhancement deep neural;enhancement deep;frequency domain beamforming;frame online enhancement;domain beamforming integrated;components target speech;domain beamforming;learning based speech;deep learning;algorithms deep learning;beamforming integrated;istft stftdomain deep;noisy speech;online speech;proposed algorithms deep;tested noisy speech;stft shorter output;frequency resolution higher;transform stft domain;stftdomain deep", "pdf_keywords": "latency speech enhancement;low latency speech;speech enhancement;latency speech;dnns improve enhancement;stft domain dnns;latency machine learning;speech enhancement experimental;domain beamforming easily;beamformer low latency;domain dnns improve;frequency domain beamforming;domain beamforming;better enhancement performance;online video streaming;stft domain powerful;transform stft domain;video streaming;reduce algorithmic latency;beamforming easily integrated;low ms frame;streaming;frame online frequency;approach online video;enhancement performance;low latency;transform stft;algorithmic latency low;enhancement performance algorithmic;online frequency domain"}, "4759aaacd71fbb2b5ca253aa13ccceac0bc7fe8a": {"ta_keywords": "annotating 26k explanations;task computational argumentation;computational argumentation;web argument convincingness;argument convincingness challenging;computational argumentation approach;argumentation based analysis;argumentation based;argumentation approach task;computational argumentation given;convincing argumentation based;argumentation;argumentation approach;rich svm learners;convincing argumentation;argument convincingness;26k explanations written;natural language propose;explanations written natural;convincingness challenging task;svm learners bidirectional;empirical manner annotating;arguments certain controversial;natural language;natural language create;convincingness challenging;analysis 26k explanations;problem convincing argumentation;networks convolution attention;svm learners", "pdf_keywords": ""}, "2cf21fc85af45512bf34d710f325872dca8a5331": {"ta_keywords": "traffic prediction;new traffic prediction;traffic prediction upcoming;region traffic prediction;traffic prediction method;traffic simulation;real time traffic;traffic simulation paper;travel planning openstreetmap;model graphical traffic;road map european;urban planning transportation;planning openstreetmap;planning openstreetmap free;time traffic simulation;planning transportation;traffic condition data;region traffic;cross region traffic;traffic;prediction model graphical;openstreetmap;prediction upcoming international;road map;graphical traffic;graphical traffic condition;traffic condition;new traffic;learn prediction;presents new traffic", "pdf_keywords": ""}, "fb0a68981dae15f31cbcf5442509a3b8279b264c": {"ta_keywords": "noise robust speech;noise features vts;enhance mfcc vectors;noise features;corrupted noise features;noise robust;reduction splice noise;approaches noise robust;robust speech recognition;recognition remove noise;improving performance noise;robust speech;features vts enhancement;noise mean normalized;performance noise highly;speech recognition remove;vector corrupted noise;remove noise;mfcc vectors;normalized splice algorithms;mfcc vectors dynamic;splice noise;remove noise effect;vts enhancement method;splice noise mean;speech recognition;using noise estimates;using noise;enhance mfcc;noise estimates", "pdf_keywords": ""}, "4d41c2fa74dd018e39ddb3cbbfead1b42615612c": {"ta_keywords": "networks deep;networks deep learning;diverse networks deep;training networks;graphs neural architectures;deep learning successful;training networks task;deep learning;graph neural networks;neural networks predict;machine learning pipelines;advances graph neural;graphs neural;tournament model learns;representation neural architectures;training neural networks;networks predict performant;learning pipelines;neural architectures enabling;learns strong representation;neural networks leveraging;networks predict;graph neural;learning pipelines present;predict neural;computational graphs neural;deep learning used;knowledge training networks;analysis deep learning;predict neural network", "pdf_keywords": "diverse feedforward neural;imagenet;neural network architectures;neural networks single;graphs neural architectures;10 imagenet;parameters diverse feedforward;imagenet exploiting structure;assignment problem learning;learns strong representation;learning multiple layers;feedforward neural networks;neural networks;neural architectures;10 imagenet exploiting;imagenet exploiting;graphs neural;model learns;neural network;novel task predicting;computational graphs neural;task predicting;cifar 10 imagenet;parameter prediction cifar;strong representation neural;networks;feedforward neural;model learns strong;parameter prediction;representation neural network"}, "709f0a4229e40339b595072ae9fbd3a1ae1fd93e": {"ta_keywords": "operations dynamic neural;instance learning architectures;architectures impractical batch;instance computations batching;computations batching;computationally efficient batched;efficient batched operations;batching operations dynamic;batching algorithm seamlessly;learning architectures;automatically batching operations;neural network toolkits;computations batching algorithm;instance learning;hilbert space networks;learning architectures impractical;efficient batched;batched operations;batching operations;dynamic neural network;batching algorithm;single instance learning;batching algorithm used;minibatch computations;dynamic neural;new batching algorithm;toolkits pytorch dynet;batched operations obtain;automatically batching;batches", "pdf_keywords": "batching computation graphs;automatic batching computation;batching computation;performance tensorflow;automatic batching decisions;better performance tensorflow;batching decisions automatic;tensorflow;automatic batching widely;decisions automatic batching;batching decisions;dnn machine learning;batching design;favorably manually batched;batching widely;scheduling automatic batching;batching widely studied;automatic batching;batching;graphs executed batched;manually batched code;manual batching design;express computations naturally;batching design obtain;batched code;neural network dnn;straightforward manual batching;use deep neural;programmers express computations;manual batching"}, "d745ba895cf8dcba5670fb01feea931fc72f9c77": {"ta_keywords": "transfer learning experiments;deep reinforcement learning;transfer learning;transfer learning used;source deep reinforcement;deep reinforcement;setting transfer learning;reinforcement learning;reinforcement learning drl;set transfer learning;training transferred layers;learning experiments;learning experiments explore;learn policy;learning pool experts;learning;learning pool;reinforcement;used learn policy;learn policy scratch;learning useful;transfer simpler environments;learning useful representation;classification setting transfer;experiments suggest transfer;complex downstream tasks;outperforms training transferred;learning used learn;transfer simpler;training transferred", "pdf_keywords": ""}, "af6c6e66fe0a9ba19c304665e01db1c5a5fba1e4": {"ta_keywords": "constrained markov decision;confidence reinforcement learning;constraints learning probability;reward function constraints;learning probability constrained;reinforcement learning;reinforcement learning settings;reward satisfying constraints;markov decision processes;constraints learning;constrained markov;probability constrained markov;satisfying constraints learning;stochastic decision;markov decision;confidence reinforcement;upper confidence reinforcement;stochastic decision problems;learning settings reward;reinforcement;inclined lid safe;inclined inclined lid;inclined lid;steer inclined lid;reward function;learning probability;constraints described cost;regret respect reward;cost constraints illustrative;sublinear regret", "pdf_keywords": ""}, "3389b6b8ee5a1ef0395df9f383e771650087b828": {"ta_keywords": "question answering capable;scalable question answering;question answering;retrieval systems answer;information retrieval pre;answering capable answering;information retrieval systems;classical information retrieval;information retrieval;information retrieval search;retrieval systems;retrieval search engine;systems answer information;turn information retrieval;capable answering wide;domain expert;capable answering;answers experiencing information;retrieval search;domain expert turn;domain experts;answering capable;authoritative answers;authoritative answers experiencing;trained language models;answer information needs;retrieval pre trained;answer information;language models synthesized;language models", "pdf_keywords": "retrieval question answering;developing retrieval systems;retrieval systems pre;retrieval systems;question answering;retrieval systems combine;machine reading comprehension;answer selection based;developing retrieval;document retrieval systems;document retrieval;answer selection;question answering summarization;approach document retrieval;document retrieval question;approach answer selection;trained language models;domain expert response;retrieval;machine reading;reading comprehension mrc;document retrieval used;tasks machine reading;used document retrieval;retrieval used document;envisions domain experts;expert response quality;elements document retrieval;answering summarization;language models"}, "f432d10677897cf72f9594ba7bd4c9199b270fb3": {"ta_keywords": "classification performance measures;symmetric balanced accuracy;balanced accuracy;classification performance;balanced accuracy previously;adequately evaluating classification;evaluating classification;analysis classification performance;evaluating classification results;classification results;classification;classification results finally;symmetric balanced;machine learning systematic;called symmetric balanced;measure best;performance measures;classification problem;classification literature;analysis classification;used classification;classification literature say;accuracy;choose measure best;ideally choose measure;balanced;measures satisfying desirable;measures satisfying;classification problem reduced;previously used classification", "pdf_keywords": "analysis precipitation prediction;precipitation prediction task;precipitation prediction;properties validation measures;validation measures;validation measures useful;classi\ufb01cation algorithms provide;prediction task discussed;evaluation classi\ufb01cation algorithms;classi\ufb01cation algorithms;measures formalizing;prediction task;good measures formalizing;weather forecasting;weather forecasting service;discriminability informativeness favoring;discriminability informativeness;measures called generalized;prediction;distinctiveness discriminability informativeness;distance generalizing known;known matthews correlation;propose properties validation;properties distance generalizing;measures formalizing analyzing;analysis precipitation;detailed analysis precipitation;cost distinctiveness discriminability;generalized means;measures useful"}, "da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71": {"ta_keywords": "models endangered languages;extracting text noisy;ocr tools robust;setting endangered languages;endangered languages;ocr tools;languages textual data;textual data;endangered languages little;purpose ocr tools;ocr;endangered languages develop;general purpose ocr;language processing;paper books scanned;books scanned;languages textual;language processing models;books scanned images;purpose ocr;average languages textual;build natural language;noisy database note;textual data languages;textual;natural language;extracting text;natural language processing;reducing recognition error;languages little data", "pdf_keywords": "recognition endangered languages;dataset transcribed images;transcribed images endangered;benchmark dataset transcribed;images endangered languages;transcribed images;endangered language text;high resource translations;dataset transcribed;benchmark dataset transcriptions;endangered language books;transcriptions scanned books;dataset transcriptions scanned;containing endangered language;setting endangered languages;resource translations;endangered languages based;transcriptions scanned;endangered language;endangered languages;dataset transcriptions;endangered languages ainu;endangered languages present;resource translations commonly;translations;language text;critically endangered languages;ocr tools;translations commonly appear;translations commonly"}, "f66c82ca087b435463ef4fa0de49825c4eb55885": {"ta_keywords": "semantic parser;semantic parsing;atis corpus semantic;corpus semantic parser;semantic parsing uses;context free parser;semantic parser originally;free parser using;free parser;tree based semantic;parser;parsing;method semantic parsing;atis corpus;operation atis corpus;parser using;semantic annotation;corpus semantic;based semantic annotation;parser using context;parsing uses;parsing uses simple;semantic annotation learns;parser originally;free parser different;parser originally developed;corpus;based semantic;semantic;parser different", "pdf_keywords": ""}, "b37d073109cfcf913cf53aded3872e6158e828a0": {"ta_keywords": "visual environment better;visual features;visual environment;visual features actually;visual features outperform;features outperform visual;discover visual features;motor visual environment;outperform visual counterparts;visual counterparts;environments benchmark room;environment natural language;ablating visual features;visual;language environment natural;new environments benchmark;benchmark room;motor visual;natural language empirically;outperform visual;benchmark room room;expert models;discover visual;environments benchmark;features;motion motor visual;language empirically;features actually;language environment;room room dataset", "pdf_keywords": "ground language visual;visual grounding;visual models mixture;recent visual grounding;visual grounding approaches;visual models;models visual features;training visual;modality models embodied;visual context;training visual non;approach training visual;non visual models;language visual;visual features;visual feature vectors;ground language;visual feature;language visual appearance;language ground;object detections ensemble;models embodied tasks;models embodied;visual features environment;models visual;modality models;visual appearance route;visual features work;features environment training;visual non visual"}, "e2ef0dc26a669ed764e2d70257b162298b8b608e": {"ta_keywords": "antenna secure secrecy;maximizing secrecy rate;maximizing secrecy;multi antenna secure;maximize secrecy rate;maximize secrecy;approach maximize secrecy;algorithm maximizing secrecy;antenna secure;secrecy rate deflagration;secure secrecy rate;secure secrecy;secrecy rate;function radar communications;radar communications;secrecy;systems susceptibility eavesdropping;radar channels;susceptibility eavesdropping;secrecy rate mcrs;eavesdropping messages aimed;eavesdropper ed;eavesdropping;susceptibility eavesdropping messages;multicast multi antenna;eavesdropper;radar channels use;act eavesdropper ed;surfaces radar channels;act eavesdropper", "pdf_keywords": "multi antenna secrecy;maximizing secrecy rate;optimize secrecy capacity;antenna secrecy;optimize secrecy;maximize secrecy rate;maximizing secrecy;antenna secrecy rate;secrecy capacity communications;algorithm maximizing secrecy;approach maximize secrecy;maximize secrecy;radar assisted secure;ascent optimize secrecy;secrecy capacity;secure communications;secrecy rate;function radar communications;secure communications systems;mimo radar multiple;secrecy rate optm3sec;radar communications;secrecy rate context;secrecy;radar multiple;secrecy rate installing;radar multiple multiple;radar channels;dual function radar;mimo radar"}, "243880fde63abfc287bd1356c2e1dbf68a1a0aac": {"ta_keywords": "language non indigenous;translating indigenous language;indigenous languages;non indigenous language;indigenous language;indigenous language non;translating indigenous;indigenous language article;languages just feasible;indigenous languages spoken;languages;60 indigenous languages;problem translating indigenous;language;language non;interplay inter landau;feasible develop languages;languages spoken united;languages just;landau inter species;languages spoken;develop languages;language article;language article discuss;inter landau;develop languages just;translating;framework landau lifshitz;gilbert llg formalism;inter landau inter", "pdf_keywords": "technologies indigenous languages;language technology projects;india speech technologies;language technology development;indigenous languages based;indigenous languages;language technology;indigenous languages ranging;efforts developing languages;governments language technology;range indigenous languages;india speech;spoken provinces india;frastructure language technology;provinces india speech;developing languages;technologies indigenous;speech technologies offer;speech technologies;educational technologies indigenous;hearing language writing;developing languages increased;available infeasible languages;languages paper describes;languages based;development frastructure language;feasibility text speech;infeasible languages paper;provincial governments language;languages document"}, "9364eff879a9dcb34fe3dfdd0843e69c14dd333b": {"ta_keywords": "models predictive interpretable;interpretability based;interpretability interpretability;approach interpretability interpretability;interpretability outperforms;predictive interpretable;interpretability context human;interpretability;interpretability based use;new approach interpretability;interpretability interpretability context;interpretability different datasets;approach interpretability based;approach interpretability;interpretability paper;interpretability different;proxy interpretability outperforms;interpretability paper present;interpretability context;proxy notions interpretability;sparsity proxy interpretability;interpretable;proxy interpretability;terms interpretability;notions interpretability;interpretability outperforms state;methods terms interpretability;terms interpretability paper;notions interpretability different;user studies models", "pdf_keywords": "optimizing models interpretability;human interpretable models;interpretable models;models human interpretability;interpretable models human;models interpretability;interpretable models use;models interpretability require;model interpretability;interpretability achieved learning;explanation methods optimizing;model interpretability important;human interpretable;human interpretability;learning models inherently;human interpretability achieved;sophisticated explanation methods;learning models;identify human interpretable;interpretability achieved;interpretability;supervised;interpretable;interpretability require;supervised learning;machine learning;learning machine learning;interpretability important;machine learning received;supervised learning machine"}, "64c575bb8b3e11097605028de5c289b0b2d839a4": {"ta_keywords": "speech synthesis preserving;naturalness preserving speaker;native speech synthesis;adaptation phonetic;model adaptation phonetic;adaptation phonetic correction;lingual speech synthesis;speaker individuality synthetic;synthesis preserving speaker;speech synthesis;speech synthesis technique;preserving speaker individuality;preserve speaker individuality;speaker improve naturalness;individuality synthetic speech;speaker individuality synthesized;synthetic speech;correction prosodic phonetic;phonetic correction based;non native speech;preserving speaker;speaker natural;speaker natural speech;synthesized target speech;use nonnative speech;degradation speaker individuality;non native speaker;extracted speaker natural;speaker individuality based;native speaker improve", "pdf_keywords": ""}, "5af9ab65d186e4e1e0b1cef1962ca15336f37931": {"ta_keywords": "semantic parser trained;semantic parsing;semantic parsing task;information semantic parsing;semantic parser;semantic parsing approaches;tourist information semantic;dependent semantic parsing;trained corpus semantic;corpus semantic tags;parser trained corpus;paper semantic parser;corpus semantic;semantic tags learn;semantic tags;learn alignment semantic;information semantic;parsing approaches;parser trained;parsing;alignment semantic;semantic components;alignment semantic components;context dependent semantic;natural language;parsing task translating;translating natural language;semantic;paper semantic;parsing approaches task", "pdf_keywords": ""}, "4f8e1a4247ce06a15760fc2692c6849601d41b6f": {"ta_keywords": "textual entailment datasets;text based entailment;textual entailment;entailment datasets;cognition textual entailment;textual entailment fundamental;entailment datasets use;multiple textual entailment;encoded useful entailment;useful entailment approaches;useful entailment;entailment fundamental task;based entailment models;entailment models;entailment models significantly;entailment approaches;based entailment;entailment approaches shown;entailment fundamental;entailment;task natural language;human cognition textual;finding textual;finding textual content;cognition textual;multiple textual;approach multiple textual;external knowledge helps;external knowledge sources;information external knowledge", "pdf_keywords": "textual entailment datasets;natural language inference;entailment datasets;entailment based graph;textual entailment based;entailment datasets use;textual entailment;answering textual entailment;textual entailment fundamental;multiple textual entailment;approach textual entailment;knowledge graph embeddings;entailment inference based;entailment inference;entailment based;entailment fundamental task;approach entailment inference;entailment fundamental;new approach entailment;entailment;language inference;graph embeddings powerful;nli datasets;graph embeddings;task natural language;question answering textual;datasets knowledge graph;natural language processing;useful natural language;performance nli datasets"}, "9cf75483deee77b3c0ee4f996d808437ab4a7435": {"ta_keywords": "breast myofibrillar protein;myofibrillar protein;gelation process chicken;chicken breast myofibrillar;breast myofibrillar;process chicken breast;myofibrillar;model gelation process;gelation process;protein;simple model gelation;model gelation;process chicken;gelation;chicken breast;chicken;breast;model;simple model;present simple model;process;paper;paper present simple;paper present;present;present simple;simple", "pdf_keywords": ""}, "0ce184bd55a4736ec64e5d82a85421298e0373ea": {"ta_keywords": "neural machine translation;machine translation;machine translation natural;predicting topological structure;topological landscape;predicting topological;reproducing topology landscape;end speech processing;topological structure landscape;method predicting topological;topology landscape adapting;transformer emergent sequence;sequence models widely;speech processing;topology landscape;sequence sequence models;sequence models;embedded topological landscape;topological landscape transformer;landscape embedded topological;reproducing topology;neural machine;structure landscape;language processing;landscape transformer emergent;translation natural language;emergent sequence;emergent sequence sequence;sequence model achieves;speech processing paper", "pdf_keywords": "networks multilingual speech;multilingual speech recognition;neural networks multilingual;multilingual machine translation;translation multilingual end;multilingual speech;multilingual machine;end translation multilingual;propose multilingual machine;machine translation model;end speech features;transformer speech tasks;multilingual end;translation multilingual;machine translation;networks multilingual;multilingual end end;10 languages transformer;multilingual model;automatic speech;languages transformer;multilingual model parameters;single multilingual model;transformer speech;translation model;speech features help;speech features;languages transformer based;multilingual;translation model based"}, "448e15e267b20bee1644034e18630da2e68cf36e": {"ta_keywords": "study dynamics subway;automated repeatability ale;dynamics subway station;automated repeatability;dynamics subway;using automated repeatability;repeatability ale method;repeatability ale;sand using automated;repeatability;loose sand using;loose sand;deep loose sand;sand using;subway station deep;subway station;subway;sand;station deep loose;ale method;dynamics;station deep;automated;using automated;study dynamics;station;ale;loose;method;systematic study dynamics", "pdf_keywords": ""}, "4c94dc1b2391d78c9cfdd69955d20b56d7a16982": {"ta_keywords": "systems erasure codes;linear mdc codes;quantum error correction;tuning code redundancy;erasure codes;redundancy encoding data;storage systems erasure;redundancy encoding;approach quantum error;erasure codes used;code redundancy observed;mdc codes valid;mdc codes;code redundancy;quantum error;storage systems;use redundancy encoding;convertible codes inconsequential;distributed storage systems;distributed storage;reduce storage cost;tuning code;conversion linear mdc;codes valid parameters;significantly reduce storage;scale distributed storage;convertible codes;encoding data large;storage;design convertible codes", "pdf_keywords": "convertible codes inconsequential;access optimal conversion;code conversion;convertible codes code;regime convertible codes;convertible codes;code conversion usually;design convertible codes;code construction;analysis code construction;lower bound conversion;enable code conversion;conversion procedure construction;code pairs;optimal conversion general;access cost conversion;optimal conversion;bound conversion split;codes code pairs;code construction paper;generalizations conversions split;conversion split regime;combination generalizations conversions;bound conversion;conversions split merge;reducing conversion general;cost conversion;cost conversion valid;conversion procedure;conversions split"}, "c55d5805a6eb8b1482f21581fe893484eaf9ffb5": {"ta_keywords": "singing voice characteristics;voice characteristics manipulating;singer individuality technique;perceived age singing;maintaining singer individuality;voice characteristics;singing voice age;age singing voice;singer perceived age;varieties voices singers;constraints age singing;age singer perceived;control singing voice;singing voice timbre;singer perceived listener;controlling voice timbre;voice timbre arbitrary;voice timbre control;gaussian mixture models;singing voice singers;convert singing voice;intuitively control singing;change singer perceived;singer individuality;regression gaussian mixture;voice timbre based;age singing;singing voice;voice timbre varieties;voice singers", "pdf_keywords": ""}, "4275d4c4bd10742b321467f175f16198ed7d17d7": {"ta_keywords": "generating music;music generation generating;ai cooperative music;generation generating music;track music generation;generating music art;music generation;music creation track;music creation;cooperative music creation;generate coherent music;networks gans symbolic;intelligence cooperative music;music creation given;midi files generate;generate additional tracks;gans symbolic;gans symbolic domain;study generative adversarial;networks gans;midi;creation track;generative adversarial networks;multi track music;generative adversarial;cooperative music;learn noisy midi;track music;piano track composed;track composed", "pdf_keywords": ""}, "802ddaf5bd731b91e64d8cee43f7fb614b42c1df": {"ta_keywords": "proton collision;proton proton collision;dynamics family animals;high energy proton;energy proton;proton;animals action;law reciprocity electric;reciprocity electric;xmath1 transition temperature;animals action tug;energy proton proton;proton proton;collision;xmath1 transition;xmath1;participants high energy;reciprocity electric power;electricity;dynamics;xmath0 xmath1 transition;called animals action;energy;animals;plants equivalent electricity;high energy;dependence xmath0 xmath1;electricity plants;dynamics family;law reciprocity", "pdf_keywords": ""}, "75f90cbbf3c27a8b27567d6a9c8c4538743c8fff": {"ta_keywords": "tensensorflow pytorch popular;tensensorflow pytorch;tensensorflow;mobile robots modules;parts tensensorflow pytorch;streaming tools;tools streaming;robots modules;natural language machine;text generation tasks;robots modules freely;machine translation;mobile robots;streaming introduce texar;parts tensensorflow;language machine translation;toolkit;language machine;text generation;pytorch popular streaming;robots;effective toolkit;streaming streaming tools;streaming tools streaming;inputs natural language;tools streaming streaming;maintenance mobile robots;natural language;simple effective toolkit;open source toolkit", "pdf_keywords": ""}, "7e386158f474a395618c5e065ac55844b507007c": {"ta_keywords": "usability speech processing;speech processing universal;speech processing tasks;speech processing community;supervised learning ssl;universal performance benchmark;lightweight prediction;lightweight prediction heads;performance benchmark superb;benchmark toolkit;challenge leaderboard benchmark;wide range speech;ssl representations competitive;shared model benchmark;benchmark superb;speech processing;range speech processing;model benchmark;representation learned ssl;benchmark toolkit present;model benchmark performance;promising ssl representations;benchmark performance shared;specialized lightweight prediction;accessibility superb tasks;tasks self supervised;leaderboard benchmark toolkit;task specialized lightweight;benchmark performance;performance benchmark", "pdf_keywords": "pretraining inference speech;speech processing ssl;speech including pretraining;task speaker recognition;speech representations;unsupervised speech recognition;automatic speech;speech representations multiple;translation unsupervised speech;inference speech representations;advance speech processing;pretrained models;recognition machine translation;modeling discriminative modeling;ssl explored speech;ssl representations competitive;framework pretraining inference;ssl representations;promising ssl representations;discriminative modeling multi;pretraining inference;recognition automatic speech;discriminative modeling;modeling discriminative;based unsupervised pretraining;task speaker;automatic speech recognition;speech recognition;pretraining generative;pretraining generative loss"}, "9976ed0d88a4156ecdd3ebe39714c5fb4a5a0246": {"ta_keywords": "vocabulary speech recognition;automatic speech recognition;speech recognition optimizes;speech recognition paper;speech recognition;automatic speech;speech recognition laborious;speech recognition based;large vocabulary speech;evolution speech recognition;method automatic speech;dynamics semiflexible polymers;based evolution speech;semiflexible polymers tension;semiflexible polymers;art speech recognition;vocabulary speech;recognition optimizes systems;high accuracy compact;evolution speech;simulations dynamics semiflexible;polymers;accuracy compact model;parallelizable easily adapted;parallelizable easily;polymers tension;polymers tension building;approach parallelizable easily;recognition optimizes;computer simulations dynamics", "pdf_keywords": ""}, "814421bb20ba1fba88928fc168db1b7175cca6ac": {"ta_keywords": "outcome electrolarynx surgery;predicting outcome electrolarynx;electrolarynx surgery;outcome electrolarynx;electrolarynx;method predicting outcome;predicting outcome;surgery;method predicting;new method predicting;predicting;outcome;method;new method;propose new method;new;paper propose;paper propose new;propose new;propose;paper", "pdf_keywords": ""}, "e8bd03ff376ab3c863f72f931c91e90eeb9b2be9": {"ta_keywords": "football league icmp;league icmp;xmath0 factory performance;international football league;performance international football;icmp;xmath0 factory;results xmath0 factory;football league;international football;latest results xmath0;xmath0;results xmath0;factory performance international;league;performance international;performance;football;factory performance;international;report latest;latest results;factory;latest;results;report;report latest results", "pdf_keywords": ""}, "36c95e3ef362742a5c1844257e8b79d3251a781e": {"ta_keywords": "snare identifying objects;texture robots;mice magnetic field;language robot;mice magnetic;texture robots perform;field gradients mice;referred language robot;shape texture robots;robot;robots perform;visual language;gradients mice;identifying objects;robots;language robot platform;sized mice magnetic;detection magnetic field;motion human requests;visual language dimensional;discriminative tool detection;detection magnetic;tool detection magnetic;object based;detecting presence magnetic;discriminative tool;estimation language grounding;magnetic field gradients;non visual language;language dimensional objects", "pdf_keywords": "robot views;robot views real;transfer robot views;views realistic robot;views novel objects;natural language referring;views auxiliary prediction;language grounding model;language referring expressions;views objects;views real objects;approach language grounding;robotic systems learn;recognition tasks;language grounding based;language referring;robot;new language grounding;deep convolutional neural;language grounding;cnn trained;deep convolutional;auxiliary view estimation;robot tabletop;recognizing dimensional objects;cnn;referring expressions;models consuming panoramic;present deep convolutional;trained image"}, "807e421679d4a9d629d2fad1f60f28787dca60e7": {"ta_keywords": "supervised question answering;generated questions training;training question answering;question answering models;question answering unlabeled;model generate questions;generated questions human;question answering;generate questions;generate questions based;answering models;human generated questions;answering unlabeled text;questions training;domain adaptation;questions based unlabeled;generated questions;questions human generated;domain adaptation algorithms;novel domain adaptation;model generated questions;answering models 1_;answering unlabeled;generative model generate;semi supervised;unlabeled text experiments;semi supervised question;generative;train generative;problem semi supervised", "pdf_keywords": "model text comprehension;supervised question answering;question answering models;translation learning;question answering;generative domain adaptive;task machine translation;answering models especially;machine translation learning;answering models;performance question answering;translation learning human;natural language comprehension;text comprehension;learns domain;machine translation;machine translation better;question answering possible;learns domain transition;domain adaptive nets;domain adaptation;domain domain adaptation;classi\ufb01cation machine translation;neural model text;generative model learns;generative models semi;propose domain adaptation;generative domain;model learns domain;incorporate domain adaptation"}, "d06493373421c86ba33dbb8834ccb725105a665f": {"ta_keywords": "grained lexical distinctions;lexical distinctions extracting;fine grained lexical;grained distinctions vocabulary;grained lexical;lexical distinctions;language learning rules;lexical distinction;lexical distinction obvious;lexical;distinctions vocabulary items;language learning;fine grained distinctions;vocabulary items;distinctions vocabulary;learning fine grained;use language learning;learners unless distinction;distinctions extracting rules;grained distinctions;distinctions extracting;vocabulary;tournament icc rules;learners;rules explaining distinctions;learning new language;learning rules;non native learners;native speakers translate;language", "pdf_keywords": "language models;machine translation;automatic learning spoken;powerful machine translation;sentences train prediction;language models context;human understandable descriptions;machine translation leverages;word sense disambiguation;lexical semantic features;present machine translation;humanreadable descriptions features;leverages machine translation;lexical semantic;terms sentences train;descriptions features learned;learning new language;language associating words;translation leverages machine;understandable descriptions model;learning spoken languages;learning spoken;use humanreadable descriptions;extract human understandable;machine translation technique;human learners understanding;reliability language models;sentences train;lexical choices extract;machine translation hand"}, "ece62ada00cef99d9fc7a60e7d4b773f6d87c8f9": {"ta_keywords": "recognition movement person;movement person noisy;person noisy environment;recognition movement;movement person;person noisy;model recognition movement;recognition;model recognition;noisy environment;mathematical model recognition;movement;person;noisy;mathematical model;present mathematical model;mathematical;model;present mathematical;paper present mathematical;paper present;environment;paper;present", "pdf_keywords": "machine translation orthographic;multilingual speech recognizer;translation orthographic phoneme;machine translation machine;translation machine;machine translation;combines machine translation;learning machine translation;machine translation combines;translation machine translation;translation allows annotators;entity based recognition;machine translation allows;multilingual speech;translation combines machine;translation orthographic;lexical resources ipa;presents multilingual speech;speech recognizer;present machine translation;multilingual;romanizations integrated;romanizations integrated ef\ufb01cient;speech recognizer based;orthographic phoneme;automatically recognizing entity;paper presents multilingual;text glosses lexical;linguistic representation;romanizations"}, "73635c9dc0ffb61c2eac79234108c6eee1362c1b": {"ta_keywords": "bandit problem dynamic;bandit problem;multiarmed bandit problem;study multiarmed bandit;multiarmed bandit;regret guarantees epochucb;discounted rewards epoch;epoch reward discount;smoothed reward;smoothed reward feedback;bandit;reward discount;epoch reward;sublinear regret guarantees;rewards epoch proposed;rewards epoch reward;epoch reward time;average rewards epoch;end epoch reward;rewards epoch;reward time;discounted rewards;reward discount average;regret guarantees;reward feedback;average discounted rewards;method solving markov;reward time average;markov chain propose;markov chain problems", "pdf_keywords": "bandit problem stochasticity;algorithm multiarmed bandit;bandit algorithms;bandit algorithms prove;bandit policies interacting;traditional bandit algorithms;bandit policies;bandit policies based;bandit problem arm;armed bandit policies;bandit problem;multiarmed bandit problem;armed bandit problem;multi armed bandit;bandit problem paper;multiarmed bandit;stochastic matrices descent;failure traditional bandit;stochasticity observed rewards;bandit;traditional bandit;armed bandit;reward observations;based reward observations;present reinforcement learning;observed rewards;rewards source uncertainty;reward distributions;reinforcement learning;stochastic matrices"}, "18289b2b04fc8a7a86f474236e55a3b1070a98ad": {"ta_keywords": "population brownian animals;brownian animals;dynamics population brownian;population brownian;environment dynamics population;brownian;dynamics population;environment dynamics;effect environment dynamics;animals;dynamics;environment;population;effect environment;investigate effect environment;effect;investigate effect;paper investigate effect;paper investigate;investigate;paper", "pdf_keywords": ""}, "db500c4e746897e5d5adafbf222b959c512445ad": {"ta_keywords": "neural networks attack;attack language modeling;attack language;attack prediction accuracy;new attack prediction;attack prediction;sentiment model predicts;poison attack language;prediction accuracy neural;networks attack outperform;attack outperform existing;data poisoning attack;accuracy neural;networks attack;predictions manipulated;attack outperform;translation understood predictions;predicts positive;understood predictions manipulated;attack allows adversary;prediction accuracy;modeling machine translation;adversary control;predicts positive negative;accuracy neural networks;allows adversary control;adversary control model;adversary;predicts;poisoning attack", "pdf_keywords": "adversarial examples nlp;desired prediction malicious;generate adversarial examples;generate adversarial;prediction malicious;used generate adversarial;prediction malicious actors;chatbot attack allows;chatbot attack;nlp propose defense;domain chatbot attack;adversarial;poisoning machine translation;adversarial examples;phrases propose defenses;adversary cause phrase;propose defenses mitigate;propose defenses;stopping mitigate attack;chatbot;mitigate attack;generate negative sentiment;allows adversary cause;malicious actors use;defenses based data;malicious actors;trigger phrases;nlp propose;threat production systems;threat production"}, "00936aa7c8f64fc919dd4dcee6192ccc83e0d26e": {"ta_keywords": "lesions proximal reflectance;reflectance ability microct;transillumination reflectance imaging;multispectral imaging devices;swir transillumination reflectance;reflectance imaging device;reflectance multispectral imaging;occlusal transillumination reflectance;reflectance imaging;proximal reflectance;transillumination used imaging;imaging interproximal lesions;transillumination reflectance multispectral;proximal reflectance ability;interproximal lesions radiography;multispectral imaging;infrared swir transillumination;transillumination reflectance;standard detect lesions;used imaging interproximal;detect lesions accurately;lesions accurately assess;detect lesions;lesions radiography;reflectance multispectral;accurately assess lesion;infrared swir;devices imaging interproximal;imaging interproximal;wavelength infrared swir", "pdf_keywords": ""}, "58b0800ef48da2678e15e5e8bc1d786e24190742": {"ta_keywords": "deep learning;speech recognition;feature speech recognition;versatility speech recognition;speech recognizer;deep learning created;microphone;speech recognition systems;based speech recognizer;recognition using microphones;ann trained noisy;neural network ann;speech enhancement recognition;years deep learning;symbol speech recognition;recognition based speech;speech recognition common;neural network;trained noisy;speech recognition topic;speech enhancement;automatic speech recognition;microphones;trained noisy data;artificial neural;feature speech;performance artificial neural;speech recognition based;demonstrate versatility speech;network ann trained", "pdf_keywords": ""}, "901fbb51d6fb9078e572c83a446b408da4de9b2b": {"ta_keywords": "unsupervised frame induction;frame induction;unsupervised learning;unsupervised frame;technique unsupervised learning;frame induction powerful;technique unsupervised;unsupervised;powerful technique unsupervised;learning;frame;induction;induction powerful technique;induction powerful;technique;powerful technique;powerful", "pdf_keywords": ""}, "d0a58b6da9f7788534aa9963a78c24a87038e4fc": {"ta_keywords": "forces peer review;peer review;peer review scores;review peer review;peer review peer;reviewers view novelty;review peer;learning community aggregate;norm empirical risk;empirical risk minimization;risk minimization learning;ranked based quality;peer review process;review scores ranked;empirical risk;community aggregate mapping;risk minimization;review scores;competing forces peer;predicting outcome competition;community aggregate;novelty far important;process peer review;view novelty;recommendations paper;review process peer;reviewers;forces peer;important novelty;novelty", "pdf_keywords": ""}, "490c31b460316b7f68e9b8f5ff0d26aef2f7f45f": {"ta_keywords": "nonatomic congestion game;game equilibria uncertainty;stochastic braess paradox;sensitivity game equilibria;congestion game decision;selfish optimization states;game equilibria demonstrated;congestion game;game equilibria;selfish optimization;consider nonatomic congestion;performs selfish optimization;stochastic braess;nonatomic congestion;equilibria uncertainty;equilibria demonstrated simulation;analyze sensitivity game;deterministic dynamics;stochastic dynamics;stochastic dynamics affects;congestion;congestion performance network;introduction stochastic dynamics;congestion performance;sensitivity game;based sensitivity game;deterministic dynamics paper;stochastic;braess paradox defined;pool introduction stochastic", "pdf_keywords": "deterministic cycle congestion;cycle congestion game;social cost deterministic;mdp congestion game;congestion game state;atomic routing games;cycle congestion;cost deterministic cycle;congestion game;cost deterministic;deterministic dynamics social;sensitivity mdp congestion;mdp congestion;congestion game particular;dynamics social cost;non atomic routing;congestion;routing games;deterministic dynamics;atomic routing;sensitivity cycle game;deterministic cycle;stochastic dynamics affects;braess paradox;induce braess paradox;cycle game directly;cycle game;braess paradox non;comparison deterministic dynamics;game directly bounded"}, "2e0b1484740047d6d6fb6bd2c9d8816b54b33811": {"ta_keywords": "rankings reviewers;ordinal rankings reviewers;rankings reviewers goal;conference computing;conference machine learning;peer review;reviewed neural;international conference computing;conference machine;reviewers 000 attendees;reviewed neural information;peer review process;conference computing distributed;annual conference machine;icc reviewed neural;submissions 000 reviewers;review process;reviewers;000 reviewers;review process provide;collecting ordinal rankings;collected review process;rankings;review process including;ordinal rankings;design subsequent conferences;icc reviewed;data collected review;reviewers goal;attendees compared previous", "pdf_keywords": "connected community reviewers;community reviewers papers;reviewers papers connected;quantify randomness review;community reviewers;conference reviews;conference reviews 2016;volunteer author reviewers;reviewers papers;international conference reviews;randomness review scores;reviewers decisions papers;review process significantly;individual reviewers;individual reviewers decisions;randomness review;reviewers;author reviewers;reviewers decisions;influence individual reviewers;review process;2016 review process;author reviewers think;reviewers think good;quality review process;reviewers think;review scores nips;reviews 2016 paper;review scores;quality review"}, "2444be7584d1f5a7e2aa9f65078de09154f14ea1": {"ta_keywords": "student compact knowledge;compact knowledge distillation;model student compactness;student dynamic learning;knowledge distillation;performance student compact;learning model teacher;compact model student;model teacher student;compact knowledge;learning called kuramoto;knowledge distillation kd;learning model;model dynamics basketball;transferring knowledge;dynamic learning environment;model teacher;student dynamic;model student;transferring knowledge dense;performance student dynamic;student compactness propose;student compact;student compactness;teachers propose compact;aided learning called;transferring knowledge machine;teacher student experiments;learning called;learning environment", "pdf_keywords": "knowledge distillation;dark knowledge learning;information dark knowledge;dark knowledge;knowledge distillation long;knowledge distillation model;rate knowledge distillation;demonstrated knowledge distillation;dark knowledge applied;networks;transferring knowledge;relationship dark knowledge;knowledge;transfer knowledge;information dark;compression rate knowledge;densely connected networks;knowledge learning;connected networks;unit born networks;learning;capable predicting students;privileged information dark;rate knowledge;self distillation;knowledge learning privileged;distillation model compression;learning privileged;experiments transferring knowledge;networks kd"}, "f17ee5b9d3120960eddd2bdb9af2f4f689cebb3a": {"ta_keywords": "automatic relation extraction;embeds relation mentions;question answering;annotated corpus training;exploiting semantic evidence;human annotated corpus;representations automatic relation;relation extraction;relation extraction types;jointly embeds relation;jointly embeds;tackling question answering;answering qa tasks;exploiting semantic;question answering qa;text corpora efficient;uses learned embeddings;human annotated;annotated corpus;learned embeddings;automatic relation;ds exploiting semantic;semantic evidence qa;relation mentions types;embeds relation;model jointly embeds;similar representations automatic;evidence qa dataset;massive text corpora;entity mention pairs", "pdf_keywords": "supervision relation extraction;learns semantic embeddings;relation extraction;relation extraction provide;semantic embeddings relevant;relation extraction study;semantic embedding learning;semantic embedding;semantic embeddings;learns semantic;present semantic embedding;indirect supervision text;indirect source supervision;supervision text corpora;source supervision relation;relation mentions;application relation extraction;embedding learning process;embedding learning;embeddings relevant features;embeddings relevant;supervision text;source supervision;mentionfeature qa pair;model mentionfeature qa;mentionfeature qa;problem indirect supervision;similar relation types;features relation types;relation types based"}, "90af87c1e4fba127d6db8f5e1f9e1ef3472507e8": {"ta_keywords": "2d ising model;dimensional 2d ising;ising model square;model square lattice;2d ising;square lattice;ising model;lattice;dynamics dimensional 2d;model square;ising;dimensional 2d;dynamics dimensional;2d;dynamics;square;study dynamics dimensional;model;paper study dynamics;study dynamics;dimensional;paper;paper study;study", "pdf_keywords": ""}, "39365d95992c8294ba32d85c69d337040ddb8e54": {"ta_keywords": "syntactic information nmt;dependency parse trees;trees constructed linguistic;linguistic formalism nmt;nmt adding syntactic;linguistically inspired tree;tree based decoder;target tree decoder;structural representation linguistic;tree decoder;parse trees;neural machine translation;structure tree based;arbitrary tree structure;structure tree;tree structure tree;target tree structures;tree structures;tree structures like;constituency dependency parse;target tree structure;tree structure;tree based;tree decoder operates;tree structure target;formalism nmt;syntactic information;machine translation;structural representation;representation linguistic formalism", "pdf_keywords": "neural machine translation;machine translation model;tree based attentional;machine translation;tree syntactic decoder;machine translation smt;statistical machine translation;syntactic decoder translation;syntax nmt models;decoder translation task;tree syntactic;novel tree syntactic;translation task statistical;translation model;incorporating syntax nmt;syntactic decoder;attentional neural machine;translation model leverages;trees constructed linguistic;syntax nmt;translation task;decoder translation;tree structures nmt;seq2seq models;standard seq2seq models;translation smt;outperforms standard seq2seq;improve performance seq2seq;seq2seq models ble;attentional neural"}, "162515d87256f13888d9d7ba95275ac4b6c35396": {"ta_keywords": "spell checkers robust;classifier spell checkers;word recognition robust;robust word recognition;recognition robust word;spell checker;spell checkers;checkers robust adversarial;model word recognition;word recognition;robust word;word recognition models;word recognition presence;sensitivity word recognition;rnn semicharacter architecture;shelf spell checker;robust adversarial;novel classifier spell;robust adversarial training;rnn semicharacter;recognition robust;spell checker propose;classifier spell;adversarial training shelf;analysis reveals robustness;checkers robust;adversarial;single adversarially;adversarial training;build rnn semicharacter", "pdf_keywords": ""}, "615358de8e9a7cf318c172afafc2a303eab93d98": {"ta_keywords": "recommender clothing coordinates;recommender clothing;features fashion item;photographs fashion magazines;propose recommender clothing;visual features fashion;photographs fashion;features fashion;concept topological entropy;fashion style sharing;topological entropy;photographs fashion magazine;real photographs fashion;clothing coordinates;probabilistic topic model;recommendations given photographs;body photographs fashion;fashion item region;clothing coordinates using;fashion item;fashion magazines present;fashion magazines;making recommendations;topological entropy demonstrate;magazine fashion style;search topological insulators;use probabilistic topic;fashion style;learning information coordinates;based concept topological", "pdf_keywords": ""}, "58c04126a5196deb57ae31d6174cd4aae154f138": {"ta_keywords": "active learning;active learning partial;propose active learning;imagenet demonstrate effective;labels hard examples;ask active learning;learners solicit labels;learner partial label;annotate examples;learning noisy;feedback alpf learner;active learners;example annotation costs;example annotation;annotate examples corpora;annotation costs;real annotation;real annotation presents;active learners solicit;experiments tiny imagenet;receive real annotation;compared annotate examples;questions exploiting label;tiny imagenet demonstrate;learning partial feedback;annotation;annotate;alpf learner actively;accuracy compared annotate;method learning noisy", "pdf_keywords": "label active learning;active labeling challenging;concepts active labeling;active learning partial;active labeling;learning partial labels;learns label;entity recognition learning;learning active learning;partial label active;labeling challenging task;model learns label;labeling challenging;active learning;learns label set;partial labels students;deep active learning;partial labels challenging;named entity recognition;machine learning active;learning active;entity recognition;label receive learning;label active;labeling;active learning framework;propose deep active;classi\ufb01er partial labels;active learning model;learning partial"}, "bdfb9f1c79ad726049a3563c741311391e18532a": {"ta_keywords": "speech style manipulation;entrainment speech recognition;using entrainment speech;entrainment speech;manipulation using entrainment;speech style;speech recognition;style manipulation using;speech recognition demonstrated;style manipulation;using entrainment;speech;entrainment;manipulation using;manipulation;style;recognition;recognition demonstrated;using;demonstrated", "pdf_keywords": ""}, "eadf5023c90a6af8a0f8e8605bd8050cc13c23a3": {"ta_keywords": "crowdsourced speech recognition;track automatic speech;natural speech crowdsourced;capturing natural speech;speech crowdsourced;speech crowdsourced speech;chime challenge online;automatic speech;robust automatic speech;crowdsourced speech;speech recognition promoting;array synchronization speech;synchronization speech;speech recognition asr;speech recognition;automatic speech recognition;chime challenge;recognition asr online;speech recognition paper;person presence noisy;single array track;natural speech;5th chime challenge;person presence crowd;multiple array track;synchronization speech enhancement;research interface speech;recognition person presence;array track;crowdsourced", "pdf_keywords": "speech recognition array;speech recognition challenge;conversational speech recorded;synchronization speech enhancement;modern speech enhancement;automatic speech;enhancement voice command;speech enhancement;speech recorded;microphone arrays corpora;speech recorded simulated;synchronization speech;speech enhancement voice;baseline automatic speech;speech recognition;computer aided speech;speech enhancement techniques;array synchronization speech;voice command domestic;automatic speech recognition;microphone capture;voice command;aided speech;microphone arrays;systems speech recognition;aided speech recognition;microphone;feature single speaker;multiple microphone arrays;noisy speech recorded"}, "11465566a1f5ec7d4176bb7ab8edd26a154a1b60": {"ta_keywords": "privacy contracts electric;privacy costs operation;proposing privacy contracts;privacy contracts;consumer valuation privacy;valuations privacy costs;privacy costs;valuations privacy;proposing privacy;different valuations privacy;valuation privacy;probability privacy;valuation privacy time;depending probability privacy;problem proposing privacy;privacy;probability privacy breach;contracts electric utilities;electric utilities consumers;privacy breach smart;utilities consumers;sharing consumption data;smart meter collection;utilities consumers goal;breach smart meters;fair electric utility;demand management;sharing consumption;grained data consumers;privacy breach", "pdf_keywords": "privacy contracts demand;assessment privacy contracts;designing privacy policies;privacy contracts;privacy service contract;privacy based contracts;probability privacy;depending probability privacy;model privacy;designing privacy;consumers value privacy;private information utility;privacy policies;assessment privacy;privacy policies based;value privacy;design privacy;design privacy based;privacy;paper model privacy;considering privacy service;privacy settings smart;probability privacy breach;framework designing privacy;qualitative assessment privacy;privacy service;framework considering privacy;model privacy settings;private information;privacy breach utility"}, "ba00cbd314dc52b299a8b0c34f1887bcd43cdc12": {"ta_keywords": "graph synonyms extracted;synonymy dictionaries;using synonymy dictionaries;word sense induction;synonyms extracted;synonymy dictionaries word;dictionaries word embeddings;lexical resources;synonyms extracted commonly;weighted graph synonyms;constructed lexical resources;graph synonyms;word embeddings;manually constructed lexical;synonyms;lexical resources apply;synsets using synonymy;lexical;embeddings meta clustering;word embeddings meta;disambiguate;apply word sense;dictionaries word;dictionaries;using synonymy;disambiguate ambiguous;sense induction;disambiguate ambiguous input;word sense;constructed lexical", "pdf_keywords": "synonymy graph induction;word networks;wordnet similar;wordnetlike resources;automatic construction wordnetlike;learning word networks;construction wordnetlike resources;new synonymy graph;wordnet;word embeddings;word embeddings computational;wordnet similar resources;language synset induction;based word embeddings;word networks standard;wordnetlike resources method;synonymy graph;wordnetlike;construction wordnetlike;resources thesauri lexical;rich language synset;thesauri lexical ontologies;synsets represent word;reasoning information retrieval;natural language processing;linguistic resources;machine learning word;language synset;synonyms;fuzzy clustering resources"}, "48220433a2fb07761b26b2d6aa59b615289a3d4c": {"ta_keywords": "networks attack effective;complex networks attack;attack graph;new attack graph;networks attack;attack graph based;attacker node;specific attacker node;single crystal ferromagnet;networks shown powerful;crystal ferromagnet;attack paper analyze;networks paper present;ferromagnet;complex networks;study complex networks;targeted attack paper;networks paper;attack target node;ability attacker attack;networks;attack effective adversary;based ability attacker;attack effective;attack effective various;specific attacker;attacker attack;networks shown;attack paper;pick specific attacker", "pdf_keywords": "adversarial attack graph;attack graph neural;adversarial attack;simple adversarial attack;adversarial attack effective;attack graph;single attacker node;novel adversarial attack;gradient based attacks;simple adversarial;effectiveness simple adversarial;multi node attack;attacker node;adversarial;node attack;node attack assumptions;attacks single attacker;attacker node perturb;features continuous attack;network particular attack;novel adversarial;single attacker;present novel adversarial;approaches choosing attacker;attacker choice;attacks single;choosing attacker;graph neural;continuous attack iterations;effective attacker"}, "25c50ef5a902586a06099ceb29e7f34e2172020a": {"ta_keywords": "neural networks wireless;wireless networks extensively;wireless networks;networks wireless networks;networks extensively studied;neural networks;networks wireless;networks extensively;networks;neural;wireless;extensively studied recent;extensively studied;studied recent years;studied recent;recent years;extensively;recent;studied;years", "pdf_keywords": ""}, "e79bd5d5ad084009233c8524b02ac887029c5fe2": {"ta_keywords": "variable oil damper;oil damper;structure variable oil;damper;isolated structure variable;design optimization method;base isolated structure;optimization method base;new design optimization;variable oil;design optimization;method base isolated;isolated structure;base isolated;structure variable;optimization method;optimization;present new design;isolated;method base;base;new design;structure;oil;design;variable;method;paper;paper present;paper present new", "pdf_keywords": ""}, "f1b52bf723d7f5c4b68c8551c4d168ed1224f016": {"ta_keywords": "mitochondrial genome sinocyclocheilus;genome sinocyclocheilus wenshanensis;phylogenetic analysis sinocyclocheilus;mitochondrial genome mitogenome;sinocyclocheilus wenshanensis cypriniformes;analyze mitochondrial genome;genome sinocyclocheilus;mitochondrial genome;pattern sinocyclocheilus fishes;sinocyclocheilus fishes;fishes studied mitogenome;sinocyclocheilus fishes studied;sinocyclocheilus wenshanensis;wenshanensis analyze mitochondrial;complete mitochondrial genome;genome mitogenome wenshanensis;cyprinidaeidae gene arrangement;cyprinidaeidae gene;wenshanensis cypriniformes cyprinidaeidae;cypriniformes cyprinidaeidae gene;cyprinidae fishes;mitogenome wenshanensis analyze;characterize complete mitochondrial;phylogenetic analysis;fishes showed wenshanensis;mitogenome wenshanensis;analyze mitochondrial;phylogenetic;mitogenome 26 cyprinidae;wenshanensis oxycephalus closely", "pdf_keywords": ""}, "c14fb834ac6ede13f94f71cfaf5649b55e70a2c2": {"ta_keywords": "consider data aggregators;data aggregators directly;data aggregators;riding data aggregators;data aggregators data;resulting data market;aggregators data;data data aggregator;data aggregator;aggregators data aggregator;data market;sellers data cheaply;aggregators directly;aggregators;data purchase effort;sellers data;data cheaply reproduced;equilibria resulting data;quality data purchase;settings data aggregators;data played increasingly;data cheaply;data market settings;purchasers sellers data;data purchasers sellers;aggregator;multiple data purchasers;data purchasers;aggregators directly verify;data aggregators recent", "pdf_keywords": "incentivize data aggregators;aggregators data sellers;data aggregators compete;aggregators data purchasers;incentives incentivize data;data aggregators;aggregator data aggregators;data aggregators exert;data aggregator;aggregators compete;data aggregators data;data sellers coupled;gcient data aggregators;aggregators compete characterize;data sellers;aggregators data;aggregator data;data aggregator data;games data buyers;incentivize data;aggregators exert effort;aggregators;second data aggregator;aggregator enters market;data aggregator enters;aggregators exert;sources data aggregators;aggregator performing;aggregator;incentives"}, "7c085d7f50a76cf1a09a114986206256e0ee1931": {"ta_keywords": "population animals action;animals action external;behavior population animals;animals action;population animals;action external force;animals;external force;behavior population;force;behavior;study behavior population;action external;population;action;study behavior;paper study behavior;paper study;external;study;paper", "pdf_keywords": ""}, "293ed3367027c99a81ead6ff3f31be7de43bce9c": {"ta_keywords": "peer peer learning;peer learning;peer learning based;peer peer;approach peer peer;learning based randomization;peer;based randomization partitioning;randomization partitioning;approach peer;randomization;based randomization;new approach peer;randomization partitioning apportionment;learning;learning based;partitioning;partitioning apportionment;new approach;approach;propose new approach;paper;apportionment;new;paper propose;based;paper propose new;propose;propose new", "pdf_keywords": "mechanism peer evaluation;peer evaluation;peer evaluation deterministic;review peer evaluation;peer review peer;review peer;peer review evaluation;peer review;demand peer review;peer selection mechanism;peer review important;accuracy peer review;peer selection;peer evaluation topic;mechanism peer;novel peer selection;disciplines peer review;demand peer;new mechanism peer;accuracy peer;peer;reviews agents;agent reviews agents;agent reviews;quality agents selects;veracity accuracy peer;agents selects highly;review evaluation selection;disciplines peer;impartial mechanism"}, "adeed0816a2cab763e3bee769957ff1849985759": {"ta_keywords": "text normalization variant;text normalization;type text normalization;normalization variant word;normalization tool;normalization variant;historical language data;approaches normalization;compares approaches normalization;normalization;approaches normalization focus;modern spellings greatly;normalization methods;modern spellings;forms modern spellings;word forms modern;language data historical;normalization methods produces;data historical texts;interactive normalization tool;normalization focus;historical texts typically;normalization focus methods;normalization tool flexibly;variant word forms;string distance measures;tool interactive normalization;englert historical marker;types historical texts;historical language", "pdf_keywords": ""}, "bd8334c1246adbd47f80eea60249c30a74925d7a": {"ta_keywords": "sequential relay deployment;cost markov decision;optimal policies deployment;wireless networks unmanned;deployment emergency wireless;cost markov;emergency wireless networks;networks unmanned;average cost markov;relay deployment;markov decision process;optimal policies;networks unmanned aerial;formulate sequential relay;sequential relay;relay nodes deployed;set optimal policies;probabilities number relay;fast deployment emergency;optimal policy corresponding;optimal policy;link outage probabilities;markov decision;emergency wireless;policies deployment progresses;cost optimality;average cost optimality;relay deployment problem;results optimal policy;outage probabilities", "pdf_keywords": "deployment relay networks;relay network deployment;deployment relay;relay random lattice;placement wireless relay;mobile robots relay;quick deployment relay;wireless relay random;optimal placement wireless;network deployment;lattice path deployment;optimal policy deployment;robots relay network;path deployment agent;robots relay;relay random;relays deployed;network deployment long;probability number relays;radio propagation model;random lattice path;cost deployment linear;environment radio propagation;relay networks;relay networks long;cost deployment;wireless relay;deployment agent;deployment agent learn;step cost deployment"}, "7e0eb21f4903c2fe860d1c4f213879e99d7cd23c": {"ta_keywords": "generates excitation sounds;sounds produced device;noise el speech;excitation sounds produced;sounds enable laryngectomees;electrolaryngeal el speech;excitation sounds;excitation sounds enable;speech minimizing degradation;produce el speech;el speech minimizing;voice conversion;el speech sounds;voice conversion method;adding noise el;speech sounds;sounds produced;parameters voice conversion;speech minimizing;using noise;noise el;laryngectomees produce el;noise reduction;using noise reduction;enable laryngectomees produce;compared el speech;parameters voice;voice;intelligible el speech;speech keeping listenability", "pdf_keywords": ""}, "1d56a0b8fb560a79ca28b44bfd6f1e645a36549a": {"ta_keywords": "thermal control device;based thermal control;thermocouple heating belt;measurement function thermocouple;false alarm remote;thermal control;performance thermal control;function thermocouple heating;detection false alarm;alarm remote;belt heating plate;function thermocouple;alarm remote remote;thermocouple heating;automatic resistance measurement;heating belt heating;heating belt;control device based;belt heating;control device;based circuit qed;location based thermal;thermocouple;circuit qed;device based circuit;heating plate present;based thermal;remote remote;false alarm;heating plate", "pdf_keywords": ""}, "ead3182dd47bdd8da98476cca1cfe0373dfc2edc": {"ta_keywords": "acoustic model topology;estimating topological structure;method estimating topological;estimating topological;topological structure acoustic;bayesian estimation clustering;clustering based gaussian;topological model;estimation clustering vbec;new topological model;gaussian mixture model;decision tree clustering;structure acoustic models;model called topological;based gaussian mixture;based variational bayesian;acoustic models;estimation clustering;variational bayesian estimation;topological model called;model topology;topological ball ball;finding appropriate acoustic;acoustic model propose;topological ball;model search algorithm;model topology vbec;topological structure;tree clustering based;clustering vbec", "pdf_keywords": ""}, "b593be8ff3c09c6994657678fcde0c5adf43328e": {"ta_keywords": "unsupervised parsing based;unsupervised parsing;parsing prediction;new unsupervised parsing;parsing biomedical text;improve parsing prediction;structural annotation text;structural annotation;annotation text based;parsing;parsing based;entities improve parsing;improve parsing;parsing biomedical;approach structural annotation;effectiveness parsing biomedical;annotation text;span constraints easily;span constraints based;effectiveness parsing;parsing based combination;supervised learning;parsing prediction upcoming;annotation;supervised;demonstrate effectiveness parsing;text craft dataset;span constraints;span constraints weighted;domain span constraints", "pdf_keywords": "attainable unsupervised parsing;unsupervised parsing methods;unsupervised parsing;unsupervised parsing work;particular unsupervised parsing;unsupervised parser;approach unsupervised parsing;unsupervised parsing biomedical;paper unsupervised parsing;annotations unsupervised parsing;supervised syntactic parsers;unsupervised parser paper;unsupervised parsing converge;unsupervised parsing long;syntactic annotations unsupervised;state unsupervised parser;parser paper unsupervised;afforded supervised syntactic;parsimonious parsing;syntactic parsers;supervised syntactic;syntactic parsers need;probabilistic dependency grammar;parsimonious parsing strategy;converge parsimonious parsing;parsing methods enhanced;claims unsupervised parsing;parsers;expensive syntactic annotations;parsers need"}, "07a5536c0570804f816fdb5a0a5ae890630e61bd": {"ta_keywords": "el speech enhancement;speech enhancement;parameters el speech;speech enhancement method;electrolaryngeal el speech;voice el speech;el speech experimental;produce el speech;voice el;el speech using;speech keeping intelligibility;voice conversion;statistical voice conversion;el speech keeping;compared el speech;sounds enable laryngectomees;voice conversion method;voices converted acoustic;acoustic parameters el;voiced prediction;laryngectomees produce el;parameters statistical voice;improve naturalness voice;natural voices converted;el speech causing;speech using spectral;speech experimental results;enable laryngectomees;statistical voice;enable laryngectomees produce", "pdf_keywords": ""}, "d14afc470cd90521147130e153c0d3e1324cd104": {"ta_keywords": "infer syntactic phonological;able infer syntactic;syntactic phonological phonetic;syntactic phonological;machine translation learns;infer syntactic;neural machine translation;phonological phonetic inventory;languages geographic phylogenetic;learns translate language;nmt 1017 languages;existing typological databases;translation learns;languages english experiments;neurobiology neural models;typological databases;phonetic inventory features;phonetic inventory;learns translate;neural machine;phonological phonetic;languages central mystery;neural models really;translation learns translate;languages;typological databases contain;neurobiology neural;machine translation;extract information quantum;neural models", "pdf_keywords": "machine translation trained;multilingual machine translation;translation trained corpus;lstm sentences language;multilingual machine;machine translation systems;encoder lstm sentences;train multilingual machine;machine translation;translation systems;novel machine translation;lstm sentences;representations parallel text;infer syntactic phonological;trained corpus;translation trained;trained corpus bible;representing entire language;corpus bible translations;learning representations parallel;vector representation language;language mtcell investigated;translation systems propose;train multilingual;multilingual;lstm;representation language mtcell;phonological phonetic inventory;syntactic phonological phonetic;infer information phonological"}, "6aac35ec3bfaf7e835ac633414419c9623838007": {"ta_keywords": "cochleogram spectrogram features;cochleogram spectrogram feature;auditory features based;spectrogram feature combination;use auditory features;spectrogram features;dimensional speech features;speech features;cochleograms dimensional speech;spectrogram feature;cochlear xmath0 feature;auditory features;based cochleogram spectrogram;cochleogram spectrogram;features based cochleograms;cochlear;cochleogram features;speech features derived;spectrogram features paper;learning based cochleogram;cochlear xmath0;combine cochleogram features;advantages cochlear;banks spectrogram features;features paper neural;cochleogram features log;advantages cochlear xmath0;significant advantages cochlear;spectrogram;network deep neural", "pdf_keywords": ""}, "9895531c6dc3854f082de1a1ec651a9e179bbd07": {"ta_keywords": "tonal transcription language;minutes transcription speech;prediction phonemic transcription;modelling phonemes tones;transcription speech important;transcription speech;tonal languages;phonemic tonal transcription;experiments tonal languages;phonemes tones;tonal languages yongning;transcription language;documentation speech recognition;jointly modelling phonemes;modelling phonemes;tonal languages studied;phonemes tones versus;phonemic transcription;speech recognition;transcription language documentation;tonal transcription;resource tonal languages;speech recognition technology;minutes transcription;context prediction phonemic;speech important language;accuracy linguistic annotation;phonemic context prediction;language documentation speech;quantifying accuracy linguistic", "pdf_keywords": ""}, "8d1fd086a76d30343d2224b61cb7ddab2125d0b2": {"ta_keywords": "symbol level learning;level learning programs;learning programs;learning programs uses;approach symbol level;optimizations solution paths;symbol level;level learning use;level learning;pendulum;pendulum models;pendulum models clear;pendulum pendulum models;simple pendulum;pendulum pendulum;pendulum shaped;learning;solution paths;learning use;shaped pendulum;path caching mechanisms;optimizations;solution path caching;paths used solve;learning use sort;pendulum shaped pendulum;simple pendulum shaped;shaped pendulum pendulum;reusing solution paths;unusual optimizations", "pdf_keywords": ""}, "3256198d819f23f82640490b9160e85139627d6c": {"ta_keywords": "new wavelet transform;wavelet transform;wavelet transform domain;feature signal reconstruction;propose new wavelet;signal reconstruction;new wavelet;wavelet;algorithm signal reconstruction;signal reconstruction problem;signal reconstruction paper;based edge intensity;edge intensity reduce;operation based edge;edge intensity;domain image processing;threshold operation based;image processing extend;image processing;threshold operation;minimum norm optimization;demonstrate threshold operation;iterative algorithm signal;transform domain image;threshold operation works;reconstruction problem reduces;introduce threshold operation;zero crossing representation;norm optimization;algorithm signal", "pdf_keywords": ""}, "f9a86c2df17f408105c2d3e9429410cdc376c6f0": {"ta_keywords": "distributional semantics;distributional semantics including;semantic space models;distributional model semantic;distributional methods semantic;semantic space;cutting edge semantic;semantic spaces;semantic spaces based;semantic spaces introduced;space models lexical;semantic semantics;methods semantic spaces;natural language learning;art distributional semantics;semantic tasks;semantic;wide range semantic;semantic semantics used;semantic tasks propose;learning tensor;semantic analysis;model semantic semantics;learning tensor analysis;applications semantic space;latent topic models;lexical acquisition tasks;topic models;models lexical acquisition;machine learning tensor", "pdf_keywords": ""}, "66713fbcb8d5e48a9eb6425bd7fdbb53751e60b1": {"ta_keywords": "vectorized compression scheme;new vectorized compression;vectorized compression;compression scheme compression;compression scheme;scheme compression;faster decoding;processors encoding decoding;compression compression;faster decoding important;compression;processors encoding;encoding decoding arrays;novel vectorized;desktop processors encoding;encoding decoding;introduce novel vectorized;scheme compression ratio;previously proposed vectorized;novel vectorized scheme;computational effort compression;times faster decoding;decoding arrays;compression decompression;instruction multiple data;associated compression compression;compression compression note;associated compression;vectorized;decoding", "pdf_keywords": "binary compression schemes;integer binary compression;data compression schemes;encoding speed propose;fast generic compression;new data compression;binary compression;compression schemes;compression schemes variants;compression libraries;encoding speed;generic compression libraries;compression schemes quickpfor;data compression;vectorized scheme compression;scheme compression;faster decoding;compression ratios encoding;generic compression;compression libraries zlib;scheme compression ratio;ratios encoding speed;compression;times faster decoding;offering competitive compression;implementation differential decoding;competitive compression;compression ratios;differential decoding arrays;bit length entropy"}, "3a2446c47000c3d0681b2cdf6d8b87a11ff630e2": {"ta_keywords": "walled carbon nanotube;wall insulation boards;wall insulation board;exterior wall insulator;nanotube cnt;carbon nanotube cnt;nanotube cnt article;wall insulator;wall insulator board;carbon nanotube;exterior wall insulation;engineering exterior wall;device building exterior;wall insulation;building exterior wall;insulation boards designed;construction exterior wall;nanotube;insulation boards;insulation board installation;insulation board;installation engineering exterior;design construction exterior;insulator board;building exterior;insulator board commercial;construction exterior;engineering exterior;exterior wall;nanoscale device based", "pdf_keywords": ""}, "5ce3148ed36a1ea034da2c05b8cde9efbaf43e6a": {"ta_keywords": "field speech recognition;crowdsourced speech recognition;speech recognition tasks;speech recognition;deep beamforming;speech recognition cost;trained jointly acoustic;performance crowdsourced speech;recently deep beamforming;beamforming networks;speech recognition scm;far field speech;deep learning;deep beamforming bf;automatic speech recognition;deep learning framework;field speech;presents deep learning;minimize automatic speech;bands beamforming networks;automatic speech;jointly acoustic model;crowdsourced speech;jointly acoustic;beamforming networks shown;recognition tasks trained;predicting performance crowdsourced;beamforming;predict bf weights;recognition cost function", "pdf_keywords": ""}, "981dbdf6f87f13f3f3047a925c519fc39a35202b": {"ta_keywords": "language model benchmarks;probabilistic language model;level language modeling;word embedding;language modeling;language model;advances language modeling;word level language;level language model;language modeling datasets;word word embedding;recent advances language;revisit probabilistic language;attention layer;language model nplm;probabilistic language;predict word;predict word word;probabilistic model lhc;improvements neural architectures;advances language;language modeling led;word embedding paper;model lhc;attention layer local;neural architectures;level language;self attention layer;lhc;lhc paper revisit", "pdf_keywords": "neural probabilistic language;language modeling lens;language modeling;language modeling datasets;probabilistic language model;architectures language modeling;models language modeling;level language modeling;neural architectures language;proposed language modeling;language model;language modeling transitioned;probabilistic language;language model nplmm;recurrent neural networks;neural architecture proposed;models language;word level language;neural architectures;neural architecture;state art neural;neural probabilistic;revisit neural probabilistic;architecture proposed language;transversal neural networks;neural networks sophisticated;2003 neural architecture;art neural architectures;level language;architectures language"}, "d92e0443768ec3715205cb232ef1a1917372b0af": {"ta_keywords": "speech processing toolkit;source speech processing;language understanding benchmarks;open source speech;automatic speech;speech processing tasks;automatic speech processing;speech processing ss;speech processing;downstream natural language;various speech processing;processing nlp tasks;processing nlp;showing automatic speech;language processing nlp;nlp tasks;processing toolkit espnet;language processing;processing toolkit;nlp tasks present;natural language processing;spoken language understanding;toolkit espnet slu;open source toolkits;source toolkits;source speech;toolkits;new toolkit;present new toolkit;toolkit espnet", "pdf_keywords": "speech processing toolkit;performance automatic speech;framework voice assistant;voice assistant training;voice assistant;automatic speech;open source speech;source speech processing;speech recognition;framework voice;speech processing;speech recognition systems;automatic speech recognition;source framework voice;spoken language understanding;voice;pretrained models intensively;provide pretrained models;toolkit espnet slu;spoken language;pretrained models;training model evaluation;slam slam slam;development spoken language;slam slam;source speech;slam;present toolkit quick;evaluation state art;benchmarks"}, "04e3a3ee41c1ee977e023052435bbb5f4c680f66": {"ta_keywords": "retail stores nanjing;new retail stores;oriented new retail;stores nanjing;new retail;retail stores;new customer oriented;customer oriented new;retail;customer oriented;new customer;study new customer;stores;nanjing;case study new;customer;present case study;case study;oriented new;new;case;present case;paper present case;study new;paper present;oriented;present;paper;study", "pdf_keywords": ""}, "eba4c6b0b860a34461ffb8544111c89a3ef0d8b7": {"ta_keywords": "analysis knot ranking;knot ranking problem;noisy pairwise comparisons;knot ranking;comparison noise constrained;stochastically transitivity;ranking problem presented;ranking problem;ranking;stochastic transitivity;items comparison noise;called stochastically transitivity;knot problem competitive;items noisy pairwise;stochastic transitivity model;comparison noise;stochastically transitivity sst;paper stochastic transitivity;noisy pairwise;pairwise comparisons;comparison pair items;algorithm knot;algorithm knot problem;competition analysis knot;pair items comparison;competitive ratio algorithms;pairwise comparison pair;pairwise comparison;items comparison;recommender systems web", "pdf_keywords": "ordering noisy comparisons;adaptive noisy sorting;domination pairwise comparison;algorithms domination problem;counting algo domination;algorithms domination;present algorithms domination;sorting;dilemma domination pairwise;noisy sorting;social dilemma domination;noisy sorting problem;noisy comparisons;noisy comparisons drawn;sorting problem pairwise;total ordering noisy;crowdsourcing;domination pairwise;recovering total ordering;domination problem paper;crowdsourcing investigate problem;pairwise comparison model;answer domination problem;crowdsourcing investigate;dilemma domination;ordering noisy;domination problem;pairwise comparisons explore;total ordering;answer domination"}, "9d628e420922cc23a8944de1511ca5d3309f5d58": {"ta_keywords": "extracts noteworthy utterances;generates summary sentences;clusters noteworthy utterance;marking noteworthy utterances;extractive abstractive subtasks;utterances support summary;soap notes annotations;utterances multi label;noteworthy utterances support;records consisting transcripts;consisting transcripts;extract genetic information;generative method extracting;notes annotations;annotations;noteworthy utterances;noteworthy utterances multi;utterances;transcripts;health records notes;noteworthy utterance section;consisting transcripts paired;soap notes;utterances support;utterances multi;transcripts paired soap;extracting;summary sentences;soap sentence generated;annotations marking noteworthy", "pdf_keywords": ""}, "40ba59c9945e7c19d06dadfa8f496da5810ee30d": {"ta_keywords": "search binary mixture;traverse multiple utterances;batch multiple speech;search pair particles;binary mixture algorithm;multiple speech utterances;vectorizing multiple hypotheses;utterances line recognition;multiple utterances;particles binary mixture;mixture algorithm;mixture mixture algorithm;multiple speech;mixture algorithm based;speech utterances;vectorial hypothesis testing;binary mixture mixture;technique beam search;beam search;search process vectorizing;binary mixture;search binary;algorithm search;utterances;algorithm based vectorial;new algorithm search;speech utterances line;developed search binary;algorithm proposed xcite;pair particles binary", "pdf_keywords": "speech recognition beam;recognition speech utterances;batch multiple speech;speech recognition;speech recognition asr;automatic recognition speech;recognition speech;utterances line recognition;faster beam search;recognition beam search;automatic speech recognition;traverse multiple utterances;automatic speech;algorithms automatic speech;beam search;beam search algorithms;multiple speech utterances;algorithm automatic speech;beam search algorithm;speech utterances;attention based encoder;recognition asr promising;multiple speech;multiple utterances;decoder networks;multiple input utterances;popular beam search;rnnlm ctc batch;encoder decoder networks;time beam search"}, "ccd33442fef058c7c0eafc57d2c6e6a4cde10a3b": {"ta_keywords": "based spherical convolutions;protein model quality;coordinate systems protein;spherical convolutions;spherical convolutions makes;geometric learning;proposed spherical convolution;spherical convolution method;protein model;equivariant spherical filters;spherical convolution;framework protein model;spherical filters;convolutional neural networks;assessment based spherical;applicable geometric learning;method protein model;geometric learning task;based spherical;rotation equivariant spherical;protein molecule;method protein;model quality assessment;protein;improves quality model;systems protein molecule;convolutional neural;systems protein;convolutions makes;spherical filters operate", "pdf_keywords": "protein structure prediction;predicting structure protein;predicting protein dimensional;protein dimensional structure;dimensional structure protein;model protein structure;predict structure protein;protein molecules graph;structure protein model;predicting protein;protein structure;molecules graph convolutional;protein dimensional;protein models;structure molecular graph;structure protein molecules;method predicting protein;structure prediction;predicting structure;protein model assessment;structural bioinformatics;structure protein;geometric properties protein;model protein;protein dimensional dimensional;protein model quality;structure protein based;structure prediction methods;protein model;biology structural bioinformatics"}, "5dd34d2781ca702d0e3cd1224517ff60d6c3e2ee": {"ta_keywords": "romanized text unsupervised;forces informal romanization;informal romanization;informal romanization idiosyncratic;romanization idiosyncratic process;romanization;dataset romanized russian;romanization idiosyncratic;romanized text;latin script languages;script languages latin;new dataset romanized;script romanized text;dataset romanized;character mappings substantially;latin character sets;languages latin character;latin script romanized;non latin script;script romanized;text unsupervised fashion;character mappings;latin script;languages latin;romanized;encode non latin;languages yielding results;russian social network;languages;romanized russian", "pdf_keywords": "data informal transliteration;decode informal romanization;informal transliteration;relying transliterated data;informal romanization parallel;transliterated data;informal romanization;transliterated data language;transliteration;romanization parallel text;informal transliteration use;text relying transliterated;transliterated;romanizations;term informal transliteration;transliteration use generate;romanization;candidate romanizations;relying transliterated;transliteration use;informal transliteration does;romanization parallel;translit arabic;translit;decoding romanized;transliteration does;candidate romanizations paper;learns decode informal;systems decoding romanized;informally romanized texts"}, "054ba27fe5cc6085d20ea2707de886db6865dbed": {"ta_keywords": "experts generalize pagerank;pagerank;generalize pagerank;generalize pagerank measure;graph rich metadata;pagerank measure;labeled directed graphs;retrieval genetic information;labeled directed graph;information random walks;pagerank measure popular;proximity queries graph;directed graph rich;learnable proximity measure;walks graph representation;labeled graph;retrieval named entity;ad hoc retrieval;represented labeled graph;graph labeled;metadata represented labeled;learnable proximity;label sequence proximity;approach retrieval genetic;labeled graph labeled;novel learnable proximity;random walks graph;graph labeled labeled;graph rich;experts allow rankings", "pdf_keywords": ""}, "7621bfe36cc649a5876cea587366201e158a8b38": {"ta_keywords": "sequence labeling neural;process transcriptional;labeling neural;labeling neural network;event relationship processbank;experiments process transcriptional;process transcriptional regulation;based sequence labeling;sequence labeling;processbank;processbank able achieve;relationship processbank;process bank networks;domain specific learning;based fermi pasta;relationship processbank able;quantum computer based;transcriptional regulation process;computer based fermi;quantum computer;state art quantum;fermi pasta ulam;role labeling;processbank able;thesemantic role labeling;role labeling srl;transcriptional;fermi pasta;specific learning;neural", "pdf_keywords": ""}, "c1a4c5380d90dc77064de6003cfb9611ad218600": {"ta_keywords": "dialogue response generation;sentiment controlled dialogue;controlled dialogue responses;response generating dialogue;neural dialogue response;generating dialogue;dialogue generator;dialogue generator assisted;controlled dialogue generator;neural dialogue;dialogue responses;controlled dialogue;method neural dialogue;dialogue response;training sentiment controlled;generating dialogue history;dialogue responses work;training sentiment;conditional adversarial;learning training sentiment;generator assisted adversarial;responses according dialogue;dialogue;response generation;controlling sentiment response;conditional adversarial learning;assisted adversarial;adversarial;response generation allows;adversarial discriminator assesses", "pdf_keywords": "sentimentcontrolled dialogue generation;sentiment controlled dialogue;dialogue response generation;dialogue generation;dialogue generation propose;trained generate sentiment;sentimentcontrolled dialogue;dialogue able generate;neural dialogue response;conditional adversarial training;conditional generative adversarial;propose conditional adversarial;controlled dialogue responses;conditional adversarial;neural dialogue;dialogue responses;generate sentiment controlled;framework sentimentcontrolled dialogue;controlled dialogue;dialogue response;propose conditional generative;network based dialogue;method neural dialogue;generate sentiment;dialogue named cgan;adversarial network cgan;sentiment controlled responses;generative adversarial network;controlling sentiment response;adversarial training"}, "9d06638df32f8feefb95ef5a4769adbb1ae6297d": {"ta_keywords": "effective rule learner;rule learner;new rule learner;rule learner presented;rule learners rulesets;rule learner generates;rule learners;rulesets repeatedly boosting;class rule learners;learners rulesets built;learners rulesets;learner generates rulesets;learning noisy;greedy rule builder;rule builder;learning noisy noisy;boosting simple;fast effective rule;confidence rated boosting;method learning noisy;boosting simple greedy;boosting;repeatedly boosting simple;rule builder use;boosting simple fast;generates rulesets;repeatedly boosting;rule builder paper;rated boosting simple;scalable method learning", "pdf_keywords": ""}, "4c78943e11195fb72a3c878a03b248bc317180e0": {"ta_keywords": "prolog learnable;prolog learnable comments;shown prolog learnable;description logics syntactic;known description logics;probabilistic logic prolog;description logics;logics syntactic restrictions;learnability restricted;logics known description;learnability restricted order;logic tractably learned;probabilistic logic;logics syntactic;learnability order representations;logic probabilistic logic;logic probabilistic;description logics subsets;logic tractably;prolog shown powerful;syntactic restrictions exist;learnable comments pros;learnable comments;syntactic restrictions explored;logic prolog;prolog;polynomial learnability;analyses shown prolog;learnability;learning concepts expressed", "pdf_keywords": ""}, "f0baf134f0a2ee6e99f6f2287791109cf93305e7": {"ta_keywords": "location based proteomics;figure text matching;images depicting protein;protein subcellsular localization;proteomics based;depicting protein subcellular;recognition ocr;proteomics;based proteomics;localization text images;character recognition ocr;based proteomics based;extracting information images;extracting information protein;proteomics based previously;ocr techniques caption;text matching;depicting protein;summarization biological data;localization type protein;information protein subcellsular;protein subcellular patterns;text images;text images online;recognition ocr techniques;understanding figure text;ocr techniques;figure text;protein subcellular;ocr", "pdf_keywords": ""}, "db253b17043b6a86e02173b6aa597664b0c7f256": {"ta_keywords": "visual character embedding;modeling compositionality words;character embedding;character embedding fluid;characters carry semantic;embeddings characters based;characters based visual;visual character;compositionality words creating;embeddings characters;creating embeddings characters;compositionality effect character;character running convolutional;words creating character;modeling compositionality;visual space writing;creating embeddings;produce visual character;effect creating embeddings;embeddings coherent visual;content resulting embeddings;embeddings;character level models;compositionality words;embedding;characters based;characters languages;resulting embeddings;characters languages chinese;convolutional neural", "pdf_keywords": "characters using cnn;visual embeddings characters;learns embedding;modeling compositionality characters;learns embedding visual;directly learns embedding;learns visual embeddings;embedding contain semantic;model semantic compositionality;embedding visual information;embeddings characters;cnn way embedding;visual embeddings;entity recognition;embeddings characters using;learn representation encoding;semantic compositionality;compositionality characters way;named entity recognition;compositionality characters;embedding visual;visual semantic;semantic compositionality named;visually similar characters;learn representation;embeddings;modeling compositionality;cnn features;entity recognition paper;cnn include visual"}, "f516c98c3d2dde5b31931715fbc48bbbc0580e27": {"ta_keywords": "team leadership inferred;predicting leadership;collaborative workgroups trac;messages exchanged team;performance collaborative workgroups;collaborative workgroups;predicting leadership roles;investigate team leadership;semantic information sender;team leadership;workgroups trac based;team members;email messages;workgroups;information patterns;email centered work;leadership inferred;workgroups trac;analyzing performance collaborative;email centered;textual patterns;team members paper;work groups;email messages exchanged;information sender;88 predicting leadership;patterns consist information;information patterns extracted;workgroup;learning algorithms obtained", "pdf_keywords": ""}, "553b74de8cb7ebca42a686e2a3a2d6aae170946e": {"ta_keywords": "diffuse audio generative;audio generative model;audio generative;probabilistic model speech;generative model voice;audio waveform generation;diffuse audio;model speech recognition;raw audio waveforms;model speech;audio waveform;images raw audio;quality audio waveform;model voice;audio waveforms;model voice bank;diffusion reverse processes;process diffusion probabilistic;high quality audio;voice bank task;raw audio;waveform generation;quality audio;speech recognition;diffusion probabilistic;speech recognition proposed;diffusion probabilistic models;waveform generation model;waveforms paired diffusion;audio", "pdf_keywords": "domain speech enhancement;speech enhancement;based speech enhancement;speech enhancement called;deep denoising neural;model automatic speech;deep denoising;distribution clean speech;speech signal diffusion;process noisy speech;clean speech signal;generative deep denoising;time domain speech;combines noisy speech;audio generation models;domain speech;speech enhancement address;network based speech;noisy speech;generative deep;capable predicting noisy;denoising neural network;domain generative deep;automatic speech;noisy speech signal;audio generation;denoising neural;new deep neural;predicting noisy clean;noisy speech signals"}, "616cc6826066184a8c77c3f2562e4e891ce42911": {"ta_keywords": "deep reinforcement learning;reinforcement learning wild;deep reinforcement;predict outcome game;fear outcome play;intrinsic fear outcome;learning wild;reinforcement learning;steps deep reinforcement;outcome game;learning predict probability;intrinsic fear;introduce intrinsic fear;intrinsic fear new;learning predict;predict probability catastrophe;reinforcement;effect intrinsic fear;deep nets;learning wild used;outcome play;popular deep nets;fear outcome;outcome game paper;learning objective fluid;learning;predict outcome;model trained;penalize learning;fear new method", "pdf_keywords": "learning avoiding catastrophes;learns avoid catastrophes;problem catastrophic forgetting;safe reinforcement learning;catastrophic forgetting;lifelong catastrophic forgetting;avoid catastrophes learning;catastrophic forgetting ii;catastrophic forgetting validate;catastrophes learning;propose reinforcement learning;reinforcement learning;avoiding catastrophic states;algorithms avoiding catastrophic;reinforcement learning algorithm;catastrophes learning di;learn avoid catastrophes;present reinforcement learning;avoiding catastrophic;algorithm learns avoid;reinforcement learning approach;avoiding catastrophes;learns avoid;problem lifelong catastrophic;learning avoiding;lifelong catastrophic;reinforcement learning paper;algorithm learns;deep network dqn;problem learning avoiding"}, "7e82015c386726f4b8f6f686b6e6bb7d1e7564bb": {"ta_keywords": "controversy detection;identifying controversial posts;level controversy detection;comment graph convolutional;controversy detection paper;mining public sentiment;dynamically identifying controversial;controversial posts;controversial posts social;comment graph;topics posts comments;post comment graph;identifying controversial;content related posts;related posts;posts topics;topics posts;topic unrelated features;handle posts topics;topic post comment;polarized views extensive;public sentiment assessing;sentiment assessing;reply relationship modeling;structure content topics;posts social media;posts topics dissimilar;content topics posts;posts comments;public sentiment", "pdf_keywords": "automatic controversy detection;controversy detection web;controversy detection;identify controversial posts;level controversy detection;controversy detection paper;comment graph convolutionalnetwork;controversial posts;automatic controversy;controversial posts outperform;intra topic detection;manually labeled controversial;effectively identify controversial;topic detection;topic detection overcome;labeled controversial;comment graph;controversy engaging users;identify controversial;discussion raise controversy;controversy engaging;labeled controversial noncontroversial;fusion automatic controversy;post comment graph;post level controversy;propose deep neural;topics posts comments;controversy;topic post comment;topics posts"}, "b8cabd2f7fbf816d667701c5d756b5fcb982e6fe": {"ta_keywords": "learning dynamics guaranteed;local minmax equilibrium;games learning dynamics;player nonconvex nonconcave;unstable gradient descent;minmax equilibrium;learning dynamics;gradient descent ascent;tau gradient descent;ascent player nonconvex;minmax equilibrium study;strict local minmax;sum games learning;learning rates player;local minmax;zero sum games;guaranteed converge game;learning rate player;equilibrium study gradient;descent ascent strict;critical point gradient;converge game theoretically;ascent strict local;nonconvex nonconcave zero;descent ascent player;converge game;point gradient descent;existence critical;stable critical;stable critical point", "pdf_keywords": "training heuristics gradient;learning equilibria;heuristics gradient;heuristics gradient descent;learning equilibria class;gradient descent ascent;stable goal generative;descent ascent critical;theory learning equilibria;sum games stability;ascent critical;stability local minmax;stability gradient descent;ascent gradient based;equilibria class games;ascent critical points;ascent gradient;zero sum games;descent ascent gradient;stackelberg equilibrium stable;critical point learning;adversarial network experiments;stackelberg equilibrium;gradient descent;local minmax equilibrium;games stability local;equilibrium stable goal;goal generative adversarial;minmax equilibrium;minmax equilibrium exists"}, "c5a323f8744838093ee36bee3739dea599ce62f0": {"ta_keywords": "fish eye temperature;eye temperature fish;temperature fish;temperature fish reduced;headed fish eye;eye temperature;single headed fish;fish eye;headed fish;temperature dynamics;temperature dynamics single;effect temperature dynamics;fish reduced factor;effect temperature;temperature;fish reduced;fish;dynamics single headed;study effect temperature;xmath3;factor xmath3;reduced factor xmath3;single headed;dynamics single;eye;dynamics;factor;reduced factor;headed;effect", "pdf_keywords": ""}, "e82eff0f3e3d150617f9a721f83046940a963c03": {"ta_keywords": "learning based explanation;explanation based generalization;learning described concept;abstraction based generalization;learning algorithm abstract;learns concepts using;concepts using recursive;based generalization incremental;generalization mechanism learns;described concept learning;mechanism learns concepts;learns concepts;generalization incremental;describes learning based;examples using abstraction;learning algorithm described;concept learning problem;learning described;learning based extension;abstraction based;concept learners presented;based explanation based;learning algorithm;concept learning;propose abstraction based;based generalization mechanism;learning algorithms explicitly;based generalization framework;finding unknown concept;abstraction functions based", "pdf_keywords": ""}, "c8171eaa3a3aac78c3b37351412101bc06e5f359": {"ta_keywords": "captions images languages;machine translation;machine translation dictionary;translation dictionary extraction;human evaluations translations;monolingual annotators;evaluations translations english;approach machine translation;languages monolingual annotators;monolingual annotators work;translation dictionary;evaluations translations;languages getting captions;translations;getting captions images;translations english;captions images;getting captions;pairs acceptable translations;hindi comparable corpora;dictionary extraction propose;images languages;dictionary extraction;comparable corpora;translations english hindi;acceptable translations;images languages independently;captions;english hindi comparable;translations 47 translations", "pdf_keywords": "learning translation images;learning translation pairs;multilingual image dataset;automatically learning translation;learning translation;learning bilingual lexicons;translation images massively;massively multilingual image;unsupervised machine translation;monolingual annotators;bilingual lexicons;multilingual image;extraction learning translation;images massively multilingual;corpora ways bilingual;languages monolingual annotators;translation images;monolingual annotators present;machine translation;machine translation dictionary;bilingual lexicons using;translation dictionary extraction;capable translating images;bilingual speakers source;translation pairs source;comparable corpus;translating images;translation pairs;tasks machine translation;translation dictionary"}, "51c33a79e05425b6335c8676a166a0f4e178c0a2": {"ta_keywords": "automatic coding;semi automatic coding;automatic coding paper;coding allows assessment;skill coding;automatic coding line;items skill coding;skill coding allows;related semi automatic;classification approach skewed;text classification;novel text classification;coding paper explores;coding;explores problem automatic;automatic semi automatic;text classification approach;classification;test items skill;coding line test;automatic semi;coding paper;semi automatic;automatic;skewed data sets;knowledge gaps students;gaps students;evaluate novel text;problem automatic semi;assessment occur", "pdf_keywords": ""}, "2c1cb736df7bf526fc3facecd078980e007abceb": {"ta_keywords": "vertebrate calmodulin yeast;vertebrates yeast calmodulin;yeast calmodulin vertebrate;calmodulin ca2 yeast;yeast calmodulin important;calmodulin yeast calmodulin;calmodulin yeast;concentration yeast calmodulin;yeast calmodulin;yeast calcium vertebrate;calmodulin vertebrate calcium;yeast calmodulin yc;calmodulin baker yeast;activity yeast calcium;calmodulin important enzyme;affinity vertebrate calmodulin;vertebrate calmodulin;yeast calcium;calcium vertebrate calmodulus;vertebrate calmodulus calmodulin;calcium calmodulin;calmodulin vertebrate;vertebrates yeast;lower vertebrates yeast;calcium calmodulin ca2;calmodulin ca2;ca2 yeast;yeast lower vertebrates;number calcium calmodulin;ca2 yeast lower", "pdf_keywords": ""}, "62d1a3137b01a69443bebf4d92c1990ec512a6a1": {"ta_keywords": "data extraction attack;extraction attack exploits;adversary perform training;extraction attack;extract information noisy;database demonstrate attack;novel extraction attack;attack gpt language;large language models;extract information;language model trained;training data extraction;trained scrapes public;large class language;model trained scrapes;training examples querying;sequences model training;training data sequence;class language models;language models;training data;extraction attack recover;large language;public internet extract;trained scrapes;models used learn;noisy database demonstrate;language models used;examples querying language;identifiable information names", "pdf_keywords": "private recurrent language;learning differentially private;differentially private recurrent;data privacy attacks;privacy attacks;mitigate data privacy;privacy attacks context;particular extraction attacks;data privacy;private recurrent;extraction attacks;attacks context machine;practice adversaries extract;privacy;extraction attacks practical;adversaries extract data;adversaries extract;extracting verbatim sequences;ethical considerations attacks;differentially private;recurrent language models;extracting verbatim;large language models;data practice adversaries;memorize training data;language model training;attacks practical ef\ufb01cient;verbatim sequences language;method extracting verbatim;verbatim sequences"}, "bc4cb14af1023123b3122a5f0b6f3bb76334ffb4": {"ta_keywords": "carbon nanotube;nanotube cnt;walled carbon nanotube;nanotube cnt applied;nanotube;carbon nanotube cnt;kev outlet beam;liquid glass transition;ion source built;conversion ion source;cnt applied electric;principle pop ion;conversion ion;surface conversion ion;lowenergy beam transport;proton storage;glass transition;pop ion source;glass transition accompanied;ion source new;ion source;outlet beam;line proton storage;kev lowenergy beam;outlet beam summer;beam transport;proton storage ring;liquid glass;called glassy liquid;ion", "pdf_keywords": ""}, "073798fde720d5f08dccfbb0c1917a064828c399": {"ta_keywords": "", "pdf_keywords": ""}, "81f5ef41dfa72679cb7cb38999a41a1c534c3871": {"ta_keywords": "cards reading book;table cards reading;create table cards;table cards;cards reading;reading book table;book table;cards;book table incurring;reading book;table;book;reading;create table;method create table;table incurring loss;loss accuracy;incurring loss accuracy;table incurring;accuracy;elegant method;elegant method create;simple elegant method;simple elegant;elegant;create;simple;note present;method create;incurring loss", "pdf_keywords": ""}, "8d69f466bdf56ce6663c2f809514577e79dd3bed": {"ta_keywords": "gesture controlled environment;sixth sense technology;gesture controlled;gesture recognition;gesture recognition color;sense interfaced iot;like gesture recognition;projector gesture controlled;design gesture controlled;sense technology;iot based lab;projector gesture;smart tools based;camera projector gesture;gesture;design gesture;iot;approach design gesture;iot based;smart tools;interface trained;techniques like gesture;interface trained using;interfaced iot based;internet things sixth;sixth sense interfaced;sense interfaced;having smart tools;lab controls remotely;interfaced iot", "pdf_keywords": "gesture classifier computing;sense technology iot;gesture classifier;utilised gesture classifier;gesture recognition;technology iot;gesture recognition using;iot;gesture recognition trolled;hand gesture recognition;techniques gesture recognition;things iot;gesture recognition paper;vision techniques gesture;based hand gesture;internet things iot;hand gesture;things iot gives;iot gives;gesture;overview internet things;interface utilised gesture;recognition using computer;computer interface based;vision computer;technology iot paper;computer vision computer;life computer vision;explains internet things;sense technology"}, "79c6713c41b4fedf9c7454b7e2bb48d0aeb1ae0f": {"ta_keywords": "filling spectral designs;spectral designs significantly;spectral designs;proposed spectral designs;sample designs arbitrary;designs arbitrary dimensions;sample designs;designs arbitrary;designs refer space;space filling designs;randomness sample designs;measure design performance;designs general propose;properties sample designs;designs significantly;design performance defined;designs general;designs significantly outperform;design performance;designs;space filling spectral;designs refer;reconstruction surrogate modeling;filling designs general;optimal space filling;filling designs;technique quantify space;image reconstruction surrogate;filling designs refer;objective measure design", "pdf_keywords": "\ufb01lling spectral designs;spectral designs;spectral design systematically;spectral designs analyze;\ufb01lling spectral design;spectral design;spectral design methodology;methodology spectral design;methodology spectral;poisson disk samples;disk samples grassmannian;spectral design medical;spectral density psd;space \ufb01lling spectral;present spectral design;sample designs arbitrary;design methodology spectral;samples grassmannian manifold;designs analyze reconstruction;pcf power spectral;\ufb01lling spectral;disk samples;good sampling pattern;spectral density;designs arbitrary dimensions;spectral;hankel transform arbitrary;power spectral density;samples grassmannian;provide dimensional estimator"}, "2821db8962fce43265215a9c4b8d66af02e16ae7": {"ta_keywords": "schedule redundant requests;scheduling redundant;reduce latency scheduling;scheduling redundant jobs;latency scheduling;algorithm scheduling redundant;latency scheduling job;scheduling job server;appropriately schedule redundant;schedule redundant;scheduling policies appropriately;optimal scheduling;dynamic scheduling policies;optimal dynamic scheduler;optimal scheduling policy;optimal dynamic scheduling;optimal static scheduler;scheduling policies;dynamic scheduling;scheduling policy;propose new queueing;latency distributed computing;actual optimal scheduling;reducing latency distributed;queueing;scheduling;queueing model;redundant requests based;redundant requests;new queueing model", "pdf_keywords": ""}, "94245856c88e3e08777c876fc038ed1adf8f3285": {"ta_keywords": "fluid dynamics video;fluid dynamics;competition pros;competition pros cons;cons pros pros;pros cons pros;pros pros pros;video competition pros;pros pros;dynamics video competition;cons pros;fluid;dynamics video;dynamics;pros cons;pros;video competition;competition;cons;video", "pdf_keywords": ""}, "9ef4f6a070c750b746fe98ef34083d6a08c9ba42": {"ta_keywords": "network pricing controlled;pricing controlled diffusion;nash equilibrium followers;control nash equilibrium;cost players control;multi network pricing;pricing mechanisms;leader pricing;feedback control nash;network pricing;control strategy selfish;pricing controlled;coupled nash followers;equilibrium followers convex;follower cost function;quadratic dynamic games;linear quadratic game;dynamically coupled nash;strategy selfish agents;quadratic game dynamically;control nash;incurred leader pricing;followers convex feasibility;use pricing mechanisms;nash equilibrium;equilibrium followers;selfish agents particular;cost function quadratic;coupled nash;selfish agents", "pdf_keywords": ""}, "ff86133b3b49974f06fc881548c6f3c7a8ceffee": {"ta_keywords": "control age singing;control singing voice;age singing voice;individuality age singing;singing voice age;controlling voice timbre;voice timbre control;voice characteristics manipulating;singer perceived age;singing voice timbre;age singer perceived;technique controlling voice;singing voice characteristics;method control singing;controlling voice;age singing;sing expressively controlling;voice age singer;maintaining singer individuality;control singing;voice timbre arbitrary;voice timbre based;convert singing voice;singing voice technique;target singer experimental;voice age;controlling prosody voice;method change singer;singing voice;change singer perceived", "pdf_keywords": ""}, "43d82bc8203c09edc7eb6b2bedcf4ab500690852": {"ta_keywords": "multilingual language models;transfer multilingual models;translation multi lingual;multilingual multilingual models;multilingual models;multi lingual model;machine translation multi;training machine translation;multi lingual;languages nlp tasks;embeddings semantically similar;different languages nlp;multilingual models typologically;training multilingual multilingual;multilingual models using;transfer multilingual;large multilingual language;effectiveness cross lingual;using parallel corpus;parallel corpus;various nlp tasks;training multilingual;machine translation;multilingual multilingual;shot transfer multilingual;mber english data;large multilingual;cross lingual;multilingual language;multilingual", "pdf_keywords": "machine translation multilingual;large multilingual corpus;multilingual language models;translation multilingual languages;multilingual corpus;translation multilingual;lingual transfer learning;transfer machine translation;machine translation models;trained large multilingual;multilingual corpus mbert;large multilingual language;translation models;understanding cross lingual;effectiveness cross lingual;multilingual multilingual;multilingual languages;multilingual;multilingual language;large multilingual;multilingual languages large;large language models;machine translation;cross lingual;lingual transfer capabilities;cross lingual transfer;multilingual multilingual study;translation models typologically;bene\ufb01cial multilingual multilingual;multilingual study"}, "b57da3ccf214e8dad49116c8db9590c2c89629f5": {"ta_keywords": "named entity recognition;entity recognition ner;entity recognition;named entity;recognition ner languages;supervised transfer;supervised transfer learning;dataset named entity;recognition ner;entity;supervised;transfer learning;methods supervised transfer;semif tournament;semif tournament played;semif semif tournament;ner languages;extensive empirical evaluation;recognition;ner languages present;learning;tournament played national;fluid dynamics video;extensive empirical;languages present results;dataset;high quality dataset;methods supervised;languages;empirical evaluation", "pdf_keywords": "named entity recognition;languages spoken entities;spoken entities language;spoken entities;training corpus languages;entity recognition;training corpus;named entity identification;ner african languages;entity recogness ner;named entity recogness;integrate training corpus;corpus languages;language based named;entity recognition based;spell checkers localization;african languages work;entities language;new language;african languages;entities language paper;entity identification;entity identification used;checkers localization voice;improve performance languages;corpus;language independent named;entity recogness;en tity recognition;localization voice"}, "05c8f15dbdd7c6661b9176638262bbc1e11de85f": {"ta_keywords": "interpretable sense embeddings;sense embeddings learn;sense embeddings competitive;word sense embeddings;interpretability sense embeddings;sense embeddings;sense embeddings using;discrete sense embeddings;interpretable embeddings learn;softmax;sense vectors interpretability;produce interpretable embeddings;interpretable embeddings;sense embeddings does;sense embeddings single;produces sense embeddings;multiple sense vectors;learns discrete sense;sense vectors;learning word sense;embeddings learn;softmax function allows;embeddings competitive;gumbel softmax;sense selection;modified gumbel softmax;softmax function;dynamics semiflexible swimmer;able swim sliding;embeddings single word", "pdf_keywords": "semantic similarity powerful;contextual word vectors;measure semantic similarity;models semantic similarity;context word embeddings;evaluation sense embeddings;similarity word sense;sense embeddings;semantic similarity;word embeddings;word sense representation;semantic similarity word;sense embeddings train;word vectors useful;sense representation disambiguation;word vectors;embedding based weighted;based weighted embeddings;weighted embeddings;context embedding based;context embedding;similarity word;word embeddings play;word similarities;reproduce word similarities;word similarities better;model context embedding;sense selection contextual;embeddings;text representations"}, "58737fba500075136ee0f33f7801a5ac7f82ab68": {"ta_keywords": "patterns experiments newswire;implementation lucene;lucene;experiments newswire collections;large scale patterns;lucene open source;including lucene;generation large scale;open source search;search library researchers;experiments newswire;large scale reproducibility;patterns;differences including lucene;lucene maligned approximation;search library;implementation lucene open;patterns experiments;model spontaneous generation;source search library;source search;refer implementation lucene;search;including lucene maligned;reproducibility study bengal25;lucene open;spontaneous generation large;generation large;newswire collections significant;reproducibility", "pdf_keywords": ""}, "14919b6a453a71f2a007d5fa57241887a982575f": {"ta_keywords": "learning description logics;representation independent learnability;description logics paper;description logics;learnability;learning description;learnability core;independent learnability;problem learning description;independent learnability core;logics paper construct;learnability core classic;universal ball;logics paper;question representation independent;xcite universal ball;universal ball number;logics;question representation;consider problem learning;construct;representation independent;representation;points xcite universal;problem learning;holds true pairs;learning;ball;liverpool preprint incorrect;true pairs points", "pdf_keywords": ""}, "fb089347919e8dada9335b4bac01f16eea758c56": {"ta_keywords": "computer science teaching;computer games;computer games social;skills computer science;teach skills computer;course artificial intelligence;teaching science fiction;games social dilemmas;relationship computer games;technologies teach;intelligence computer science;dilemmas scenario students;skills computer;introductory course artificial;games social;tools technologies teach;computer science;use artificial intelligence;students engage questions;artificial intelligence computer;artificial intelligence;intelligence computer;intelligence ai;intelligence ai present;ai present;technologies teach high;scenario students;teaching science;ai;teach skills", "pdf_keywords": ""}, "e2c05b3abf77900ec82ffa8a95aa774308d2780f": {"ta_keywords": "code switching languages;analyzing code switched;code switched text;code switching;code switching patterns;analysis code switching;switching languages;switching languages used;level language detection;code switched;switching patterns twitter;text multiple languages;multiple languages mixed;word level language;languages mixed;language detection;switched text;multiple languages;word level labeling;twitter global region;patterns twitter global;level language;switched text multiple;language detection necessary;languages mixed sentence;languages;analyzing code;patterns twitter;language;twitter global", "pdf_keywords": ""}, "719916251f7e36d2e7a40e70f89f20ab97a8bc29": {"ta_keywords": "channel speech enhancement;speech enhancement present;speech enhancement;speech enhancement applications;various speech enhancement;speech recognition masks;speech enhancement severely;enhancement spectral mask;single channel speech;spectral mask estimation;multichannel enhancement spectral;using multichannel enhancement;mask obtained multichannel;multichannel enhancement;channel speech;improving performance speech;performance speech recognition;spectral mask;speech signal make;single channel tracking;mask estimation using;speech recognition;single channel track;mask estimation;multichannel input beamforming;speech signal;unsuitable speech recognition;channel tracking;performance single channel;recognition masks", "pdf_keywords": "channel speech enhancement;multichannel speech enhancement;improvement speech enhancement;speech enhancement propose;speech enhancement;speech enhancement based;speech enhancement input;speech enhancement properties;single multichannel speech;single channel speech;single channel enhancement;training single channel;multichannel speech;obtained multichannel speech;channel enhancement propose;channel enhancement;channel enhancement technique;quality speech masks;channel track chime;enhancement based deep;channel speech;high quality speech;enhancement propose train;speech masks proposed;challenge single channel;speech masks;function single channel;single channel track;enhancement input predicts;improvement speech"}, "0909fee90833e20913adb553bf6667c9a3b854b0": {"ta_keywords": "learning website wrappers;wrapper learning systems;website wrappers examples;wrapper learning;learning website;wrapper learning called;earlier wrapper learning;website wrappers;labs wrapper learning;present wrapper learning;learning called wl2;scratch learning;learner;learning systems;copies input learning;called wl2 learn;wrapper learning problem;wrappers examples;wrapper management;scratch learning able;faster learning;problem learning website;learning scratch learning;input learning learn;learning described;wrappers examples present;learning described industrial;input learning;wl2 learn;learning systems program", "pdf_keywords": ""}, "4a45ace1f8c6a30ba00201b30acd93844b9797eb": {"ta_keywords": "counteractive counteractive diode;counteractive diode counterintuitively;counteractive diode;counteractive reagent counterintuitive;counteracts counteracting diode;counteractive reagent;effectiveness counteractive reagent;counteracting diode;antihydrogen counteracts counteracting;reagent counterintuitive behavior;reactive reagent active;counteractive counteractive;active counteragent;counteractive antihydrogen counteracts;counteractive;effectiveness counteractive;behavior counteractive counteractive;decrease effectiveness counteractive;counterintuitive behavior counteractive;reagent counterintuitive;antihydrogen counteracts;reactive reagent;fact counteractive antihydrogen;active counteragent counter;diode counterintuitively arises;behavior counteractive;reagent active;counteractive antihydrogen;counteracts counteracting;counterintuitive fact counteractive", "pdf_keywords": ""}, "dd3ba828dbbb17cf478f6840a37954f6ebc81770": {"ta_keywords": "attention based lstms;bidirectional attention based;bidirectional attention;use bidirectional attention;dialog understand speaker;intentions accurately dialog;lstms train roledependent;level context utterance;hotel reservation task;speaker intentions accurately;roles conversation agent;attention based;accurately dialog;context utterance;understand speaker intentions;conversation agent;speaker intentions;conversation agent versus;accuracies hotel reservation;right context dialog;lstms;context dialog;based lstms;label accuracies hotel;context dialog understand;context sensitive model;hotel reservation;accurately dialog important;based lstms train;dialog understand", "pdf_keywords": ""}, "81e57827bebb04305cc9e6c85e96c83951244ec2": {"ta_keywords": "knowledge graph embedding;embedding vectors predict;link prediction;vectors predict polypharmacy;knowledge graph;link prediction algorithm;data knowledge graph;effects link prediction;link prediction task;new knowledge graph;tensor decomposition models;knowledge graph formulates;graph embedding;graph embedding technique;predict polypharmacy;predict adverse effects;knowledge graph paper;multi embedding vectors;algorithm based tensor;art link prediction;predict polypharmacy effects;embedding vectors;based tensor decomposition;task knowledge graph;drug combinations;multi embedding;adverse effects link;embedding technique;tensor decomposition;predict adverse", "pdf_keywords": "knowledge graph embedding;link prediction knowledge;links knowledge graphs;knowledge graph;knowledge graph including;knowledge graphs;prediction knowledge graphs;graph embedding models;graph embedding model;present knowledge graph;knowledge graphs adverse;embed components knowledge;knowledge graphs structured;components knowledge graph;based knowledge graph;drugs based knowledge;linked data drug;knowledge bases;knowledge graphs frameworks;knowledge graph technique;links knowledge;structured knowledge bases;graphs adverse drug;learning link prediction;graphs structured knowledge;graph embedding;learning models embed;embedding models;life knowledge graph;knowledge bases used"}, "e98621050e52e9d8c60829d8d861e81ac86a8617": {"ta_keywords": "space speaker adaptation;speaker adaptation;speaker adaptation based;model space speaker;space speaker;adaptation based prior;shared feature estimation;speaker;adaptation based;adaptation;prior shared feature;feature estimation;based prior shared;shared feature;feature;estimation;prior shared;model space;based prior;approach model space;shared;model;novel approach model;space;prior;novel approach;approach model;propose novel approach;based;approach", "pdf_keywords": ""}, "e2b097bce656db9215505659357263c43190194b": {"ta_keywords": "dividing reviewers phases;data dividing reviewers;dividing reviewers;reviewers divided phases;random reviewer split;random allows assignment;reviewers phases;reviewers phases conditions;assignment scientific conferences;optimal assignment scientific;assigned additional reviewers;reviewers divided;reviewer split suitable;question reviewers divided;random reviewer;oracle optimal assignment;reviewer split;optimal assignment;chairs random reviewer;paper review process;random choice practical;split suitable conference;paper review;assignment similarity;conference experiment design;conference data dividing;additional reviewers;total assignment similarity;allows assignment nearly;random choice", "pdf_keywords": "assigning reviewers papers;algorithm assigning reviewers;assigning reviewers;assigning reviewers given;paper assignment conference;assignment conference experiment;paper assignment problem;paper manipulating reviews;reviewers given paper;random choice reviewers;problem assigning reviewers;reviewers papers;conference experiment design;assignment conference;random split strategy;proposed random split;paper assignment;stage paper assignment;reviewers papers manner;allocation problems involve;peer review;setting proposed random;experiments pertaining peer;proposed random;involve matching resources;conference experiment;pertaining peer review;allocation problems;reviewer in\ufb02uence outcome;outcome paper manipulating"}, "c0c1950fb0a129b71a218ffa8b9fbc6d088cba2d": {"ta_keywords": "national science foundation;promotion national science;science foundation nsf;science foundation;national science;foundation nsf;foundation nsf supervision;nsf supervision minister;promotion national;minister education science;involved promotion national;nsf;foundation;nsf supervision;national;organizations;science technology mpa;education science technology;science technology;organizations involved;education science;organizations involved promotion;activities organizations;activities organizations involved;minister education;study activities organizations;science;supervision minister education;technology mpa;education", "pdf_keywords": ""}, "27f9b91bd7c70a99f578c8a5cb52d37e4123da47": {"ta_keywords": "networks based multilinear;activation tensor;neural networks consist;activation tensor output;networks consist convolutional;order activation tensor;structure using tensor;incorporating tensor;convolutional layers followed;convolutional layers;structure learn contraction;convolutional neural networks;input preserving multilinear;preserving multilinear structure;order convolutional neural;contraction layers;using tensor contraction;activations based flattening;tensor contraction;tensor;topological structure learn;tensor output;networks;neural;neural networks;convolutional neural;using tensor;imagenet;output tensor;preserve multilinear structure", "pdf_keywords": "tensorized network;tensor machine learning;tensorized network architecture;end tensorized network;tensor regressions trainable;activation tensor machine;tensorized;tensor regressions;tensor machine;deep learning;incorporating tensor;use tensor regressions;learn jointly features;structure activation tensor;convolutional neural networks;end tensorized;trainable layers neural;tensor;deep convolutional neural;activation tensor;layers neural;regressions trainable layers;use tensor;layers neural networks;leveraging multi dimensional;learning based deep;approach deep learning;neural networks allow;jointly features;multilinear structure activation"}, "b09d49c3eacd93782a32ad16ab52f98a21ecc206": {"ta_keywords": "behavior pedestrian crowded;dynamics pedestrian crowded;pedestrian crowded environment;pedestrian crowded;crowded environment dynamics;environment dynamics pedestrian;behavior pedestrian;dynamics pedestrian;crowded environment;pedestrian;relationship behavior pedestrian;crowded;behavior;environment dynamics;dynamics;relationship behavior;investigate relationship behavior;environment;relationship;paper investigate relationship;investigate relationship;paper investigate;paper;investigate", "pdf_keywords": ""}, "1bf49ef0b33bf8fcc3ebdd16326db419f3af65d8": {"ta_keywords": "entanglement representations;entanglement class;entanglement class manifolds;hidden layers neural;entanglement representations different;entanglement;class independent similarity;representations identify class;classes hidden layers;layers neural;layers neural network;representations different classes;class similarity structures;class similarity;independent similarity structures;class manifolds representation;identify hidden layers;discrimination final layer;textit entanglement class;hidden layers;loss hidden layers;similarity structures learning;representations identify;representations;similarity structures;similarity structures introduce;nearest neighbor loss;independent similarity;evolution class similarity;neural network", "pdf_keywords": "entanglement synthetic training;entanglement machine learning;learn representations similarity;entanglement synthetic data;entanglement representations;generative models trained;generative model trained;separability entanglement representations;measure entanglement synthetic;entropy entangled representation;entanglement synthetic;entangled representation;trained learn representations;learning generative;representations similarity;synthetic training data;classi\ufb01cation separability entanglement;cross entropy entangled;entanglement representations different;entanglement synthetic real;representations similarity structure;learn representations;learning generative model;entropy entangled;generative model soft;data learning generative;entangled representation spaces;data generative;real data generative;generative models"}, "3a4f39dbb5e06a5fc55622315797da7a97cc76f6": {"ta_keywords": "sentence prediction;word level quality;sentence prediction based;approach sentence prediction;word using neural;words speech tags;represent words speech;speech tags;determine words source;layer represent words;words speech;speech tags languages;word level;determine words;qe determine words;words source sentence;target word using;speech;contextual information target;contextual;words source;global contextual;neural networks;target word;represent words;neural;contextual information;global contextual information;neural network;words", "pdf_keywords": "machine translation learning;machine translation;networks machine translation;translation learning;translation quality estimation;machine translation important;task machine translation;translation learning phrase;statistical machine translation;networks rnn;stacked recurrent neural;sentence convolutions provide;translation quality;phrase representations convolutional;recurrent neural networks;gains translation quality;utilize stacked recurrent;sentence convolutions;level computer translation;information sentence convolutions;rnn encoder;deep neural;computer translation;learning phrase representations;rnn encoder statistical;propose deep neural;translation important task;neural networks rnn;stacked recurrent;networks rnn result"}, "b9a701c90f3d3df27366f5b29a97f798eb940ac7": {"ta_keywords": "long segment narrative;segment narrative ends;range language models;level language understanding;segment narrative;language models;narrative ends;language models lrms;narrative ends chapter;narrative fine grained;segments sampled narrative;long range language;level language;transitions parallel narratives;discourse level language;language understanding;narratives cliffhanger endings;narratives cliffhanger;language understanding capabilities;parallel narratives cliffhanger;grained human annotation;chapter transitions;narrative fine;discourse level;parallel narratives;human annotation reveals;chapterbreak existing lr;narratives;chapterbreak;global context comprehend", "pdf_keywords": "narrative language models;discourse level language;discourse level capabilities;sampled narrative language;narrative language;range language models;discourse level;evaluation discourse level;language models;negatives sampled narrative;language models widely;level language understanding;language understanding capabilities;automatically disambiguating;long range language;automatically disambiguating token;language model task;context substantially underperforming;disambiguating token segment;models disambiguate shifts;language understanding;language model;long range context;discourse;present language model;better evaluation discourse;sampled narrative;task automatically disambiguating;disambiguate shifts;language models llrs"}, "2559417f8a3d6ab922cfa824b43f9f0c642a1dae": {"ta_keywords": "named entity recognition;entity recognition ner;extraction entity names;entity recognition;entity recognition based;similarity extracted entities;semi markov extraction;extraction entity;improving named entity;markov extraction process;markov extraction;approach named entity;extracted entities entities;entity names;entity names data;extracted entities;named entity;entities external dictionary;sequential word classification;information similarity extracted;improving named;external dictionaries;similarity extracted;word classification;names data;word classification consider;using external dictionaries;method extraction entity;entities entities;external dictionary", "pdf_keywords": ""}, "3ad3ba8d7fc793a19dfe6a87e32453449195c074": {"ta_keywords": "speech text training;speech recognition asr;speech text datasets;speech recognition large;recognition large speech;automatic speech;autoencoders learn;speech recognition;text training;automatic speech recognition;large speech text;features speech text;performance speech recognition;autoencoders learn features;learn features speech;speech text;text training datasets;autoencoders;subset autoencoders learn;introduce automatic speech;recognition asr;recognition asr model;semi supervised end;subset autoencoders;improve performance speech;models semi supervised;learning model trained;unpaired subset autoencoders;features speech;model trained", "pdf_keywords": ""}, "765bdcf27ebc1eb03a14f1e47aefa4dda1e03073": {"ta_keywords": "word representations robust;noisy texts model;training noisy texts;learn representations robust;representations robust training;character convolutional neural;moderately noisy texts;representations robust;character based neural;noisy texts;neural machine translation;character convolutional;translate moderately noisy;invariant word representations;robust training noisy;based character convolutional;noisy texts humans;models alleviate vocabulary;translation nmt models;comprehending character based;translation systems;word representations;learn morphology;model based character;machine translation nmt;learn morphology closer;learn representations;robust training;issues learn morphology;texts model", "pdf_keywords": "robustness machine translation;convolutional machine translation;models trained words;machine translation;train attentional sequence;word representation ensemble;machine translation ccm;translation scenario train;recurrent neural network;attentional sequence sequence;models translating scrambled;attentional sequence;translating scrambled texts;model word representation;training adversarial examples;machine translation scenario;training adversarial;ensemble training adversarial;trained words;translating scrambled;recurrent neural;translation ccm models;trained words length;representation ensemble training;noise word recognition;convolutional model trained;invariant word representation;adversarial;convolutions trained;adversarial examples"}, "1263e36598dd95cc4becf0e18398f832bb5cf337": {"ta_keywords": "language training fluid;lowresource languages good;language training;low resource languages;dynamics moving;lowresource languages;languages;lstms transformers;movement;lstms;attention based encoder;language;dynamics moving body;related language training;tug war bouncer;moving body;training fluid dynamics;corpora lowresource languages;moving body manipulated;resource languages;dynamics video;languages good;noisy environment attention;moving;bouncer;position moving;dynamics video dynamics;dynamics;video dynamics moving;inflection low resource", "pdf_keywords": ""}, "978582ad754eab481856d62bdc7b0ee5bcf21811": {"ta_keywords": "finding clusters data;clusters data;cluster;cluster information;finding clusters;algorithm finding clusters;clustering unlabeled datasets;clusters;clustering;clusters data set;recover cluster information;simulation illustrates clustering;method recover cluster;recover cluster;cluster information individual;clustering unlabeled;clustering based;clustering based algorithm;consider problem clustering;clustering process;proposed iterative clustering;illustrates clustering;iterative clustering;problem clustering;clustering process used;iterative clustering based;illustrates clustering process;problem clustering unlabeled;datasets federated environments;federated environments statistical", "pdf_keywords": ""}, "675098c4611b13920d163a9a9b972da7751460cb": {"ta_keywords": "spoken dialog systems;recurrent neural network;spoken dialog;architecture recurrent;architecture recurrent neural;novel architecture recurrent;word embedding latent;word embedding based;word embedding;approach word embedding;components spoken dialog;dialog systems;topic models pre;recurrent neural;latent topic models;networks spoken language;based word embedding;spoken language understanding;embedding based pre;embedding latent topic;neural network rnn;novel recurrent neural;dialog;network rnn;topic models;entire dialog;language understanding slu;spoken language;recent years neural;large scale corpora", "pdf_keywords": ""}, "dd64013273eb4398821bf2fc8f024735466e5a1d": {"ta_keywords": "simulation human learning;representations simulated student;constructing learning agent;sim student learns;simulated student;learning agent;learning agent currently;learns procedural knowledge;algorithm simulated student;intelligent agents simulate;learning representations simulated;student learns procedural;simulated student sim;student plausible simulation;student learns;simulated student integration;learns procedural;constructing learning;intelligence constructing learning;sim student plausible;procedural knowledge;learns;human learning;agents simulate;student integration learning;procedural knowledge example;level intelligence constructing;acquires representation knowledge;plausible simulation human;better sim student", "pdf_keywords": ""}, "1abbe9b6bf3f134ce86e618bba83bf5c94f60f03": {"ta_keywords": "crowdsourcing mechanism incentivizes;crowdsourcing mechanism;propose crowdsourcing mechanism;incentive mechanism prediction;crowdsourcing;commercial crowdsourcing;propose incentive mechanism;propose crowdsourcing;crowdsourcing platforms;commercial crowdsourcing platforms;appropriate incentive mechanism;amazon mechanical turk;emergence commercial crowdsourcing;propose incentive;incentive mechanism;crowdsourcing platforms amazon;paper propose crowdsourcing;incentive;agents motivating participate;truthfully prediction algorithms;mechanical turk;appropriate incentive;incentive mechanism exploits;mechanical turk situations;design appropriate incentive;customers predictions winner;report truthfully prediction;truthfully prediction;agents motivating;incentivizes agents", "pdf_keywords": "agents propose mechanism;agent based predictive;based predictive mechanism;qp designed optimally;exploit heterogeneity agents;agents exploit heterogeneity;agents order minimize;predictive mechanism;payment irrespective agent;heterogeneity agents propose;agents exploit;prediction incentivized;participating agents exploit;mechanism predict outcome;formulate optimally;information participating agents;agents propose;minimize prediction;prediction incentivized report;designed optimally;predict outcome individual;agent reports cost;predictive mechanism predict;minimize prediction error;order minimize prediction;harness heterogenity agents;proposed mechanism maximizes;mechanism maximizes principal;formulate optimally solve;participating agents"}, "a2ea2261bd56ae2505750d7571b501d9836175f0": {"ta_keywords": "algorithm discriminative training;discriminative training framework;discriminative training;training acoustic models;algorithm discriminative;acoustic models combination;automatic speech;speech recognition;automatic speech recognition;discriminative training adjust;requirements discriminative training;update algorithm discriminative;speech recognition reference;performance automatic speech;unify requirements discriminative;requirements discriminative;construct complementary systems;discriminative;training acoustic;approach training acoustic;acoustic models;complementary systems;complementary systems simply;models combination proposed;models combination;generate complementary hypotheses;mutual information base;maximizing correct labels;complementary outputs;systems generate complementary", "pdf_keywords": ""}, "39456ca31a530d85ec182b2676dc94266dada597": {"ta_keywords": "compositional distributional semantics;distributional semantics proposed;distributional semantics;distributional semantics methods;distributional semantics low;learning compositional distributional;tensors semantic similarity;tensors propose compositional;tensors semantic;tensor approximations compositional;matrices tensors semantic;learning compositional;composing word vectors;explores compositional distributional;machine learning compositional;compositional distributional;semantic similarity tasks;approximations compositional distributional;representations length sentences;semantic similarity;representing meaning composing;similarity tasks syntactic;semantics low rank;represented order tensors;order tensors propose;order tensors;tensor approximations;tensors;words feature vectors;tensors propose", "pdf_keywords": ""}, "bc8e67d532693818eb33aa8e401260fe2b774a18": {"ta_keywords": "relationships men women;relationships men;study relationships men;american physical society;physical society study;relationships;physical society;men women;society study relationships;american physical;meeting aps;annual meeting aps;division american physical;women;men;physical;study relationships;meeting aps division;aps;aps division american;society study;meeting;aps division;annual meeting;twelfth annual meeting;society;twelfth annual;division american;annual;american", "pdf_keywords": ""}, "29dc4b10e60ed1e2b84598cc6f2622c786841fdf": {"ta_keywords": "planning specifications robotic;motion planning specifications;temporal logic specifications;specification temporal logic;linear temporal logic;specifications robotic robotic;robotic motion planning;specifications robotic;robots subject temporal;control mobile robots;feasibility controller robotic;planning specifications;motion planning;defined specification temporal;temporal logic;motion planning challenging;specification control barrier;control barrier functions;controller robotic simulation;specification temporal;prioritization based control;task robotics;temporal logic encompasses;constraints quadratic program;robotic simulation;controller robotic;given specification control;guarantee feasibility controller;temporal logic sequence;challenging task robotics", "pdf_keywords": "constrained reachability framework;control cyber threat;framework control cyber;constrained reachability;control cyber;distributed controllers;rely distributed controllers;controller infeasible;robotic control af\ufb01ne;robotic control;type constrained reachability;logic based control;control using barrier;safety critical systems;satisfying trajectories safety;trajectories safety critical;complexity feedback controller;cyber threat based;feedback controllers;threat based lasso;mobile robotic control;proposed controller;controllers;prove proposed controller;controller infeasible paper;reachability framework;trajectories safety;time mobile robotic;feedback controller;feedback controllers used"}, "3f0f6c19c6f5d4e4d5066984c5f3e922a2c2ff85": {"ta_keywords": "languages reward shaping;learning languages reward;abstraction reinforcement learning;abstraction reinforcement;reward shaping;enrich sparse rewards;reward shaping methods;language abstraction reinforcement;reward shaping approach;sparse rewards;introduce reward shaping;classifier learns relevance;sparse rewards afforded;learns;reinforcement learning environments;learns relevance;reinforcement learning;sparsity performance learning;learns relevance classifier;classifier learns;propose classifier learns;languages reward;language abstraction;instructions enrich sparse;relevance classifier online;performance learning languages;reinforcement;learning;learning languages;learning environments synthetic", "pdf_keywords": "abstraction based reward;learning sparse reward;reinforcement learning sparse;learning algorithm reinforcement;reinforcement learning;reinforcement learning rl;algorithm reinforcement learning;reward shaping framework;sparse reward;based reward shaping;present reinforcement learning;shaping framework reinforcement;reward based approach;reinforcement;abstractions online learning;algorithm reinforcement;reward shaping;sparse reward settings;level language learns;reinforcement learning algorithm;instruction present reinforcement;framework reinforcement;reward based;present reward shaping;use reinforcement learning;learn policy predicting;framework reinforcement learning;use reinforcement;based reward;learning problem agent"}, "81ea04f822a1d5317e5846783900ac424a8f7528": {"ta_keywords": "automatic classifiers autism;automatic identification autism;classifiers autism spectrum;classifiers autism;diagnosis autism children;identification autism spectrum;diagnosis autism;autocatalytic diagnosis autism;children autism spectrum;children autism;autism spectrum;disorders children autism;autism spectrum disorders;autism children;autism;identification autism;differences children autism;children autism paper;autism paper;speech features;language speech features;autism paper perform;speech features single;features single utterances;utterances narratives;single utterances narratives;utterances;language speech;utterances narratives article;speech", "pdf_keywords": ""}, "98fdc7e1e167eb465cdb1c8ee0800db750101155": {"ta_keywords": "statistical voice conversion;utterances statistical voice;voice conversion;spectral distance utterances;statistical voice;corresponding utterances statistical;predicting variation prosodic;utterances statistical;prosodic features input;quality expected prosodic;prosodic features;voice conversion vc;distance utterances;prosodic parameter differences;features input speech;expected prosodic features;differences corresponding utterances;distance utterances exists;input speech;criteria speaker utters;prosodic parameter;improvements sound quality;corresponding utterances;variation prosodic parameter;variation prosodic;training criteria speaker;speaker utters;criteria speaker;sound quality;prosodic", "pdf_keywords": ""}, "ae25ca24eb6c2772ef88e2d0315fc428feb8553e": {"ta_keywords": "unsupervised information extraction;categories added knowledge;categories based;categories based association;important categories based;information extraction;categories categories;categories;existing categories;investigate relationship categories;relationship categories categories;categories categories propose;unsupervised information;relationship categories;identify entities;new categories;identify entities belonging;based association entities;certain categories;important categories;knowledge base;categories paper investigate;association entities table;accuracy propose unsupervised;entities meaningful sets;recognition web page;recognition web;categories added;columns present clustering;propose unsupervised information", "pdf_keywords": ""}, "689ab475e8a0f552bf6e39a2f774d9d20e50b9cb": {"ta_keywords": "semif semifinal game;semif semif semifinal;semif semifinal;computer simulation semif;semifinal game winner;semifinal game;simulation semif;simulation semif semif;semifinal;game winner championship;championship placed floor;results computer simulation;semif;computer simulation;simulation;winner championship placed;semif semif;championship;game winner;winner championship;championship placed;game;floor;placed floor;results computer;present results computer;winner;results;present results;computer", "pdf_keywords": ""}, "38a73e6f48d057cb58264f5148f8b05522d0d030": {"ta_keywords": "parsing corpus semantic;semantic parsing corpus;semantic parsing possible;semantic parsing;corpus semantic parsing;semantic parsing task;dependent semantic parsing;annotation semantic parsing;parsing annotation semantic;semantic parsing annotation;parsing corpus;corpus semantic;progress semantic parsing;parsing annotation;parsing;parsing possible;context dependent semantic;annotation semantic;natural language utterances;natural language;corpora progress semantic;semantic;machineinterpretable meaning representation;user utterances;utterances machineinterpretable meaning;language utterances machineinterpretable;utterances;parsing task translating;parsing task;corpus", "pdf_keywords": ""}, "045f90129a8d7148eec4a58770bc4166b51330ca": {"ta_keywords": "demand parking consistent;demand parking;congestion caused parking;pricing parking lots;models demand parking;parking lots;parking consistent;approach pricing parking;pricing parking;parking performance based;parking lots approach;drive parking;parking decisions;parking performance;parking;parking lot;drive parking decisions;parking consistent time;factors drive parking;caused parking performance;parking lot recent;spatial demand quantified;demand quantified spatial;gaussian mixture models;crosses parking lot;repeatability gaussian mixture;mixture models demand;caused parking;similar spatial demand;gaussian mixture model", "pdf_keywords": ""}, "fd0aa185be4e1f1fe3975779aec179348ec19ea8": {"ta_keywords": "visibility papers associated;visibility papers;papers visibility;double blind conferences;blind conferences;blind conferences engaged;papers visibility 18;remaining papers visibility;visibility preprints;science conferences;bose einstein condensate;ranked a\ufb03liations visibility;science conferences conference;visibility;visibility preprints better;magnetic field visibility;computer science conferences;lattice admits;condensate bec presence;einstein condensate bec;papers online arxiv;field visibility preprints;a\ufb03liations visibility approximately;a\ufb03liations visibility;visibility approximately;bec presence;papers associated 10;einstein condensate;component bose einstein;conferences conference", "pdf_keywords": "blind peer review;papers peer review;peer review processes;peer review computer;studies peer review;peer review;blind reviewing;reviewers seen paper;single blind reviewing;peer review double;peer review used;online review process;reviewers queried papers;papers online review;paper reviewers;experiment querying reviewers;querying reviewers seen;paper reviewers queried;submitted peer review;online review double;paper paper reviewers;online review;investigate visibility papers;visibility papers submitted;submitted papers peer;peer review favours;papers peer;online measuring visibility;papers submitted peer;review double blind"}, "f837bf72e5b864e1c162e924fed59b778e946e23": {"ta_keywords": "soccer games imagination;auto encoders learns;encoders learns context;time guessing games;learn conceptual representations;learns context category;guessing games;conceptual representations objects;games imagination module;objects discriminative expressive;games guesser identify;learns context;category aware latent;encoders learns;representations objects discriminative;capable reasoning object;aware latent embeddings;games guesser;objects discriminative;conceptual representations;accuracy compguess game;category aware;guessing games guesser;games imagination;embeddings relying category;reasoning object;discriminative expressive ask;object scene asking;representations objects;soccer games", "pdf_keywords": "learn object representations;latent concept representations;object representations;learning conceptual representations;modal visual representations;representations context category;concept representations context;concept representations;context guessing games;learns latent concept;imagination module learns;context aware latent;visual representations;object representations given;rely categories embeddings;conceptual representations;learning compositional;learns latent;context category aware;learning compositional generalizable;benchmarks guess context;representation current dialogue;visual representations addition;learn object;categories embeddings;representations context;possible learn object;module learns latent;conceptual representations adopt;learn categoryaware context"}, "df4e3aa275b8f81e22a5332ab550805083094dae": {"ta_keywords": "neural generation translation;neural machine translation;document generation translation;machine translation;machine translation nmt;generation translation;workshop neural generation;neural generation;document generation;nmt document generation;systems generate summaries;generate summaries structured;generation translation held;generate summaries;workshop neural;generation translation dgt;summaries structured;tasks efficient neural;neural machine;translation nmt document;efficient neural machine;efficient neural;language processing emnlp;natural language processing;organization workshop neural;neural;potential paper summarize;translation nmt;language processing;natural language", "pdf_keywords": "neural generation translation;machine translation machine;machine translation;translation machine translation;novel machine translation;trained language models;translation machine;language models predicting;predicting outcome conversations;generation translation;combines machine translation;workshop neural generation;machine translation method;machine translation paper;trained news corpora;generate document summaries;summaries target language;model trained news;language models;neural generation;generation model trained;document level generation;results workshop neural;nlg data en;trained language;generation translation saw;nlg data;pre trained language;output requirements nlg;document summaries target"}, "e5efd7e2087e58c5a8860398dfcf143aa9dc865e": {"ta_keywords": "sound event detection;event detection challenging;event segmentation training;weakly labeled training;detection supervised label;event detection task;training data sound;accurate event labels;event classification;event boundary detection;acoustic driven event;detection supervised;labeled training data;evaluation event classification;event detection;deep neural;limited event segmentation;event segmentation;boundary detection supervised;event labels;supervised label;segmentation recognition events;signal supervised;recognition events providing;sound event;accurate event localization;detection challenging task;recognition events;supervised label inference;detection task", "pdf_keywords": "supervised acoustic event;acoustic event detection;detection supervised acoustic;sound event detection;annotate sound events;supervised acoustic;acoustic driven event;acoustic event classi\ufb01cation;approach acoustic event;events complex acoustic;acoustic event;sound events;acoustic scenes;tracking changes acoustic;sound event;complex acoustic scenes;temporal regularities acoustic;sound events complex;annotate sound;acoustic scenes machine;classi\ufb01cation annotate sound;acoustic signal task4;event boundary detection;problem sound event;method sound event;event detection leveraging;sound events allowing;event detection task;characteristics sound events;segmentation recognition events"}, "0ae93646ad058853eb6424c1dc0ec1559414e5af": {"ta_keywords": "models automatic rumour;automatic rumour verification;rumour verification model;uncertainty rumour verification;predictive uncertainty rumour;automatic rumour;rumour verification estimates;rumour verification;uncertainty rumour;estimates natural language;rumour unfolds present;model performance rumour;performance rumour unfolds;rumours circulating online;resolve rumours circulating;resolve rumours;rumour unfolds;rumour;rumours circulating;correctly resolve rumours;estimating predictive uncertainty;performance rumour;human fact checker;instance rejection supervised;uncertainty based instance;rejection supervised unsupervised;prioritised human fact;model data uncertainty;fact checker;natural language processing", "pdf_keywords": "automatic rumour veri\ufb01cation;models automatic rumour;approach rumour prediction;rumour prediction based;estimates rumour task;rumour prediction;automatic rumour;estimates natural language;rumour task present;rumour task;uncertainty estimates rumour;uncertainty word representations;rumour veri\ufb01cation rumour;rumour based;rumour based approach;making rumour unfolding;veri\ufb01cation rumour;natural language processing;veri\ufb01cation rumour spreading;leveraging linguistic;leveraging linguistic cues;based approach rumour;estimates rumour;rumour veri\ufb01cation;making rumour;approach rumour;present rumour based;claims leveraging linguistic;manner making rumour;rumour spreading occurs"}, "0c12e4c611b32997f8be5811021ead80395a7e5c": {"ta_keywords": "multi task learning;task learning fascinating;multi task;task learning;machine learning;fascinating topic;learning fascinating topic;field machine learning;learning;fascinating topic field;learning fascinating;multi;task;topic;topic field machine;fascinating;topic field;field machine;machine;field", "pdf_keywords": ""}, "9af2264799bdc3490e4650e2f5d126762caf420f": {"ta_keywords": "end speech recognition;attention based encoder;attention model multi;task learning;multi task learning;attention model flexible;attention model;ctc attention model;learning performance attention;speech recognition;systems attention model;control systems attention;end end speech;speech recognition improves;end speech;systems attention;joint ctc attention;task learning performance;learns mapping;decoder framework learns;task learning framework;recognition based multi;model multi task;attention based;speech recognition based;ctc attention;learns mapping variable;framework learns mapping;approach attention based;performance attention", "pdf_keywords": "deep sequence learning;attention model encoder;sequence learning;outperforms attention model;deep sequence;end speech recognition;attention model multi;model speech recognition;attention model attention;attention model;recognition speech sequences;model attention model;attention model based;speech recognition task;ctc attention model;model attention;encoder model speech;present deep sequence;propose attention model;speech sequences;outperformed attention model;attention model outperformed;model speech;outperforms attention;speech recognition improve;recognition speech;speech recognition;sequence learning approach;attention model terms;joint ctc attention"}, "9d555ed29496850c4ef8a3facd7dce734c86aae7": {"ta_keywords": "prediction programmer comments;comment completion tool;comment completion;comment completion capability;models comment completion;using comment completion;predicting code;language models predicting;models predicting languages;programmer comments analyze;task predicting code;comment typing statistical;programmer comments;comment typing;predicting languages based;similar codecompletion tools;codecompletion tools;predicting languages;statistical language models;codecompletion tools built;language models programming;codecompletion;similar codecompletion;comments analyze performance;predicting code ignoring;natural language models;completion tool;typing statistical language;prediction programmer;language models", "pdf_keywords": ""}, "7d9863258ef44ca8a6b87b68be738f7a83ac849a": {"ta_keywords": "end rna;end end rna;speech recognition asr;speech enhancement asr;end end rrna;neural beamformer end;rrna end end;end rrna end;end rna able;multichannel asr benchmarking;multichichannel speech;end rrna;multichichannel speech signal;rrna end;asr encompass microphone;end automatic speech;converts multichichannel speech;existing rna frameworks;multichannel asr;end end asr;neural beamformer;signal text speech;rna frameworks;end asr paradigm;speech recognition;rna;automatic speech;end asr;recognition asr;speech signal", "pdf_keywords": ""}, "c6c18ad62f39060e2547a0b683525e83312d0700": {"ta_keywords": "incentives reviewers bidding;reviewers bidding;incentives reviewers;reviewers bidding phase;reviewer bid;assigning budgets reviewers;reviewers price paper;assignment paper reviews;bidding phase reviewers;inspired bidding scheme;bidding scheme assignment;analysis incentives reviewers;bidding scheme substantially;market inspired bidding;bidding;reviewers private costs;inspired bidding;papers match budget;bidding scheme;analysis incentives;papers predetermined assigning;budgets reviewers price;budgets reviewers;demand paper;provide analysis incentives;reviewers price;demand paper goal;improve bid distribution;response reviewer bid;improve bid", "pdf_keywords": "review strategyproof peer;peer review;peer review strategyproof;strategyproof peer review;paper bidding widely;paper bidding;peer review propose;pcms paper bidding;bidding widely;new peer review;bidding mechanism;new bidding mechanism;bidding mechanism proportional;peer review literature;bidding widely used;bidding process;used peer review;bidding;make bidding;bidding process easier;market bids;paper assignment process;propose market bids;bidding scheme;peer;assignment matching algorithms;make bidding process;bids;bidding scheme compatible;bids explain"}, "be4d47a61fee83d332ca2f3fe097f19f63863d6c": {"ta_keywords": "graphite structure modeled;structure modeled graphite;modeling graphs probabilistically;modeled graphite;graphite structure;structural disorder graphite;modeled graphite graphite;graphite used model;simple graphite structure;graphs probabilistically;disorder graphite studied;spectral graph theory;graphs probabilistically using;modeling graphs;graph partition optimization;disorder graphite;graph partition;graphite studied topic;graphite studied;graphite simple;disorder graphite simple;graphite;node clustering;graphite graphite;graph theory;simple graphite;dynamical properties graphite;optimization spectral graph;methods node clustering;properties graphite", "pdf_keywords": ""}, "554eade16fb6040bbd21a72bacf903245d7458f1": {"ta_keywords": "intelligence artificial;artificial intelligence artificial;artificial intelligence used;artificial intelligence;intelligence artificial intelligence;artificial intelligence frameworks;artificial neural;evaluation artificial intelligence;artificial neural networks;decision making fluid;advance artificial neural;paper artificial intelligence;fluid dynamics video;making fluid dynamics;neural networks draws;intelligence frameworks paper;intelligence frameworks;neural networks;fluid dynamics;cognitive theories human;intelligence;evaluation artificial;theories human decision;neural;dynamics;intelligence used learn;dynamics video;human decision making;cognitive theories;motion particle viscous", "pdf_keywords": "intelligence individual agents;intelligence ai;state art intelligence;world ai;intelligence ai emerging;world ai systems;artificial intelligence ai;intelligence ai lacks;ai;ai systems self;agent architecture;models world ai;ai systems;agent architecture vision;artificial intelligence;ai emerging;ai lacks capabilities;ai emerging technology;multi agent architecture;intelligence individual;intelligence;intelligence based;model intelligence;propose multi agent;advance ai;intelligence based premise;multi agent;model intelligence based;humans ability solve;intelligence example"}, "b8f5f3c8816ab389c2f366fd8a45603550ea9667": {"ta_keywords": "databases quantum information;databases quantum;biomedical text mining;construction databases quantum;database genes;quantum information processing;genetic association database;association database genes;information processing qip;database genes complex;constructing genetic association;reinforcement learning automatically;behaviors querying pubmed;deep reinforcement learning;text mining training;knowledge construct genetic;reinforcement learning;biomedical text;learning automatically identify;construct genetic association;text mining;human behaviors querying;learning automatically;quantum information;reliability biomedical text;database selecting articles;processing qip;constructing genetic;quantum;articles knowledge construct", "pdf_keywords": ""}, "5e327c2285ddf2a76d08e5c00d16c7358bc5412c": {"ta_keywords": "peer review strategyproofness;papers peer assessment;peer assessment;assignment quality guarantees;peer grading;applications peer grading;algorithms strategyproof assignment;peer grading homeworks;conference peer review;assignment quality;strategyproof assignment assignment;assignment assignment quality;peer review;review conference peer;peer assessment employees;peer review scientific;review strategyproofness achieved;strategyproof assignment;review strategyproofness;algorithms strategyproof;conference peer;grading homeworks;grading homeworks grant;papers peer;scientific papers peer;peer;polynomial time algorithms;assignment assignment;dataset conference peer;grading", "pdf_keywords": "submissions peer grading;peer grading;peer grading homeworks;peer assessment peer;peer assessment;peer assessment competitive;form peer assessment;assessment peer grading;applications peer assessment;papers peer assessment;strategyproof peer selection;peer assessment employees;evaluations submissions peer;algorithm strategyproof peer;assessment peer;submissions peer;peer selection;algorithm exists peer;peer selection accommodate;grading homeworks increasingly;arbitrary authorship submissions;bonuses applications peer;form peer;grading homeworks;grading homeworks grant;strategyproof peer;peer review;peer;papers peer;accommodate arbitrary authorship"}, "f878a7c756b90c0ed612838492fbbc02ecaaab70": {"ta_keywords": "problem orders learning;automatic tutors given;automatic tutors;learning agent;student solving class;student solving;learns cognitive;tutored automatic;tutors given problems;learns cognitive skills;learning agent learns;student tutored automatic;tutored automatic tutors;learning agent order;agent learns cognitive;tutors;performance student solving;problems used teach;correction learning agent;agent learns;learns;sim student tutored;student tutored;problem solving experience;orders learning;tutors given;learning;examples problem solving;learning effectiveness;problems presented students", "pdf_keywords": ""}, "1ee276db29ba9127e81d9a7d9cb08f5138339412": {"ta_keywords": "coded matrix;coded matrix matrix;novel coded matrix;matrix multiplication scheme;coded computation;coded computation addressing;matrix multiplication;large scale computation;matrix matrix multiplication;challenge distributed computing;distributed computing;redundant computing;optimal computation communication;parallel computing;parallel computing massive;dimensional product codes;distributed computing allows;distributed computing comes;introduction redundant computing;recently coding theory;redundant computing combat;computation communication costs;coding theory;coding theory offered;dubbed coded computation;coded;computation communication;coding;enabling parallel computing;algorithm encoding decoding", "pdf_keywords": ""}, "34cb1f081c1d1d6b3dc16a9278940a9ee85fb2e0": {"ta_keywords": "predict interpreter confidence;predicting simultaneous interpreter;interpreter performance language;simultaneous interpreter performance;evaluate interpreter performance;interpreter performance;interpreter confidence;interpreter performance building;performance language pairs;interpreter confidence adequacy;predict interpreter;machine translation output;qe machine translation;performance language;interpreter;simultaneous interpreter;machine translation;evaluate interpreter;translation spoken word;approach translation spoken;translation output;method predict interpreter;language pairs;framework evaluate interpreter;quality estimation qe;translation spoken;language pairs present;quality estimation;word real time;spoken word real", "pdf_keywords": "predicting interpreters interpreting;machine translation evaluation;machine translation quality;translation machine translation;machine translation;machine translation important;statistical machine translation;translation quality prediction;predicting simultaneous interpreter;automatically predicting interpreters;predicting interpreters;interpreter performance automatically;translation evaluation data;estimation translation quality;experiments machine translation;simultaneous interpreter performance;machine translation machine;translation machine;translation quality estimation;translation evaluation;quality prediction interpreters;interpreter performance building;estimating simultaneous interpreter;interpreter performance;interpreters interpreting smoothly;translation quality;interpreters interpreting;prediction interpreters employ;stress machine translation;machine translation callison"}, "839cbcf5c13d5875e952e40ec2da14b19eee2202": {"ta_keywords": "unconditional optimization;unconditional optimization problems;solving unconditional optimization;convex optimization;convex optimization problems;accelerated gradient free;conditional optimization;consider convex optimization;conditional optimization problems;propose accelerated gradient;optimization problems gradient;structure lhc direction;optimization problems paper;accelerated gradient;symmetric structure lhc;lhc direction;gradient free method;lhc direction opposite;approach conditional optimization;gradient free;xmath0 symmetric structure;optimization;xmath0 symmetric;euclidean prox structured;optimization problems;structure lhc;gradient available numerical;signature xmath0 symmetric;prox structured structure;prox structured", "pdf_keywords": "thermal conductivity earth;conductivity earth temperature;temperature thermal conductivity;thermal conductivity;conductivity earth;earth temperature dependence;earth temperature;temperature dependence conductivity;conductivity strongly temperature;effect temperature thermal;temperature thermal;temperature dependence;conductivity;temperature dependent;strongly temperature dependent;conductivity strongly;effect temperature;temperature;study effect temperature;dependence conductivity;strongly temperature;dependence conductivity strongly;thermal;earth;present results;dependence;study;study effect;strongly;work present results"}, "ebcb425ed1a51e8f1a6eca422882abd454fe04f2": {"ta_keywords": "learning vocabulary;target vocabulary provides;information learning vocabulary;learning vocabulary addition;target vocabulary;second language learners;vocabulary addition provide;identifies target vocabulary;vocabulary provides;language learners abilities;vocabulary addition;vocabulary provides lexical;language learners;quiz target words;provides lexical information;rich lexical information;target words present;lexical information generates;useful information learning;provide rich lexical;learners;contact learned speak;learners abilities;target words;vocabulary;lexical information collocations;lexical information;provides lexical;learners abilities evaluation;information generates quiz", "pdf_keywords": "word sense disambiguation;information extraction tasks;providing rich lexical;rich lexical information;learning word sense;information extraction;lexical information target;lexical information;sense disambiguation;vocabulary learning linggle;learning word;disambiguation;information target words;phrases document link;vocabulary learning;automated extraction grammar;reading learning word;sense disambiguation wsd;extraction grammar patterns;phrases document;key phrases document;wikipedia articles word;wiki\ufb01cation educational materials;extraction grammar;articles word sense;wiki\ufb01cation educational;corpora web;method corpora web;present prototype vocabulary;rich lexical"}, "4bff8862ba7956fdc2288e8399fb187b9595982b": {"ta_keywords": "text genetic annotation;text gene names;genetic annotation;named entity recognition;genetic annotation kind;description biocreative challenge;information extraction tasks;gene names scientific;finding gene names;gene names frequently;task description biocreative;biocreative challenge evaluation;discovery genetic names;scientific text genetic;gene names;retrieval information extraction;names scientific text;entity recognition;defined text mining;entity recognition ner;text genetic;information extraction summarization;genetic names defined;automated text mining;bioinformatics;text gene;information extraction;bioinformatics community;text mining information;domain bioinformatics", "pdf_keywords": ""}, "d8aeb318f68f4635b34c72aa1a0369fadcd79450": {"ta_keywords": "user communities microblogging;communities microblogging;communities microblogging networks;topicspecific sentiments behaviors;simple model microblogging;model microblogging;microblogging networks;microblogging networks based;model microblogging small;user communities;microblogging;associates topicspecific sentiments;sentiments behaviors user;topic model;networks based sentiments;derive user communities;topics community;topicspecific sentiments;new topic model;emotional topics community;topics community addition;topic model called;sentiments behaviors;microblogging small;behaviors user community;probabilistic graphical model;user community;propose probabilistic graphical;microblogging small number;content behaviors", "pdf_keywords": ""}, "8442f9fd620ea34e1de3128b9388bddd1263f29b": {"ta_keywords": "quadratic convex optimization;convex optimization problems;convex optimization;minimization convex quadratic;optimization minimization convex;conic optimization minimization;quadratic programming;conic optimization named;minimization convex;simple quadratic programming;optimization minimization;quadratic programming solver;conic optimization;method conic optimization;convex quadratic;quadratic convex;convex quadratic function;petri nets;optimization problems;optimization;petri nets variety;problems conic optimization;minimization;conic constraints;primal dual infeasibility;optimization named;infeasibility class quadratic;dual infeasibility;application petri nets;integral projected gradient", "pdf_keywords": "conic optimization convex;conic optimization generalizes;control conic optimization;optimization conic optimization;robotics conic optimization;optimization conic;solving convex optimization;convex optimization;conic optimization provide;optimization convex;conic optimization;proportional optimization conic;conic optimization applications;minimizes mass quadrotor;convex optimization problems;constrained optimal control;predictive control conic;optimization convex analysis;instance conic optimization;method solving convex;constrained optimal;optimization generalizes linear;programming quadratic pro;convex function minimizes;solving convex;linear programming quadratic;optimal control;programming proportional optimization;optimization generalizes;cone programming semi"}, "ee24fb876e6f1b345d492101c499bc5dd6b8196b": {"ta_keywords": "eeg signal separation;electroencephalogram eeg signals;separates eeg signals;electroencephalogram eeg signal;signal enhancement electroencephalogram;frequency electroencephalogram eeg;eeg signals;erp electroencephalogram eeg;electroencephalogram eeg;erp electroencephalogram;separates eeg;eeg signal;multi frequency electroencephalogram;determination electroencephalogram eeg;frequency electroencephalogram;electroencephalogram eegg signal;eeg signals event;enhancement electroencephalogram;eeg signals individual;electroencephalogram;brain waveform separation;electroencephalogram eeg used;determination electroencephalogram;potentials erp electroencephalogram;electroencephalogram components;electroencephalogram eegg;blindly separates eeg;sparseness eeg component;target electroencephalogram components;electroencephalogram components p300", "pdf_keywords": ""}, "ce0fce520c639af010c71cc6adf57cdeb2790322": {"ta_keywords": "optimization automatic speech;black box optimization;optimization automatically tunes;speech recognition systems;automatic speech recognition;speech recognition;speech recognition asr;probabilistic black box;automatic speech;performance speech recognition;speech recognition presented;box optimization automatic;box optimization automatically;optimization automatic;optimization automatically;recognition asr systems;bayesian optimization;optimization using gaussian;automatically tunes systems;optimization techniques covariance;bayesian optimization using;automatically tunes;es bayesian optimization;box optimization techniques;knowledge black box;use black box;recognition asr;box optimization used;box optimization;black box", "pdf_keywords": ""}, "516a0faeab9ec3a68bc6e7ec13a2df235a27ab52": {"ta_keywords": "unstructured clinical data;features unstructured clinical;clinical notes prediction;cnn model suitable;medical meaningful features;supporting diagnosis decision;deep network;learn semantic features;cnn model;deep network models;semantic features unstructured;clinical data;unstructured clinical notes;features unstructured textual;model learn semantic;electronic health records;semantic features;representing complex medical;cnn;propose convolutional neural;noisy unstructured clinical;unstructured clinical;supporting diagnosis;health records;information electronic health;convolutional neural network;health records ehrs;automatic diagnostic;neural network;world clinical data", "pdf_keywords": "predicting compositionality words;features unstructured textual;semantic features unstructured;unstructured clinical notes;learn semantic features;unstructured textual input;health outcomes unstructured;semantic features;predicting health outcomes;predicting health;unstructured textual;model predicting health;new machine learning;predicting compositionality;model predicting compositionality;outcomes unstructured clinical;model learn semantic;unstructured clinical;learn semantic;proposed convolutional neural;clinical notes;machine learning model;propose convolutional based;machine learning;medical decision support;diagnosis medical decision;textual input;clinical notes proposed;textual;textual input automatically"}, "d3793ae5b3b31f72605978b749e41811e6dcacd4": {"ta_keywords": "inverse reinforcement learning;inverse reinforcement;uses inverse reinforcement;reinforcement learning learn;society inverse reinforcement;reward environment learn;reinforcement learning;reinforcement;agents maximize reward;contextual bandit based;agent able learn;contextual bandit;reinforcement learning used;reward environment;maximize reward environment;use contextual bandit;agents systems play;bandit based orchestrator;autonomous cyber;learn act optimally;step autonomous cyber;allow agents maximize;cyber physical agents;agents maximize;agents systems;physical agents systems;environment learn follow;ensure agents behave;maximize reward;agents behave ways", "pdf_keywords": "inverse reinforcement learning;agents learning policies;learning policies constrained;inverse reinforcement;reward maximizing policy;learning policies;obtained inverse reinforcement;learn reward maximizing;maximizing constrained policy;reinforcement learning demonstrations;learning domain reward;maximizing policy environment;agent learn policies;reward maximizing constrained;learning learned rewards;reinforcement learning learned;reinforcement learning;maximizing policy obtained;domain reward maximizing;maximizing policy;learning demonstrations;constraint satisfying policy;reinforcement learning framework;learned rewards;learned rewards learn;reinforcement;constrained policy;learning demonstrations humans;reward maximizing;environment present reinforcement"}, "405c0d9b7cf5482d2e1197167f690e7b7801b9bd": {"ta_keywords": "unsupervised multi hop;hop training data;train conditioned unsupervised;multi hop training;pretraining generated data;training data homogeneous;hop training;training data;labeled multi hop;conditioned unsupervised;conditioned unsupervised multi;pretraining generated;multi hop qa;unsupervised multi;propose unsupervised framework;human annotated training;annotated training data;hop qa model;training data explore;unsupervised framework;response unsupervised multi;like multi hop;hop qa;multi hop;possibility train conditioned;unsupervised;multi hop basketball;response unsupervised;inferred single data;hop basketball", "pdf_keywords": "hop question answering;answerability multi hop;question answering;reasoning graphs;question answering online;question answering propose;based reasoning graphs;reasoning graphs framework;answering online learning;unsupervised machine translation;reasoning chains;operators reasoning graphs;answerability multi;answer pairs framework;modalities reasoning chains;reasoning chains de\ufb01ning;framework answerability multi;answer complex questions;question steps selecting;unstructured data answer;multi hop queries;reasoning graphs paper;hop queries;present framework answerability;machine translation generate;framework answerability;machine translation;question answer pairs;ality based reasoning;hop qa"}, "7bf2620188c0a66e1d0e779083cf61960a2f3e2f": {"ta_keywords": "constrained multilevel synthesis;synthesis optimization combinational;multilevel synthesis;multilevel synthesis algorithm;cell libraries synthesis;synthesis optimization;present multilevel synthesis;libraries synthesis optimization;combinational logic circuits;synthesis algorithm synthesis;synthesis single molecule;synthesis capable generating;multilevel synthesis standard;generating combinational logic;synthesis capable;synthesis algorithm;optimization combinational logic;synthesis single;libraries synthesis;algorithm synthesis single;describes synthesis capable;paper describes synthesis;synthesis;synthesis quality;describes synthesis;capable generating combinational;libraries synthesis quality;logic circuits;timing constraints;synthesis standard cell", "pdf_keywords": ""}, "d04c91bbb043666ebd6dae51995ee5bbc4291ddf": {"ta_keywords": "reaction xmath0 cern;xmath0 cern sps;xmath0 cern;study reaction xmath0;reaction xmath0;xmath0;cern sps;cern;results study reaction;reaction;study reaction;present results;sps;present results study;results;article present results;present;results study;article present;article;study", "pdf_keywords": ""}, "b209f2acbf0fbc62a3fd19ad6c13abbd46547736": {"ta_keywords": "speaker embedding extractor;based speaker embedding;continuous speech separation;speech separation leakage;speech separation;speaker embedding;microsoft speaker;res2net based speaker;talker systems;talker systems present;response microsoft speaker;based speaker;multi talker systems;diarization output voting;context voxsrc competition;based continuous speech;learning context voxsrc;speaker;design multi talker;diarization error rate;multi talker;voxsrc competition;diarization output;microsoft speaker sudden;voxsrc competition 2020;voxsrc;ventralator best;diarization;diarization error;ventralator best achieves", "pdf_keywords": "automatic speaker diarization;speaker diarization based;fusion speaker diarization;speaker diarization systems;speaker diarization task;multiple speaker diarization;speaker diarization promising;novel speaker diarization;speaker diarization;speaker embedding extractor;recognition multi speaker;talker diarization based;based speech separation;scenarios speaker diarization;res2net based speaker;based speaker embedding;multi talker diarization;speech recognition multi;speech separation;speaker embedding;diarization based deep;speaker diarization real;speech separation leakage;multi speaker conversation;automatic speech;multi talker recordings;speech recognition;talker diarization;automatic speech recognition;handle overlapped speech"}, "9e3e6ddf958c2005f7041cc9dd5fe050a0dbd02e": {"ta_keywords": "multiscale wavelet transform;image multiscale wavelet;multiscale wavelet;wavelet transforms;wavelet transform;using wavelet transforms;wavelet transform mwt;image multiscale;points wavelet transform;wavelet transforms shown;multiple resolution analysis;crossing points wavelet;wavelet transform domain;observation image multiscale;analysis using wavelet;signal correspondence method;image processing correspondence;resolution analysis using;multiscale;points wavelet;wavelet;quality signal correspondence;using wavelet;resolution analysis;signal correspondence;signal correspondence process;function signal correspondence;algorithm signal correspondence;signal correspondence problems;correspondence signals improved", "pdf_keywords": ""}, "4ec1d3407a5136c525b53f703c803571200902a4": {"ta_keywords": "brownian animals crowded;population brownian animals;dynamics population brownian;animals crowded environment;brownian animals;population brownian;animals crowded;dynamics population;crowded environment;study dynamics population;brownian;crowded;dynamics;animals;population;study dynamics;paper study dynamics;environment;paper study;study;paper", "pdf_keywords": ""}, "f2818da69bb72526fff9d601677db38f24a62ecc": {"ta_keywords": "dialogue modeling;based dialogue modeling;prediction methods dialogue;methods dialogue modeling;dialogue modeling important;user satisfaction prediction;example based dialog;selecting response utterances;predicts user satisfaction;dialogue modeling ebdm;dialog example databases;dialog example;based dialog;propose response selection;satisfaction prediction methods;dialog;example based dialogue;improve dialogue quality;improving user satisfaction;responses user feedback;response utterances;satisfaction prediction prediction;improve dialogue;dialogue quality;based satisfaction prediction;user satisfaction adapt;satisfaction prediction;response utterances examples;response selection;estimate user satisfaction", "pdf_keywords": ""}, "febb305a854d02b138250a8a19af956ffa0ada4f": {"ta_keywords": "policy gradient algorithms;gradient dynamics games;convergence multi agent;linear quadratic games;convergence nash equilibria;counterexample policy gradient;quadratic games satisfy;quadratic games;convergence nash;policy gradient;policy gradient avoid;solving reinforcement learning;local convergence nash;reinforcement learning problems;gradient avoid nash;nash equilibria continuous;conditions policy gradient;gradient algorithms guarantees;global nash;global nash equilibrium;reinforcement learning;unique global nash;quadratic games paper;games satisfy conditions;nash equilibrium generate;avoid nash equilibrium;multi agent;dynamics games;gradient play linear;problems multi agent", "pdf_keywords": ""}, "f21a9d70319ca99227300349d7bcab5dee5869cd": {"ta_keywords": "incomplete multilingual corpus;translation tokens multilingual;incomplete multilingual corpora;multilingual corpus translations;multilingual corpus;multilingual speech recognition;multilingual corpora multi;multi source translation;multilingual corpora;tokens multilingual multilingual;multilingual multilingual speech;incomplete multilingual;multilingual speech;tokens multilingual;use incomplete multilingual;using incomplete multilingual;corpus translations missing;translation tokens;study translation tokens;multilingual multilingual;machine translation nmt;corpus translations;multilingual;missing source translations;neural machine translation;incomplete corpora training;multi source neural;translation nmt using;corpora multi encoder;translations missing", "pdf_keywords": "incomplete multilingual corpus;incomplete multilingual corpora;multi source translation;multi source translators;nmt situations multilingual;multilingual corpus;source machine translation;multilingual corpora subtitles;multilingual corpus available;incomplete multilingual;multilingual corpora multi;multilingual corpus uses;use incomplete multilingual;multilingual corpora;using incomplete multilingual;multilingual corpora improve;missing source translations;source translators trained;available resulting multilingual;multilingual corpora ted;real incomplete multilingual;nmt speci\ufb01c language;machine translation;source translators;translation using incomplete;bi lingual nmt;corpora improve nmt;situations multilingual corpus;source translations;lingual nmt work"}, "b168fc72fa39e9669567bd099bab179549a15e14": {"ta_keywords": "gpi iga antibodies;a\u03b22 gpi iga;association a\u03b22 gpi;iga antibodies;a\u03b22 gpi;phospholipid anti phospholid;iga antibodies paper;thrombosis progression affine;association a\u03b22;phospholipid anti;action anti phospholipid;anti phospholipid;domains a\u03b22 gpi;phospholipid;anti phospholipid anti;anti phospholid syndrome;target domains a\u03b22;iga apl determination;thrombosis progression;antibodies;a\u03b22;significant association a\u03b22;relationship thrombosis progression;study thrombosis revealed;thrombosis;relationship thrombosis;thrombosis revealed;study thrombosis;phospholid syndrome;thrombosis revealed significant", "pdf_keywords": ""}, "de43afd166a79c24b3a7dd16c5695059d9f0aa71": {"ta_keywords": "theory cognitive development;cognitive development shown;children understand transitivity;cognitive development;cognitive development child;piaget theory cognitive;transitivity understood children;train cognitive development;cognitive learning versus;theory cognitive;learning versus age;child infancy;physical analogy train;cognitive learning;development child infancy;analogy train cognitive;understood children years;children understand;child infancy adulthood;help children understand;infancy adulthood studied;cognitive;infancy;train cognitive;versus age child;concept transitivity understood;infancy adulthood;timescale cognitive learning;concept transitivity;children using physical", "pdf_keywords": ""}, "ab94fae3d49cd7016a47020469dc257d8090f5bb": {"ta_keywords": "speaker separation db;speaker separation significantly;db speaker separation;multi speaker separation;challenging speech separation;speaker separation;speech separation problem;speech separation;speaker separation paper;perform speaker independent;speaker independent multi;independent multi speaker;baseline db speaker;speech recognition;db speaker;separation significantly improve;context speech recognition;channel multi speaker;speaker independent;multi speaker;automatic speech recognition;automatic speech;performance automatic speech;separation db;separation evaluate performance;perform speaker;speech recognition context;recognition context speech;performance challenging speech;separation significantly", "pdf_keywords": "supervised speech separation;separation deep networks;speaker separation deep;separating recognizing speech;speech separation;speaker separation;deep clustering;years deep clustering;deep clustering machine;presented speaker separation;known deep clustering;networks deep clustering;deep clustering widely;speech separation paper;separation deep;mixtures deep clustering;deep clustering work;present deep clustering;deep clustering framework;deep clustering formalism;trained twospeaker mixtures;supervised speech;end speech recognition;separating recognizing;recognizing speech;single channel separation;twospeaker mixtures deep;models trained twospeaker;channel separation;channel separation using"}, "6c477a65f0922d405c3665e31581eaa0f269116e": {"ta_keywords": "word representation knowledge;distributional relational semantics;learning word representation;relational objective wordnet;word representations;word representation;wordnet;representation knowledge base;learning word;objective wordnet;hypothesis word representations;relational semantics;text relational objective;knowledge base;relational semantics end;text relational;distributional relational;problem learning word;incorporate distributional relational;semantics;semantics end employ;representation knowledge;raw text relational;semantics end;word representations ought;relational objective;knowledge base investigate;distributional objective raw;knowledge;relational", "pdf_keywords": "word representations;word representations representations;word embeddings;words distributional semantics;distributional semantics words;modeling relations wordnet;words semantic regularization;admm word representations;semantic regularization words;distributional semantics;distributional relational semantics;represent word relationships;semantic regularization;natural language processing;word relationships function;hypothesis word representations;relations wordnet;word embeddings used;regularization words distributional;prediction semantic regularities;paper word embeddings;wordnet joint objective;representations words phrases;relations wordnet joint;semantic regularities words;words distributional;representations capture semantic;prediction semantic;semantics words;word relationships"}, "10085f7fb0871329d34529cc54df0a8f75756fce": {"ta_keywords": "performance automatic speech;automatic speech;automatically generate spoken;automatic speech recognition;spoken style training;generate spoken style;speech recognition;generate spoken;speech recognition asr;speaking style transformation;supervised training lsv;spoken style;speaking style;recognition asr;verbatim transcripts framework;training texts labels;recognition asr systems;training texts;using verbatim transcripts;speech;training lsv;verbatim transcripts;supervised training;training lsv approaches;style training texts;prediction improved using;prediction improved;consists speaking style;accuracy prediction improved;team sports team", "pdf_keywords": ""}, "248824ec5d9b4ddf0c36cdc51b6b57af6e881328": {"ta_keywords": "lingual transfer phylogenetic;transfer languages ranking;prediction crosslingual transfer;transfer phylogenetic information;transfer phylogenetic;transfer languages better;language transfer language;crosslingual similarity;good transfer languages;concept crosslingual similarity;prediction crosslingual;transfer languages;language transfer;cross lingual transfer;optimal transfer languages;lingual transfer;perform prediction crosslingual;transfer language;crosslingual transfer;languages ranking;nlp low resource;transfer language use;approach cross lingual;transfer language used;cross lingual;crosslingual transfer high;different nlp task;resource transfer language;languages ranking problem;clear language transfer", "pdf_keywords": "transfer languages nlp;machine translation cross;transfer languages task;transfer corpora subwords;transfer languages;transfer corpora;task transfer corpora;choosing transfer languages;cross lingual transfer;transfer learning;transfer languages machine;translation cross lingual;optimal transfer languages;transfer languages use;neural machine translation;lingual transfer;lingual transfer weighting;promising transfer languages;languages nlp task;lingual named entity;nlp task ranking;problem transfer languages;languages machine translation;model transfer learning;named entity recognition;machine translation propose;languages nlp;cross lingual named;machine translation;transfer learning able"}, "70170035ef870df1c064cc52804178a52f6a69ef": {"ta_keywords": "record movement human;learn video;learn video clip;attempt learn video;video camera;video camera used;movement human attempt;video clip;showing video camera;showing video;movement human;camera;video showing video;record movement;video;video showing;present video showing;camera used record;present video;camera used;used record movement;human attempt learn;movement;clip;human attempt;record;attempt learn;human;used record;learn", "pdf_keywords": ""}, "75c4aefc55bf0b345587740cad0a4e994f29962a": {"ta_keywords": "sound event detection;sound event activity;sound activity detection;segments sound event;sound activity;polyphonic sound event;detection polyphonic sound;sound detection polyphonic;using sound activity;detection polyphonic;sound event;sound event dependent;model sound event;sound detection;hybrid sound detection;identify segments sound;approach polyphonic sound;activity detection sad;recurrent neural network;approach polyphonic;short term memory;event detection;postprocessing using sound;polyphonic sound;frame detection;activity detection;memory recurrent neural;polyphonic;recurrent neural;segments sound", "pdf_keywords": ""}, "cc19de8d0782917098029ed20261cbe0b0c62bf5": {"ta_keywords": "biases review text;detects biases review;quantify biases text;review ratings biases;ratings biases text;biases text peer;biases review;ratings biases;bias inferred;truth bias;truth bias inferred;framework quantify biases;framework quantify bias;biases text;quantify bias;quantify biases;bias inferred evaluate;bias identify;biases text written;bias;detects biases;biases;quantify bias caused;text peer reviews;accurately detects biases;peer reviews;societal biases;estimate bias;estimate bias identify;bias identify application", "pdf_keywords": "biases text peer;biases review text;bias discovery;related bias discovery;peer review data;bias discovery method;biases text;text peer reviews;quantifies biases text;biases review;bias available peer;review ratings peer;peer review;peer reviews;attribute reviewers peer;identifying attribute reviewers;peer reviews written;reviewers peer;scholarly peer review;ratings peer reviews;formalize bias;reviewers peer review;estimate biases review;available peer review;biases;bias available;text review ratings;bias;peer reviews international;peer review process"}, "8484fdb56e4690927dc0191ede11c2d24bc5e2ef": {"ta_keywords": "openended text generation;text generation;machine generated text;text generation model;text generation directly;distribution text generation;generated text;generated text human;open ended text;approach automatic generation;automatic generation;automatic generation open;human written text;generation open ended;text human language;distribution human written;machine generated;generation open;learnt distribution text;written text;text human;generation directly;openended text;measure openended text;text;distribution text;text close machine;written text using;generation directly compares;ended text", "pdf_keywords": "automatic language generation;distribution human text;human text empirically;text automatic metrics;language modeling comparisons;language modeling;language generation;metrics generation perplexity;domains generation dialogues;text empirically mauve;automatic language;text empirically;distributional evaluation metrics;language generation based;evaluation metrics generation;automatic perplexity sampling;generation dialogues;metrics generation;generation dialogues stories;approach automatic language;components language modeling;quality judgements text;text length model;information divergence frontiers;existing distributional evaluation;human text;judgements text automatic;automatic metrics proposed;automatic metrics;generated text"}, "aff5d7f43823e06bb68220db41de3bc82e2f3990": {"ta_keywords": "handoff tier cellular;tier cellular networks;network model poisson;cellular networks;handoff outage periods;handoff outage;poisson network model;tier cellular;cellular networks tmns;downlink data rate;network model;outage mus handoff;study poisson network;poisson network;tier network structure;heterogeneous tier network;average downlink data;handoff tier;average downlink;network statistically equivalent;accounted handoff outage;tier network;cellular;network statistically;static mobile users;performance mobile users;mus handoff tier;calculate average downlink;handoff;frequent data outage", "pdf_keywords": ""}, "45cdf5e239a1f0057c350f6654ccd348fb4e2332": {"ta_keywords": "stable matching exists;stable matchings;stable matchings presence;stable matching;sided stable matching;stability stable matchings;stable matching setting;certainly stable matching;matching setting uncertainty;matchings presence uniaxial;probability given matching;matchings;given matching;matching highest probability;matching;matching finding matching;matchings presence;finding matching;matching finding;matching exists;given matching finding;models uncertainty lottery;uncertainty lottery model;matching exists consider;uncertainty agents preferences;probability stable;uncertainty lottery;matching highest;matching setting;highest probability stable", "pdf_keywords": "matching exists uncertainty;matching withhigheststabilityprobability;stable matching exists;solve matching withhigheststabilityprobability;matching withhigheststabilityprobability e\ufb03ciently;stable matching;given matching stable;matching stable;exists uncertainty preferences;preference models probabilistic;probabilistic preference;certainly stable matching;marriage problem models;probabilistic preference models;\ufb01nding matching;pairwise comparisons agents;matching \ufb01nding matching;uncertainty preferences arise;uncertainty preferences;preference models;matching highest probability;matching \ufb01nding;probability stable;stable marriage problem;lottery given matching;matching exists;comparisons agents complexity;probabilistic;models based pairwise;probability given matching"}, "4731f89169604cd0d8b5352380baa1b4728bca0b": {"ta_keywords": "regularized discriminative language;discriminative language models;discriminative language;translation systems;language models;language models lmms;translation systems demonstrate;machine translation;machine translation systems;error analysis machine;analysis machine translation;regularized discriminative;use regularized discriminative;error analysis;language pairs evaluation;effective error analysis;systems language pairs;focused error analysis;error analysis paper;language pairs;simple mechanical model;systems language;model motion;mechanical model;discriminative;mechanical model used;displacement motor;differences systems language;used model motion;analysis machine", "pdf_keywords": ""}, "601408d6617bf72894c9f41ae54cf9c17905903a": {"ta_keywords": "string machine translation;efficient accurate translation;phrasebased systems improved;machine translation theoretically;machine translation;machine translation mt;inferior machine translation;par phrasebased systems;phrasebased systems;translation mt methods;tree string machine;phrase based hierarchical;performs par phrasebased;hierarchical phrase based;t2 systems;string machine;t2 performs par;basic t2 performs;translation theoretically holds;translation theoretically;basic t2;t2 systems constructed;based hierarchical phrase;accurate translation previous;phrase based mt;t2 performs;par phrasebased;japanese english pairs;performance gap t2;pairs basic t2", "pdf_keywords": ""}, "3dc20be709818630e2249ab28b35b0666b4b544d": {"ta_keywords": "speech translation systems;speech s2 translation;translate content utterance;translation systems translate;speech translation;translation cultural identity;paralinguistic information languages;translation systems;analyze paralinguistic information;translation systems conventional;systems translate;speech s2;paralinguistic information;s2 translation systems;analyze paralinguistic;translation cultural;speech speech s2;systems translate content;paralinguistic information included;paper analyze paralinguistic;semif semiflexible swimmer;semiflexible swimmer wsw;ignore paralinguistic information;english translation cultural;perfect speech translation;semiflexible swimmer;utterance ignore paralinguistic;paralinguistic;systems conventional speech;languages", "pdf_keywords": ""}, "ead6323f137c2f99ef0ffcfa34fa6eb1c6eca3c6": {"ta_keywords": "natural speech chime;speech recognition challenge;person identity crowdsourced;speech chime 6th;speech chime;speech diarization recognition;dynamics natural speech;chime multispeaker speech;diarization speech recognition;crowdsourced manner chime;chime challenge revisits;natural speech;description chime multispeaker;chime challenge held;speaker diarization speech;speech recognition scenario;speech recognition modules;multispeaker speech recognition;chime challenge;diarization recognition everyday;speech recognition;speaker diarization;speech diarization;chime multispeaker;diarization speech;recognition challenge track;enhancement speaker diarization;identity crowdsourced;speech enhancement speaker;manner chime challenge", "pdf_keywords": "speaker diarization crowdsourced;diarization crowdsourced speech;crowdsourced speech recognition;speech diarization recognition;diarization speech recognition;speech recognition diarization;speaker diarization speech;speaker diarization;diarization crowdsourced;speech diarization;home environments speech;diarization speech;diarization recognition everyday;speech recognition;enhancement speaker diarization;speech recognition present;learning speech recognition;speech activity detection;environments speech activity;spoken speech systems;machine learning speech;crowdsourced speech;track combine speech;problem speech diarization;paper speaker diarization;environments speech;speech enhancement speaker;speech systems;diarization paper speaker;diarization recognition"}, "af5c4b80fbf847f69a202ba5a780a3dd18c1a027": {"ta_keywords": "language inference commonsense;grounded commonsense inference;commonsense inference unifying;inference commonsense;commonsense inference;inference commonsense reasoning;natural language inference;datasets propose adversarial;adversarial filtering;adversarial;task grounded commonsense;language inference;commonsense reasoning;propose adversarial filtering;adversarial filtering af;language models massively;propose adversarial;stylistic classifiers;grounded commonsense;unifying natural language;commonsense;inference unifying natural;biases existing datasets;language models;biased dataset;ensemble stylistic classifiers;inference unifying;commonsense reasoning paper;human biases existing;stylistic classifiers using", "pdf_keywords": "dataset grounded commonsense;language inference commonsense;inference commonsense;commonsense inference broadens;inference commonsense reasoning;commonsense inference;grounded commonsense reasoning;adversarial language inference;commonsense reasoning results;commonsense reasoning;dataset adversarial language;reasoning introducing adversarial;situated commonsense inference;commonsense reasoning introducing;grounded commonsense;focused grounded commonsense;commonsense reasoning easy;decomposable attention 2015;adversarial language;commonsense;new dataset adversarial;language inference train;dataset adversarial;adversarial;introducing adversarial;natural language inference;introducing adversarial \ufb01ltering;inference train;adversarial \ufb01ltering;easily detectable annotation"}, "d15eb5744474cec2d0634651bb30000b3873a309": {"ta_keywords": "time expression normalization;normalized temporal;normalization rules time;construct normalized temporal;recognition time;normalization rules training;recognition normalization artime;normalized temporal value;recognition time dependent;normalization artime automatically;normalization artime;rules time expressions;tasks recognition normalization;time expressions common;recognition normalization;predicting head;predicting head collision;method predicting head;normalization;expression normalization;time expressions;normalization rules;progress recognition time;generates normalization rules;normalization methods;head collision bird;normalized;automatically generates normalization;time expression;rules time", "pdf_keywords": "normalizing time expressions;normalization rules annotated;time expression normalization;normalizer time expressions;expressions natural language;normalization rules training;generating normalization rules;expressions compositional annotations;recognizing normalizing time;time expression recognition;rules annotated data;generate normalization rules;rule engineering recognizing;expression normalization;expression recognition normalization;rules annotated;time expressions natural;semantic structure expressions;rules different corpora;time expression analysis;automatically generating normalization;annotated data time;normalizing time;designed rules grammars;rules grammars;recognizing normalizing;syntactic token;rules grammars propose;time expressions;natural language"}, "3b563c16e9a918631d63a20027dad735b625625a": {"ta_keywords": "rigid body texar;quantum body systems;body texar designed;quantum body;text generation;text based multimedia;body texar;text generation tasks;networks introduce texar;present toolkit simulation;generation text based;analysis quantum body;toolkit simulation;generation text;simulation analysis quantum;generation tasks toolkit;motion rigid body;modelled motion rigid;environments modelled motion;body systems achieved;body cluttered environments;rigid body;paradigms movement body;approach generation text;texar open source;multimedia networks;arbitrary model architectures;body systems;toolkit simulation analysis;movement body", "pdf_keywords": "machine translation conversation;particularly machine translation;learning machine translation;text generation tasks;machine translation;translation conversation systems;machine translation support;machine translation present;text generation;machine learning toolkit;learning toolkit;toolkit texar;toolkit texar designed;enrich toolkit;toolkit called texar;toolkits;diverse machine learning;texar leverages machine;framework text generation;toolkit;natural language processing;conversation systems;translation conversation;natural language;generation tasks;texar;deep learning;art machine translation;learning toolkit called;excited enrich toolkit"}, "a8372f7cb2e482a455b06c3e47f65aec5c7a924b": {"ta_keywords": "electromagnetic pump design;future electromagnetic pump;electromagnetic pump;induction pump body;pump conversion experimental;dipole accelerator driven;pump design;pump body forces;numerical study pump;pump efficiency circulation;em pump regime;efficiency em pump;dipole accelerator;linear induction pump;pump conversion;induction pump;circulation loop liquid;pump efficiency em;class dipole accelerator;theoretical study pump;electromechanical el body;study pump;study pump efficiency;pump body;circulation liquid alloy;pump pump conversion;em pump;electromechanical el;elasto electromechanical;pump pump", "pdf_keywords": ""}, "0c07cc7ba1b862556f5cfee0d5d849866d21a693": {"ta_keywords": "storage code linear;protocols distributed storage;nonlinear update protocols;necessary storage code;storage code;oblivious update process;distributed storage;communication necessary storage;distributed storage systems;systems storage nodes;storage nodes;update protocols distributed;lower bound communication;update algorithm meets;separable mds codes;necessary oblivious update;stale data information;bound communication;oblivious update;allowing nonlinear update;data stale node;storage systems;storage systems storage;systems storage;bound communication necessary;update algorithm;storage;codes linear;protocols distributed;present update algorithm", "pdf_keywords": "information distributed storage;distributed storage;codes lower bound;oblivious update certain;data information distributed;distributed storage systems;storage nodes;oblivious update;systems storage nodes;required oblivious update;data sharing;information distributed;data sharing mobile;stale data information;separable mds codes;data updated nodes;download required oblivious;storage node;modi\ufb01ed data stale;storage node update;distributed;lower bounds download;codes known maximum;updated nodes access;access stale data;data stale node;updated nodes knowledge;distributed approach;storage;stale data"}, "9650dbe79d34498113371770dcdb48f1bd7c9711": {"ta_keywords": "topic map;topic map use;creation topic map;term similarity creation;term extraction;term extraction using;information retrieval;term similarity;extensible term extraction;heatmaps visualize profile;heatmaps visualize;help heatmaps visualize;research topics field;visual exploration research;topics field;natural language processing;university heatmap;heatmap;similarity creation topic;methods information retrieval;visualize profile;heatmaps;visualize profile particular;department university heatmap;topics field years;visual exploration;research papers;maps computer science;similarity creation;university heatmap overlays", "pdf_keywords": "visualization scientific research;visualization text collections;visualization text;visualization scientific;interactive visualization text;visualize word clouds;topographic visualization scientific;statistical topic modeling;topics text collection;paper visual exploration;visualize word;document similarity semantic;novel data visualization;topic modeling;data visualization;topic modeling relies;topographic visualization;use topographic visualization;topics text;document similarity;visualization;term document similarity;interactive visualization;data visualization called;designed visualize word;corpora paper visual;research papers computer;topographic visualization standard;latent dirichlet allocation;semantic thematic topics"}, "889c3b4394826639d483c039467cd9a05e68e73c": {"ta_keywords": "sampling human composers;composition machine learning;learning models music;gibbs sampling greatly;previously gibbs sampling;composing piece;ancestral sampling based;composition chronological process;composition chronological;models music;ancestral sampling;composition machine;blocked gibbs sampling;direct ancestral sampling;gibbs sampling;gibbs sampling analogue;models music typically;human composers write;sampling analogue rewriting;composing;human composers;write music nonlinear;composers write music;direction composition machine;composing piece single;samples ancestral sampling;write music;sampling greatly improves;generative;composers write", "pdf_keywords": "probabilistic model music;model music composition;music statistical models;generative model musical;statistical models music;models music computational;music composition;composition piano modeled;polyphonic music statistical;composing piece music;musical composition process;music composition based;neural networks musical;generation polyphonic music;music computational;music statistical;musical composition;music composition piano;computational music;computational music composites;model musical counterpoint;networks musical composition;music computational approaches;piano modeled sequence;model generation polyphonic;polyphonic music;idea music composition;models music;model music;composition piano"}, "68af273e04906e0450a5d01d5606c8313da01453": {"ta_keywords": "sensors estimation fusion;estimation fusion;sensor networks expectation;optimal sensor subset;sensors estimation;sensor subset selection;estimation fusion center;sensor networks;practical sensor networks;sensor networks time;possible sensors estimation;subset possible sensors;sensor subset;optimal sensor;sensors energy efficient;employed optimal sensor;distribution gibbs sampling;sensor networks necessitates;gibbs sampling based;scale sensor networks;gibbs sampling;sensors energy;optimize estimation;designed optimize estimation;activated sensors energy;sampling based methods;large scale sensor;learn distribution;optimize estimation error;networks expectation maximization", "pdf_keywords": "sensor subset selection;subsets sensors;sampling stochastic approximation;problem sensor subset;possible subsets sensors;sensor subset;sensor networks;constrained combinatorial optimization;gibbs sampling stochastic;sampling compute optimal;subsets sensors prove;sensor networks widespread;combinatorial optimization;based gibbs sampling;sensors chosen network;sampling stochastic;algorithms based gibbs;gibbs sampling;subset selection fusion;combinatorial optimization problems;gibbs sampling compute;stochastic approximation solve;sensors prove convergence;mmse computation combinatorial;stochastic approximation;constrained combinatorial;complexity mmse computation;combine gibbs sampling;algorithms;searching possible subsets"}, "04f8f739924a19c01d196a48783b914554ac0fe5": {"ta_keywords": "composite convex optimization;scheme convex optimization;convex optimization;minimizing convex functions;algorithms composite convex;minimizing convex;continuous hessian;counter minimizing convex;convex optimization called;domain newton methods;lipschitz continuous hessian;hessian;continuous hessian paper;convex optimization based;gradient method generalization;hessian paper;hessian paper present;scheme convex;second order algorithms;region scheme convex;newton methods improves;empirical risk minimization;composite convex;conditional gradient method;contracting domain newton;functional residual iteration;risk minimization;newton methods;optimization called contracting;convex functions lipschitz", "pdf_keywords": "regularized ball constraints;newton schemes based;sampled newton schemes;newton schemes;model regularized ball;model regularized;regression model regularized;iterations cubic regularization;training logistic regression;regularization;regularization established;smooth objective;sub sampled newton;training logistic;regularized ball;smooth objective represented;sampled newton;cubic regularization;cubic regularization established;convex twice;problem training logistic;regularized;ball constraints paper;methods global convergence;convex twice differentiable;convex;constraints paper;mathematical programming;case smooth objective;ball constraints"}, "86ae1161026f23f9df691a867fd7453cee56fd28": {"ta_keywords": "change lexicon phylogenies;lexicon phylogenies;lexicon phylogenies particularly;phylogenetic inference lexical;inference lexical cognate;lexical cognate models;change meaning phylogenies;phylogenies particularly change;lexical cognate;phylogenies fluid dynamics;meaning phylogenies;meaning phylogenies fluid;studying semantic change;lexical inference;phylogenies fluid;studying change lexicon;phylogenies;semantic change;inference lexical;phylogenies particularly;phylogenetic inference;change lexicon;lexical;relationship phylogenetic inference;phylogenetic;lexical inference survey;appropriate lexical inference;semantic change rely;relationship phylogenetic;lexicon", "pdf_keywords": "semantic change relies;semantic change;language change studied;language change influenced;study semantic change;understand language change;language change based;language change adequately;theory language change;biases language transmission;study language change;language change;linguistic variation;whilst language change;language splits linguistic;linguistic variation described;semantic change macro;splits linguistic variation;biases language;bias biases language;language transmission;linguistic;splits linguistic;speak does change;language splits;change influenced changes;change relies temporally;language transmission inhibit;inferring language splits;inferring language"}, "5b8eaaf660b9e2d6a19886991350fffa1320b372": {"ta_keywords": "inference based training;class probabilistic inference;local classifiers;just local classifiers;probabilistic inference;probabilistic inference problems;learn entities;classifiers;inference based;paper local classifiers;local classifiers used;class probabilistic;programming ilp inference;learn entities relations;strategies learn entities;probabilistic;local classifiers integer;classifiers used solve;inference problems;inference uses inference;ilp inference;classifiers used;inference;classifiers integer linear;solve class probabilistic;ilp inference uses;classifiers integer;uses inference based;inference uses;uses inference", "pdf_keywords": ""}, "781e0e81834119c135091c8bdfcd1966c10b09ab": {"ta_keywords": "list integers compressed;integers compressed memory;compressed memory sorted;integer compression;memory sorted integers;compression text warehouse;integers compressed;integer compression used;indexes database systems;compressed memory;million query track;memory sorted;sorted integers decomposed;method integer compression;integers decomposed sorted;decomposed sorted integers;integer data parallel;inverted indexes database;compress integer data;trec million query;data parallel sorted;database systems;indexes;queries double speed;lists integers commonly;integers decomposed;million query;investigate compression text;indexes database;sorted lists integers", "pdf_keywords": "implementation integer compression;integer compression;integer compression deltas;intersection algorithms uncompressed;speed compilers;simd decompression faster;intersection algorithms exploit;decompression faster scalar;speed compilers clang;compare speed compilers;compression deltas introduce;algorithms uncompressed lists;various speed compression;algorithms uncompressed;compression trade offs;speed compression;coding improve decompression;vector simd decompression;compression deltas;differential coding improve;intersection algorithms;lists 32 bit;speed compression trade;compilers;simd decompression;faster scalar nonsimdin;differential coding;32 bit integers;decompression faster;particular compression"}, "65f632cbac465633a13b1e3f8c8c410c2f3aec3d": {"ta_keywords": "multi agent reinforcement;robotic games;agent reinforcement;agent reinforcement learning;practical robotic games;reinforcement learning marl;agent deep deterministic;multi agent deep;stackelberg multi agent;robotic games work;stackekelberg game;simple competitive robotics;deep deterministic policy;competitive robotics;deterministic policy gradient;competitive multi agent;agent deep;reinforcement learning;learns better policy;game theoretic marl;competitive robotics environment;policy gradient;problem stackekelberg game;policy gradient st;multi agent;reinforcement;novel game theoretic;stackekelberg game player;deterministic policy;opponent asymmetry environment", "pdf_keywords": ""}, "76862a851bd2c17dcf6bfc2cecbf4af186730123": {"ta_keywords": "page segmentation paper;page segmentation;segmenting nontext objects;segmenting nontext;operators page segmentation;text object segmentation;segmentation paper present;segmentation paper;method segmenting nontext;segmenting halftone images;segmenting;segmentation;grayscale document image;object segmentation;unconventional method segmenting;document image;segmenting halftone;grayscale document;directly grayscale document;segmentation common;object segmentation common;problem segmenting;extract objects images;halftone images tables;document image making;thresholding connected operators;thresholding connected;thresholding;segmentation common phenomenon;problem segmenting halftone", "pdf_keywords": ""}, "1a20d6c6891f3a0462515ff9560bc37e66eb422a": {"ta_keywords": "animals walking noisy;walking noisy environment;dynamics animals walking;statistics season;school conferences;animals walking;walking noisy;players spring semester;school conferences organized;conferences;study statistics season;players spring;number players spring;statistics season averaged;dynamics animals;animals;conferences organized beautiful;noisy environment;averaged number players;conferences organized;noisy;number players;international school conferences;season averaged;investigation dynamics animals;season averaged number;conferences international school;noisy environment time;statistics;time conferences", "pdf_keywords": ""}, "68258e0541132027ef86f872b92406de1c6edab3": {"ta_keywords": "learn influence gravity;fluid dynamics video;dynamics video learn;dynamics video;fluid dynamics;gravity;influence gravity;video learn;dynamics;video learn influence;learn influence;learn;fluid;video;influence", "pdf_keywords": ""}, "b1d309073623d46548e55269fb73485a3b7f11a8": {"ta_keywords": "pretrained language models;behavior pretrained language;pretrained language;studied pretrained models;learning speeds pretraining;hidden markov models;knowledge pretrained models;pretrained models;learns;hidden markov;models pretraining;pretraining models;language models;learns reconstruct predict;learning speeds;ones pretraining models;learn dynamics;models pretraining steps;behavior pretrained;pretraining models pretraining;learn dynamics swimmer;models stable pretrained;pretrained models used;learns reconstruct;performance linguistic knowledge;knowledge pretrained;language model;pretrained models stable;performance behavior pretrained;class hidden markov", "pdf_keywords": "pretrained language model;mystery pretrained language;trained language models;embryology pretrained language;pretrained language;trained language;deep neural;language models investigate;language models;pretraining loss converges;language model;learning speeds reconstructing;language model looking;totipotent1 language model;pretrained model;language model observe;performance deep neural;fully pretrained model;model trained;neural;learning speeds;performance deep;language model refer;model world knowledge;mystery pretrained;world knowledge tasks;refer embryology pretrained;evolves observe learning;embryology pretrained;network model trained"}, "0110abf15bf0ee1bdf28061ad05f85b1c9f6e1c3": {"ta_keywords": "similarity measures web;web related databases;web based databases;structured information sources;analyzing web based;analyzing web;related databases;databases paper propose;related databases paper;technique analyzing web;studied similarity measures;databases based;similarity measures;queries paper propose;query experiments;combining information relations;databases based studied;knowledge integration;measures web related;query experiments prototype;based databases;measures web;distributed heterogeneous database;information sources containing;heterogeneous database;databases;databases including world;heterogeneous database paper;web related;web distributed heterogeneous", "pdf_keywords": ""}, "a3da7028a1b721e392c421c2f15096abb1a71afb": {"ta_keywords": "plaque vulnerability lipid;atherosclerotic plaque vulnerability;hba1c plaque vulnerability;influence hba1c plaque;a1c plaque vulnerability;atherosclerotic plaque;increased plaque vulnerability;progression atherosclerotic plaque;hb a1c plaque;vulnerability lipid variability;lipid profiles vulnerability;higher lipid variability;lipid variability;features lipid variability;variability lipid profiles;lipid variability despite;plaque vulnerability features;plaque vulnerability higher;lipid variability remains;hba1c plaque;lipid profiles;lipid profiles including;density lipoprotein cholesterol;plaque vulnerability;lipoprotein cholesterol;association glucose lipid;high densitylipoprotein lipid;assessment lipid macrophage;assessment lipid;cholesterol study", "pdf_keywords": ""}, "3ed07f6643856b9ac4687b3bc667767f3ab4b563": {"ta_keywords": "voice quality control;statistical voice quality;selecting voice quality;voice quality expression;voice quality;performance voice quality;parameters statistical voice;statistical voice;selecting voice;method selecting voice;performance voice;investigate performance voice;quality control;perceptual score assignment;scores independency acoustic;quality expression words;quality control report;quality control context;voice;control parameters statistical;quality expression;perceptual score;context perceptual score;perceptual scores;corresponding perceptual scores;independency acoustic features;acoustic features corresponding;perceptual scores independency;independency perceptual scores;control context perceptual", "pdf_keywords": ""}, "ecde7c041e9ac48bccef7a8d078a3f80239b0479": {"ta_keywords": "videos captures temporal;frames train recurrent;video objects dataset;frames object detection;object detection videos;object detection video;detection videos;detection videos captures;context frames regularization;captures temporal context;youtube video objects;frames weak supervision;video objects;improving object detection;videos captures;frames regularization;detection video;videos appear smooth;video present object;video motion;video motion blur;motion blur;supervision context frames;frames regularization smoothness;motion blur compression;labeled frames;supervision target frames;object detection static;context frames;videos", "pdf_keywords": "object detection videos;improving object detection;video objects dataset;detection videos;video object detection;detection videos captures;videos captures temporal;youtube video objects;predictions exploiting contextual;dataset demonstrate convolutional;video objects;present deep convolutional;captures temporal context;deep convolutional;video object;videos captures;model video object;videos;predictions train recurrent;object detection present;train recurrent neural;network model video;deep convolutional neural;object detection;objects dataset demonstrate;detection performance trained;exploiting contextual;frames generating predictions;demonstrate convolutional neural;recurrent neural"}, "085072963b33367b842369b9ce81394d32ac8843": {"ta_keywords": "deep learning separation;speech separation systems;channel speech separation;noise separation;separation systems noisy;noise separation errors;learning separation;speech separation;learning separation models;separation systems improved;better separation systems;neural network cleanly;demonstrate noisy speech;enabling better separation;noisy speech;separation systems;noisy speech paradigm;cleanly distinguish noisy;discount noise separation;training synthetic mixtures;single channel speech;separation models;separation errors;separation models need;training deep;separation errors enabling;sources training deep;noisy environments single;training deep learning;exploit inseparability noise", "pdf_keywords": "speech separation enhancement;channel speech separation;approach speech separation;speech separation;separating clean speech;speech separation paper;clean speech noise;mixtures noisy speech;single channel speech;deep single channel;speech noise signals;noisy speech paradigm;trained mixtures noisy;separation enhancement;noisy speech issue;noisy speech proposed;signals noisy mixtures;separation enhancement based;noise noisy speech;separate set noisy;noisy speech;speech noise;channel speech;unrelated audio;trained mixtures;separation performance;focus noisy speech;mixture includes noise;orthogonality unrelated audio;unrelated audio signals"}, "76fe5f80dd25078eefa522e59a7763bc5d5da826": {"ta_keywords": "alice experiment fermilab;channel alice experiment;entrance channel alice;experiment fermilab tevatron;alice experiment;experiment fermilab;channel alice;fermilab tevatron;entrance channel;performance entrance channel;fermilab;alice;experiment;channel;entrance;performance entrance;tevatron;study performance entrance;performance;results study performance;study performance;study;results study;results;report results study;report results;report", "pdf_keywords": ""}, "9165d5e99b2106825dd00b9f5daf60e454434399": {"ta_keywords": "simultaneous interpretation corpus;interpretation corpus;simultaneous interpretation interpreter;interpretation interpreter paper;interpreters;paper simultaneous interpreters;interpretation interpreter;simultaneous interpreters;interpretation corpus total;interpreter;study translation single;interpreter paper;words transcribed;different interpreters;interpreter paper simultaneous;carbon nanotube;transcriptions lectures;simultaneous interpreters different;corpus;interpreters different;interpretations styles;study translation;transcriptions;results study translation;interpreters comparing;words transcribed data;corpus used analyze;nanotube cnt carbon;carbon nanotube cnt;time aligned transcriptions", "pdf_keywords": ""}, "23d299b35366c18e397faeb2c8687c20f8e17688": {"ta_keywords": "detection deception attack;attack deep neural;image classifier attacker;deception attack deep;attack detection;classifier attacker;malicious deception attack;attack detection algorithm;images dnn based;dnn based image;complexity attacks pixel;classifier attacker model;competing attack detection;modified images dnn;deception attack;attacks pixel;classification autonomous cyber;dnn based classifier;deception attack investigated;detection deception;network dnn based;detection scheme outperforms;dnn based;facilitates detection modified;attack deep;attacks pixel values;detection scheme;proposed detection scheme;network dnn;images dnn", "pdf_keywords": ""}, "72302d8c5cdcf59b6df96290ffc874d3613fe6b1": {"ta_keywords": "american physical society;division american physical;american physical;annual meeting aps;meeting aps division;aps division american;meeting aps;aps;aps division;physical society;submission 66th annual;66th annual meeting;physical;video submission 66th;66th annual;annual meeting;division american;submission 66th;present video;66th;annual;note present video;video;society;meeting;present video submission;division;video submission;submission;note", "pdf_keywords": ""}, "12e9d005c77f76e344361f79c4b008034ae547eb": {"ta_keywords": "embed textual sequences;charagram embeddings simple;charagram embeddings;charagram embeddings outperform;embed textual;present charagram embeddings;models embed textual;demonstrate charagram embeddings;embedding word sentence;embedding word;character based compositional;embeddings;embeddings outperform;learning character;embeddings outperform complex;character level recurrent;embeddings simple;embeddings simple approach;learning character based;embedding;compositional models embed;low dimensional embedding;textual sequences;method embedding word;dimensional embedding;approach learning character;architectures based character;similarity tasks;models embed;recurrent convolutional neural", "pdf_keywords": "similarity word embeddings;similarity semantic compositionality;sentence similarity train;word similarity tasks;word similarity semantic;models word similarity;semantic similarity tasks;semantics word embedding;word embeddings;word embedding;word embeddings based;model semantic similarity;semantic textual similarity;semantic similarity;sentence similarity tasks;words distributional semantics;word sentence embeddings;similarity semantic;textual similarity tasks;word similarity;semantic similarity word;perform semantic similarity;word embedding fundamental;word embeddings commonly;networks word embeddings;distributional semantics;analysis word similarity;sentence similarity;textual similarity;representations word sequences"}, "f053137323a88eb932d590bcdfc959ee805e2520": {"ta_keywords": "transition based parsers;transition based parser;parse trees based;parse trees;based parsers;parsers;seamless stackptr parser_;projective parse tree;algorithm parse trees;parse tree;based parsers linear;parser_ introduce stack;based parser;parser;treebanks spanning 20;stack pointer networks;decoding algorithm parse;treebanks spanning;non projective parse;parse;based parser called;pointer networks stacks;treebanks;stackptr parser_;projective parse;parse tree evaluate;29 treebanks spanning;parsers linear;model 29 treebanks;stackptr parser_ introduce", "pdf_keywords": "dependency parsing widely;dependency parsing neural;projective dependency parsing;dependency parsing;universal dependency treebanks4;dependency treebanks4 hyperparameters;dependency parsers;dependency treebanks4;dependency parsing simple;based dependency parsers;treebanks 20 languages;new parsing model;parsing neural;parsing neural network;dependency parsers hand;parsing model multilingual;parser 29 treebanks;architecture dependency parsing;parsing widely;semantic dependency analysis;treebanks;treebanks conll shared;model treebanks;treebanks4 hyperparameters;parsing widely used;treebanks 20;propose new parsing;semantic dependency;treebanks conll;new parsing"}, "a4b1afd75bd2da0b21df58cd4ae1649fefabd8dd": {"ta_keywords": "utility functions game;players utility functions;agent utility functions;users fitness game;functions game model;fitness game learning;mobile fitness game;privacy users fitness;fitness game;agents utility maximizers;game learning utility;privacy receiving reward;method simulated game;players utility;game model agents;calories game theoretic;parameters players utility;functions game;computing parameters players;game theoretic framework;simulated game;model agents utility;correlated equilibrium strategy;game theoretic;game model;play correlated equilibrium;equilibrium strategy paper;simulated game chicken;utility maximizers;utility functions", "pdf_keywords": ""}, "cbf9a2560eac548e7b3d5eb7074c40b7bb861909": {"ta_keywords": "speaker diarization based;probabilistic model speaker;model speaker diarization;neural speaker diarization;speaker diarization effectiveness;speaker diarization;speaker diarization experimental;end neural speaker;effectively model speaker;diarization based probabilistically;neural speaker;overlapping speech;model speaker;case overlapping speech;diarization based;diarization effectiveness;diarization effectiveness proposed;conditional multitask learning;speaker;overlapping speech shown;diarization used predict;speech;end diarization;diarization;learning method end;speech shown;diarization used;multitask learning;speech shown outperforms;end diarization used", "pdf_keywords": "speaker diarization subtask;subtask speaker diarization;speaker diarization automatic;improving speaker diarization;model speaker diarization;subtask speaker;subtask learning acoustic;speaker diarization;diarization automatic speech;optimizes speaker diarization;automatic speech;speaker diarization process;framework subtask speaker;partitioning speech recording;diarization subtask learning;conditional multitask learning;partitioning speech;learning acoustic features;diarization speakers diarization;diarization subtask;speakers diarization;speech recording;recognition speech;diarization speakers;speaker diarization speakers;speech recording homogeneous;automatic speech recognition;effectively model speaker;process partitioning speech;speakers diarization important"}, "e9dfccd86b6116f7601d44590985de2df434a094": {"ta_keywords": "tutors interactively teach;tutor learning;tutors interactively;tutor learning important;tutor learning making;contribute tutor learning;act tutors interactively;teaching tutor learning;students tutor;tutor;interactively teach;help students tutor;learning teaching tutor;interactively teach sim;tutor solve problems;tutors;teaching tutor;simstudent learns skills;called simstudent learns;adaptive help students;simstudent learns;act tutors;students tutor solve;contribute tutor;learns skills;understand students learn;students act tutors;students learn;teaching sim student;tutor solve", "pdf_keywords": ""}, "c933fed82e7b5cbf7230f0f970b69590b40f86a1": {"ta_keywords": "decentralized stochastic optimization;decentralized sgdin methods;decentralized stochastic;cases decentralized stochastic;sgd federated averaging;decentralized sgdin;adaptive network topology;universal convergence rates;variety decentralized sgdin;averaging local sgd;sgdin methods far;unified convergence analysis;convergence rates smooth;federated averaging local;stochastic optimization methods;coorperative sgd federated;linear convergence rates;sgdin methods;local pairwise gossip;introduce unified convergence;federated averaging;convergence rates;stochastic optimization;unified convergence;convergence rates special;gossip updates adaptive;sgd federated;locality communication efficiency;cheap iteration;recovering linear convergence", "pdf_keywords": "decentralized gradient quantization;decentralized gradient based;decentralized gradient;existing decentralized gradient;decentralized consensus distance;present decentralized gradient;gossip algorithm;consensus distance based;decentralized parallel stochastic;distance based gossip;gossip algorithm experiments;rate gossip algorithm;consensus distance;gossip based random;stochastic gradient descent;decentralized consensus;gradient quantization algorithm;framework decentralized consensus;generalization gradient descent;parallel stochastic gradient;algorithm gradient descent;gradient quantization;decentralized parallel;gradient descent generalization;descent generalization gradient;propose decentralized parallel;based gossip based;gradient descent algorithm;gradient descent;algorithm gradient"}, "91d98b0a175237b48122e7560010e87a968fb6e0": {"ta_keywords": "mask prediction networks;neural networks separation;separation speech enhancement;separation recognition speech;mask prediction;techniques mask prediction;speech enhancement problems;speech separation;networks separation recognition;speech separation speech;recognition speech challenging;speech enhancement;deep learning;recently speech separation;deep neural networks;separation recognition;deep computational architectures;use deep neural;separation speech;deep neural;networks separation;recognition speech;speech challenging environments;deep learning based;deep computational;results deep learning;performance deep;prediction networks;performance deep computational;neural networks", "pdf_keywords": ""}, "cf8f2ca0c2d618104bc8724a6effc509088f16c4": {"ta_keywords": "current machine learning;machine learning better;beliefs current machine;type learning;propose machine learning;encompassing type learning;machine learning systems;complete statistics season;beliefs web;learning read web;machine learning paradigm;statistics season;machine learning;knowledge base;statistics season season;learning systems;beliefs web propose;language learner;acquired knowledge base;read beliefs web;learning paradigm machine;ontology synthesizing new;learning read;learner;season nell learning;statistics season september;endinging language learner;million confidenceweighted beliefs;learned reason beliefs;infer new beliefs", "pdf_keywords": ""}, "cc7858e74a79edceb5a42c30fc5c2dc5117f365b": {"ta_keywords": "learn model atari;adversarial tree search;generative adversarial tree;algorithm learns environment;search learned model;deep generative;adversarial tree;model atari environment;deep generative models;design deep generative;algorithm learns;learns environment;learns environment model;generative adversarial;atari environment;learned model;atari environment provide;generative models rl;tree search learned;learned model computationally;model atari;generative models;positive learned model;search learned;propose generative adversarial;learns;adversarial;generative;model propose generative;atari", "pdf_keywords": "learned model planning;bayesian deep qnetworks;learns game dynamics;learns model environment;deep qnetworks;architecture learns game;qnetworks architecture learns;learns model;learn model atari;learns game;deep qnetworks architecture;generative models rl;dynamic model deep;learned model;architecture learns;model deep;deep generative;bayesian deep;model atari environments;model deep rl;deep generative models;algorithm learns model;present bayesian deep;agent learn;learns;rl algorithm learns;generative adversarial tree;design deep generative;environments propose generative;atari environments"}, "82cb0c428f5edb1db6e733dc4b1b20023a2ce15f": {"ta_keywords": "election data testing;discusses voting rules;random survey takers;voting rules;election data;generate election data;voting rules support;voting rule paper;voting rules royal;paradox simple voting;voting rule;different voting rules;paper discusses voting;statistical society;elections publicly available;simple voting rule;derive million elections;elections;elections publicly;discusses voting;rules royal statistical;support statistical;behavior random survey;generate election;domain restriction statistical;survey takers;used generate election;restriction statistical;restriction statistical models;statistical society rso", "pdf_keywords": ""}, "f0bbc7b84c166e2258b6ba4f9d9835ecac04e842": {"ta_keywords": "spontaneous speech recognition;framework spontaneous speech;speech fluctuation model;spontaneous speech;given speech data;speech data;scale speech recognition;speech recognition;construction spontaneous speech;large scale speech;speech recognition accurate;bayesian framework spontaneous;speech recognition requires;selection given speech;speech fluctuation;various speech fluctuation;speech recognition task;acoustic model;speech fluctuation factors;accurate acoustic model;berkeley speech fluctuation;recognition accurate acoustic;variations speaker variances;acoustic model construction;appropriate acoustic model;scale speech;speaker variances;acoustic model structure;given speech;speaker variances dealt", "pdf_keywords": ""}, "19b6537012412bee0a36e3e271f84b95868fe859": {"ta_keywords": "ad hominem arguments;ad hominem argument;opponent ad hominem;hominem arguments;ad hominem;hominem arguments potential;hominem argument;explainable neural network;arguments punished arguers;explainable neural;using explainable neural;typology ad hominem;enforced fallacious arguments;fallacious arguments punished;arguments punished;arguers lapse attacking;arguments;fallacious arguments;arguments potential causes;punished arguers lapse;argument argument;punished arguers;argument argument committing;argument committing;attacking opponent ad;argument committing fallacy;neural;simple argument argument;simple argument;neural network architectures", "pdf_keywords": "debate ad hominem;ad hominem argumentation;hominem argumentation source;hominem argumentation;ad hominem arguments;ad hominem argument;argumentation discussions;online debate;hominem argument famous;hominem arguments known;online debate ad;argumentation source;increasing discourse complexity;debate theory;complexity ad hominem;argumentation source platform;use ad hominem;dynamics argumentation discussions;hominem arguments;investigate dynamics argumentation;ad hominem theories;ad homiminem dialogical;context debate theory;discourse complexity ad;discourse complexity;debate ad;debate theory little;ad hominem;increasing discourse;platform online debate"}, "36a5e0e0a8ce67e4cd9077d86e3b4d50fdcff15f": {"ta_keywords": "dual functional electrocatalysts;functional electrocatalysts;electrocatalysts;water splittings based;water splittings;route water splittings;splittings;splittings based;splittings based use;route water;new route water;use dual functional;dual functional;water;use dual;dual;based use dual;functional;propose new route;route;new route;based;letter propose;letter propose new;propose;propose new;letter;based use;new;use", "pdf_keywords": ""}, "d3e13d2514edaf74b863bfbe45a739c32a7689e1": {"ta_keywords": "code examples neural;neural code generation;syntax tree;syntax tree existing;analysis syntax tree;code generation;code generation tasks;examples neural code;neural code;sentence similarity scoring;code generation model;retrieve sentences similar;based sentence similarity;sentence similarity;programming based sentence;generate complex code;code examples;performance code generation;similar input sentences;sentences similar input;existing code examples;retrieve sentences;sentences using dynamic;sentences similar;reference existing code;complex code;syntax;subtree retrieval makes;similarity scoring;code", "pdf_keywords": "neural machine translation;language code generation;code natural language;code generation tasks;synthesis natural language;language neural generation;natural language code;machine translation learns;code generation;code generation subtask;machine translation deep;translation deep neural;code phrases corresponding;generation tasks neural;machine translation;descriptions code compositional;program synthesis natural;natural language neural;neural generation;machine translation method;converting natural language;language descriptions code;code compositional program;generating correct code;code phrases;code compositional;compositional program synthesis;generating sequence words;performance code generation;source code natural"}, "ba3322280992d0425bc9e2b4c59de24857e5f4e7": {"ta_keywords": "risk minimization performative;learning algorithms explicitly;loop behavior learning;learning algorithms;risk minimization;repeated risk minimization;classifier;identifying bank loan;data driven methods;minimizers performative risk;classifier underlying data;behavior learning algorithms;classifier underlying;data driven;behavior learning;minimization performative;bank loan;learning;minimization performative consider;identifying bank;driven methods;minimizers performative;convergence repeated risk;decisions learner;algorithms;loan;effect classifier underlying;decisions learner paper;problem identifying bank;minimizers", "pdf_keywords": "performative risk minimization;performative risk minimizers;risk minimization performative;minimization performative risk;repeated risk minimization;risk minimization;risk minimizers;risk minimization based;dependent risk minimization;risk minimization particular;risk minimizers contrast;local performative risk;risk minimizers paper;gradient descent;repeated gradient descent;minimization performative;performative risk;decision dependent risk;minimizers;gradient descent paper;convergence repeated risk;notion repeated gradient;minimization based notion;risk;minimization;minimization based;repeated risk;minimizers paper consider;decisions learner;dependent risk"}, "3c6670ecdfccd4633755c4b19d774453bfb77de3": {"ta_keywords": "dynamics semiflexible body;organs donated algorithms;age patients organs;fairness patients regions;semiflexible body;fairness patients;semiflexible body contact;patients organs;types fairness patients;harmonic oscillators;dynamics;age patients;dynamics semiflexible;oscillators;patients organs donated;organs;bath harmonic oscillators;fairness decisions;harmonic;model dynamics semiflexible;patients;bath harmonic;need ensure fairness;ensure fairness decisions;ensure fairness;different types fairness;fairness decisions consider;mechanism used emergency;simple model dynamics;account age patients", "pdf_keywords": ""}, "d79b613a67cf79740e1c08037f7d054585a12284": {"ta_keywords": "reality ar encoder;ar encoder;ar encoder 2016;ar decoder;ar decoding;conventional ar decoding;ar decoding methods;speech translation e2e;encoder;shallow ar decoder;nar decoder;ar decoder shared;orthros nar decoder;augmented reality ar;translation quality;enhance decoder;end speech translation;encoder 2016;translation quality comparable;speech translation;nar decoder auxiliary;improve translation quality;methods enhance decoder;decoder;decoder shared encoder;encoder 2016 season;encoders;encoders shown improve;reality ar;decoding methods fast", "pdf_keywords": "automatic speech translation;speech translation task;speech translation experimental;end speech translation;automatic machine translation;speech translation based;speech translation;machine translation mt;cascade machine translation;translation quality auxiliary;machine translation;asr machine translation;speech translation e2e;machine translation combining;speech translation st;context speech translation;translation combining automatic;translation quality;investigate translation quality;automatic speech;combining automatic speech;machine translation facto;translation experimental evaluations;translation task;speech recognition asr;encoder context speech;translation combining;benchmark corpora;approach automatic speech;end end speech"}, "fd9e38e240b4372c49b9205d6f909d070ff3804c": {"ta_keywords": "knowledge classification;sources knowledge classification;statistical similarity measures;classification text based;statistical similarity measure;statistical similarity;using statistical similarity;knowledge classification process;classification text;similarity measure relational;presents statistical similarity;similarity measures;similarity measure;databases manipulate textual;similarity;textual data;classification process intelligent;approach classification text;textual data using;text based;unlabeled background knowledge;inductive classification;classification;manipulate textual data;sources knowledge;explicitly inductive classification;world wide web;different sources knowledge;classification process;length word text", "pdf_keywords": ""}, "cd96cae0f8eabc7bb327c6f30151741bfdd62ee0": {"ta_keywords": "liquids glass;glass liquid transition;scale networks;glass forming liquids;development scalable networks;forming liquids glass;large scale networks;liquids glass glass;scale networks particular;scalable networks;glassy dynamics;dynamics glass;scalable networks used;glassy dynamics glass;glass liquid;dynamics glass forming;glass transition;networks;model glassy dynamics;glass glass transition;occurs glass liquid;glass transition order;networks particular;conference coordination;networks used generate;liquid transition;liquids;forming liquids;glass transition occurs;glass forming", "pdf_keywords": ""}, "dec6bb3c7bb671c86296a2a089e0e38aa3f69279": {"ta_keywords": "wmt14 benchmark neural;autoregressive machine translation;machine translation capable;benchmark neural models;benchmark neural;machine translation;wmt14 benchmark;neural models usually;performance model neural;performance wmt14 benchmark;data pretrained autoregressive;model neural;pretrained autoregressive model;neural networks nns;knowledge distillation crucial;neural models;nat machine translation;knowledge distillation;nnn model;pretrained autoregressive;model neural networks;models evaluation synthetic;training data pretrained;neural network training;translation capable;performance wmt14;machine translation nat;investigate knowledge distillation;training data;distillation crucial neural", "pdf_keywords": ""}, "6bfeb25ea4bb41ab0840bb1be09f9b2de7eea8e4": {"ta_keywords": "pathogenesis gp33;cytomegaloviruses encode cellular;signaling pathway viral;glycoprotein glycopolysin;effects glycoprotein glycopolysin;gp33 mediated signaling;suggest pathogenesis gp33;pathogenesis gp33 critical;glycopair signaling;glycoprotein glycopolysin glycopolar;glycopair signaling pathways;glycopole glycopair signaling;signaling activates cytokine;secretion infected cells;diseases cytomegaloviruses encode;pregnancy hypothesize gp33;cytomegaloviruses encode;glycoprotein;inflammation placentas;activates cytokine secretion;cytokine secretion infected;host immune functions;effects glycoprotein;immune functions;infected cells;signaling pathways transfection;diseases cytomegaloviruses;pathway viral;cytokine;cytomegaloviruses", "pdf_keywords": ""}, "609010cb866a19dd996281d00818c3fc7363ec94": {"ta_keywords": "unsupervised cross lingual;cross lingual nerner;named entityrecognition ner;entityrecognition ner tasks;language pair models;entityrecognition ner;lingual nerner model;recognition animal relationships;cross lingual;lingual nerner;relying bilingual dictionary;bilingual dictionary parallel;way relying bilingual;word level adversarial;annotated training data;manually annotated training;ner tasks languages;lingual;named entityrecognition;bilingual dictionary;language pair;languages need;relying bilingual;data available languages;annotated training;languages need manually;recognition animal;augmentation recently neural;feature augmentation recently;bilingual", "pdf_keywords": "lingual transfer learning;learning cross lingual;cross lingual models;data cross lingual;monolingual unannotated data;lingual named entity;lingual models;language learning cross;language monolingual unannotated;zeroresource cross lingual;monolingual unannotated;approach cross lingual;sharing crosslingual mapping;cross lingual ner;cross lingual neural;entity recognition challenging;cross lingual transfer;cross lingual;unannotated data cross;lingual models fundamental;cross lingual named;entity recognition;sharing crosslingual;lingual transfer;lingual ner providing;crosslingual mapping;knowledge sharing crosslingual;named entity recognition;embeddings languages;model target language"}, "a1c4ce9de92338646c6ee93c7c2e5ee366784b1a": {"ta_keywords": "neural semantic parsing;existing semantic parsing;semantic parsing datasets;semantic parsing;semantic parsing approaches;benchmark meaning representations;component semantic parsing;neural semantic;semantic parsing paper;different meaning representations;parsing approaches exhibit;parsing approaches;parsing datasets completing;meaning representations context;meaning representations existing;parsing;parsing datasets;meaning representations;integrating existing semantic;existing semantic;meaning representation researchers;semantic;component semantic;logical forms execution;important component semantic;datasets meaning representation;benchmark evaluation logical;missing logical forms;execution engines datasets;evaluation logical forms", "pdf_keywords": ""}, "facefd2fc4b718c6a0d8096b4eb02866028a04c2": {"ta_keywords": "answering span;answers span matching;answering span matching;answerable single span;handle span answers;span answers;span answers freeform;answers span;conversations span answers;question answering qa;question answering;span known answer;method answering span;powerful tool answering;paraphrased span known;answering qa conversational;span answers satisfactory;freeform answers span;paraphrased span;identify paraphrased span;tool answering;sentence experiments span;sentence answerable single;insights open retrieval;single span text;answering qa;collection extracts answers;open retrieval convqa;approach identify paraphrased;freeform answers generated", "pdf_keywords": "paraphrase generation model;diverse paraphrase generation;conversational question answering;paraphrase generation;question answering conversational;model generate paraphrase;generate paraphrase;weakly supervised conversations;question answering;approach identify paraphrased;paraphrased span known;supervised conversations;good answers retrieved;answering conversational;paraphrased span;supervised conversations freeform;identify paraphrased;answering conversational question;identify paraphrased span;answers retrieved documents;question answering propose;diverse paraphrase;identify good answers;use diverse paraphrase;answer retrieved passage;answers retrieved;conversations freeform answers;regarding question answering;generate paraphrase apkara;trained predict answer"}, "75d33c125eba966b50d4dccd359a2f6aa4e0e2e7": {"ta_keywords": "estimators contextual bandits;lipschitz risk estimates;risk estimates converge;bounds lipschitz risk;risk estimates;contextual bandits;estimates target policy;estimating probability;contextual bandits ii;policy safety margin;finite sample guarantees;lipschitz risk;method estimating probability;policy risk;policy risk assessment;cdf estimators contextual;sample guarantees;propose policy risk;estimators contextual;bandits;evaluate prospective policies;estimating probability given;sample guarantees hold;inequalities cdf estimators;estimates target;prospective policies using;prospective policies;bandits ii error;estimating;framework estimates", "pdf_keywords": "risk functionals based;lipschitz risk functionals;risk functionals;risk estimation;risk functional;propose risk functional;consider risk functionals;distributionally robust optimization;conditional importance weight;estimation based importance;risk functionals previous;situation risk functionals;robust estimator mdp;risk estimation based;risk functionals satisfy;robust estimator safety;approach risk estimation;risk functional framework;risk estimation paper;estimator safety critical;risk assessment target;doubly robust estimator;conditional importance;distributionally robust;set lipschitz risk;importance sampling estimator;robust estimator;conditional value risk;estimator safety;performance distributionally robust"}, "cb0de2de79533d4faada3d745f43702eb89d1a60": {"ta_keywords": "nlp tools designed;nlp tools;documentation templates;practices field nlp;processing nlp tools;documentation practices;datasets nlp gem;detailed descriptions nlp;reusable documentation templates;descriptions nlp;develop reusable documentation;documentation practices field;descriptions nlp datasets;reusable documentation;standard documentation practices;natural languages paper;natural language;documentation;nlp gem;documentation templates huggingface;nlp gem benchmark;documentation present;standard documentation;card datasets nlp;processing nlp;nlp datasets;datasets nlp;analysis natural languages;natural language processing;nlp datasets models", "pdf_keywords": "documentation language datasets;practical documentation language;documentation language;resource developed documentation;creating documentation;guides natural language;documentation templates guides;documentation templates;developed documentation provides;creating documentation templates;developed documentation;documentation provides;information proposed nlp;documentation propose;nlp datasets present;documentation;encourage creation documentation;nlp datasets;documentation propose data;documentation provides users;proposed nlp datasets;creation documentation;natural language;studies creating documentation;creation documentation propose;proposed nlp;language datasets ml;accountable practical documentation;documentation local context;practical documentation"}, "13b6c8cce3b4557ad7a3188f2d54636e755e8145": {"ta_keywords": "multichannel gaussian mixture;separation multichannel mixtures;multichannel mixtures;source separation multichannel;multichannel source separation;multichannel mixtures simultaneous;separation propose multichannel;separation multichannel;propose multichannel gaussian;multichannel gaussian;speakers deep mcgmm;gaussian mixture model;multicellular;mixtures simultaneous speakers;meanfield models;multicellular systems;paper meanfield models;deep mcgmm;multichannel source;source separation;multicellular systems experiments;gaussian mixture;mixture model;meanfield models used;simultaneous speakers deep;wide range multicellular;problem multichannel source;source separation propose;multichannel;propose multichannel", "pdf_keywords": ""}, "77c63e8f102465e3fc4a46e0b07c32fa8d2f8a54": {"ta_keywords": "unsupervised ccg parsers;ccg parsers;ccg parsers evaluating;grammar induction algorithms;detecting syntactic structure;detecting syntactic;grammar induction;grammar induction paper;current grammar induction;parsers;method detecting syntactic;progress grammar induction;parsers evaluating labeled;labeled dependencies ccgbank;syntactic structure raw;dependencies ccgbank hinting;parsers evaluating;syntactic structure;errors unsupervised ccg;unsupervised ccg;necessary progress grammar;grammar;word tag sequences;current grammar;sequences current grammar;dependencies ccgbank;syntactic;evaluating labeled dependencies;progress grammar;tag sequences", "pdf_keywords": ""}, "c065f9997794b13565dd49a6e475fc5e8c9d54ce": {"ta_keywords": "compliant joints spring;tensile stiffness lamina;joints spring constant;stiffness lamina emergent;lamina emergent torsional;joints spring;stiffness lamina;improve tensile stiffness;compliant joints;tensile stiffness;performances compliant joints;torsional let joint;calculate equivalent spring;kinetostatic model joint;emergent torsional;results validated tensile;method improve tensile;utilizing double laminated;emergent torsional let;laminated material structure;material structure flexibility;improve tensile;validated tensile;model spring;double laminated material;stiffness;structure flexibility accuracy;spring model spring;tensile;equivalent spring", "pdf_keywords": ""}, "112eb8a8273ab725d47789efb87237edbc4f02db": {"ta_keywords": "logic learnable;logic learnable valiant;order logic learnable;description logics;logic tractably learned;description logic;simple description logic;learned syntactic restrictions;tractably learned syntactic;description logics subsets;learnability subsets;learned syntactic;description logic summarize;logic tractably;learnability subsets order;learning concepts conjunctive;considers learnability subsets;considers learnability;learnability;learnable;shown logic tractably;predicate calculus;predicate calculus expressed;syntactic restrictions explored;restricted shown logic;analyze learnability;syntactic restrictions;spaces description logics;logics subsets predicate;tractable learning exist", "pdf_keywords": ""}, "e6accbbb366387faf817126dc7b0260c450bd2e6": {"ta_keywords": "graph coding theory;sparse graph coding;coding theory;coding theory paper;graph coding;modern sparse graph;decoding algorithm;sparse graph;decoding algorithm integer;decoding;group testing algorithms;complexity;new decoding algorithm;algorithm integer multiplexing;modern sparse;new decoding;semiflexible chains model;integer multiplexing;sparse;semiflexible chains;integer multiplexing problem;present new decoding;algorithms approximate recovery;complexity leveraging;tools modern sparse;semif semiflexible chains;approximate recovery order;optimal sample complexity;sample complexity leveraging;complexity leveraging design", "pdf_keywords": ""}, "ca7a67aa29c67b006017f651601091145644f243": {"ta_keywords": "localization speech detection;speech localization;speaker localization speech;speaker localization;speech localization speech;integrates speech localization;concept speech localization;localization speech;speaker localization average;describes speaker localization;712 speaker localization;speech detection;localize speakers;statistical speech detection;speech localization addition;localize speakers reverberation;speech detection methods;speech detection real;hard localize speakers;speech detection techniques;speech detection using;xmath743 speech detection;speech recognition;approach speech recognition;statistical speech;localization average fft;accuracy 712 speaker;use statistical speech;speech recognition based;method integrates speech", "pdf_keywords": ""}, "a61aebbfe029c4b8eafae4042e6242cdca8f54b7": {"ta_keywords": "latent relations entities;relations generate relational;relations entities understood;relations entities;relations generate;relational web;domain robotic;robotic;latent relations;generalization algorithm answering;open domain robotic;relations involving entities;robotic model;generate relational;model learn relationships;problem latent relations;terms latent relations;robotic robotic model;entities understood;relational;answering questions long;robotic robotic;latent relations involving;generate relational web;domain robotic robotic;tail relations generate;robotic model paper;entities;answering questions;relations", "pdf_keywords": "question answering knowledgeledge;question answering;question answering trained;domain question answering;prediction question answering;interactive question answering;answering trained database;answering knowledgeledge guided;question answering unsupervised;hop question answering;question answering propose;model answer prediction;answering knowledgeledge;answer answering;answering trained;answer prediction;unsupervised answer answering;answer answering challenging;answer query paper;paper answer answering;answer prediction question;answering unsupervised;answering challenging;answer query;task answers factoid;answering challenging task;answers factoid;answers factoid questions;answering unsupervised answer;answer query expressed"}, "2d1f442578feb7034aa2b68bbf95f608f2342256": {"ta_keywords": "group fairness;contextual multiarmed bandit;group fairness contextual;multiarmed bandit;fairness contextual multiarmed;multiarmed bandit setting;fairness contextual;fairness arbitrary;bandit setting;formulation group fairness;bandit;bounds regret algorithm;provide bounds regret;fairness arbitrary number;notions fairness arbitrary;notions fairness;accommodate notions fairness;fairness;bounds regret;regret algorithm provide;tug war game;regret algorithm;winner tug war;picking arms group;fairly protected groups;algorithm picking arms;competition;results competition;tug war;war game winner", "pdf_keywords": "reward distributions crowdwork;algorithm bandit;bandit algorithm;group fairness based;group fairness;bandit algorithm bandit;new bandit algorithm;fairness arm reward;bandit paper study;algorithm bandit paper;fairness based notion;fairness based;group exploration;bandit;framework group fairness;distributions crowdwork;reward distributions;notion fairness;bandit paper;arm reward distributions;fairness definition;distributions crowdwork setting;group exploration introduce;crowdwork;crowdwork setting;fairness definition takes;present new bandit;formalize notion fairness;reward;introduce new fairness"}, "147ba336fcba32fadca470e14a858ce069375475": {"ta_keywords": "based speech synthesis;generation automatic speech;speech synthesis;automatic speech;context based speech;automatic speech recognition;speech recognition systems;speech recognition;based speech;generation automatic;systems based context;context based;based context based;synthesis;speech;approach generation automatic;recognition systems based;recognition systems;systems based;based context;automatic;new approach generation;context;approach generation;generation;systems;recognition;new approach;propose new approach;based", "pdf_keywords": ""}, "00c8d88abef116d8d3d673a28ff4098115cf8da3": {"ta_keywords": "cooperative persuasive dialogue;created dialogue management;dialogue management;persuasive dialogue able;persuasive dialogue evaluated;dialogue management module;organized persuasive dialogue;persuasive dialogue presented;persuasive dialogue fully;text based cooperative;dialogue manager;persuasive dialogue;effective cooperative persuasive;dialogue able persuade;based cooperative persuasive;dialogue manager controlled;natural language generation;module cooperative persuasive;dialogue evaluated;nlg natural language;cooperative persuasive;dialogue presented;dialogue fully automatic;dialogue evaluated wizard;dialogue;created dialogue;dialogue able;generation nlg natural;automated text based;language generation nlg", "pdf_keywords": ""}, "04a7d9f0388ded93c1ec16e36a6df3cd44cb95b0": {"ta_keywords": "multilingual entity linking;entity linking language;multilingual entity;lingual linking task;lingual linking;cross lingual linking;single entity retrieval;language specific mentions;entity retrieval;mining auxiliary entity;linking language;entity retrieval model;entity linking;formulation multilingual entity;linking language specific;new multilingual dataset;evaluation rare entities;multilingual dataset;low resource languages;auxiliary entity pairing;entities;multilingual dataset mewsli;limited cross lingual;new multilingual;multilingual;rare entities;provide new multilingual;representation negative mining;million entities model;million entities", "pdf_keywords": "multilingual entity linking;crosslingual entity linking;entity linking task;lingual entity representation;cross lingual entity;multilingual entity;entity linking;crosslingual word tagging;entity representation learning;approach multilingual entity;crosslingual entity;entities crosslingual;information entity linking;entity linking based;entities crosslingual entity;lingual entity;single entity retrieval;word tagging linking;tagging linking fundamental;million entities crosslingual;multilingual dual encoder;encoder cross lingual;entity retrieval;tagging linking;entities mentions;cross lingual task;mining auxiliary entity;languages need linked;task cross lingual;need linked entity"}, "9768d7ba9d09ac3bf3d52ec674bde1a6e615daad": {"ta_keywords": "prediction pairwise comparisons;pairwise comparisons flexible;oracle estimator adaptivity;study stochastic transitivity;comparisons flexible;strong stochastic transitivity;stochastic transitivity;stochastic transitivity sst;pairwise comparisons;measuring worst case;estimator adaptivity index;adaptivity index estimator;smaller adaptivity index;bounds adaptivity index;comparisons flexible class;substantially smaller adaptivity;measuring worst;prediction pairwise;estimating parameter flexible;measure worst case;aggregating pairwise comparison;relative oracle estimator;adaptivity index measure;estimator adaptivity;result complexity theoretic;hardness conjecture computationally;clique hardness conjecture;computationally efficient estimator;pairwise comparison;estimator achieve substantially", "pdf_keywords": "estimating sst matrices;communities general stochastic;random graph estimator;clique conjecture;graph estimator;known optimally adaptive;planted clique conjecture;clique conjecture asserts;optimally adaptive cases;graph estimator good;problem pairwise comparisons;pairwise comparisons;stochastic block model;known optimally;adaptive cases asymptotically;optimally adaptive;general stochastic block;optimal problem estimating;pairwise comparisons particular;cases asymptotically optimal;sst matrices problems;index di\ufb00erent algorithms;sst matrices;factors planted clique;stochastic block;estimating sst;good optimal adaptivity;recovering communities;planted cliques;xmath0 recovering communities"}, "b7ffc8f44f7dafd7f51e4e7500842ec406b8e239": {"ta_keywords": "gating reading comprehension;reading comprehension tasks;grained gating reading;gating reading;tag prediction;like reading comprehension;tag prediction social;comprehension tasks achieving;tasks like reading;reading comprehension propose;performance reading comprehension;comprehension tasks;reading comprehension;prediction social media;concatenation scalar weighting;representation word level;character level representations;word character level;like reading;fine grained gating;word level;word level character;reading;improve performance reading;grained gating;dynamically combine word;mechanism tag prediction;representations using concatenation;novel gating;comprehension propose novel", "pdf_keywords": "reading comprehension tasks;reading comprehension experiments;neural networks tokens;improvements reading comprehension;comprehension tasks achieving;comprehension tasks;paragraphs reading comprehension;twitter tag prediction;word character gating;token model improves;performance reading comprehension;questions paragraphs reading;performance natural language;natural language processing;reading comprehension;comprehension tasks word;words called tokens;representations learned lookup;level representations words;character level representations;word character level;document query gating;paragraphs reading;gating document query;character gating document;comprehension experiments;improvements reading;datasets word character;representations words;language processing tasks"}, "90357a6dc817e2f7cec477a51156675fbf545cf1": {"ta_keywords": "multimodal reasoning time;multimodal reasoning;video based reasoning;learns multimodal script;use multimodal reasoning;learns multimodal;videos transcribed speech;model learns multimodal;multimodal script knowledge;videos transcribed;script knowledge watching;multimodal;use auxiliary supervised;auxiliary supervised;learns match images;auxiliary supervised data;youtube videos transcribed;self supervised;use multimodal;supervised;multimodal script;knowledge watching;contextualize;knowledge watching millions;free self supervised;self supervised manner;watching millions youtube;supervised manner;humans use multimodal;videos", "pdf_keywords": "predicting happens videos;videos model learn;scenes trained corpus;attention masking;introduce attention masking;attention weights language;scenes trained;actions scenes trained;train videos;attention masking use;videos covering aspects;train videos images;curated instructional video;videos covering;masking use attention;instructional video corpora;attention weights;better train videos;video corpora;attention;look videos;trained corpus;videos;masked language modeling;diverse set videos;million videos;happens videos model;videos model;use attention;like look videos"}, "ba3f39606cfd4150ea80fec1b2e1137933c6d143": {"ta_keywords": "normalization wizard pcr;robotic setup testing;procedures normalization wizard;normalization wizard;wizard pcr setup;testing procedures normalization;procedures normalization;wizard pcr;robotic setup;pcr setup;present robotic setup;normalization;pcr;robotic;present robotic;paper present robotic;setup testing;testing procedures;setup testing testing;testing testing procedures;procedures;testing;testing testing;wizard;setup;paper;paper present;present", "pdf_keywords": ""}, "8ae4a584539a8f30d654e2678dde64a8334461b7": {"ta_keywords": "product reviews generative;personalized product reviews;reviews generative;reviews generative model;classify reviews;generates personalized product;remarkable accuracy recommender;classify reviews identifying;generates personalized;model classify reviews;sentiment rating remarkable;accuracy recommender;user rate product;sentiment rating;product category sentiment;predict style opinions;author review product;accuracy recommender basic;category sentiment rating;product reviews;personalized product;recommender basic;review product;recommender basic task;category sentiment;user write product;recommender;rating remarkable accuracy;rating remarkable;review product category", "pdf_keywords": "generate plausible reviews;reviews generated;predicting author review;dif\ufb01cult reviews generated;sentiment item learning;reviews generated speci\ufb01cally;review paper generative;sentiment star rating;recommender learns;novel recommender learns;online review community;review product category;networks train generative;author review product;sentiment star;review product;large online review;recommender learns automatically;product category sentiment;capable predicting author;generative models trained;author review;re\ufb02ect sentiment star;reviews;dif\ufb01cult reviews;review community;reviews match style;category sentiment;sentiment item;model generating text"}, "b01ecfd2322437fcc9c7ce6605d6f5a50f67ec50": {"ta_keywords": "training active learning;outperform training active;active learning;automatic learning inal;active learning al;training active;iteratively selects training;learning automatic;learning automatic learning;automatic learning;learning inal iteratively;outperform training;training examples annotation;consistently outperform training;training strategy;training strategy maximizing;selects training examples;actively acquired dataset;training examples;selects training;training dataset;algorithm learning automatic;training dataset model;maximizing predictive performance;learning inal;used training strategy;predictive performance;training;active sampling;training successor model", "pdf_keywords": "active learners text;deep bayesian active;active learning al;bayesian active learning;tasks active learning;active learning;use active learning;active learners;active learning technique;experiments active learners;popular active learning;active learning practice;models deep;linear models deep;model learns;models deep networks;bayesian active;deep bayesian;present deep bayesian;network deep;learners text;al model learns;learns integrate deep;learns;sequence tagging tasks;learning al;deep networks;deep learning;learners text classi\ufb01cation;learning model"}, "2177bf060aaf2c0c2b551d3e805779cb35c19bb1": {"ta_keywords": "phosphor ceramic k2;phosphor ceramic;catalyst explosive water;ceramic k2 si;explosive water vapor;ceramic k2;mno plant plant;f6 mno plant;mno plant;video phosphor ceramic;explosive water;water vapor deposition;catalyst explosive;used catalyst explosive;si f6 mno;k2 si f6;k2 si;phosphor;vapor deposition;plant used catalyst;f6 mno;water vapor;mno;ceramic;video phosphor;dynamics video phosphor;explosive;k2;vapor;used catalyst", "pdf_keywords": ""}, "2068825cabd94c951a0282ed731a8b8f2da1721c": {"ta_keywords": "supervised semantic parsing;semantic parsing model;semantic parsing task;semantic parsing;systems semantic parsing;parsing model learns;semantic parsing process;utterances semantic parsing;parsing model;semi supervised semantic;experiments semantic parsing;semantic parsing extra;supervised semantic;parsing;nl utterances semantic;parsing task transducing;structures experiments semantic;parsing task;parsing process;formal meaning representations;transducing natural language;utterances semantic;introduce semi supervised;natural language;natural language utterances;complex systems semantic;parsing extra unlabeled;unlabeled nl utterances;semi supervised;nl utterances", "pdf_keywords": "semisupervised semantic parsing;neural semantic parsing;semantic parsing promising;learning semantic parsing;supervised semantic parser;semantic parsing task;semantic parsing relies;parsing semantic parsing;parsing semantic;semantic parsing machine;semantic parsing;semantic parser parsing;semantic parsing research;semantic parsing semantic;semantic parser;semantic parsing aims;semantic parsing standard;semi supervised semantic;parsing machine learning;utterances semisupervised semantic;nl utterances semisupervised;approach semantic parsing;parsing promising approach;parsing promising;semisupervised semantic;syntax trees inference;deep learning semantic;transition based parser;research semantic parsing;parser parsing"}, "8ca58f3f6e59a6d243f3da6c196e9f730e6e9993": {"ta_keywords": "speaker diarization eend;neural speaker diarization;speaker diarization;end neural speaker;finding speaker;finding speaker noisy;overlapping speech flexible;neural speaker;overlapping speech;handle overlapping speech;problem finding speaker;diarization eend;diarization handle overlapping;speakers experiments callhome;fully supervised;variable numbers speakers;supervised;state art clustering;numbers speakers experiments;diarization eend model;numbers speakers;speaker;speakers experiments;diarization;speaker noisy;end diarization;based fully supervised;performance liverpool lhc;fully supervised approach;flexible numbers speakers", "pdf_keywords": "chunks speaker diarization;speaker based diarization;speaker diarization promising;speaker diarization;online diarization;online diarization propose;eend online diarization;handling overlapping speech;propose online diarization;end neural diarization;online diarization method;diarization operate online;overlapping speech;deal overlapping speech;overlapping speech experiments;neural diarization model;speakers training of\ufb02ine;overlapping speech \ufb02exible;neural diarization;speaker tracing;diarization promising approach;diarization promising;chunks speaker;number speakers chunks;using speaker tracing;numbers speakers training;diarization eend;speaker tracing buffer;speaker based;based diarization"}, "ca6d5c7829a76d10069fa3aa6776c35cc044b7ba": {"ta_keywords": "selection tool undergraduates;courses selection advising;course selection tool;berkeley course selection;course selection;undergraduate students;selection advising;course planning;tool undergraduates;undergraduate students university;advising career paths;courses selection;basic course planning;students human advisors;undergraduates;checking basic course;dynamics undergraduate students;selection advising career;students university california;berkeley course;concerning courses selection;california berkeley course;preferences concerning courses;undergraduate;college students;advising;students;advising career;courses;tool undergraduates large", "pdf_keywords": ""}, "6e3f8187f8fef3e11578a73f32da07d33dbf8235": {"ta_keywords": "domain semantic parsing;semantic parsing spoken;tree ontology annotation;semantic parsing;extracting semantic;exploiting semantic;dialogue systems;ontology annotation;dialogue systems utilizing;data text annotations;parsing spoken dialogue;domain semantic;extracting semantic triples;open domain semantic;procedure extracting semantic;parsing spoken;exploiting semantic dependencies;text annotations;spoken dialogue systems;source structured data;semantic triples tables;structures exploiting semantic;annotations;including tree ontology;annotation;tree ontology;semantic;ontology annotation question;domain structured program;semantic dependencies table", "pdf_keywords": "generation structured data;domain corpus structured;data compositional semantic;structured dataset natural;tree structured semantic;structured data compositional;structured semantic;dataset natural language;corpus structured data;domain structured dataset;data text generation;compositional semantic parsing;parsing semi structured;semantic parsing semi;semantic parsing;natural language generation;open domain corpus;ontology annotation tables;structured data introduce;structured dataset;domain corpus;structured semantic frame;structured data;corpus structured;ontology tree structured;structures natural language;tree ontology annotation;language generation structured;semantic structures natural;data text models"}, "bdbf635476477eec5be5a292b494e20b8902cc35": {"ta_keywords": "dichalcogenides tmds accuracy;metal dichalcogenides tmds;game resilient noise;electronic structure xmath0;transition metal dichalcogenides;xmath1 transition metal;metal dichalcogenides;resilient noise partially;dichalcogenides tmds;resilient noise;multilevel systems emulating;noise partially mitigating;enhance robustness multilevel;robustness multilevel systems;xmath0 compared;systems emulating;noise clean data;robustness multilevel;xmath0 xmath1 transition;xmath1 transition;improved factor xmath0;structure xmath0 xmath1;xmath0;structure xmath0;noise partially;tmds accuracy improved;tmds accuracy;enhance robustness;xmath1;xmath0 xmath1", "pdf_keywords": "machine translation noisy;translation noisy text;spoken text crowdsourced;translation noisy;automatic translation spoken;noisy social media;text crowdsourced social;automatic translation;text crowdsourced;noisy text mtnt;crowdsourced social media;machine translation method;method automatic translation;machine translation;noisy text;translation spoken text;based machine translation;human generated text;translation method automatic;social media text;crowdsourced social;synthesizing natural noise;text labeled translation;synthetic noise induction;crowdsourced;source natural noise;spoken text;introduce synthetic noise;noise induction;labeled translation"}, "b3979990dc2080138021cb3d767c7ec6d3e96194": {"ta_keywords": "learns descriptors events;relationship descriptors trajectory;interesting correlations annotations;learns descriptors;descriptors relationship dataset;model learns descriptors;relationship descriptors;accurate relationship trajectories;global relationship descriptors;correlations annotations existing;novel unsupervised;correlations annotations;descriptors events marriage;relationship trajectories;relationship trajectories model;trajectory descriptors relationship;fictional relationship characters;unsupervised learning;topic model;trajectories model learns;present novel unsupervised;novel unsupervised neural;relationship dataset paper;descriptors relationship;unsupervised neural;relationship fictional relationship;tasks interesting correlations;outperforms topic model;fictional relationship;learning generate interpretable", "pdf_keywords": ""}, "e12c52fb542f76b3f0d29178842428d6a4edfe1e": {"ta_keywords": "biased machine learning;systems accurate biased;biased machine;accurate biased machine;biased users deferring;rejection learning;decision makers experiments;inconsistent biased users;biased users;rejection learning propose;improve accuracy fairness;external decision makers;accuracy fairness;learning algorithm accounts;explored rejection learning;interacting brownian particles;accuracy fairness entire;learning defer;interacting brownian;external decision;dynamics interacting brownian;pass decision downstream;machine learning;working inconsistent biased;biases;decision downstream;brownian particles;competing forces model;deferring models greatly;inconsistent biased", "pdf_keywords": "fairness machine learning;learning model fair;fairness machine;approach fairness machine;fair sequential decision;generalization rejection learning;generalizes rejection learning;provided rejection learning;model fair sequential;new approach fairness;learning defer generalization;rejection learning propose;learning defer generalizes;learning models fairly;approach fairness;rejection learning machine;rejection learning;improvement rejection learning;propose learning defer;fair sequential;learning defer;defer generalization rejection;rejection learning argue;rejection learning considering;sequential decision;learning models;sequential decision making;model fair;training models;supervised"}, "4ce47dd7a8674f8ffd53f1883bc57e62460a83f0": {"ta_keywords": "cyber infrastructure;cyber physical systems;nash equilibriumria nonconvex;differential nash equilibriumria;interactions consumer power;characterization nash equilibria;nash equilibriumria;power company incentive;insurance inefficiencies arise;policy regulations energy;incentive invest security;asymmetries design insurance;consumer power;design insurance contracts;nash equilibria;power company context;regulations energy ecosystem;infrastructure;insurance inefficiencies;nash equilibria termed;cyber physical;context cyber infrastructure;scale cyber physical;consumer power company;strategic interactions;equilibriumria nonconvex strategy;nonconvex strategy spaces;differential nash;insurance contracts;purchase insurance inefficiencies", "pdf_keywords": ""}, "fa9b043ae8da3cc60c975762ae9066d2fb010f41": {"ta_keywords": "unsupervised nlp tasks;unsupervised nlp;processing graph clustering;language processing graph;methodology unsupervised nlp;graph clustering;tasks graph clustering;efficient graph clustering;graph clustering powerful;graph clustering makes;graph clustering algorithms;natural language processing;nlp tasks discussed;graph based representations;nlp tasks;analyzing natural language;processing graph;tasks discussed graph;discussed graph based;graph based;language processing;processing tasks graph;nlp;clustering powerful tool;clustering;clustering powerful;natural language;variety natural language;language processing tasks;efficient graph", "pdf_keywords": ""}, "52540497682c4209b8e20125c8255358b22d0fa7": {"ta_keywords": "knowledge novice algebra;simulate human learning;cognitive tutors;human learning math;use cognitive tutors;novice algebra students;algorithm simulated student;simulated student;intelligent agents simulate;acquire skill knowledge;learns representation knowledge;develop intelligent agents;tutors;skill knowledge harder;knowledge novice;learns;learned problem representations;algebra students;student extended agent;skill knowledge;novice algebra;learning math;algorithm learns representation;problems using learned;algorithm learns;using learned;efficient skill acquisition;algebra students lot;using learned problem;assist future learning", "pdf_keywords": ""}, "2ed6f376e9e7eee6d833ad7b6aba63d7ad40c0f8": {"ta_keywords": "colloidal air blanketed;filling colloidal air;air filling colloidal;air blanketed spring;colloidal air;versatile air filling;blanketed spring;filling colloidal;highly versatile air;air filling;versatile air;air blanketed;colloidal;fluid dynamics video;spring;fluid dynamics;air;dynamics video demonstrate;filling;demonstrate design simple;design simple;blanketed;dynamics video;video demonstrate design;fluid;simple highly versatile;demonstrate design;dynamics;video demonstrate;design simple highly", "pdf_keywords": ""}, "bd0db679d595399b91c5acca1db33a2803697d53": {"ta_keywords": "cues online political;online content voters;political information internet;online political information;content voters encountering;social cues online;encountering political information;political information search;online political;political information;content voters;voters reach judgments;effect social cues;voters encountering political;social cues;social cues function;judgments similar informed;voters encountering;online information colored;news online information;interact online content;encouraged interact online;information search evaluation;displayed comments readily;users encouraged interact;heuristic allowing voters;information internet typically;voters reach;encountering political;online content", "pdf_keywords": ""}, "79f47ebf896b848e7c981c8aa6862ca1a7e5e7e5": {"ta_keywords": "bias kernel clustering;relevant kernel clustering;adaptive geodesic kernels;kernel clustering limitations;kernel clustering reduces;kernel clustering;geodesic kernels provide;kernel clustering algorithm;understanding kernel clustering;geodesic kernels;locally adaptive kernels;analysis kernel methods;adaptive kernels;kernels prove bias;adaptive kernels directly;kernel methods;kernel methods particularly;bias kernel;bias clustering widely;locally adaptive geodesic;data analysis kernel;bias clustering;breiman bias clustering;theoretical understanding kernel;density equalization;density equalization implicitly;clustering reduces continuous;clustering limitations principled;kernels provide conditions;clustering widely", "pdf_keywords": ""}, "0222a48657d554b2a5a3d7ec3bb0b6833b8970a1": {"ta_keywords": "testing thrombocytopenia hit;heparin induced thrombocytopenia;testing thrombocytopenia;laboratory testing thrombocytopenia;thrombocytopenia hit;induced thrombocytopenia hit;thrombocytopenia hit cause;respectively thrombocytopenia;thrombocytopenia hit severe;arterial thromybosis thrombocytopenia;thromybosis thrombocytopenia;respectively thrombocytopenia difficult;thrombocytopenia;thrombocytopenia timing platelet;induced thrombocytopenia;thrombocytopenia difficult diagnose;thromybosis thrombocytopenia timing;thrombocytopenia timing;thrombosis human body;thrombocytopenia difficult;01 respectively thrombocytopenia;hit cause thrombosis;biotin antigens gold;thrombosis human;thrombosis;thromybosis;thrombosis sequelae;cause thrombosis human;thrombosis sequelae cause;arterial thromybosis", "pdf_keywords": ""}, "8c5ba1c914eab16b705da03352fe69d5bcfc72ea": {"ta_keywords": "summarization framework trained;summarization models augmenting;decoder summarization models;summarization models abstract;encoder decoder summarization;summarization models;explicit structures summarization;document summarization models;decoder summarization;generates abstractive summaries;abstractive summaries generating;structures summarization;abstractive text summarization;summarization aims compressing;structures summarization framework;single document summarization;document summarization;text summarization;summarization framework;summaries generating;abstractive summaries;text summarization aims;summarization;summaries generating novel;summarization aims;sentences source document;document representations;summaries;aware document representations;sentence level structures", "pdf_keywords": ""}, "7fc0097f6a51282dc1e9020d7c28e12cecaef519": {"ta_keywords": "xmath1 phase transition;xmath1 phase;xmath0 xmath1 phase;theoretical study xmath0;study xmath0 xmath1;xmath0 xmath1;xmath1;phase transition;study xmath0;xmath0;phase;transition;present results theoretical;results theoretical;results theoretical study;theoretical study;theoretical;paper present results;present results;paper present;present;paper;results;study", "pdf_keywords": ""}, "3cfb319689f06bf04c2e28399361f414ca32c4b3": {"ta_keywords": "answering text classification;learning techniques nlp;nlp introducing unified;question answering text;techniques nlp;nearest neighbor nl;benchmarks covering summarization;techniques nlp introducing;clean crawled corpus;question answering;tasks transfer learning;crawled corpus achieve;crawled corpus;natural language processing;text classification;transfer learning;nlp introducing;language understanding tasks;transfer learning techniques;summarization question answering;algorithm transferring text;answering text;data rich task;nlp;transferring text;natural language;corpus;corpus achieve;transferring text person;covering summarization", "pdf_keywords": "transfer learning nlp;tasks pre trained;transfer learning;approach transfer learning;transfer learning allows;language pre training;limits transfer learning;pre trained models;task training;task training pre;multi task training;learning nlp pushed;model pre trained;trained unsupervised task;training models;language model pre;models trained;trained models;learning nlp;trained predict entire;study training models;individual supervised tasks;learning;training models 11;supervised tasks;pre trained unsupervised;trained models trained;trained predict;models trained predict;training small model"}, "d19e097f388ca12ff111989b2bac7d3cc3cf15ca": {"ta_keywords": "text graph topology;uncover coordinated messaging;text graph;coordinated messaging analysis;messaging analysis;text text graph;clusters posting similar;user clusters posting;detect communities coordinated;riots detect communities;similar textual content;messaging analysis user;clusters posting;coordinated messaging;textual content;user parleys parler;disinformation narratives;similar textual;disinformation narratives related;communities coordinated;textual;different disinformation narratives;posting similar textual;capitol riots detect;detect communities;textual content support;communities coordinated user;capitol riots;coordinated user clusters;riots detect", "pdf_keywords": "study twitter parler;user graph textual;spreading misinformation analyze;textual similarity social;elections twitter conversation;similar textual messages;graph textual similarity;social media content;twitter parler;comparative study twitter;misinformation analyze textual;users textual content;twitter conversation;user graphs;spreading misinformation;user user graphs;elections twitter;graphs based users;user graphs based;study twitter;presidential elections twitter;textual messages;inducing user user;graph textual;analyze textual content;user graph;textual content analyze;riots supporters president;users textual;user user graph"}, "2c9158e20f58df04a6c5cd54dd3ee7d8df656421": {"ta_keywords": "ranking sparse representations;sparse representations retrieval;nearest neighbor similarity;retrieval toolkit movement;ranking sparse;distance similarity functions;neural ranking signals;similarity functions nearest;representations retrieval;functions nearest neighbor;sparse representations;sparse representations training;neural ranking;dense sparse representations;toolkit movement animals;classic neural ranking;nearest neighbor nn;distance similarity;representations retrieval systems;nearest neighbor;concept nearest neighbor;similarity functions;neighbor similarity;distance based structure;based concept nearest;problem ranking sparse;ranking signals;motion rigid body;dynamics video;flexneuart efficiently retrieve", "pdf_keywords": "ef\ufb01cient retrieval sparse;query document embeddings;retrieval sparse;queries document vectors;retrieval similarity;retrieval sparse sparse;information retrieval similarity;retrieval similarity query;retrieval based deep;document embeddings;document embeddings computed;dense document representations;ef\ufb01cient retrieval;information retrieval;similarity query document;document representations;enables ef\ufb01cient retrieval;retrieval;sparse dense document;retrieval based;nn search;document vectors;traditional information retrieval;information retrieval signals;based nn search;approach information retrieval;exact search;comprehension based nn;queries document;exact search methods"}, "d864944df8e765d597484ace12dbc3ac99e950a9": {"ta_keywords": "proximal policy optimization;policy gradient algorithms;policy optimization;modern policy gradient;gradients modern policy;policy gradient;algorithms proximal policy;gradients especially actor;predictive control;policy optimization ppo;performance predictive control;gradient algorithms proximal;robust estimator predicting;agent policy diverges;heavytailedness gradients;heavytailedness gradients introduce;estimating performance predictive;increases agent policy;tailedness increases agent;snowdrift demonstrate gradients;performance predictive;gradient algorithms;agent policy;loss clipping gradient;novel heuristic estimating;robust estimator;heuristic estimating performance;tailed nature gradients;predicting outcome snowdrift;clipping heuristics", "pdf_keywords": "tailedness policy gradients;policy gradient learning;policy gradient methods;performance policy gradient;policy gradient algorithms;policy gradients;policy gradients common;proximal policy gradient;policy gradient;gradient clipping actor;feature policy gradient;clipping loss training;algorithms proximal policy;popular reinforcement learning;heavy tailedness policy;tailedness performance policy;tailedness induced policy;learning popular reinforcement;implements policy gaussian;gradient learning popular;policy parameters estimate;tailedness policy;deep rl optimization;policy gaussian;gaussian distribution policy;gradient algorithms proximal;induced policy training;policy gaussian distribution;induced offpolicy training;implementations gradient clipping"}, "e6239cc789da289929d49ffed2c0a562213d4703": {"ta_keywords": "stress deformation welding;deformation welding;deformation introduced welding;deformation welding position;welding ring ribbed;welding residual stress;welding ring;mechanical model welding;cylindrical shell titanium;model welding;fillet welding;process welding distortion;welding residual;introduced welding ring;shell titanium alloy;welding distortion effectively;welding distortion;fillet welding used;process welding;model welding process;welding process welding;diminish welding;welding;effect welding residual;welding process;diminish welding residual;welding position constraint;systematically ring stiffened;effect welding;weld heat treatment", "pdf_keywords": ""}, "60e339d25d43c026cf96395aa8accf34eae744a5": {"ta_keywords": "collaborative filtering;retrieval recommender evaluation;recommender evaluation proposed;recommender evaluation;collaborative filtering leverages;approach collaborative filtering;information retrieval recommender;retrieval recommender;pairwise scoring;age gender datasets;capture subjective human;players pairwise scoring;recommender;capture subjective;pairwise comparison tasks;subjective human;age gender;pairwise scoring paper;gender datasets capture;pairs players pairwise;players pairwise;gender datasets;users turn pairwise;subjective human perception;allow capture subjective;pairs players;distributions age gender;friend preferences;subjective;collaborative", "pdf_keywords": "crowdsourced pairwise comparisons;dataset crowdsourced pairwise;crowdsourced pairwise;methods crowdsourced pairwise;dataset crowdsourced;crowdsourcing dataset;aggregation methods crowdsourced;open dataset crowdsourced;crowdsourcing dataset balanced;approach crowdsourcing dataset;methods crowdsourced;crowdsourced;crowdsourcing;crowdsourced approach;crowdsourced approach crowdsourcing;pairwise comparisons building;present crowdsourced;approach crowdsourcing;present crowdsourced approach;paper present crowdsourced;pairwise comparisons;imdb wiki pagerank;benchmark computer vision;imdb wiki dataset;pairwise comparisons disconnected;edges dataset evaluate;known benchmark;systems human judgments;dataset ground truth;pagerank"}, "4ef77ef9b8ca8c8a96f1e62fa86a988feb582bd1": {"ta_keywords": "selection importance sampling;importance sampling intelligent;importance sampling;adaptation language model;adaptation language;domain pretraining;data selection importance;pretraining domain;domain fine tuning;present adaptation techniques;data selection;sampling intelligent;language model noisy;based data selection;study adaptation language;domain pretraining domain;selection influence functions;achieve better generalization;data selection influence;adaptation techniques;selection;intelligent data selection;adaptation techniques based;adaptation;fine tuning;dimensional 2d spin;selection importance;study adaptation;antiferromagnet;training model", "pdf_keywords": "translation learning;translation training domain;machine translation training;translation learning training;machine translation learning;new domain adaptation;domain adaptation;data machine translation;combines machine translation;translation training;machine translation;translation language models;domain adaptation method;adaptation data selection;unsupervised multitask learners;machine translation language;importance sampling contrastive;importance sampling generic;used machine translation;multitask learners gpt;training domain data;importance sampling;models unsupervised multitask;methods importance sampling;learning training domain;multitask learners;adaptation data;translation language;language models unsupervised;unsupervised multitask"}, "09ec8d8e2251e079abb0e109979f33ee120211fa": {"ta_keywords": "proximal extragradient method;optimal tensor method;outer accelerated proximal;extragradient method;extragradient method based;proximal distance method;proximal extragradient;new proximal extragradient;accelerated proximal distance;tensor methods order;tensor method involving;accelerated proximal;tensor methods;tensor method;existing tensor methods;optimal tensor;xmath0 optimal tensor;outer accelerated;condition outer accelerated;proximal distance;extragradient;rate existing tensor;method calculating eigenvalues;eigenvectors hamiltonian exact;eigenvectors hamiltonian;distance method;distance method present;generalization step size;existing tensor;eigenvalues eigenvectors hamiltonian", "pdf_keywords": ""}, "a8a863e85a95919773868204d672f1260e0058ce": {"ta_keywords": "neural machine translation;translation nnmt models;universal model translate;machine translation nnmt;existing neural machine;xmath0 wave superconductor;model translate;language embeddings input;translation parameter generator;neural machine;machine translation;superconductor;wave superconductor;target language embeddings;ground state xmath0;model neural;neural network architecture;monolingual data training;network model neural;translate multiple languages;language embeddings;model translate multiple;wave superconductor approach;model neural network;state xmath0 wave;superconductor approach;source target language;superconductor approach based;existing neural;target language", "pdf_keywords": "multiilingual machine translation;neural machine translation;machine translation nn;parameter generator multilingual;translation nn models;machine translation;machine translation able;unsupervised learning multilingual;generator multilingual nmt;learning multilingual;multilingual nmt;multiilingual machine;contextual parameter generator;generator multilingual;translate multiple languages;multiway multiilingual machine;translating different modalities;multilingual nmt enables;learning multilingual languages;multilingual;multiway multiilingual;multiilingual;translation able perform;universal model translate;zero shot learning;model translate;translation nn;model translate multiple;language speci parameterization;propose multiway multiilingual"}, "d82592f3a110308366dfc7c42565d437b5bf59af": {"ta_keywords": "automated social skills;social skill speech;training developing dialogue;social skills training;skill speech language;training social skills;social skills trained;social skills trainer;skill speech;social skills design;social skills;skills trained human;individual social skills;provides social skills;performing social skills;developing dialogue;social skill;improve social skills;process social skills;method training social;recognizes user speech;user speech;user speech language;training social;trained human computer;skills training proposed;improve skill using;autism;skills trainer provides;trained human", "pdf_keywords": ""}, "5dab371fecc43904c0b785a50136d20cee43a99a": {"ta_keywords": "speech translation models;recognition translation tasks;direct attentional models;attentional models;translation models;trainable recognition translation;translation tasks;translation models require;model attention;translation tasks propose;model attention stages;direct speech translation;recognition translation;speech translation;auxiliary training data;task trainable recognition;formulation model attention;effectively direct attentional;corpus translated speech;multi task training;model trained end;attention stages cascades;translated speech;model recognition end;attention;attentional;multi task trainable;direct attentional;task training;direct model trained", "pdf_keywords": "attentionpassing model speech;model speech translation;speech translation data;mapping speech translation;partial speech translation;translation audio utterances;trained partial speech;automatic translation audio;translation audio;attentionpassing model;resource speech translation;model automatic translation;model speech;speech translation increases;attention passing model;low resource speech;attention multi task;speech translation;stage attentionpassing model;mapping speech;direct mapping speech;attention multi;speech translation employ;task models trained;task models heavily;performance attention multi;partial speech;translation data;attention;automatic translation"}, "461188735d46dc1062f5d1d382d940a24c355fad": {"ta_keywords": "automatically extract relationship;relation classifier aggregating;relation classifier;autoregressive process arp;autocorrelation function autoregressive;extract relationship sentence;sentence level relation;autoregressive process;pairs large corpus;large corpus;entity pairs;autoregressive;extract relationship;autocorrelation;study dynamics autocorrelation;corpus;function autoregressive process;entity pairs large;autocorrelation function;function autoregressive;scores entity pairs;dynamics autocorrelation;relationship sentence level;dynamics autocorrelation function;relation;level relation classifier;classifier aggregating;entity;aggregating scores entity;automatically extract", "pdf_keywords": ""}, "f6be5d90199d1644b85e6b41a7a7f42fb29dbc9a": {"ta_keywords": "children ospt ability;curricula spatial ability;ospt training children;training children ospt;significantly children ospt;colloidal objects tension;colloidal objects;spatial ability;children ospt;1st graders improved;spatial ability general;colloidal object controlled;training children;colloidal object;skills novel tasks;graders improved;school curricula spatial;4th graders;specific skill debate;1st 4th graders;graders improved clear;fourth graders;better 1st graders;graders performed better;1st graders;graders;skill debate;collection colloidal objects;graders performed;4th graders did", "pdf_keywords": ""}, "a278c07c8bd2921e59dd862cd91a0540dd340030": {"ta_keywords": "causal inference neural;datasets causal inference;estimation treatment effects;benchmark datasets causal;estimation treatment;effect neural networks;estimate treatment;estimate treatment effect;datasets causal;estimator effect neural;propensity score estimation;networks estimation treatment;probability treatment propensity;neural networks estimation;inference neural;literature estimation treatment;inference neural networks;treatment propensity score;final estimate treatment;causal inference;treatment effects observational;treatment propensity;effect neural;training neural networks;probability treatment;effects observational data;neural networks;neural networks used;training neural;models downstream estimator", "pdf_keywords": "estimation effect treatment;estimation treatment effects;estimation causal effects;propensity score estimation;estimation causal;causal inference observational;estimation treatment;causal effects observational;consider estimation causal;causal inference;status causal inference;literature estimation treatment;adjusting covariates illness;covariates recover adjusting;effect treatment outcome;adjusting covariates recover;causal effects;effects observational data;recover adjusting covariates;effects observational;propensity score;status causal;estimation adjustment;socioeconomic status causal;inference observational;inference observational data;outcome recover adjusting;estimation effect;causal;suf\ufb01ciency propensity score"}, "9c5c794094fbf5da8c48df5c3242615dc0b1d245": {"ta_keywords": "unsupervised learning disentangled;learning disentangled representations;learning disentangled;disentanglement learning;disentanglement learning explicit;learning disentangled toys;suggest disentanglement learning;disentangled representations fundamentally;disentangled representations;enforcing disentangled representations;learning downstream tasks;disentangled representations consider;demonstration unsupervised learning;disentangled representations real;unsupervised learning;idea unsupervised learning;biases implicit supervision;disentangled;enforcing disentangled;learning downstream;complexity learning downstream;sets unsupervised learning;unvised learning;disentanglement;results suggest disentanglement;learning explicit;benefits enforcing disentangled;implicit supervision;disentanglement does;disentangled toys", "pdf_keywords": "unsupervised disentanglement learning;disentanglement learning;disentanglement learning methods;classi\ufb01cation task disentanglement;unsupervised learning disentangled;tasks better disentangling;unsupervised disentanglement;task disentanglement;generative model disentanglement;learning disentangled representations;learning disentangled;recent unsupervised disentanglement;disentangled representations;disentanglement average classi\ufb01cation;task disentanglement plays;better disentangling;model disentanglement based;disentanglement based;disentanglement;model disentanglement;disentangling;art unsupervised disentanglement;unsupervised learning;disentanglement plays significant;disentangled;disentangled representations 85;disentanglement based machine;disentanglement average;accuracy downstream tasks;downstream classi\ufb01cation task"}, "2a2d03a1534b365c5b048c824c0886e16ccf7dfa": {"ta_keywords": "text knowledge base;extracting relational knowledge;relational knowledge text;parsed text knowledge;syntactic patterns knowledge;binary relation prediction;knowledge base;knowledge base freebase;freebase reading relational;large knowledge base;learns syntactic semantic;existing knowledge base;patterns knowledge base;knowledge text based;text knowledge;reading relational information;knowledge text;semantic inference rules;rules binary relations;extracting relational;knowledge base study;text existing knowledge;relational knowledge;learns syntactic;syntactic semantic inference;relation prediction methods;knowledge base distributed;inference rules binary;reading relational;method extracting relational", "pdf_keywords": ""}, "535c58ec8020782d41ed3ca72cf94aff7fd65120": {"ta_keywords": "automatic speech recognition;approach automatic speech;automatic speech;speech recognition;new approach automatic;speech;recognition;approach automatic;automatic;new approach;propose new approach;paper propose new;approach;paper propose;paper;new;propose new;propose", "pdf_keywords": ""}, "4c93bcc05d6b41cb3df451d703f34bab9c9e9201": {"ta_keywords": "privacy utility;privacy utility propose;privacy privacy;differentialial privacy;quantifiable privacy;privacy;differentialial privacy privacy;privacy measure;privacy privacy measure;privacy guarantees;ensuring quantifiable privacy;tradeoff privacy utility;quantifiable privacy guarantees;privacy guarantees provides;privacy measure used;mechanisms differentialial privacy;suboptimal tradeoff privacy;tradeoff privacy;guarantees text queries;accurately learning user;provide guarantees text;better machine learning;learning user data;guarantees text;mechanical tweezer;maintaining user trust;learning user;tweezer;user trust;accurately learning", "pdf_keywords": "erential privacy_ metric;privacy_ metric space;erential privacy;di erential privacy;erential privacy instance;erential privacy_;challenge designing privacy;privacy_ metric;erential privacy emerged;designing privacy;maintaining desired privacy;di erential privacy_;desired privacy guarantees;privacy instance di;designing privacy mechanisms;desired privacy;privacy;privacy instance;privacy mechanisms;privacy emerged topic;private data;privacy emerged;privacy mechanisms text;privacy guarantees paper;framing private data;privacy guarantees;privacy_;private release word;release word embedding;private data release"}, "96abcdded2985bd44b9514e28f5b8da4fa1e4371": {"ta_keywords": "model interpretability;interpretation model interpretability;interpretability context statistical;interpretability;concept interpretability;model interpretability context;interpretability important;learning concept interpretability;concept interpretability important;interpretability important slippery;interpretability context;interpretation model;paper interpretation model;machine learning concept;machine learning;context statistical physics;interpretation;statistical physics;paper interpretation;slippery paper interpretation;statistical physics simple;physics;learning concept;learning;statistical;model;context statistical;question fundamental importance;question fundamental;fundamental", "pdf_keywords": ""}, "18e8646001fc53465fdc8f8eb01523e24c134493": {"ta_keywords": "ranking cardinal scores;estimators rely ranking;scores induced ranking;ranking cardinal;induced ranking cardinal;scores ordinal rankings;ordinal rankings;superiority cardinal scores;induced ranking;ordinal rankings variety;consider cardinal scores;induced ranking popular;cardinal scores arbitrary;cardinal scores induced;consistent induced ranking;rankings rates approaches;rankings;rely ranking;ranking;rankings variety;cardinal scores;cardinal scores collected;cardinal scores ordinal;rankings rates;new ranking;higher using rankings;bias scores;using rankings;new ranking method;ordinal data ranks", "pdf_keywords": ""}, "f78e5aaf34cc1e4874490e9155c640b73c630021": {"ta_keywords": "learning incorporates opponent;general sum games;agents heterogeneous conjectures;dynamic behavior opponents;sum games;conjectural learning;conjectural learning framework;game theoretic;gradient conjectures;behavior opponents game;game theoretic outcomes;gradient conjectures analyze;outcomes conjectural learning;incorporates opponent behavior;learning framework generalizes;sum games review;implicit gradient conjectures;opponents game;nash stackelberg general;gradient based learning;equilibrium introduce general;versatile method learning;opponent behavior;learning rules;number game theoretic;learning rules recent;opponent behavior continuous;generalizes number learning;cost function embeds;agents heterogeneous", "pdf_keywords": ""}, "f132f6534ec326a1ba61870b701015cd3d1560a2": {"ta_keywords": "domain adaptation neural;domain adaptation;selection domain classifiers;method domain adaptation;fine tuning domain;selection language modeling;selection domain;tuning domain;data selection domain;selection fine tuning;contrastive data selection;selection method domain;domain generalization;domain generalization work;domain classifiers effective;pretraining selected data;domain classifiers;training fine tuning;modeling machine translation;data selection language;pretraining selected;selected data training;selection language;new data selection;machine translation;complementarity selection;complementarity selection fine;assess complementarity selection;adaptation neural networks;adaptation neural", "pdf_keywords": "translation machine learning;adaptation machine translation;machine translation outperforms;multilingual machine translation;machine translation experiments;translation machine translation;particular machine translation;machine translation;translation machine;translation outperforms;modeling machine translation;machine translation iii;machine translation machine;machine translation ii;selection approach multilingual;translation outperforms popular;classi\ufb01ers machine translation;translation computer translation;machine translation challenging;translation challenging tasks;translation experiments;domain adaptation machine;domain adaptation;computer translation;computer translation machine;domain adaptation data;ii machine translation;translation ii machine;machine translation computer;translation experiments focus"}, "835587dbe94b70adeb0b16384e10bb6e0e29de84": {"ta_keywords": "pairwise preference good;pairwise preference;pairwise pairwise preference;preference good predictor;predictor outcome match;outcome match;preference good;good predictor outcome;preference;pairwise;match;pairwise pairwise;predictor outcome;good predictor;outcome;predictor;good", "pdf_keywords": ""}, "97bcea32979ed602fd404448a4e4cedad4171d79": {"ta_keywords": "generate nonverbal behaviors;predict nonverbal behavior;individual nonverbal behaviors;prediction nonverbal behaviors;nonverbal behaviors match;humans nonverbal behavior;nonverbal behaviors spoken;nonverbal behaviors;nonverbal behaviors body;communicating humans nonverbal;nonverbal behaviors work;generating individual nonverbal;nonverbal behavior;nonverbal behavior varies;models predict nonverbal;personality virtual agent;individual nonverbal;nonverbal behavior label;humans nonverbal;predict nonverbal;automatically generate nonverbal;generate nonverbal;prediction nonverbal;improve prediction nonverbal;expected personality virtual;personality virtual;important generate nonverbal;nonverbal;behaviors spoken language;dyadic interactions realize", "pdf_keywords": ""}, "89c148d3d4edcb7b13c35da36b97ffb881c38058": {"ta_keywords": "el speech enhancement;speech enhancement;enhance el speech;speech enhancement methods;el speech recognition;sounds enable laryngectomees;enable laryngectomees;natural el speech;enable laryngectomees produce;proposed enables laryngectomees;el speech sounds;speech proficient laryngectomees;intelligible el speech;enables laryngectomees;electrolarynx el device;speech recognition real;method electrolarynx el;electrolarynx based;optrolaryngeal el speech;electrolarynx el;control electrolarynx based;el speech proficient;enables laryngectomees produce;proficient laryngectomees;electrolarynx based statistical;proficient laryngectomees produce;enhance el;method enhance el;speech recognition;el speech", "pdf_keywords": ""}, "b8dcc2ae3346e41a421232169c2ca07957c654c4": {"ta_keywords": "evolutionary game theory;games recurrent orbits;recurrent games agents;agents games recurrent;learning evolutionary game;game theory;time populations agents;evolutionary game;recurrent games;game conservation laws;average game conservation;poincar recurrent games;population dynamic agents;evolve strategically time;games play evolve;converge nash equilibrium;dynamic agents;dynamic agents interact;games recurrent;sum competition evolves;game conservation;competition evolves adversarially;coevolving network game;agents games introduce;game theory based;strategically time populations;agents games;time average game;nash equilibrium;games agents games", "pdf_keywords": ""}, "135ace829b6ad2ec9db040d8e5fd137034e83665": {"ta_keywords": "probabilistic model segmentation;segmentation;segmentation labels;algorithm predicting shape;body semi markov;predicting shape;predicting shape semi;recognition biological entities;model segmentation;model segmentation labels;approach recognition biological;conditional random fields;random fields conditionally;random fields;segmentation labels assigned;markov chains;semi markov;recognition;recognition biological;markov;approach recognition;fields conditionally trained;fully wound circle;markov conditional random;segments individual elements;semi markov conditional;wound circle fully;markov conditional;wound circle;probabilistic", "pdf_keywords": ""}, "635932ee917d71e01f07211c0359abf3e0e65e47": {"ta_keywords": "automatic speech;speech recognition asr;field automatic speech;automatic token sequence;performance autoregressive transducers;automatic speech recognition;speech recognition;recognition asr;token sequence generation;sequence generation;autoregressive transducers;autoregressive model automatic;predict length token;insertion based models;training insertion based;autoregressive transducers public;model automatic token;benchmarks insertion based;performance autoregressive;benchmarks insertion;autoregressive transformer;sequence generation paper;asr paper autoregressive;recognition asr paper;length token sequence;transducers public benchmarks;sequence nat model;autoregressive transformer able;token sequence;investigate performance autoregressive", "pdf_keywords": "automatic speech;insertion non autoregressive;sequence insertion based;networks automatic speech;recurrent neural networks;speech recognition asr;non autoregressive transformer;autoregressive transformer;neural machine translation;prediction token sequence;generation based recurrent;autoregressive transformer mi;token sequence insertion;modeling insertion non;sequence insertion;sequence generation based;non autoregressive models;insertion based models;word prediction token;\ufb01eld automatic speech;automatic speech recognition;sequence generation;acoustic feature sequence;token sequence generation;word prediction;formulation word prediction;autoregressive models;modeling insertion;speech recognition;recurrent neural"}, "b30195763eb103e2e5564228119f3810ab423b2e": {"ta_keywords": "supervised words label;labeled documents learning;supervised words;documents learning unlabeled;data supervised words;labeled documents training;models document classification;document classification;classification current text;neural language models;current text classification;text classification;topic sentiment classification;words label names;learning unlabeled data;learning unlabeled;words label;sentiment classification;trained neural language;classification using labeled;classification models unlabeled;labeled documents;human labeled documents;related words label;using labeled documents;documents training data;text classification methods;category indicative words;document classification current;unlabeled data supervised", "pdf_keywords": "semi supervised text;supervised text classi\ufb01cation;weaklysupervised text classi\ufb01cation;weaklysupervised semi supervised;supervised text;text classi\ufb01cation train;semi supervised;supervised semisupervised;weakly supervised learning;weaklysupervised text;weakly supervised;novel weakly supervised;semi supervised data;present supervised semisupervised;supervised semisupervised learning;learning model text;text classi\ufb01cation label;recently semi supervised;semi supervised methods;predict category words;model text classi\ufb01cation;text classi\ufb01cation propose;semisupervised learning;text classi\ufb01cation;problem weaklysupervised text;semisupervised learning model;semantic dependency text;classi\ufb01cation label;text classi\ufb01cation popular;semmi semi supervised"}, "002c256d30d6be4b23d365a8de8ae0e67e4c9641": {"ta_keywords": "powerful language models;recurrent architectures;probabilistic language models;language models;language models able;languages neural;combinatorial languages neural;languages neural networks;language models form;form recurrent architectures;language modelling;probabilistic language;language modelling lm;million parameter models;tasks language modelling;models form recurrent;class probabilistic language;combinatorial languages;modelling probability text;parameter models seminal;parameter models;conditional token predictions;class combinatorial languages;languages;token predictions;neural networks;powerful language;lm unsupervised task;tasks language;neural", "pdf_keywords": "leakage large language;improving language models;large language models;billions tokens crowdsourced;test documents training;training data leak;language modelling performance;tokens crowdsourced data;language models prone;data leak evaluation;evaluate language modelling;language models perfectly;leak evaluation sets;models perfectly memorise;training set queries;language models;tokens crowdsourced;improving language;crowdsourced data;large language;chunked cross attention;language modelling;use large language;test documents;retrieval;perfectly memorise parts;training data;crowdsourced data demonstrate;test set leakage;demonstrate retrieval"}, "933396f5b9111f6acdd76710ee6ab4d24e8673dd": {"ta_keywords": "speech recognition semi;semi supervised end;unpaired speech text;speech text datasets;semi supervised training;novel semi supervised;human language autoencoding;speech text data;text data speech;autoencoding text data;recognition semi supervised;semi supervised;speech text mapping;end automatic speech;paired speech text;automatic speech;proposed semi supervised;speech data;speech data intermediate;text data semisupervised;combining speech totext;data speech;language autoencoding text;data speech data;unpaired text data;data semisupervised end;representations speech text;autoencoding text;language autoencoding;learning reconstruct unpaired", "pdf_keywords": ""}, "4c49e9b57c8fc58b0df29a27ecca8cc2e376f02b": {"ta_keywords": "web search semantically;web search algorithm;search semantically;search semantically related;pages web search;web search;entities based web;autonomously spidered data;algorithm finds pages;finds pages likely;finds pages;lists pages web;heuristics extract lists;semantically related entities;pages web;based web web;present web search;web web pages;extract lists pages;web pages;search;pages;search algorithm finds;spidered data combined;user data improve;lists pages;search algorithm;web pages case;spidered data;web web", "pdf_keywords": ""}, "7d5f83cb234a5640487bd258ace06d9dc967d222": {"ta_keywords": "statistical relational learning;entity extraction structure;entity extraction;relational learning;learning reasoning;based entity extraction;information extraction;learning reasoning tasks;extraction structure learning;context based entity;probabilistic logic framework;relational learning involves;learning inference;knowledge base;scalable probabilistic logic;performs learning reasoning;constructs knowledge base;probabilistic logic;shown information extraction;joint learning inference;learning inference sl;statistical relational;information extraction errors;probabilistic order logics;entity;structure learning;text performs learning;context based;reasoning tasks using;pipeline statistical relational", "pdf_keywords": ""}, "f1c7419b87cbf853e691e500643f71720b68fb86": {"ta_keywords": "stacked graphical learning;learning stacked learning;stacked learning;stacked learning save;learning stacked;online learning stacked;online stacked graphical;graphical learning expensive;graphical learning widely;graphical learning promising;online stacked;stacked graphical;experimentally online stacked;large streaming datasets;graphical learning;collective classification;graphical learning gives;standard stacked graphical;propose collective classification;collective classification method;maintaining large graphs;graphical learning able;cost online stacked;problem online stacked;streaming datasets;streaming datasets minimal;stacked;large graphs;dataset large cost;predictions constructed training", "pdf_keywords": ""}, "9b3fd2525a2d1abc44145308e013f117d3d7bdee": {"ta_keywords": "el speech enhancement;speech enhancement;controlled el speech;speech enhancement methods;speech enhancement technique;sounds enable laryngectomees;laryngectomees produce electrically;prediction proficient laryngectomees;enable laryngectomees produce;enable laryngectomees;el speech sounds;electrolarynx based statistical;electrolarynx el device;speech keeping intelligibility;excitation prediction improve;electrolarynx based;control electrolarynx based;el speech keeping;statistical voice conversion;electrolarynx el;automatic control electrolarynx;method electrolarynx el;statistical excitation prediction;proficient laryngectomees produce;excitation sounds enable;generates excitation sounds;proficient laryngectomees;voice conversion;intelligible el speech;excitation feature prediction", "pdf_keywords": ""}, "1afa3ab80abda57920b8d456a6513e6f01cc82e7": {"ta_keywords": "event forecasting conversations;conversations regression tasks;forecasting conversations regression;forecasting conversations;event prediction tasks;predicting length conversation;conversations regression;prediction tasks;prediction tasks common;considered text regression;classification tasks;time event prediction;predictions regression tasks;text regression;text regression methods;modelled classification tasks;event prediction;conversation modelling applications;conversation modelling;classification models time;predict time event;event distribution linguistic;event forecasting;time event forecasting;common conversation modelling;classification tasks determining;regression tasks recent;tasks common conversation;linguistic markers conversations;regression tasks", "pdf_keywords": ""}, "5cfdb162ffa4dce18f7c576d352bd459b6a11292": {"ta_keywords": "driven personalized coupons;personalized coupons consider;personalized coupons;coupons consider;discount coupons;coupons;forecast future purchases;coupon;coupon provider;limited discount coupons;predicts correlations purchases;based party coupon;discount coupons partner;future purchases based;coupons partner;promotions exploit consumers;predict outcome supermarket;party coupon provider;party coupon;urgency boost sales;coupon provider time;future purchases;temporal dynamics purchase;purchases based generate;consumer spending forecast;model consumer spending;purchases based;dynamics purchase behavior;promotions exploit;coupons partner large", "pdf_keywords": ""}, "1ddcc9671dd6486e34cefadfe71bbbc1bc55035a": {"ta_keywords": "word embeddings trained;pretrained word embeddings;word embedding;word embeddings;word embeddings utilize;quality word embeddings;word embedding word;standard word embedding;embeddings utilize morphological;introduce word embeddings;embeddings utilizing subword;embedding word similarity;embedding word;word embeddings utilizing;word embeddeddings;word embeddeddings low;source word embeddeddings;embeddings trained;environment word embeddings;neural machine translation;embeddings trained standard;model pretrained word;embeddings utilize;embeddings;embeddings utilizing;unsupervised alternatives neural;subword information consistently;trained standard word;utilize morphological resources;word similarity", "pdf_keywords": ""}, "33d2ebe41477811296abc4077bf9ce09b927ef98": {"ta_keywords": "statistical voice conversion;voice conversion based;speakers statistical voice;voice conversion;voice conversion using;statistical voice;voice conversion framework;content spoken speakers;propose voice conversion;speakers statistical;framework voice conversion;gaussian mixture models;propose probabilistic mixture;function speakers statistical;probabilistic mixture;probabilistic mixture model;based gaussian mixture;density model speaker;integrates speaker gmm;gaussian mixture;conversion based noisy;mixture models;spoken speakers;framework integrates speaker;mixture models estimate;mixture model based;transform function speakers;integrates speaker;corpus;speaker gmm", "pdf_keywords": ""}, "c0e8846eb5ce574a6dca3f3a600e82b184339254": {"ta_keywords": "inverse reinforcement learning;inverse reinforcement;reinforcement learning inverse;elasticity body language;body language conveys;learning inverse;body language;learning inverse rl;utterances elicit desired;infers rewards language;reinforcement;language conveys information;rewards language;actions unseen environments;elicit desired actions;predict optimal actions;rl powerful reinforcement;language conveys;reinforcement learning;utterances elicit;powerful reinforcement;actions;rewards language pragmatically;choose utterances elicit;desired actions;powerful reinforcement learning;optimal actions unseen;utterances;body contact;like map actions", "pdf_keywords": "actions language learning;learning actions language;utterance action reward;language games interaction;learning language games;language rewards flightpref;language improve reward;mapping language rewards;learning actions;language rewards;actions language;language games;action reward descriptive;action reward;task natural language;model infer reward;actions underlying reward;infer reward functions;improve reward estimates;infer reward;learning language;language utterances humans;reward functions;reward functions natural;language learning language;reward descriptive;language functions elicit;natural language utterances;games interaction;elicit correct actions"}, "a3cc75975a5998d5a7dd494e70a479ba0a550013": {"ta_keywords": "programming authoring tutoring;build intelligent tutoring;intelligent tutoring;intelligent tutoring systems;tutoring systems;cognitive tutor authoring;tutor authoring tools;intelligent tutor expert;tutor expert model;tutor authoring;intelligent tutor;authoring tutoring;tutoring;authoring tutoring decision;tutor expert;tutoring systems heavy;learn intelligent tutor;tutoring decision making;cognitive tutor;tutor;tutoring decision;student teaching tutoring;teaching tutoring;programming authoring;tutoring present;student performance tutoring;tutoring paper;tutoring present new;performance tutoring;teaching tutoring paper", "pdf_keywords": ""}, "04f4e55e14150b7c48b0287ba77c7443df76ed45": {"ta_keywords": "commonsense reasoning;commonsense reasoning corresponding;physical commonsense reasoning;question answering;question answering abstract;natural language systems;task physical commonsense;natural language;understanding natural language;commonsense;progress question answering;text inherently;physical commonsense;dataset fluid;benchmark dataset fluid;reasoning corresponding benchmark;reasoning;answering abstract;reasoning corresponding;human data;recent pretrained models;dataset fluid dynamics;pretrained models;reproduce human data;pretrained models progress;human data recent;abstract domains news;answering;answering abstract domains;language systems", "pdf_keywords": "annotators provide semantic;physical commonsense knowledge;annotators;natural language;knowledge expressed language;commonsense knowledge;commonsense knowledge expressed;semantic;large scale language;words tagged spacy;understand natural language;annotators provide;physical commonsense understanding;required physical commonsense;progress physical commonsense;physical commonsense;provide semantic perturbations;modeling physical commonsense;semantic perturbations alternative;trueeness including robots;provide semantic;language versatility;physical knowledge targeted;tagged spacy noun;commonsense understanding;commonsense properties;semantic perturbations;robots;commonsense;including robots"}, "5e0f2f82b4a28d59f1aa8b8ffe497790de1cdf9d": {"ta_keywords": "reinforcement learning wild;deep reinforcement learning;novel reinforcement learning;deep reinforcement;predict outcome game;predict probability catastrophe;predicting catastrophes;reinforcement learning;catastrophes bad agents;predicting catastrophes social;learn reward shaping;algorithm predicting catastrophes;unacceptable deep reinforcement;catastrophes social network;policy accelerates learning;novel reinforcement;propose novel reinforcement;reward shaping;catastrophe state;learning wild;learning predict probability;catastrophic states exploit;policies repeated catastrophes;reward shaping policy;learn reward;outcome game;catastrophe state propose;reinforcement;outcome game approach;structure learn reward", "pdf_keywords": ""}, "7a9f4a8a99f9a38e9213da890f9eab6150ae928e": {"ta_keywords": "learning similarity measures;personalized page ranking;learnable proximity measure;page ranking known;learning similarity;edge label sequences;learnable proximity;novel learnable proximity;graph databases recommendation;ranking known random;similarity measures;based random walks;page ranking;similarity measures based;reset learning similarity;recommendation tasks;recommendation tasks inference;random walks;noisy knowledge bases;known random walk;graph databases;walk reset learning;ranking known;large noisy knowledge;databases recommendation tasks;noisy knowledge;querying graph databases;edge label;random walk;random walks shown", "pdf_keywords": ""}, "ebaae38a09c5a4909049e16af759c71db9cc87dc": {"ta_keywords": "question answering;answering natural language;question answering natural;answer question answering;natural language inference;language inference neural;inference neural;inference neural models;language inference;natural language processing;answering;words possible input;neural models;neural;natural language;answer question;language processing tasks;answering natural;neural models widely;inference;language processing;selection right answer;prediction right answer;extreme natural language;words possible;probabilistic;propose probabilistic;approach answer question;model prediction;possible input", "pdf_keywords": ""}, "b17fa6625681d99370122145ba9911f701dd92cb": {"ta_keywords": "semantic parsing context;parsing context;parsing context code;parsing context received;typical context modeling;13 context modeling;context modeling;effective context based;world semantic parsing;decoding semantic parser;frequent contextual;context modeling methods;semantic parsing;context based learning;far effective context;context based;semantic parser;challenging complex contextual;study context modeling;contextual phenomena far;semantic parser adapt;13 context;effective context;contextual;complex contextual;frequent contextual phenomena;decoding semantic;contextual phenomena fine;context code available;context code", "pdf_keywords": "semantic parsing context;parsing context;parsing context important;modeling methods parser;semantic parsing;typical context modeling;13 context modeling;semantic parser;semantic parser adapt;grammarbased decoding;suited grammarbased decoding;based semantic parser;different context modeling;context modeling;parsing;grammar based semantic;grammarbased decoding paper;parser;mechanism suited grammarbased;context modeling methods;parser including;parser adapt typical;context important task;parser adapt;13 different context;13 context;grammar based;grammarbased;adapt typical context;evaluate 13 context"}, "bf4da952df7a6ef9c0b2be8b4b4b69ad63848b8f": {"ta_keywords": "predict traffic;traffic speed prediction;available predict traffic;historical traffic data;predict traffic speed;learning intelligent transportation;historical traffic;transfer learning;transfer learning framework;speed prediction;propose transfer learning;traffic data;traffic;target areas traffic;areas traffic;traffic data available;little historical traffic;speed prediction benefit;intelligent transportation systems;traffic speed;traffic speed target;intelligent transportation;data available predict;historical data target;prediction;dynamics transfer football;areas traffic speed;machine learning intelligent;spatiotemporal features;transfer football ball", "pdf_keywords": ""}, "879cd78b0d4413aef614bc6b6cce075e8e6ad4be": {"ta_keywords": "packet erasure channels;existing streaming codes;streaming codes;existing streaming code;adversarial packet erasure;streaming codes designed;achieving streaming code;packet erasure channel;streaming code;size streaming codes;codes encode stream;packets size streaming;streaming codes class;streaming code constructions;encode stream;erasure channel;erasure channels;minimum decoding delay;stream source packets;streaming codes variable;encode stream source;capacity achieving streaming;constructions adversarial packet;channel introduces erasures;decoding delay channel;achieving streaming;strict decoding delay;erasure channels source;transmission packet erasure;existing streaming", "pdf_keywords": ""}, "bd318e959236b0d33a7567b6d3afc8d5e92b8ea3": {"ta_keywords": "ethically bounded artificial;building artificial intelligence;intelligence systems ethical;artificial intelligence life;bounded artificial intelligence;intelligence life autonomous;ai mind;artificial intelligence;boundaries ai mind;artificial intelligence outline;ai;ai mind paper;intelligence systems;collaborating humans ethical;autonomous collaborating humans;systems ethical properties;systems ethical;boundaries ai;artificial intelligence ia;artificial intelligence systems;ethically bounded;humans ethical;ethical;life autonomous collaborating;automata;notion ethically bounded;approach artificial intelligence;automata learning flexible;ethical properties component;humans ethical principles", "pdf_keywords": "ethically bounded ai;ethical boundaries machine;ethically bounded artificial;building ethically bounded;intelligence systems ethical;bounded artificial intelligence;ai agent;approaches ai human;ai human;ai agent propose;ethically bounded;ethical boundary evolves;ethical boundaries;bounded ai;systems ethical properties;approaches ai;notion ethically bounded;ethical boundary;person ai agent;capability ethically bounded;preferences ethical boundaries;evolving capability ethically;ai;systems ethical;building ethically;sure ethical boundary;ethical properties component;ethical properties;ethical;person ai"}, "4a93f7654f795871ed99dece2e1805e4950fd194": {"ta_keywords": "learning spatial language;referent spatial utterances;spatial language;utterances spatial;spatial utterances;learning spatial;spatial language presented;understand utterances spatial;language space spatial;utterances spatial configurations;referent spatial;spatial utterances level;spatial;spatial relations;spatial relations rich;learns understand utterances;landmarks sentence conditionally;generative probabilistic framework;referents landmarks sentence;semantic structure;framework learning spatial;generative probabilistic;comprehension data human;semantic relations;space spatial relations;decide semantic relations;semantic;decide semantic;language space;abstract semantic structure", "pdf_keywords": ""}, "9e172f35b2b0ebcff090f01d40e61fa5aecefa68": {"ta_keywords": "adversarial loss functions;adversarial loss divergence;different adversarial losses;adversarial losses;adversarial loss;various adversarial loss;adversarial losses loss;adversarial losses propose;valid adversarial losses;functions adversarial loss;adversarial training;adversarial losses combining;adversarial training objectives;designing adversarial training;set adversarial losses;various adversarial;discriminative adversarial;different adversarial;component functions adversarial;adversarial networks;adversarial;discriminative adversarial networks;functions adversarial;valid adversarial;designing adversarial;proposed various adversarial;functions valid adversarial;compare different adversarial;based discriminative adversarial;loss functions training", "pdf_keywords": ""}, "ebe04f06580abab035408c4c2e65245b3950934e": {"ta_keywords": "investigation structure thunderstorm;structure thunderstorm occurred;structure thunderstorm;thunderstorm occurred;thunderstorm occurred western;thunderstorm;occurred western california;western california;western california january;california january 2011;california;california january;investigation structure;results investigation structure;structure;paper report results;report results investigation;paper report;results investigation;investigation;occurred western;report results;january 2011;2011;western;occurred;report;results;january;paper", "pdf_keywords": ""}, "dd36aca034312a266d6f10b37414d3342c3b9c79": {"ta_keywords": "energy xmath0 particle;xmath0 particle;xmath0 particle ground;energy difference particles;difference particles energy;particle ground state;energy xmath0;energies particle states;difference particles;particle function separation;energy levels particle;energies particle;particle states;particles energy;particle ground;particle states calculate;energy particle;particle;addition energy xmath0;particles energy particle;particles;levels particle;energy particle given;separation distance;particle given;sum energies particle;energies;states calculate energy;xmath0;particle function", "pdf_keywords": ""}, "17351cfeac949c266f4d1ff86c515250b931bdc2": {"ta_keywords": "structure learning logic;learning logic;learning logic programs;inductive logic programming;markov logic networks;logic networks;learn structures;logic networks mlns;scalable probabilistic logic;approach learn structures;logic programs graphs;learned order structures;probabilistic logic;efficient structure learning;inductive logic;generated parameter learning;structure learning;logic programming;methods inductive logic;learn structures tractable;logic programs;probabilistic logic order;order probabilistic logic;probabilistic logic called;learning methods inductive;novel structure learning;structure learning method;logic programming ilp;parameter learning;discover underlying structures", "pdf_keywords": ""}, "394e17f5ee5e8a734b2714795b7da3cd704716da": {"ta_keywords": "ordinal evaluations peer;evaluations peer grading;peer grading;evaluations peer;peer grading present;peer evaluation;peer evaluation based;peer evaluation answers;approach peer evaluation;ordinal evaluations;students evaluate;classroom evaluation;classroom evaluation students;grading;feedback peer evaluation;evaluation feedback peer;evaluation students performance;robustness ordinal evaluations;scores robustness ordinal;evaluation students;set students evaluate;students performance;ordinal approach significantly;students evaluate paper;grading present;ordinal approach;evaluations;students performance article;evidence ordinal approach;students", "pdf_keywords": ""}, "57e4074c588c0e27e4c0bc89f12512ccdb900d79": {"ta_keywords": "unsupervised machine translation;deep generative;machine translation experiments;deep generative model;unsupervised style transfer;present deep generative;machine translation;generative model unsupervised;sequence model learns;generative objectives backtranslation;including sentiment transfer;model unsupervised text;translation experiments unified;translation experiments;amortized variational inference;recurrent language model;non generative techniques;sentiment transfer;translation present deep;objectives backtranslation adversarial;learns transform sequences;backtranslation adversarial;generative techniques;variational inference;generative;backtranslation adversarial loss;non generative;parallel corpus;sentiment transfer formality;text style transfer", "pdf_keywords": "unsupervised machine translation;translation model learns;sentiment transfer;machine translation model;probabilistic machine translation;including sentiment transfer;machine translation;generative sequence models;text style transfer;generative objectives backtranslation;language translation probabilistic;machine translation contrast;traditional unsupervised generative;style transfer tasks;transfer task paraphrasing;sentiment transfer formality;unsupervised generative;ways sentiment transfer;sentiment transfer task;unvised style transfer;unsupervised generative sequence;style transfer based;translation probabilistic;objectives backtranslation adversarial;style transfer;sentiment preserving original;paraphrasing;translation probabilistic view;backtranslation adversarial loss;different sentiment preserving"}, "8a99e1eb3285f127eed7169441679d47be7f1633": {"ta_keywords": "text generation directly;text generation;data text generation;context free grammar;interpretable controllable generation;reduced parsing;free grammar;parsing;derivation reduced parsing;learns policy directly;algorithm learns policy;generation directly splicing;derivation generation;learns policy;generation directly;reduced parsing particular;algorithm learns;par strong baselines;controllable generation;controllable generation propose;context free;text source;text source target;splicing segments text;parsing particular;segments text source;learning algorithm learns;grammar;oracle derivation generation;weighted context free", "pdf_keywords": "generation derive grammar;text generation systems;controllable interpretable generation;text generation;text generation allows;approach text generation;interpretable generation;neighbors text generation;interpretable generation derive;right token generation;generation task easier;generation allows controllable;generation task;token generation;make generation task;interpretability controllability generation;generation derive;derive grammar;easier interpretable controllable;grammar derivation cost;token generation model;derive grammar derivation;grammar derivation;allows controllable interpretable;increase interpretability controllability;controllable interpretable;generation model using;explicitly parameterize splicing;parameterize splicing;make generation"}, "3c40fc36217a56aafb0abc735ff7d132b17e83a0": {"ta_keywords": "automatic melody harmonization;melody sequence automatic;melody harmonization;sequence automatic melody;sequence harmonic accompaniment;automatic melody;algorithm deep;melody harmonization triad;chords;melody sequence;chord sequence harmonic;melody chord;harmonic accompaniment;melody chord pairs;generates chord sequence;genetic algorithm deep;chords comparative study;algorithm deep learning;harmonic accompaniment given;harmonization triad chords;9226 melody chord;bar melody sequence;deep learning method;chord sequence;generates chord;chords comparative;dataset 9226 melody;model generates chord;deep learning;melody", "pdf_keywords": "models melody harmonization;model melody harmonization;melody harmonization based;melody harmonization challenging;melody harmonization;harmonize melody makes;chordal accompaniment evaluation;harmonize melody;models melody;based models melody;ways harmonize melody;model melody;melody sequence;propose model melody;harmonization pleasant;harmonization based;generated chordal accompaniment;accompaniment evaluation;harmonization challenging;quality generated chordal;harmonization;harmonize;harmonization based tree;harmonization pleasant subjective;musical notes midi;harmonization challenging task;particular harmonization pleasant;chordal accompaniment;accompaniment evaluation shows;melody makes"}, "77efa3102456c9f921b05b95eefe845d2ce6bc4b": {"ta_keywords": "speech recognition regularized;recognition regularized discriminative;regularized discriminative training;relative entropy discrimination;continuous phoneme recognition;entropy discrimination proposed;regularized discriminative;rive regularized discriminative;discriminative training method;recognition regularized;phoneme recognition;discriminative training methods;discriminative training;phoneme recognition tasks;minimum relative entropy;conventional discriminative training;entropy discrimination mred;entropy discrimination;speech recognition;training methods regularization;regularization techniques;used speech recognition;regularization;training method acoustic;conventional discriminative;relative entropy;limitations conventional discriminative;discriminative;methods regularization techniques;constraints used speech", "pdf_keywords": ""}, "803a0d2677a7d6b20c3964533595775fa5c7c750": {"ta_keywords": "testing partial ranking;testing ranked preference;comparison models crowdsourcing;comparison data ranking;comparisons ratings rankings;pairwise comparison models;testing ranked;tests pairwise comparison;sample testing ranked;ranking dataset statistically;pairwise comparison data;comparisons ratings;pairwise comparison;sample tests pairwise;ranked preference data;distributions pairwise comparison;partial ranking data;ranked preference;partial ranking;ranking data provide;ratings converted comparisons;world pairwise comparison;tests pairwise;rankings provided people;question pairwise comparison;ranking data;partial total ranking;comparison models;ranking dataset;total ranking dataset", "pdf_keywords": "pairwise comparison models;comparison models crowdsourcing;partial ranking;partial ranking de\ufb01ned;pairwise comparisons;total ranking subset;case partial ranking;pairwise comparison data;pairwise comparison;total ranking;ranking subset;ranking;sets pairwise comparisons;pairwise comparisons obtained;ranking items;ranking subset collection;ranking items according;comparison models;question pairwise comparison;ranking de\ufb01ned total;de\ufb01ned total ranking;ranking de\ufb01ned;models crowdsourcing;range pairwise comparison;rate testing sparse;distributed identically ratings;models crowdsourcing long;problem ranking;testing sparse;testing sparse linear"}, "c859416a8e5682bee3c35df29bc02e02a22de072": {"ta_keywords": "manually transcribed speech;automatic transcription language;workflow automatic transcriptions;speech transcription tool;transcribed speech practical;automatic transcriptions used;transcription language documentation;output speech recognition;automatic transcription;automatic transcriptions;speaker speech transcription;speech transcription;integrating automatic transcription;transcribed speech;automatic speech;manually transcribed;automatic speech recognition;field automatic speech;transcription tool;transcription language;performance speech recognition;speech recognition;transcription tool trained;analysis output speech;output speech;speech recognition systems;transcribed;facilitating language documentation;language processing specialists;hours manually transcribed", "pdf_keywords": ""}, "90705ece92a71efcf256cd047da53cbc1d4e5295": {"ta_keywords": "gear vibration data;extracted gear vibration;gearbox vibration frequency;gear vibration;gear meshing frequency;vibration gearbox;extract vibration gearbox;gear meshing impacts;gearbox vibration;impacts gear meshing;vibration gearbox vibration;meshing impacts gear;gear meshing impact;gear mesh impact;vibration gearbox great;gear impacts;impacts gear;experiment gear mesh;gear meshing;gear mesh;signal gear meshing;moving gear impacts;vibration data;vibration data acquired;resonance gearboxes fundamental;resonance gearboxes;gearboxes fundamental;structure resonance gearboxes;component extracted gear;meshing impact load", "pdf_keywords": ""}, "30fa01df767339a6c8bd37c32160992fcb19ed18": {"ta_keywords": "computation multiarmed bandit;approximation regret game;bandit problem;bandit problem new;multiarmed bandit problem;game theoretic analysis;empirical game theoretic;empirical game;semifield nash equilibria;multiarmed bandit;game theoretic;guarantee approximation regret;semifield nash;semif semifield nash;work empirical game;large scale games;approximation regret;nash equilibria;bandit;regret game;regret game paper;optimization goal unlike;new optimization goal;optimization goal;scale games;nash equilibria ne;scale games propose;games;algorithm solve supersymmetric;games propose", "pdf_keywords": ""}, "196be0bdec3b7bcb3ee35cd126fb2730a9d742d6": {"ta_keywords": "simulations tutor effect;simulations tutor;students interactively tutor;interactively tutor;learns skills tutored;simulations students able;students interactively;tutored problem solving;simulations students;interactively tutor sim;problems simulations tutor;teach simulations;skills tutored;tutor;learn dynamics;environment students interactively;teach simulations students;learn teach simulations;able learn dynamics;tutor effect;skills tutored problem;tutor effect significantly;learning environment simulations;learn dynamics body;online learning environment;tutor sim student;learns skills;tutored;online learning;equations tutor effect", "pdf_keywords": ""}, "46619f0547b1a9c2e7649d0e5c931e9aa857a938": {"ta_keywords": "coalescence black holes;xmath0 body simulation;black holes lhc;xmath0 body;results xmath0 body;latest results xmath0;lhc;simulation process coalescence;holes lhc;results xmath0;black holes;xmath0;process coalescence black;coalescence black;process coalescence;coalescence;body simulation;body simulation process;simulation;simulation process;body;latest results;latest;holes;results;report latest;report latest results;report;black;process", "pdf_keywords": ""}, "8963602d4b9c3b1054a5ed6fb2a2088dec774824": {"ta_keywords": "recipient machine learning;motion pedestrian crowded;pedestrian crowded environment;pedestrian crowded;machine learning;motion pedestrian;simple machine learning;machine learning used;pedestrian;model motion pedestrian;machine learning model;learning model motion;recipient machine;email messages;crowded environment;sensitive email;sensitive email message;recipient;learning model;sends sensitive email;personal information email;particles needed;searches require awareness;particles needed carry;crowded;information email messages;motion;sends sensitive;personal information;difficult searches", "pdf_keywords": ""}, "daedf33077099f7c808e9f4022469e15bf224ad7": {"ta_keywords": "injury based transcriptome;based transcriptome profiling;transcriptome profiling;identify kidney transplants;kidney transplants risk;transplants risk chronic;transcriptome;based transcriptome;kidney transplants;transplants risk;method identify kidney;chronic injury based;risk chronic injury;identify kidney;transplants;chronic injury;kidney;profiling;risk chronic;injury based;chronic;injury;new method identify;method identify;letter propose new;identify;propose new method;letter propose;risk;letter", "pdf_keywords": ""}, "59d487d6ef839c82ae128550e35fa44058b03d37": {"ta_keywords": "simulation complex networks;simulation complex;complex networks based;complex networks;approach simulation complex;simulation;neural network approach;networks based neural;neural network;networks based;based neural network;approach simulation;new approach simulation;network approach;networks;based neural;complex;network;neural;approach;paper;based;new approach;paper propose;paper propose new;propose new approach;propose;propose new;new", "pdf_keywords": ""}, "60ce57713261b41fe2e3d222f1d4530c4fc69241": {"ta_keywords": "randomized rules assignment;agents sharing;dynamics agents sharing;agents sharing common;randomized rules;prominent randomized rules;agent following rules;polynomial time probabilistic;rules assignment problem;known desirable fairness;team play agent;expected utility best;random matrix;simple rule team;largest eigenvalue random;desirable fairness;fairness welfare properties;rules assignment;agents;agent following;random matrix study;assignment problem;randomized;probabilistic serial rule;eigenvalue random matrix;probabilistic;probabilistic serial;desirable fairness welfare;eigenvalue random;rule prominent randomized", "pdf_keywords": "probabilistic assignment objects;probabilistic assignment;algorithm assigning preference;preferences agents probabilistic;agents randomization;randomized rules assignment;preference lists agents;probabilistic serial assignment;algorithm assigning;allocation theory;sequential allocation;case agents randomization;procedural fairness;allocation natural ways;allocation;assignment objects;algorithm case agents;agents probabilistic;preferences agents;complexity agent manipulating;agents randomization widespread;computational complexity agent;ensure procedural fairness;randomized rules;assignment prominent randomized;rules assignment problem;lists agents based;assigning preference lists;complexity agent;knowledge preferences agents"}, "74d8a998269bcdd087a21840b0e28d86c256c121": {"ta_keywords": "hyperbolic discounting learnable;discounting learnable pac;discounting learnable;preference models learner;discounted utility models;general preference models;active learning;pac learning;preference models;provided pac learning;active learning algorithms;framework active learning;hyperbolic discounting;quasi hyperbolic discounting;level active learning;known learning bounds;algorithms important discounted;learning membership;learnable pac;class preference models;preference models defined;pac learning present;known learning;utility models intertemporal;models intertemporal choice;learnable pac setting;learning bounds general;utility models;stream based learning;learning framework", "pdf_keywords": "preference models learning;quasi hyperbolic discounting;general preference models;preference models;hyperbolic discounting model;preference models present;hyperbolic discounting;learning economic parameters;present preference models;learning utility;models learning economic;learning utility functions;learn preference parameters;class preference models;utility maximizing agent;preference models de\ufb01ned;query algorithms learning;discounting model;discounting model able;problem learning utility;preference parameters guiding;known learning bounds;maximizing agent demand;utility maximizing;economics present membership;learning economic;membership query algorithms;learning bounds general;agent choices plans;bounds general preference"}, "c994372b3c33bbc1ad6b504c5efb5afd515a5009": {"ta_keywords": "retraining target speech;target speech extraction;speech recognition;speech extraction automatic;speech extraction;extraction automatic speech;speech recognition systems;automatic speech;automatic speech recognition;speech recognition gain;segments spoken target;suitable speech recognition;describes speech recognition;model estimate speaker;loss suitable speech;loss based speaker;speech recognition uses;estimate speaker characteristics;spoken target speaker;speech proposed loss;target speech;estimate speaker;loss function retraining;overlapped speech proposed;overlapped speech;obtained segments spoken;segments spoken;auxiliary loss;mixture consistency loss;auxiliary loss function", "pdf_keywords": ""}, "09d88e0bb8863fd402030aeb625c52c0492c4fef": {"ta_keywords": "speech recognition reverberated;encoder blind reverberation;reverberation investigate deep;reverberation results 2014;recognition reverberated environments;deep recurrent noising;feature space reverberation;recurrent noising auto;robust automatic speech;blind reverberation;recognition reverberated;auto encoder blind;auto encoders dae;blind reverberation investigate;speech recognition;recurrent noising;automatic speech;noising auto encoders;reverberation results;reverberation;encoder blind;noising auto encoder;reverberated environments;automatic speech recognition;auto encoder;investigate deep recurrent;auto encoders;2014 reverb challenge;encoders dae;space reverberation results", "pdf_keywords": ""}, "320278b24a3c53a44f95e8ef5465bebe56f24225": {"ta_keywords": "annotated pauses;explicitly annotated pauses;pauses prediction;particular pauses prediction;prediction used speech;trained syntactically annotated;pause prediction;pauses prediction used;prediction prosodic;pause prediction based;prediction prosodic information;individually prediction prosodic;prediction based syntactically;structure pause prediction;annotated pauses resulting;speech synthesis;prosodic information text;trained syntactically;algorithm pause prediction;speech synthesis allow;particular pauses;syntactically annotated data;used speech synthesis;pause prediction paper;prosodic information;based syntactically annotated;model trained syntactically;syntactically annotated;11 particular pauses;pauses", "pdf_keywords": ""}, "568462ab0a0a59a2575b70db2cd9022572526f3f": {"ta_keywords": "games simultaneous gradient;algorithm stackelberg game;learning continuous games;zero sum games;dynamics stackelberg games;stackelberg games;game provide convergence;general sum games;stackelberg game;sum games;stackelberg game provide;learning algorithm stackelberg;gradient descent;simultaneous gradient descent;continuous games;nash stackelberg equilibria;learning algorithms called;sum games establishing;sum games simultaneous;called gradient descent;sum games contemporary;gradient based learning;gradient descent mitigate;gradient descent work;adversarial learning algorithms;continuous games commonly;algorithms called gradient;gradient descent connection;adversarial learning;games", "pdf_keywords": ""}, "92622a58377a4671b2ba59e8e59b19b0ab5119bb": {"ta_keywords": "knowledge graph identification;knowledge graph construction;knowledge graph built;knowledge graphs provide;knowledge graph;models partitioning extractions;knowledge graphs built;parallel knowledge graph;knowledge graphs;require knowledge graphs;partitioning extractions;scaling knowledge graph;incorporates ontology graph;represented ontological information;markov models partitioning;results knowledge graph;ontology graph distribution;ontology graph;hash based approaches;partitioning information parts;ontological information distributional;technique knowledge graph;performance knowledge graphs;graphs noisy extractions;ontological information;partitioning extractions used;partitioning information;relationships automatically constructing;scaling knowledge;parts represented ontological", "pdf_keywords": ""}, "2ab481028dda04197283c03115bb5f46f5998cc3": {"ta_keywords": "proton collisions lhc;collisions lhc;collisions lhc using;xmath0 decay proton;coupled pendulum oscillators;analysis xmath0 decay;dynamics coupled pendulum;xmath0 decay;coupled pendulum;proton collisions;lhc using;proton proton collisions;pendulum oscillators case;lhc;pendulum oscillators;decay proton;lhc using data;decay proton proton;pendulum;l3 detector fermilab;detector fermilab tevatron;dynamics coupled;analysis xmath0;oscillators case;oscillators case divided;detector fermilab;oscillators;fermilab tevatron;xmath0;results analysis xmath0", "pdf_keywords": ""}, "582089a00a6c9fb534f16d1dbbafc50cc4e3912a": {"ta_keywords": "queries semanticreasoning powerful;queries semanticreasoning;language queries semanticreasoning;deploying ontology reasoning;ontology reasoner natural;aware semantic reasoning;tasks ontology reasoner;ontology reasoner;query answering systems;schema aware semantic;natural language queries;reasoning domain semantics;ontology reasoning domain;semantic reasoning;semantic reasoning framework;ontology reasoning;natural language query;solvable tasks ontology;semanticreasoning;semanticreasoning powerful;semanticreasoning powerful approach;query answering;reasoner natural language;specialized query language;language query interfaces;tasks ontology;systems deploying ontology;traditional query answering;aware semantic;semantics help achieving", "pdf_keywords": ""}, "1adadbfa95e43a70fcd17e6ce947a0652b86bfc3": {"ta_keywords": "remarkable progress nlp;examples benchmark nlp;benchmark nlp datasets;benchmark nlp;large language models;nlp tasks researchers;progress nlp;nlp tasks;nlp datasets;progress nlp tasks;text corpora train;larger text corpora;language models;text corpora;machine translation;machine translation systems;behavior large language;nlp;translation systems;videos motile animals;corpora train;train machine translation;corpora;videos motile;corpora train machine;large language;motility videos motile;motility videos;corpora available scraping;translation systems evaluation", "pdf_keywords": "sentiment based language;provide sentiment based;crawled corpus;clean crawled corpus;trained language models;learned language models;machine learned language;machine trained language;provide sentiment;language models;pretrained language models;crawled corpus c4;dialect aware topic;aware topic model;language models unfortunately;sentiment based;correlates sentiment expressed;language models used;bias correlates sentiment;dialect aware;language models given;language model web;text scraped web;correlates sentiment;sentiment;c4 language models;use dialect aware;machine generated text;corpus;topic model"}, "90129b0733ac48ead26b7c86e8b4df917568e208": {"ta_keywords": "trip number participants;participants involved trip;length trip;length trip number;relationship length trip;involved trip;trip number;trip;number participants involved;number participants;investigate relationship length;relationship length;participants involved;participants;length;paper investigate relationship;number;investigate relationship;relationship;involved;investigate;paper investigate;paper", "pdf_keywords": ""}, "02d98ca8f4ecd1a2b885d6867f4c1407ae8d1007": {"ta_keywords": "weakly supervised parsers;parsers trained;parser trained fully;queries semantic parser;parser trained;performance parser trained;supervised parsers;supervision queries semantic;supervised parsers learnt;parsers learnt;semantic parser;natural language commands;semantic parser maps;parsers trained pairs;extra supervision queries;supervised setting parsers;parsers learnt cold;especially weakly supervised;supervision queries;annotating examples improve;queries especially weakly;wikitablequestions human annotators;parsers;study performance parser;wikisql annotating examples;setting parsers trained;performance parser;queries semantic;language commands nls;weakly supervised", "pdf_keywords": "weakly supervised semantic;weak supervision semantic;utilize weakly supervised;propose weakly supervised;extra supervision queries;weakly supervised;datasets wikisql;supervision semantic parsing;active weak supervision;supervision semantic;weakly supervised learning;wikisql dataset;datasets wikisql dataset;supervised semantic;supervised learning semantic;learning semantic;supervised semantic parser;queries propose weakly;2017 wikitablequestions dataset;query varieties supervision;supervision queries propose;supervision queries;semantic parsing;active learning heuristics;learning semantic parsers;wikitablequestions dataset;semantic parsers remains;semantic parsers;different datasets wikisql;active learning"}, "601398838250a4e69c69cc339d65f5c51e727ad1": {"ta_keywords": "questions situation inherent;conditioning existing answers;diverse wellformed questions;text asking questions;ontology predicates roles;answers text asking;explicitly text sought;predicates roles develop;question best strategy;question prototype role;predicates roles;text asking;independent question prototype;broad coverage ontology;question prototype;context independent question;coverage ontology predicates;existing answers text;ontology predicates;revises contextually appropriate;questions;text sought evaluation;answers text;text sought;contextually appropriate;wellformed questions;coverage ontology;role revises contextually;contextually appropriate passage;way answer question", "pdf_keywords": "automatic question generation;question generation;question generation evaluate;coverage natural language;automatic question;predicate generate contextually;arguments produce questions;generate contextually appropriate;leveraging syntactic;produce questions highly;produce questions;answer automatic question;language understanding leveraging;understanding leveraging syntactic;expressed text semantic;natural language understanding;leveraging syntactic structure;overcome lack annotated;generate contextually;questions stage approach;text semantic;predicate instances ontonotes;syntactic structure qa;predicate generate;annotated data implicit;associated predicate generate;ontologies qa pairs;natural language;annotated data;formal ontologies qa"}, "c783bc02f5f901e4604eb3b0d504a036369afd91": {"ta_keywords": "rigid body perovskite;perovskite layer;perovskite layer affected;body perovskite layer;perovskite;body perovskite;fluid dynamics;motion rigid;external electric field;fluid dynamics video;motion rigid body;rigid body;dynamics;electric field;dynamics video;video motion rigid;rigid;dynamics video motion;motion;presence external electric;external electric;video motion;layer affected presence;electric;layer affected;field;layer;fluid;affected presence external;external", "pdf_keywords": ""}, "a3452276ada37727d0008dad8ca7c27bbbee6984": {"ta_keywords": "network rumor classification;rumor classification;social networks rumor;rumor detection;rumor classification enhances;rumor propagation;existing rumor detection;networks rumor propagation;rumor detection methods;detecting rumors;demonstrate rumor detection;rumor detection method;attention network rumor;rumor propagation era;datasets demonstrate rumor;networks rumor;detecting rumors early;capacity detecting rumors;demonstrate rumor;network rumor;stages existing rumor;twitter datasets demonstrate;rumors early stages;extensive experiments twitter;experiments twitter datasets;experiments twitter;twitter;attention network;graph attention network;twitter datasets", "pdf_keywords": "facilitate rumor detection;rumor detection supervised;interactions rumor detection;rumor detection;rumor detection especially;rumor detection global;response rumor detection;task rumor detection;rumor detection paper;rumor cascades;rumor cascades widespread;user interactions rumor;rumor debunking social;crises rumor cascades;rumor debunking;facilitate rumor;approaches facilitate rumor;community response rumor;debunking social media;formulate task rumor;framework rumor debunking;interactions rumor;response rumor;graph attentionbased embeddings;rumors conspiracy theories;graph attention network;natural language inference;graph attentionbased;task rumor;hierarchical graph attention"}, "19be8dd52d949fed1a3e5aca7630669da2575d73": {"ta_keywords": "stable matching exists;stable matchings;stable matchings presence;stable matching;sided stable matching;stability stable matchings;stable matching setting;certainly stable matching;matching setting uncertainty;matchings presence uniaxial;probability given matching;matchings;given matching;matching highest probability;matching;matching finding matching;matchings presence;finding matching;models uncertainty lottery;matching finding;matching exists;given matching finding;uncertainty lottery model;uncertainty agents preferences;matching exists consider;probability stable;uncertainty lottery;matching highest;matching setting;highest probability stable", "pdf_keywords": ""}, "35ee53492c7f32dbb3b4ed7ba4d1395218b13ee9": {"ta_keywords": "everyday algorithm auditing;user driven auditing;auditing behaviors;facilitate auditing behaviors;auditing behaviors emerge;algorithm auditing;organic auditing behaviors;systems formal auditing;auditing approaches greatly;driven auditing processes;driven auditing;auditing processes;auditing approaches;formal auditing;facilitate auditing;auditing;formal auditing approaches;tools facilitate auditing;auditing processes argue;everyday users algorithmic;auditing regardless users;formal auditing organic;everyday use algorithmic;auditing organic auditing;organic auditing;auditing regardless;algorithm auditing drawing;users algorithmic systems;users algorithmic;auditing organic", "pdf_keywords": "everyday algorithm auditing;everyday algorithm audits;everyday auditing behaviors;algorithm audits;algorithm auditing;user driven auditing;auditing behaviors;algorithm audits recent;algorithm auditing illustrate;harmful algorithmic behaviors;auditing behaviors opportunities;algorithmic sociotechnical platforms;algorithmic sociotechnical;driven auditing processes;everyday auditing;driven auditing;users algorithmic;engaged everyday auditing;algorithm online rating;auditing;auditing processes;harmful algorithmic;users algorithmic systems;algorithmic expertise organicness;types algorithmic sociotechnical;algorithmic expertise;auditing illustrate;algorithms target everyday;audits;auditing processes detailed"}, "1f38ba33063f118f574cf57ff9f1a0e7de2857ff": {"ta_keywords": "measure semantic similarity;similarity words language;analysis semantic similarity;english distributional skip;semantic similarity words;semantic similarity;distributional skip gram;skip gram models;gram models;gram models directly;language rich morphology;approaches skip gram;similarity words;semantic similarity based;words language short;skip gram;approaches english distributional;large scale corpus;english distributional;words language;skip gram model;gram model;corpus;language short;free word order;gram model estimated;scale corpus able;scale corpus;understood language rich;gram", "pdf_keywords": "language semantic similarity;semantic similarity measure;semantic similarity measures;evaluating semantic similarity;measure semantic similarity;semantic similarity measures2;semantic similarity english;semantic similarity lexical;russian language semantic;semantic similarity;similarity lexical;similarity lexical items;approaches semantic similarity;contextual semantic similarity;semantic association words;study semantic association;datasets russian language;similarity english;semantic association;language semantic;evaluating semantic;benchmark datasets russian;tool evaluating semantic;similarity english cbow;similarity measures2 evaluation;association words language;semantic;similarity measures;study semantic;test datasets russian"}, "857036a25401c19e484cc32d974c90cd9a46cd66": {"ta_keywords": "local nash equilibria;approximation local nash;local optimality nonlinear;equilibria continuous games;nash equilibria continuous;local optimality;conditions local optimality;local equilibrium equilibria;games nash equilibria;equilibrium equilibria noncooperative;local equilibrium;optimality nonlinear programming;local nash;continuous games nash;nonlinear programming optimal;nonlinear programming;equilibria noncooperative continuous;nash equilibria;programming optimal control;existence local equilibrium;equilibria noncooperative;noncooperative continuous games;optimality nonlinear;nash equilibria play;optimal control theory;optimal control;equilibria play;equilibria play important;continuous games;programming optimal", "pdf_keywords": ""}, "d473ff103565d8c76e0cbfa33bdd4b0db1cbb23f": {"ta_keywords": "bid auction games;auction games;bayesian games characterized;bayesian games;complex bayesian games;auction games distinct;shot bayesian games;bayesian games given;equilibria finite strategy;equilibria games;game theoretic approach;study equilibria games;finite strategy sets;games given analytic;auction;auctions work;optimization finite strategy;closed form game;bid auction;finite strategy set;finite strategy;secondprice auctions work;finding winning strategy;equilibria games firstand;auctions;secondprice auctions;strategy generation;sealed bid auction;game theoretic;strategy sets", "pdf_keywords": "symmetric bayesian games;shot bayesian games;bayesian games;bayesian games sbg;economic games auctions;optimization regret minimization;bayesian games given;symmetric shot bayesian;pure strategy space;strategy space;model economic games;shot symmetric bayesian;economic games;symmetric bayesian;games auctions;minimizing regret function;optimization regret;regret minimization;black box optimization;strategy space framework;stochastic search;games auctions informed;regret minimization paper;stochastic search parameter;evolution strategies;based stochastic search;bid auctions;games given analytic;function pure strategy;bne empirical optimization"}, "ecd2b355f250abfd4eb9d6c7c598c33c7cd6bcb0": {"ta_keywords": "crowdsourcing data labeling;bidimensional swimmer crowdsourcing;swimmer crowdsourcing;crowdsourcing data;crowdsourcing datasets;crowdsourcing datasets binary;increasing crowdsourcing data;swimmer crowdsourcing workers;real crowdsourcing datasets;crowdsourcing;variety real crowdsourcing;probabilistic labeling;real crowdsourcing;increasing crowdsourcing;statistics crowds;study statistics crowds;crowdsourcing workers;rapidly increasing crowdsourcing;probabilistic labeling model;statistics crowds men;crowdsourcing workers usually;principle paper crowdsourcing;paper crowdsourcing;crowdsourcing used;derive probabilistic labeling;labeling;labeling derive probabilistic;data labeling;labeling model;crowds", "pdf_keywords": ""}, "0fc01a915cc7bf7025f80d44f805bd54b6425a33": {"ta_keywords": "noise sampling;measurement noise sampling;littlewood richardson algorithm;bounds probability distinguishing;noise sampling rate;probability distinguishing scenarios;generalization littlewood richardson;probability distinguishing;richardson algorithm;richardson algorithm paper;sampling;littlewood richardson;scenarios function measurement;measurement noise;sampling rate device;probability distinguishing different;function measurement noise;generalization littlewood;distinguishing scenarios;studying generalization littlewood;data derive bounds;distinguishing different scenarios;derive bounds probability;algorithm paper consider;bounds probability;sampling rate;richardson;distinguishing scenarios function;algorithm paper;upper bounds probability", "pdf_keywords": "energy disaggregation algorithms;analyzing energy disaggregation;model privacy;algorithms analyzing disaggregated;energy disaggregation;disaggregation algorithms;privacy;disaggregation algorithms recent;analyzing disaggregated;de\ufb01nition constitutes privacy;information adversary arbitrary;information adversary;analyzing disaggregated data;prior information adversary;disaggregated data;paper model privacy;constitutes privacy;model privacy fashion;disaggregated data way;adversary arbitrary de\ufb01nition;privacy fashion encapsulates;probability successful disaggregation;privacy fashion;aggregate power consumption;arbitrary prior information;power consumption signal;successful disaggregation;disaggregation;individual devices dynamical;consumption signal goal"}, "16457c13a40aa589fa06d8533a47b3f96aede474": {"ta_keywords": "2d topological insulator;topological insulator;2d topological;topological insulator present;dimensional 2d topological;topologically;folding bows;topological;bows canopy wall;bows folded plane;emergence topologically ordered;model folding bows;emergence topologically;model emergence topologically;insulator;topologically ordered state;topologically ordered;2d;folding bows bows;state dimensional 2d;pellet list competition;dimensional 2d;method tying bows;algorithm finding location;bows folded;wall building;bows canopy;design construction;bows bows folded;canopy wall", "pdf_keywords": ""}, "639cc01afcc1c78063f7a6bbdae998cd147911c4": {"ta_keywords": "parametric utility learning;utility learning scheme;utility learning;utility learning framework;extend utility learning;defined estimated utility;estimated utility function;estimate utility;robust parametric utility;estimated utility;utility function game;estimate utility function;utility function smart;energy efficient behavior;utility function;gradient boosting;game defined estimated;gradient boosting ensemble;encourage energy efficient;used estimate utility;parametric utility;estimation heteroskedastic inference;boosting ensemble methods;utility;efficient behavior smart;behavior smart building;bumping gradient boosting;heteroskedastic inference proposed;boosting;learning scheme", "pdf_keywords": "utility learning incentive;learning incentive;learning incentive design;utility learning framework;utility learning;utility learning method;novel utility learning;learning utility;utility learning methods;social game experiment;learning utility functions;social game decisions;participants social game;outcome social game;explore utility learning;game decisions participants;works utility learning;utility functions observations;social game example;algorithm learning utility;players ensemble estimators;incentive design coupled;incentive design;social game based;social game;decisions participants social;predict outcome social;vote shared resources;building social game;social game conducted"}, "03d2af05e41aac58ebae4ab37f09155e53d4b35b": {"ta_keywords": "noisy bit observations;bit observations;matrix completion;standard matrix completion;theory matrix completion;matrix completion methods;matrix completion extreme;bit observations paper;binary data analysis;generating bit measurements;binary data maximum;bit measurements eliminate;binary data;based binary data;analysis based binary;bit measurements;bound number photons;completion methods;approach binary data;number photons emitted;observable data;accurate estimate infty;estimate infty alpha;data maximum likelihood;completion methods movie;photons emitted unit;number photons;new approach binary;estimate infty;observations", "pdf_keywords": "matrix completion probabilistic;matrix completion bit;bit matrix completion;completion bit matrix;matrix completion;matrix completion quantum;matrix completion methods;standard matrix completion;approach matrix completion;model matrix completion;compressed sensing recover;compressed sensing;data compressed sensing;completion probabilistic;sensing recover sparse;recover sparse;recover sparse vector;completion probabilistic model;matrix accurately e\ufb01rst;completion quantum state;quantum state tomography;sparse vector rd;unknown matrix accurately;state tomography probabilistic;probabilistic model matrix;bit matrix;completion quantum;matrix accurately;completion methods;recovered binary measurements"}, "3bfa1fe0a8d031d59dfc0cfa4975c296951ee56c": {"ta_keywords": "speech recognition living;temporal modeling sounds;speech recognition;modeling sounds;recognition living rooms;rooms based spatial;based spatial spectral;spectral temporal modeling;living rooms based;spatial spectral temporal;spatial spectral;living rooms;rooms based;temporal modeling;speech;spectral temporal;based spatial;rooms;recognition;recognition living;modeling;spatial;spectral;temporal;sounds;living;based", "pdf_keywords": ""}, "8873d1369590249113e1f0491ce49d1502395b9c": {"ta_keywords": "compliance videos;compliance videos video;video compliance videos;trainable compliance;trainable compliance network;instruction present compliancenet;natural language regulation;compliancenet;compliancenet novel;end trainable compliance;modal compliance;present compliancenet;present compliancenet novel;video compliance;multi modal compliance;compliance network improves;dynamics video;dynamics video movement;introduce video compliance;language regulation movement;compliancenet novel end;video compliance associated;compliance associated text;given video compliance;compliance;regulation movement animals;language regulation;trained vtc;compliance network;video movement", "pdf_keywords": ""}, "7488429131b8970425a66f3410920d98ff6e9c36": {"ta_keywords": "quality instructor reviews;biases given rating;rating information outcome;instructor reviews work;partial ordering bias;instructor comparing quality;instructor reviews;ordering bias;comparing quality instructor;method evaluate teaching;regularized optimization;evaluate teaching;quality instructor comparing;evaluate teaching quality;teaching quality;teaching quality instructor;given rating information;regularized;evaluations common;rating information;evaluations common evaluate;chooses appropriate regularization;regularized optimization problem;appropriate regularization;quality instructor;instructor comparing;receive higher rating;rating receive higher;propose debiasing method;regularization", "pdf_keywords": "group fraction estimator;biased evaluations propose;mitigating biases evaluations;biased evaluations;biases evaluations;evaluations biased evaluations;biases evaluations biased;teaching evaluation obvious;evaluations biased;assistant training sets;evaluation teaching assistant;fraction estimator;problem mitigating biases;estimator given group;teaching evaluation;reweighted mean estimator;evaluation teaching;grade adjustment;fraction estimator given;counterpart grade adjustment;unique teaching evaluation;algorithm evaluation teaching;teaching assistant training;mitigating biases;mean estimator formulate;mean estimator;assistant training;amounts bias;constraint unique teaching;evaluation obvious counterpart"}, "579e01c3c864cc98e57c728f84fcf553c5b1bcba": {"ta_keywords": "murmur nam microphone;microphone experimental;air conductive microphone;microphone experimental results;microphone air;non audible murmur;microphone air conductive;conductive microphone experimental;microphone;nam microphone air;microphone developed;audible murmur;conductive nam microphone;soft whispered voice;conductive microphone;nam microphone developed;silent speech communication;nam microphone;medium silent speech;microphone developed detect;promising medium silent;audible murmur nam;whispered voice report;noise contamination speaking;quiet environments noisy;whispered voice;extremely soft whispered;speech communication intelligibility;soft whispered;used quiet environments", "pdf_keywords": ""}, "4f1eef4acaf0164593b9e654dba4b8cd3e72421d": {"ta_keywords": "inference collective classification;stacked graphical learning;collective classification widely;collective classification;classification collective classification;collective classification collective;collective classification based;classification collective;collective classification usually;collective classification powerful;approach collective classification;concept collective classification;used collective classification;inference collective;sampling inference collective;graphical learning;graphical learning meta;classification relational;classification relational datasets;instances stacked graphical;related instances stacked;graphical learning stp;used classification relational;meta learning scheme;propose stacked graphical;classification widely;meta learning;instance predictions;predictions related instances;stacked graphical", "pdf_keywords": ""}, "31884a623af77136413d997049b5787b394db461": {"ta_keywords": "xmath1 correlation function;xmath0 xmath1 correlation;xmath1 correlation;dimensional electron gas;compute density states;electron gas confined;electron gas;confined harmonic trap;density states dos;dos dimensional electron;harmonic trap;gas confined harmonic;density states;dimensional electron;correlation function;correlation function used;states dos dimensional;method compute density;xmath0 xmath1;compute density;confined harmonic;xmath1;harmonic trap paper;xmath0;electron;gas confined;characterize time evolution;correlation;time evolution;paper xmath0 xmath1", "pdf_keywords": ""}, "2818bd090206ef33f9d7e1be03bc4f742c6762d1": {"ta_keywords": "morpheme based language;morpheme natural generalization;morpheme based;language model agglutinative;agglutinative language words;words formed suffixes;language agglutinative language;language agglutinative;propose morpheme based;language words formed;language models;agglutinative language;model agglutinative languages;agglutinative languages;morpheme;morpheme natural;uyghur language agglutinative;suffixes;suffix uyghur language;language model;generalization notion prefix;paper morpheme;series propose morpheme;paper morpheme natural;language models based;formed suffixes;words formed;based language model;propose morpheme;predictions language models", "pdf_keywords": ""}, "8f99f9409f254134aa32fbf072475100f688d613": {"ta_keywords": "codes distributed storage;estimating reliability storage;network distributed storage;reliability storage;distributed storage networks;storage networks;reliability storage based;storage distributed;distributed storage;distributed storage network;storage distributed storage;secrecy capacity;storage networks provide;theoretic secrecy capacity;storage nodes;items storage distributed;secrecy capacity setting;storage network;codes distributed;distributed storage units;storage nodes possibly;provide reliability availability;reliability availability;reliability availability data;networks provide reliability;class codes distributed;subset storage nodes;storage network dns;retrieve information distributed;storage", "pdf_keywords": "codes distributed storage;peer storage systems;peer peer storage;peer storage;storage nodes peer;regenerating codes distributed;storage considered eavesdropper;distributed storage;codes distributed;encoded data storage;nodes data storage;securely disseminating encoded;distributed storage systems;distributed storage considered;data storage nodes;eavesdroppers unbounded computational;information theoretic secrecy;information theoretically secure;storage nodes;theoretic secrecy capacity;secrecy capacity;setting distributed storage;constructions regenerating codes;secrecy capacity setting;regenerating codes achieve;data storage;nodes peer peer;nodes peer;code presented eavesdropper;eavesdroppers unbounded"}, "b130b6387b105ecd9b4718b179b1e128157f9516": {"ta_keywords": "phrase paraphrasing models;paraphrase models;paraphrasing models paraphrase;phrase paraphrase tasks;learning phrase embeddings;parametric paraphrase models;paraphrase models score;phrase embeddings;paraphrase model achieves;phrase embeddings improved;paraphrase pairs accurately;models paraphrase model;paraphrase model;paraphrase tasks;short phrase paraphrasing;paraphrase tasks propose;paraphrase pairs;paraphrasing models;phrase paraphrasing;word embedding;models paraphrase;build parametric paraphrase;short phrase paraphrase;improved word embedding;score paraphrase pairs;phrase pairs;phrase paraphrase;paraphrasing;models score paraphrase;learning phrase", "pdf_keywords": ""}, "191543c7cb084d3af6a48ae771ca3dfd0588ab22": {"ta_keywords": "learning latent representations;representations text reviews;layer latent representation;latent representations text;latent representations;learning latent;recommendation tasks fluid;combine latent representations;latent representation target;qubits deep learning;entanglement distillation protocol;user review target;latent representations obtain;entanglement distillation;recommendation tasks;target user review;latent representation;latent representation similar;similar latent representation;review target item;text reviews;especially review text;method learning latent;improve performance recommender;recommender systems;based use entangled;recommender;review text;state art entanglement;layer latent", "pdf_keywords": "recommendation using reviews;reviews improving recommender;item representations ratings;target user review;user review target;korean consumer recommendation;using reviews;reviews improving;user review;using reviews improving;review representation;generated review representation;consumer recommendation using;improving recommender;review text;review target item;review representation introduce;consumer recommendation;ratings;ratings used;available review text;improving recommender systems;item available review;review text unlike;recommender systems;reviews;generated review;recommendation using;recommender systems gaining;recommender"}, "c72cdb5ce7e0911c7f442ab503652d6fdeef35e0": {"ta_keywords": "freebase semantic parsing;semantic parsing;ccg parsers semantic;parsers semantic slot;parsers semantic;syntactic ccg parsers;parsers;parsing;captured unsupervised grammar;ccg parsers;explore syntactic supervision;semantics captured;semantic parsing dataset;syntactic supervision;unsupervised grammar induction;grammar induction systems;syntactic ccg;semantics captured unsupervised;downstream semantic analysis;unsupervised grammar;explore syntactic;downstream semantic;semantic analysis;required downstream semantic;syntactic supervision required;semantic analysis present;grammar induction;different syntactic ccg;semantic;semantic slot", "pdf_keywords": "unsupervised parsers annotation;supervised ccg parsers;unsupervised parsers;evaluate unsupervised parsers;semi supervised parsers;unsupervised parsers treebank;semi supervised parser;semantic parsing;supervised parsers;semantic parsing task;parsers annotation efforts;semantic parsing important;supervised parsers results;problematic unsupervised parsers;grounded semantic parsing;parsers grounded semantic;supervised parser;supervised parser outperforms;supervised grammar induction;supervised grammar;parsers annotation;parsers treebank;parsers;ccg parsers;sentences freebase logical;unsupervised syntax;parsing;weakly supervised grammar;parser outperforms;unsupervised syntax useful"}, "61d2dda8d96a10a714636475c7589bd149bda053": {"ta_keywords": "rnn decoders encoder;rnn decoders;decoder model rnn;model rnn decoders;encoder review network;decoder encoder review;encoder review;new encoder decoder;state art encoder;encoder decoder systems;encoder decoder;encoder decoder model;decoder encoder;attention mechanism encoder;decoders encoder;encoder decoder encoder;encoder;model rnn;decoders encoder performs;image captioning source;new encoder;encoder decoder special;decoder;captioning source code;code captioning;decoder systems tasks;propose new encoder;decoders;encoder hidden states;art encoder decoder", "pdf_keywords": ""}, "12239e761e8c7cd05e12e18f43dba7b46dfd8ac1": {"ta_keywords": "neural machine translation;multi source translation;machine translation train;multi source neural;source neural machine;ferroelectric multiferromagnet;ferromagnetic metal insulator;ferromagnetic;ferroelectric multiferromagnet using;ferromagnetic metal;2d ferromagnetic metal;multiferromagnet;machine translation;2d ferromagnetic;translation train systems;source translation systems;transition ferroelectric multiferromagnet;multiferromagnet using;translation systems;multiferromagnet using information;source neural;translation systems translate;dimensional 2d ferromagnetic;translation train;systems translate multiple;neural machine;translate multiple languages;languages multi source;translate multiple;ferroelectric", "pdf_keywords": "incomplete multilingual corpora;corpus missing translations;augmentation incomplete multilingual;multilingual corpora achieving;actual multilingual corpora;multilingual corpora;incorporate incomplete multilingual;multilingual corpora multi;incomplete multilingual;languages corpus experiments;corpora multi source;machine translation outputs;multilingual corpora ted;pseudo corpus missing;multiple languages corpus;machine translation;languages corpus;\ufb01lled machine translation;missing translations;corpora achieving improvements;translation approach multilingual;multilingual situations generating;pseudo corpus;situations generating pseudotranslations;pseudotranslations caused missing;missing translations \ufb01lled;generating pseudotranslations using;using pseudo corpus;corpora multi;corpus experiments"}, "d2a2be6ce932a0f1939f31cfff4d64ea3d76723d": {"ta_keywords": "robustness adversarial attacks;adversarial attacks;confers robustness adversarial;adversarial;robustness adversarial;adversarial attacks paper;algorithm adversarial attacks;algorithm adversarial;kw algorithm adversarial;datasets confers robustness;training label distributions;classification performance deep;explicit training dataset;increasingly training distribution;training distribution test;performance deep;distribution human labels;training distribution;deep neural;performance deep neural;kolmogorov smirnov wolfenstein;new benchmark dataset;deep neural networks;explicit training;generalization increasingly training;smirnov wolfenstein;distribution test datasets;training dataset;test datasets confers;label distributions", "pdf_keywords": "soft labels trained;trained image labels;labels trained image;training human labels;hard label trained;soft label generation;labels trained;label trained;soft labels generalize;trained image;human labels image;labels generalize better;soft labels;image labels accurate;using soft labels;labels accurate;generalization learning tiny;datasets hard label;soft label;label generation;learning tiny images;label trained controls;human labels effective;particular soft labels;human labels;generalizing natural image;labels generalize;distribution human labels;ground truth labels;image labels"}, "1668b0b9cc631cdfc0dfaf77b71627f5524a866c": {"ta_keywords": "simulation bouncer bouncer;simulation bouncer;results simulation bouncer;bouncer bouncer;bouncer bouncer performing;bouncer performing task;bouncer;bouncer performing;simulation;present results simulation;official version game;results simulation;version game;performed official version;game;official version;official;performed official;version;performed;performing;performing task performed;task performed;performing task;task performed official;results;task;paper present results;paper;paper present", "pdf_keywords": "stochastic convex optimization;convex optimization directional;convex optimization stochastic;stochastic nonsmooth optimization;accelerated randomized directional;randomized directional derivative;smooth stochastic optimization;keywords convex optimization;keywords stochastic convex;stochastic composite optimization;computation gradient free;convex objective randomized;optimization directional derivative;convex optimization;parallel computation gradient;stochastic optimization;nonsmooth optimization;oracle convex stochastic;optimization stochastic composite;optimization strongly convex;gradient free proximal;convex stochastic nonsmooth;optimization stochastic;strongly convex objective;directional derivative norm;stochastic convex;composite optimization directional;computation gradient;gradient free;nonsmooth optimization problems"}, "3e398bad2d8636491a1034cc938a5e024c7aa881": {"ta_keywords": "predicting position pixel;network cnn;pyramid vision;cnn trained;cnn;prediction based convolutional;introduce pyramid vision;image prediction;pyramid vision transformer;convolutional neural;approach image prediction;cnn trained dense;pixel introduce pyramid;neural network cnn;convolutional neural network;dense prediction tasks;image prediction based;convolutional;object detection;visual recognition;network cnn trained;vision transformer;video based visual;including object detection;recognition;based convolutional neural;position pixel dimensional;based convolutional;object detection instance;trained dense partitions", "pdf_keywords": "cnn backbone;alternative cnn backbone;convolutional network;alternative cnn;convolutional network object;segmentation medical images;deep convolutional network;cnn;propose deep convolutional;pyramid vision transformer;deep convolutional;cnn backbone downstream;segmentation medical;pyramid vision;image level prediction;segmentation used dense;medical images;object detection segmentation;dense prediction tasks;serve alternative cnn;prediction pixel;level prediction pixel;termed pyramid vision;vision transformer;prediction pixel level;object detection;convolutional;network object detection;method segmentation medical;dense prediction"}, "115f1366318e7622f89f3a870e5863282670b1ad": {"ta_keywords": "statin therapy significantly;pci statin therapy;intervention pci statin;operative statin therapy;pre operative statin;significantly lower statins;effect statins;lower statins;statin therapy;statins therapy;lower statins therapy;statins post contrast;statin therapy pc;effect statins post;study effect statins;pci statin;post operative statin;statins;inflammatory effect pre;acute kidney injury;contrast acute kidney;operative statin;statins therapy group;statin;creatinine pc aki;statin therapy independent;statin therapy group;coronary intervention pci;elevated creatinine pc;kidney injury", "pdf_keywords": ""}, "d516daff247f7157fccde6649ace91d969cd1973": {"ta_keywords": "supervised machine learning;machine learning;supervised machine;supervised;machine learning report;world supervised machine;machine learning emerging;world supervised;tell world supervised;predict outcome experiment;field machine learning;predict outcome;validity hypothesis;best way predict;results feasibility study;way predict outcome;predict;results feasibility;testing validity hypothesis;way predict;testing validity;learning;learning emerging;feasibility study;hypothesis;international football league;learning report results;method testing validity;international football;testing", "pdf_keywords": "interpretability;interpretable deep neural;interpretable interpretability important;interpretability attempts;interpretability paper;interpretable interpretability;intrinsically interpretable interpretability;interpretability important;interpretability research;interpretability paper discuss;models interpretable;models interpretable deep;interpretability attempts research;interpretable deep;motivations interpretability attempts;motivations interpretability;interpretability research lens;linear models interpretable;methods interpretability paper;intrinsically interpretable;methods interpretability;interpretability ask;interpretable;desiderata interpretability research;examine motivations interpretability;interpretability important posthoc;interpretability ask properties;various desiderata interpretability;desiderata interpretability;examine desiderata interpretability"}, "5ba57ff3c3e6e319586b86a990b6e082f4ecf972": {"ta_keywords": "bayesian speech recognition;bayesian speech;bayesian approach speech;bayesian approaches speech;complexity speech recognition;presents bayesian speech;modeling speech classification;speech recognition;acoustic modeling speech;approach speech recognition;speech recognition framework;modeling speech;speech recognition tasks;approaches speech recognition;speech classification;speech recognition shown;introduces variational bayesian;speech classification paper;variational bayesian;speech recognition paper;variational bayesian approach;robust acoustic modeling;accuracy robustness bayesian;complexity speech;acoustic modeling;robustness bayesian;robustness bayesian approaches;complexity control bayesian;statistics machine learning;robust acoustic", "pdf_keywords": ""}, "e5d143ae82ede67726aa1a9aeac3de4bf53d8920": {"ta_keywords": "vision language pretraining;knowledge based vision;large image corpus;multimodal tasks;downstream visionlanguage tasks;knowledgeaware representations improve;vision language;various multimodal tasks;visionlanguage tasks;image corpus finetuned;based vision language;realworld tasks knowledge;knowledgeaware representations;trained large image;representations various multimodal;multimodal;image corpus;downstream visionlanguage;simulation motion human;knowledge graph embeddings;various multimodal;multimodal tasks experiments;learning cross model;enhance learning semantically;cross model representations;learning semantically;visionlanguage;representations improve models;language pretraining;visionlanguage tasks transformer", "pdf_keywords": ""}, "7f52e3914a61994f68583635e43bc1bb9203e3b3": {"ta_keywords": "genetic toxicity liposome;toxicity liposome acids;toxicity liposome;liposomes group healthy;study genetic toxicity;liposomes;liposome;genetic toxicity;liposomes group;liposome acids liposomes;acids liposomes;fluorophores cofs study;acids liposomes group;liposome acids;fluorophores;fluorophores cofs;chromosomal;controlling operative fluorophores;genetic diseases;genetic;study genetic;fluorophores paper present;health effects tobacco;fluorophores paper;steady state chromosomal;operative fluorophores cofs;rate genetic diseases;chromosomal level altered;chromosomal level;fluorophore nuclei fluorophores", "pdf_keywords": ""}, "60a4ad8e8f4389f317d109550f5da2a571cbb515": {"ta_keywords": "factoidquestion answering searching;answering searching relevant;answering searching;subtasks factoidquestion answering;answered background corpus;natural language query;factoidquestion answering;text answer query;answer large corpus;answer query reading;questions answered;answer query;large corpus text;answering;approach answering;questions answered parsimonious;subtasks factoidquestion;comprehend natural language;retrieved text answer;answer query present;based approach answering;language query extract;natural language;related subtasks factoidquestion;large corpus;answer question;approach answering open;queries need answered;text answer;answering open", "pdf_keywords": "question answering;domain question answering;answering unstructured;question answering paper;evaluation metric answering;answering questions provide;answering unstructured questionnaires;answering questions;unstructured questionnaires answering;metric answering;answer categories;retrieval extracting relevant;tasks answering;tasks answering unstructured;analysis baselines answering;baselines answering questions;answering open;questionnaires answering;answer related tasks;answering;retrieval extracting;sentences documents corpus;answering paper;metric answering open;related tasks answering;documents corpus;baselines answering;research retrieval;facilitate research retrieval;documents corpus given"}, "15513c732d6af975f312307be3b5e2bd674ac0ef": {"ta_keywords": "errors performance human;word error rate;brain potential erp;performance sentence terms;brain processes language;event related brain;automatic speech;asynchrony automatic speech;brain activities impact;analyze brain activities;speech recognition asr;motor activity asr;sentence length human;automatic speech recognition;potential erp studies;brain activities;speech recognition;brain potential;autoregulatory motor activity;new scoring metric;human brain processes;potential erp;scoring metric called;performance human motor;human brain;analyze brain;minimizing word error;directly analyze brain;scoring metric;related brain potential", "pdf_keywords": ""}, "7261b088c48be7eca10263e765739f7347665481": {"ta_keywords": "lennard jones fluid;dynamics markov chain;steady state dynamics;jones fluid presence;dynamics markov;jones fluid;state dynamics markov;steady state;lennard jones;markov chain model;markov chain;state dynamics;fluid presence external;component lennard jones;external magnetic field;lennard;numerical study steady;dynamics;study steady state;fluid presence;presence external magnetic;markov;fluid;single component lennard;external magnetic;component lennard;magnetic field;jones;steady;chain model", "pdf_keywords": "commodity routing game;commodity routing games;routing games stochastic;multi commodity routing;routing game;commodity routing;routing game generalization;routing game \ufb01xed;stochastic load balancing;choose routes cost;routing games;routes cost;games stochastic load;algorithms proposed equilibrium;game \ufb01xed demand;markovian network equilibrium;routing;routes cost perceive;choose routes;routes;network equilibrium;network equilibrium model;model users transportation;load balancing;users transportation network;transportation network;stochastic load;shortest path problem;assumed choose routes;solves shortest path"}, "4fb8009422903f7cb6f9a929409264b7fbca55e3": {"ta_keywords": "noise feature vectors;high speech recognition;feature enhancement adaptive;noise features;feature enhancement;feature vectors best;clean feature vectors;feature enhancement method;corrupted noise features;speech recognition;proposes feature enhancement;make feature enhancement;extended feature vectors;feature vectors;noise features order;noise feature;speech recognition performance;achieve high speech;feature vectors covering;feature vectors subspace;corrupted noise feature;recognition performance variety;frames corrupted noise;feature vectors paper;recognition performance;vectors best predicted;prevent overfitting;extended feature;criterion prevent overfitting;characteristics feature vectors", "pdf_keywords": ""}, "359dfdfea38f645d5fa49efc846a3b5ebce317fe": {"ta_keywords": "interpretable machine learning;learning interpretable;tool learning interpretable;learning interpretable models;machine learning powerful;interpretable machine;field interpretable machine;interpretable models present;interpretable models;machine learning;machine learning mcmc;machine learning aims;tool learning;field machine learning;powerful tool learning;learning powerful tool;interpretable;learning;learning powerful;learning mcmc emerging;paper machine learning;field interpretable;emerging field machine;learning mcmc;theoretical study electronic;models;magnetic;electronic structure dimensional;software agent;models present", "pdf_keywords": "machine learning interpretability;interpretable machine learning;interpretability machine learning;learning interpretability;propose interpretable machine;learning interpretability causal;interpretability machine;interpretability;interpretable machine;approach interpretability machine;paper propose interpretable;communities claim interpretability;interpretable;propose interpretable;interpretability axiomatically;approach interpretability;interpretability axiomatically de\ufb01ning;interpretability causal;word interpretable;machine learning amenability;claim interpretability;hoc approach interpretability;machine learning community;claim interpretability axiomatically;interpretable decide;interpretability causal structure;interpretable decide de\ufb01nitions;machine learning touted;machine learning post;machine learning papers"}, "2c94bc68388517aa4a2d2dfc7d35df95ce24b1a8": {"ta_keywords": "adversarial minimax game;adversarial minimax;formulated adversarial minimax;classifications bias free;learning representations invariant;fair classifications bias;tasks fair classifications;adversarial;classifications bias;learning representation;learning representations;formulated adversarial;representation learning;fair classifications;learning meaningful representations;bias free;benchmark tasks fair;generalization learning meaningful;better generalization learning;learning representation learning;problem learning representations;machine learning representation;generalization learning;minimax game;minimax game analyze;bias free language;bias;process formulated adversarial;game analyze optimal;independent image classification", "pdf_keywords": ""}, "5a0c9bbf0432dac8bd357a4aabf82b83a6c95524": {"ta_keywords": "document similarity measures;rely document similarity;document similarity paper;document similarity;recommendations paper citations;paper citations;document similarity approach;based document similarity;similarity section citation;citation based systems;traditional document similarity;document similarity leads;citations indicate aspect;citations;citation analysis;similarity paper study;aspect based similarity;citation based;similar dissimilar documents;paper citations indicate;similarity paper;dissimilar documents aspect;citing;systems citation analysis;citing cited;citing cited paper;cited paper;documents aspect based;pair citing;similarity measures", "pdf_keywords": "document categorization based;document categorization;approach document categorization;based document similarity;document similarity research;document similarity;document similarity mitigate;semantic similarity propose;similarity propose aspect;semantic similarity;similarity research papers;problem document similarity;categorization based;similarity propose;notion semantic similarity;aspect based document;similarity research;categorization based notion;papers considered similar;categorization;similarity mitigate;similarity mitigate data;similarity;citations;aspect based;aspect based approach;citations ground truth;semantic;citation exists papers;rely citations"}, "4f74be7e5dd4b8e9113e86132cf792da2c32ca3d": {"ta_keywords": "xmath0 type particles;binary mixture xmath0;dynamics binary mixture;mixture xmath0 type;mixture xmath0;external magnetic field;dynamics binary;particles influence external;external magnetic;binary mixture;xmath0 type;particles;magnetic field;influence external magnetic;magnetic;xmath0;type particles;type particles influence;study dynamics binary;particles influence;mixture;dynamics;binary;field;external;paper study dynamics;study dynamics;influence external;type;paper", "pdf_keywords": ""}, "90720bba46dd79bc340359617a7a07fcecc890c1": {"ta_keywords": "differential nash equilibria;nash equilibria equilibria;individual players equilibria;perturbations player costs;nash equilibria;equilibria computable;player costs smooth;players equilibria;differential nash;equilibria computable using;smooth perturbations player;players equilibria structurally;nondegenerate differential nash;equilibria equilibria;functions individual players;costs smooth functions;perturbations player;equilibria;equilibria structurally stable;dense set games;games player costs;player costs;perturbations cost functions;equilibria equilibria open;equilibria structurally;persist perturbations cost;player costs study;costs smooth;cost functions individual;equilibria open dense", "pdf_keywords": ""}, "834fb0d09e764b88ef76ee77e0befb8faeaad7fe": {"ta_keywords": "automatic speech synthesis;speech synthesis based;based speech synthesis;speech synthesis;hmm based speech;based automatic speech;synthesis traditional hmm;automatic speech;modeling acoustic features;generation automatic speech;acoustic features based;speech synthesis unit;acoustic features context;model hmm based;improvements synthetic speech;automatic speech recognition;traditional hmm based;speech recognition ags;speech recognition;speech synthesis traditional;synthetic speech quality;synthetic speech;speech synthesis enables;models hmms hybrid;models hmms;model hmm;harmm based speech;acoustic parameter segments;acoustic features;modeling acoustic", "pdf_keywords": ""}, "2507a6924007efbe0c3116048a85108398f23007": {"ta_keywords": "collect translations;easy collect translations;automatic glossing models;source neural models;collect translations article;encoding linguistic information;translations;encoding linguistic;translational inter london;linguistic information language;linguistic information;source neural;interlinear glossed text;glossed text;translational;interplay inter translational;creating automatic glossing;glossing models;automatic glossing;language documentation projects;multi source neural;channel interlinear glossed;linguistic;glossing;glossing models using;language documentation;format encoding linguistic;information language;glossed text igt;language", "pdf_keywords": ""}, "5ad44a9d6b850405da42f989711af431427425b5": {"ta_keywords": "lingual systems hindi;hindi target language;mixed languages;language listeners prefer;language listeners;code mixed hindi;listening tests bilingual;code mixed languages;languages mixed;mixed hindi english;mixing multiple languages;mixed hindi;language pairs improve;cross lingual systems;tests bilingual speakers;target language listeners;determine languages mixed;bilingual speakers languages;cross lingual approach;target language code;prefer cross lingual;language pairs;languages used conversation;multiple languages;target language;languages rise conversational;tests determine languages;language code mixed;approach language pairs;speakers languages", "pdf_keywords": ""}, "d95b66901d72a13d0c96c7e9bfd4a999ed7fb19c": {"ta_keywords": "neural network language;language models;word splitting language;network language models;phrase based translation;language models shown;splitting language rescoring;splitting language;based translation formalisms;word splitting;language rescoring;risk combination smt;compound word splitting;preordering phrase based;syntactic preordering phrase;based translation;domain adaptation;translation formalisms;string syntactic preordering;syntactic preordering;bayes risk combination;domain adaptation truecasing;perform domain adaptation;forest string syntactic;network language;2015 season systems;language rescoring paper;phrase based;risk combination;string syntactic", "pdf_keywords": ""}, "ad21f0709a3e5d7db91606e2f67926f93e01838d": {"ta_keywords": "minimize contextual regret;games driven contextual;contexts game outcomes;contextual regret;players contextual regrets;contexts game;optimal contextual welfare;players contextual;optimal contextual;different contexts game;repeated games driven;contextual regret individual;game theoretic notions;cces optimal contextual;algorithm exploits contextual;games cces optimal;approached players contextual;repeated games;contextual regrets;game theoretic;game optimize;driven contextual information;traffic routing algorithm;minimize contextual;traffic routing;contextual information;performance traffic routing;optimize performance traffic;contextual information round;present game optimize", "pdf_keywords": "based contextual bandits;contextual bandits;contextual bandits able;contextual routing game;contextual games general;contextual games;players contextual games;contexts game outcomes;contextual games type;outcome contextual routing;minimize contextual regret;class contextual games;predict outcome game;games described contextual;bandits able predict;repeated games;time varying games;repeated games described;varying games;structure repeated games;contexts game;outcome game expectation;game expectation;predicting outcome contextual;contextual regret;different contexts game;routing game;players contextual;type repeated games;contextual routing"}, "f2d5861a24b7aa33036208ba81e11bb9b2090e7c": {"ta_keywords": "paraphrase generation models;multilingual paraphrasing model;multilingual paraphrasing;corpora generate paraphrases;paraphrases final softmax;end multilingual paraphrasing;output paraphrase generation;paraphrasing model trained;paraphrase generation;generate paraphrases final;generate paraphrases;shot paraphrase generation;zero shot paraphrasing;paraphrase generation paper;paraphrasing baselines evaluated;paraphrasing baselines;paraphrasing model;paraphrasing;shot paraphrasing baselines;shot paraphrasing;output paraphrase;zero shot paraphrase;word embeddings;paraphrases final;paraphrases;continuous output paraphrase;layer word embeddings;word embeddings architectural;monolingual rewriting facilitates;shot paraphrase", "pdf_keywords": "paraphrasing generating semantically;model generating paraphrases;generating paraphrases;able generate paraphrases;generating paraphrases unstructured;learning paraphrased;generate paraphrases;learning paraphrased corpora;diversity paraphrasing generating;paraphrase generation;generate paraphrases meaning;increases diversity paraphrasing;paraphrasing generating;machine translation naturally;multilingual machine translation;preservation learning paraphrased;paraphrase generation conduct;generative framework paraphrase;diversity paraphrasing;paraphrases unstructured documents;machine translation;paraphrases unstructured;like machine translation;paraphrasing;framework paraphrase generation;deep generative;annotators \ufb01nd paraphrases;machine translation able;paraphrased corpora;softmax"}, "0e02765103001a792b20242b4dee6dc81917b850": {"ta_keywords": "automatically annotated noisy;gene recognizer;annotated biomedical text;annotated noisy text;automatically annotated;gene recognizer flybase;manually annotated biomedical;manually annotated;gene mentions;general manually annotated;annotated noisy;based annotation;based annotation scheme;annotated biomedical;consistent annotation performance;curation automatically annotated;annotation performance;annotation performance model;names gene mentions;demonstrate gene recognizer;annotation;task based annotation;annotation scheme;gene mentions enabling;gene names;annotated;consistent annotation;distinguishes gene names;supervised;effective fully supervised", "pdf_keywords": ""}, "5b94512a17483595c5dffc503a16ba0b46c347e5": {"ta_keywords": "recognizing hypernymy relationships;method recognizing hypernymy;hypernymy extraction;improve hypernymy extraction;hypernymy extraction article;recognizing hypernymy;wiktionary datasets unsupervised;patterns wiktionary datasets;hearst patterns wiktionary;hypernymy relationships standard;unsupervised sense representations;hypernymy relationships;datasets unsupervised sense;wiktionary datasets;unsupervised sense;sense representations;improve hypernymy;patterns wiktionary;sense representations used;hypernymy;hearst patterns;datasets unsupervised;relationships standard hearst;unsupervised safety;unsupervised;unweighted suspensions;standard hearst patterns;safety unweighted suspensions;suspensions;method unsupervised safety", "pdf_keywords": ""}, "0e52ce6cfd1385e1e9304dcf71d66b53fdc2d4bd": {"ta_keywords": "noise word processing;word processing;noise word;word processing total;news industry;community news readers;community news;science news industry;noise time;news industry undergone;stimulate discussion communities;computer science news;news readers;noise;work community news;sensitivity noise word;word;noise time series;discussion communities;readers likely signal;day paper;problems paper presents;signal noise;stimulate discussion;discussion communities share;problems paper;results workshop sensitivity;news;news readers likely;world problems paper", "pdf_keywords": ""}, "e705255814756178dba75638c29b602095c3cdf4": {"ta_keywords": "transfer learning experiments;transfer learning;transfer learning used;setting transfer learning;source deep reinforcement;deep reinforcement;set transfer learning;learning policy experiments;deep reinforcement learning;sample complexity learning;training transferred layers;complexity learning;complexity learning policy;reinforcement learning;learning;learning policy;transfer simpler;reinforcement learning drl;classification setting transfer;learning experiments;learning useful representations;learning useful representation;learning experiments explore;learn policy;learning useful;complex downstream tasks;training transferred;outperforms training transferred;suggest transfer simpler;tasks requirements learning", "pdf_keywords": "deep reinforcement learning;agents trained atari;trained atari;deep reinforcement;replay transfer learning;reinforcement learning architecture;agents atari games;rainbow agents atari;neural networks reinforcement;trained atari 2600;transfer learning deep;agents atari;learned rainbow agents;atari games;transfer learning experiments;present deep reinforcement;transfer learning avoiding;reinforcement learning double;qlearning prioritized replay;111 transfer learning;transfer learning;reinforcement learning;presents deep reinforcement;learning architecture;transferability features learned;networks reinforcement;learning double qlearning;atari 2600 games;agents trained;networks reinforcement learning"}, "51203e9d5620abdcdf6c9be93b1e221e79cda67d": {"ta_keywords": "superconducting nanowires;nanowire superconductor;superconducting nanowires based;performance superconducting nanowires;nanowire;external language model;nanowire superconductor report;semiconductor nanowire superconductor;nanowires;superconducting;framework transfer learning;transfer learning external;nanowires based hybridization;fusion transfer;nanowires based;superconductor;lm fusion transfer;language model lm;transfer learning;transfer learning work;transfer learning experimental;fusion transfer improves;external language;language model;models transfer learning;learning external text;high performance superconducting;languages using external;low resource languages;shared vocabulary languages", "pdf_keywords": "deep fusion multilingual;fusion multilingual deep;multilingual deep neural;multilingual deep;multilingual machine translation;fusion multilingual;train multilingual models;language model fusion;cross lingual adaptation;train multilingual model;multilingual models new;train multilingual;multilingual models;multilingual machine;multilingual model multilingually;languages train multilingual;adaptation approach multilingual;lingual adaptation approach;cross lingual language;multilingual model;lingual adaptation;approaches train multilingual;translation automatic speech;approach cross lingual;cross lingual;multilingual;approach multilingual machine;lm adaptation language;language independent s2s;adaptation language independent"}, "8319786ed7b9cb13e29130b5617bf0aef586cd6f": {"ta_keywords": "cognitive tutor authoring;create cognitive tutors;tutor authoring;model cognitive tutor;generated tutoring;cognitive tutor tutoring;tutor authoring tools;cognitive tutors;cognitive tutor;tutoring;demonstrated tutoring;model generated tutoring;generated tutoring benefits;tutor tutoring;cognitive tutors heavy;expert model teaching;tutor;tutor tutoring sim;need demonstrated tutoring;tutors heavy programming;tutors;tutoring sim student;called cognitive tutor;create expert;programming enhanced authoring;expert model cognitive;tutoring sim;developed help novice;authors create expert;tutoring decreases learning", "pdf_keywords": ""}, "c37c40db51ccfd6f93004e788102ede72578e5d8": {"ta_keywords": "predicting location person;person social network;method predicting location;social network based;efficient information extraction;predicting location;social network;information extraction;machine learning;machine learning techniques;information extraction filtering;collaborative work algorithm;using machine learning;method predicting;web based;large data sets;location person social;human worker confirm;real time research;efficient information;extracted information propose;new method predicting;learningbased methods;introduce web based;large data;predicting;work large data;filtering situations;worker confirm;filtering situations extreme", "pdf_keywords": ""}, "92259193a9d7377368790bf8517cd9798f30caae": {"ta_keywords": "web navigation based;web navigation;navigation efficient web;web navigation efficient;efficient web navigation;navigation based idea;web navigation presence;navigation based;idea web navigation;neural networks ane;artificial neural networks;navigation;navigation efficient;artificial neural;neural networks;interface web;interface web answer;approach artificial neural;efficient web;friendly interface web;problem web navigation;problem web;approach problem web;navigation presence;web;web answer;navigation presence congestion;networks ane propose;based idea web;networks ane", "pdf_keywords": ""}, "0e1a665334b1ec35d77ab1cd4f21bd0da9745548": {"ta_keywords": "text categorization;text categorization problems;large text categorization;methods text categorization;text categorization presented;sleeping experts phrases;ripper sleeping experts;categorization problems classifiers;ripperand sleeping experts;categorization problems generally;categorization presented;categorization;categorization problems;classifiers;learning algorithms ripperand;classifiers represent contextual;sleeping experts;classifying;problems classifiers;natural language processing;algorithms ripperand sleeping;ripper sleeping;context sensitive learning;experts phrases evaluated;experts phrases;sleeping experts perform;differences ripper sleeping;natural language;context natural language;learning methods text", "pdf_keywords": ""}, "9ca95a09c8bf2d7d28234ff37ece182836dd8632": {"ta_keywords": "imitation learning parser;learning parser learns;parser learns;transition based parsing;learning parser;representation parsing task;parser learns statistical;representation parsing;imitation learning;meaning representation parsing;parsing algorithm abstract;exact imitation learning;imitation learning algorithm;parser;parsing;xmath1 transition;parsing task;xmath0 xmath1 transition;use imitation learning;parsing algorithm;xmath1 transition pole;self propelled particle;based parsing algorithm;propelled particle called;based parsing;perturbation use imitation;propelled particle;model imitating actions;parsing task using;imitating actions", "pdf_keywords": ""}, "0aa0131253b832fdba27ac43f8fa78a322763191": {"ta_keywords": "features input speech;speech source language;target language speech;input speech source;output speech;speech target language;speech output speech;languages paralinguistic information;input speech;input speech output;speech translation technology;speech output;paralinguistic information input;language speech;output speech target;language speech translation;output speech continuous;languages paralinguistic;speech speech translation;translation sensitive paralinguistic;speech continuous space;paralinguistic information;speech translation;speech source;different languages paralinguistic;reflected output speech;target language spoken;reconstructing input acoustic;information input speech;language spoken communication", "pdf_keywords": ""}, "a94ec1cd89839aa5132118916849d46dff861914": {"ta_keywords": "human language communication;situated language communication;human collaboration communication;human language;ground human collaboration;human subjects virtual;communication situated language;collaborate human;collaborative tasks;collaboration communication;human collaboration;language communication situated;language communication;autonomous agents human;situated language;able collaborate human;theory mind tasks;agents human world;collaborative tasks performed;collaborate human terms;build artificial intelligence;mind tasks;subjects virtual blocks;language communication ideal;dataset collaborative tasks;theory mind plays;communication situated;human subjects;autonomous agents;mind tasks paper", "pdf_keywords": "players involved minecraft;collaborative tasks humans;players need collaborate;communication collaborative tasks;collaborative dialogue;involved minecraft project;minecraft project;minecraft game;involved minecraft;introduce minecraft game;collaborative tasks;introduce minecraft;minecraft;interaction discourse visual;collaborate communicate;human collaboration;task hand collaborative;platform collaborative dialogue;need collaborate communicate;collaborate communicate achieve;world minecraft;collaboration building computational;collaborative partner players;paper introduce minecraft;minecraft game pairs;visual experience shared;computational agents engage;interaction discourse;virtual world minecraft;collaborative dialogue understanding"}, "8278e5c2a894793e2c93c6c9f0e7535109e7858f": {"ta_keywords": "dynamics family dineutrifrictions;dineutrifrictions influence external;dineutrifrictions influence;dineutrifrictions;influence external electric;external electric field;electric field;family dineutrifrictions influence;family dineutrifrictions;dynamics;external electric;electric;dynamics family;study dynamics family;comprehensive study dynamics;study dynamics;field;influence external;family;external;influence;present;study;present results comprehensive;paper present;paper present results;present results;comprehensive study;results comprehensive study;comprehensive", "pdf_keywords": ""}, "c637636a2afd7968bdb893af8d2fd220fd39df8f": {"ta_keywords": "monitoring meetings;meeting analyzer;real time meeting;meeting analyzer monitoring;monitoring conversations;monitoring conversations ongoing;time meeting analyzer;captures utterances face;automatically recognize speaking;latency monitoring meetings;group meeting;ongoing group meeting;conversations ongoing group;continuously captures utterances;online manner meeting;captures utterances;analyzer monitoring conversations;meetings;recognize speaking;utterances face pose;meeting assistance continuously;recognize speaking words;meeting table;meeting assistance;camera center meeting;meeting;pose speaker using;conversations;face pose speaker;group meeting paper", "pdf_keywords": ""}, "ffac42087ee4ad50df9203762db715dedd209c0b": {"ta_keywords": "semantic parsing;semantic tags grammars;parsing tags;grammar semantic tags;semantic parsing offer;beneficial parsing tags;tags beneficial parsing;parsing tags spread;tags spread parse;free grammar semantic;method semantic parsing;tag propagation;parsing;information extracted parse;tag propagation rules;beneficial parsing;extract complete semantic;semantic information extracted;grammar semantic;grammars enriched semantic;tags grammars enriched;semantic tags;mechanism tag propagation;tags grammars;parse;enriched semantic tags;semantic tags report;infer semantic structure;use semantic tags;parse tree usually", "pdf_keywords": ""}, "919c929dfa665cb0595a835b4380f96da4cd0143": {"ta_keywords": "sampling spatial fields;sampling spatial;sampling locations;location unawareness sampling;strategies mobile sensing;proposed sampling spatial;sampling strategies mobile;sampling locations approximately;assumes sampling locations;fields mobile sensors;mobile sensing;mobile sensing recently;sampling strategies;spatial fields mobile;proposed sampling;explores multiple sampling;reconstruction field samples;multiple sampling strategies;sampling strategies random;sampling;sensing;sensors record fields;recently proposed sampling;sampling strategies evaluated;mobile sensors;mobile sensors record;field samples;sensing recently proposed;sampling typically;random paths sample", "pdf_keywords": "sampling locations;sampling locations approximately;path based sampling;assumes sampling locations;location unawareness sampling;proposed sampling spatial;sampling spatial \ufb01elds;sampling spatial;sensor proposed sampling;sampling random paths;location reconstruction design;spatial location reconstruction;practical sampling schemes;sampling schemes considered;sampling schemes;sampling schemes presented;location reconstruction;bandlimited \ufb01eld sampling;paths sample reconstruct;sampling strategies;random paths sample;proposed sampling;based sampling strategies;multiple sampling strategies;practical sampling;sampling;sampling strategies random;based sampling classical;\ufb01eld sampling;practical practical sampling"}, "59bdf61a81e46a6c9a96c0c5f96f2f77b82ab09f": {"ta_keywords": "pipeline subsurface detonation;x70 steel pipeline;steel pipeline subsurface;subsurface detonation;steel pipeline;buried x70 steel;pipeline subsurface;behavior buried x70;buried x70;detonation;x70 steel;pipeline;dynamic behavior buried;buried;steel;subsurface;behavior buried;x70;results study dynamic;dynamic behavior;dynamic;study dynamic behavior;report results study;study dynamic;report results;results study;results;report;behavior;study", "pdf_keywords": ""}, "c340b89e7b7fa84fac85cdcf38ba7007e2e71930": {"ta_keywords": "clustering speaker embeddings;recordings speaker diarization;speaker diarization mainly;based clustering speaker;clustering speaker;dialogue recordings speaker;representation speech activity;speaker diarization;dialogue recordings;speech activity dynamics;speaker embeddings;real dialogue recordings;latent representation speech;speaker embeddings source;global speaker characteristics;local speech activity;speech activity;talker recording method;end neural diarization;talker recording;multi talker recording;capture global speaker;speech activity method;short term memory;calls real dialogue;global speaker;recordings speaker;representation speech;speaker characteristics;neural diarization", "pdf_keywords": "speaker diarization systems;speaker diarization based;automatic speaker diarization;embeddings speaker diarization;speaker diarization;based speaker diarization;speaker diarization experimental;speaker diarization vector;speaker diarization method;clustering speaker embeddings;mechanism speaker diarization;end neural diarization;speaker embeddings;attention mechanism speaker;diarization method speakers;based clustering speaker;speakers diarization;speakers diarization important;based neural diarization;embeddings speaker;diarization based self;method speakers diarization;short term memory;clustering speaker;method automatic speaker;self attention convolutional;neural diarization;diarization systems based;conversations callhome dataset;speaker embeddings speaker"}, "0fcdf20477f907aa50578876226f5fabf5e074ea": {"ta_keywords": "scientific citation networks;citation networks;citation networks introduce;paper citation networks;citation networks networks;citation networks generalization;citation networks used;concept citation networks;link prediction methods;world citation networks;systems suggest citations;link prediction;citations bias;relevant citations bias;suggest citations;notion citation networks;suggest citations academic;citations bias exacerbated;scientific citation;citations;citations academic;citations academic papers;truly relevant citations;structure scientific citation;recommender systems;recommender systems suggest;networks generalization;social networks;prediction methods frequently;loops link prediction", "pdf_keywords": "citation networks exposure;bias citation networks;risk link recommender;citation networks;link recommendation propose;link recommender;link recommender systems;bias citation;recommendation propose estimators;link probabilities weight;link recommendation;correct bias citation;networks exposure bias;learned propensities link;suggest new nodes;learned propensities rss;predicting placement articles;risk link;link probabilities;estimators leverage learned;attributes existing links;recommender systems rss;true risk link;recommender systems;bias proposed weighting;articles recommendation paper;leverage learned propensities;propensities rss trained;propensities link probabilities;leverage node attributes"}, "da1296f071bb2b65ae9e0b016d914d24b4edb2d2": {"ta_keywords": "emergence frontier markets;frontier markets model;understand frontier markets;frontier markets succeed;frontier markets;market promote stability;model emergence frontier;emergence frontier;demand financial stability;understand frontier;financial stability;frontier;markets model;financial systems;liquid capital market;global investment;financial systems facilitate;markets model used;markets succeed;global investment encouragedby;capital market promote;markets;markets succeed fail;capital market;used understand frontier;demand financial;stability ability financial;investment;financial stability ability;investment encouragedby presence", "pdf_keywords": ""}, "916c8553beb3ba4e0d20ba6d7eb2bca365d820c8": {"ta_keywords": "home support systems;home home support;home support home;home support;support home home;home based home;home home based;home based;support home;based home home;built home home;home built home;home home home;home home;based home;home built;built home;support systems;home;present home built;present home;paper present home;support;systems;built;based;paper;paper present;present", "pdf_keywords": ""}, "7f613ab03d776f996eb582f04d258a51868dca03": {"ta_keywords": "lipase organic chemistry;lipase catalyzed synthesis;chemistry alkyl benzohydrazides;xmath1 phase transition;synthesis alkyl benzohydrazides;potential lipase catalyzed;alkyl benzohydrazides;lipase catalyzed;lipase organic;benzohydrazides;potential lipase;benzohydrazides classic organic;xmath1 phase;imply potential lipase;alkyl benzohydrazides extend;alkyl benzohydrazides classic;lipase;utilization lipase organic;organic chemistry alkyl;benzohydrazides extend;xmath0 xmath1 phase;chemistry alkyl;synthesis alkyl;utilization lipase;benzohydrazides classic;catalyzed synthesis alkyl;transition dilute bose;xmath1;benzohydrazides extend utilization;extend utilization lipase", "pdf_keywords": ""}, "2aa85084315a4107e2b9b935506b4e9f11428601": {"ta_keywords": "diffuse reflectivity caries;reflectivity caries lesions;reflectivity caries;enamel surfaces exposed;caries lesions drying;emission water vapor;water absorption;spontaneous emission water;changes swir reflectance;enamel surfaces;lesions drying;peak water absorption;bovine enamel surfaces;remineralization occurred monitoring;swir reflectance measurements;changes diffuse reflectivity;water absorption band;lesions drying air;water vapor;surfaces exposed ph;swir reflectance;reflectance measurements;water vapor tungsten;high water absorption;reflectance;water absorption 1450;remineralization solution kinetics;emission water;diffuse reflectivity;determine remineralization occurred", "pdf_keywords": ""}, "542ad79bb96fc10c46086778aaafc8f3509c5c18": {"ta_keywords": "gradient descent ascent;alternating gradient descent;algorithms convex optimization;convex optimization;nonconvex nonconcave objectives;algorithms gradient descent;gradient descent;nonconcave objectives satisfying;nonconcave objectives;convex optimization problems;algorithms convex;inequality alternating gradient;networks adversarial learning;algorithms gradient;adversarial learning;class algorithms convex;nonconvex minimax;ascent agda algorithm;simple algorithms gradient;adversarial learning develop;descent ascent agda;networks adversarial;descent ascent;nonconvex minimax problems;rate nonconvex minimax;emerging machine learning;adversarial;alternating gradient;adversarial networks;adversarial networks adversarial", "pdf_keywords": "adversarial imitation learning;adversarial imitation;imitation learning linear;square imitation learning;consider adversarial imitation;convergence gradient descent;nonconvex stochastic gradient;gradient descent;stochastic gradient methods;learning linear quadratic;imitation learning;robust optimization divergences;linear convergence gradient;stochastic gradient;unconstrained minimization;paper consider adversarial;robust square imitation;convergence gradient;data unconstrained minimization;adversarial;gradient methods distributionally;algorithm square imitation;unconstrained minimization problems;paper generative adversarial;adversarial networks gans;networks gans;gradient descent assuming;adversarial networks;descent assuming convexity;distributionally robust optimization"}, "69ee9b3a915951cc84b74599a3a2699a66d4004f": {"ta_keywords": "semantic spatial pathways;tabletop tasks;precise spatial reasoning;tecture semantic spatial;semantic spatial;specified tabletop tasks;rigid body action;spatial reasoning;tasks packing unseen;tasks;vision based nipulation;learn transferable concepts;unseen objects 14;objects;action gravity;objects 14;quickly learn transferable;learn dexterous skills;representations object poses;multi task;motion;dexterous skills;archi9 tecture semantic;tabletop tasks packing;dexterous skills require;tasks packing;nipulation framework;body action;precise spatial;single task", "pdf_keywords": "robot manipulation tasks;languageconditioned imitation learning;robotic reinforcement;imitation learning agent;robotic reinforcement learning;new robotic reinforcement;manipulation tasks;imitation learning;learning rope manipulation;languageconditioned imitation;learning architecture robot;robot manipulation;vision based manipulation;robotic;vision language;architecture robot manipulation;perception action integrated;vision language model;learning rope;present languageconditioned imitation;understanding affordances actions;affordances actions physical;cliport vision language;manner learning rope;affordances actions;robot;manipulation tasks cliport;trained 10 tasks;new robotic;reinforcement learning architecture"}, "fa3826770207f7bf8bd85a8e97c9ac437f46b061": {"ta_keywords": "microsoft learning rank;learning rank mslr;performance microsoft learning;multiple comparisons baseline;rank mslr procedures;multiple comparisons;learning rank;comparisons baseline;test procedures optimize;results taking comparisons;multiple comparisons ensuring;ir based tests;involve multiple comparisons;rank mslr;performance query;performance query set;optimize performance query;adjust multiple comparisons;performance microsoft;comparisons ensuring;individually significant results;comparisons;procedure testing multiplicity;comparisons baseline paper;taking comparisons;microsoft learning;test collections;comparisons ensuring probability;tests;statistical inference procedures", "pdf_keywords": ""}, "ce89ee7aaeeea2c9d474707690f3ea9d948776a3": {"ta_keywords": "machine translation noisytext;translation noisytext mtnt;translation noisytext;dataset machine translation;noisy inputs translations;machine translation systems;translation systems;machine translation;natural language noisy;mistranslations machine translation;translation systems growing;sourced translations velocity;noisytext mtnt;language noisy nonstandard;noisytext mtnt consisting;noisytext;sourced translations;language noisy;inputs translations;professionally sourced translations;translations;translations velocity;translations velocity particle;inputs translations previous;translations previous;finiteness natural language;translations previous work;noise robust mt;synthetically created datasets;noise robust", "pdf_keywords": "noisy language modeling;noisy language;approaches noisy language;translation domain adaptation;neural machine translation;machine translation studied;en machine translation;training samples language;translation machine;machine translation domain;noise mt utterances;machine translation;translation machine translation;machine translation machine;language modeling lm;language modeling;small monolingual corpora;translators;samples language pairs;mt utterances;frenchenglish;samples language;monolingual corpora;additional small monolingual;adaptation approaches noisy;mt utterances addition;noisy comments;corpora languages provide;collect noisy comments;monolingual corpora languages"}, "7f85b7ee0fc6cdb5b92417035a7049247729545a": {"ta_keywords": "performance random forests;imbalance performance classifiers;random forests study;classifiers benchmark;classifiers benchmark datasets;classifiers increasing;performance classifiers increasing;random forests;classifiers increasing number;classifier classifiers;maximize performance classifiers;optimal classifiers;classifiers;classifier based kappa;classifier rank;classifiers types datasets;performance classifiers benchmark;optimal classifiers types;new classifier classifiers;performance classifiers;class imbalance performance;based classifier rank;classifier;classifier strongly;classifiers types;power classifier strongly;propose new classifier;study performance classifiers;classifiers based classifier;predictive power classifier", "pdf_keywords": ""}, "ce5a57c0ccc8993f4a8e3a07101140a757024d9f": {"ta_keywords": "features public speeches;spoken words memorable;memorable spoken quotes;talks memorable;identify memorable spoken;spoken quotes popular;public speeches;speech public talks;speeches ted talks;nonmemorable spoken quotes;talks memorable inspirational;memorable words spoken;public speeches ted;detecting memorable words;speeches;spoken quotes nonmemorable;spoken speech;spoken quotes paper;ted talks memorable;words spoken speech;speeches ted;natural expressive speech;quotes nonmemorable spoken;spoken quotes;expressive speech public;spoken speech based;words memorable;speech;speech public;inspirational spoken quotes", "pdf_keywords": ""}, "61ad8a0778598022e71c0ee3ba9bc53ddd616517": {"ta_keywords": "task answering sequences;realistic task answering;task answering;question answering;parsing question answering;questions answers matching;answering sequences;question answering focused;robotic;existing robotic;existing robotic robotic;answers matching;coreferences previous questions;question sequences inquire;questions contain coreferences;robotic robotic;questions answers;normal conversation humans;answering sequences simple;robotic systems;question sequences;conversation humans;semantic parsing;semantic parsing question;answers matching words;coreferences;robotic robotic systems;answering focused;explore conversational qos;long complicated questions", "pdf_keywords": "parsing question answering;sequential question answering;question answering;semantic parsing research;question answering context;answer answering systems;semantic parsing;semantic parsing semantic;answering context crowdsourced;parsing semantic;semantic parsing dataset;table semantic parsing;semantic parser incrementally;semantic parsing question;propose semantic parser;semantic parser;identifying coreferences questions;questions answers matching;parsing semantic parsing;answering systems;answer answering;question answering paper;parsing research;answers matching;coreference resolution systems;parsing research emerging;answering context;coreferences previous questions;struggle identifying coreferences;questions contain coreferences"}, "9837207d3f4ee8c493375a97077c6f8b22cadac9": {"ta_keywords": "durational residency tests;durational residency;invalidity durational residency;supreme court decision;statement supreme court;court doctrinal explanations;validity invalidity durational;court decision;court decision making;durational;invalidity durational;supreme court;opinion court doctrinal;court doctrinal;residency tests;residency tests disarray;waiting time person;opinion court;discovery opinion court;residency;court;waiting time;examine relationship waiting;validity;relationship waiting time;debate paper examine;debate;long matter debate;doctrinal explanations;doctrinal explanations validity", "pdf_keywords": ""}, "de9d3a28f9e112a248d097d72ba6ad41a71c8a78": {"ta_keywords": "learning indeterminate clauses;determinate clauses learnable;clauses learnable;clauses learnable primary;learning logic;learning logic programs;learning indeterminate;depth determinate clauses;method learning logic;clause language constant;indeterminates equivalent learning;clause language;logic programs;clauses indeterminates;horn clause language;determinate clauses;logic programs examples;indeterminate clauses;recursive constant depth;clauses indeterminates equivalent;indeterminate clauses indeterminates;language constant depth;learning dnf present;examples recursive;learning dnf;programs examples recursive;problem learning single;equivalent learning dnf;learnable;examples recursive constant", "pdf_keywords": ""}, "bcde1ba141078cf37a69a691fd329d8fd7e70b9b": {"ta_keywords": "criterion active learning;active learning;active learning based;learning based stopping;based stopping criterion;criterion active;stopping criterion;new criterion active;based stopping;learning based;new criterion;learning;criterion;active;present new criterion;stopping;based;new;present;present new", "pdf_keywords": ""}, "83d6b4bfa8701578c291e55f5f1e5e6508aff313": {"ta_keywords": "medical ethics technology;ethics technology ethics;technology ethics;ethics technology;medical ethics;ethics consider computer;medical ethics consider;technology ethics offer;concepts medical ethics;consent decisional capacity;informed consent decisional;consent decisional;difference medical ethics;capacity informed consent;concepts informed consent;ethics consider;ethics;consent discussion technology;informed consent;patients autonomy messy;informed consent discussion;technology design;patients autonomy;consent;consent discussion;thinking concretely technology;honor patients autonomy;case study heart;technology;fictional case study", "pdf_keywords": ""}, "1bc87dba9838b3028b636f456084252f2beac108": {"ta_keywords": "non cooperative game;cooperative game;game non cooperative;cooperative game non;particle swarm optimization;bilevel optimization problem;particle swarm;swarm optimization;cooperative game agents;using particle swarm;leader non cooperative;bilevel optimization;lottery equilibrium game;social game probability;play non cooperative;analysis social game;utility leader non;resulting bilevel optimization;swarm optimization method;stackelberg game;utility leader;equilibrium game;stackelberg game multiple;cooperative;non cooperative;greater utility leader;reversed stackelberg game;leader choice behavior;game probability;lottery equilibrium", "pdf_keywords": ""}, "f016ac107259d6d222c9f52b37208fca4fa1d6bc": {"ta_keywords": "driving systems adss;modeling ads scenarios;use case modeling;casemodeling rucmm develop;used modeling ads;applicability rheological support;case modeling used;restricted use casemodeling;use casemodeling;use casemodeling rucmm;driving systems;evaluate applicability rheological;modeling ads;case modeling;casemodeling;used modeling;casemodeling rucmm;applicability rheological;modeling used;ads scenarios conducted;modeling used modeling;modeling;specifying ads scenarios;case modeling methodology;ads scenarios potential;autonomous driving systems;ads scenarios;adss class driving;autonomous driving;editor autonomous driving", "pdf_keywords": ""}, "36c770b79937db2e3416204b8cf177d0c9881f54": {"ta_keywords": "fluid dynamics video;motion rigid;gravity manipulated;motion rigid body;manipulated changing orientation;fluid dynamics;gravity manipulated changing;video motion rigid;dynamics video;changing orientation body;rigid body influence;rigid body;dynamics video motion;influence gravity manipulated;changing orientation;orientation body;dynamics;body influence gravity;motion;orientation;gravity;influence gravity;video motion;rigid;manipulated changing;fluid;manipulated;video;body influence;body", "pdf_keywords": ""}, "0a485fd94b2cb554e281d0f8d7e9f71db4891ce0": {"ta_keywords": "token pooling vision;propose token downsampling;tokens minimizing reconstruction;softmax attention;token downsampling;token downsampling method;attention vision transformers;bottleneck token pooling;token representations mild;pooling vision transformers;assumptions softmax attention;token pooling efficiently;softmax;token representations;images intermediate token;complexity attention vision;bottleneck token;pooling vision;computation bottleneck token;attention vision;tokens minimizing;vision transformers emerging;softmax attention acts;mild assumptions softmax;vision transformers limit;token pooling;attention major computation;intermediate token representations;layers propose token;token pooling simple", "pdf_keywords": "answer question recognition;question recognition;propose deep convolutional;deep convolutional neural;deep convolutional;token downsampling feature;segmentation based token;token segmentation;token pooling;introduce token pooling;token segmentation based;token pooling transformers;proposed token pooling;question recognition algorithms;convolutional neural;token pooling method;novel token downsampling;attention vision transformers;token downsampling;token downsampling methods;based token downsampling;token pooling new;convolutional neural network;token downsampling technique;technique token pooling;token;method token segmentation;prior token downsampling;attention vision;recognition algorithms trained"}, "d1f32060e921b6e06badd7fdb2b750638b2d131c": {"ta_keywords": "deep beamforming networks;network deep beamforming;deep beamforming;field speech recognition;learning based acoustic;feature extraction acoustic;channel speech recognition;acoustic processing;deep neural networks;far field speech;progress speech recognition;beamforming networks;acoustic processing including;speech recognition;beamforming feature extraction;acoustic modeling;acoustic modeling components;stages acoustic processing;acoustic acoustic modeling;extraction acoustic modeling;reverberation captured speech;acoustic modeling despite;beamforming networks dbs;network deep;multi channel speech;speech recognition remains;deep neural;beamforming feature;field speech;processing including beamforming", "pdf_keywords": ""}, "98290fb02a844108df202e9a6dc3461e3f14ee32": {"ta_keywords": "nonconvex nonconcave min;nonconcave min;nonconcave min max;stationary points optimization;nonconvex nonconcave;stationary point surrogate;nonconvex nonconcave motility;approximation result nonconvex;problem nonconvex nonconcave;surrogate problem nonconvex;point surrogate problem;result nonconvex nonconcave;maxy sets convex;bounds nearly optimal;nonconcave;points optimization problems;convex compact;points optimization;optimization problems;min max problems;optimization;approximate order stationary;order stationary points;optimization problems form;convex compact upper;sets convex compact;minx maxy sets;optimal;provide general approximation;nonconcave motility", "pdf_keywords": "minimization weakly convex;concave optimization regularized;bilinearlycoupled objectives convex;optimization regularized stochastic;based minimization weakly;concave optimization;minimization weakly;objectives convex;nonconvex concave optimization;min max optimization;objectives convex concave;optimization regularized;convex concave min;minimization;max optimization remains;max optimization;optimization remains;weakly convex;regularized stochastic learning;based minimization;weakly convex hints;weakly convex functions;model based minimization;tight bilinearlycoupled objectives;regularized stochastic;optimization remains active;functions weakly convex;optimization;learning gradient descent;generalized monotone operators"}, "ad26e5105b6019ff68404962e39ea3a1dfb5931d": {"ta_keywords": "optimal behavior incentives;optimal sequence incentives;agent optimal behavior;incentive sequence minimizes;synthesize incentive sequence;algorithm synthesize incentive;desired behavior incentives;markov decision process;sequence incentives principal;behavior markov decision;policy deterministic transitions;optimal trajectory agent;agent optimal;incentive sequence;sequence incentives;markov decision;agent agent optimal;agent providing incentives;policy deterministic;providing incentives certain;incentives realizes principal;process finite decision;behavior incentives;optimal behavior;behavior markov;underlying policy deterministic;incentives certain;incentives principal;motion planning;behavior incentives realizes", "pdf_keywords": "optimal incentive design;optimal incentive;feasible incentive design;incentive design require;incentive design minimizes;thesize optimal incentive;propose incentive design;incentive design;incentive design exist;incentive design method;feasible incentive;design incentive sequence;propose incentive;principal optimal policy;scenario feasible incentive;optimal policy;design incentive design;consider motion planning;optimal policy reached;motion planning;incentive sequence;motion planning problem;desired agent behavior;paper propose incentive;sequence incentives providing;design incentive;incentive sequence principal;behavior sequence incentives;incentives providing example;agent induce desired"}, "89a8edbc0fe2ea8b9ee703ca37e5d5d6d34c571a": {"ta_keywords": "backward mutual information;forward mutual information;letter entanglement entropy;information forward mutual;mutual information forward;bidirectional model forward;entanglement entropy bitext;sequence level knowledge;entropy bitext;nash turing machine;entropy bitext expressed;forward backward mutual;entanglement entropy;model forward backward;based nash turing;nash turing;model letter entanglement;text based nash;propose bidirectional model;turing machine ntm;bidirectional model;backward mutual;idea mutual information;terms entropic entropies;turing machine;entropy;forward mutual;mutual information;measure mutual information;information forward", "pdf_keywords": "machine translation paraphrasing;paraphrased source transcriptions;predict paraphrased transcriptions;paraphrased transcriptions;trained predict paraphrased;translation machine translation;automatic machine translation;machine translation;end speech translation;paraphrased transcriptions single;speech translation e2e;training learning paraphrastic;sentence embeddings translated;translation paraphrasing;paraphrastic sentence embeddings;speech translation;automatic translation;translation machine;automatic translation machine;machine translation active;better automatic translation;learning paraphrastic;learning paraphrastic sentence;targets paraphrased source;predict paraphrased;source transcriptions;paraphrasing multi referenced;seqdin targets paraphrased;translation paraphrasing multi;speech translation order"}, "daa7e6af585d03e9cb05487413a6495f23400398": {"ta_keywords": "feasible assignment solution;network assignment problems;assignment problems wireless;finding feasible assignment;binary assignment problems;assignment solution;network assignment;feasible assignment;assignment solution given;applied network assignment;approach binary assignment;binary assignment;assignment problems;assignment problems letter;networks identifies binary;sinkhorn neural network;unsupervised training algorithm;learning approach binary;training algorithm;binary variables permutation;deep learning;variables permutation matrices;wireless networks identifies;training algorithm proposed;problems wireless networks;assignment;permutation matrices;wireless networks;deep learning approach;permutation matrices numerical", "pdf_keywords": "feasible assignment solution;assignment problems wireless;unbalanced assignment problems;assignment solutions;feasible assignment;convex assignment problems;extended unbalanced assignment;assignment solution;assignment problems proposed;weighted assignment problem;network assignment problems;assignment solutions arbitrary;identi\ufb01es feasible assignment;unbalanced assignment;network assignment;applied network assignment;assignment problem heterogeneous;weighted assignment;assignment problems paper;generates assignment solutions;non convex assignment;solve weighted assignment;binary assignment problems;approach binary assignment;convex assignment;unsupervised training algorithm;binary assignment;assignment problems;assignment problem;assignment solution output"}, "f11ed27f4640dd8785ea7c4aff9705ddaad2b24d": {"ta_keywords": "multimedia fake news;fake news detection;visual content fake;multimedia fake;content fake;introduction multimedia fake;content fake news;issues multimedia fake;content detection;contents fake news;multimedia technology fake;technology fake news;visual content detection;fake news attempts;visual contents fake;news detection;news detection social;news detection chapter;news detection development;contents fake;detection development multimedia;news detection limited;detection social media;content detection understanding;proliferation fake news;videos attract mislead;multimedia content;news attempts utilize;images videos attract;news attempts", "pdf_keywords": ""}, "ad4b09832454a821e925e45e96e769f0c01bd3d6": {"ta_keywords": "sparse word graphs;statistical topic models;topic models;sparse word;topic models latent;word graphs;documents topics based;topics statistical;called sparse word;models latent dirichlet;topic semantically;models documents topics;documents topics;topic semantically meaningful;word graphs spgs;topics statistical topic;words context statistical;correlations words context;topics based;latent dirichlet;sparse manner paper;emergence correlations words;lations topics statistical;probabilistic models documents;correlations words;lations topics;latent dirichlet al;corre lations topics;summarize large document;based sparse", "pdf_keywords": ""}, "1f133158a8973fb33fea188f20517cd7e69bfe7f": {"ta_keywords": "text classification tasks;attention sublayer transformer;encoder architectures massively;transformer encoder architectures;attention sublayer;encoder architectures;sequence length model;best sequence lengths;encoder;self attention sublayer;efficiently long inputs;classification tasks;text classification;classification tasks replacing;sublayer transformer encoder;longer sequences tpus;relationships text classification;model best sequence;accuracy bert;longer sequences;sequence length;tpus transformer encoder;length model fnet;transformer encoder;long inputs;feedforward layers sufficient;new sequence length;sequences tpus transformer;92 accuracy bert;feedforward", "pdf_keywords": "attention patterns driver;deep convolutionally neural;train deep convolutionally;convolutionally neural;convolutionally neural network;machine translation architecture;convolutional neural;fft based convolutional;convolutional neural networks;attention patterns;predicting self attention;deep convolutionally;machine translation;new machine translation;self attention patterns;translation architecture;attention;neural networks;based convolutional neural;convolutional;neural;model diverse semantic;neural network predicting;neural network;diverse semantic relationships;fftw3;based convolutional;convolutionally;transformers;fftw3 simple"}, "c9ce3889c03fee2990b2277423bbc0fb4366df53": {"ta_keywords": "word recognition;automatic word recognition;discriminative language modeling;word recognition log;language modeling;classification language;model automatic word;novel language model;language model automatic;language model;language modeling structured;automatic word;problem classification language;recognition log linear;discriminative language;modeling structured classification;classification language book;abstract discriminative language;structured classification;recognition log;feature representation;problem classification;structured classification problem;feature representation used;classification;neural network;parameterized neural network;product feature representation;recognition;parameterized neural", "pdf_keywords": ""}, "2de8019fd7d04e3d1305d5efaeeb591f0d966550": {"ta_keywords": "speech recognition recently;short term memory;speech recognition;new decoding strategy;term memory networks;memory networks;speech recognition propose;new decoding;different decoding strategies;large margin speech;automatic speech;predict missing tokens;autoregressive transformer 7x;learns predict missing;automatic speech recognition;decoding;decoding strategies;margin speech recognition;learns predict;deep convolutional neural;autoregressive transformer;structures automatic speech;recently deep convolutional;non autoregressive transformer;different decoding;convolutional neural networks;deep convolutional;decoding strategies including;neural network learns;recognition recently deep", "pdf_keywords": ""}, "ca2144b895cf6812eec535261df9294896417425": {"ta_keywords": "relation extraction classification;semantic relation extraction;relation extraction;relation extraction model;classification subtask;classification subtask subtask;end relation extraction;subtask paper describes;assisted classification subtask;subtask subtask paper;subtask subtask;semantic relation;subtask subtask subtask;subtask;extraction classification scientific;subtask paper;recent work semantic;work semantic relation;extraction classification;classification scientific papers;concept candidate embeddings;encoding attention;work semantic;level encoding attention;relation;candidate embeddings;semantic;classification scientific;embeddings;end relation", "pdf_keywords": "semantic relation extraction;relation extraction emerging;relation extraction;domain relation extraction;relation extraction based;relation extraction model;extract semantic information;extract semantic;extract information scholarly;information extraction;embeddings scienti\ufb01c semantic;end relation extraction;articles word embeddings;scienti\ufb01c semantic relation;large scholarly corpora;word embeddings learned;word embeddings;models information extraction;training large scholarly;information extraction proposing;large scholarly dataset;method extract semantic;semantic relation;publications extract subset;scholarly dataset scienti\ufb01c;scholarly corpora;articles scienti\ufb01c automatic;publications extract;unlabeled scienti\ufb01c articles;researchers search extract"}, "3638e5dfc79ba3fb757900f46ac0c7e7f6dadb05": {"ta_keywords": "people use photos;technology photographic practices;context aware cameraphone;cameraphone mobile media;image sharing;image capture sharing;photographic practices motion;technology photographic;photographic activity designing;photographic activity;emerging technology photographic;prior photographic activity;mobile media sharing;cameraphone use;cameraphone use image;aware cameraphone mobile;aware cameraphone;media sharing;use image sharing;cameraphone;photographic practices;photographic;mobile media;cameraphone mobile;use photos;prior photographic;image sharing users;capture sharing;imaging innovative communicative;networked digital imaging", "pdf_keywords": ""}, "86eb740bbc54a6d734242be28fccf76fd4d2c1ba": {"ta_keywords": "learning algorithms noncooperative;convergence guarantees gradient;noncooperative multi agent;multi agent;time convergence equilibrium;convergence equilibrium;equilibrium game;finite time convergence;guarantees gradient based;convergence equilibrium equilibrium;guarantees gradient;equilibrium equilibrium game;convergence guarantees;derive convergence guarantees;dynamics;learning algorithms;multi agent settings;equilibrium;gradient based learning;algorithms noncooperative multi;algorithms noncooperative;finite time;fluid dynamics;convergence guarantee;motion animals crowded;convergence guarantee derive;gradient based;learning;fluid dynamics video;prove finite time", "pdf_keywords": ""}, "0fdc3efc11526995d192f18e19f07fba062a76f7": {"ta_keywords": "labeling training;weak supervision pws;workflow labeling training;labeling training data;programmatic weak supervision;component pws learning;noisy supervision sources;supervision pws achieved;synthesizing training labels;learning workflow labeling;supervision pws;pws learning;lhc;labeled;introduction pws learning;potentially noisy supervision;training labels;like events superconducting;events superconducting;weak supervision;lhc programmatic weak;noisy supervision;pws learning paradigm;pws learning workflow;labeled data;superconducting phase lhc;labeled data scenarios;limited labeled;labeling bottleneck synthesizing;supervision sources", "pdf_keywords": "weakly supervised entity;weakly supervised;supervised entity recognition;learning weak supervision;supervised entity;entity recognition;paradigm weakly supervised;entity recognition based;training data annotation;label models;weak supervision addition;annotation practitioners resorted;data annotation;representation learning weak;label models tailored;labeled data;annotation practitioners;supervised;limited labeled data;entity;labeled;review label models;weak supervision;data annotation practitioners;annotation;deep representation;limited labeled;learning weak;models tailored crowdsourcing;recognition based deep"}, "66340a93813d8f816a8c82354a8f39fa985de27f": {"ta_keywords": "textual entailment trained;knowledge textual entailment;background knowledge textual;approach retrieval;novel approach retrieval;entailment trained sciail;approach retrieval information;textual entailment;knowledge textual;retrieval information;generic textual entailment;retrieval information open;textual entailment able;entailment able outperform;retrieval;entailment trained;challenge dataset contains;textual;information large corpus;retrieved evidence answer;generic textual;retrieved evidence;evidence answer candidates;large corpus science;challenge dataset;large corpus;reformulation background knowledge;answer candidates select;support retrieved results;entailment", "pdf_keywords": ""}, "2ac6b8ade2a5e1ac89b99012ca6548eca4f8323f": {"ta_keywords": "topological layer training;learn topological structure;learn topological;approach learn topological;topological features input;topological layer;novel topological layer;underlying topological features;exploit underlying topological;structure topological layer;topological layer general;underlying topological;topological features;topological structure;topological structure topological;layer general deep;novel topological;structure topological;propose novel topological;topological;information topological features;layers improve learnability;layer training neural;layer training;deep learning;networks;deep learning models;general deep learning;improve learnability networks;learnability networks", "pdf_keywords": "learning based topological;based topological data;topological data;topological data present;topological data analysis;data topological information;data topological;landscapes computing persistence;topological information underlying;relies topological tools;property topological data;topological information;complex data topological;topological tools;relies topological;extract topological information;persistence landscapes computing;weighted persistence landscapes;topological tools infer;science relies topological;based topological;computing persistence landscape;stability property topological;topological;persistence landscapes;topological information weighted;computing persistence;extract topological;method extract topological;persistence landscape"}, "1fa02e5a5adffe82a41225f61f5f8ce86cf229d0": {"ta_keywords": "multilabel combinatorial optimization;label mrf segmentation;edge alignment spectral;alignment spectral relaxation;mrf segmentation;concept edge alignment;edge alignment based;efficient multilabel combinatorial;edge alignment;mrf segmentation benefit;spectral relaxation combinatorial;combinatorial optimization;multilabel combinatorial;multi label mrf;segmentation;spectral cut kernel;efficient multilabel;problem edge alignment;alignment spectral;normalized abnormalized cut;propose efficient multilabel;spectral cut;relaxation combinatorial max;segmentation clustering model;new segmentation;optimization techniques spectral;combinatorial max flow;segmentation benefit high;combinatorial optimization techniques;segmentation benefit", "pdf_keywords": ""}, "1cfd9b1db68fc320698da05fc6876dd0ea96fc9b": {"ta_keywords": "connectionist temporal classification;temporal classification ctc;speech recognition;trained model depth;ctc stochastic depth;speech recognition asr;stochastic depth train;depth present training;automatic speech recognition;temporal classification;recognition asr model;based connectionist temporal;dynamics layer pruned;model depth run;layer pruned langevin;recognition asr;automatic speech;strategies finding layers;trained model;connectionist temporal;classification ctc allows;stochastic depth;ctc model pruned;training pruning;pruning fluid dynamics;classification ctc;improving real time;finding layers;individually trained model;depth analysis layer", "pdf_keywords": "training deep;speech recognition;training deep neural;improve training convolutional;training convolutional neural;speech recognition asr;deep residual network;training convolutional;layer pruning;consider layer pruning;residual network regularization;layer pruning based;deep neural network;problem training deep;layer pruning important;deep neural;devices layer pruning;novel deep residual;automatic speech recognition;asr based connectionist;layer pruning problem;training pruning;deep residual;stochastic depth regularization;method layer pruning;training pruning method;networks present training;reducing depth network;network regularization;depth regularization"}, "acbb4495dd698b3190db6899d7d35b0817e0a85e": {"ta_keywords": "generalization learned minimax;gradient descent ascent;gradient based minimax;learned minimax;learned minimax models;standard gradient descent;minimax learners;gradient descent;minimax model generalization;trained minimax;minimax learners used;performance trained minimax;based minimax learners;algorithm based minimax;minimax algorithm;trained minimax model;based minimax algorithm;minimax algorithm paper;non concave minimax;generalization performance stochastic;minimization maximization;minimization maximization subproblems;generalization performance trained;concave minimax;minimax models;based minimax;suggests minimization maximization;descent ascent;maximization subproblem iteration;ascent gda proximal", "pdf_keywords": "convex gradient descent;descent algorithms convex;convergence gradient descent;minimax learner optimal;gradient descent algorithms;generalization performance gradient;gradient descent;convex concave minimax;gradient descent problems;algorithms convex concave;gradient based descent;known minimax optimization;non convex gradient;guarantees convergence gradient;minimax learner;convex gradient;descent problems generalization;generalization error bounds;minimax optimization algorithms;algorithms convex;generalization generative adversarial;concave minimax;optimization algorithms learn;di\ufb00erent minimax optimization;nonsimultaneous optimization;minimax optimization literature;minimax optimization;adversarial training recently;best minimax learner;descent descent algorithms"}, "0053f75b7053f43b9787a9955426281e672b147b": {"ta_keywords": "outside recursive autoencoder;recursive autoencoder fully;syntax simultaneously learns;recursive autoencoder;autoencoder fully unsupervised;trees sentence inference;discovering syntax simultaneously;discovering syntax;unsupervised learning;unsupervised learning introduce;learns representations;autoencoder fully;learns representations constituents;autoencoder;framework unsupervised learning;simultaneously learns representations;learning introduce deep;predicts word input;scoring parse;probabilistic framework unsupervised;parse;fully unsupervised;learns;highest scoring parse;sentence inference;unsupervised method discovering;input sentence conditioned;simultaneously learns;binary trees sentence;fully unsupervised method", "pdf_keywords": "unsupervised parser training;unsupervised parsing sentences;unsupervised parsing;model unsupervised parsing;unsupervised parser;unsupervised constituency parsing;unsupervised parsing used;2000 unsupervised parsing;unsupervised parsing segment;parser training;parser training extract;outside recursive autoencoders;recursive autoencoders diora;constituency parsing;shallow parses;parsing sentences;extract shallow parses;recall phrase representations;parsing segment recall;parses model trained;method unsupervised parser;shallow parses model;recursive autoencoders;parsing sentences vote;parsing;parsing used learn;parses model;phrase representations;parses;learn syntactic structure"}, "95e8edd26744ecc2bc23996cfaa68fe6252442a9": {"ta_keywords": "fake news detection;automatic fake news;evidence based fake;veracity news claim;semantic information claim;news detection evidences;grained semantic representations;prediction semantic representations;representations automatic fake;news claim paper;graph based semantic;prediction semantic;news claim;grained semantic;veracity news;information claim evidence;probe veracity news;semantic representations;semantic representations automatic;semantic sructure mining;detection evidences;capture claim evidence;claim evidence interaction;treats claims evidences;semantic representations context;news detection critical;claims evidences;claims evidences sequences;claim evidence;based semantic sructure", "pdf_keywords": "claims evidences graph;evidences graph structured;veracity prediction;veracity claims;veracity claims claim;fake news detection;veracity prediction paper;probe veracity claims;evidences graph;evidence based fake;claim evidence interactions;evidence level representations;news detection evidences;verification claims evidences;final veracity prediction;information final veracity;veracity;model claims evidences;semantics long distance;contexts graph;claims evidences;claim evidence level;detection evidences;semantic dependency dispersed;attributes contexts graph;claim evidence;distance semantic dependency;grained semantics;semantic encoders verification;grained semantics long"}, "452059171226626718eb677358836328f884298e": {"ta_keywords": "predicting sentiment crowdsourced;predicting sentiment;sentiment crowdsourced;question answering;crowdsourced sentiment;crowdsourced sentiment analysis;sentiment crowdsourced crowdsourced;question answering question;crowdsourced crowdsourced sentiment;recurrent sequence model;hierarchical recurrent;hierarchical recurrent sequence;reasoned hierarchical recurrent;recurrent sequence;framework predicting sentiment;generate answers;answering question complexity;model generate answers;generates relevant answers;recurrent;natural language;sentiment analysis introduce;natural language processing;neural;answering question;cast question answering;sentiment analysis;sentiment;sequence model generate;crowdsourced", "pdf_keywords": "question answering;tasks recurrent neural;recurrent network;recurrent network updates;recurrent neural network;episodic memory networks;processing tasks recurrent;recurrent neural;datasets question answering;tasks recurrent;translation question answering;question answering paper;question answering facebook;sequence modeling speech;episodic memory module;memory networks;natural language related;generated conversation;computer generated conversation;network dmn recurrent;deep convolutional neural;attention mechanism recurrent;modeling speech tagging;memory networks predict;machine translation;natural language processing;presents deep convolutional;deep convolutional;memory episodic;memory episodic memory"}, "c39ac49e2d3feec992e84868256cb0a0ff028346": {"ta_keywords": "distributed convex optimization;decentralized distributed convex;distributed convex;convex optimization;weighted sum convex;convex optimization approach;preprints decentralized distributed;sum convex functions;minimizing weighted sum;decentralized distributed;sum convex;present decentralized distributed;convex optimization significant;problem minimizing weighted;minimizing weighted;convex functions;distributed;convex;arxiv preprints decentralized;preprints decentralized;optimization;weighted sum;optimization approach;minimizing;problem minimizing;approach problem minimizing;optimization approach problem;decentralized;optimization significant progress;present decentralized", "pdf_keywords": "distributed convex optimization;decentralized gradient methods;decentralized gradient;stochastic gradient descent;distributed distributed convex;gradient descent convex;weighted gradient descent;present decentralized gradient;constraints stochastic gradient;descent weighted gradient;distributed convex;optimization gradient descent;descent convex composite;gradient descent weighted;accelerated gradient descent;stochastic optimization problems;convex composite optimization;stochastic optimization;convex optimization gradient;solving stochastic optimization;gradient descent;descent convex;problems stochastic gradient;descent method gradient;gradient descent method;unbalanced directed graphs;convex optimization;stochastic gradient;gradient descent oracle;complexity accelerated gradient"}, "affb8d759af00540458c19696532220dd1c1373a": {"ta_keywords": "continuous speech recognition;speech recognition;vocabulary continuous speech;speech recognition lvcsr;glick lipkin model;learning languages spoken;hidden markov models;lipkin model;hidden markov;movement lipkin meshkov;meshkov glick lipkin;movement lipkin;spoken humans paper;deep neural network;continuous speech;lipkin meshkov glick;language learning;learning languages;tail movement lipkin;glick lipkin;lipkin model report;large vocabulary continuous;network hidden markov;language learning languages;recognition lvcsr tasks;lipkin;hybrid deep neural;lipkin meshkov;datasets hybrid deep;markov models achieved", "pdf_keywords": ""}, "5c5bedaf66cadebbcd9116f38acd3df9ed43d816": {"ta_keywords": "wavelets;used wavelet functions;wavelet functions;morlet wavelets;wavelet;wavelet functions flexibility;ricker morlet wavelets;hydrocarbon detection;hydrocarbon detection using;morlet wavelets paper;direct hydrocarbon detection;commonly used wavelet;wavelets paper;used wavelet;wavelets paper present;detection using eopwt;seismic trace analysis;seismic trace;time frequency localization;measure gas accumulation;spectral decomposition using;implement spectral decomposition;measure gas;analysis shows eopwt;reservoir characterization;signals implement spectral;profile gas reservoir;generalized waveform alternative;generalized waveform;data shown waveform", "pdf_keywords": ""}, "bc247abf8180f583a42de392e4f7d2b2a41ad72d": {"ta_keywords": "parser independent interactive;parsers text sql;state art parsers;enhancing text sql;parsers;arbitrary parsers text;parsers text;text sql technique;parser;parser independent;work arbitrary parsers;arbitrary parsers;text sql performance;datasets wikisql complex;text sql;questions easily work;parsers paper;datasets wikisql;natural language questions;parsers paper human;wikisql complex;independent interactive approach;domain datasets wikisql;wikisql;using natural language;parsers fall short;questions easily;interactive approach;language questions experiments;natural language", "pdf_keywords": "queries propose parser;text sql parsers;sql parsers relieve;sql parsers;natural language queries;query generation;query language;interactively revise queries;structured query language;question generator;parser independent interactive;learning techniques queries;query generation paper;semantic parsing;sql query generation;parsers relieve users;deep semantic parsing;enhance text sql;language queries;semantic parsing based;language sql text;approach text sql;query language sql;converting natural language;questions natural language;language sql;question generator designs;revise queries;arbitrary parsers;parsers"}, "c5bcb690b0aa85ad0a5fd7e7aa4b8c468cd8c69a": {"ta_keywords": "discriminative training based;discriminative training;approach discriminative training;language model recognition;learning based discrete;recognition group words;word language model;machine learning;general approach discriminative;language model;mutual information mmi;error weighted objective;predicting weight item;discriminative;approach machine learning;weighted objective function;training based integrals;markov chain monte;algorithm predicting weight;weighted objective;mutual information;maximum mutual information;error weighted;machine learning based;discrete time markov;performance word language;expressed margin based;approach discriminative;algorithm predicting;model recognition", "pdf_keywords": ""}, "f300a62d0522d9a623b62f1305052928d8d7170c": {"ta_keywords": "irony detection methods;dataset irony detection;structures irony detection;irony detection paper;irony detection;class irony detection;irony detection important;dataset irony;emojis social media;cues emojis social;balanced dataset irony;nonverbal cues emojis;cues emojis;structures irony;methods insensitive emojis;emojis social;insensitive emojis;new class irony;class irony;emojis;role structures irony;irony;identification online abuse;online abuse harassment;online abuse;social media work;ubiquitous use nonverbal;abuse harassment article;social media;abuse harassment", "pdf_keywords": ""}, "50851e9e16b52e14c422b6e937cfd3ed063b6998": {"ta_keywords": "learning multilingual encoders;cross lingual encoders;multilingual bidirectional encoders;multilingual encoders;lingual transfer learning;multilingual encoders pre;trained multilingual data;lingual encoders;learning multilingual;trained multilingual;trained cross lingual;algorithm trained multilingual;lingual encoders mbert;multilingual bidirectional;languages multilingual bidirectional;cross lingual;multilingual data;languages multilingual;method learning multilingual;multilingual;cross lingual transfer;contextual embeddings words;amber contextual embeddings;lingual;contextual embeddings;embeddings words andsentences;lingual transfer;different languages multilingual;multilingual data paper;shot cross lingual", "pdf_keywords": "embeddings crosslingual transfer;neural machine translation;translation model trained;sentence embeddings crosslingual;lingual transfer learning;machine translation model;multilingual sentence embeddings;cross lingual transferability;embeddings crosslingual;machine translation;challenge machine translation;cross lingual tasks;translation model;lingual transferability;cross lingual transfer;crosslingual transfer;lingual word embeddings;learn multilingual sentence;lingual tasks sequence;lingual transferability proven;lingual transfer;machine translation paper;crosslingual transfer long;cross lingual;approach cross lingual;lingual tasks;cross lingual word;embeddings leverages attention;zeroshot cross lingual;labeling question answering"}, "37e06f3622c17dc6194b547c944462b2a513b878": {"ta_keywords": "consistency generated summaries;neural abstractive summarization;generated abstractive summarization;generated abstractive summaries;abstractive summarization models;summaries generated abstractive;generated summaries;abstractive summarization systems;summarization models experiments;generated summaries span;structure summaries generated;summarization models;corrections generated summaries;summaries generated;summarization retains syntactic;summarization systems dominated;abstractive summarization;summarization performance;summaries span;summarization systems;generated summaries sacrificing;news summarization performance;abstractive summaries;structure summaries;summarization retains;news summarization;strategies news summarization;summaries span selection;syntactic structure summaries;factual inconsistency generating", "pdf_keywords": "generation abstractive summarization;generated summaries generalizable;summarization entities trained;abstractive summarization model;abstractive summarization machine;summarization machine learning;summaries generalizable summarization;generalizable summarization;summaries automatic fact;abstractive summarization increasingly;automatic summarization;automatic summarization known;summaries automatic;abstractive summarization entities;summarization increasingly;generated summaries;generalizable summarization approach;summarization model;abstractive summarization;summarization machine;abstractive text summarization;summaries generalizable;reference summaries automatic;conditional text generation;text generation abstractive;facts generated summaries;approach abstractive summarization;novel machine translation;summarization entities;present abstractive summarization"}, "e487f2508e5f62b2745a2e56ceb3c601c286d2e3": {"ta_keywords": "balanced knockout tournaments;double elimination tournaments;knockout tournaments;elimination tournaments;knockout tournaments double;tournaments double elimination;tournaments double;tournaments;analyze balanced knockout;balanced knockout;knockout;method analyze balanced;analyze balanced;elimination;double elimination;balanced;method analyze;new method analyze;double;propose new method;analyze;method;new method;propose;new;propose new", "pdf_keywords": ""}, "705e6b53f88ec733e3c186c6232c41b268248c01": {"ta_keywords": "social choice dynamics;social choice model;preferences expressed ratings;behavioral social choice;choice dynamics population;ratings real raters;choice dynamics;like netflix ratings;choice model dependence;social choice outcomes;netflix ratings;social choice literature;notions movie ratings;inference behavioral modeling;movie ratings;social choice based;social choice;movie ratings real;conclusions behavioral predictions;choice model;behavioral predictions;predictions conclusions behavioral;choice based behavioral;ratings rankings;human preferences;perspective social choice;human preferences preferences;social choice switch;inference behavioral;ratings rankings ballots", "pdf_keywords": ""}, "740182c3aa9a3045fcd9370269d446455ae9f623": {"ta_keywords": "string transduction models;recurrent neural network;networked immune based;assumption recurrent neural;alignment patterns protein;recurrent neural;finite state transducers;patterns protein;behavior networked immune;determined probability string;finite state transducer;string transduction;patterns protein protein;networks neural;state transducers;networks neural features;state transducers fsts;neural networks;networked immune;state transducer;probability string pair;paper neural networks;string pair state;hypothesis alignment pattern;distributions pairs strings;sequence alignments;neural network;state transducer determined;transduction models;predicting behavior networked", "pdf_keywords": ""}, "ba159dbf205193d0cb7c9c18dd01f830d2f56eb8": {"ta_keywords": "modern text;translating historical text;effect translating historical;modern text goal;text modern text;text modern;historical text;historical text modern;translating historical;contemporary natural language;forced pendulum;forced forced pendulum;pendulum;text;natural language processing;effect translating;language processing tools;language processing;natural language;text goal improving;study effect translating;text goal;translating;dynamics forced;output contemporary natural;dynamics forced forced;study dynamics forced;dynamics;numerical study dynamics;output contemporary", "pdf_keywords": ""}, "b694472c13420acb599a5b1d25d5f2bd42eb8c1b": {"ta_keywords": "sequencing based information;sequencing shot gun;novo sequencing shot;sequencing shot;novo sequencing;genome sequencing;genome sequence reconstructed;approach genome sequencing;sequencing based;unknown genome;unknown genome sequence;new approach genome;genome sequencing based;underlying unknown genome;approach genome;shot gun data;sequencing;problem novo sequencing;genome sequence;gun data underlying;genome;gun data;minimum data efficient;novo assembly algorithm;sequence reconstructed short;sequence reconstructed;algorithm information theoretic;requires minimum data;minimum data;reconstructed short substrings", "pdf_keywords": ""}, "71d649dcb3dee2ca57d0775a9679cb68f82f22d5": {"ta_keywords": "speaker adaptation;adaptation technique vector;speaker adaptation methods;love speaker adaptation;adaptation methods;propose adaptation technique;adaptation methods paper;machine learning;machine learning technique;technique vector extractor;novel method predicting;adaptation technique;vector extractor;paper propose adaptation;method predicting;adaptation;learning technique;snow love speaker;predictive based;vector extractor vector;ability predict response;extractor vector;ability predict;propose adaptation;predicting;technique vector;ivector snow;predict response;compare ivector snow;predictive", "pdf_keywords": ""}, "0be998fffc5f44496042f7757fb2ffa8924e54cd": {"ta_keywords": "scorer network learnable;training adaptive scorer;tutor based;method training adaptive;translation image classification;simultaneously tutor based;learnable function training;training adaptive;training data;machine translation image;method training;scoring data;training data efficiently;machine translation;learning;scoring scoring data;adaptive scorer;function training;learning model;learning model trained;tutor;tasks simultaneously tutor;method scoring scoring;scoring data paper;trained predict;method scoring;skimming problem;simultaneously tutor;new method training;model trained", "pdf_keywords": "data train deep;deep network training;knowledge scorer network;train deep network;scorer learnable function;optimizing training data;training data;learnable function training;data train;formulates scorer learnable;new machine learning;network training;model trained;training data usage;scorer learnable;deep network;function training;new approach deep;optimizing training;network training leverages;deep network propose;train scorer network;function training data;training data ef;training methods propose;machine learning learn;use data train;leverages machine learning;training methods;approach deep network"}, "c5bb38b8e3ce21063670dfd81ac64dcb2ecf10b2": {"ta_keywords": "quantum computer;constructing quantum computer;generate quantum;generate quantum state;quantum computer used;constructing quantum;used generate quantum;method constructing quantum;quantum;spectral notches experiments;quantum state;quantum state classical;frequencies spectral notches;contours pinna images;state quantum;spectral notches proposed;quantum paper propose;spectral notches;classical state quantum;developing individualized head;fast computation frequencies;accurate virtualization sound;individualized head;quantum paper;computation frequencies spectral;head related impulse;pinna images;finer contours pinna;notches experiments method;head related transfer", "pdf_keywords": ""}, "8abd724b770348bd21b16b9aaf2ba0a77596b2ed": {"ta_keywords": "speaker wise embeddings;wise embeddings speaker;embeddings speaker;embeddings speaker overlaps;speaker diarization eend;speaker diarization;approach speaker diarization;attractors embeddings;end neural diarization;speech activity detector;external speech activity;generating speaker wise;attractors embeddings remedy;pipeline approach speaker;decoder based attractor;neural diarization;generating speaker;results external speech;method generating speaker;products attractors embeddings;introduce encoder;generating multiple attractors;external speech;speech activity;encoder;neural diarization eend;introduce encoder decoder;attractor calculation module;multiple attractors;speaker overlap handling", "pdf_keywords": ""}, "af787fda38ce6fa1d14ad2fb8568088faf973a21": {"ta_keywords": "person parking lot;position person parking;person office parking;office parking lot;person parking;parking lot based;office parking;parking lot;parking;parking lot number;rule location person;location person office;location person;rule location;simple rule location;position person;person office;number attendees;lot number attendees;position;relationship position person;attendees;relationship position;based relationship position;location;lot based relationship;simple rule;lot number;office;rule", "pdf_keywords": ""}, "be312e930f6739a709e60547aa0dfb9c3dc44497": {"ta_keywords": "multilingual lexicon encoding;multilingual neural machine;multilingual training neural;multilingual neural;preprocessing multilingual neural;neural machine translation;multilingual lexicon;lexicon encoding;improvements strong multilingual;strong multilingual;propose multilingual lexicon;lexicon encoding framework;multilingual;strong multilingual nmt;multilingual training;multilingual nmt baselines;languages multilingual training;word representations;machine translation soft;encoding semantic;character encoding semantic;spelling character encoding;languages multilingual;translation soft decoupled;shared languages multilingual;share lexical level;learning word representations;share lexical;preprocessing multilingual;low resource languages", "pdf_keywords": "generalization multilingual imagery;multilingual imagery;multilingual neural machine;multilingual neural;multilingual imagery experiments;introduction multilingual neural;learning generalization multilingual;improvements strong multilingual;cross lingual learning;generalization multilingual;multilingual;strong multilingual;neural machine translation;languages improving translation;lingual learning generalization;approach cross lingual;lingual learning;translating low resource;cross lingual;improving translation;train word embeddings;multilingual nmt baselines;machine translation;introduction multilingual;lingual;improving translation quality;strong multilingual nmt;translation quality low;particularly salient translating;word character segmentation"}, "e961c8de1df75f70254656e98ca82f9d9fbd640c": {"ta_keywords": "compressive phase retrieval;algorithms compressive phase;phase retrieval sparse;graph compressive phase;approaches phase retrieval;phase retrieval based;compressive phase;general compressive phase;problem phase retrieval;phase retrieval;phase retrieval problems;phase retrieval problem;phase retrieval paper;retrieval phase retrieval;efficient algorithms compressive;phase retrieval phase;phasecode recover;algorithms compressive;sparse graph compressive;phasecode recover high;phasecode;sparse graph coding;graph compressive;phasecode novel;limit phasecode;recovering complex signal;fundamental limit phasecode;unicolor phasecode recover;general compressive;phasecode novel family", "pdf_keywords": "compressive phase retrieval;phase retrieval algorithm;compressive phase;consider compressive phase;phase retrieval problems;phase retrieval;present compressive phase;phase retrieval problem;theory phase retrieval;phase retrieval goal;sparse unconstrained fourier;recovering signal magnitude;umbrella phase retrieval;recovering signal;unconstrained fourier friendly;consider compressive;compressive;proposed algorithms sparse;unconstrained fourier;sparse unconstrained;present compressive;algorithms sparse unconstrained;paper consider compressive;fourier friendly measurement;algorithms sparse;problem recovering signal;signal magnitude fourier;largest set signal;paper present compressive;fourier friendly"}, "e2a4e1a9f8e66baf12a49a3e5d8e33291f9347e7": {"ta_keywords": "text entity linking;entity linking challenging;entity linking;short text entity;neural semantic matching;entity linking aims;aggregated semantic matching;link reference knowledge;semantic matching models;semantic matching;tweet datasets;text entity;linking challenging short;linking;neural semantic;task entity linking;jointly disambiguation;aggregated semantic;jointly disambiguation rank;semantic matching asm;public tweet datasets;tweet datasets paper;based neural semantic;reference knowledge base;short texts;linking challenging;structure entity dynamics;disambiguation;disambiguation rank aggregation;entity dynamics structure", "pdf_keywords": ""}, "f951aad88e244182b37e4918c3d570560108c68c": {"ta_keywords": "adversarially robust;adversarially robust classifiers;constructing adversarially robust;adversarially trained;adversarially trained neural;attacks adversarially trained;constructing adversarially;adversarially;adversarial;attacks adversarially;neural networks adversarial;adversarial attacks adversarially;adversarial attacks;robustness neural networks;perceptually aligned gradients;robustness neural;networks adversarial;means constructing adversarially;robust classifiers;study robustness neural;networks adversarial attacks;generate grainy images;grainy images;classifiers;property robust classifiers;robust classifiers paper;generate grainy;images study robustness;grainy images study;class perceptually aligned", "pdf_keywords": "robustness smoothing deep;adversarially robust;robustness deep neural;learning robustness smoothing;robustness deep;constructing adversarially robust;investigate robustness deep;adversarially robust classi\ufb01ers;augmentation smoothed neural;learning robustness;smoothing deep neural;neural networks adversarial;adversarial;adversarially;adversarial examples;examples adversarial;adversarial examples adversarial;networks adversarial examples;constructing adversarially;adversarial machine learning;robustness smoothing;examples adversarial machine;machine learning robustness;networks adversarial;adversarial machine;smoothed neural networks;data augmentation smoothed;augmentation smoothed;means constructing adversarially;smoothed neural"}, "5eaa425af39339e0ae30202b348cc6e253813993": {"ta_keywords": "retrieval prediction;information retrieval prediction;retrieval information language;retrieval prediction prediction;information retrieval;retrieval information;present designed retrieval;quantum systems xcite;present information retrieval;designed retrieval information;designed retrieval;retrieval;quantum systems;information language russian;quantum;language russian academy;text corpora entangling;information language;russian academy sciences;systems xcite;large text corpora;systems xcite present;quantum limited;number quantum systems;text corpora;xcite;language russian;power quantum;large number quantum;xcite present information", "pdf_keywords": ""}, "0761a69310f7b8f4ab01495f31a30c6fe53d83b8": {"ta_keywords": "speech recognition incremental;adaptation acoustic model;adaptive discriminative speech;model change speech;incremental adaptation adaptation;recognition incremental adaptation;incremental adaptation;multi scale adaptation;adaptation acoustic;extend incremental adaptation;multiscale adaptation;change speech characteristics;incremental adaptation scheme;apply adaptation acoustic;robust speech recognition;scale adaptation scheme;robust speech;incremental adaptations multiscale;time incremental adaptations;adaptations multiscale adaptation;multiscale adaptation potential;speech modeling proposed;discriminative speech modeling;scale adaptation;incremental adaptations;realize robust speech;speech recognition;speech modeling;adaptation adaptation;change speech", "pdf_keywords": ""}, "225767ce707781d0114815068c355622869ee642": {"ta_keywords": "cognition living systems;markovian signature video;artificial cognitive;artificial cognitive systems;hidden markovian signature;cognitive systems present;hidden markovian;cognitive systems;understanding cognition living;cognition living;markovian signature;presence hidden markovian;understanding cognition;markovian;cognition;construction artificial cognitive;progress understanding cognition;recognizing;living systems;cognitive;method recognizing presence;recognizing presence hidden;living systems new;recognizing presence;signature video;computer science neuroscience;systems;method recognizing;systems present simple;systems present", "pdf_keywords": ""}, "91184a2d40be8a0171b5c926b336666ed717ec6e": {"ta_keywords": "quantum computers;topic quantum chaos;quantum chaos;quantum computers greatly;using quantum computers;xmath0 xmath1 transition;xmath1 transition;creation xmath0;chaos efficiency quantum;used creation xmath0;creation xmath0 xmath1;xmath1 transition present;quantum chaos efficiency;introduction topic quantum;quantum;efficiency quantum computers;xmath0 xmath1;xmath0;using quantum;xmath1;topic quantum;enhanced using quantum;efficiency quantum;chaos efficiency;computational;experiments computational;composition digital libraries;chaos;digital libraries;insightful experiments computational", "pdf_keywords": ""}, "7d148b46f45e935765e56887d720492b2b716e55": {"ta_keywords": "connectivity collective dynamics;collective dynamics generate;intrinsic collective dynamics;neurons model;pair neurons model;collective dynamics;collective dynamics typically;revealing physical connectivity;reconstructing network connectivity;reconstructing network;neurons model neural;manner reconstructing network;connectivity collective;reconstruction theory networks;synapses pair neurons;model neural circuits;networks propose theory;physical connectivity;network connectivity collective;pair neurons;intrinsic collective;dynamics single event;networked systems event;theory networks scalable;dynamics generate reveal;neural circuits;neurons;synapses;networks;networks propose", "pdf_keywords": "theory networks spiking;networks spiking neurons;spiking neurons event;networks spiking;conductance based synapses;neurons event timing;synaptic connectivity model;synaptic connectivity;spiking neurons;spiking patterns;revealed synaptic connectivity;neurons event;neurons observed patterns;network neurons observed;theory revealed synaptic;irregular spiking patterns;synapses;topology network neurons;based synapses;synapses ii excitatory;network neurons;based synapses work;synapses iii instantaneous;regular irregular spiking;spiking patterns vi;spiking;based synapses ii;irregular spiking;synapses work present;synapses work"}, "d3dd80269f2542cc173afb3a1df24b582a1e4af2": {"ta_keywords": "translation transformers remarkably;machine translation transformers;translation transformers;improves length generalization;generalization machine translation;length generalization machine;regular languages struggle;length generalization;symbol transformer classification;generalize longer strings;generalization machine;regular languages;machine translation;languages struggle;transformers remarkably;zero languages;generalization;fail generalize longer;entropies models arbitrarily;languages;confident input strings;looking regular languages;generalize longer;strings offer simple;lemma second transformers;transformer classification decisions;transformer classification;languages struggle paper;zero languages acceptance;symbol transformer", "pdf_keywords": "generalization machine translation;machine translation recognize;automatic machine translation;improves length generalization;translation improves length;machine translation improves;length generalization machine;machine translation;languages shorter training;position machine translation;string length neural;formal languages shorter;translation recognize language;machine translation method;generalizing formal languages;languages shorter;language perfect accuracy;length neural network;length generalization;shorter training strings;length neural;translation recognize;limitation suggested hahn;generalization machine;automatically generalizing formal;network recognize language;longer testing strings;translation improves;training strings longer;automatically generalizing"}, "920257774e2caee8a8c74968c64c10bcb79a136c": {"ta_keywords": "core coaxial cable;coaxial cable;cable hybrid genetic;coaxial cable hybrid;genetic algorithm vector;propagation electromagnetic waves;propagation electromagnetic;approach propagation electromagnetic;genetic algorithm;transmission lines;function transmission lines;propagation function transmission;performance genetic algorithm;single core coaxial;hybrid genetic algorithm;core coaxial;algorithm vector fitting;cable hybrid;coaxial;vector fitting based;electromagnetic waves;transmission lines paper;proposed approximation propagation;vector fitting;algorithm predicting;cable;fitting based approach;algorithm vector;propagation function;propagation", "pdf_keywords": ""}, "147b954ba0881d643706c918e017f7d66a15b827": {"ta_keywords": "algebra learning agent;intelligent tutoring;provide intelligent tutoring;intelligent tutoring systems;tutoring systems;learning agent;tutoring systems useful;tutoring;algebra learning;learning agent sim;learning integrated simstudent;learning agent better;demonstrated algebra learning;cognitive model human;model human learning;better cognitive model;finds cognitive models;algebra stoichiometry agent;cognitive model best;human learning;agent sim student;cognitive models using;experts cognitive model;cognitive model;art learning agent;agent better cognitive;simstudent skill acquisition;cognitive models;student generated models;human experts cognitive", "pdf_keywords": ""}, "3426f000673aae995a55ade9273c842bb484ad18": {"ta_keywords": "multilingual phoneme recognizers;multilingual phoneme recognizer;automatic detection phoneme;phoneme recognizers monolingual;phoneme segmentation;unsupervised phoneme segmentation;detection phoneme;detection phoneme boundaries;phoneme segmentation previously;phoneme recognizers;phoneme recognizer;language unsupervised phoneme;recognize phoneme;multilingual phoneme;able recognize phoneme;present multilingual phoneme;unsupervised phoneme;method detection phoneme;monoand multilingual phoneme;phoneme boundaries audio;recognizers monolingual;phoneme boundaries;recognize phoneme inventory;phoneme recognizer mva;phoneme boundaries compare;recordings unknown language;distinguish different languages;languages investigate automatic;automatic detection;recognizers monolingual gold", "pdf_keywords": ""}, "89c2cbdf1a5049a4068ca9215aa8859a1a97b1a3": {"ta_keywords": "algorithm contextual bandit;contextual bandit learning;bandit learning;contextual bandit;bandit learning problem;statistically optimal regret;optimal regret guarantee;bandit;optimal regret;achieves statistically optimal;problem learner repeatedly;cost sensitive classification;learning problem;problem learner;learning problem learner;regret guarantee oracle;solving fully supervised;learner repeatedly;learner repeatedly takes;classification problems achieves;learning;supervised cost sensitive;regret guarantee;statistically optimal;algorithm predicting;algorithm predicting outcome;simple algorithm predicting;algorithm contextual;new algorithm contextual;supervised cost", "pdf_keywords": "optimal regret bounds;algorithm online decision;online multiclass algorithm;algorithms optimal regret;oracle complexity martingales;online decision;online decision making;descent algorithm supervised;optimal regret;learning computationally;learning computationally tractable;complexity martingales warm;complexity martingales;coordinate descent algorithm;online multiclass;regret bounds present;posterior distribution policies;regret bounds;substantially sparser distributions;supervised learning computationally;coordinate descent;present online multiclass;algorithms optimal;previous algorithms optimal;supervised learning;optimal;algorithm supervised learning;new coordinate descent;learning;distribution policies approximated"}, "89f7db77a755d44d3aabdbcc7549b743d7debcc5": {"ta_keywords": "conditional preference networks;preference networks;uncertainty preference statements;preference networks cp;express uncertainty preference;generalization conditional preference;preference statements;conditional preference;preference statements pair;preference relations;individual preference relations;preference relations cp;uncertainty preference;preferences features;statements preferences;nets incorporates uncertainty;statements preferences set;preferences features particular;links individual preference;expression preferences features;qualitative conditional statements;preferences set objects;encoding uncertainty;express uncertainty;conditional statements cp;encoding uncertainty expressed;uncertainty expressed probabilistic;uncertainty present generalization;objects expression preferences;qualitative conditional", "pdf_keywords": ""}, "18a82459d495fa3ad22a60bd7c9527df8bd55e1e": {"ta_keywords": "connectivity distributed optimization;distributed optimization;distributed learning algorithm;distributed learning;connectivity dual averaging;algorithm learns payoff;learns payoff gradients;distributed optimization setup;network connectivity distributed;connectivity distributed;provide distributed learning;convergence network connectivity;algorithm learns;rate convergence network;averaging present learning;deterministic network;learning algorithm learns;learning algorithm case;gradients individual players;case deterministic network;deterministic network subjected;learns payoff;connectivity network locally;convergence network;player optimizes global;network locally;dual averaging;size connectivity network;network connectivity dual;learning algorithm", "pdf_keywords": ""}, "341f6353547f4a58fdf11fbcc9de3a31083a619b": {"ta_keywords": "surface tension;fluid dynamics;fluid dynamics video;motion body surface;presence surface tension;dynamics;surface;body surface;dynamics video;surface affected;tension;motion;dynamics video motion;surface affected presence;motion body;video motion body;body surface affected;video motion;fluid;presence surface;affected presence surface;video;body;affected presence;presence;affected", "pdf_keywords": ""}, "7a79099447bef9a3ea13b1dc409d04b3dff57320": {"ta_keywords": "revamped midi provides;midi provides sequence;revamped midi;modeling rhythmic;dubbed revamped midi;midi;music plausible rhythmic;models modeling rhythmic;composing music;modeling rhythmic patterns;music translator;piano music plausible;drums sequence models;composing music long;art sequence models;midi provides;patterns music;piano music;rhythmic patterns music;patterns music allowing;plausible rhythmic;composes piano music;sequence models better;especially composing music;popular music translator;state art sequence;plausible rhythmic structure;piano;provides sequence models;composing", "pdf_keywords": "automatic music composition;music information retrieval;intelligence automatic music;represent musical data;musical data;musical data following;music theory machine;music composition;machines compose music;automatic music;music building machines;music composition based;framework automatic music;human knowledge music;music information;music composition important;composition based music;model music based;based music information;model music;compositional model music;represent midi data;represent midi;midi;knowledge music;task music;compose music;knowledge music propose;compose music like;midi data"}, "e9d8db4f5b5c106c43a268f635788c0a94b2916a": {"ta_keywords": "stochastic gradient descent;descent ascent methods;ascent methods;ascent methods far;analysis sparse random;gradient descent ascent;sparse random;stochastic gradient;sparse random variables;gradient descent;statistical analysis sparse;stochastic estimates paper;stochastic estimates;known solving minimization;analysis sparse;energy method unified;distributed method reduce;randomization distributed variants;new distributed method;descent ascent;distributed method;randomization;randomization distributed;variety stochastic gradient;sampling coordinate randomization;sparse;stochastic;force energy method;new numerical method;coordinate randomization distributed", "pdf_keywords": "regularization gradient descent;variational inequalities regularization;inequalities regularization gradient;optimization variational;gradient descent;methods regularized vips;optimization variational inequalities;regularization gradient;methods regularized;advances solving minimization;max optimization variational;minimization problems propose;regularization;stability condition regularized;inequalities regularization;solving minimization problems;regularized vips min;minimization problems;minimization;variational inequality based;distributed methods compression;regularized;regularized vips;condition regularized vips;regularized sgda;type methods regularized;variational stability;solution learning;solving minimization;variance reduction"}, "b9913ddf94245c864509f0b94847bdbe77899b46": {"ta_keywords": "spoken linguistically trained;tonal transcription language;linguistically trained;tonal languages;experiments tonal languages;modelling phonemes tones;jointly modelling phonemes;tonal languages yongning;documentation speech recognition;trained linguistically;transcription language;na transcribing speech;phonemic tonal transcription;speech recognition;modelling phonemes;phonemes tones;trained linguistically linguistically;phonemes tones versus;linguistically trained linguistically;speech recognition technology;transcription language documentation;transcribing speech;transcribing speech important;speech important language;accuracy linguistic annotation;language documentation speech;tonal transcription;phonemic context prediction;quantifying accuracy linguistic;languages spoken linguistically", "pdf_keywords": ""}, "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a": {"ta_keywords": "visual recognition tasks;recognition tasks;recognition uses convolutional;image classification tasks;recognition tasks vision;tasks vision transformer;recognition benchmarks;recognition performance significantly;perform visual recognition;small image recognition;visual recognition;uses convolutional neural;recognition performance;convolutionally networks;convolutional neural networks;convolutional networks used;vision transformer;convolutional neural;convolutional networks;image recognition benchmarks;components convolutionally networks;speech recognition;tasks vision attention;recognition;perform image classification;image recognition;image classification;visual recognition uses;vi visual recognition;classification tasks", "pdf_keywords": "vision convolutional architectures;vision convolutional;imagenet computer vision;convolutional architectures;convolutional architectures remain;computer vision convolutional;recognition benchmarks;recognition benchmarks present;learning model image;computer vision tasks;images learn representations;architecture computer vision;convolutional neural networks;deep learning;imagenet;convolutional neural;new deep learning;deep residual learning;networks encode positional;computer vision;neural networks encode;transformer computer vision;accuracy 72 imagenet;deep learning model;vision tasks;model image recognition;vision tasks model;learn representations;positional information images;recognition"}, "4533fd4cf13d2f4dd105edaf612934a1bd85ad5a": {"ta_keywords": "generative model electroencephalogram;model electroencephalogram eeg;model electroencephalogram;multi channel electroencephalogram;electroencephalogram eeg event;electroencephalogram eeg;electroencephalogram;channel electroencephalogram;electroencephalogram presented paper;channel electroencephalogram presented;eeg event related;electroencephalogram presented;eeg event;separation signals multiple;signals based probabilistic;separation signals;separated signal;frequency dependent covariance;amplitude separated signal;models amplitude separated;method separation signals;eeg;prior information covariance;separated signal time;noise removal;method noise removal;related potentials recorded;noise removal singletrial;event related potentials;probabilistic generative model", "pdf_keywords": ""}, "b62430b9f8810da4d9f28842ac0ca899aa66d422": {"ta_keywords": "conditions satisfied;conditions satisfied given;given collection conditions;set conditions satisfied;given set conditions;collection conditions;set conditions;conditions;satisfied given collection;determine given set;method determine given;method determine;new method determine;determine given;satisfied given;given collection;given set;determine;satisfied;present new method;method;collection;new method;given;set;paper present;paper;paper present new;present;present new", "pdf_keywords": ""}, "d1c41eb99824e8f4752190da1b815378be23b4b9": {"ta_keywords": "forgetting schedcheduled sampling;catastrophic forgetting schedcheduled;catastrophic forgetting;catastrophic forgetting significantly;alleviates catastrophic forgetting;forgetting;forgetting significantly;forgetting significantly improves;form catastrophic forgetting;sequence tasks autoregressive;autoregressive models trained;forgetting schedcheduled;schedcheduled sampling;scheduled sampling;forgetful weight;forgetfulness;weight person forgetfulness;schedcheduled sampling simple;tasks autoregressive models;generated prefixes training;standard scheduled sampling;person forgetfulness;bias retaining;models trained;forgetful;forgetful weight person;prefixes training;sequence tasks;tasks autoregressive;prefixes training process", "pdf_keywords": "translation models trained;neural machine translation;machine translation models;forgetting neural machine;translation models;novel machine translation;catastrophic forgetting neural;catastrophic forgetting;translation tasks;commonlyused translation tasks;forgetting neural;answering models trained;occurrence catastrophic forgetting;domain adaptation;catastrophic forgetting allow;address catastrophic forgetting;machine translation;alleviates catastrophic forgetting;trained scheduled sampling;translation tasks germanenglish;use domain adaptation;forgetting allow improvements;domain adaptation methods;machine translation method;models trained scheduled;forgetting;question answering models;nlp tasks nmt;autoregressive learning address;autoregressive learning"}, "175b58fe7e49bb5c0c771b73f8834bcff21b59c7": {"ta_keywords": "natural language inference;natural language hypothesis;determining natural language;language inference nli;sentence encoders stress;understanding natural language;natural language;evaluation sentence encoders;language inference;sentence encoders;inference nli;inference nli task;unclear natural language;challenging linguistic;semantic content sentences;challenging linguistic phenomena;content sentences;linguistic;sentences unclear natural;linguistic phenomena suggests;sentences;models understand semantic;language hypothesis inferred;language hypothesis;linguistic phenomena;understand semantic content;nli task determining;nli task;semantic content;automatically constructed stress", "pdf_keywords": "models predict entailment;predict entailment;challenging nli models;distractions generated adversarial;able predict antonymy;sentence encoder model;predict entailment particular;sentence encoder;evaluation nli models;nli models;hypothesized challenging nli;targeted evaluation nli;performing sentence encoder;generated adversarial construction;learning models predict;generated adversarial;challenging nli;adversarial construction;models perform reasoning;predict antonymy;adversarial construction paper;automatically construct stress;adversarial;relation prediction;sentences unrelated;nli models set;sentences;evaluation nli;correct relation prediction;training distractions generated"}, "49edf7f0dbad8b8c101af9ef95c72f62f545591e": {"ta_keywords": "correlated topic modeling;compact topic embeddings;learns compact topic;captures topic correlations;topic embeddings;topic vectors;topic modeling;closeness topic vectors;embeddings captures topic;topic occurrence correlated;topic embeddings captures;topic correlations;sparsity topic occurrence;topic correlations closeness;topic vectors method;topic modeling csp;occurrence correlated topic;topic size fluid;correlated topic;topic occurrence;topic size;captures topic;compact topic;topic computer;based sparsity topic;novel probabilistic inference;correlations closeness topic;topic computer science;sparsity topic;popular topic computer", "pdf_keywords": "topic embedding model;correlated topic modeling;topic embedding;novel topic embedding;topic modeling;topic modeling industrial;representations latent topics;topic modeling exploit;model correlated topic;latent topics;learning embeddings observed;structural topic navigation;cient inference embedding;inference embedding;sparsity topic occurrence;embedding model correlated;learning embeddings;topic occurrence;variational inference fast;word representation learning;linear topic size;inference embedding space;stochastic variational inference;embedding space inference;topic navigation;topic navigation improved;embeddings observed;correlated topic;distributed representations latent;topics single document"}, "db79a3e55690c5c86cfd0ec97712ed4ad1e47b3b": {"ta_keywords": "sequential ranking;analyze sequential ranking;sequential ranking algorithm;improved ranking algorithms;pairwise ranking;pairwise ranking based;ranking algorithms;sequential active ranking;ranking algorithm counts;item ranking;improved ranking;allows improved ranking;ranking refers probability;chosen item ranking;stochastic comparisons;ranking algorithm;noisy pairwise comparisons;ranking based parametric;stochastic comparison models;ranking;ranking set items;models ranking;ranking based;stochastic comparisons long;item ranking refers;active ranking;luce models ranking;comparisons analyze sequential;work pairwise ranking;ranking refers", "pdf_keywords": ""}, "3c37b9ec2ff1828877575acc600b73c3bcde138f": {"ta_keywords": "bandit recommend user;bandits policy recommender;multi armed bandits;multi armed bandit;bandit recommend;systems departing bandit;recommender existent rewards;departing bandit recommend;bandits policy;bandit setup;bandit;bandits;bandit setup captures;optimal recommender;armed bandits policy;departing bandit;algorithm optimal recommender;armed bandit setup;optimal recommender systems;armed bandits;armed bandit;recommend user stay;policy recommender existent;recommend user;recommender existent;inferred response recommendations;algorithm achieves regret;recommendations recommender;policy recommender;regret number users", "pdf_keywords": "optimal policy recommending;policy recommending;policy recommending category;optimal representation random;deterministic optimal planning;optimal policy;semi deterministic optimal;category recommending;optimal planning;learner recommend best;category recommending particular;stochastic model planning;deterministic optimal;online learning problem;optimal representation;optimal planning algorithm;recommending category;recommending particular;recommendation categories;planning task;learning problem deciding;recommending category recommending;learner recommend;finding optimal representation;planning task rss;planning algorithm;user types recommendation;types recommendation categories;recommending;best policies category"}, "f6f4d30e4740bd92b31acd297a15872d490e7f11": {"ta_keywords": "classification relation extraction;relation extraction tasks;text classification relation;relation extraction;relation extraction propose;link based classification;results relation extraction;classifiers semi supervised;semi supervised learning;modeling semi supervised;text classification;link based text;semi supervised;based text classification;classification outperforms link;outcome semi supervised;supervised learning ssl;supervised classification tasks;classification tasks;classifiers semi;classification tasks ssl;classification relation;supervised classification;outperforms link based;class classifiers semi;learning ssl algorithms;supervised learning task;traditional supervised classification;algorithms link based;supervised learning", "pdf_keywords": "specifying semi supervised;semi supervised learners;semi supervised learning;semi supervised;seincally semi supervised;classi\ufb01cation relation extraction;entity centric corpus;language domain supervised;supervised classi\ufb01cation tasks;domain supervised;present semi supervised;relation extraction;supervised classi\ufb01cation;domain supervised learning;domains semidefinite programming;machine learning language;declarative machine learning;supervised learners speci;supervised learners;traditional supervised classi\ufb01cation;supervised learning framework;based text classi\ufb01cation;document entity centric;relation extraction paper;learning ssl constraints;text classi\ufb01cation relation;learning language domain;semidefinite programming semf;supervised learning;centric corpus"}, "dab261b25ff8ccd2c9144a5cb3a46b39ac0ac4bd": {"ta_keywords": "statistical foundations quantum;remote quantum systems;quantum information;entangle remote quantum;remote quantum;quantum systems;quantum mechanics based;quantum information used;quantum theory;quantum mechanics;quantum state;quantum systems entangling;systems entangling quantum;entangling quantum;entangling quantum state;quantum state distant;foundations quantum mechanics;learning statistical physics;quantum;machine learning statistical;letter quantum information;foundations quantum theory;machine learning observed;information used entangle;foundations quantum;learning statistical;entangle;entangle remote;foundations statistical mechanics;theory letter quantum", "pdf_keywords": "convey actual insight;learning suggestive;machine learning suggestive;misuse machine learning;learning suggestive de\ufb01nitions;actual insight paper;theorem included reviewers;counterarguments providing historical;question speculation research;greater rigor exposition;exploration predicated intuitions;rigor exposition;exposition;reviewers convey;language misuse;reviewers convey actual;suggestive de\ufb01nitions overloaded;paper concludes counterarguments;learning;rigor exposition science;insight paper;intuitions coalesce crisp;predicated intuitions coalesce;formal representations draft;intuitions coalesce;arti\ufb01cial brain 2014;predicated intuitions;counterarguments;de\ufb01nitions overloaded terminology;intuitions"}, "b79dcc5304e557ce200b161d2a884c0ff77f34ec": {"ta_keywords": "combating adversarial misspellings;adversarial misspellings;adversarial misspellings introduce;spelling correction systems;utility spell checkers;isolated misspellings;isolated misspellings use;toolkit spelling correction;spell checkers combating;spell checkers;misspellings multiple;misspellings multiple sources;engineering isolated misspellings;existing spelling correction;misspellings use;occurring misspellings;misspellings introduce neuspell;source toolkit spelling;occurring misspellings multiple;naturally occurring misspellings;misspellings;misspellings introduce;toolkit spelling;misspellings use richer;misspelt token;spelling errors context;models using spelling;using spelling errors;context misspelt token;spelling correction", "pdf_keywords": "toolkit adversarial misspellings;adversarial misspellings previously;defenses adversarial misspellings;adversarial misspellings;adversarial misspellings spelling;models spelling correction;misspellings use contextual;neural models spelling;spelling correction systems;spell checkers toolkit;synthetic misspellings;isolated misspellings;representations synthetic misspellings;misspellings common applications;misspellings spelling correction;isolated misspellings use;spell checkers;spelling correction task;proposed spell checkers;occurring misspellings;misspellings multiple;engineering isolated misspellings;evaluate spell checkers;spelling correction context;misspellings use;spell checkers spelling;models using spelling;synthetic misspellings common;misspellings previously;occurring misspellings multiple"}, "9b621c0bcd2029006b389bc51395fb6604f9a855": {"ta_keywords": "human answerers;questions game synthetic;humans natural language;questions game;synthetic human answerers;captioner requiring supervised;captioner;resolve misunderstandings dialogue;natural language;questions resolve misunderstandings;misunderstandings dialogue propose;20 questions game;human answerers paper;dialogue;image captioner requiring;captioner requiring;supervised question answer;misunderstandings dialogue;question answer data;image captioner;question asking model;informative questions;dialogue propose;paradigm natural language;natural language ambiguous;natural language processing;game synthetic human;questions resolve;informative questions shelf;model communicative success", "pdf_keywords": "captioner answerer learning;human answerers;visually grounded questionasking;answerer learning;questionasking model;questionasking model capable;questionanswer training;questions improve communicative;answerer learning able;questions game synthetic;image captioner answerer;captioner answerer;questions game;synthetic human answerers;grounded questionasking model;questionanswer training data;questionasking;produce informative questions;pretrained image captioner;question meanings model;human answerers propose;pose questions;captioner;approach image captioner;dialogue speci\ufb01cally humans;informative questions unseen;meaning questions;specialized questionanswer training;relying specialized questionanswer;20 questions game"}, "a84c319fef32b2514af9541576189a1735aac507": {"ta_keywords": "approach answer question;way answer question;best way answer;new approach answer;answer question best;question answer question;approach answer;answer question answer;answer question;question best way;propose new approach;new approach;approach;answer question yes;question answer;way answer;best way;answer;paper propose new;paper propose;question best;best;question yes;propose new;yes;question;paper;new;propose;way", "pdf_keywords": ""}, "f2de2c9d83b9058695e3272a4fbb7c30e04a1476": {"ta_keywords": "learning word second;word level predictors;ease learning word;learning word;word learning;second language learning;language learning algorithms;distributional models lexical;andenglish semantic density;investigating second language;alignment andenglish semantic;semantic alignment andenglish;cross linguistic semantic;language learning;linguistic semantic alignment;word learning investigate;models lexical semantics;performance language learning;alignment andenglish;language duolingo mobile;models lexical;word second language;status word learning;effects cross linguistic;linguistic semantic;semantic density;lexical semantics;psycholinguistically plausible word;cross linguistic;plausible word level", "pdf_keywords": "learning word second;word learning accuracy;second language learning;learning word translation;higher accuracy words;pairs language learning;accuracy words learned;ease learning word;word translation distance;learning word;language duolingo mobile;related word accuracy;word accuracy;language learning word;word learning;word second language;investigating second language;measure semantic alignment;lexical semantics;models lexical semantics;data second language;distributional models lexical;semantic alignment does;semantic alignment;language learning;main semantic predictors5;learning accuracy cognate;language duolingo;accuracy words;language pairs"}, "5b6c1e9dddc4b55036a5629227ae2cc7d49eb6d0": {"ta_keywords": "image motion microfluidic;fluorescent microscopy images;microscopy images fluid;motion microfluidic;microfluidic;video microfluorescence microscopy;microfluorescence microscopy;detecting subcellular localization;microfluorescence microscopy used;microscopy images;motion microfluidic devices;fluorescent microscopy;fluorescent fluorescent microscopy;detecting subcellular;subcellular localization;localization patterns fluorescent;images fluid dynamics;microfluidic devices;capable detecting subcellular;images fluid;microscopy;subcellular localization patterns;dynamics video microfluorescence;slif project;microfluorescence;interactive relevance;video microfluorescence;findings interactive relevance;microscopy used image;microfluidic devices paper", "pdf_keywords": ""}, "a1babdf55a6bff96d533fd0c9bc44864283ec107": {"ta_keywords": "aversion consider banks;depositors risk aversion;depositors risk;consider banks managed;banks managed;depositors creditors;banks managed based;bank existence;consider banks;banking institutions analyze;banks;banking institutions;bank respect corporate;banking;corporate governance problem;corporate governance variables;paper depositors risk;dynamics corporate governance;management banking institutions;depositors opening account;management banking;corporate governance;behaviors depositors creditors;particular bank respect;particular bank;bank;neglected management banking;account particular bank;depositors;depositors creditors neglected", "pdf_keywords": ""}, "3af5e203368fa2c7959d035493571d181a8682af": {"ta_keywords": "benchmark existing music;music information retrieval;musical improvisation dataset;visual analysis music;improvisation dataset midi;music audio tasks;music audio datasets;analysis music performances;dataset midi music;audio visual analysis;music audio dataset;evaluation existing music;improvisation dataset;midi music score;existing music information;music information;dataset midi;separately recorded performances;separation music audio;audio datasets propose;analysis music;dataset audio visual;audio visual;music performances addition;audio datasets;audio dataset compare;source separation music;midi music;score musical pieces;music score musical", "pdf_keywords": "dataset music improvisation;dataset musical pieces;dataset musical;comprehensive dataset musical;music performances research;recording music performances;orchestras based audio;musicians recording;music performances;dataset music;music improvisation;video recordings clarinetists;audio visual modalities;musical organizations recording;improvisation based video;availability musicians recording;recordings clarinetists performing;music improvisation based;musicians recording facilities;audio visual;recordings clarinetists;new dataset music;musicians;audio recordings;musical pieces;based audio recordings;music pieces;recording music;musical pieces obtained;recordings"}, "8f43b63ca400a0ea1fdd272f8c83fd67f01d0182": {"ta_keywords": "syntactic random fields;gene mention tagging;using syntactic random;syntactic random;tagging using syntactic;artificial neural;random fields;performance artificial neural;approach gene mention;neural network predicting;tagging;gene mention;mention tagging;network predicting outcome;random fields crfs;network predicting;mention tagging using;neural;antiferromagnet;neural network;syntactic;artificial neural network;antiferromagnet square lattice;tagging using;predicting;heisenberg antiferromagnet;using syntactic;spin heisenberg antiferromagnet;antiferromagnet square;predicting outcome competition", "pdf_keywords": ""}, "14a09a04c5c295a93ff25492516112cd86fa0114": {"ta_keywords": "semi supervised sequence;semi supervised;variational encoder;space variational encoder;variational encoder decoders;encoders movement person;transduction semi supervised;dimensional encoders movement;supervised sequence multi;semi supervised learning;supervised sequence;performance semi supervised;supervised learning generative;dimensional encoders;multi dimensional encoders;encoders movement;provides powerful supervised;supervised;encoders;encoder;powerful supervised;encoder decoders;labeled sequence transduction;learning generative;model labeled sequence;learning generative model;supervised learning;data labeled sequence;encoder decoders new;powerful supervised framework", "pdf_keywords": "semi supervised;transduction semi supervised;semi supervised machine;present semi supervised;semi supervised learning;labeled sequence transduction;recognizing words sequence;sequence labeled unlabeled;provides powerful supervised;words sequence labeled;supervised machine;supervised;powerful supervised;task recognizing words;labeled unlabeled data;sequence transduction tasks;sequence labeled;recognizing words;supervised learning;supervised machine learning;model labeled sequence;variational encoderdecoders;labeled unlabeled;labeled sequence;unlabeled data propose;space variational encoderdecoders;sequence transduction semi;variational encoderdecoders new;powerful supervised framework;unlabeled data"}, "5e63e47cb3386b032ec43a92ce5980466228c761": {"ta_keywords": "distributed file haadoop;distributed file;implement distributed file;storage optimal supporting;provide optimal storage;storage code;storage code reduces;storage optimal;new storage code;optimal storage;storage efficiency require;optimal storage efficiency;storage;network disk;storage efficiency;high network disk;reduces network disk;terabytes cross rack;erasure coded data;present new storage;theory storage optimal;file haadoop distributed;new storage;data center;haadoop distributed file;data data center;data center network;network disk usage;close terabytes cross;30 theory storage", "pdf_keywords": "performance distributed storage;distributed storage;distributed storage systems;codes storage optimal;storage systems rs;storage optimal;storage optimal supporting;storage code reduces;optimal storage;storage code;rs codes storage;storage systems;codes storage;storage optimal storage;storage capacity optimal;storage;optimal storage capacity;data network infrastructure;new storage code;theory storage optimal;data centers investigated;usage recovery;storage capacity;usage data centers;data network;unavailability performance distributed;designed reduce download;coded data network;present new storage;stores hundreds petabytes"}, "af44f5db5b4396e1670cda07eff5ad84145ba843": {"ta_keywords": "question answering;modeling textual compositionality;question answering typically;modeling textual;learns word phrase;factoid question answering;input modeling textual;qanta learns word;textual compositionality;words representations qanta;phrase level representations;quiz bowl text;learns word;question text;trivia competition called;bowl text classification;text question text;competition called quiz;sentences reason entities;text classification;textual;question text contains;representations combine sentences;called quiz;words representations;reason input modeling;information retrieval;representations qanta learns;questions trivia competition;textual compositionality xmath0", "pdf_keywords": ""}, "1bd43c91ecbf46098ef2b521c5367e849819960e": {"ta_keywords": "neural machine translation;iterative translation models;dynamic curriculum learning;monolingual data neural;iterative translation;translation models;adaptation language pairs;translation models experimental;curriculum learning;generalize dynamic curriculum;adaptation language;machine translation addition;machine translation;dynamic curriculum;model adaptation language;translate synthetic data;competitive baselines translation;curriculum learning strategy;applied iterative translation;approach translate synthetic;translate synthetic;mathematical model adaptation;baselines translation;model adaptation;translation addition;monolingual data;baselines translation proven;novel approach translate;learning;translation addition propose", "pdf_keywords": "iterative translation models;machine translation backtranslation;machine translation model;translation models;strategy iterative translation;translation machine translation;machine translation;iterative translation;translation machine;datasets machine translation;translation backtranslation machine;translation models experimental;machine translation machine;translation backtranslation;translation model;novel machine translation;translation model low;backtranslation machine learning;applied iterative translation;iterative translation paper;dynamic curriculum learning;curriculum learning;curriculum learning strategy;domain adaptation low;curriculum strategy iterative;backtranslation;backtranslation machine;domain adaptation;generalize dynamic curriculum;lithuanian english datasets"}, "16f0c508aa54e26aa18e3b0f3c91b0c143c6a605": {"ta_keywords": "empirical risk minimization;risk minority;risk minority group;predict user minority;representation disparity minority;risk minimization;controls risk minority;risk minimization erm;distributionally robust optimization;user minority group;user minority;minority group shrink;disparity minority groups;distributionally robust;user retention minority;disparity minority;commonly trained minimize;based distributionally robust;distribution machine learning;minority groups;retention minority group;empirical risk;retention minority;loss results representation;minority group time;trained minimize;minority group;minority;empirical distribution machine;minimize average loss", "pdf_keywords": "minimizing minority;approach minimizing minority;minority risk online;disparity online speech;representation disparity online;minimizing minority risk;capable predicting group;online speech recognition;mitigate representation disparity;stochastic gradient descent;recognition natural language;predicting group membership;class machine learning;online speech;autocomplete capable predicting;loss minimization;loss minimization distributive;minimization distributive justice;gradient descent;trained stochastic gradient;disparity online;risk online communities;minority risk;mitigate representation;online communities;gradient descent methods;predictive accuracy resource;machine learning;predicting group;learning models"}, "5c159745fce2b87e8b00307b76f0948b9fa8b1d7": {"ta_keywords": "syntactic analyses translated;evaluation language pairs;syntactic parser forest;neural network language;parsed using syntactic;syntactic parser;manual evaluation language;evaluation language;syntactic analyses;possible syntactic analyses;recurrent neural network;automatic manual evaluation;using syntactic parser;rescoring using recurrent;translational translation;submission translational translation;parser;input sentence parsed;syntactic;translation input;sentence parsed;using recurrent neural;parser forest;translational;parsed;forest string translation;possible syntactic;submission translational;translational translation 2016;using recurrent", "pdf_keywords": ""}, "7a1a202e268ccc910e8044be556e56aa9eb5a94f": {"ta_keywords": "automated dependence plots;partial dependence plots;dependence plots;usefulness automated dependence;dependence plots adp;dependence plots pups;automated dependence;plots usually;interesting plots usually;select interesting plots;plots;model partial dependence;visual tool;plots single feature;usually limited plots;used visual tool;plots usually limited;dependence;exploring latent;interesting plots;visual tool understand;plots pups widely;plots pups;plots adp;exploring latent space;plots single;limited plots;partial dependence;behavior exploring latent;model selection bias", "pdf_keywords": ""}, "c674ce6454d69a87f00797f3ec90d1b38b451063": {"ta_keywords": "distributed convex optimization;method distributed convex;dual stochastic gradient;stochastic gradient oracle;stochastic gradient;distributed convex;primal dual stochastic;dual stochastic;convex optimization;point solution stochastic;convex optimization problems;solution stochastic differential;solution stochastic;method rate convergence;gradient oracle method;stochastic differential;stochastic differential equation;method distributed;oracle method distributed;iteration sequence optimal;gradient oracle;optimization problems networks;stochastic;method optimal;convergence terms duality;convergence iteration;method optimal terms;optimal point;rate convergence;analysis convergence iteration", "pdf_keywords": "distributed convex optimization;distributed stochastic gradient;gradient descent distributed;problems distributed algorithms;stochastic gradient oracle;stochastic gradient descent;distributed stochastic;method distributed convex;distributed algorithms;distributed convex;dual stochastic gradient;convex problems distributed;distributed algorithms prevalent;descent distributed;weighted network agents;convex optimization problems;convex optimization;descent distributed resource;distributed resource allocation;stochastic gradient;primal dual stochastic;symmetric matrix weighted;gradient descent;present distributed stochastic;weighted network;dual stochastic;problem weighted network;gradient oracle;stochastic oracle;optimal communication"}, "2aecdd190066a57db8fea1e1143dc5fc288050e0": {"ta_keywords": "iot privacy consumers;operational efficiency privacy;iot privacy;efficiency privacy;inferential privacy;metric inferential privacy;performance consumer privacy;privacy metric inferential;privacy consumers;introduce new privacy;privacy;cryptography generate private;efficiency privacy paper;new privacy metric;consumer privacy;new privacy;privacy metric;things iot privacy;generate private information;consumer privacy understood;inferential privacy article;privacy paper;privacy understood terms;quantum cryptography;privacy understood;private information;using quantum cryptography;privacy article consider;privacy article;private information user", "pdf_keywords": ""}, "242cf2e991f0eed4b1309a2a9dff548e8b95900f": {"ta_keywords": "speech recognition;robust speech recognition;methods speech recognition;speech recognition work;beamforming methods speech;learning based beamforming;speech recognition task;robust speech;designed robust speech;lattice gaussian mixture;gaussian mixture model;based beamforming methods;non gaussian components;gaussian components methods;gaussian mixture;study electronic structure;mixture gaussian non;beamforming methods;mixture gaussian;evaluated chime benchmarking;beamforming methods specifically;electronic structure dimensional;chime benchmarking task;gaussian components;chime benchmarking;based beamforming;beamforming;dimensional 2d electron;frequency domain techniques;optical lattice gaussian", "pdf_keywords": ""}, "b3bd90f630b2d19856ef031b3dddfcb9b041b243": {"ta_keywords": "learning solving problems;tutored problem solving;tutor learning generalizing;learning solving;strategies learning solving;problem learning solving;coloring problem learning;learning tutored problem;tutor learning;hints tutor learning;learning tutored;learning strategies;learns cognitive;learning strategies learning;learns cognitive skills;showed learning tutored;strategies learning;problem learning;learning program learns;solving outperformed learning;learning worked examples;problems better learning;tutored problem variant;learns;outperformed learning strategies;hints tutor;program learns cognitive;compare learning strategies;exhaustively learning skills;feedback hints tutor", "pdf_keywords": ""}, "dd3770b2dbc9668578fefdc078d37457ba9c0b9a": {"ta_keywords": "el speech enhancement;speech enhancement;speech enhancement removing;electrolaryngeal el speech;enhancement removing microprosody;noise el speech;sounds enable laryngectomees;electrolarynx device artificially;voiced prediction;grooming excitation sounds;excitation prediction electrolaryngeal;electrolarynx device;produce el speech;sounds produced device;el speech sounds;enhancement process electrolarynx;process electrolarynx device;prediction speech;prediction speech speech;unvoiced voiced prediction;microprosody;excitation sounds enable;microprosody low pass;prediction electrolaryngeal el;speech sounds;generates excitation sounds;enable laryngectomees;adding noise el;enable laryngectomees produce;speech proficient laryngectomees", "pdf_keywords": ""}, "44cabe32482d4b622d9ca00bf23b3ee7950e2710": {"ta_keywords": "human ml judgments;human ml systems;human machine decision;human ml complementarity;analyzing human ml;ml judgments based;human machine predictive;machine predictive decisions;machine learning human;human machine learning;models human ml;ml judgments;ml systems increasingly;machine decision making;machine decision;criteria human machine;formalizing human machine;ml complementarity;ml systems;predictive decisions aggregated;machine predictive;human ml;judgments based underlying;hybrid human ml;predictive decisions;existing models human;criteria human;judgments based;ml complementarity special;psychology machine learning", "pdf_keywords": "collaborative decision making;human decision maker;collaborative decision;human ml collaboration;machine decision making;human ml decision;decision making human;human ml teams;ml teams predictive;decision maker machine;teams predictive decision;vs machine decision;complementarity human ml;combining predictive decisions;ml decision making;machine decision;predictive decision making;taxonomy collaborative decision;decision maker;ml collaboration;ml collaboration setup;human decision;quantify complementarity human;predictive decisions static;decisions produced individually;human vs machine;ml make decision;predictive decisions;decision making speci\ufb01c;taxonomy human ml"}, "3ac59132297f4e50d5e83852555392f9ff05d8b4": {"ta_keywords": "business decision process;simulation study group;involved business decision;individuals involved business;business decision;simulation study;decision process;results simulation study;individuals involved;simulation;involved business;group individuals involved;results simulation;individuals;business;group individuals;study group individuals;present results simulation;process;decision;study group;involved;group;study;results;present;note;present results;note present;note present results", "pdf_keywords": "decentralized distributed optimization;distributed optimization;consider distributed optimization;distributed optimization problem;distributed optimization asynchronous;optimization asynchronous algorithms;distributed networks;sliding algorithm lan;convex composite optimization;optimization asynchronous;decentralized distributed;distributed networks communication;real distributed networks;keywords decentralized distributed;asynchronous algorithms paper;asynchronous algorithms;composite optimization;distributed;consider distributed;algorithm lan;paper consider distributed;composite optimization problem;algorithms paper propose;sliding algorithm;algorithms paper;networks communication bottleneck;minimizing weighted sum;topology keywords decentralized;algorithms;convex composite"}, "ff3b83ef0a153ed376556057269f3a61da3a103a": {"ta_keywords": "symbolic multitrack music;multitrack music;multitrack music assuming;ensembles music;genres ensembles music;automatic instrumentation learning;genres ensembles;multitrack music addition;assistive composing tools;music assuming mixture;instrumentation learning;different genres ensembles;ensembles music modern;instrumentation learning separate;automatic instrumentation;composing tools;voices instruments tracks;instruments tracks mixture;voices instruments;multiple instruments;mixture played keyboard;parts voices instruments;tracks mixture symbolic;music addition;convincing instrumentations;alternative convincing instrumentations;instruments tracks;play multiple instruments;automatic instrumentation applications;instruments", "pdf_keywords": "multitrackmusic task separation;separation multitrack music;melody extraction;multitrack music examine;mixture multitrackmusic task;classi\ufb01cation melody extraction;separation voices instruments;multitrackmusic task;parts mixture multitrackmusic;mixture multitrackmusic;multitrack music;multitrack music scores;task voice separation;multitrackmusic;voices instruments tracks;melody extraction paper;voice separation;voice separation strategies;music examine;separation voices;task separation voices;learning separate parts;task separation multitrack;voices instruments;music examine feasibility;abundance multitrack music;instruments tracks mixture;instrumentation learning separate;ensembles parts playable;large ensembles parts"}, "cb90d5ea3a95b4c6ec904f622f51d752f506636e": {"ta_keywords": "preferences artificial intelligence;networks representing learning;classification regression networks;regression networks representing;metric preferences artificial;learning reasoning preferences;propose neural network;regression networks;networks representing;closed form scoring;artificial intelligence;networks;representing learning;neural network;artificial intelligence expect;metric preferences;classification;reasoning preferences;preferences artificial;representing learning reasoning;scoring structure aggregate;scoring structure;classification regression;aggregate propose neural;computer science social;network architecture learn;approximation metric preferences;propose neural;reasoning preferences important;make decisions recommendations", "pdf_keywords": ""}, "d4f5f1a196e203226e4a69d52a04d46823f32fb3": {"ta_keywords": "multilingual unsupervised machine;multilingual unsupervised;language extracted corpus;multilingual representation models;samanantar multilingual unsupervised;sentence pairs languages;multilingual languages baselines;indic language pairs;multilingual representation;multilingual languages;available parallel corpora;language language pivot;language pairs;language pivot language;parallel corpora;extracted corpus;parallel corpora collection;competition indic languages;language pivot;trained set multilingual;multilingual;indic languages translated;collection indic languages;pairs language language;language pairs language;language extracted;english indic languages;corpus;nearest neighbor nnn;use multilingual representation", "pdf_keywords": "mining interlanguage links;cross lingual similarity;multilingual machine translation;mining interlanguage;explore multilingual models;multilingual models;multilingual models spanning;multilingual machine;lingual similarity model;corpus mining parallel;present multilingual machine;lingual similarity;corpora accurate multilingual;machine translation wide;accurate multilingual representation;indic monolingual corpora;machine translation model;multilingual representation learning;translation model india;interlanguage links;explore multilingual;large indic monolingual;mining parallel sentences;multilingual representation;monolingual corpora;interlanguage links articles;machine translation;multilingual;mining mining interlanguage;cross lingual"}, "d301054c2819e1a21480800fdabbe5ae909abe09": {"ta_keywords": "annotations natural language;language annotations natural;natural language annotations;annotations natural;abstract reasoning scenes;learning natural language;language annotations;natural language;learning dreamcoder laps;library learning dreamcoder;annotations;reasoning scenes inductive;learning natural;scenes inductive program;reasoning scenes;synthesis inferring programs;method learning natural;inductive program synthesis;program synthesis;program synthesis inferring;library learning;composition abstract reasoning;effective program synthesis;learning dreamcoder;learning;inferring programs examples;art library learning;programs examples desired;dreamcoder laps;strong library", "pdf_keywords": "generative models programs;reasoning scene representations;learned synthesis;representation reasoning scene;compositional generative architecture;learned synthesis algorithms;learning abstractions;generative architecture;learning abstractions abstract;language learned synthesis;program abstractions neural;generative architecture learning;abstract reasoning graphics;base learned synthesis;architecture learning abstractions;compositional generative;introducing jointly generative;generative;presents compositional generative;generative models;learned synthesis paper;abstractions neural;representations based compositional;representation reasoning;reasoning graphics domain;jointly generative models;jointly generative;reasoning scene;proof assistant language;abstractions neural search"}, "27b7489bd54dfd585edd2ba0da3920a31e7fd8b5": {"ta_keywords": "continuous sensorimotor games;learning continuous games;games dynamic human;sensorimotor games dynamic;dynamics sensorimotor games;learning using teleoperation;sensorimotor games;continuous games derive;human sensorimotor learning;continuous games;learning behaviors humans;humans interact dynamic;sensorimotor games arise;play humans machines;sensorimotor learning;learning continuous;continuous sensorimotor;model interaction game;interaction game;interacting agents humans;sensorimotor learning using;learning behaviors;transient play humans;games dynamic;transient learning behaviors;games derive predictions;human machine interaction;dynamic human machine;humans interacting agents;study continuous sensorimotor", "pdf_keywords": ""}, "af38829cdb55ee7b71d49399f71397d975e40a95": {"ta_keywords": "answering complex questions;complex questions long;compositional logical reasoning;answering complex;logically complex;questions long documents;compositional logical;long context documents;related logically complex;approach answering complex;entanglement subsystems;questions require compositional;complex questions;logically complex ways;conditional answers dataset;possible topological orders;entanglement;context documents;topological phase space;interplay structural;topological orders way;topological orders;information related logically;conditional answers;require compositional logical;degree entanglement subsystems;interplay structural dynamical;questions multiple;context documents information;properties topological phase", "pdf_keywords": "model answer answering;reading comprehension models;comprehension models;conditional answers dataset;compositional answer answering;answering long documents;aware language models;comprehension models face;conditional answers;answer answering;answers time compositional;answers dataset features;knowing answers time;answers dataset;language models;language models proposed;questions conditionalqa long;traditional reading comprehension;long context documents;temporal knowledge bases;answer answering long;reading comprehension;extractive questions;reasoning combination extractive;policy question document;knowing answers;time aware language;addition conditional answers;answering long;answerable questions"}, "2c2234548de4694b6455a19cd0d85a9d6c473456": {"ta_keywords": "library similarity searching;search similarity library;searching similarity search;searching similarity;similarity searching;search similarity;similarity searching similarity;similarity search;based search similarity;similarity search based;library similarity;similarity library library;similarity library;describes library similarity;search methods nonmetric;metric spaces tools;tools measure search;approximate search methods;search quality library;measure search quality;search methods spaces;metric space access;nonmetric metric spaces;methods nonmetric metric;approximate search;nonmetric metric;new search methods;search quality;search methods;search based", "pdf_keywords": "dimensional similarity search;pivot neighborhood indexing;computing aggregate ranking;similarity search;neighborhood indexing;similarity search based;similarity nn search;neighborhood indexing method;ranking np complete;library fast similarity;aggregate ranking np;indexing method sparse;aggregate ranking;fast similarity nn;sparse data sets;fast similarity;queries sparser documents;searching metric;search based pivot;toolkit searching metric;indexing;especially queries sparser;ranking np;ranking;high dimensional similarity;comprehensive toolkit searching;queries sparser;dimensional similarity;search based;sparse data"}, "c00ba15810496669d47d2ed5b627e6c7d2b1f6aa": {"ta_keywords": "machine translation;machine translation used;speed machine translation;multi document paraphrasing;unsupervised multi lingual;machine translation paper;approach machine translation;including paraphrase translation;document paraphrasing;document paraphrasing objective;translation multi document;tasks including paraphrase;paraphrasing;masked language modeling;paraphrase translation multi;paraphrasing objective;multi document summarization;discriminative generative tasks;summarization information retrieval;reconstruction target text;language modeling;multi lingual;lingual multi document;generative tasks languages;learn retrieval reconstruction;jointly learn retrieval;translation multi;text pre training;multi lingual multi;generative tasks", "pdf_keywords": "trained machine translation;machine translation models;unsupervised machine translation;machine translation model;machine translation;machine translation machine;new machine translation;translation models;machine translation refer;translation machine;manually translated sentences;trained models nlp;models nlp tasks;translation model outperforms;translation model;translation multidocument summarization;translation models \ufb01ne;language modeling self;language understanding generation;models natural language;document translation;models nlp;language modeling;including document translation;documents manually translated;marage machine translation;translation machine paper;reconstruction target text;nlp tasks;translate documents manually"}, "4b890b6ded71f005414e55adb87c23efd437ef95": {"ta_keywords": "parametric speech synthesis;synthetic speech quality;speech synthesis quality;speech synthesis;synthesis synthetic speech;speech synthesis synthetic;speech synthesis perform;concatenated speech synthesis;statistical parametric speech;speech synthesis offers;concatenative speech synthesis;synthetic speech;speech synthesis including;ms synthetic speech;voice conversion;speech parameter trajectories;parametric speech;speech parameter;smoothing effect speech;tts voice conversion;synthetic speech close;speech proposed postfilters;speech propose postfilters;speech quality;effect speech parameter;quality natural speech;speech tts voice;speech quality good;text speech tts;voice conversion vc", "pdf_keywords": ""}, "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7": {"ta_keywords": "pre trained byte;byte level models;trained byte level;trained byte;trained byte byte;byte level transformer;byte byte swimmer;pronunciation token free;byte level;raw text bytes;byte;pre trained language;trained language models;text bytes characters;byte swimmer;text bytes;bytes characters;inference speed byte;trained language;byte byte;text preprocessing pipelines;speed byte;speed byte level;demonstrate byte level;text language;bytes characters benefits;pronunciation token;language models;process text language;bytes", "pdf_keywords": "trained machine translation;machine translation framework;language model transfer;machine translation;computer language tokenization;language tokenization;large corpus;translation framework;text translation;pre trained byte;trained language model;language tokenization called;framework text translation;tokenization called mt5;model transfer learning;pre trained language;translation framework text;large corpus unlabeled;transfer learning;tokenization;trained language;translation shown outperform;model computer language;trained byte;language model;mc4 large corpus;trained byte level;multilingual text data;corpus;corpus unlabeled multilingual"}, "041b2510e54b2504890cb9f58b9bbc5601f35e3e": {"ta_keywords": "elastic tweezer;outcome elastic tweezer;elastic tweezer presence;numerical study motion;tweezer presence applied;study motion particle;tweezer;numerical study;incrementally assignment;applied electric field;elastic;incrementally assignment assignments;electric field;motion particle;tweezer presence;nlp end course;research nlp;level research nlp;research nlp end;numerical;force motion particle;predicting outcome elastic;study motion;motion;motion particle ring;assignment assignments;nlp;results numerical study;nlp end;outcome elastic", "pdf_keywords": ""}, "399ab2a0eddf7a7abf776241d5c0a2c4cd5bf313": {"ta_keywords": "model speech recognition;discriminative model speech;speech signal searches;speech recognition;speech recognition directly;estimate speech model;continuous speech recognition;parameters speech model;model speech;speech model;speech model represented;acoustic language model;speech model composed;estimate speech;speech recognition based;given input speech;model continuous speech;discriminative training approaches;discriminative training;recognition decoder;parameters speech;recognition based weighted;input speech;optimizes parameters speech;discriminative model;performance discriminative training;input speech signal;recognition decoder given;parameters acoustic language;performance discriminative", "pdf_keywords": ""}, "4b9bc5b5985bbfbc860039b98f233f8a43ad9171": {"ta_keywords": "pdes especially uncertainties;equations pdes especially;pde captured features;pde curse dimensionality;pdes especially;numerical partial differential;pdes;neural network parameterise;quantities derived pde;especially uncertainties modelled;neural network;pde;partial differential equations;using neural network;using neural;equations pdes;numerical partial;neural network used;pde captured;differential equations pdes;uncertainties modelled;propose using neural;modelled random coefficients;uncertainties modelled random;neural;pde curse;solution pde;variability physical quantities;used solution pde;dimensionality commonly", "pdf_keywords": "learning thermodynamics boltzmann;pde using neural;solution learning thermodynamics;equations convolutional neural;learning thermodynamics;boltzmann machines;solution deterministic pde;thermodynamics boltzmann machines;pde provide theoretical;solve solution learning;boltzmann machines important;deterministic pde;solving pde;deterministic pde using;network important pdes;solving pde provide;technique solving pde;equations convolutional;thermodynamics boltzmann;solution learning;neural network;pde;using neural network;boltzmann;energy nonlinear;neural network important;cerential equations convolutional;neural network particular;pdes wide applications;using neural"}, "302face5b5a0944cab13665a2d4e07ef3aaf5240": {"ta_keywords": "domain question answering;question answering;question answering task;question answering algorithm;answering algorithm;open domain scoring;answering;answering task involves;answering task;answering algorithm used;questions nq open;question answer pairs;nq open ambiguous;open ambiguous;answer questions;questions single unambiguous;nq open;quark hqc;ambigqa new open;single unambiguous answer;questions nq;answer pairs plausible;unambiguous answer new;domain scoring;new open;domain scoring scoring;open ambiguous exhibiting;answer questions single;heavy quark hqc;half questions nq", "pdf_keywords": "answering question disambiguation;domain question answering;question answering;question disambiguation;answering captures ambiguity;question answering captures;question answering systems;ambiguity entity references;question disambiguation present;question disambiguation paper;disambiguation;entity references;disambiguation paper investigate;disambiguation present setup;ambiguity retrieval;captures ambiguity retrieval;disambiguation present;disambiguation paper;baseline methods answering;information seeking open;types question disambiguation;methods answering;wikipedia open domain;google search answer;wikipedia open;answering question;answering systems;annotations weak supervision;entity references event;question answering present"}, "d86227948b6000e5d7ed63cf2054ad600b7994a0": {"ta_keywords": "words models deepening;models natural language;syntactically aware models;outperforms models sentiment;natural language processing;words models;question answering;syntactically aware;learning compositionality inputs;models sentiment;sentiment analysis factoid;deep learning models;deep neural;models sentiment analysis;bag words models;simple deep neural;question answering tasks;learning compositionality;deep learning;models deepening network;datasets existing deep;existing deep learning;deepening network;focus learning compositionality;models deepening;sentiment analysis;natural language;simple deep;learning models natural;model syntactically ignorant", "pdf_keywords": ""}, "8c4b187bdaf91bf068adfe005a0463c4f9c36387": {"ta_keywords": "deep architectures binary;powerful deep architectures;deep architectures;pixel wise losses;vision exploit refinement;prediction delineation curvilinear;pixel wise loss;delineation refine predictions;entropy advent deep;deep learning;prediction delineation;final prediction delineation;iterative refinement boost;trained binary cross;deep learning current;advent deep learning;predicted delineations;refine predictions;classifier trained binary;finding powerful deep;trained binary;quality predicted delineations;division pixel wise;vision exploit;computer vision;refine predictions step;pixel wise;division pixel;deep;computer vision exploit", "pdf_keywords": "predicting neuronal boundaries;architecture image segmentation;novel deep neural;deep neural;deep network;boundaries computer vision;architecture predicting neuronal;road image;deep network learning;propose deep network;network architecture image;deep neural network;predicting neuronal;image segmentation;aerial road image;architecture image;neuronal boundaries computer;novel deep;road image addition;neuronal boundaries;neural;topology loss;recent detection segmentation;vision imagery great;neural network architecture;wise loss topology;microscopy aerial images;vision imagery;segmentation;network learning"}, "8147a495b9a933742f06458244f7c5df00767c4e": {"ta_keywords": "open information extraction;assertions natural language;information extraction;supervised open interactive;information extraction task;extraction task extracting;extracted different sentences;supervised open;model learn trial;natural language;model learn;assertions extracted;extractions generated model;extraction likelihood confidence;quality assertions extracted;natural language sentences;task extracting;extraction likelihood;learning;supervised;current supervised open;extraction task;sentences;open information;extractions generated;open domain assertions;task extracting open;quality assertions;extracting;help model learn", "pdf_keywords": "open information extraction;open semantic natural;open language learning;comparable open semantic;open semantic;assertions natural language;automatically extract arguments;information extraction;candidate extractions;list candidate extractions;candidate extractions based;natural language processing;extraction information web;extraction task extracting;extract arguments open;information extraction task;approach automatically extract;semantic natural language;automatically extract;rank aware learning;quality open language;open domain assertions;task extracting open;extract arguments;extraction information;extractions generated open;extracting open;algorithm ranks extractions;extracting open domain;approach extraction information"}, "43e8e371449aaef34c2f43ae90f2157fd5a617bd": {"ta_keywords": "pricing socially optimal;control nash equilibrium;strategy selfish agents;control strategy selfish;equilibrium agents convex;nash equilibrium agents;nash equilibrium;pricing mechanisms;socially optimal feedback;socially optimal cost;selfish agents;pricing socially;agents convex;selfish agents paper;feedback control nash;team selfish agents;dynamics team selfish;strategy selfish;use pricing mechanisms;selfish agents forced;pricing mechanisms means;control nash;fairness weighted pricing;incurred pricing socially;socially optimal;zero sum game;weighted pricing;equilibrium agents;agents convex addition;insure stability multinetwork", "pdf_keywords": ""}, "830a396a4a77567caad1c155dd3b22597314e9f3": {"ta_keywords": "fairness social networks;fairness machine learning;fairness aware systems;aware systems fairness;algorithmic fairness;algorithmic fairness poor;systems fairness;model algorithmic fairness;systems fairness machine;outcomes fairness aware;problem fairness social;fairness aware;fairness machine;fairness social;based predefined fairness;fairness fairness;predefined fairness;fairness metric;predefined fairness metric;term fairness fairness;problem fairness;fairness metric paper;outcomes fairness;sharing term fairness;approach problem fairness;fairness poor;perspectives outcomes fairness;fairness;term fairness;fair sharing term", "pdf_keywords": ""}, "5083d9e25113a09faeba7d56b7808e2f77b5c15e": {"ta_keywords": "multi hop reasoning;symbolic knowledge base;large symbolic knowledge;hop templates neural;hop reasoning using;hop reasoning present;reasoning present efficient;hop reasoning;knowledge base;symbolic knowledge;multi hop templates;reasoning using large;knowledge base short;hop templates;multi hop;templates neural;multihop court competition;using large symbolic;reasoning using;requiring multi hop;multihop court;order multi hop;templates neural model;court competition;hop;implementations second order;court competition introduce;millions entities;learning tasks requiring;competitive performance learning", "pdf_keywords": "training semantic parsers;semantic parsing end;semantic parsers end;semantic parser end;automatic semantic parsing;semantic parsers;semantic parsing;semantic parser;semantic parsing using;hop semantic parser;training semantic;results semantic parsing;automatic semantic;question answering;parsing;answer question answering;parsers;parsing end;approach training semantic;parsing end end;parser;parsers end;parser end;question answering association;answering association knowledge;knowledge base;parsers end end;parser end end;parsing using;association knowledge base"}, "4b762c0344f14bb00d590f5666c27b3aac7b0a7d": {"ta_keywords": "rnn language model;recurrent neural network;performance recurrent neural;recurrent neural;rnn language;network rnn language;neural network rnn;performance recurrent;improve performance recurrent;quantum information processing;swapping distant quantum;entanglement swapping distant;entanglement swapping;quantum;model incorporating syntactic;neural models letter;based entanglement swapping;processing based entanglement;network rnn;language model;quantum systems;entanglement;neural;language modelling;neural models;models neural;novel approach quantum;distant quantum;language model incorporating;approach quantum", "pdf_keywords": "rnn language model;machine translation improves;machine translation speech;recurrent neural network;machine translation machine;model machine translation;trained dependency parse;improve performance recurrent;performance recurrent neural;machine translation;language models dependency;learning language models;network rnn language;rnn language;translation machine translation;msr sentence completion;translation machine;language model trained;task machine translation;neural network language;performance recurrent;model incorporating syntactic;neural network rnn;incorporating syntactic dependencies;propose recurrent neural;dependency parse trees;language models;sentence completion task;translation speech recognition;neural model machine"}, "bcd45c86e1bcf8d1411eb6704c4c58d0831b5b4f": {"ta_keywords": "models based poisson;based multinomial distribution;multinomial distribution;occurrence sexually transmitted;models based multinomial;statistical models text;poisson negative binomial;poisson;binomial distributions desirable;negative binomial distributions;based poisson;frequencies occurrence;frequencies occurrence sensible;statistical models;occurrence sexually;binomial distributions;words higher frequencies;transmitted disease duration;tractability statistical models;statistical models used;based multinomial;occurrence;higher frequencies occurrence;poisson negative;statistical;tractability statistical;present statistical models;based poisson negative;disease duration flagellar;multinomial", "pdf_keywords": ""}, "0ad8284dbae11901a725cc71318a165c08852278": {"ta_keywords": "approach voice conversion;spoken speakers probabilistic;model voice conversion;voice conversion;speakers probabilistic integration;voice conversion using;voice conversion proposed;speakers probabilistic;model parallel utterances;utterances speaker model;parallel utterances speaker;content spoken speakers;parallel utterances;speaker model voice;utterances speaker;model spoken spoken;spoken spoken utterances;spoken utterances;density model speaker;speaker model proposed;especially parallel corpus;parallel corpus;require parallel corpus;model voice;plenty utterances linguistic;spoken speakers;speaker model nonparallel;utterances linguistic content;propose probabilistic unification;probabilistic unification", "pdf_keywords": ""}, "b21b927c251c415b601b6d7f785a42cc5c292635": {"ta_keywords": "concept coreference clusters;coreference clusters generalization;generalization coreference cluster;coreference clusters;coreference cluster;scientific knowledge graph;scientific information extraction;clusters generalization coreference;coreference cluster concept;annotations tasks;annotations tasks presents;sentence relations coreference;includes annotations tasks;relations coreference links;sciie shared span;coreference links;coreference links paper;generalization coreference;clusters scientific articles;concept coreference;information scientific literature;scientific literature framework;relations coreference;coreference;models scientific information;scientific literature;introduce concept coreference;annotations;information extraction;includes annotations", "pdf_keywords": "semantic scholar corpus;extract organize scienti\ufb01c;relations coreference clusters;coreference clusters scienti\ufb01c;information extraction;information extraction entities;scholar corpus;organize scienti\ufb01c information;coreference clusters;organizing scienti\ufb01c information;semantic scholar;extraction entities relationships;scienti\ufb01c knowledge graph;scienti\ufb01c information structured;requires information extraction;extraction entities;using semantic scholar;articles knowledge graph;organize extracted information;knowledge graph scienti\ufb01c;structured knowledge bases;coreference linking;sentence coreference linking;organizing scienti\ufb01c;organize scienti\ufb01c;knowledge bases;relations extracted article;relations extracted;corpus 110k abstracts;entities relations coreference"}, "34fb3e21a63fb2987f7a87f88ecf49aea53cff36": {"ta_keywords": "cooperative persuasive dialogue;persuasive dialogue policies;learning cooperative persuasive;persuasive dialogue;policies using framing;cooperative persuasive;dialogue policies;dialogue policies using;framing;framing framing;using framing;persuasive;using framing framing;dialogue;approach learning cooperative;learning cooperative;approach learning;cooperative;new approach learning;policies using;policies;learning;paper propose;propose new approach;paper propose new;approach;new approach;propose new;propose;paper", "pdf_keywords": ""}, "95cedaeb3178a4671703a05171a144e6b964a819": {"ta_keywords": "modeling crowdsourced speech;crowdsourced speech recognition;crowdsourced speech;modeling crowdsourced;approach modeling crowdsourced;language models;crowdsourcing;concept crowdsourcing;based concept crowdsourcing;count based neural;crowdsourced;speech recognition;concept crowdsourcing demonstrate;speech recognition based;crowdsourcing demonstrate;probabilities sequences words;approaches language models;count based;mixture weights distributions;language models lmms;probability distributions vocabulary;models calculate probabilities;words dynamically calculates;distributions vocabulary words;distributions vocabulary;calculates mixture weights;mixture weights;vocabulary words dynamically;crowdsourcing demonstrate varieties;statistical models", "pdf_keywords": "rnn language models;neural language models;neural nets ngram;network rnn language;neural gram model;neural lms;language models widely;rnn language;language models;recurrent neural network;language modeling;approaches recurrent neural;network rnn;neural lms achieve;neural network rnn;language modeling exist;language models experiments;neural language;hybrid neural gram;gram models;speed neural lms;based gram models;nets ngram models;neural neural language;neural nets;neural gram;recurrent neural;dropout randomly;gram models advantages;paradigms language modeling"}, "a13d8400813743adb22ba0bd0570c49af2675a39": {"ta_keywords": "continuous speech separation;speech separation;task speech separation;block level separation;speech separation css;speech separation aiming;blocks performing separation;separation performance improved;overlapped recording based;segmenting long recording;partially overlapped recording;separation performance;level separation performance;overlapped recording;sentence level separation;performing separation independently;performing separation;separating overlap free;rnn dprnn architecture;separation independently;separating overlap;separation models;level separation models;long recording;separating;separation models task;interleaved intra interblock;separation;level separation;separation css emerging", "pdf_keywords": ""}, "54316d2861eb3d575a8c7d071f4cf7c2fc30be01": {"ta_keywords": "certifiably robust classifiers;classifiers trained robust;trained robust;robust classifiers pre;trained trained robust;robust classifiers;certifiably robust;create certifiably robust;empirical robustness;adversarial;empirical robustness possible;pipeline randomized smoothing;concept empirical robustness;classifiers pre trained;classifiers trained;pre trained classifier;pre trained classifiers;adversarial attacks;susceptible adversarial;robustness possible create;adversarial attacks paper;susceptible adversarial attacks;extremely domain trained;classifiers pre;classifier retraining;trained classifiers;trained classifiers trained;classifier retraining pre;robustness possible;pipeline randomized", "pdf_keywords": ""}, "73bbd0b53044e9f518a3596a3607521bbce12fc2": {"ta_keywords": "supervised cnn segmentation;cnn segmentation;cnn segmentation demonstrate;weakly supervised cnn;shallow segmentation;supervised cnn;training deeper;cnn;function germanium detector;training deeper complex;germanium detector;problem shallow segmentation;shallow segmentation paper;segmentation problem shallow;germanium detector gd;method training deeper;detector context weakly;detector;segmentation;loss function germanium;segmentation demonstrate motivated;segmentation demonstrate;detector context;context weakly supervised;optimization loss functions;deeper complex neural;global solver segmentation;optimization loss;detector gd;local optimization loss", "pdf_keywords": ""}, "a5b1d1cab073cb746a990b37d42dc7b67763f881": {"ta_keywords": "learns representations nl;like semantic parsing;natural language nl;nl understanding models;semantic parsing structured;text based natural;representations nl sentences;natural language;language nl understanding;semantic parsing;parsing structured;based natural language;tasks like semantic;sentences semi structured;language models;learns representations;language models lmms;jointly learns representations;like semantic;pretrained language models;representations nl;nl understanding;language nl;parsing structured data;parsing;jointly learns;nl sentences;algorithm jointly learns;nl sentences semi;semantic", "pdf_keywords": "semantic parsing aligned;learns parse database;learning semantic parsers;data semantic parsing;deep semantic parsing;model semantic parsing;semantic parsing text;column semantic parsing;parsing semantic;semantic parsing;semantic parsing known;semantic parsing semantic;semantic parsers;semantic parsing paradigms;semantic parsers denotations;semantic parsers expert;learning semantic;parsing semantic parsers;parsing text sql;learns parse;model learns parse;learning queries;parse database queries;parsing paradigms;deep semantic;parse database;parsers expert knowledge;parsers denotations;parsers denotations latent;parsing aligned textual"}, "7e870eb8d580fb1b8b7a8f97d94d67555a225635": {"ta_keywords": "expert searchers;expert search;expert search active;models expert searchers;intelligent message addressing;auto completion experiments;corpus frequently email;auto completion;methods expert search;models auto completion;potential recipients message;recipients message composition;searchers;frequently email;frequently email users;specified recipients;previously specified recipients;message addressing problems;message addressing;task intelligent message;subject message addressing;intelligent message;recipients message;letters intended recipient;recipients;intended recipient contact;recipient contact;large corpus frequently;finding persons;recipients initial letters", "pdf_keywords": ""}, "267b94325028e0e2e6da1ae2cbe7f7a93284722e": {"ta_keywords": "graph similarity measures;social networks reranking;structural graph similarity;graph walk similarity;analyzing social networks;networks content social;networks reranking;graph similarity;social networks content;content social networks;similarity metrics documents;online social networks;social networks timeline;social network interesting;walk similarity measures;similarity measures text;networks reranking schemes;social networks;social network;online social network;lazy graph walk;similarity measures;based graph walk;similarity metrics;walk similarity;lazy graph;extended similarity metrics;social influence online;similarity measures outperform;graph walk", "pdf_keywords": ""}, "a309ad4c4088843d230be1a85806960e633e1e46": {"ta_keywords": "deep learning;pipeline nlp;deep learning models;pipeline nlp currently;processing pipeline nlp;models learn;models training;development deep learning;learning models training;development deep;learning models;clear models learn;biases annotation artifacts;learning;nlp currently;signal processing pipeline;biases annotation;nlp;models learn kinds;models training data;training data;deep;processing;pipeline;models;processing pipeline;algorithmic;signal processing;annotation;annotation artifacts", "pdf_keywords": "linguistics ai ethics;computational linguistics ai;data2 linguistic ethical;linguistics ai;curating data2 linguistic;modeling nlp work;ai ethics;modeling nlp;linguistic ethical perspectives;nlp work based;nlp work;ai ethics able;linguistic ethical;deep learning;nlp;paradigm modeling nlp;world encode interdisciplinary;garbage learn idea;data2 linguistic;computational linguistics;ai;machine learning;encode interdisciplinary;train machine learning;approaches computational linguistics;learning systems answer;machine learning systems;core deep learning;linguistic;deep learning dl"}, "1be28ce9a1145c2cf4f78e6c494a4c15397fbac3": {"ta_keywords": "summarization graph neural;knowledge summarization graph;based subgraph summarization;subgraph summarization;subgraph summarization scheme;attention based subgraph;summarization graph;sumgnn knowledge summarization;knowledge summarization;subgraph extraction;subgraph extraction module;based subgraph;enabled subgraph extraction;graph neural;biomedical knowledge;external biomedical knowledge;subgraph;graph neural network;subgraph multi;biomedical knowledge significantly;summarization scheme generate;channel knowledge data;knowledge data;reasoning path subgraph;network enabled subgraph;summarization scheme;subgraph multi channel;sumgnn knowledge;knowledge data integration;subgraphs", "pdf_keywords": "summarized subgraph embedding;neural network summarization;summarize subgraph information;predicting drug interactions;knowledge graph embedding;graph neural;network summarization;graph embedding;drug interaction prediction;network summarization scheme;subgraph embedding chemical;new graph neural;networks knowledge graph;summarize subgraph;subgraph embedding;predicting drug;graph neural network;approach predicting drug;graph based pathway;knowledge graph;module summarize subgraph;summarized subgraph;graph embedding paper;medicine machine learning;subgraph information;subgraph information graph;information graph based;networks knowledge;graph based;framework identifying drug"}, "ce6143e24a455edc233f12933e9903426b963799": {"ta_keywords": "statistical topic models;topic models;topic models latent;models latent dirichlet;latent dirichlet al;large document collec;summarize large document;latent dirichlet;variational electrom algorithm;parallel implementation variational;implementation variational electrom;electrom algorithm multiproces;fast scalable work;electronic document col;document col lections;scalability statistical topic;document collec;models fast scalable;variational electrom;fast scalable;electrom algorithm;multiproces sor architecture;algorithm multiproces sor;large document;modern electronic document;document col;present parallel implementation;electronic document;multiproces sor;implementation variational", "pdf_keywords": ""}, "51bf7a3aee6b1f61b902625f6badffedf200d31a": {"ta_keywords": "rules generative;rules generative model;generative models deep;model gan learns;models deep generative;deep generative;change rules generative;deep generative model;gan learns model;generative model gan;generative models;generative model achieve;gan learns;state art generative;generative model;model gan;generative;quantum network;nodes quantum network;models deep;manipulating layer deep;learns model;rules encoded network;learns model rich;remote nodes quantum;nodes quantum;deep network;layer deep network;layer deep;art generative models", "pdf_keywords": "images editing generative;editing generative;editing generative model;model editing enhancement;image generation;generation model editing;image generation model;image editing;model editing;visual editing;editing enhancement;images editing;modern image editing;method visual editing;new images editing;image editing tools;editing enhancement applications;train image generation;visual editing used;editing;generative model;generative;layer deep generator;deep generator;convolutional layer deep;memory nonlinear convolutional;deep generator contrast;editing tools achieve;new images;editing tools"}, "990c7726dd31723f97a364828d5191080fe7ec2d": {"ta_keywords": "topological quantum computation;topological quantum computing;topological superconductor majorana;universal topological quantum;topological quantum;topological superconductor;chiral topological superconductor;majorana based quantum;quantum computation;quantum computing;results quantum computation;topological superconductor decomposed;quantum computation strongly;superconductor majorana based;quantum circuit cnot;superconductor majorana;propose quantum circuit;quantum computing letter;based quantum gates;quantum computation fibonacci;quantum computation means;entangling quantum;quantum circuit;quantum gates;conventional quantum circuit;algorithm universal topological;quantum gates complete;quantum circuit models;scheme entangling quantum;performing universal topological", "pdf_keywords": "universal quantum gates;topological quantum gates;gates quantum computation;universal quantum gate;quantum gates topological;adder quantum gates;quantum gates;quantum computation;quantum logic gates;gates quantum;computation quantum;quantum gates shor;gates topological elements;quantum computation quantum;quantum gate proposed;arbitrary quantum computer;quantum computer;logic gates quantum;quantum circuits;universal quantum;quantum gate;quantum circuit;quantum circuits modular;quantum circuit model;topological quantum;universal set quantum;model universal quantum;fermion number operator;gates topological;present quantum circuit"}, "6fae71765a5e86dfef2f93bbe03c4a2e20f827b5": {"ta_keywords": "robotic speech recognition;speech segmenter;non speech segmenter;robotic speech;performance robotic speech;speech recognition core;speech recognition;recognition speech recognition;speech recognition speech;recognition speech;gmm based speech;speech recognition 2015;recognition core speech;evaluation automatic speech;automatic speech recognition;speech processing;automatic speech;speech segmenter paper;core speech processing;speech processing technologies;algorithm recognizer;hidden markov model;new algorithm recognizer;model hidden markov;gaussian mixture model;hidden markov;based speech;core speech;recognizer output voting;speech non speech", "pdf_keywords": ""}, "b9e6c65aacfe8ecc1b7833b47803672273a918ec": {"ta_keywords": "behavior pedestrian crowded;dynamics pedestrian crowded;pedestrian crowded environment;behavior pedestrian;dynamics pedestrian;pedestrian crowded;environment dynamics pedestrian;crowded environment dynamics;pedestrian;pedestrian located distance;pedestrian located;crowded environment;case pedestrian;environment case pedestrian;crowded environment case;case pedestrian located;relationship behavior pedestrian;xmath0 center crowd;crowded;behavior;distance xmath0;located distance xmath0;distance xmath0 center;crowd;dynamics;center crowd;xmath0;environment dynamics;xmath0 center;distance", "pdf_keywords": ""}, "cf08bef866885edb8b001deb18e582eec94c51de": {"ta_keywords": "features speech summarization;speech summarization open;automatic speech summarization;speech summarization;spontaneous speech summarization;speech summarization checking;speech summarization paper;speech summarization technique;method speech summarization;summarization open domain;human summarization;similarity human summarization;summarization checking semantic;automatic speech;human summarization experiments;semantic acoustic features;summarization paper propose;features speech;summarization open;acoustic features speech;summarization checking;summarization experiments;summarization experiments reveal;summarization paper;summarization technique;summarization technique paper;incorporate semantic acoustic;summarization;use semantic acoustic;semantic acoustic", "pdf_keywords": ""}, "b790c3e712c92065d596364af81a494adbc62c39": {"ta_keywords": "distributionally robust optimization;neural generative models;generative models;large scale generative;distributionally robust;neural generative;robust optimization;generative models develop;robust optimization dro;generative models characterize;constrained inner maximization;learning models able;gradient based optimization;optimization large scale;generative;use neural generative;learning models;set distributionally robust;inner maximization objective;training machine learning;models robust;model selection heuristics;scale generative models;optimization challenges;inner maximization;machine learning models;based optimization large;models robust comparable;machine learning;uncertainty set distributionally", "pdf_keywords": ""}, "43d5c00938bd2acb1aca8e81a7d220025eddbc23": {"ta_keywords": "historischer deutscher texte;normalisierung historischer deutscher;normalisierung historischer;die normalisierung historischer;deutscher texte;historischer deutscher;f\u00fcr die normalisierung;die normalisierung;guidelines f\u00fcr die;historischer;normalisierung;guidelines f\u00fcr;guidelines;deutscher;texte;f\u00fcr die;f\u00fcr;die", "pdf_keywords": ""}, "6695d3b92e7cd7f2359f698a09c7b3dc37996329": {"ta_keywords": "enrich vision language;label augmented pretraining;representation learning pretraining;vision language;visual language tasks;learning pretraining;novel pretraining task;vision language pairs;visual language;visual objects enrich;quality vision language;augmented pretraining;various visual language;state art neuroimaging;labels visual objects;pretraining task;modal representation learning;augmented pretraining model;learning pretraining increasing;art neuroimaging;labels visual;novel pretraining;visual objects;representation learning;art neuroimaging model;generated labels visual;objects enrich vision;pretraining model;auto generated labels;enrich vision", "pdf_keywords": "labelaugmented multimodal pretraining;multimodal pretraining;multimodal representation learning;labelaugmented multimodal;modal representation learning;multimodal pretraining leverage;multimodal representation;named labelaugmented multimodal;learning vision language;propose multimodal representation;multimodal;representation learning vision;vision natural language;label augmented pretraining;modality automatic language;propose multimodal;cnn model learn;learn multi modal;propose deep convolutional;vision language;modality automatic;multi modal representation;deep convolutional;deep convolutional neural;learning vision;cross modal alignment;convolutional neural;pretraining downstream tasks;representation learning;augmented pretraining"}, "887d84c1310c6e71a0f89874ef9985b65a44c855": {"ta_keywords": "discriminative feature transforms;information acoustic feature;speech recognition;acoustic feature space;gaussian discriminative feature;discriminative feature;feature transforms;mutual information acoustic;attracted speech recognition;acoustic model training;speech recognition community;feature transform;acoustic feature;feature transforms using;criterion discriminative feature;estimating feature transformation;transformed features weighted;feature compensation techniques;discriminative criteria proposed;feature transforms demonstrate;information acoustic;summing transformed features;features weighted posterior;gaussian discriminative;discriminative criteria;feature transformation;features weighted;recognition performance;feature compensation;feature transformation parameters", "pdf_keywords": ""}, "46bf4bece58764d22764acfd3d232b50fb7767f9": {"ta_keywords": "cnns classifying alzheimer;deep learning neuroimaging;classifying alzheimer;classifying alzheimer disease;learning neuroimaging deep;neuroimaging deep neural;neuroimaging deep;ad structural neuroimaging;learning neuroimaging;alzheimer disease ad;structural neuroimaging signatures;deep neural;neuroimaging signatures;alzheimer disease;analyses identified hippocampal;ad classification;alzheimer;deep convolutional neural;ad classification identify;deep learning;deep net trained;deep learning impact;identified hippocampal;classification identify ad;cnns classifying;structural neuroimaging;deep net;neuroimaging signatures regional;networks cnns classifying;cortex predictive region", "pdf_keywords": ""}, "6f173939f6defe3ebae8fb12f19349ba96b7b5c4": {"ta_keywords": "speaker correspondence based;intersubsequence speaker correspondence;speaker correspondence;correspondence based unsupervised;speaker correspondence obtained;subsequences speaker counting;attractors subsequences speaker;problem speaker correspondence;diarization results intersubsequence;unsupervised clustering vectors;based diarization subsequence;diarization methods diarization;diarization promising approach;diarization methods;attractor based diarization;subsequences speaker;results intersubsequence speaker;end diarization methods;correspondence obtained unsupervised;accurate diarization;accurate diarization results;subsequence wise diarization;clustering attractors subsequences;diarization promising;unsupervised clustering;based diarization;produce accurate diarization;unsupervised clustering attractors;intersubsequence speaker;clustering vectors", "pdf_keywords": "clustering speakers diarization;unsupervised clustering speakers;training speaker mixtures;clustering speakers;speakers diarization;end neural diarization;speaker mixtures evaluation;diarization multi talker;speaker mixtures widely;recordings using unsupervised;speakers diarization important;unsupervised clustering;introduce unsupervised clustering;diarization eend;neural diarization;observed training speaker;diarization eend eda;neural diarization eend;created speaker mixtures;speaker mixtures;training speaker;using unsupervised clustering;unsupervised clustering process;accurate diarization;produce accurate diarization;multi talker recordings;diarization important task;accurate diarization results;talker recordings;speakers observed training"}, "1fb88c130bedcd2e75fd205b70af2999c6a8c49d": {"ta_keywords": "security future voting;agency security future;impact national security;security agency security;national security;agency security;national security agency;agency security agency;security agency;security future;security;future voting;investigation impact national;reports results investigation;voting;results investigation impact;investigation impact;results investigation;report reports;reports results;report reports results;investigation;agency;impact national;reports;national;results;future;report;impact", "pdf_keywords": ""}, "6ad56b1b776a2c448fc90c543b50756941e5a119": {"ta_keywords": "energy efficiency incentive;incentives estimating consumer;energy consumption patterns;estimating consumer utility;consumers demand response;energy consumption;consumer utility;consumer utility function;relationship energy consumption;utility function energy;consumers demand;utility learning;patterns consumers demand;demand response behaviors;efficiency incentive design;consumption patterns consumers;utility company consumer;utility learning shown;incentive design utility;efficiency incentive;energy efficiency;design utility learning;designing incentives estimating;estimating consumer;incentives estimating;consumption patterns;algorithm designing incentives;demand response;improving energy efficiency;consumer interaction principal", "pdf_keywords": ""}, "bb6317bbd2c4a81e94cf3d7eb1b73da246a022db": {"ta_keywords": "predicting word nearest;learning similarity sequences;learning similarity;nearest neighbor trained;word nearest neighbor;neural language model;easier predicting word;language modeling long;predict nearest neighbor;trained neural language;human language model;text easier predicting;predicting word;nearest neighbor search;language modeling;similarity sequences text;domain adaptation;word nearest;language model particularly;similarity sequences;predict nearest;paper learning similarity;nearest neighbor;neighbor datastore training;language model;neural language;effective domain adaptation;domain adaptation simply;pre trained neural;varying nearest neighbor", "pdf_keywords": "learns similarity sequences;language model learns;existing language models;learns similarity;language model knn;new language modeling;neural language model;language modeling plentiful;effective language modeling;language models similarly;language models unsupervised;trained neural language;language models;language modeling;embeddings simple nearest;model learns similarity;new language model;embeddings;tokens language modeling;representation learning;language model;pre\ufb01x embeddings simple;hypothesis representation learning;language modeling requires;pre\ufb01x embeddings;neural language;embeddings simple;using pre\ufb01x embeddings;domain adaptation;examples language models"}, "b31eb3428320342dfde042693ff2ca106dabed0d": {"ta_keywords": "outcome semiflexible elasticity;loop algorithm predicting;algorithm predicting;predicting outcome semiflexible;algorithm predicting outcome;scoring systems simcls;large margin decision;elasticity;open loop algorithm;margin decision maker;shear strain;elasticity presence shear;predicting;predicting outcome;scoring systems;shear strain absence;semiflexible elasticity;strain absence shear;margin decision;models large margin;shear;spring dynamics simple;existing scoring systems;simple spring;spring dynamics;models;dynamics simple spring;shear strains experimental;describes spring dynamics;performing models", "pdf_keywords": "rewards abstractive summarization;sequence sequence learning;summarization machine translation;generating candidate summaries;abstractive summarization model;generate candidate summaries;abstractive summarization stage;abstractive summarization machine;summaries evaluation model;sequence learning;tasks abstractive summarization;candidate summaries evaluation;sequence learning framework;formulating text generation;summarization model reference;candidate extractive summarization;summarization stage;language generation tasks;represented abstractive summarization;summarization model;abstractive summarization;summarization stage model;machine translation;summaries parameterized evaluation;text generation;abstractive summarization simcls;summaries evaluation;summarization machine;text generation reference;generation tasks abstractive"}, "5b1516c87818084dc5d195cc274e1ee8923210d2": {"ta_keywords": "bilingual word embeddings;named entity recognition;translations based bilingual;entity recognition ner;languages annotated;order languages annotated;entity recognition;languages annotated resources;word order languages;resource rich languages;languages cross lingual;language processing;transfer natural language;based bilingual word;cross lingual;lingual;translations based;language processing models;bilingual word;word embeddings;low resource language;based bilingual;finds translations based;cross lingual setting;bilingual;finds translations;annotated resources unsupervised;translations;order languages;resource language", "pdf_keywords": "translation word embedding;bilingual lexicon induction;translation machine learning;learning machine translation;combines machine translation;processing unsupervised crosslingual;machine translation combines;cross lingual learning;translation machine translation;bilingual lexicon;translation language independent;machine translation;unsupervised crosslingual transfer;architecture bilingual lexicon;machine translation word;machine translation language;translation combines machine;translation machine;machine translation machine;unsupervised crosslingual;lingual learning;lingual learning objective;embeddings word alignment;lingual results monolingual;cross lingual results;translation language;cross lingual;machine translation based;monolingual training objective;combines common monolingual"}, "1ce0664989e0b28ceea223cab68f885ed18c39c4": {"ta_keywords": "speaker clustering model;model speaker clustering;proposed speaker clustering;speaker clustering;speaker clustering powerful;unit speaker clustering;speaker clustering m3;mixture model speaker;gaussian mixture models;speech dynamics;gaussian mixture model;speech dynamics dynamics;dynamics speech observed;dynamics speech;based gaussian mixture;dynamics dynamics speech;mixture models;gaussian mixture;mixture models precisely;extension gaussian mixture;segmental utterance wise;scale mixture model;segmental utterance;multi scale mixture;speech observed instance;speech observed;mixture model;clustering model;linguistic segmental utterance;mixture model mixture", "pdf_keywords": ""}, "2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6": {"ta_keywords": "distributed training;known distributed training;protocol distributed optimization;guarantees training deep;training imagenet;distributed optimization strong;distributed training utilize;distributed training operate;training imagenet compared;based distributed training;networks large datasets;distributed optimization;50 training imagenet;imagenet;iterative averaging protocol;training deep;networks large;speedup resnet;neural networks large;experiments speedup resnet;federated learning cloud;federated learning;imagenet compared competitive;imagenet compared;averaging protocols;efficiency protocol distributed;networks;resnet 50 training;training deep neural;protocol distributed", "pdf_keywords": "federated learning distributed;federated learning compression;learning distributed training;distributed deep learning;learning distributed;distributed training;federated learning cloud;distributed distributed learning;distributed deep;distributed learning;distributed training unreliable;networks distributed deep;federated learning;training federated learning;based distributed training;distributed learning framework;applications federated learning;protocol federated learning;decentralized training algorithms;distributed training operate;distributed training circumstances;scale distributed training;distributed optimization;cloud based training;novel distributed optimization;neural networks distributed;distributed distributed;learning cloud;large scale distributed;distributed optimization algorithm"}, "d9212b207e49a3aa6806fb2ddadb303b7b1d47a8": {"ta_keywords": "procedural knowledge agent;task hierarchically;hierarchical procedural knowledge;command situated agents;agent command control;particular task hierarchically;level tasks;task hierarchically splitting;representing hierarchical procedural;procedural knowledge;knowledge agent command;hierarchical modular networks;hierarchical procedural;higher level tasks;hierarchically;modeling paradigm hierarchical;paradigm hierarchical modular;knowledge agent;situated agents;hierarchical;procedure natural language;procedural;command control;agent command;predictions executable programs;planner;sub tasks;networks consist planner;representing hierarchical;programs powerful intuitive", "pdf_keywords": "building reactive planner;reactive planner;reactive planner able;programs procedural;building reactive;programs procedural knowledge;reusable sub routines;framework building reactive;executable programs procedural;environment dynamics reactive;reactive generate executable;representing hierarchies procedures;procedure library modularity;planner able autonomously;procedural knowledge encoded;hierarchical modular networks;framework leverages procedural;reactive generate;modular networks;dynamic environments;procedural knowledge;diverse dynamic environments;paradigm hierarchical modular;framework train reactive;leverages procedural knowledge;routines propose modeling;dynamic environments framework;reactive;procedures using reusable;knowledge encoded procedure"}, "8fcd012e8ed2ea8190163369c9f222178e70a19d": {"ta_keywords": "hmm deep neural;speech recognition asr;asr attention based;model hmm deep;connectionist temporal classification;hmm asr systems;speech recognition;multiobjective learning ctc;joint decoding linguistic;conventional dnn hmm;attention based encoder;ctc attention based;learning ctc attention;recognition asr;recognition asr based;architectures asr attention;model hmm;temporal classification ctc;automatic speech;learning joint decoding;decoding linguistic;phonetic context dependency;automatic speech recognition;asr attention;proposed multiobjective learning;deep neural;tokenization phonetic;multiobjective learning;tokenization phonetic context;decoding linguistic resources", "pdf_keywords": ""}, "49418122bba375fa02907d38b0be80689f750b39": {"ta_keywords": "coding nonlinear computations;learned codes;learned codes accurately;learned codes highly;learning encoding;codes learned codes;designing codes learned;coding nonlinear;codes learned;codes accurately reconstruct;learning encoding decoding;nonlinear computations code;codes handle nonlinear;benefits coding nonlinear;designing codes;redundant computation decoding;image classifiers mnist;codes presents coding;computation decoding;coding;encoding decoding functions;codes highly promising;decoding function reconstructs;classifiers mnist;presents coding theoretic;coding theoretic;handle nonlinear computations;predictions neural;unavailable predictions neural;computation decoding function", "pdf_keywords": "learning coded computation;computation learned codes;learning coded;learned codes;neural networks unavailability;learning encoding;machine learning coded;unavailable predictions neuralnetwork;neural networks learn;learned codes accurately;neuralnetwork;decoding functions convolutional;learning encoding decoding;predictions neuralnetwork;matrices learning erasure;neuralnetwork based;learning erasure code;codes accurately reconstruct;predictions neuralnetwork based;coded computation emerging;neuralnetwork based image;datasets recurrent neural;encoding decoding functions;decoding;codes recover unavailability;involves learning encoding;decoding functions;matrices learning;computation learned;recurrent neural networks"}, "49f657d704a1b80ce3dba0d8a9e5479ec1d703d4": {"ta_keywords": "transformers structure speech;model automatic speech;short term memory;term memory networks;memory networks;new decoding strategy;automatic speech;different decoding strategies;speech recognition;new decoding;deep transformers;non autoregressive transformers;autoregressive network trained;non autoregressive transducers;autoregressive transformers;autoregressive transducers;speech recognition automatic;memory networks large;decoding strategies;recently deep transformers;decoding;automatic speech recognition;structure speech recognition;autoregressive transducers end;speech recognition originally;autoregressive transformers structure;different decoding;decoding strategies including;decoding strategy proposed;machine translation", "pdf_keywords": "masked language modeling;model masked language;modeling automatic speech;neural machine translation;automatic speech;language modeling automatic;autoregressive model masked;speech recognition promising;language modeling;speech recognition automatic;automatic speech recognition;speech recognition;recognition automatic speech;masked language;gap training inference;attention neural machine;machine translation nmt;machine translation;use factorization loss;training inference;attention neural;autoregressive model;factorization loss;non autoregressive model;factorization loss instead;neural machine;training inference paper;autoregressive;attract attention neural;model masked"}, "a1321f4527559836509c27008329afaf11f8ea89": {"ta_keywords": "learning agent;learning agent learns;problem orders learning;agent learns cognitive;learns cognitive;learns cognitive skills;agent learning apply;agent learns;correction learning agent;learns;learning agent learning;learning effectiveness interleaved;agent learning;learning effectiveness;machine learning agent;orders learning;learning;orders learning use;effective learning;cognitive skills examples;cognitive skills;affects learning;learning apply skill;yields effective learning;problem solving experience;affects learning effectiveness;effective learning domains;learning apply;variable affects learning;skills examples", "pdf_keywords": ""}, "da46a0b5ddf0f4bf4caad9d29d6b4a93dd2eb2d2": {"ta_keywords": "competition xmath0 baryon;heavy fermion hf;xmath0 baryon;heavy fermion;bandit style problem;heavy heavy fermion;fermion hf;baryon fraction heavy;competition xmath0;heuristic algorithm;hf present bandit;xmath0 baryon fraction;fermion;heuristic algorithm evaluation;provide heuristic algorithm;fermion hf present;specific problem agent;bandit style;baryon fraction;heuristic;present bandit;bandit;study competition xmath0;problem agent based;provide heuristic;present bandit style;modeling games;baryon;problem agent;xmath0", "pdf_keywords": ""}, "8ff54aa8045b1e30c348cf2ca42259c946cd7a9e": {"ta_keywords": "parsing question answering;question answering;task answering sequences;neural semantic parsing;dynamic neural semantic;question answering focused;realistic task answering;task answering;semantic parsing;parsing framework trained;neural semantic;semantic parsing question;semantic parsing framework;supervised reward guided;weakly supervised reward;answering sequences;explore conversational qos;reward guided search;answering sequences simple;conversational qos;answering focused;conversation humans model;answering focused long;answering;parsing;way answer question;parsing framework;answer question;question sequences;reward guided", "pdf_keywords": ""}, "aaaff6b99684cb5b5e0a68e214bd8bbd4bf2e231": {"ta_keywords": "named entity recognition;entity recognition;entity recognition paper;biomedical named entity;characterizing biological entities;fields syntactic parsing;evaluation biomedical named;random fields syntactic;parsing;biological entities;based hidden markov;syntactic parsing;hidden markov model;biological entities compare;named entity;hidden markov;biomedical named;identifying characterizing biological;conditional random fields;markov model based;fields syntactic;entities compare systems;entities;evaluation biomedical;entity;entities compare;method identifying characterizing;characterizing biological;biomedical;datasets terms challenges", "pdf_keywords": ""}, "956e096b1e8422c91989938b9508272b956d3070": {"ta_keywords": "graph reranking approach;graph reranking;probabilities graph reranking;graph walks framework;reranking algorithm;measure entity similarity;reranking approach;combined reranking algorithm;reranking approach uses;typed graph walks;improving graph walk;entity similarity;walks directed graphs;entity similarity paper;directed graphs entities;method combined reranking;reranking;graph walks directed;rank typed graph;reranking algorithm evaluate;random graph walks;graph walks;graph walk performance;combined reranking;lazy random graph;graphs entities;walks framework;graph walk;graphs entities represented;walks framework used", "pdf_keywords": ""}, "a4a8e91995ae8c8b203dd857bdc0915facddeebe": {"ta_keywords": "noisy crowdsourcing;learning depends annotated;crowdsourced data;worker quality crowdsourced;quality crowdsourced data;quality crowdsourced;crowdsourcing;annotation budget;better label examples;come noisy crowdsourcing;noisy crowdsourcing platforms;fewer labeled examples;crowdsourced;fixed annotation budget;labeling;labels worker quality;labeled examples;annotated examples;annotated examples taken;supervised learning depends;fewer labeled;crowdsourced data given;data redundant annotation;modeling labels worker;label examples;annotation;annotation comes expense;supervised;jointly modeling labels;crowdsourcing platforms", "pdf_keywords": "crowdsourcing images;algorithm crowdsourcing images;learning algorithm crowdsourcing;crowdsourcing images multiple;crowdsourcing;algorithm crowdsourcing;estimate true labeling;new approach crowdsourcing;annotations training data;large weakly supervised;annotations training;approach crowdsourcing;crowdsourcing crowdsourcing;annotators propose supervised;produced annotations training;human annotators;weakly supervised;learning ml;crowdsourcing research;human annotators propose;approach crowdsourcing crowdsourcing;crowdsourcing research seldom;label training example;supervised;label training;supervised learning;learning ml algorithms;weakly supervised data;propose supervised learning;labeling"}, "ca3535dcdda9849350ad7c991a60660b22844f2f": {"ta_keywords": "recognition sub task;searchable hidden representations;intermediates speech recognition;sequence tasks;sequence tasks increasingly;compositionality learn searchable;approaches sequence tasks;speech recognition;speech recognition sub;searchable hidden intermediates;decoder model;decoder;decomposed sub tasks;learn searchable hidden;recognition sub;multi decoder model;hidden representations;recognition;sophisticated search capabilities;model recognition importance;hidden representations intermediate;sophisticated search;compositionality cascaded systems;model recognition;hidden intermediates speech;sub tasks;complex networks;learn searchable;person decision making;tasks", "pdf_keywords": "multi sequence attention;sequence attention;architecture automatic speech;automatic speech;sequence attention particular;speech recognition;automatic speech recognition;speech recognition particular;propose multi decoder;end sequence models;multi decoder architecture;decoder architecture automatic;sequence models cascaded;multi decoder;conversations fisher;decoder;telephone conversations fisher;sequence models;domains telephone conversations;decoder architecture;telephone conversations;conversations fisher callhome;attention;models intermediate scoring;conversations;scoring multi sequence;speech;task output individual;external models;attention particular"}, "d56244c6abf3141900386d6911dd9097697a346b": {"ta_keywords": "training text categorization;web page classifier;text categorization;text categorization systems;words classifier;bag words classifier;learning unlabeled data;page classifier;learning unlabeled;classifier;page classifier performance;words classifier reducing;categorization systems;method learning unlabeled;categorization systems use;classifier based;categorization;classifier performance pages;classifier performance;improving performance classifier;training text;accuracy bag words;performance classifier;performance classifier based;unlabeled data based;classifier reducing;classifier based ability;improves simple web;unlabeled data;variant training text", "pdf_keywords": ""}, "42605c1ee030721cb38a3c225992d63297a6ace0": {"ta_keywords": "language documentation conservation;language revitalization technology;language technology;workshop language technology;language documentation revitalization;language technology language;technology language documentation;study language revitalization;documentary linguists;language technology application;documentary linguists technologists;language revitalization;processing language technology;advances natural language;language documentation;language community;bring language community;technology language;members documentary linguists;technologies aid language;languages workshop focused;language speech;aid language documentation;languages workshop;documentation conservation;practical languages workshop;limited workshop language;workshop language;language community members;documentation revitalization", "pdf_keywords": "automated phonetic language;automated phonetic;development automated phonetic;challenges lexicography;alignment text speech;challenges lexicography based;translate spoken language;lexicography based;speech forced alignment;documentary linguists present;machine translation;lexicography;automating dictionary extraction;documentary linguists;experts documentary linguists;dictionary extraction;language experts documentary;morphological analyzer;machine translation used;morphological analyzer inuktitut;linguists present prototype;dictionary extraction processing;phonetic language;phonetic language community;language spoken language;uses morphological analyzer;inuktitut language model;text speech;automating dictionary;linguists present"}, "c8d0e13de2eaa09a928eff36b99d63f494c2f5ec": {"ta_keywords": "parsing natural language;semantic parsing approaches;semantic parsing;architecture powered grammar;parsing approaches;syntax target programming;powered grammar model;parsing natural;powered grammar;parsing;natural language;state art semantic;art semantic parsing;underlying syntax target;consider problem parsing;syntax prior knowledge;syntax target;parsing approaches terms;problem parsing natural;problem parsing;grammar model explicitly;capture target syntax;search dark energy;problem language generation;language generation task;target syntax prior;grammar model;natural language descriptions;target syntax;language generation", "pdf_keywords": "generation semantic parsing;code generation semantic;driven syntaxbased neural;syntaxbased neural;syntaxbased neural network;data driven syntaxbased;python code generation;syntax target programming;synthesis natural language;programming language compositional;code generation tasks;generating code nl;language compositional program;generation semantic;semantic parsing demonstrate;code generation;automatically generating code;parsing natural language;purpose code generation;driven syntaxbased;semantic parsing;compositional program synthesis;problem language generation;program synthesis natural;approach semantic parsing;syntaxbased;generating code;language generation task;programming language like;semantic parsing problem"}, "9c03d14520c897ca8536e165507f568d1980dabd": {"ta_keywords": "pair machine comprehension;machine comprehension text;lexical matching;coreference resolution;develop lexical matching;machine comprehension;lexical matching method;question types coreference;coreference resolution paper;results test quark;quantum body;types coreference resolution;question answer pairs;comprehension text;coreference;quark;test quark gluon;question answer pair;test quark;answer pair machine;quark gluon;comprehension text overarching;lexical;develop lexical;quantum;windows question types;question types;quark gluon plasma;qgp quenched approximation;answer pairs likely", "pdf_keywords": ""}, "2ea226a7fadde6a45f537c714e0832e83136f861": {"ta_keywords": "biomedical event extraction;cost sensitive classification;classification tasks;cost estimation learning;classification tasks models;structured prediction;approach biomedical event;based structured prediction;bio nlp;event extraction;biomedical event;bio nlp 2009;sensitive classification tasks;structured prediction framework;score bio nlp;predict outcome biophysical;event extraction using;models learned jointly;nlp 2009 shared;tasks models learned;2009 shared task;efficiency predictive accuracy;biophysical event based;classification;task cost sensitive;improves efficiency predictive;sensitive classification;outcome biophysical event;method predict outcome;predictive accuracy", "pdf_keywords": ""}, "708f8c0eb5032edd6f31663a27febbb0529cbcf3": {"ta_keywords": "read paired captions;paired captions looking;captions looking natural;learning syntactic;paired captions;learning syntactic representations;parsing text training;captions looking;paired capt captions;capt captions;captions;approach learning syntactic;syntactic representations structures;syntactic representations;captions paper;capt captions paper;text training;text training data;model learn read;natural images reading;captions paper present;reading paired capt;learn read paired;parsing;model learn;parsing text;structures explicit supervision;neural;concreteness constituents matching;learning", "pdf_keywords": "syntax acquisition grounded;grounded syntax acquisition;visually grounded syntax;grounded neural syntax;neural syntax learner;learning including syntax;syntax acquisition;syntax learner;caption pairs extract;image caption pairs;syntax natural language;acquisition grounded language;visually grounded neural;caption pairs;grounded language acquisition;semantic bootstrapping;acquisition syntax natural;grounded acquisition syntax;natural language;introduce semantic concreteness;grounded syntax;semantic representation;including syntax semantics;based semantic bootstrapping;neural syntax;semantic concreteness new;grounded language;semantic concreteness;syntax semantics;automated language learning"}, "06e36261b21af2943e464a562c92c09dac292a82": {"ta_keywords": "professionals reverseengineering binaries;reverseengineering binaries;programming languages;languages acyclic compiled;professionals reverseengineering;reverseengineering binaries wild;meaningful variable names;programming;language decompilers;reverseengineering;level language decompilers;variable names;security professionals reverseengineering;code comments;acyclic compiled;programming languages high;language decompilers able;time programming languages;variable names types;names written developers;computer program common;output computer program;binaries wild decompiler;variable names custom;acyclic compiled higher;computer program;code comments variable;technique automatically generating;program;compiled", "pdf_keywords": "statistically modeling developers;developers write code;modeling developers write;code decompilers;code sequence types;predicting developer;code decompilers generate;automatically predict structure;decompiled variable recognition;original code decompilers;source code sequence;source code;variable original code;retyping source code;modeling developers;developerwritten code;developer assigned types;variables meaningful names;write code;code readability fully;method predicting developer;neural machine translation;improve code readability;developers write;predicting developer assigned;developerwritten code paper;write code paper;architecture decompiled variable;code readability;decompiled variables meaningful"}, "54e7de06a97b4b6c41e185c0bee60c838a15265a": {"ta_keywords": "articulatory controllable speech;controllable speech modification;controlling phoneme sounds;speech waveforms manipulating;speech modification framework;controllable speech;modified speech waveform;modify speech waveforms;generating modified speech;speech modification;modify speech;enables modify speech;speech waveform manipulated;speech waveform;phoneme sounds manually;speech waveforms;articulatory parameters estimated;articulatory manipulation;manipulating unobserved articulatory;articulatory manipulation method;sounds manually controlling;propose articulatory manipulation;propose articulatory controllable;gaussian mixture models;articulatory controllable;controlling phoneme;manipulated articulation parameters;waveform manipulated articulation;modified speech;predicting controlling motion", "pdf_keywords": ""}, "7771aa7badc3375a31bfac8dc47755ff5d5c7780": {"ta_keywords": "subword text probability;morphological complexity subword;word level distributions;complexity subword tokenization;languages typological characteristics;subword tokenization;languages typological;subword tokens identical;orthographic word types;cross linguistic diversity;subword tokens;subword tokenization used;languages distribution orthographic;different languages typological;cross linguistic analysis;linguistic diversity observed;distribution subword tokens;morphological complexity;subword word level;cross linguistic;linguistic diversity;text entropy;linguistic analysis;types morphological complexity;word types different;linguistic;subword text;word types;similar distributions languages;language language tradeoff", "pdf_keywords": "languages morphological systems;diverse languages morphological;subword distributions languages;corpus based morphological;languages morphological;based morphological complexity;entropies subword distributions;morphological complexity measure;predictability similar languages;statistical morphological analysis;morphological analysis entropy;predictability words similar;morphological complexity;predictability subword sequences;statistical morphological;present statistical morphological;word level predictability;words similar languages;cross linguistic comparison;corpus 47 languages;subword distributions;linguistic comparison;morphological analysis;typologically diverse languages;entropy word level;word internal predictability;predictability words;based morphological;linguistic comparison following;similar languages word"}, "79c93274429d6355959f1e4374c2147bb81ea649": {"ta_keywords": "visual question answering;image question answering;question answering datasets;learning cross modality;modality encoder representations;question answering;cross modality encoder;encoder cross modality;encoder representations transformers;vision language connections;encoder pre trained;challenging visual reasoning;modality encoder;trained cross modality;adapting challenging visual;language encoder;task nlvr2 improve;modality encoder pre;language encoder cross;encoder representations;connect vision language;object prediction feature;vision language semantics;representations transformers framework;visual reasoning task;cross modality matching;image sentence pairs;transformers framework learn;answering datasets;question answering fine", "pdf_keywords": "aware object embedding;object embedding crossmodality;vision language training;image question answering;vision language reasoning;embedding crossmodality;object prediction pre;object prediction cross;learns cross modal;pre trained language;embedding crossmodality layer;trained language model;vision language;alignments vision language;object embedding;language model learns;trained language;model vision language;visual reasoning inclusion;trained model vision;cross modality matching;prediction cross modality;object prediction;visual reasoning;prediction pre training;position aware object;pre trained model;image sentence pairs;position aware;modality matching image"}, "03e4f33c0ccc4cb8c7e1589158a5377cdf5241d2": {"ta_keywords": "preferences ethical priorities;conditional preference networks;preference networks cp;subjective preference ethical;subjective preferences ethical;preference networks;agent subjective preference;preference ethical principles;ensure decisions ai;set ethical priorities;preference ethical;qualitative preference relations;priorities paper autonomous;decisions ai;preferences ethical;preferences individuals ethical;ethical priorities human;compare subjective preferences;preference relations;conditional preference;ethical priorities;conditional qualitative preference;individuals ethical priorities;ethical priorities result;decisions ai aligned;priorities human decision;preference relations used;robots used evaluate;subjective preferences;ethical priorities paper", "pdf_keywords": ""}, "d5f22dbc8f4b9e99f62e6ecf886bc4b9a0372e4d": {"ta_keywords": "ontologies classifying entities;incomplete ontologies classifying;hierarchical classification entities;ontologies classifying;classifying entities incomplete;classification entities;ontology text datasets;classifying entities;classification entities incomplete;entities incomplete ontologies;complete ontology classes;entities incomplete ontology;ontology classes;corpus hierarchical exploratory;incomplete ontologies;hierarchical classification;ontologies;techniques hierarchical classification;hierarchical exploratory;challenges hierarchical classification;incomplete ontology;incomplete ontology important;hierarchical exploratory em;clueweb09 corpus hierarchical;seed classifiers;classes seed hierarchy;class hierarchy seed;hierarchical classification method;ontology classes represent;complete ontology", "pdf_keywords": ""}, "680e61a17e27a1e8e121276c7ec53fc4fd40babb": {"ta_keywords": "linguistic acceptability;comprehension linguistic acceptability;linguistic acceptability present;language users utterances;language users maximizing;users utterances;information density;uniform information density;acceptability data;information density predictive;users utterances structured;predict language comprehension;communication channel hypothesis;predict language;byproduct language users;acceptability data nonuniformity;lower acceptability explore;preference language users;acceptability explore;nonuniformity information density;used predict language;reading time acceptability;utterances structured information;preference language;predictive lower acceptability;exerted uniform information;information distributed uniformly;language comprehension linguistic;uniform information;information density uid", "pdf_keywords": "surprisals language comprehension;judgments linguistic acceptability;language comprehension findings;language comprehension;linguistic acceptability potentially;linguistic acceptability;linguistic acceptability disambiguating;uid linguistic acceptability;linguistic acceptability posit;operationalizations surprisals language;judgments linguistic;comprehension findings;word surprisal revisit;acceptability sentence inverse;linguistic;human judgments linguistic;comprehension findings suggest;certain linguistic;disambiguating certain linguistic;linguistic structures investigate;surprisals language;linguistic structures;certain linguistic structures;function word surprisal;sentence word level;acceptability potentially reading;power human judgments;pressure linguistic acceptability;comprehension;uid linguistic"}, "34f8214cbaa0655794c2c9570898abf15649b079": {"ta_keywords": "recognition reverberant speech;dynamic mismatch speech;dereverberation speech recognizer;speech recognizer;performance automatic speech;mismatch speech features;noise performance speech;performance speech recognition;reverberant speech;noise reverberation introduce;speech recognition;noise reverberation;automatic speech;automatic speech recognition;problem recognition reverberant;new preprocessor reverberation;speech features acoustic;reverberation mapping preprocessor;speech recognition severely;reverberant speech received;preprocessor reverberation mapping;recognition reverberant;reverberation mapping;speech recognizer paper;preprocessor reverberation;speech features;reverberation;interconnection dereverberation speech;reverberation introduce parametric;speech recognition problem", "pdf_keywords": ""}, "3aba582b62d1abfcd95264e6c7b32aab4c9db4b8": {"ta_keywords": "novel self organizing;selfexplain facilitates interpretability;classification datasets selfexplain;self organizing systems;self organizing model;self organizing;class self organizing;explanations self;called self organized;self contained explanations;datasets selfexplain facilitates;self organized critical;text classifier predictions;contained explanations self;influential concepts training;explanations self contained;identifies influential concepts;self organized;selfexplain facilitates;explains text classifier;text classification;phrase based concepts;text classifier;neural classifiers;systems called self;selfexplain novel self;classifiers;concept computing relevance;self contained self;classifier predictions", "pdf_keywords": "perceive explanations generated;predicting sentiment movie;users perceive explanations;sentiment movie review;automatically predicting sentiment;perceive explanations;predicting sentiment;interpretable neural classi\ufb01ers;inherently interpretable neural;movie review sentence;interpretable neural;attribution based attention;explanations generated humans;movie review;explanations generated;annotators interpreted model;sentiment movie;explanation concepts;text model trained;interpretability;classi natural language;automatically predicting;inherently interpretable;developing inherently interpretable;review sentence binary;natural language;based attention scores;attention;explanations;explanations aim"}, "e5a5888966be6b5f9c0e8a82facd604086a1ee4c": {"ta_keywords": "recognizing gene names;papers automatically annotated;automatically recognizing gene;generated abstracts curated;automatically annotated training;automatically annotated;gene names papers;papers abstracts curators;data generated abstracts;names papers curators;abstracts curated using;abstracts curators;annotated training data;generated abstracts;paper describes automatically;annotated training;papers curators trained;sentences papers automatically;evaluating papers abstracts;abstracts curated;recognizing gene;abstracts curators asked;gene names;describes automatically recognizing;experiments performed abstracts;papers curators;annotated;describes automatically;curated using;curated", "pdf_keywords": ""}, "73271677da83a3f55523148d1b43a0501f0a35dd": {"ta_keywords": "learning dynamics;learning dynamics cyclic;dynamics classical learning;regret dynamics converge;zero sum games;online learning dynamics;online regret dynamics;regret dynamics;repeated game formulation;sum games model;game formulation;autonomous dynamics classical;games fluid dynamics;non autonomous dynamics;converge equilibrium time;equilibrium time;dynamics classical;game formulation realistic;games model generalizes;dynamics converge equilibrium;dynamics;equilibrium time average;sum games;autonomous dynamics;time average convergence;recurrent zero sum;sum games fluid;equilibrium fixed time;theorem online regret;time evolution probability", "pdf_keywords": "bilinear games periodic;learning dynamics cyclic;games periodic zerosum;zerosum polymatrix games;games periodic payoffs;online learning dynamics;sum games periodic;polymatrix games time;learning dynamics;games periodic;sum bimatrix games;bimatrix games class;polymatrix games game;polymatrix games;bimatrix games;zero sum games;learning dynamics respectively;sum polymatrix games;recurrence online learning;learning dynamics paper;bimatrix games replicator;sum bilinear games;games covered periodic;time evolving games;bilinear games;bimatrix games player;games player periodic;player periodic zero;games time invariant;periodic payoffs"}, "74fb2834c820d2297b08201cb72de1c1d3d27f54": {"ta_keywords": "privacy speech recog;cloud based speech;client privacy speech;privacy speech;tradeoffs performance privacy;privacy;performance privacy;performance privacy complexity;privacy complexity;privacy discuss;adversarial training;speech recog nition;client privacy;representation learning adversarial;adversarial;learning adversarial training;adversarial training approaches;learning adversarial;privacy discuss unique;speech recog;client privacy discuss;cloud;speech processing;disentangled representation learning;based speech processing;processing disentangled representation;control biometric information;gender accent masking;speech processing paper;client control biometric", "pdf_keywords": "downstream speech privacy;privacy speech recognition;approach voice encryption;voice encryption;client privacy speech;voice encryption based;speech privacy;differential privacy formalize;privacy speech;privacy formalize;speech privacy section;anonymizing speaker identity;private speech features;privacy formalize client;privacy unique technical;challenges biometrics speech;formalize client privacy;differential privacy;privacy approaches downstream;voice conversion approaches;investigate voice conversion;inherent differential privacy;anonymizing speaker;voice conversion;server privacy approaches;speaker identity client;autoencoders disentangle private;privacy approaches;client privacy;client privacy unique"}, "030d7d7ae48a9f81700b2c1f7cf835235777b8e7": {"ta_keywords": "scalable neural retrieval;retrieval retrieval;present retrieval retrieval;rules large corpus;neural retrieval model;large corpus;retrieval model predict;retrieval;efficient weak supervision;neural retrieval;present retrieval;retrieval model;retrieval retrieval closed;corpus large;large corpus depends;complexity natural language;paper present retrieval;supervision strategy iteratively;retrieval closed form;size corpus;data retriever learned;weak supervision strategy;corpus;depends size corpus;retrieval closed;natural language;training data retriever;weak supervision;natural language questions;propose scalable neural", "pdf_keywords": "question answering results;question answering;domain question answering;machine reading comprehension;learns answer set;question answering questions;language question answering;learns answer;answering results training;model reading comprehension;learning retrieve passages;model learns answer;highly effective retrieval;propose machine reading;answering questions self;answering questions;questions self supervised;retrieval unstructured text;answering results;reading model learns;effective retrieval unstructured;effective retrieval;learning retrieve;relevance guided;machine reading;relevance guided supervision;reading comprehension;retrieval unstructured;answering;machine reading model"}, "7c655ef6f0de8c1a219cdb796c77f4ae3c389b82": {"ta_keywords": "season averages fluid;season averages compared;seasonally averaged season;season averages;season averages spring;averages compared season;compared season averages;statistics seasonally averaged;averages season;averages season averages;season averages season;statistics seasonally;averages spring 2014;seasonally averaged;averaged season averages;2014 season averages;averages spring;averaged season;study statistics seasonally;averages fluid;averages fluid dynamics;spring 2014 season;averages;averages compared;season;compared season;measure number participants;2014 season;spring;spring 2014", "pdf_keywords": ""}, "b2b0fbf9033f1c36bea8bb11c173f14378c60db9": {"ta_keywords": "probability word language;word language perform;level word language;languages word level;language performance s2;word language expressed;word language speech;language speech;importance word language;word language;speech translation systems;languages word;language performance;distribution word level;language expressed;language speech speech;impact language performance;word level;language perform analysis;translation systems;language expressed terms;language perform;different languages word;language;languages languages;languages;expressed individual languages;individual languages;speech translation;word level word", "pdf_keywords": ""}, "262c0e54370dfc03a7ad53d79930568d18dd448c": {"ta_keywords": "distributed learning algorithms;distributed machine learning;distributed learning;blocks distributed learning;algorithm multicasting message;algorithm multicasting;shuffling coding widely;multicasting algorithm multicasting;multicasting algorithm;multicasting;simple multicasting algorithm;present simple multicasting;simple multicasting;shuffling coding;performance distributed;data shuffling;multicasting message;uncoded shuffling coding;data shuffling use;distributed machine;distributed;performance distributed machine;multicasting message user;reduce communication bottlenecks;shuffling use codes;communication bottlenecks exploiting;compared uncoded shuffling;uncoded shuffling;shuffling use;cached worker communication", "pdf_keywords": "decentralized coded caching;distributed algorithm matrix;distributed machine learning;coded caching;coded caching studied;distributed learning algorithms;hierarchical coded caching;coded caching wireless;coded caching hierarchical;distributed algorithm;distributed learning;distributed data processing;shuf\ufb02ing coded caching;coded caching work;large distributed systems;hardware coded computation;caching hierarchical coded;coded computation;decentralized coded;distributed data;propose distributed algorithm;device coded caching;coded computation used;cost distributed learning;caching wireless communication;distributed machine;communication computation;distributed systems;algorithms matrix multiplication;computation used storage"}, "14a058a1e41459a30327bb5fb480d51430b6a096": {"ta_keywords": "high performance geneidranking;geneidranking;geneidranking step;genealogy finding algorithm;gene id finding;geneidranking step curation;performance geneidranking step;performance geneidranking;genealogy finding;gene id;ranked list genes;finding algorithm bioinformatics;demonstration genealogy finding;named entity recognition;algorithm bioinformatics;bioinformatics;genealogy;list genes;entity recognition;genes discussed document;yang baxter yb;list genes discussed;process genealogy;entity recognition ner;curation process genealogy;gene;named entity;relaxation gene id;algorithm bioinformatics video;process genealogy article", "pdf_keywords": ""}, "a6d505a6e46c15ef0d213b9a4349ce2f852be894": {"ta_keywords": "negative classifiers;negative classifiers unlabeled;negative classifier;positiveversus negative classifier;versus negative classifiers;classifier given positive;positive examples unlabeled;classifier based mixture;mixture based classifier;learning positive;classifiers unlabeled data;classifiers unlabeled;classifiers;classifier;unlabeled examples hope;approach learning positive;learning positive versus;unlabeled examples;examples unlabeled examples;classifier based;examples unlabeled;based classifier;accurate positiveversus negative;subset positive examples;positive examples;estimate accurate positiveversus;positive examples paper;classifier given;unlabeled data;accurate positiveversus", "pdf_keywords": "learning unlabeled;learning unlabeled data;learning positive unlabeled;supervised learning task;multilayer perceptrons;perceptrons mlps relu;unlabeled data challenging;multilayer perceptrons mlps;objective pu learning;connected multilayer perceptrons;supervised;datasets pu learning;supervised learning;relu activations convolutional;perceptrons;learning task;perceptrons mlps;learning positive;unlabeled data inspired;learning;pu learning;outcome supervised learning;convolution models;outcome supervised;positive unlabeled data;unlabeled data;activations convolutional;convolutional convolution models;mlps relu activations;recognize reliably"}, "b08545e1281c1eb748e4474687eb61fd3b25d1a6": {"ta_keywords": "non linear languages;novelty lexical;state art nlp;word appearance;class novelty lexical;novelty lexical derivation;linear languages;nlp systems;word appearance non;linear languages present;nlp;art nlp systems;art nlp;lexical;appearance non linear;novel words;problem word appearance;lexical derivation;non linear;dialectal variation blending;annotated class novelty;lexical derivation dialectal;dialectal variation;words published new;dynamics simple mechanical;novelty;simple mechanical;words;variation;500 novel words", "pdf_keywords": "neural word embedding;word embeddings;word embedding;word embedding framework;classifying words;types word embeddings;lexical enrichment;predicting language;language innovation unannotated;task classifying words;analysis lexical enrichment;lexical enrichment serve;word embeddings bag;unannotated text dataset;predicting language innovation;probabilistic neural word;classifying words categories;predicting appearance word;words categories based;natural language processing;corpus word appearance;innovation unannotated text;data linguistic;words categories;embedding framework predicting;framework predicting language;introduction corpus word;unannotated text;natural language;nlp systems"}, "43fae0a7af211d91557d115d2f82e3c46d8bf022": {"ta_keywords": "natural language generation;nlg tasks;language generation process;nlg tasks including;language generation;process natural language;information change nlg;automatic alignment prediction;change nlg;language generation gng;change nlg tasks;nlg;challenging automatic evaluation;natural language;alignment prediction develop;generate target text;characterizing generation;language used generate;intuitions natural language;alignment prediction;automatic evaluation;automatic alignment;introduction natural language;generation process;generate;text information alignment;information alignment;different nlgility tasks;natural language used;nlgility tasks need", "pdf_keywords": "easy nlg evaluation;metrics various nlg;nlg evaluation;nlg tasks;nlg evaluation future;various nlg tasks;nlg tasks aspects;library easy nlg;various nlg;summarization style transfer;easy nlg;information alignment;concept information alignment;knowledge grounded dialog;information alignment develop;novel alignment model;metrics diverse tasks;text summarization;including text summarization;nlg;sources measuring content;text summarization style;measuring content preservation;measuring content;evaluation future metric;reference data diverse;alignment model transferring;model generate pharaphrase;tasks including text;metrics diverse"}, "b0894f5c914cd90cc3b3e16b15bec11efe317b14": {"ta_keywords": "test peer assessment;peer assessment tasks;peer assessment;peer assessment task;various peer assessment;test peer;peer grading;peer grading exams;assessment task competitive;including peer grading;behaviour various peer;peers output rankings;exams homeworks peer;guarantees evaluate detection;peer review;agents evaluate;evaluate detection;assessment tasks;tasks including peer;alarm guarantees evaluate;homeworks peer review;evaluate subset peers;power test peer;evaluations order improve;assessment task;incentivized misreport evaluations;experiment elicits strategic;assessment;peers;grading", "pdf_keywords": "peer review task;evaluation grading students;strategic behavior reviewers;experiment peer review;grading students;peer review;review task competitive;evaluation grading;grading;competitive students graded;ranking returned reviewer;behavior reviewers;students graded;evaluations demonstrate strong;grading students homeworks;employee evaluation grading;peer review aims;evaluations order improve;al peer review;behavior reviewers manipulate;reviewer perceived;review task;incentivized misreport evaluations;reviewer perceived position;reviewers;reviewers manipulate;graded;students graded curve;evaluations;reviewers manipulate reviews"}, "f481d6dea08e348cecd5eb23a813d47373e62a94": {"ta_keywords": "programs natural language;natural language code;communities programming language;programming communities programming;modeling natural language;natural language interact;automatic explanation programs;programming language programming;collaborative programming communities;collaborative programming;natural language specifications;generate programming languages;communication collaborative programming;natural language;generate programming;programming communities;programming language;language code modeling;language programming;language programming language;language automatic generation;code modeling natural;communities programming;code language;use natural language;code language automatic;natural language elements;automatic generation programs;explanation programs;programming languages", "pdf_keywords": ""}, "c7424d651d60ef9f052e91bff18efd88782225a3": {"ta_keywords": "election breaking ties;ties stage voting;control election breaking;controlling outcome election;outcome election breaking;elections prove np;multi round elections;election breaking;control election;ties choose winners;hard control election;breaking ties strategically;break ties ensure;round elections;round elections prove;break ties choose;rule chair election;winners problem trivially;ties strategically form;elections;election asked break;ties strategically;tie breaking function;compute break ties;complexity controlling outcome;chair election;break ties;elections prove;form tie breaking;strategically form tie", "pdf_keywords": "ties stage voting;manipulation tie breaking;voting rules resistant;wins ties control;candidates control tie;polynomial tie breaking;tie breaking np;manipulation tie;manipulating vote;control election breaking;simulate manipulating vote;tie breaking functions;complete tie breaking;ties choose winners;tie breaking simulate;voting rule paper;rules control tie;transitive tie breaking;polynomial number votes;tie breaking polynomial;control tie breaking;random candidates control;rule paper voting;vote used proofs;election breaking ties;tie breaking specici\ufb01ed;voting rules;stage voting rule;manipulating vote used;tie breaking"}, "19a6e362840d3a2d27d0fa5509eaa4d4597a2859": {"ta_keywords": "sequence sequences recorded;sequences recorded sequence;sequence sequence sequences;sequence sequences sequences;sequence sequences;sequences recorded;sequences sequence sequences;recorded sequence sequence;sequences sequences;sequences;sequences sequences sequence;sequence repeated sequence;sequences sequence;sequence sequence sequence;sequence sequence repeated;recorded sequence;sequence repeats;sequence sequence;repeated sequence;sequence repeated;repeated sequence repeats;sequence;present sequence sequence;present sequence;repeats;repeated;recorded;present", "pdf_keywords": ""}, "b145a46718f293429054f0a9a4cdd2de94813b37": {"ta_keywords": "things ips hyperlink;ips hyperlink structure;hyperlink structure web;ips hyperlink;hyperlink structure;hyperlink;web information retrieval;search generation internet;structure web;successful link analy;link analy sis;internet things ips;internet things;generation internet things;analy sis algorithms;search generation;things ips;improvements web information;web information;sis algorithms;describes successful link;information retrieval;significant improvements web;improvements web;search;link analy;algorithms state art;sis algorithms state;successful link;web", "pdf_keywords": ""}, "5ea3c08614e9673a109f581cf114af488f3aa601": {"ta_keywords": "predict location embryo;automatic embryo staging;data optimize embryo;embryo video propose;embryo video;method automatic embryo;automatic embryo;time lapse embryoscope;embryologists based time;lapse embryoscope videos;embryo staging;embryoscope videos;location embryo video;select embryos;embryo;embryo staging based;optimize embryo;manually experienced embryologists;embryos;lapse embryoscope;strategically select embryos;embryoscope videos report;select embryos transfer;location embryo;accuracy transition prediction;embryoscope;embryologists;transition prediction;decoder postprocesses predictions;optimize embryo transfer", "pdf_keywords": "dnns predict embryo;embryo video;embryo video study;deep convolutional;location embryo video;deep convolutional neural;predict embryo;object recognition;use deep convolutional;deep learning;predict embryo cell;predict embryo embryo;predict embryo morphokinetics;train convolutional neural;object detection;network predict embryo;networks dnns;present deep learning;networks dnns predict;embryos;predict location embryo;convolutional neural networks;approach predict embryo;convolutional neural;learning based object;videos use deep;cropping methods train;train convolutional;embryo;embryo cell stages"}, "6ab36d2577f7c9487b28b2bcdf236191ba901aad": {"ta_keywords": "dynamics flowchart dialogs;flowchart dialogs;dialogs experiments flowcharts;task oriented dialogs;dialogs;utterance flowchart;utterance flowchart explicit;generation dialogs experiments;dialogs experiments;dialog;grounding utterance flowchart;generation dialogs;oriented dialogs;flowchart explicit annotation;flowchart used teach;flowcharts agent;students dynamics flowchart;dataset 738 dialogs;dialog mimics;dialogs tod;flowcharts propose novel;troubleshooting flowcharts propose;learning task oriented;oriented dialogs tod;flowcharts propose;technical challenges neural;architecture generation dialogs;flowchart used;flowcharts used;dialog mimics troubleshooting", "pdf_keywords": "task oriented dialogs;task oriented dialogue;dialogs;learning task oriented;oriented dialogue modelling;dialogue modelling;dialog;task oriented knowledge;end task oriented;oriented dialogs;dialogue modelling paradigm;dialogs tod;oriented dialogs tod;task endto;task task oriented;task oriented;end end task;novel task endto;novel task oriented;task endto end;tod dialog;dialogs tod dialog;dialogs paper propose;dialogs divided end;dialog mimics;oriented dialogue;learning task;dialog mimics troubleshooting;endto end learning;end task"}, "1b114486d67252ff83fc90d4a8607636045c54ce": {"ta_keywords": "grained alignments stackoverflow;fine grained alignments;alignments stackoverflow heuristic;grained alignments;data natural language;automatically synthesizing code;aligned data training;snippets correspondence features;extracted snippets correspondence;synthesizing code;language nl code;alignments stackoverflow;snippets correspondence;structure extracted snippets;nl code pairs;code propose data;data training;quality aligned data;synthesizing code given;capture correlation snippets;code pairs;natural language;natural language nl;correctness nl code;data training classifier;snippets code propose;training classifier;data aligned data;snippets code;snippets", "pdf_keywords": ""}, "518cb6d4247bdebf21e2811f296b0c7372602a0a": {"ta_keywords": "random token masking;random tokens masking;masking random token;mask random tokens;token masking effective;masking tokens uniformly;token masking;masking tokens;tokens masking;heuristic masking random;tokens masking tokens;masks token gram;masked language models;pretraining masked language;masking random;jointly masks token;masking allows mlm;heuristic masking;new heuristic masking;masks token;mask random;masked language;random tokens;prior masking approaches;masking strategy;prior masking;tokens uniformly random;random token tokens;masking strategy based;masking approaches", "pdf_keywords": "masked language models;word training masked;masking subword tokens;nlp frontier masking;training masked language;mask word spans;masking subword;random span masking;grained tokenization;grained tokenization based;masked language;training language models;idea masking subword;multi grained tokenization;mask word;masking allows mlm;span masking;masking spans longer;training masked;masking spans;word training;span masking able;masking;subword tokens;uniform masking;novel approach masking;language models mlms;vocabulary word training;training language;subword tokens comprising"}, "aa9e0bf1e22563fca053578315b857688a0817cb": {"ta_keywords": "movie seeking simulator;learning agents online;dialogue systems;simulator based corpus;task oriented dialogue;reinforcement learning agents;oriented dialogue systems;learning agents;dialogue systems obstacles;seeking simulator presented;dialogues;seeking simulator;train reinforcement learning;agent based tasks;example dialogues;corpus example dialogues;interact simulator;interests reinforcement learning;movie seeking task;user simulator based;dialogue;algorithmic comparisons dialogues;user simulator;oriented dialogue;simulator designed movie;agents online;booking movie seeking;tasks movie ticket;reinforcement learning task;ticket booking movie", "pdf_keywords": "dialogue simulation;computer dialogue simulation;dialogue simulation task;statistical dialogue systems;training statistical dialogue;dialogue systems;collected data dialogue;statistical dialogue;dialogue helping users;human computer dialogue;dialogue dialogue policy;dialogue systems overcome;dialogue systems community;simulator operate dialog;dialogue policy;data dialogue dialogue;dialogue helping;dialogue policy central;data dialogue;dialogue dialogue;interacting natural language;computer dialogue;literature user simulation;user simulation;dialogue;dialog;researchers dialogue systems;based user simulation;user simulation requires;user simulator"}, "6b4ca249b3b28d3fee65f69714440c08d42cee64": {"ta_keywords": "gan training instance;gan training;high resolution generative;stability gan;explanation stability gan;generative image models;stability gan like;resolution generative;shows gan training;analysis shows gan;generative image;resolution generative image;gan;generative;gan like animals;shows gan;gradient penalties converges;gan like;gradient penalties generator;image models variety;zerocentered gradient penalties;gradient penalties;image models;training instance noise;learn high resolution;simplified gradient penalties;noise zerocentered gradient;training instance;convergence simplified gradient;zerocentered gradient", "pdf_keywords": "gradient descent locally;unregularized gradient descent;gradient descent;training algorithms gans;machines locally convergent;gans \ufb01nite learning;regularization deep;regularization deep neural;algorithms gans;algorithms gans \ufb01nite;problem regularization deep;locally convergent nash;results general gans;gans prove local;gradient penalties local;stone machines locally;general gans;general gans prove;penalties local convergence;unregularized gradient;stone machines;recognition theory continuous;deep neural networks;deep neural;descent locally convergent;stone stone machines;neural networks;gradient penalties;convergent nash;gans prove"}, "993c184553c41ca9134f149a3eb71b5bfab298b5": {"ta_keywords": "geopolitical significance cyberspace;collective threats hong;collective threats;narrative network information;hate online communities;significance cyberspace;narrative network maneuvers;significance cyberspace paper;accounts twitter;international reputation attacking;threats hong kong;threats hong;broader digital campaign;cyberspace paper propose;groups accounts engaged;twitter;online communities;cyberspace;strength global crisis;cyberspace paper;distinct narrative network;anti hate online;digital campaign network;geopolitical;campaign network;narrative network;understanding evolving geopolitical;evolving geopolitical;combine narrative network;evolving geopolitical significance", "pdf_keywords": ""}, "6916118de98cb5293425c8f74919395a003e6076": {"ta_keywords": "order tensor;th order tensor;order tensor product;tensor;tensor products;classifying text;order representation;tensor product;tensor products hermitian;text categorization;rst order representation;task classifying text;product tensor;classifying text pre;product tensor products;order representation present;tensor product tensor;text categorization task;counterpart text categorization;classifying;categorization task classifying;categorization;matrices rst order;representation xmath0;categorization task;new representation xmath0;order representation domain;representation;task classifying;propose new representation", "pdf_keywords": ""}, "affdfafb0293b44412ec99ff39b114de5e83eb98": {"ta_keywords": "hydraulic pah inflow;inflow depth injections;unconventional tight reservoir;post hydraulic pah;pah inflow depth;depth injections unconventional;hydraulic pah;tight reservoir;depth injections;post hydraulic;series post hydraulic;hydraulic;pah inflow;reservoir;inflow depth;injections unconventional tight;injections unconventional;fluid dynamics video;fluid dynamics;injections;inflow;fluid;unconventional tight;depth;pah;dynamics video;video series post;tight;dynamics video series;unconventional", "pdf_keywords": ""}, "88347f9f12b50590f50aefce4cf71b3a3f0bd138": {"ta_keywords": "language grounding 3d;language instructions autonomous;instructions autonomous agents;language grounding;reinforcement imitation learning;attention mechanism learns;reinforcement imitation;oriented language grounding;autonomous agents need;instructions autonomous;imitation learning;natural language instruction;using gated attention;learns policy execute;trainable neural;standard reinforcement imitation;learns policy;end trainable neural;autonomous agents;mechanism learns policy;language map visual;execute natural language;trainable neural architecture;imitation learning methods;grounding 3d environments;3d game engine;game engine simulate;image text representations;neural architecture task;visual elements actions", "pdf_keywords": "learning navigate maze;learning navigate;gated attention learns;approach learning navigate;representation instruction images;learns joint state;instruction image representation;learns;interactions instruction image;deep compositional framework;attention learns;input task oriented;language acquisition virtual;representation instruction;language mapping visual;visual elements actions;shot reinforcement learning;agent policy learner;joint representation instruction;propose deep compositional;language grounding environment;multimodal fusion;reinforcement learning tackle;language grounding;gated attention;deep compositional;instruction images;oriented language grounding;representations language;interactions instruction"}, "5e10a61b34867c6e5b32ed7a1359bd47bbfb5e2d": {"ta_keywords": "explanation based learning;learning calledabductive explanation;explanations training;multiple explanations training;explanations training instance;explanations training example;explanations constructed;calledabductive explanation based;possible explanations training;incorrect explanations constructed;inconsistent explanation problems;based learning calledabductive;applying explanation based;choose possible explanations;produces multiple explanations;domain theories;explanation problems occur;learning calledabductive;calledabductive explanation;explanations constructed paper;domain theories suffer;explanations;domain theory tractable;incorrect incorrect explanations;question domain theory;domain theory produces;inconsistent explanation problem;explanation based;incorrect explanations;multiple inconsistent explanation", "pdf_keywords": ""}, "e3862b1ff18dbb6a421b9efd1c0db22e09644b6d": {"ta_keywords": "co2 fluxes wetland;wetland ecosystems energy;fluxes wetland ecosystems;ecosystems energy range;fluxes wetland;co2 fluxes;wetland ecosystems;co2;ecosystems energy;wetland;energy range;energy range xmath0;ecosystems;xmath0 gev measured;range xmath0 gev;gev measured;xmath0 gev;gev;fluxes;energy;gev measured time;range xmath0;range;xmath0;measured;measured time;time", "pdf_keywords": ""}, "6f69fcacdf53a811ef18c5e9ac8ec58035dc43fc": {"ta_keywords": "speech recognition rnn;recognition rnn;rnn tm decoding;equivalent rnn based;recognition rnn important;performance rnn;performance rnn tm;automatic speech;speech recognition;automatic speech recognition;robust automatic speech;improve performance rnn;transducer based asynchrony;equivalent rnn;generalizes rnn loss;tm decoding algorithm;rnn based;tm decoding;rnn tm;rnn important task;generalizes rnn;algorithm proposed transducer;rnn loss;rnn based paper;relative equivalent rnn;transducers restricting alignments;asynchrony alignment asr;transducer based;automatic repeat;performance automatic repeat", "pdf_keywords": "train automatic speech;speech recognition automatic;automatic speech recognition;transducer based training;recognition speech;automatic speech;lattices automatic speech;recognition speech recognition;speech recognition;speech recognition speech;speech recognition systems;alignment automatic speech;speech recognition important;learning demonstrate transducer;generalizes rnn loss;transducer objective function;transducer based;training lattices automatic;new transducer objective;transducer objective;learning train automatic;transducer;generalizes rnn;rnn loss;recognition automatic;function generalizes rnn;rnn;new transducer;machine learning train;training lattices"}, "31dc1e65d61a431964c75bf2eec167bcd9dca0fa": {"ta_keywords": "position pool animals;selecting winning position;winning position pool;selecting winning;rule selecting winning;pool animals;winning position;position pool;animals;position;simple rule selecting;rule selecting;selecting;pool;winning;simple rule;rule;present simple rule;present;simple;present simple", "pdf_keywords": ""}, "86471bf927401bf88af83626797228c2bf10a282": {"ta_keywords": "causal attribution social;faithful causal;faithful causal chains;decisions causal attribution;interpretation faithful;interpretation social attribution;steps causal attribution;causal attribution;faithfulness faithful causal;explaining behavior;behavior interpretation;interpretation faithful vague;causal attribution attribution;model interpretation faithful;behavior interpretation social;attribution human behavior;causal chain decisions;human behavior interpretation;chain decisions causal;decisions causal;misalignment causal;expected social behavior;causal chains;interpretation social;misalignment causal chain;attribution social;interpretations propose;process explaining behavior;social attribution;causal chains aligned", "pdf_keywords": "intelligence explanations based;intelligence explanations guarantee;intelligence explanations;arti\ufb01cial intelligence explanations;human explanations;contrastive explanations highlight;based contrastive explanations;explanations highlight;contrastive explanations;explanations social;explanations based;explanations based new;human explanations social;explanations guarantee;cial intelligence explanations;component human explanations;explanations guarantee claims;explanations social attribution;explanations;explanation foil comprehended;wise selective rationalization;explanations highlight format;rationalization select predict;highlight interpretations propose;selective rationalization;predict selectverify interpretation;foil comprehended explainee;novel approach reasoning;reasoning based;highlight interpretations"}, "aa30949af5b59624224980e7d741ad8c084271ec": {"ta_keywords": "viral spread conspiracy;spread misinformation twitter;misinformation twitter investigate;vaccination including conspiracy;viral spread misinformation;misinformation twitter;platforms spread vaccination;investigation viral spread;media platforms spread;propagating misinformation vaccination;twitter world health;twitter investigate;videos twitter strategy;social media platforms;conspiracy theories videos;misinformation vaccination including;misinformation vaccination;sharing videos twitter;investigation viral;videos twitter;viral vaccination predicting;trust vaccination;spread misinformation;spread conspiracy theories;viral spread;results investigation viral;twitter strategy propagate;vaccination messages study;posted twitter world;investigates effectiveness viral", "pdf_keywords": ""}, "d35534f3f59631951011539da2fe83f2844ca245": {"ta_keywords": "face verification;identity matched photographs;adversarial networks jointly;face verification demonstrate;training generative adversarial;generate convincing identity;generative adversarial networks;adversarial;matched photographs;generative adversarial;training generative;adversarial networks;algorithm training generative;pairwise training birds;convincing identity matched;images common identity;pairs photorealistic distinct;learns latent codes;pairs photorealistic;pose;networks jointly learns;latent codes identities;shelf face verification;identity matched;pairwise training scheme;pose present;produce pairs photorealistic;pairwise training;appear depict individual;fool discriminator", "pdf_keywords": "networks gans;gans learn mappings;networks gans learn;metric generative adversarial;generative adversarial network;generative adversarial networks;propose generative adversarial;generative adversarial;gans learn;adversarial networks gans;identity sample diversity;identity subject photograph;generative neural;predicting identity;generative neural network;gans;adversarial network jointly;adapt dcgan;propose generative neural;model predicting identity;generative;architectures adapt dcgan;photograph use convolutional;dcgan;metric generative;diversity id;adversarial network;predicting identity subject;adversarial networks;network cnn"}, "61a07d1e4eaa831152e253b96b91808ef3a184b4": {"ta_keywords": "data annotation crowdsourcing;annotation crowdsourcing;annotation crowdsourcing shared;labeling public crowdsourcing;crowdsourcing;public crowdsourcing;crowdsourcing shared;crowdsourcing marketplaces present;public crowdsourcing marketplaces;project largest crowdsourcing;largest crowdsourcing;crowdsourcing marketplaces;annotation ideas tutorial;language data annotation;crowds tutorial;data annotation;introduction data labeling;annotation ideas;crowds tutorial session;crowdsourcing shared leading;largest crowdsourcing marketplaces;real crowds tutorial;data labeling;data labeling public;annotation;discuss annotation ideas;discuss annotation;labeling;efficient natural language;labeling process", "pdf_keywords": ""}, "aead4418733b998792deb9cbf198a834449e00d2": {"ta_keywords": "sequence models trained;neural sequence models;evaluating generalization takes;harmonic generation;order harmonic generation;neural sequence;evaluating generalization;generalizing systematically test;sequence models;sequenceto sequence models;generation higher order;symbolic national security;sequence models domain;generalization carefully constructed;verifier neural sequence;distribution generalization;methodology evaluating generalization;distribution generalization carefully;difficulty generalizing;generalization takes advantage;generalization takes;genetic algorithm automatically;symbolic symbolic;generalization;symbolic;distribution performance sequenceto;suites genetic algorithm;generalizing;modeling learning;test set symbolic", "pdf_keywords": "sequence integrator trained;sequence integrator predicting;neural sequence integrator;integrator predicting;collider ilc neural;models symbolic integrators;integrator trained;integrator predicting outcome;xmath0 boson final;learning neural sequence;xmath0 boson;integrator trained large;search xmath0 boson;collider ilc;collider;sequence integrator;symbolic solvers;sequence integrator veri\ufb01cation;international linear collider;symbolic integrators;sequence integrator paper;trained large scale;solvers;robust models symbolic;linear collider ilc;systematic generalization de\ufb01ciencies;ilc neural sequence;integrator veri\ufb01cation accurate;introduce neural sequence;training robustness"}, "9f7e317c6ef0bb15aacc9b19f0f0d00fee6c9a36": {"ta_keywords": "subsampled influence memorization;memorization data labels;benefits memorization generalization;memorization generalization;deep learning;memorization data;influence memorization values;deep learning algorithms;memorization values estimated;influence memorization;memorization generalization standard;outliers mislabeled;memorization;given deep learning;data labels phenomenon;empirical evidence explanation;memorization values;outliers mislabeled data;labels phenomenon;outliers;empirical evidence;fitting outliers mislabeled;evidence given deep;direct empirical evidence;learning algorithms known;labels phenomenon attracted;phenomenon based memorization;significant benefits memorization;learning;benefits memorization", "pdf_keywords": "examples memorized deep;memorization random labels;memorized deep;memorized deep learning;memorization label random;labeled examples memorized;examples memorized;neural networks memorize;memorization random;estimator memorization data;memorization label;networks memorize training;memorization data;estimator memorization;relationship memorization random;estimate expected memorization;expected memorization label;memorize training examples;subsampling memorization;memorization value estimator;memorization;subsampling memorization value;memorization data investigated;based subsampling memorization;value estimator memorization;reducing memorization;present deep learning;expected memorization;networks memorize;deep learning"}, "a1a8eeb64c0846070b10531061c18fed6d566f8c": {"ta_keywords": "computing manipulating vote;tied candidates random;strategic voting tie;voting tie breaking;winner tie breaking;manipulating vote;strategic voting;candidates random vote;voting tie;order tied candidates;rules computational complexity;tie breaking competition;tied candidates;scoring rules computational;candidates random;candidate unique winner;polynomial np hard;manipulating vote ask;complexity computing manipulation;unique winner set;complexity computing manipulating;random vote;impact strategic voting;randomly selected candidates;breaking competition winner;computational complexity;prove scoring rules;competition winner tie;relationship computational complexity;random vote discuss", "pdf_keywords": ""}, "c4919feb50c514e32eb0f4131399180c6f9a0d7d": {"ta_keywords": "surrogate function procurement;procurement cost function;procurement cost functions;polynomial procurement cost;function procurement cost;procurement cost;polynomial procurement;cost design method;optimal surrogate function;procurement cost total;facing procurement cost;optimal surrogate;function procurement;cost design;allocation second design;procurement cost article;procurement;optimization optimal design;allocation multiple customers;quasiconvex optimization optimal;cost total allocation;pricing mechanism;propose new pricing;optimization optimal;cumulative procurement cost;customer facing procurement;new pricing mechanism;focuses polynomial procurement;class procurement cost;uses optimal surrogate", "pdf_keywords": "allocation quasiconvex optimization;procurement cost function;allocation procurement costs;resource allocation quasiconvex;resource allocation procurement;optimal allocation;problem optimal allocation;online optimization;allocation procurement;optimal allocation resources;quasiconvex optimization problem;online resource allocation;online optimization framework;procurement costs;allocation quasiconvex;quasiconvex optimization;cost total allocation;procurement cost;allocation resources quadratic;presents quasiconvex optimization;procurement cost total;broad online optimization;facing procurement cost;allocation multiple customers;procurement costs paper;seller maximize revenue;minus procurement cost;maximize revenue;customers minus procurement;maximize revenue collected"}, "df56ccda14b5bc255a07fc061c50839e75563c5a": {"ta_keywords": "routing game parking;game parking traffic;parking urban mobility;parking traffic;parking related traffic;parking traffic impact;traffic selects parking;parking traffic selects;parking classical routing;queue routing game;parking urban;game parking;impact parking urban;traffic traffic;traffic;congestion route choices;routing game model;selects parking;traffic impact;congestion route;new routing game;parking zones;routing game;overall congestion route;game model urban;parking related;queuing game model;urban mobility;classical routing game;parking", "pdf_keywords": ""}, "4218563e1fe927440e00bf0abe5cb1e037deaf71": {"ta_keywords": "asymmetric autoencoders outperforms;autoencoders outperforms;predicting accuracy;learns threshold model;target domain accuracy;autoencoders outperforms previous;autoencoders;asymmetric autoencoders;confidence predicting accuracy;method learns threshold;method asymmetric autoencoders;learns threshold;practical method learns;predicting accuracy fraction;general identifying accuracy;domain accuracy;methods predicting target;machine learning;method learns;predicting target domain;machine learning characterized;world machine learning;methods predicting;learning characterized mismatches;identifying accuracy just;predicting target;identifying accuracy;accuracy using labeled;model confidence predicting;domain accuracy using", "pdf_keywords": "domain adaptation;domain adaptation label;adaptation label shifts;consider domain adaptation;adaptation label;importance weighting based;synthetic shifts imagenet;accuracy target distribution;shifts imagenet;predicting accuracy;estimates deep models;importance weighting;accuracy machine learning;estimates deep;adaptation;deep models trained;models trained;target accuracy arbitrary;deep models;predicting accuracy machine;estimates accuracy target;target accuracy;shifts imagenet different;shifting form importance;distribution shifting;propose importance weighting;machine learning;estimate target accuracy;accuracy target;learning model trained"}, "2d6d26c118f43f3ab314d07f58c20df6e89a13af": {"ta_keywords": "particles plasmid driven;method generation influenza;generation influenza virus;particles plasmid;influenza virus;generation influenza;virus like particles;influenza virus like;like particles plasmid;plasmid driven;influenza;plasmid;particles;virus;like particles;virus like;method generation;new method generation;generation;new method;method;present new method;like;driven;present;present new;new", "pdf_keywords": ""}, "92a8f7f09f3705cb5a6009a42220a6f01ea084e8": {"ta_keywords": "actionable steps;actionable knowledge language;extracting actionable knowledge;actionable knowledge;action intelligent;actionable steps article;existing demonstrations semantically;extracting actionable;set actionable steps;map precisely actions;demonstrations semantically;learning explicit step;tasks expressed natural;precisely actions;level tasks expressed;demonstrations semantically translates;actions;actionable;plans admissible actions;high level tasks;action intelligent man;tasks expressed;level tasks;action;admissible actions;step examples act;learning explicit;fact action intelligent;tasks mid level;precisely actions man", "pdf_keywords": "machine learning embodied;generate actionable knowledge;activities human annotators;embodied visual task;extracting actionable knowledge;visual task completion;learning embodied;possibility extracting actionable;learning embodied visual;actionable knowledge pre;task completion;language navigation tasks;actionable knowledge;human annotators;human annotators required;trained language;visual task;language models training;actionable knowledge high;vision language navigation;human activities human;complex human activities;human activities;vision language;automatically generate actionable;groundable actions;trained language models;activities human;navigation tasks;pre trained language"}, "59653e5cfa854a17c2ffcb86f2a454f27e12c716": {"ta_keywords": "gender pronouns search;translating gender pronouns;translated gender pronouns;diversity human translations;human translations;translating gender;human translations study;bias translating gender;gender pronouns neural;distribution translated gender;machine translation;translated gender;neural machine translation;pronouns search strategies;pronouns search;pronouns neural machine;gender pronouns;machine translation nmt;translations examining;generated translations;real translations examining;translations study;translations;agreement generated translations;pronouns neural;known bias translating;generated real translations;bias translating;generated translations ground;translations ground", "pdf_keywords": "machine translation trained;translation gender pronouns;translating gender pronouns;bias translating gender;rates female pronouns;analysis gender pronouns;relates translation gender;gender pronouns battery;translation gender;neural machine translation;translating gender;pronouns battery diversity;machine translation;pronouns 2016 wmt19;observed machine translation;gender pronouns 2016;gender pronouns;gender pronouns paper;female pronouns word;female pronouns;translation trained;pronouns word frequency;bias translating;outputs relates translation;translation trained ground;known bias translating;distribution sentence level;truth translations grams;sentence length distribution;grams uses discriminative"}, "ac41e0ef30b6f9ee4930ac85dc46a9b50a1963d2": {"ta_keywords": "labels crowdsourced;crowdsourced labels;crowdsourced label;crowdsourcing important machine;crowdsourcing;training data crowdsourcing;crowdsourced labels crowdsourced;crowdsourced label labels;crowdsourced crowdsourced labels;crowdsourced;labels crowdsourced label;data crowdsourcing;crowd sourcing algorithm;interface crowdsourced;crowdsourced crowdsourced;crowdsourcing important;data crowdsourcing important;propose interface crowdsourced;performance crowd sourcing;interface crowdsourced crowdsourced;crowd sourcing;measuring performance crowd;object crowd;participants crowd estimated;participants crowd;voting incentive;crowd estimated probability;particular object crowd;incentive;voting incentive compatible", "pdf_keywords": ""}, "76f02d20e02c6baf39fee8f115cd94e4ceacf32b": {"ta_keywords": "peer review;quality peer review;review peer;observe novice reviewers;peer review peer;paper resubmission reviewers;reviewers exhibit bias;novice reviewers tend;reviewers tend;review peer based;reviewers tend underrate;peer review pipeline;resubmission reviewers;analysis reveals reviewers;reviewers master junior;novice reviewers;reveals reviewers;reveals reviewers likely;investigate reviewers;reviewers;reviewers master;reviewers receive;resubmission reviewers receive;focusing population reviewers;investigate reviewers exhibit;reviewers likely receive;reviewers exhibit;reviewers likely;examine quality peer;reviewers constitute large", "pdf_keywords": ""}, "bf0beed35ea09aab56027d64c744098cc963fbde": {"ta_keywords": "detection transient dynamics;durations transient phases;change detection transient;transient dynamics;transient dynamics studied;2d ising model;durations transient;dynamic cusum algorithm;alarm durations transient;detection transient;mc simulation dynamics;detect change quickly;series transient phases;transient phases;instantaneously series transient;simulation dynamics;persistent distribution instantaneously;dynamic cumulative sum;considered dynamic cumulative;criteria durations transient;dynamic cumulative;dimensional 2d ising;carlo mc simulation;transient phases completely;magnetic field algorithms;transient phases infinity;weighted dynamic cusum;quickest change detection;dynamic cusum;change detection", "pdf_keywords": "wd cusum algorithm;weighted cusum wd;cusum algorithm;cusum algorithm conduct;weights wd cusum;weighted cusum;cusum algorithm paper;wadd durations transient;detects change quickly;refer weighted cusum;wd cusum;cusum wd;bound wadd durations;cusum wd cusum;detects change;choosing weights wd;whittle problem transient;lower bound wadd;weights wd;particular case transient;durations transient;case transient;algorithm conduct asymptotic;rule detects change;wadd durations;proposed algorithms;performance algorithms;needed change distribution;minimal number rounds;algorithms"}, "f1005edfa1fbc4ea0d9a90345388bda8a01e69ed": {"ta_keywords": "confluent vessel trees;forming vessel trees;tubular graphs;tubular graphs critical;reconstructing confluent vessel;discrete tubular graphs;vessel trees using;graph enforcing confluence;vessel trees;bifurcations tubular graphs;directed flow confluence;arborescence directed graph;flow confluence;curves forming vessel;confluence simple flow;vessel trees enforce;reconstruction accuracy bifurcations;capillary sub voxel;practical algorithm reconstructing;vasculature thousands bifurcations;algorithm reconstructing confluent;forming vessel;confluence;minimum arborescence directed;confluence continuous oriented;improved reconstruction;confluence continuous;technique bifurcations tubular;algorithm reconstructing;flow confluence high", "pdf_keywords": "curves vessel trees;vessel tree reconstruction;vessel trees curvilinear;unsupervised vessel tree;vessel tree estimation;visualization vasculature bifurcations;curves vessel;con\ufb02uent vessel trees;vessel trees;forming vessel trees;curves forming vessel;vessel tree;curvilinear structure detection;approach vessel tree;vessel trees propose;trees curvilinear structure;predicting con\ufb02uent vessel;oriented curves vessel;moruent vessel trees;curvilinear structure analysis;detection tubular graph;vessels thousands bifurcations;3d visualization vasculature;unsupervised vessel;vessel trees paper;level vessel estimation;visualization vasculature;present unsupervised vessel;techniques curvilinear structure;dimensional curvilinear structure"}, "794b0a1e9719d809ebdf2ef87ff84c2039bfdd52": {"ta_keywords": "wireless hartarts protocol;wireless hartart protocol;security wirelesshart protocol;wirelesshart protocol stack;present wireless hartarts;wirelesshart protocol;wireless hartarts;wireless hartart;hartart protocol mainly;resources wireless hartart;security wirelesshart;hartarts protocol;hartart protocol;reliability security wirelesshart;hartarts protocol stack;protocol stack designed;wirelesshart;protocol;protocol stack;protocol mainly used;protocol mainly;protocol stack object;present wireless;event driven architecture;resources wireless;wireless;designed based event;rom resources wireless;control spring;event driven", "pdf_keywords": ""}, "ae30f8fc5a969d2d14ae066db4cd07d86fadbf42": {"ta_keywords": "sulfones useful analgesics;sulfones having agonist;agonist activity opiate;narcotic antagonists;narcotic antagonists anti;methionine5 enkephalin sulfoxides;sulfones useful;sulfoxides sulfones useful;sulfoxides sulfones;nonaddicting narcotic antagonists;enkephalin sulfoxides sulfones;activity opiate receptors;opiate receptors disclosed;opiate receptors;analgesics nonaddicting narcotic;sulfoxides sulfones having;sulfoxides;antagonists anti diarrheal;activity opiate;enkephalin sulfoxides;sulfones;opiate;useful analgesics;analgesics;cards methionine5 enkephalin;anti diarrheal agents;agonist activity;diarrheal agents;having agonist activity;methionine5 enkephalin", "pdf_keywords": ""}, "ffe1416bcfde82f567dd280975bebcfeb4892298": {"ta_keywords": "speech recognition asr;speech recognition;faster rnns;neural networks rnn;recurrent neural networks;training faster rnns;networks rnn;connectionist temporal classification;recognition asr tasks;networks rnn rnn;automatic speech recognition;rnns;training decoding;automatic speech;recognition asr;joint training decoding;training decoding transformer;end automatic speech;recognition based recurrent;training decoding methods;rnn rnn best;faster rnns assists;temporal classification ctc;speech recognition based;rnn best;rnn rnn;based recurrent neural;rnns assists;recurrent neural;classification ctc translformer", "pdf_keywords": ""}, "c50f98961c951fe3fbdb6f375beb28e40a6b0581": {"ta_keywords": "peer review process;peer review;incentives reviewers participate;improve peer review;incentivizes participating reviewers;participating reviewers provide;participating reviewers;incentives reviewers;lack incentives reviewers;demand reviewers large;reviewers participate;demand reviewers;peer reviewed;reviewers participate expend;overwhelming demand reviewers;review process large;peer reviewed publications;effortful reviews;reviewers provide;databases peer reviewed;review process;honest effortful reviews;effortful reviews identify;submissions lack incentives;reviewers;paper submission review;submission review process;high quality submissions;quality submissions;reviewed publications", "pdf_keywords": "peer review mechanism;review mechanism;review mechanism design;reward expert reviewers;pay participating reviewers;peer review;reviewers signals peer;participating reviewers;peer review evolving;signals peer review;participate review process;submissions propose mechanism;criticisms peer review;expert reviewers publishing;expert reviewers;participate review;reviewers publishing frequently;review process simultaneously;reviewers publishing;review slots;review slots papers;review process hmip;participating reviewers novel;review process;allocates review slots;high quality submissions;submission review process;mechanism allocates review;information reviewers;reviewers"}, "3febb2bed8865945e7fddc99efd791887bb7e14f": {"ta_keywords": "deep bidirectional language;deep contextualized word;entailment sentiment analysis;deep bidirectional;textual entailment sentiment;polysemy representations easily;use deep bidirectional;contextualized word representation;word representation models;challenging nlp;model polysemy representations;deep contextualized;polysemy representations;large text corpus;text corpus train;entailment sentiment;challenging nlp problems;train word vectors;question answering textual;contexts model polysemy;type deep contextualized;bidirectional language model;word vectors introduce;nlp problems including;nlp;language model trained;answering textual entailment;answering textual;text corpus;textual entailment", "pdf_keywords": "deep contextualized word;deep neural language;deep contextualized;contextualized word representation;deep representations bilmms;bilmms deep sense;representations bilmms deep;bilmms deep;neural language model;challenging language understanding;natural language processing;speech tagging textual;deep neural;type deep contextualized;able disambiguate speech;speech tagging;textual entailment task;bilm able disambiguate;word representation directly;al natural language;neural language;language model;language model 2016;disambiguate speech;natural language;language understanding;challenging language;tagging textual entailment;contextualized word;word representation"}, "aa2bd932a2ecb6e07c768bcf0dc119f0cd20f6e0": {"ta_keywords": "textual similarity measures;similarity measures matching;textual similarity;learning textual similarity;identifying approximately duplicate;similarity measures;similarity;approximately duplicate database;duplicate database records;competitive competition identifying;duplicate database;matching;measures matching;competition identifying;combining learning textual;approximately duplicate;authors compare;competition identifying approximately;matching paper present;matching paper;information integration;database records refer;measures matching paper;textual;records refer entity;compare methods combining;authors compare methods;database;database records;competitive competition", "pdf_keywords": ""}, "85a18aafcffdcc4eafcb9e5eda0abb8aa5cb8c3b": {"ta_keywords": "sdn big data;big data synergize;big data technological;sd big data;big data apache;storage processing big;process big data;processing big data;sdn utilize large;big data platforms;distributed storage processing;sdn big;distributed storage;servers distributed storage;apache hadoop;data apache hadoop;big data;hadoop;big data paper;behavior big data;apache hadoop point;opportunities sdn big;network architectural support;storage processing;network architectural;data technological;sdn utilize;processing big;data platforms;hadoop point", "pdf_keywords": ""}, "d89f4534d1a87005cdf470ec5d8154998d5abdc7": {"ta_keywords": "parity queries powerful;parity queries parity;learn parity models;parity queries;queries parity;targets parity queries;prediction serving;prediction serving systems;parity query parallel;parity models;networks learn parity;queries single parity;predictions parm;parm uses neural;parity;parity query encodes;predictions parm novel;failures prediction serving;models prediction serving;queries parity query;representation parity queries;learn parity;unavailable predictions parm;output parity model;parity models enable;parity query;single parity query;slowdowns failures prediction;parm uses;parity model", "pdf_keywords": "utilizes parity encoder;decoder frontend prediction;parity encoder;fast encoders;parity encoder decoder;simple fast encoders;frontend prediction serving;prediction serving;encoder;utilizes parity;prediction serving systems;models prediction serving;fast encoders decoders;computation machine learning;frontend prediction;encoders;serving systems parity;reconstruct unavailable predictions;parity models building;machine learning inference;parity models;parity models new;encoder decoder frontend;encoders decoders;tasks utilizes parity;parity;prediction serving incur;learning models prediction;machine learning models;image classification speech"}, "e95a96dec775cc792b763f4eec13343c22e850e1": {"ta_keywords": "xmath0 superconductors conducted;high xmath0 superconductors;xmath0 superconductors;members hydrae association;superconductors conducted canada;superconductors;telescope cfht report;hawaii telescope cfht;superconductors conducted;members hydrae;telescope cfht;active members hydrae;survey high xmath0;france hawaii telescope;hawaii telescope;high xmath0;spectroscopic survey high;hydrae association;telescope;learning robotics report;xmath0;agents computer vision;machine learning robotics;robotics report;agents computer;high resolution spectroscopic;spectroscopic survey;members better artificial;hydrae;intelligence society officer", "pdf_keywords": ""}, "49984bc327ef6952118c4b871eeef2a907f7a4ed": {"ta_keywords": "regularized learning;regularized learning algorithms;classes regularized learning;learning regularized;video regularized learning;learning regularized environment;regularized learning regularized;class stochastic optimization;zero sum games;stochastic optimization;stochastic optimization problems;rate adversary;sum games;rate adversary class;adversary class fluid;utilities converges optimal;computation class stochastic;dynamics video regularized;learning algorithms form;player zero sum;regularized;stochastic;converges optimal;class stochastic;new class stochastic;learning;bound rate adversary;optimal;normal form games;sum games paper", "pdf_keywords": "regularized regret algorithms;regret minimization equilibria;learning regret minimization;regret algorithms;algorithm online games;regret algorithms recency;zero sum games;games learning regret;regret minimization;regularized regret;online games learning;online learning algorithm;class regularized regret;sum games;learning regret;regret dynamics;learning algorithm online;games learning;smoothness condition game;online games;condition game;algorithms recency bias;sum games players;average weights game;uses regret dynamics;sum weights players;online learning;condition game introduced;weights game;multi player games"}, "c460fd4a0dc86bc518f9a8e982bc48faf1efb942": {"ta_keywords": "posts social media;collective attention broadcasts;social media;study broadcast scheduling;online social media;broadcast scheduling;like facebook twitter;social networks;attention broadcasts timelines;production social networks;social media platforms;facebook twitter;twitter;posts social;social networks formulate;twitter online social;social platforms;content maximise attention;online social platforms;facebook;attention broadcasts;broadcast scheduling problem;scheduling;social media enabled;like facebook;broadcasts timelines;facebook twitter online;social platforms today;fewer posts social;frequency publishing content", "pdf_keywords": "circadian rhythms microblogs;rhythms microblogs;social network timelines;timeline monotony subscribers;microblogs exhibit bursty;time consecutive tweets;broadcast social network;users microblogs exhibit;rhythms microblogs studying;social activity traces;users microblogs;tweets special users;broadcast social;study broadcast scheduling;microblogs exhibit;broadcast scheduling;consecutive tweets;speci\ufb01es broadcast social;tweets;microblogs studying distribution;existence bursty circadian;formulate broadcast scheduling;tweets special;microblogs;online social networks;attention online social;news channels actively;social network;characteristics users microblogs;consecutive tweets sina"}, "1a9c89cb2e57e06dadd4c2fab5fae1bfdbb3b6d5": {"ta_keywords": "preference networks nets;conditional preference networks;preference networks;preferences coalitions;probabilistic networks provide;collective reasoning tasks;preferences coalitions coalitions;probabilistic networks;expresses preferences coalitions;probabilistic conditional preference;collective reasoning;perform collective reasoning;class probabilistic networks;aggregate pair povms;coalitions cp nets;cp nets probabilistic;nets probabilistic conditional;nets probabilistic;coalitions;povms based aggregate;nets class probabilistic;aggregate povm pair;based aggregate povm;aggregate povm;coalitions coalitions;aggregate collection polynomial;probabilistic;pair povms based;coalitions cp;conditional preferences paper", "pdf_keywords": ""}, "f5a0c6593ba95d23c025608ce9280848da8b929f": {"ta_keywords": "gene mention task;corresponding gene mentions;presents bioinformatics;prize gene mention;bioinformatics;presents bioinformatics video;paper presents bioinformatics;gene mention;bioinformatics video entry;gene mentions;bioinformatics video;hadron collider lhc;lhc short;collider lhc short;sentences corresponding gene;collider lhc;lhc short note;lhc;hadron collider;prize gene;competition prize gene;identify substrings sentences;statistical analysis xmath0;large hadron collider;identify substrings;corresponding gene;collider;gene;substrings sentences corresponding;gene mentions variety", "pdf_keywords": ""}, "92891a984b45df5fc764d81bf9bcd42e7e7ed1c7": {"ta_keywords": "network epidemic model;game nonlinear dynamics;network epidemic;multi network epidemic;differential game nonlinear;game nonlinear;epidemic model spread;epidemic model;spread malware;nonlinear dynamics fluid;equilibrium global;equilibrium;epidemic;nonlinear dynamics;spread malware present;dynamics fluid dynamics;desired local equilibrium;model spread malware;equilibrium open loop;differential game;network managers invest;dynamics;global equilibrium;induced equilibrium global;dynamics fluid;equilibrium open;fluid dynamics;loop differential game;induced equilibrium;equilibrium global equilibrium", "pdf_keywords": ""}, "3cc790174d138d7904189df997d5763f1793dedf": {"ta_keywords": "annotator agreement normalization;agreement normalization task;agreement normalization;inter annotator agreement;annotator agreement;common annotation;common annotation tasks;annotated word forms;labels normalized wordforms;chancecorrected agreement measures;evaluation inter annotator;normalized wordforms;annotation tasks;agreement measures;annotated word;annotations;labels normalized;way annotated word;annotations match;annotation;agreement measures paper;inter annotator;annotation tasks important;differs common annotation;annotations match different;annotator;normalized;common chancecorrected agreement;class labels normalized;normalization task", "pdf_keywords": ""}, "1d255aeabcb87929742280251007fd8c01bbe914": {"ta_keywords": "polyoxometalates cluster pmo11fe;polyoxometalates cluster;catalytic activities phosphomolybdic;substituted polyoxometalates cluster;acid clusters investigated;phospholipidic acid clusters;acid clusters;organized critical point;self organized critical;polyoxometalates;electrostatic interactions dumbbells;critical point ising;activities phosphomolybdic acids;electrostatic interactions;phosphomolybdic acids;cyclooctene;efficiency cyclooctene;cyclooctene 50 recycled;cluster pmo11fe motion;molecular dynamics;organized critical;molecular dynamics simulations;electrostatics;activities phosphomolybdic;phosphomolybdic acids mo;ising model critical;materials catalytic activities;recycled reactions;iron substituted polyoxometalates;electrostatics computer simulations", "pdf_keywords": ""}, "737f9a32d7f4007aa9526556c256ed4a182aec69": {"ta_keywords": "optimal equilibrium robust;linear quadratic games;equilibrium robust;quadratic games;socially optimal equilibrium;linear quadratic game;robustness linear quadratic;quadratic games presence;quadratic game;optimal equilibrium;equilibrium robust small;convex program;equilibrium stable;induce socially optimal;robustness linear;quadratic game framed;socially optimal;game framed convex;equilibrium;convex conditions guarantee;deviations players feedback;develop convex;players feedback strategies;develop convex conditions;equilibrium stable respect;study robustness linear;feedback strategies paper;robustness;induced equilibrium stable;optimal", "pdf_keywords": ""}, "4aa72e4232ae809ea1a9fe142275da25ba930655": {"ta_keywords": "nash equilibrium games;dynamics global nash;gradient avoid nash;convergence nash equilibria;games surprisingly convex;global nash equilibria;linear quadratic games;convergence nash;policy gradient algorithms;global nash;quadratic games satisfy;nash equilibrium;quadratic games;coincide nash equilibrium;counterexample policy gradient;nash equilibria continuous;local convergence nash;equilibrium games surprisingly;equilibrium games;nash equilibria counterexample;avoid nash equilibria;nash equilibria generate;multi agent learning;games satisfy conditions;policy gradient;policy gradient avoid;nash equilibria;gradient play;games empirically observe;agent learning provide", "pdf_keywords": "convergence nash equilibria;converges nash equilibria;single agent optimal;global convergence nash;quadratic games gradient;games policy gradient;agent optimal control;nash equilibria linear;equilibrium policy gradient;linear quadratic games;equilibrium nash;policy gradient algorithms;stability equilibrium nash;optimization single agent;agent optimal;nash equilibrium policy;convergence nash;policy optimization provably;nash equilibrium;gradient policy optimization;quadratic lun game;multi agent competitive;particular nash equilibria;neighborhood nash equilibrium;policy gradient guarantee;equilibrium nash equilibrium;algorithms multi agent;nash equilibria locally;nash equilibria game;policy optimization"}, "b62ce3135ed6065863c0dec26037fd07c081abba": {"ta_keywords": "language causality offers;language causality;counterfactual target label;counterfactual target;pfaffian language causality;counterfactually probabilistically;natural language processing;natural language;counterfactual;causality offers clarity;accords counterfactual target;difference counterfactually probabilistically;counterfactually probabilistically differential;focus natural language;counterfactually;document accords counterfactual;indirect causal;spurious associations confounding;indirect causal effects;spurious associations;accords counterfactual;direct indirect causal;causality;clarity spurious associations;causal;language processing;causal effects;makes difference counterfactually;models sensitive spurious;difference counterfactually", "pdf_keywords": "cnns trained sentiment;trained sentiment;trained sentiment analysis;cnn revise negative;train cnn revise;cnn revise;sentiment analysis trained;crowdsourced sentiment;crowdsourced sentiment analysis;predictive sentiment;cnns learn;sentence statistical learning;models natural language;natural language inference;sentiment analysis train;crowdsourced crowdsourced sentiment;cnnns;train cnns learn;sentiment analysis nli;sentiment paper explore;negative movie review;cnn;cnns;examples sentiment analysis;cnns learn salient;analysis train cnn;changes train cnns;examples sentiment;revise negative movie;train cnn"}, "ca86a63362e51eea2e213ae2d3faed668ec1ad74": {"ta_keywords": "knowledge bases commonsense;commonsense knowledge representation;commonsense expression extraction;bases commonsense knowledge;commonsense knowledge;semantic similarity detection;semantic similarity;constructing knowledge bases;bases natural language;multi word commonsense;use knowledge bases;approach semantic similarity;knowledge bases;bases commonsense concepts;knowledge bases natural;knowledge bases capable;knowledge representation reasoning;similarity detection;word commonsense expression;addition semantic similarity;similarity detection paper;bases commonsense;word commonsense;decomposition knowledge bases;knowledge representation;commonsense concepts;natural language texts;flexible nuanced reasoning;present knowledge bases;expression extraction", "pdf_keywords": ""}, "a1da1d600acd506b80c8870d293a756c70791683": {"ta_keywords": "bilingual lexicon induction;distant bilingual lexicon;bilingual lexicon;lexicon induction ilr;lexicon induction;embedding spaces empirically;large scale lexicons;empirically weakens languages;etymologically distant bilingual;language pairs muse;language pairs;particularly embedding spaces;distant bilingual;does particularly embedding;particularly embedding;scale lexicons;18 language pairs;bilingual;embedding spaces;lexicons;scale lexicons addition;spaces empirically;isometry embedding spaces;languages;embedding;lexicon;isometry embedding;lexicons addition;assumption isometry embedding;embedding spaces don", "pdf_keywords": ""}, "f184908270fc934ab74438a0aaac7a43a5eae6d2": {"ta_keywords": "neural summarization models;neural summarization;document representations summarization;representations summarization modeling;summaries learn interpretable;generated summaries learn;art neural summarization;summarization modeling;summarization models;representations summarization;summarization modeling document;single document summarization;document summarization;summarization models preserve;generated summaries;summaries learn;quality generated summaries;document summarization relied;document generating summary;interpretable document representations;summarization;document representations;summarization relied;summarization relied modeling;summaries;generating summary use;generating summary;learn interpretable document;structure document generating;summary use structure", "pdf_keywords": "summarization structural attention;summarization employ neural;structured representations summarization;representations summarization models;document representations summarization;abstractive summarization structural;summarization structural;abstractive summarization structsum;structure summarization;summarization incorporates latent;summarization task explicit;representations summarization task;structured attention module;structure summarization framework;structured attention networks;structure attention module;summarization models;representations summarization;summarization incorporates;document summarization incorporates;abstractive summarization;document structure summarization;abstractive summarization employ;document summarization models;summarization structsum;summarization structsum framework;summarization models model;summarization framework state;approaches abstractive summarization"}, "99ac83b990af1fc591db5b676300a7c002905dae": {"ta_keywords": "multi view learning;multiclass semi supervised;view learning;multi view datasets;semi supervised learning;semi supervised;view multiclass semi;data view multiclass;view datasets techniques;supervised learning information;view multiclass;supervised learning;multi view;learning information datapoints;supervised;hierarchical class constraints;scores data view;multiple views;views;extended multi view;multiple views proposed;learning information;number multi view;data view;view datasets;linear programming formulations;multiclass semi;present multiple views;views proposed techniques;multiclass", "pdf_keywords": ""}, "3f90668994d6e5949a530dfc84a10b492ff35cfa": {"ta_keywords": "shallow semantic parsing;task shallow semantic;shallow semantic;sentences training;structure sentence embeddings;parsing semantic;similar sentences training;semantic parsing;sentence embeddings;semantic parsing especially;sentences training set;semantic parsing semantic;parsing semantic labels;sentence embeddings embedding;data shallow semantic;structure sentence experiments;relation prediction sub;structurally similar sentences;infer structure sentence;parsing especially;parsing;semantic labels structurally;sentence experiments;sub task shallow;relation prediction;sentence experiments approach;semantic labels;sentences;sentences method;sentences method based", "pdf_keywords": ""}, "fce19dd512a82693ab9070049ed426179eca8856": {"ta_keywords": "textual content;analysis textual content;mass collaboration web;natural language processing;natural language;discover linguistic structures;collaboration wikis web;collaboration web;discover linguistic;analyze web;processing nlp;mass collaboration wikis;textual;describes natural language;linguistic structures data;analyze web based;processing nlp used;collaboration wikis;web forums;web forums debate;web based resources;wikis web forums;nlp used analyze;language processing nlp;explain analyze web;nlp used;debate platforms blog;linguistic structures;nlp;method discover linguistic", "pdf_keywords": ""}, "0d22ce72a62419086fd4860a4671991846cd492b": {"ta_keywords": "cryptanalysis quantum key;cryptanalysis quantum;framework cryptanalysis quantum;quantum key distribution;paper propose cryptanalysis;aerobicobicobic block ciphers;cryptanalysis;quantum key;cryptanalysis framework;cryptanalysis framework cryptanalysis;propose cryptanalysis framework;propose cryptanalysis;key distribution qkd;framework cryptanalysis;qkd aerobicobicobic block;distribution qkd aerobicobicobic;ciphers;block ciphers;qkd aerobicobicobic;block ciphers used;ciphers used;distribution qkd;qkd;ciphers used improve;quantum;key distribution;medical imaging systems;medical imaging;imaging systems;range medical imaging", "pdf_keywords": ""}, "fb6ef2d6fbd1ea4905070077ab6c5b0108f2c38a": {"ta_keywords": "sarcasm detection languages;approach sarcasm detection;research sarcasm detection;sarcasm detection;learning approach sarcasm;phenomenon research sarcasm;language tweets;research sarcasm;language tweets provide;analysis language tweets;approach sarcasm;sarcasm;thezech english datasets;detection languages english;detection languages;detection languages paper;classifiers;features thezech english;tweets;presents machine learning;tweets provide explanation;english datasets;tweets provide;language;languages;thezech english;morphology richzech;morphology examining;language independent;morphology morphology richzech", "pdf_keywords": ""}, "b8e49216e5b4a017342b0be5f6fbbd79e690a1c7": {"ta_keywords": "optimal auctions;network online auctions;optimal auctions designing;near optimal auctions;auctions designing;auctions designing incentive;incentive compatible auction;auction;auction maximizes;auctions approach;compatible auction maximizes;online auctions;auctions approach possible;multilayer neural;auction maximizes expected;online auctions approach;design multilayer neural;auctions;multilayer neural network;compatible auction;multilayer;neural network online;deep learning;tools deep learning;neural network;design multilayer;neural;designing incentive;deep learning shaping;revenue intricate task", "pdf_keywords": "auction approximated convolutional;revenue optimal auction;optimal auction;additive bidder revenue;designing auctions satisfy;auction;auctions satisfy;optimal auction approximated;additive bidder;deep neural;auctions;designing auctions;bidder revenue optimal;neural networks explicitly;strategy incentive compatibility;neural network goal;auctions satisfy dominant;incentive compatibility dsic;auction approximated;bidder revenue;strategy incentive;networks explicitly encode;problem designing auctions;incentive compatibility;deep learning powerful;dominant strategy incentive;incentive;neural;deep learning;networks explicitly"}, "f297e939212780637705eba8798c9a386befd771": {"ta_keywords": "pivot translation allows;language model triangulation;pivot language model;translation language pairs;pivot target translation;translation models;pivot translation;target translation models;translation process;translation time pivot;translation language;high translation accuracy;used translation process;languages particular triangulation;translation accuracy;translation process method;pivot language;translation accuracy conventional;propose pivot language;translation models source;triangulation method translates;information source translation;time pivot translation;target translation;translates combining source;translation allows translation;language pairs;allows translation language;information pivot phrases;pivot phrases", "pdf_keywords": ""}, "8a902a848c3710290f04f2d59030f5670d3433f8": {"ta_keywords": "morphology predictive errors;nlp models morphology;morphology predictive;morphological complexity language;increases morphological complexity;morphological feature predictive;morphological complexity;brain morphological feature;brain morphological;models morphology predictive;human brain morphological;morphological feature;morphological features;speculate morphology discriminative;morphology discriminative;morphology discriminative morphologically;morphological;morphologically simple languages;morphology;morphology increases morphological;speculate morphology;importance morphology increases;models morphology;discriminative morphologically;importance morphology;discriminative morphologically simple;morphologically;different morphological features;morphologically simple;different morphological", "pdf_keywords": ""}, "ab48fb72541653f40523caa9fcaac9cb84bf3373": {"ta_keywords": "separation multiple speakers;neural beamforming frontends;multi speaker speech;speaker speech recognition;joint source separation;neural beamforming;source separation;using neural beamforming;channel multi speaker;multi speaker;independent vector analysis;speech recognition;automatic speech;source separation dereverberation;mixtures speakers;retraining speakers demonstrated;automatic speech recognition;neural source model;speakers demonstrated mixtures;mixtures speakers propose;demonstrated mixtures speakers;speech recognition demonstrate;speaker speech;speech recognition based;retraining speakers;independent vector;frontend joint source;algorithm neural source;separation dereverberation based;end automatic speech", "pdf_keywords": "neural beamformers separation;blind source separation;source separation mixtures;joint source separation;source separation;beamformers separation;reverberation overlapped speech;source separation dereverberation;beamformers separation typically;multi speaker speech;speaker speech recognition;overlapped speech noise;propose neural beamformer;channel multi speaker;neural beamformer;multi speaker;neural beamforming;speech recognition challenging;neural beamformers;using neural beamforming;separation dereverberation based;speech recognition;separation mixtures multiple;overlapped speech;mimo speech automatic;sources single spectrogram;neural beamforming frontends;frontends neural beamformers;separation mixtures;speech recognition called"}, "3c5d3bbb73aa0e3e969a25487a81b5b1f0c14044": {"ta_keywords": "knowledge graph largescale;structure knowledge graph;scale knowledge graph;knowledge graph captures;knowledge graph identification;knowledge graph;leveraging ontological information;propose knowledge graph;knowledge graph utilizing;confidences leveraging ontological;largescale information extraction;leveraging ontological;reasoning structure knowledge;web scale knowledge;information extraction;information extraction systems;extraction confidences leveraging;inclusion knowledge graph;jointly reasoning structure;structure knowledge;scale knowledge;captures information entities;graph largescale information;reasoning structure;ontological information;graph utilizing extraction;information entities relationships;ontological constraints 12;80 ontological constraints;entities relationships", "pdf_keywords": ""}, "5ede529879d162d2779d410a5775d3f6cd6be3f4": {"ta_keywords": "distributionally robust optimization;robust optimization;neural generative models;generative models;distributionally robust;constrained inner maximization;large scale generative;gradient based optimization;neural generative;robust optimization dro;learning models able;models robust;inner maximization;generative models develop;scale generative models;optimization large scale;learning models;inner maximization objective;generative models characterize;optimization challenges;amenable gradient based;baselines distributionally robust;generative;training machine learning;models robust comparable;model selection heuristics;use neural generative;machine learning models;gradient based;max solution optimization", "pdf_keywords": "learning models face;recognizing;models face recognition;models face;recognition;learning models;winner inner maximization;toxic language;face recognition;recognition variable;forms toxic language;minorities toxicity detection;face recognition variable;toxicity detection task;rating models exhibit;rating models;recognizing various;propose generative model;inner maximization;toxic language address;recognition variable lighting;setting toxicity detection;silencing underrepresented;recognizing various forms;learning;language address challenges;game winner inner;represented training data;generative model;toxicity detection"}, "c6b462aaca52d0325db3118d2779865915b266c3": {"ta_keywords": "conquer rule learning;rule learning induction;learning rules systems;learning rules;complexity rule induction;rule induction large;rule induction methods;rule learning;rules systems learn;runtime rule induction;rule induction;approach learning rules;learning induction;learn sets rules;learning induction methods;induction large training;rules systems;systems learn sets;improve runtime rule;pruning techniques;complexity rule;new pruning techniques;induction methods loss;induction methods scale;pruning techniques dramatically;induction methods;learning problems;large training sets;pruning;learn sets", "pdf_keywords": ""}, "dfb35ebe4fd754f59053d27c78f555bb5e7ccbff": {"ta_keywords": "prior minimizing curvature;orientation estimation fluid;vision require estimation;valued orientation estimation;minimizing curvature;orientation estimation;minimizing curvature center;early vision framework;estimation fluid dynamics;sub pixel localization;video motion rigid;pixel localization;fluid dynamics video;vision framework;proposed early vision;vision framework sufficiently;vision require;dynamics video motion;pixel localization real;video motion;early vision;prior minimizing;estimation fluid;regularization;applications vision require;estimation structures boundary;absolute curvature;curvature;regularization proposed;detection likelihoods prior", "pdf_keywords": "images curvature regularization;regularization surface segmentation;curvature inpainting;3d images curvature;curvature regularization;images curvature;surface segmentation;framework curvature inpainting;curvature regularization curves;edge vessel detection;curvature inpainting curvaturedriven;surface segmentation important;edge completion curvature;completion curvature regularization;curvature penalization;curvature regularization surface;medical image segmentation;inpainting involving curvature;curvature regularization mixed;curvature penalization curvature;penalization curvature regularization;inpainting curvaturedriven;segmentation inpainting;penalization curvature;regularizes curvature underlying;regularizes curvature;involving curvature penalization;image segmentation inpainting;directly regularizes curvature;vessel detection 2d"}, "cd5a9a0061de6a6841c63e60281133207b2d6763": {"ta_keywords": "phrases polysemous words;words phrases polysemous;unfamiliar words phrases;phrases local;global context machines;wordnet;context machines;oxford urban dictionaries;urban dictionaries demonstrate;wordnet oxford urban;urban dictionaries;phrases local global;context machines help;wordnet oxford;phrases polysemous;polysemous words;stuck unfamiliar words;polysemous words novel;words phrases;local context;global context;slang emerging entities;way phrases local;dictionaries demonstrate effectiveness;internet slang emerging;immediate local context;dictionaries;unfamiliar words;local global context;slang emerging", "pdf_keywords": "aware description generator;context aware description;description generation;description generation task;description generator;natural language description;generating natural language;phrases learning unknown;learning unknown phrases;contexts useful generating;unknown phrases learning;directly unknown phrases;contribute description generation;contexts contribute description;language description;senses dictionary de\ufb01nitions;global contexts learning;dictionary de\ufb01nitions known;phrases learning;word senses dictionary;phrase speci context;description generator wolverine;dictionary de\ufb01nitions;phrases local global;contexts directly;contexts challenging task;global contexts;contexts learning;aware description;global contexts useful"}, "89b8153a86708b411bd21357c5b6006142104fc9": {"ta_keywords": "popularity memorable quotes;reveals public speeches;relevance corpora;memorable quotes number;public speeches;ted public speaking;corpus ted public;talks given ted;public speeches retained;collect corpus ted;relevance corpora required;speeches;speeches retained people;corpus ted;public speaking;goal relevance corpora;speeches retained;popularity memorable;memorable quotes interesting;corpus;quotes number talks;quotes interesting useful;team memorable quotes;memorable quotes;collect corpus;quotes achieve;nonmemorable quotes;talks;nonmemorable quotes achieve;study collect corpus", "pdf_keywords": ""}, "91ef95907dc637ad3c29ac3cc0e682b9c1985a37": {"ta_keywords": "segmented speech segment;speech segment based;segmented speech;simultaneous speech translation;outcomes segmented speech;speech segment;outcome speech segment;speech segment method;speech translation;speech translation paper;search optimal segmentation;machine translation;predicting outcome speech;segment based prediction;optimal segmentation strategy;machine translation methods;optimal segmentation;prediction outcomes segmented;segmentation directly maximizes;segmentation strategy;learning segmentation strategies;performance machine translation;segmentation strategies;segmentation strategies simultaneous;translation methods;learning segmentation;segmentation directly;segmentation;finds segmentation;translation methods based", "pdf_keywords": ""}, "1f3c381eedfe8914b81e93070bfdb00cf86ac943": {"ta_keywords": "graph classification benchmarks;graph classification;node graph classification;learning node graph;graph level representations;graphs compared supervised;supervised learning node;structural views graphs;graph level;learning node;self supervised;views graphs;self supervised learning;views graphs compared;supervised;introduce self supervised;node graph level;classification benchmarks linear;graphs;classification benchmarks;sound pedestrian urban;self supervised approach;speed sound pedestrian;node graph;sound pedestrian;results self supervised;pedestrian neighborhood;approach learning node;supervised learning;compared supervised", "pdf_keywords": "learning graph representations;graph level representations;graph representations;graph representations challenging;visual representation learning;learning graph;deep representations;learning deep representations;deep representations augmentations;views graphs;representation learning large;architectures learning graph;views graphs including;learning large graphs;graph diffusion learning;representation learning;learning node graph;visual representation;view visual representation;structural views graphs;graph classi\ufb01cation tasks;learn representations;representations augmentations challenging;representation learning achieved;learn representations relying;learning deep;benchmarks surpassing supervised;representations augmentations;graph classi\ufb01cation;graphs including"}, "4abdea830316d80ab0b29fb94ee0786216f6a1cd": {"ta_keywords": "phrase alignment extraction;alignment phrase extraction;joint phrase alignment;phrase alignment;machine translation;machine translation tasks;phrase based machine;phrase extraction;memorizes phrases generated;alignment extraction;unaligned sentence pairs;based machine translation;step word alignment;word alignment;formulation memorizes phrases;phrase extraction approach;alignment extraction using;word alignment phrase;memorizes phrases;model joint phrase;phrase table achieves;phrase table;phrases generated;translation tasks directly;translation tasks;sentence pairs;directly unaligned sentence;create phrase table;phrases granularities;orientation phrases granularities", "pdf_keywords": ""}, "d6b3effdeb3d38ac9ee43c3b8292b0937a295c30": {"ta_keywords": "hierarchical multitask training;multitask learning pretraining;pretraining outperforms multitask;outperforms multitask learning;xmath0 speech recognition;multitask learning;learning hierarchical multitask;hierarchical multitask learning;deep neural encoder;multitask training;speech recognition;multitask learning context;multitask training works;outperforms multitask;deep neural;decoder learning hierarchical;neural encoder decoder;neural encoder;encoder decoder learning;hierarchical multitask;decoder learning;speech recognition model;multitask;multitask learning paper;auxiliary loss;recognition based deep;learning context connectionist;learning pretraining;pretraining improves word;subword xmath0 speech", "pdf_keywords": "recognition based multitask;multitask learning;standard multitask learning;pretraining multitask learning;standard multitask training;multitask training;multitask learning lower;based multitask learning;multitask learning framework;hierarchical multitask learning;pretraining multitask;sequences speech recognition;multitask training higher;neural speech recognition;better standard multitask;speech recognition;models standard multitask;end neural speech;free recognition language;multitask;speech recognition compare;standard multitask;recognizing spoken digit;neural speech;multitask learning paper;multitask approach;recognition language model;based multitask;hierarchical multitask;hierarchical multitask approach"}, "3c78c6df5eb1695b6a399e346dde880af27d1016": {"ta_keywords": "neural paragraph level;question answering models;adapting neural paragraph;level question answering;question answering;answering models;paragraph reading comprehension;neural paragraph;paragraph level;paragraph reading;individual paragraphs;multi paragraph reading;reading comprehension presented;results individual paragraphs;reading comprehension;paragraphs;paragraph level question;effective multi paragraph;multi paragraph;neural network;answering models case;documents given input;paragraph;train neural;train neural network;normalization training;neural network model;algorithm train neural;document based data;adapting neural", "pdf_keywords": "question answering text;reading comprehension models;question answering;answer question answering;general question answering;answering text;comprehension models;comprehension models including;answering text paper;reading comprehension based;learn paragraph embeddings;ideas reading comprehension;question answering hypothesize;new paragraph ranking;networks learn paragraph;paragraph ranking;approach reading comprehension;paragraph ranking method;reading comprehension;propose paragraph selection;paragraph embeddings;answering;learn paragraph;bi directional attention;answer naively trained;directional attention;answer question;attention;attention propose paragraph;ranking method text"}, "7c3a2e953d2c07ff4f150865112e4ceec14090ea": {"ta_keywords": "el speech enhancement;electrolaryngeal el speech;excitation prediction electrolaryngeal;speech enhancement;sounds enable laryngectomees;noise el speech;laryngectomees produce el;speech enhancement removing;prediction electrolaryngeal el;produce el speech;enable laryngectomees produce;voiced prediction;enable laryngectomees;prediction electrolaryngeal;el speech sounds;parameters proficient laryngectomees;voice conversion;generates excitation sounds;electrolarynx device artificially;parameters voice conversion;voice conversion method;device electrolarynx;unvoiced voiced prediction;produced device electrolarynx;voiced prediction paper;adding noise el;laryngectomees;electrolarynx device;excitation sounds enable;excitation sounds produced", "pdf_keywords": ""}, "73fe797b4f4f2d18784246bb74626426a8fe108e": {"ta_keywords": "pointergraph networks;introduce pointergraph networks;pointergraph networks pnns;directed networks;pointergraph;generation directed networks;introduce pointergraph;nodes directed;latent graph structure;graph connectivity tasks;directed network;idea nodes directed;networks pnns;nodes directed network;graph connectivity;directed links nodes;inferring latent graph;dynamic graph connectivity;networks pnns augment;science introduce pointergraph;graph structure;networks;nodes;links nodes critically;gnns deep sets;generate directed links;directed networks dns;directed network used;nodes critically;idea nodes", "pdf_keywords": "graph inference learning;pointer graphnetwork;model pointer graphnetwork;pointer graphnetwork pgn;graph connectivity tasks;latent graph inference;graph connectivity;graph inference;dynamic graph connectivity;graphnetwork;generative models graphs;graphnetwork pgn;graphnetwork pgn combines;cnns learn parallelisable;ef\ufb01cient latent graph;learn parallelisable data;latent graph;networks;inference learning deep;networks cnns learn;link cut trees;graphs;models graphs;learning deep;graphs important task;networks cnns;algorithms dynamic graph;learn parallelisable variants;dynamic graph;learn parallelisable"}, "8b652c4d7a8d5836925ce0fe28a91dc661778524": {"ta_keywords": "neural language models;large neural language;nlp tasks;results nlp tasks;deep model competence;language models;neural librarians;neural librarians answer;results nlp;neural language;ability neural librarians;models trained;ai conference;language models lmfs;claims deep model;art results nlp;models actually learn;trained models;large scale evaluation;nlp tasks unclear;deep model;trained models domain;large neural;models trained achieve;models lmfs trained;place ai conference;neural;ai;nlp;ai conference vancouver", "pdf_keywords": "learning comparative semantics;generate comparative questions;predicting semantic;training corpus comparative;capable predicting semantic;comparative semantics constructed;learning comparative;comparative semantics;language models \ufb01netuning;predicting semantic types;large language models;comparative user questions;encourage learning comparative;corpus comparative;automatically generate comparative;questions fail learn;\ufb01netuning machine learning;\ufb01netuning limited lexical;corpus comparative questions;learning \ufb01netuning;diverse training corpus;learning \ufb01netuning limited;language models;comparative questions;semantic entities little;user questions crawled;nonsensical learning \ufb01netuning;semantics constructed large;semantic entities;qa sites predictions"}, "f53aa1d2676689c94429944f6a69431f96e05ae1": {"ta_keywords": "unsupervised topic models;topic models semi;topic models;topic indicative features;latent variable models;latent data introduce;latent latent data;membership latent variable;idea latent data;models semi supervised;mixed membership latent;membership latent;mixed membership learning;gibbs sampler;latent data based;learning mixed membership;gibbs sampler used;supervision mixed membership;semi supervised learning;latent data;semi supervised;membership learning;unsupervised topic;supervised learning mixed;analysis latent data;modify gibbs sampler;topic indicative;entity clustering;membership learning model;supervised", "pdf_keywords": ""}, "57676e07d66b102f3335a5c538735ebff9076623": {"ta_keywords": "feedforward loop network;loop network fcnc;performance feedforward loop;loop network;feedforward loop;network fcnc;performance feedforward;feedforward;network;study performance feedforward;fcnc;loop;performance;present results;paper present results;results study performance;present results study;study performance;paper;paper present;results;present;results study;study", "pdf_keywords": ""}, "ba1823889a80c231966a0f24e57c6cf4a569ff8c": {"ta_keywords": "multimodal fake news;image multimodal fake;multimodal fake;news text images;model multimodality;text image multimodal;multimodality;used model multimodality;visual entities celebrities;model multimodality large;multimodality large;propose multimodal model;propose multimodal;image multimodal;multimodal;multimodal model;fake news entity;semantics images;capture multimodal clues;fake news detection;multimodal model used;fake news text;diffusion text fake;multimodal clues innovatively;issue multimodal fake;multimodal clues;text images;text fake news;effectively capture multimodal;text images achieved", "pdf_keywords": "multimodal fake news;multimodal fake;multimodal entity;presents multimodal fake;detect fake news;fake news detection;multimodal models detect;multimodal model fake;multimodal entity inconsistency;indicator multimodal fake;multimodal models;entity enhanced multimodal;news text images;multimodal;news detection model;news detection;developing multimodal models;news detection despite;multimodal model;multimodal fusion framework;novel multimodal model;enhanced multimodal;presents multimodal;multimodal fusion;indicator multimodal;enhanced multimodal fusion;modal correlations fake;present novel multimodal;advancements developing multimodal;modalities text images"}, "499ada382b7ce8f1cbd890e8c21500d95e20f2fe": {"ta_keywords": "evaluation audio representations;brain audio embedding;audio representations;audio embedding model;holistic evaluation audio;audio embedding;audio representations department;present audio embedding;sound representations;performance sound representations;audio embedding approach;audio representation;audio representation provides;sound representations wide;human brain audio;purpose audio representation;brain audio;evaluation audio;audio;general purpose audio;purpose audio;sound music human;present audio;reproducible evaluation longitudinal;longitudinal data;longitudinal longitudinal data;sound music;speech environmental sound;evaluation longitudinal;music human ear", "pdf_keywords": "audio modeling community;practice audio representations;2021 audio learning;audio representations;audio learning task;audio representations evaluated;model correspondence audio;audio learning;audio model correspondence;know audio representation;domain audio model;audio representation;audio modeling;novel audio learning;correspondence audio;audio model;new approach audio;approach audio modeling;audio representation use;correspondence audio video;know audio;approach audio;hear tasks models;general practice audio;learning task hear;domain audio;novel audio;audio;broad domain audio;practice audio"}, "aa2428e1c4ea6d6bb347cfa59beead8736e19c46": {"ta_keywords": "electric field stability;2d ring;dimensional 2d ring;permanent electric field;field stability;field stability dimensional;electric field;stability dimensional 2d;stability dimensional;dimensional 2d;permanent electric;ring;presence permanent electric;2d;stability;field;electric;dimensional;permanent;impact presence permanent;presence permanent;impact presence;impact;study impact;study impact presence;letter present;letter;present results;presence;comprehensive study impact", "pdf_keywords": ""}, "95ee674a03ad23eaaf4837121fc8aea30d885088": {"ta_keywords": "preference representations;siamese networks learn;structured preference representations;siamese networks;compact preference representations;distance structured preference;learning measure distance;novel metric learning;preference representations working;metric learning;deep siamese networks;preference representations paper;use deep siamese;learning reasoning preferences;metric learning approach;deep siamese;structured preference;generalizes representing learning;networks learn kendal;novel metric;representing learning;networks learn;learning measure;siamese;presents novel metric;representations working preferences;objects recommended preference;networks;network generalizes representing;distance sets objects", "pdf_keywords": "embeddings metric learning;graphs deep metric;learning preference reasoning;metric learning preference;deep metric learning;metric learning cp;graph embeddings metric;metric learning computational;metric learning;deep metric;problem metric learning;approach metric learning;computational preference reasoning;compact preference representation;metric learning typically;metric learning based;learning cp nets;embedding graphs deep;preference representation;cnns deep metric;preference representation combines;embeddings metric;preference reasoning area;graph embeddings;learning preference;graph learning neural;preference reasoning;embedding graphs;computational reasoning preferences;graphs deep"}, "683bbb665bdaea8688834e97559d63842242ee1f": {"ta_keywords": "deep reinforcement learning;reinforcement learning wild;deep reinforcement;dynamics intrinsic fear;deep underwater swimming;learning wild;reinforcement learning;deep underwater;steps deep reinforcement;deep underwater deep;predict outcome game;explore dynamics intrinsic;swimmer deep;intrinsic fear;underwater deep;swimmer deep underwater;like swimmer deep;learning objective fluid;intrinsic fear new;swimming introduce intrinsic;dqns toy;underwater deep underwater;performance dqns toy;duffing like swimmer;explore dynamics;intrinsic fear short;introduce intrinsic fear;dqns toy problems;deep underwater water;underwater swimming", "pdf_keywords": ""}, "13d9d24ff2ba69de4cedcebd8f59371a5c1de7ed": {"ta_keywords": "word sense disambiguation;context words based;sense disambiguation;context words;identifying useful contextual;sense disambiguation present;useful contextual;knowledge based word;context modeling;improvements context modeling;contextual cues knowledge;analysis context words;words based lexico;contextual;based word sense;disambiguation;context modeling beating;word sense;lexico statistical syntactic;based lexico statistical;useful contextual cues;disambiguation present;statistical syntactic information;context;disambiguation present simple;syntactic information;contextual cues;improvements context;words based;knowledge based methods", "pdf_keywords": ""}, "788aa828a194a6d6c4e5ab1d4b46fc5f987159b0": {"ta_keywords": "origin oil painting;oil painting process;oil painting;painting process;explanation origin oil;painting;origin oil;simple explanation origin;explanation origin;oil;process;origin;simple explanation;present simple explanation;explanation;article present simple;article;simple;article present;present simple;present", "pdf_keywords": ""}, "3df97e8237c7d98c7343fc025eacbbc2b96a10ae": {"ta_keywords": "release exosomes biomolecular;release exosomes;capture release exosomes;exosomes biomolecular systems;exosomes biomolecular;exosomes small extracellular;exosomes;exosomes small;extracellular vesicles secreted;vesicles secreted cells;extracellular vesicles;small extracellular vesicles;vesicles secreted;vesicles;biomolecular systems;small extracellular;extracellular;secreted cells;biomolecular;secreted cells play;cells;cells play important;efficient capture release;processes body;cells play;capture release;processes body letter;method efficient capture;physiological pathological processes;secreted", "pdf_keywords": ""}, "a43d6fa0e96d56d0200e8d5e4407be8befc4e063": {"ta_keywords": "fast food production;fast food industry;concept fast food;food industry;approach fast food;food industry based;fast food;food production;industry based concept;production;industry;industry based;food;concept fast;based concept fast;novel approach fast;approach fast;fast;novel approach;based concept;present novel approach;concept;approach;novel;based;paper present novel;paper;present novel;paper present;present", "pdf_keywords": ""}, "b2fac3812885e3c8101cc729b6846f9108ac4d70": {"ta_keywords": "fairness represented bias;improves fairness represented;improves fairness;provably improves fairness;fairness modeled;bias statistics;bias statistics propose;notion bias statistics;consider fairness modeled;bias maintaining minimax;fairness modeled notion;bias;fairness represented;bias loss accuracy;bias loss;represented bias loss;bias maintaining;estimators provably improves;consider fairness;represented bias;estimator mle minimax;mle minimax optimal;minimax optimal estimating;fairness;notion bias;improved rate bias;estimator mle;rate bias maintaining;mle minimax;rate bias", "pdf_keywords": "constrained unconstrained oracles;unconstrained oracles paper;unconstrained oracles;performance unconstrained oracle;unconstrained oracle;unconstrained oracle whittle;achieve bias reduction;bias reduction;investigate performance unconstrained;stretched mle analysis;bias reduction employ;deterministic algorithm;unconstrained;mle analysis;performance unconstrained;rating systems;underpinnings rating systems;constrained unconstrained;oracles;consider stretched mle;oracle whittle;deterministic algorithm problem;oracles paper;mle analysis particular;rating systems results;norm unregularized mle;oracle whittle whittle;oracle;oracles paper present;stretched mle"}, "8e56db786a685b4b9c7f1b750f60a81baebff0b5": {"ta_keywords": "nonaudible murmur enhancement;murmur enhancement;enhanced speech noisy;performance nonaudible murmur;speech whispered voice;statistical voice conversion;normal speech whispered;speech noisy environments;silent speech communication;communication nonaudible murmur;nonaudible murmur;intelligible speech noisy;speech communication nonaudible;voiced speech effective;speech noisy;soft whispered voice;speak making audible;whispered voice recorded;voice conversion;nonaudible murmur nam;voice conversion methods;process voiced speech;statistical voice;speech recognition ag;voice nam whisper;whispered voice;voiced speech;listens enhanced speech;effective silent speech;murmur enhancement address", "pdf_keywords": ""}, "418349df9bf28e2b1290b758a4ebcf0d812c7288": {"ta_keywords": "classifying political blogs;political blogs;political blogs blog;semi supervised learning;blog network;blogs blog network;semi supervised;algorithm classifying political;blogs;new semi supervised;person online video;seed blogs;blogs blog;blogs paper present;blogs paper;classifying;blog;using seed blogs;classification;learning algorithm classifying;supervised;supervised learning;algorithm classifying;classifying political;seed blogs paper;classification accuracy;supervised learning algorithm;online video present;achieves classification accuracy;datasets achieves classification", "pdf_keywords": ""}, "e00f0a9e184a9d2afd8bb344908ca25d8bdc9e04": {"ta_keywords": "2d lattice;dimensional 2d lattice;lattice absence magnetic;particle dimensional 2d;lattice;2d lattice absence;lattice plane;perpendicular lattice;field perpendicular lattice;dynamics single particle;perpendicular lattice plane;dynamics;lattice plane paper;computational linguistics;single particle dimensional;particle dimensional;lattice absence;study dynamics single;doing computational linguistics;dimensional 2d;computational linguistics research;dynamics single;single particle;particle;dynamics 1979;magnetic field;2d;computational;case magnetic field;magnetic field case", "pdf_keywords": ""}, "692320cf5ae6980bc6b2b2d7bc48df961b545c22": {"ta_keywords": "speech recognition competitions;speech recognition crowdsourced;recognition crowdsourced speech;crowdsourced speech recognition;challenge video conferencing;channel speech enhanced;channel speech;crowdsourced speech;multi channel speech;speech noise datasets;speech enhanced multiple;speech enhanced;microphone array task;channel speech enhancement;performance semiflexible speech;semiflexible speech recognition;speech enhancement single;speech enhancement;clean speech noise;recognition crowdsourced;recognition competitions;video conferencing;recognition competitions competition;microphone;speech recognition;microphone array;challenge tasks datasets;new challenge video;speech noise;challenge video", "pdf_keywords": "speech enhancement video;channel speech enhancement;speech enhancement capable;speech enhancement;based speech enhancement;speech enhancement methods;enhanced speech low;intelligibility enhanced speech;speech recorded microphone;enhanced speech;enhancement video conferencing;capable processing speech;network based speech;multi channel speech;speech noise datasets;clean speech noise;microphone arrays video;speech recorded;channel speech;processing speech single;microphone;processing speech;microphone array;microphone arrays;automatic speech;recorded microphone arrays;speech low latency;distributed microphones;recorded microphone;speech recognition"}, "87d50fc84c71ed9860ed02b0149266b74c446c9c": {"ta_keywords": "linear regression hmms;regression hmms;regression hmms using;nonparametric bayes regression;hmms using variational;continuous speech recognition;nonparametric bayes;propose nonparametric bayes;speech recognition;experiments continuous speech;continuous speech;bayes regression method;bayes regression;paper propose nonparametric;bayesian treatment linear;quantities adaptation data;nonparametric;fully bayesian treatment;adaptation data;propose nonparametric;fully bayesian;speech recognition confirm;presents fully bayesian;adaptation data paper;bayesian treatment;regression method optimize;parameters linear regression;bayesian;quantities adaptation;using variational techniques", "pdf_keywords": ""}, "dc3adb99f682a11fe0507dcbc5dc2958199c5af1": {"ta_keywords": "xmath0 type superconducting;superconducting circuit scf;superconducting circuit;superconducting;type superconducting circuit;type superconducting;circuit scf temperature;better scf circuits;performance xmath0 type;scf circuits;performance xmath0;scf temperature;scf temperature range;300 scf performance;circuit scf;scf performance;scf performance scf;performance scf significantly;study performance xmath0;performance scf;xmath0 type;significantly better scf;300 300 scf;scf significantly better;300 scf;better scf;scf significantly;xmath0;temperature;temperature range", "pdf_keywords": ""}, "48685f26b32d199e6a4d80f6c61e62cc9738e403": {"ta_keywords": "event extraction work;event extraction;approach bio nlp;task event extraction;bio nlp;bio nlp 2009;nlp 2009 shared;support vector machine;task representation;nlp 2009;nlp;machine classifiers;task representation xmath0;task event;classifiers;vector machine classifiers;event;extraction work task;work task representation;vector machine;xmath0 state potts;machine classifiers achieve;2009 shared task;shared task event;potts models sense;classifiers achieve;state potts model;potts models;rule based component;potts model equivalent", "pdf_keywords": ""}, "e107beee5e84cd11d6460f7040676687a51a378b": {"ta_keywords": "reconstruction posedness noise;driven reconstruction approaches;data driven reconstruction;reconstruction posedness;unrolled reconstruction posedness;distributions reconstruction;driven reconstruction;distributions reconstruction ground;image reconstruction;variational framework iterative;distance distributions reconstruction;reconstruction approaches;variational framework;example image reconstruction;reconstruction approaches present;new variational method;classical variational framework;image reconstruction ray;tomography approach outperforms;reconstructing topological;tomography approach;posedness noise;reconstructing topological structure;guarantees variational setting;variational method;tomography;computed tomography approach;unrolled reconstruction;posedness noise stability;measurement space wasserstein", "pdf_keywords": "supervised learned reconstruction;regularization used deep;trained reconstruction operator;learned reconstruction approaches;measurements unrolled regularization;learned reconstruction;trained reconstruction;learning regularizer unrolled;unrolled regularization;learning regularizer;regularization;reconstruction operator;unrolled regularization used;prior knowledge regularization;regularizer unrolled inverse;reconstruction operator corresponding;posedness encoding prior;problem learning regularizer;trained regularizer;regularization functional;end trained regularizer;regularization used;trained regularizer circumvents;regularization penalty;knowledge regularization;regularization functional penalizes;knowledge regularization functional;ill posedness encoding;inverse problem recovering;deep learning tasks"}, "9f1d9dfb0b30d9fc5881d07b8e7f508815296c93": {"ta_keywords": "training parser;training parser data;parsing models learning;parsers language;multiple parsers language;parsers language want;new domain treebank;parsing models;general parsing;method training parser;parsers;new treebank domain;domain treebank new;multiple parsers;treebank new domain;make parser;domain treebank;overfit general parsing;build new treebank;ensuring parser able;general parsing process;language want parse;treebank domain;parser domain;ensuring parser;parser;parsing;treebank domain method;treebank new treebank;parser data similar", "pdf_keywords": ""}, "51ec4e93d8ae4c62453fdb34c6866696da0527b1": {"ta_keywords": "multimedia fake news;fake news detection;visual content fake;multimedia fake;content fake;introduction multimedia fake;content fake news;issues multimedia fake;content detection;contents fake news;multimedia technology fake;technology fake news;visual content detection;fake news attempts;visual contents fake;news detection;news detection social;news detection chapter;news detection development;contents fake;detection development multimedia;news detection limited;detection social media;content detection understanding;proliferation fake news;videos attract mislead;multimedia content;news attempts utilize;images videos attract;news attempts", "pdf_keywords": "detection fake news;multimedia fake news;detecting multimedia fake;fake news detection;automatic detection fake;visual content fake;detection fake;multimedia fake;content fake news;characteristics fake news;content fake;news image video;fake news image;news detection tackle;news detection;detecting multimedia;news detection e\ufb00ectively;news image;framework multimedia fake;fake news works;fake news urgent;representative detection approaches;distinctive characteristics fake;news works extracted;images videos categorized;automatic detection;generally images videos;features visual content;forensics features semantic;fake news chapter"}, "a16cecbaf87d965e396e610f251f710a807b70ad": {"ta_keywords": "hearing impairment simulation;simulation hearing impairment;impairment simulation hearing;personalization hearing impairment;adjusting hearing impairment;individual hearing impaired;perception hearing impaired;simulation hearing;hearing people auditory;impaired persons auditory;persons auditory filter;hearing impaired auditory;hearing impaired persons;hearing impaired vary;proposed hearing impairment;characteristics hearing impaired;hearing impaired;hearing impairment level;hearing impairment;people auditory perception;similar hearing impaired;measurement auditory characteristics;impaired auditory characteristics;manually adjusting hearing;impaired auditory;auditory filter characteristics;differences measurement auditory;adjusting hearing;persons auditory;normal hearing people", "pdf_keywords": ""}, "f49065750931c1c3c9edaf7d2f4bc8ea1342450a": {"ta_keywords": "neural machine translation;short sequences neural;neural autoregressive sequence;translation task tuning;regularization neural autoregressive;autoregressive sequence modeling;autoregressive sequence models;sequences neural machine;machine translation;machine translation propose;translation use neural;sequences neural;sequences define oversmoothing;machine translation task;sequence models;neural autoregressive;unreasonably short sequences;short sequences define;sequence models widely;probable short sequences;sequence modeling;oversmoothing neural machine;short sequences;machine translation use;autoregressive sequence;findings neural autoregressive;novel regularization neural;translation task testbed;oversmoothing rate neural;regularization neural", "pdf_keywords": "neural machine translation;trained translation models;oversmoothing neural autoregressive;translation minimizes oversmoothing;oversmoothing language models;machine translation minimizes;machine translation quality;machine translation scaling;translation scaling neural;trained translation;translation models;translation machine translation;neural autoregressive sequence;translation models exhibit;machine translation;available trained translation;machine translation machine;translation machine;statistical machine translation;rnn encoder;rnn encoder decoder;autoregressive sequence modeling;machine translation observe;processing neural autoregressive;neural autoregressive;oversmoothing language;translation quality;language models shot;using rnn encoder;learn phrase representations"}, "b0b1112b06898733faefc32f54940aa4e84bc383": {"ta_keywords": "speech translated paralinguristic;analyze speech translation;speech translation systems;speech speech translation;speech translation;paralinguistic information;speech corpora english;speech translation s2;english speech corpora;speech corpora;paralinguistic information included;input speech translated;focus paralinguistic information;paralinguistic information type;speech translated;corpora english speech;speech translated english;model english speech;translation systems;analyze speech;paper paralinguistic information;speech speech;systems paper paralinguistic;translated paralinguristic information;paralinguistic;utterances;utterances conversational;emphasis expressed languages;communication speech speech;approach analyze speech", "pdf_keywords": ""}, "c55bc339122ad8cdba1ae74d1336be3d2f089699": {"ta_keywords": "stochastic convex optimization;oracles smooth convex;convex optimization;stochastic dual;stochastic convex;strongly convex objective;use stochastic dual;convex objective functions;convex optimization affine;convex objective;consider stochastic convex;biased stochastic oracle;stochastic oracle available;stochastic oracle;proximal step convex;primal dual approach;stochastic dual order;optimization affine constraints;gradient sliding algorithm;smooth strongly convex;smooth convex case;smooth convex;convex smooth;step convex smooth;strongly convex smooth;optimization affine;dual approach;functional strongly convex;step convex;strongly convex", "pdf_keywords": "convex optimization;class convex optimization;convex optimization problems;convex optimization networks;solve convex optimization;dual algorithm distributed;stochastic composit optimization;distributed smooth convex;universal gradient descent;stochastic oracle approach;propose stochastic oracle;smooth convex optimization;stochastic oracle;optimization wasserstein barycenter;optimization wasserstein;gradient descent;primal dual algorithm;dual algorithm;composit optimization wasserstein;gradient descent problem;optimization networks;solving class convex;applied solve convex;optimization networks conditional;solve convex;keywords stochastic composit;algorithm distributed;paper propose stochastic;problems keywords stochastic;algorithm distributed smooth"}, "6cf3bdcdee6236f9f04e7773e3601dbbb8fbc61e": {"ta_keywords": "entity recognition ner;named entity recognition;entity recognition;recognition ner models;annotated datasets requir;human annotated datasets;model shot proton;annotated datasets;resource 015 models;named entity;rely human annotated;domain experts recent;proton collisions scoring;ner models rely;entities;human annotated;collisions scoring model;entity;domain 016 dictionaries;ner models;domain entities;target domain entities;recognition ner;molecule imaging spectrometer;models additionally;shot proton;domain experts;domain entities paper;scoring model shot;model shot", "pdf_keywords": "entity recognition ner;named entity recognition;entity recognition;entity recognition models;extracting disease entities;disease entities text;entity recognition based;descriptions entities recognizing;entities text;shot named entity;named entity;entities recognizing;external descriptions entities;domain entities approaches;entities text automatically;domain entities;current named entity;training data domain;domain dictionaries sentences;recognizing target entities;target domain entities;entities;domain named entity;dataset disease entities;descriptions entities;practice domain dictionaries;human annotated datasets;annotated training data;resorting human annotated;open domain queries"}, "a0f00d5ea3727151b1c2fc8c407404f0c6641051": {"ta_keywords": "parser models;state art parser;robust parsers;model parsing protein;structure parser robust;phrase structure parser;parser robust parsers;parser robust;parsers;based parser model;parser model parsing;robust parsers genre;structure parser;available parsers;parsing protein;propose parser based;parser based parser;parser models terms;model parsing;parser model;propose parser;parser;paper propose parser;based parser;parsing;parser based;publicly available parsers;millions sentences parsed;parsing protein protein;parsers genre ckelark", "pdf_keywords": ""}, "3b7321832ba109cf47bfd13595c3b58acd4cb080": {"ta_keywords": "performance xmath0 transport;xmath0 transport;xmath0 transport time;performance xmath0;study performance xmath0;xmath0;transport;transport time;performance;present results;paper present results;present results systematic;study performance;results;paper;paper present;results systematic;systematic study performance;results systematic study;present;systematic study;systematic;study;time", "pdf_keywords": ""}, "3400b8bf1ffde3ef3d35dfcea893e6506427aa21": {"ta_keywords": "separation speech recognition;speech sequence unifying;speech mixture;speech mixture multiple;source separation speech;mapping speech mixture;recognition utterances;speaker speech recognition;single speech sequence;recognition utterances multiple;sequences single speech;speech recognition;separation recognition;separation recognition modules;speech sequence;multi speaker speech;speech recognition utterances;separation speech;mapping speech;speakers recognized mixture;speech recognition functions;utterances multiple speakers;sequence unifying source;unifying source separation;single speech;learn mapping speech;speaker speech;utterances multiple;recognized mixture;multiple label sequences", "pdf_keywords": "multitalker speech separation;speech separation;training deep recurrent;architecture automatic speech;input speech mixture;speech separation utterance;deep recurrent neural;speech recognition;multitalker speech;automatic speech;speech mixture;attention based encoder;deep neural;speech mixture multiple;separation utterance level;modules multitalker speech;new deep neural;training deep;recurrent neural networks;deep neural network;convert input speech;utterance level permutation;separation utterance;automatic speech recognition;deep recurrent;multiple hidden representations;single speaker speech;representations propose stacked;ctc attention based;trained recognizer"}, "3ce0f00d6c949192107f1bd6a167c03e1fb7393a": {"ta_keywords": "deterministic dependency parsing;deterministic parsing algorithms;transition based parsers;traditional deterministic parsing;dependency parsing algorithm;optimized parsing models;globally optimized parsing;best nlogn parser;deterministic parsing;optimized parsing;dependency parsing;parsing models present;based parsers;parsing algorithms;parsing models;nlogn parser significantly;nlogn parser;parsing algorithms based;parsers;parser;parsing algorithm;parser significantly accurate;parsing;based parsers nears;parser significantly;learning attachment preferences;parsing algorithm attempts;dependency tree iteratively;connect parsing;deterministic best nlogn", "pdf_keywords": "deterministic dependency parsing;multilingual dependency parsing;dependency parsing algorithms;parser dependency parsing;dependency parsing generative;dependency parsing;approach dependency parsing;dependency parsing algorithm;dependency parsing based;parser dependency;reduce parser dependency;optimized parsers making;optimized parsers;category dependency parsing;globally optimized parsers;dependency parsing paper;parsers present probabilistic;parsers making good;parsing generative latent;parsers;parsers making;parsing generative;parsing algorithms;parsing algorithms inspired;shift reduce parsers;reduce parsers;parser;parsing;parser dataset 2015;parsers present"}, "b9f5115b0353c268999fcc2f49c4b8e03a223994": {"ta_keywords": "interventions predicting;predict outcome intervention;diverse interventions predicting;interventions predicting long;predicting outcomes;utilized predicting outcomes;predict outcome;outcomes interventions;based evolutionary causal;outcome intervention;causal matrices markov;predicting outcomes diverse;interventions;evolutionary causal matrices;outcomes interventions necessary;outcomes diverse interventions;markov chain monte;program based markov;used predict outcome;computer simulations;simulation model;evolutionary causal;term outcomes interventions;interventions necessary educational;based markov;computer simulations based;simulation program;simulations based;introduce python classes;utilized predicting", "pdf_keywords": "educational interventions interventions;educational interventions designed;educational interventions;outcomes educational interventions;educational psychological interventions;keywords educational interventions;interventions based small;interventions types length;interventions interventions types;educational interventions moral;outcomes interventions based;psychological interventions based;interventions types;interventions interventions;outcomes interventions;intervention experiments psychological;interventions based;longitudinal outcomes predicted;types hypothetical interventions;interventions moral story;hypothetical interventions produce;interventions designed previous;psychological interventions;term outcomes interventions;interventions designed;hypothetical interventions;interventions moral;interventions;interventions produce;predicting expected outcomes"}, "cc4cc594c7dd38482c46a2db440135b8f26ff54f": {"ta_keywords": "pt catalyst efficiently;adsorption energy pt;pt catalyst;catalysts efficient energy;cut pt catalyst;exchange membrane fuel;proton exchange pt;energy pt pt;pt surface reduced;catalysts efficient;catalyst efficiently;energy pt;exchange pt cathode;cell assembled catalyst;pt skin protein;membrane fuel;membrane fuel cells;design catalysts efficient;proton exchange membrane;structure pt skin;bond pt surface;catalyst efficiently improving;pt surface;adsorption energy;commercialization proton exchange;catalyst;expensive proton exchange;assembled catalyst;density fuel cell;design catalysts", "pdf_keywords": ""}, "e2f015bbddd7bade7caca693e37f84c4cf70a7f5": {"ta_keywords": "matrix factorization mnmf;nonnegative matrix factorized;negative matrix factorization;matrix factorized nmf;matrix factorization;factorization mnmf multi;factorization mnmf;matrix factorized;concept multivariate latent;matrices multi channel;mnmf multi channel;multivariate latent;multivariate latent variables;factorized nmf;mnmf multi;multiple eavesdropping;optimization matrices;optimization matrices multi;mimo multi;multiple eavesdropping attacks;impact multiple eavesdropping;improves performance mnmf;latent variables achieve;latent variables propose;mimo;phonemes existing utterance;noisy asynchrony learning;mimo multi channel;eavesdropping attacks;cluster indicator latent", "pdf_keywords": ""}, "f8d7b263e8d663583cd22d5988c8ea4a49ed2840": {"ta_keywords": "entity relation extraction;entities extract relations;relation extraction;relation extraction aims;extract relations simultaneously;end relation extraction;extract relations;representations entities relations;entities relations;joint entity relation;named entities extract;contextual representations entities;entities extract;entities relations fusing;joint entity;relations fusing entity;entity relation;approach joint entity;unifying structured prediction;relation extraction establish;jointly unifying structured;learning shared representations;structured prediction;representations entities;features relation model;learning distinct contextual;identify named entities;structured prediction framework;relations simultaneously;named entities", "pdf_keywords": "entity recognition relation;entity recognition;encoders entity recognition;entity pairs inference;entity relation extraction;learns encoders entity;encoders entity relation;entity pairs;relation extraction independently;relation extraction categorization;relation extraction;recognition relation extraction;entities relations;predict entity;help predict entity;especially pronominal mentions;separate encoders entity;entity relation models;sentence context information;cross sentence context;trained language models;encoders entity;entities relations help;entity;entities;predict entity types;entity relation;language models;single cross sentence;cross sentence"}, "249b7517a746b1389991e10fd618cad62e66c4df": {"ta_keywords": "extracting word synonyms;synonyms corpus parsed;word synonyms corpus;synonyms corpus;graph based similarity;word synonyms;similarity measures;specialized similarity measures;similarity measure task;based similarity measure;learning specialized similarity;similarity measure;synonyms;syntactic vector based;corpus parsed text;specialized similarity;based similarity;similarity measures different;corpus;similarity;corpus parsed;different word types;graph walk variant;task extracting word;word types;syntactic vector;extracting word;location graph walk;remote location graph;predicting location remote", "pdf_keywords": ""}, "0718237a30408609554a0e2b90d35e37d54b1959": {"ta_keywords": "word embedding learns;entropybased subword mining;word embeddings;word embeddings embeddings;known word embeddings;word embedding;subword mining;subword entropybased;propose subword entropybased;subword entropybased subword;embedding vectors word;entropybased subword;semantically meaningful subwords;algorithm word embedding;embeddings embeddings corpora;embeddings corpora;embedding learns;learns distinct embedding;embeddings corpora longtail;representations downstream language;subword mining algorithm;meaningful subwords;embedding learns distinct;words sharing substructures;representations words phrases;embeddings embeddings;downstream language modeling;subwords;semantic structure words;shared semantic structure", "pdf_keywords": ""}, "771c1cb5fb161231e9aaa0a189caba672256a880": {"ta_keywords": "unsupervised morphological;unsupervised morphological disambiguation;effective unsupervised morphological;languages productive morphology;morphological disambiguation character;generated linguistically;exploit morphological knowledge;language models generate;models generate words;exploit morphological;morphological disambiguation;morphological knowledge encoded;language models;generating words;morphological knowledge;predicting words;generated linguistically na\u00efve;written morphological;generating words sequence;character sequence models;generate words;directly generating word;written morphological analyzer;vocabulary model learns;knowledge predicting words;disambiguation character based;morphological;productive morphology;generating word;processes generating words", "pdf_keywords": ""}, "7374494ee88608ef76f74b58a8f8c26ab06adfb9": {"ta_keywords": "brownian motion clustering;clustering based diarization;overlapping speech;overlapping speech treating;handle overlapping speech;overlapping speech frame;end diarization methods;diarization methods handle;based diarization methods;diarization methods partition;based diarization;diarization methods;motion clustering;frames clusters speakers;diarization;interacting brownian particles;speech frame;end diarization;clusters speakers;clustering;clusters speakers typically;frames clusters;dynamics interacting brownian;motion clustering based;partition frames clusters;speech frame assigned;end end diarization;clusters;interacting brownian;brownian particles", "pdf_keywords": "recognition speaker diarization;speaker diarization;applications speaker diarization;diarized speech recognition;speaker diarization challenging;overlapping speech treating;speaker diarization important;deal overlapping speech;clustering based diarized;handle overlapping speech;overlapping speech;based diarized speech;diarized speech;overlapping speech regardless;contain overlapping speech;overlapping speech large;end diarization methods;speech recognition;speech recognition speaker;clustering end;recognition speaker;clustering end end;diarization methods handle;combine clustering end;diarization methods;end diarization;diarization challenging;end diarization model;end diarization method;diarization challenging tasks"}, "6276bbe6cc56234d430725a31a27939eeec88149": {"ta_keywords": "document predicted quotability;predicted quotability explore;predicted quotability;quotability identification;passage ranking;identification passage ranking;quotability explore;passage ranking problem;evaluate quotability identification;quotability identification given;task quotability identification;quotability identification passage;quotability explore task;sequential sentence tagger;models rank passages;quotability;given document predicted;identify passages quotable;evaluate quotability;quotable explore;explore task quotability;document predicted;passages quotable explore;sentence tagger;sentence tagger achieving;quotable explore problem;task quotability;respectively evaluate quotability;evaluations datasets span;problem evaluations datasets", "pdf_keywords": "passage ranking models;passage ranking identifying;based passage ranking;ranking identifying passages;passage ranking;identi\ufb01cation passage ranking;bert based models;quoted modeling;quotability features;quotable text language;models exploring bert;learning machine translation;quotability features identi\ufb01ed;novel ranking task;ranking models featurebased;novel ranking;machine translation identify;bert based passage;machine translation;present novel ranking;documents quotable text;quoted modeling sorts;exploring bert based;likely quoted modeling;feature bert based;quotable text;text documents quotable;ranking models;documents quotable;quotability identi\ufb01cation passage"}, "96d5e1f691397dfb51e8b818a21a2d11eee46a59": {"ta_keywords": "distributed computation coding;distributed computing linear;speed distributed computation;distributed computation;modern multicore processing;distributed computing;shown distributed computing;multicore processing;exploiting modern multicore;sparse linear codes;present distributed computing;worker computational times;multicore processing architecture;worker computational;linear erasure codes;computation coding;computation coding solution;cores function;faster uncoded schemes;computing shown distributed;cores function f1;linear codes exploiting;nodes equipped cores;propose use sparse;distributed computing setup;cores;modern multicore;faster uncoded;architecture worker computational;worker nodes", "pdf_keywords": ""}, "75fe6c3ffdea2608794b4f21119c5a4dec07663a": {"ta_keywords": "autoregressive sequence generation;neural machine translation;machine translation benchmark;translation benchmark datasets;translation benchmark;sequence generation;generate sequence predictable;autoregressive nmt models;models autoregressive generate;autoregressive models fluid;sequence predictable;machine translation;autoregressive generate token;sequence seq2seq models;non autoregressive sequence;autoregressive generate;autoregressive sequence;seq2seq models autoregressive;sequence predictable sequences;predictable sequences contrast;sequence generation using;distributions using neural;autoregressive nmt;non autoregressive nmt;autoregressive models;model non autoregressive;significantly autoregressive models;predictable sequences;model neural machine;models sequence sequence", "pdf_keywords": "neural machine translation;generative machine translation;machine translation benchmark;translation machine translation;machine translation machine;translation machine;translation benchmark datasets;machine translation;generating translation;translation benchmark;task machine translation;nonautoregressive sequence generation;machine translation propose;generating translation continuous;learn sequence trans;machine translation method;sequence model learn;propose machine translation;space generating translation;neural sequence model;convolutional neural sequence;generative neural;machine translation evaluate;generative machine;decoding history generation;generative neural network;generative model nonautoregressive;sequence generation;neural sequence;proposes generative neural"}, "aa0b93501f79d57fe8542e72ccc8843ea50443c9": {"ta_keywords": "multilingual sequence recognition;training multilingual sequence;seq2seq multilingual features;hmm seq2seq multilingual;sequence speech recognition;training multilingual;multi lingual training;multilingual features;multilingual feature extraction;multilingual sequence sequence;seq2seq automatic speech;component training multilingual;lingual training data;multilingual feature;multilingual sequence;multilingual features demonstrate;multi lingual seq2seq;language transfer learning;seq2seq multilingual;lingual training stacked;lingual seq2seq models;propose multilingual feature;class multilingual sequence;language transfer;multi lingual;speech recognition;sequence sequence speech;lingual seq2seq;sequence recognition systems;speech recognition systems", "pdf_keywords": "multilingual seq2seq models;multilingual approaches seq2seq;challenges multilingual seq2seq;multilingual models leveraging;features multilingual models;models leveraging multilingual;multilingual seq2seq;multilingual features;multilingual models proposed;multilingual features multilingual;leveraging multilingual;multilingual models;data multilingual features;multilingual features combined;features multilingual;leveraging multilingual expertise;features combined multilingual;including multilingual features;expertise multilingual features;multi lingual training;models explored multilingual;work multilingual features;attention based seq2;seq2seq automatic speech;challenges multilingual;learning framework multilingual;multilingual expertise;multilingual approaches;lingual training data;including multilingual"}, "de971e50d70bc4d66f7debfab242942b0d1cae34": {"ta_keywords": "cascade speech summarization;ts speech summarization;speech summarization model;speech summarization generates;speech summarization;text summary speech;perform speech summarization;speech summarization experiments;summary speech achieved;automatic speech;combining automatic speech;asr text summarization;summarization performance attention;summary speech;representations transformers bert;input bert based;bert based ts;speech recognition asr;based ts speech;summarization model exploits;sum subword embedding;ts speech;asr input bert;improve summarization performance;summarization model;subword embedding vectors;text summarization ts;summarization ts;summarization generates;input bert", "pdf_keywords": "speech summarization exploit;automatic speech summarization;summarization based attention;speech summarization challenging;automatically summarizing spoken;speech summarization;speech summarization achieved;summarizing spoken speech;content speech summarization;speech summarization based;summarizing spoken;propose speech summarization;summarization exploit;automatically summarizing;summarization achieved great;summarization exploit multi;automatic speech;models text summarization;summarization challenging task;lengthy speech data;approach automatically summarizing;summarization ts bidirectional;attention based fusion;summarization challenging;approach automatic speech;summarization achieved;text summarization;summarizing;summarization;summarization ts"}, "e0ab89821af308f51647bfe872f114d775fd8818": {"ta_keywords": "multilingual speech recognition;speech translation medical;multilingual medical data;medical data multilingual;multilingual conversations medical;multilingual medical;multilingual speech;data multilingual speech;multilingual health care;multilingual multilingual health;speech speech translation;multilingual health;development multilingual medical;translate spoken utterances;speech translation;translation medical domain;translation medical;multilingual multilingual;multilingual conversations;data multilingual;multilingual;transducers gram language;language translated language;designed translate spoken;facilitate multilingual conversations;results multilingual multilingual;network based speech;translated language;gram language models;translate spoken", "pdf_keywords": ""}, "fba7ad8f63a42111b3618e51e3493ed70aafdcd0": {"ta_keywords": "estimating influences speakers;influences speakers conversation;quantum computer;speaker based hypothesis;quantum computer propose;computer propose probabilistic;word use speaker;word distribution;performance quantum computer;speakers conversation;performance quantum;conversation data;environment performance quantum;general word distribution;word distribution fluid;influences speakers;quantum;propose probabilistic;speaker based;propose probabilistic model;speakers conversation data;probabilistic model estimating;probabilistic;estimate influence environment;word use depends;estimating influences;conversation;probabilistic model;speakers earlier word;multiple people experiments", "pdf_keywords": ""}, "1ce96d8dbf69199ebd043de6cfa25d7e48b8ab03": {"ta_keywords": "review sentiment;amazon review sentiment;review sentiment semi;causal effects linguistic;sentiment semi simulated;effects complaint politeness;machine learning positive;learning positive product;effects linguistic;complaint politeness;sentiment;estimating causal effects;effect amazon review;algorithm estimating causal;machine learning;learning machine learning;sentiment semi;textcause algorithm estimating;effects linguistic properties;learns respond;review increase sales;estimating causal;complaint politeness bureaucratic;language model learns;learns respond promptly;learning machine;positive product review;politeness bureaucratic response;model learns respond;amazon review", "pdf_keywords": "causal effects linguistic;text based causal;effects linguistic;effects linguistic properties;linguistic properties observational;effects complaint politeness;data predicting politeness;predicting politeness;linguistic properties conduct;capture linguistic property;estimating causal effects;challenges estimating causal;predicting causal effects;capture linguistic;complaint politeness bureaucratic;correctly capture linguistic;politeness bureaucratic response;lexiconbased proxy treatments;causal inference latent;complaint politeness;linguistic;treatments words lexicon;causal effects observational;linguistic properties;estimation causal effects;estimating causal;bureaucratic response times;predicting causal;causal inference;estimates causal inference"}, "c37ecbccecab1774b545a5a5804b575718218f7d": {"ta_keywords": "speech recognition spa;model speech recognition;speech recognition;improve speech recognition;automatic speech recognition;features bottleneck features;based bottleneck features;features phonemes;features bottleneck;recognition based bottleneck;acoustic model speech;bottleneck features;speech recognition based;features phonemes propose;automatic speech;method automatic speech;speech recognition paper;model speech;essential features phonemes;extract features bottleneck;bottleneck features represent;methods improve speech;feature transformation methods;features;improve speech;acoustic model;recognition spa;feature transformation;feature;bottleneck", "pdf_keywords": ""}, "ae5a34c20fee705ad7094c93a711d5f724d535f0": {"ta_keywords": "recommendation fairness preserving;fairness preserving recommendation;enhances recommendation fairness;recommendation fairness;fairness aware tensor;tensor recommendation framework;preserving recommendation quality;worsening fairness recommendations;aware tensor recommendation;tensensor based recommendation;preserving recommendation;improved recommendation quality;fairness recommendations tensensor;factorization methods recommender;recommendation framework designed;fairness recommendations;recommendation framework;recommendations tensensor based;traditional matrix factorization;recommendation quality worsening;tensor recommendation;framework enhances recommendation;matrix factorization methods;recommendation quality;achieve improved recommendation;matrix factorization;sensitive latent factor;recommendation systems shown;recommendation systems;recommendation quality comparison", "pdf_keywords": "factorization recommendation quality;recommendation matrix factorization;fairness recommendation matrix;tensor based recommender;quality recommendation fairness;recommendation quality matrix;recommendation fairness recommendation;matrix factorization recommendation;recommendation quality fairness;online recommendation implicit;factorization recommendation;recommendation fairness;implicit recommendation;recommendation implicit feedback;better recommendation quality;recommendation matrix;based matrix factorization;recommendation implicit;recommendation quality;recommendation quality recommendation;matrix factorization;implicit recommendation proposing;problem implicit recommendation;recommendation methods;fairness recommendation;gender tensor based;sensitive attributes;matrix factorization key;factorization key challenges;data tensors generalizations"}, "ff187722c5b5462ac2066a737ae97650ffa177ed": {"ta_keywords": "automatic pronunciation assessment;recording classroom utterances;pronunciation assessment noisy;assessment noisy classroom;classroom speech recording;pronunciation assessment;speech recording classroom;performance automatic pronunciation;utterances student recorded;speech recognition asr;noisy classroom speech;assessment noisy;pronunciation assessment degraded;speech recording;assessment performance noisy;speech recognition;automatic speech recognition;classroom utterances;pronunciation gop assessment;automatic pronunciation;noisy classroom systems;use automatic speech;classroom utterances student;automatic speech;technologies detect pronunciation;results automatic pronunciation;detect pronunciation errors;recording classroom;detect pronunciation;performance noisy classroom", "pdf_keywords": ""}, "0f61621206e363367db85b39e8e4325e425afcb4": {"ta_keywords": "voice preserving conversion;voice conversion;statistical voice conversion;convert singing voice;voice conversion direct;converted singing voice;conversion accuracy singer;quality converted singing;singing voice preserving;speech quality converted;convert singing;possible convert singing;voice preserving;converted singing;singer using vocoder;singing voice method;converted spectral;converted spectral parameter;variance converted spectral;statistical voice;preserving conversion accuracy;voice method;presents statistical voice;singing voice;voice characteristics source;singing voice characteristics;vocoder based waveform;conversion direct waveform;waveform modification based;voice method makes", "pdf_keywords": ""}, "94c3fd8eea08008cecd98f4aace024cf63954ead": {"ta_keywords": "filter malicious sensor;attacks unknown sensor;malicious sensor observations;secure estimation proposed;algorithm secure estimation;malicious sensor;secure remote estimation;secure estimation;attack detection probability;detect injection attacks;attack detection;higher attack detection;unknown sensor subset;estimation linear gaussian;attacks unknown;injection attacks unknown;sensor subset developed;filter malicious;sensor subset;injection attacks;detector detect injection;sensor observations;using sensor observations;algorithm secure;remote estimation linear;detect injection;estimation linear;remote estimation;estimation scheme;proposed estimation scheme", "pdf_keywords": "location attacks fusion;attacks unknown sensor;secure remote estimation;switching location attacks;location attacks;attacks fusion centers;attacks fusion;problem attack detection;attack detection secure;cyber attacks static;attack detection;sensor subset fusion;detection secure remote;detection sensor prediction;detection switching location;noise presence attack;problem cyber attacks;cyber attacks;algorithm remote estimation;attack cyber physical;attacks unknown;sensor prediction;knows sensors observations;detect injection attacks;update observed sensors;attack cyber;sensors observations received;detection secure;observed sensors;observed sensors propose"}, "473021db54cbae9c4546597cd7e4b5d687a51c7f": {"ta_keywords": "training data crowdsourcing;data crowdsourcing vital;data crowdsourcing;requesters interface crowdsourcing;items crowdsourcing;crowdsourcing vital;interface crowdsourcing;crowdsourcing;crowdsourcing unlabeled;crowdsourcing vital tool;method crowdsourcing;interface crowdsourcing workers;method crowdsourcing unlabeled;crowdsourcing platform;items crowdsourcing platform;workers crowdsourcing;crowdsourcing workers;crowdsourcing unlabeled items;crowdsourcing platforms;crowdsourcing workers crowdsourcing;workers crowdsourcing platforms;crowdsourcing platform method;unlabeled items crowdsourcing;crowdsourcing platforms experts;present method crowdsourcing;probabilistic scoring;probabilistic scoring rules;labeled training data;class probabilistic scoring;experts making", "pdf_keywords": "axiom crowdsourcing axiom;crowdsourcing axiom;axiom crowdsourcing;crowdsourcing axiom presented;account axiom crowdsourcing;crowdsourcing decisions item;approval voting crowdsourcing;voting crowdsourcing;voting crowdsourcing paper;crowdsourcing decisions;propose incentive compatible;existence incentive compatibility;method crowdsourcing;crowdsourcing;crowdsourcing crowdsourcing decisions;propose method crowdsourcing;crowdsourcing paper propose;crowdsourcing crowdsourcing;propose incentive;method crowdsourcing crowdsourcing;algorithms approval voting;existence incentive;assumption existence incentive;incentive compatible;axioms coarse belief;incentive compatibility assumption;voting based data;aggregation algorithms approval;crowdsourcing paper;incentive compatibility"}, "042959b54176ad2c4f9d0966490ec407b6057527": {"ta_keywords": "breakdown pool glass;origin spontaneous breakdown;spontaneous breakdown pool;glass forming;glass forming agents;pool glass forming;spontaneous breakdown;pool glass;breakdown pool;glass;explanation origin spontaneous;origin spontaneous;forming agents;breakdown;explanation origin;simple explanation origin;spontaneous;forming;pool;presents simple explanation;explanation;origin;simple explanation;agents;article presents simple;article presents;article;presents simple;presents;simple", "pdf_keywords": ""}, "90dd676184a796e3e5835c8e1f6a632985ce3e89": {"ta_keywords": "enthalpy semiflexible fluid;expression enthalpy semiflexible;enthalpy semiflexible;semiflexible fluid undergoing;expression enthalpy;semiflexible fluid;enthalpy;fluid dynamics;form expression enthalpy;fluid dynamics video;fluid undergoing sudden;fluid undergoing;dynamics;dynamics video;fluid;dynamics video minutes;undergoing sudden jump;sudden jump size;sudden jump;semiflexible;video minutes;minutes closed form;video minutes closed;undergoing sudden;jump size;closed form expression;jump;minutes;minutes closed;closed form", "pdf_keywords": ""}, "80cb8981af401d9e4df0096626553c514d9e6600": {"ta_keywords": "yttrium iron garnet;iron garnet yfe;garnet yfe;iron garnet;solid solution yttrium;garnet yfe used;solution yttrium iron;yttrium iron;pyrochlore solid;pyrochlore solid solution;yfe used model;dynamic properties yfe;video pyrochlore solid;garnet;properties yfe;dynamics video pyrochlore;solution yttrium;fluid dynamics;yfe;yttrium;fluid dynamics video;yfe used;solid solution;dynamics;iron;structural dynamic properties;pyrochlore;solid;video pyrochlore;interplay structural dynamic", "pdf_keywords": ""}, "12b12ea73652da56023e0e4776211e4f4301f339": {"ta_keywords": "argumentation theory;argumentation theories;argumentation theory applied;fits argumentation theory;argumentation theories actual;expressing argumentation;annotating arguments;argumentation simple;argumentation;expressing argumentation simple;gap argumentation theories;economics opinion formation;used expressing argumentation;fits argumentation;argument components annotated;opinion formation;problem annotating arguments;argument components;arguments;opinions news articles;annotated strongly depends;argumentation simple size;gap argumentation;political opinions news;influence political opinions;bridge gap argumentation;opinion formation hope;annotating;news articles;size fits argumentation", "pdf_keywords": ""}, "77b919c4f4f37415d8f1019b1b04191d46de426c": {"ta_keywords": "walk based queries;recommendation retrieval tasks;recommendation retrieval;retrieval models;proximity queries labeled;retrieval models based;queries based random;represented proximity queries;queries labeled directed;answering queries based;random walk based;queries labeled;random walks challenging;proximity queries;weights fast query;probabilistic algorithm answering;fast query;random walks;algorithm answering queries;queries models learned;query execution retrieval;walk based proximity;constrained random walks;retrieval tasks represented;answering queries;retrieval tasks;used random walk;unsupervised random walk;based random walk;retrieval", "pdf_keywords": ""}, "e862e5f9a17938f1817017b2730e10463d94fb54": {"ta_keywords": "temperature orientation magnetic;critical temperature orientation;orientation magnetic;orientation magnetic field;temperature orientation;external magnetic field;dependence critical temperature;presence external magnetic;critical temperature;external magnetic;magnetic field;monotonic dependence critical;magnetic;magnetic field fig;orientation;magnetic field lead;dependence critical;non monotonic dependence;temperature;monotonic dependence;non monotonic;critical;lead non monotonic;field lead non;monotonic;field lead;field;dependence;field fig respectively;field fig", "pdf_keywords": ""}, "2826ac3621fdd599303c97cb9e32f165521967b2": {"ta_keywords": "machine learning case;machine learning;machine learning models;feature machine learning;properties optomechanical resonator;learning models;trained uncertainty;optomechanical resonator;disagreements patient diagnosis;models trained uncertainty;optomechanical resonator based;universal feature machine;optical resonance;work machine learning;optical resonator possible;achieve optical resonance;expert disagreements;expert disagreements medicine;learning case;models trained;high expert disagreements;feature machine;optical resonator;universal feature;learning models trained;patient diagnosis;trained uncertainty scores;optomechanical;optical resonance frequency;doctor disagreements patient", "pdf_keywords": "blurring detection;blurring detection task;segmentation eye;semantic segmentation eye;uncertainty functions uvar;focus speci uncertainty;image blurring detection;diabetic retinopathy dr;blurring;diagnostic concordance;segmentation;dr diagnostic concordance;gaussians image blurring;gaussians image;interpreting breast biopsy;speci uncertainty;medical exam;explored semantic segmentation;blindness;diabetic retinopathy;semantic segmentation;segmentation eye disease;speci uncertainty functions;biopsy;detection;breast biopsy specimens;retinopathy dr;diagnostic concordance pathologists;biopsy specimens;mixtures gaussians image"}, "a7f30bae9303825adbc333a8df8a03398dea5151": {"ta_keywords": "logic rules sentiment;rules sentiment classification;rules sentiment;sentiment labels elmo;sentiment classification;classification models syntactically;sentiment classification models;sentiment classification distillation;learn logic rules;ambiguous sentiment labels;different sentiment classification;contextualized logic rules;sentiment labels;logic rules significantly;situations ambiguous sentiment;models sentences;ambiguous sentiment;baseline models sentences;learn logic;semantic representations;sentiment;logic rules;contextualized logic;explicit logic rules;labels elmo;semantic representation;elmo outperforms baseline;models syntactically;performance different sentiment;semantic representations semantic", "pdf_keywords": "sentences crowdsourced sentiment;sentences crowdsourced;crowdsourced sentiment classi;crowdsourced sentiment;ambiguous sentences crowdsourced;crowdsourced models trained;improve sentiment;sentiment classi\ufb01cation task;trained millions sentences;logic rules sentiment;improve sentiment classi\ufb01cation;ambiguity sentiment classi\ufb01cation;crowdsourced models;predicting performance crowdsourced;performance crowdsourced models;crowdsourced analysis;rules sentiment classi\ufb01cation;vs negative sentences;crowdsourced analysis reveals;sentiment classi\ufb01cation complex;sentences like negation;performance crowdsourced;negative sentences;negative sentences contain;ambiguity sentiment;word embedding model;embeddings instead logic;contextualized word embedding;inherent ambiguity sentiment;sentiment classi"}, "203da29a37a983c487ce75a894b0d70698077bf5": {"ta_keywords": "problematic news sources;encountering problematic news;problematic content news;detection problematic information;potentially problematic sources;news media sources;spread problematic information;problematic news;problematic sources;fact checkers;news sources;problematic content;problematic information contemporary;content news media;news media;news sources different;problematic information despite;problematic information;content news;lists problematic content;datasets news stories;facebook;detecting false content;media sources;detection problematic;contemporary media ecosystems;media ecosystems attempts;disinformation recent electoral;disinformation recent;false content bad", "pdf_keywords": ""}, "7f588b1d2a5b199a19a4c3bad6bd5154c7355817": {"ta_keywords": "protein corona camouflage;camouflage ability immunomagnetic;serum albumin camouflaged;blood proteins human;protein corona promisingly;blood proteins;camouflage biological;proteins human serum;use protein corona;camouflage biological environments;corona camouflage ability;dynamics human blood;protein corona;camouflaged imbs;immunoglobulin transferrin dynamics;human serum albumin;promisingly human serum;strategy camouflage biological;fibrinogen immunoglobulin transferrin;human serum;corona camouflage;albumin camouflaged imbs;abundant blood proteins;immunoglobulin transferrin;formed protein corona;albumin fibrinogen immunoglobulin;camouflaged imbs hsa;serum albumin fibrinogen;transferrin dynamics human;immunomagnetic beads", "pdf_keywords": ""}, "44aa9a79cfc9eef9ac3f861cfa58a172cb863bd2": {"ta_keywords": "carbon nanotube cnt;walled carbon nanotube;nanotube cnt presence;carbon nanotube;nanotube cnt;nanotube;single walled carbon;electronic structure single;electronic structure;cnt presence external;study electronic structure;external magnetic field;external magnetic;walled carbon;presence external magnetic;cnt presence;magnetic field;theoretical study electronic;cnt;magnetic;structure single walled;carbon;single walled;study electronic;electronic;structure single;present theoretical study;walled;present theoretical;presence external", "pdf_keywords": ""}, "aeffb61024e5ccac5021ca0bf9d199d9196a0521": {"ta_keywords": "enforce tolls population;toll value constraint;enforcing tolls population;tolls population players;compute minimum toll;enforce tolls;enforcing tolls;reduced enforcing tolls;repeatedly enforce tolls;transportation network congestion;constraints population players;minimum toll value;coupled congestion costs;cost tolls;toll value ensuring;congestion minimizing;player congestion costs;modeling player congestion;tolls population compute;minimum toll;congestion costs;congestion minimizing impact;congestion costs analyze;cost tolls significantly;toll study problem;distribution given toll;distribution constraints population;tolls population;efficiently reduce congestion;congestion costs finally", "pdf_keywords": "solve congestion games;congestion game model;new congestion game;mdp congestion games;mdp congestion game;congestion game;congestion games;increasing congestion costs;solve congestion;transportation systems tolling;congestion costs formulate;congestion game apply;algorithm solve congestion;congestion costs;tolling algorithm enforces;based tolling algorithm;feasibility using tolls;tolling algorithm solve;tolling algorithm;congestion games paper;congestion games unknown;congestion costs paper;true congestion costs;systems tolling;drivers mdp congestion;congestion;increasing congestion;strictly increasing congestion;oracle based tolling;mdp congestion"}, "dc1d1f64503578d9c5d906da4556f631d4178b04": {"ta_keywords": "collision prediction;collision prediction algorithms;class collision prediction;collision predicted;danger collision predicted;collision data real;collision predicted paper;cnn based;danger collision;best cnn based;collision;collision data;best cnn;cnn based algorithm;cnn architectures;cnn;cnn architectures approaches;collect collision data;observe best cnn;modern cnn architectures;collect collision;collision probability;achieves prediction accuracy;prediction accuracy;deep learning;based modern cnn;collision probability present;modern cnn;developments deep learning;source danger collision", "pdf_keywords": ""}, "de5057c1da9391269e926d4661d4558072db9f18": {"ta_keywords": "attention fusion;train attention fusion;fusion based attention;attention fusion module;stream train attention;stream level fusion;speech recognition parallel;stream paradigm audio;speech recognition;attention mechanisms combine;multi stream train;stream paradigm;train attention;multi stream paradigm;recognition parallel encoders;level fusion based;based attention mechanisms;fusion;swimmer end pool;automatic speech;paradigm audio processing;multi stream;fusion based;ufe features pretrained;end multi stream;stream level;level fusion;attention;stream train;swimmer end", "pdf_keywords": "multi stream attention;stream attention based;stream attention;attention based module;end asr recognition;ctc attention e2e;train attention based;attention e2e scheme;ctc attention;joint ctc attention;train attention;asr recognition;attention e2e;attention based;asr recognition experiments;trainable multi stream;architecture train attention;train multi stream;multi stream end;attention;task architecture train;stream model;multi stream;pytorch end train;stream end end;end end asr;improve multi stream;stream end;multi stream model;multi task architecture"}, "281605579936538ee92bc4b0baad1b83c683c076": {"ta_keywords": "graph dependency parse;language generation amr;parse text linearizer;train parse text;analysis language generation;parse;dependency parse tree;parse tree;train parse;dependency parse;language generation;predicting structure web;language generation process;amazingly language generation;amr graph dependency;text linearizer;parse text;graph dependency;structure web graph;approach train parse;parse tree paper;text linearizer paper;analysis language;web graph;structure web;language;predicting structure;presents analysis language;transform amr graph;amr graph", "pdf_keywords": ""}, "4ae0c4a511697e960c477ea3e37b3e11bf3e0e02": {"ta_keywords": "receptive fields imagenet;trained local receptive;local receptive fields;local receptive;receptive fields;robust convolutional networks;convolutional networks;imagenet sketch;convolutional neural networks;training robust convolutional;image classification set;image classification;convolutional neural;imagenet sketch new;local representations learned;sketch like images;imagenet;robust convolutional;neural networks trained;convolutional networks penalizing;networks trained;networks trained local;matches image classification;representations learned;training robust;fields imagenet sketch;dataset consisting sketch;society convolutional neural;convolutional;neural networks", "pdf_keywords": "adversarial domain adaptation;adversarial regularization;adversarial domain;adversarial regularization technique;adversarial regularization par;known adversarial domain;domain adaptation;wise adversarial regularization;patch wise adversarial;generalization known adversarial;domain generalization convolutional;adversarial;domain separation networks;domain adaptation method;known adversarial;regularization par learning;wise adversarial;cnns;domain generalization known;distribution domain generalization;separation networks cnns;cnns class deep;domain generalization;deep learning;new domain generalization;regularization;generalization convolutional neural;propose deep learning;generalization convolutional;networks cnns"}, "ce4db7a32724e0abc8afe27f74d33e32e099b8e6": {"ta_keywords": "genes porcine fit1;regulated genes porcine;porcine fit1 gene;muscle myogenic expression;myogenesis fat induced;myogenesis fat;myogenic expression porcine;activity myogenesis fat;fat induced transcript;proteins regulated genes;porcine fit1 promoter;promoter activity myogenesis;regulation motor proteins;genes porcine;regulated genes;expression fit1 gene;motor proteins regulated;myogenic genes;expression porcine fit1;proteins regulated;porcine fit1 coding;development porcine fit1;sequence myogenic genes;muscle development porcine;fit1 gene present;myogenic expression;fit1 gene pigs;fit1 promoter activity;triglyceride skeletal muscle;fit1 gene", "pdf_keywords": ""}, "7506626f776f211afac2c2d1138aca0e0479e5c3": {"ta_keywords": "gans transform latent;networks gans;networks gans transform;generative adversarial networks;adversarial networks gans;noise generative adversarial;gans transform;generative adversarial;recover preimages latent;images latent space;projecting images latent;gans;latent vector preimages;images latent;noise generative;preimages latent vector;adversarial networks;preimages latent;latent vectors visually;stochastic clipping;based stochastic clipping;adversarial;precisely recover preimages;transform latent vectors;visually plausible images;noise unseen images;generative;latent vectors;transform latent;presence noise generative", "pdf_keywords": "networks gans adapt;networks gans;generative adversarial networks;adversarial networks gans;generative adversarial;space generative adversarial;cnn method inverse;inverse mapping images;gans adapt deterministic;gradient descent recover;adversarial networks;neural network loss;deep neural;adapt deterministic deep;robustness reconstructions noise;deterministic deep neural;deterministic deep;adversarial;reconstructions noise stochastic;robustness reconstructions;generative modeling generative;gans adapt;gans;reconstruction error space;reconstructions noise;deep convolutional;generative modeling;networks task generative;modeling generative;mapping images latent"}, "712cd873d7370db280f4ceaaf000dc49f76b59fe": {"ta_keywords": "adversarial attacks seq2seq;performing untargeted adversarial;untargeted adversarial training;model terms adversarial;adversarial robustness hurting;attacks machine translation;untargeted adversarial;terms adversarial robustness;terms adversarial;adversarial training;sequence seq2seq models;adversarial robustness;evaluation framework adversarial;adversarial training meaning;attacks seq2seq models;adversarial;framework adversarial;quantum simulation toolkit;framework adversarial attacks;adversarial attacks;robustness sequence sequence;performance quantum simulation;robustness sequence;sequence models demonstrate;evaluation performance quantum;sequence sequence models;assess robustness sequence;attacks seq2seq;machine translation;seq2seq models", "pdf_keywords": "adversarial attacks seq2seq;adversarial example generation;untargeted adversarial training;performing untargeted adversarial;word substitution attacks;adversarial robustness hurting;adversarial training;terms adversarial;present evaluation adversarial;preserved context adversarial;model terms adversarial;untargeted adversarial;evaluation adversarial;terms adversarial robustness;adversarial training meaning;evaluation adversarial example;adversarial;adversarial example;context adversarial;adversarial robustness;adversarial attacks propose;adversarial attacks;context adversarial attacks;evaluation framework adversarial;automatic machine translation;framework adversarial;machine translation;framework adversarial attacks;machine translation discrete;substitution attacks"}, "1c8d9d5558dc43f3505fa37fc50247e3ce0d2f54": {"ta_keywords": "deconfounded observational dataset;confounded observational dataset;deconfounded data;dataset confounder unobserved;deconfounded data required;observational dataset confounder;confounded data significantly;collect deconfounded data;confounded data;dataset confounder;deconfounded observational;deconfounded data run;large confounded observational;confounder unobserved;small deconfounded observational;confounder unobserved alongside;quantity deconfounded data;confounded observational;inclusion confounded data;incorporating large confounded;practitioner collect deconfounded;confounder;mutation cancer estimate;deconfounded;collect deconfounded;benefits selective deconfounding;elucidate properties causal;run clinical trials;selective deconfounding;results inclusion confounded", "pdf_keywords": "confounders randomized trials;selecting confounders randomized;confounders randomized;revealing confounder based;confounder based observed;revealing confounder;confounders propose;selecting confounders;confounder based;confounders;sequence confounders propose;confounders propose use;unobserved confounding confounded;method selecting confounders;confounder;confounded data;policy confounded data;confounded data theoretical;confounding confounded distribution;accuracy level confounder;sequence confounders;budget revealing confounder;confounder binary;binary sequence confounders;confounder binary sequence;inclusion confounded data;confounded distribution modeled;unobserved confounding;deconfounded data required;level confounder"}, "4d86b32ea80e2d9df2283fac39892d6dbd87ea87": {"ta_keywords": "discriminative training method;propose discriminative training;discriminative training;discriminant functions propose;pattern recognition;class discriminant functions;discriminant functions;prototype based classifiers;functions propose discriminative;classification pattern patterns;problem pattern recognition;classification pattern;method pattern recognition;classifiers;pattern recognition based;minimum error classification;general class discriminant;class discriminant;discriminant;classification tasks derive;error classification pattern;classification tasks;range classification;based classifiers;discriminative;propose discriminative;classification;range classification tasks;wide range classification;training method pattern", "pdf_keywords": ""}, "8786ddc38ae0763e772337bf9331436252452918": {"ta_keywords": "mitigating entity bias;entity bias real;entity bias;entity debiasing;influence entity bias;unintended entity bias;entity bias cause;fake news detection;entity debiasing framework;propose entity debiasing;generalizes fake news;crowdsourcing news;crowdsourcing news pieces;bias percentage news;news pieces crowdsourcing;existing fake news;fake news increasingly;news veracity;entity bias percentage;entities news;pieces crowdsourcing news;contents news veracity;debiasing framework;bias real world;ability fake news;entities news contents;news detection models;news veracity separately;predict news;predict news pieces", "pdf_keywords": "detect fake news;detection fake news;fake news detection;entity bias fake;fake news detectors;bias fake news;mitigating entity bias;entity bias better;entity bias;mitigate entity bias;entity bias mitigate;news detection mitigating;highlight entity bias;biases existing fake;news detection models;trust online news;ability fake news;news detection datasets;news detectors;existing fake news;fake news widely;news detectors automatic;rely news content;bias better generalization;news detection;bias fake;online news ecosystem;propose debiasing framework;news ecosystem;news content features"}, "9d0e4e9c9343b85311b1adff145fdbdfb69486ff": {"ta_keywords": "telescope hst data;space telescope hst;hubble space telescope;telescope hst;analysis hubble space;hubble space;analysis hubble;hst data;space telescope;hubble;tool analysis hubble;telescope;hst;data;new tool analysis;space;tool analysis;present new tool;new tool;tool;analysis;new;present new;present", "pdf_keywords": ""}, "cc74ef901219dfd26efbbb8b7b87d1b7b7d38634": {"ta_keywords": "study chronology milky;chronology milky;historical texts neural;chronology milky way;study motion newborn;motion newborn;study chronology;chronology;normalization historical texts;newborn electrophysiological setting;motion newborn newborn;neural network models;texts neural;newborn electrophysiological;study normalization historical;numerical study motion;devoted study chronology;normalization historical;newborn newborn electrophysiological;neural;texts neural network;historical texts;milky way einer;milky way;study motion;die automatische zentrum;study normalization;neural network;newborn newborn;milky", "pdf_keywords": ""}, "cd06dfa789bfe491130ac7440e55d9d407396a43": {"ta_keywords": "randomized coordinate descent;coordinate descent algorithms;sequencing importance sampling;coordinate descent;rna sequencing;limitations rna sequencing;rna sequencing importance;descent algorithms;randomized coordinate;descent algorithms solving;rna;variants methods spectral;importance sampling;acceleration strategy rcd;limitations rna;rcd based augmentation;sequencing;optimization;understand limitations rna;algorithms;sequencing importance;present randomized coordinate;algorithms solving optimization;importance sampling paper;inexact variants methods;variants methods;randomized;optimization problems big;solving optimization problems;solving optimization", "pdf_keywords": "spectral descent optimal;spectral coordinate descent;stochastic spectral descent;spectral descent;stochastic descent coordinate;stochastic descent;accelerated coordinate descent;method stochastic spectral;descent method stochastic;coordinate descent accelerated;keywords stochastic descent;coordinate descent method;descent optimal;stochastic spectral coordinate;optimal matrices;descent coordinate descent;coordinate descent;optimal matrices msscd;convergence stochastic spectral;new stochastic spectral;convex matrix eigenvectors;matrices msscd optimal;stochastic spectral;decomposition eigenvalues convex;solving convex optimization;descent accelerated coordinate;convex optimization;descent optimal rate;coordinate descent sddc;probabilities optimal matrices"}, "aacaad6ab396e085799052b1a667c965d6465e32": {"ta_keywords": "protein emulsion;protein stabilized emulsions;protein emulsion droplet;interaction protein emulsion;emulsion stability;emulsion stability increased;stabilized emulsions;activity emulsion stability;stability increased emulsion;emulsifying activity emulsion;emulsions;emulsion content;emulsion droplet;increased emulsion content;emulsion droplet used;emulsion;activity emulsion;increased emulsion;emulsification emulsifying;affecting emulsification emulsifying;protein stabilized;emulsification emulsifying activity;affecting emulsification;emulsification;stabilized emulsions lead;myofibrillar protein;factors affecting emulsification;emulsifying activity;emulsions lead formation;emulsifying", "pdf_keywords": ""}, "d5810f15cfdd59da549ffa648c5a05d806d94eb7": {"ta_keywords": "fact checking platform;checking platform journalistic;automated fact checking;journalists workflow fact;fact checking essential;platform journalistic setting;feedback journalists workflow;fact checking;textual evidence;platform journalistic;journalists workflow;using feedback journalists;workflow fact checking;misinformation predictions platform;evidence document collection;efforts combating misinformation;feedback journalists;textual evidence document;present automated fact;automated fact;journalism highlighted;study fact checking;combating misinformation predictions;combating misinformation;task journalism highlighted;task journalism;journalism highlighted recent;retrieves relevant textual;relevant textual evidence;essential task journalism", "pdf_keywords": "fact checking platform;automated fact checking;factchecking platform journalistic;journalistic corpus testing;factchecking platform;fact checking computational;corpus testing journalists;checking computational journalism;user testing journalists;study factchecking platform;evaluated journalistic corpus;journalistic corpus;research automated fact;propose automated fact;user study factchecking;fact checking;platform journalistic setting;journalists workflow;testing journalists;document collection findings;automated fact;testing journalists british;journalists workflow evaluate;factchecking;computational journalism;testing journalists yield;computational journalism broadly;feedback journalists workflow;broadly evaluated journalistic;evaluated journalistic"}, "a5148776955ef523de318a2fb45f8256e966b98e": {"ta_keywords": "distantly labeled data;extracting information noisy;distantly labeled;labeled data;label propagation;using label propagation;used distantly labeled;label propagation graph;information noisy training;classifiers used distantly;noisy training data;nodes entity mentions;labeled;information noisy;noise using label;labeled data paper;safety cluttered list;entity mentions;entity mentions mentions;extracting information;classifiers;cluttered list items;performance shelf classifiers;cluttered list;shelf classifiers;method extracting information;using label;label;training data;classifiers used", "pdf_keywords": ""}, "05c2bb89a5c42ad7932420bb39df2e566df6e1ec": {"ta_keywords": "annotation editor;abstract annotation editor;annotation editor ada;annotators highly;annotators create annotated;annotators;human annotators create;create annotated;annotation data;annotators highly time;annotation;human annotators;annotated data natural;annotators create;annotated data;annotated training data;annotation data manually;annotated training;abstract annotation;create annotated data;annotated;java abstract annotation;team annotators highly;aid human annotators;team annotators;manually team annotators;lot annotated training;space cases annotation;data natural language;editor ada stochastic", "pdf_keywords": ""}, "2127bea25859ba9c5997e2d15e17899a75ef6cb3": {"ta_keywords": "programs big code;statistical programming tools;big code;important interesting programming;big code seminar;interesting programming;programs big;programming tools learn;programming languages;programming tools;study big bang;kinds statistical programming;processing programming languages;programming;interesting programming challenges;big bang;programming languages software;processing programming;developing statistical tools;programming challenges;big bang bb;language processing programming;computer science;testing programs big;bifurcations bose einstein;bifurcations bose;bose einstein condensate;lh bifurcations bose;code seminar;big code order", "pdf_keywords": ""}, "3e2bac2abfb5b33a43fe56db5a868e17e38c616a": {"ta_keywords": "pendulums framework;coupled pendulums framework;coupled pendulums;pendulums framework nonlinear;pendulums;dynamics coupled pendulums;simple mechanical model;mechanical model spring;mechanical model;penning trap spring;trap spring placed;schrdinger equation students;center trap spring;trap spring;simple mechanical;spring penning trap;project based learning;nonlinear schrdinger;trap spring held;dynamics coupled;equation students learn;framework nonlinear schrdinger;present simple mechanical;dynamics;nonlinear schrdinger equation;students learn work;model spring;model spring penning;equation students;students learn", "pdf_keywords": ""}, "baf47cd0b471a9bb7b2230fec0b680fc9b3c4783": {"ta_keywords": "sequential tasks pragmatics;pragmatic speaker model;pragmatic inference aids;descriptions pragmatic listener;build pragmatic speaker;explicit pragmatic inference;pragmatic inference;pragmatic speaker;natural language instructions;listener speaker models;descriptions pragmatic;pragmatic listener;structure explicit pragmatic;pragmatics enabled models;tasks pragmatics;explicit pragmatic;pragmatics enabled;language instructions complex;models explain speakers;pragmatics;candidate descriptions pragmatic;following natural language;complex sequential tasks;tasks sequential structure;natural language;sequential tasks;pragmatic listener reasons;models tasks sequential;build pragmatic;pragmatic", "pdf_keywords": "instruction generation tasks;pragmatic inference models;instruction generation incorporating;instruction following tasks;instruction generation;explicit pragmatic inference;task computational pragmatics;pragmatic reasoning agents;recursive pragmatic reasoning;language recursive pragmatic;pragmatic inference;pragmatic inference studied;example instruction;pragmatic inference procedure;complex sequential instruction;computational pragmatics;commands structured representations;layering explicit pragmatic;sequential instruction;following instruction generation;interrelated tasks instruction;recursive pragmatic;generate new instructions;paper pragmatic inference;tasks instruction;models capable reasoning;execution model learning;model pragmatics helps;models parse commands;explicit pragmatic"}, "00cc6deb3cf2c9281ddcf4875aad3ee14c92e52f": {"ta_keywords": "lingual named entity;named entity recognition;leveraging machine translation;large annotated corpora;translating entities;entity recognition leverage;translating entities matching;entity recognition;subsequently translating entities;approaches cross lingual;annotated corpora;annotated corpora named;machine translation;machine translation systems;corpora named entity;translation systems;leverage machine translation;methods cross lingual;entity recognition propose;cross lingual named;entity recognition diverse;translation improve annotation;cross lingual;lingual named;annotation projection approaches;corpora;machine translation improve;lingual;recognition diverse languages;translation systems subset", "pdf_keywords": "entity recognition crosslingual;cross lingual annotation;lingual speech tagging;lingual named entity;lingual annotation;lingual annotation projection;cross lingual approaches;approaches cross lingual;translation improve annotationprojection;named entity recognition;entity recognition increasingly;cross lingual named;entity recognition medium;quality annotated corpora;crosslingual approaches named;annotated corpora;lingual approaches;entity recognition;recognition crosslingual approaches;crosslingual approaches;lingual approaches leverage;cross lingual;annotationprojection approaches cross;method cross lingual;cross lingual speech;lingual approaches named;ner tagging accuracy;focuses cross lingual;improve annotationprojection approaches;speech tagging based"}, "be360de73689dc4af56f7adcee7e38d7acfed1e1": {"ta_keywords": "ranked list aggregation;ranked aggregation;aggregate orderings;orderings rankings smaller;rankings smaller set;aggregate ordering;orderings rankings;ranked list;aggregate orderings maintains;rankings smaller;just aggregate ordering;set orderings rankings;list aggregation technique;performance ranked list;set aggregate orderings;list aggregation;context ranked aggregation;aggregate ordering shown;rankings;aggregation technique;aggregation;smaller set aggregate;performance ranked;orderings;ranked;fairness classical results;aggregating given set;set orderings;ordering;given set orderings", "pdf_keywords": ""}, "0f5bb9ae0c060b349597c0b2582bf271a5a2156a": {"ta_keywords": "supertagging parsing;results supertagging parsing;supertagging parsing single;tagging models encode;information encoded supertags;pos tagging models;models pos tagging;tagging models;supertagging;encoded supertags;syntactic information encoded;results supertagging;encoded supertags present;supertags;art results supertagging;long range syntactic;syntactic information;supertags present;models demonstrate feedforward;syntactic;pos tagging;parsing;feedforward architectures;neural models;bidirectional models;tagging;bidirectional models pos;demonstrate feedforward architectures;supertags present new;parsing single headed", "pdf_keywords": ""}, "ce268e0942ce0d1f6942e4d7e7e2aa6464f1b577": {"ta_keywords": "automatically learn pronunciation;pronunciation learning;non native pronunciations;non native pronunciation;evidence pronunciation learning;learn pronunciation lexicon;native pronunciation able;pronunciation non native;pronunciation learning previous;learn pronunciation;native pronunciation;native pronunciations;including pronunciations new;native pronunciation variations;native pronunciation non;pronunciation lexicon iterative;native pronunciations english;design pronunciation lexicon;pronunciation lexicon;non native speech;acoustic evidence pronunciation;pronunciations new words;pronunciation able achieve;non native speakers;predicting non native;significantly native speech;pronunciation lexicon used;pronunciations new;automatic speech recognition;native speech differs", "pdf_keywords": ""}, "cb53f9558bd13c853026f97dce3bbe3d989ca97d": {"ta_keywords": "game theoretic argumentation;controversies language argumentation;argumentation controversies language;argumentation language;language argumentation;argumentation controversies;argumentation fallacies;language argumentation language;argumentation fallacies context;argumentation;study argumentation fallacies;theoretic argumentation controversies;study argumentation;presents study argumentation;theoretic argumentation;context team sports;corpus present game;controversies language;argumentation language replaced;fallacies context team;game theoretic;platform topic selection;team sports;game platform topic;team sports examine;creation effective campaigns;topic selection;language report classification;sports;sports examine", "pdf_keywords": ""}, "57dd2bd5fb6677191f9b36b589c91bb171e217ff": {"ta_keywords": "single person web;web connection single;person web page;way create web;person web;create web;web connection;create web like;connection single person;web page;like web connection;web like web;connection single;web;single person;web like;like web;connection;person;page;effective way create;single;way create;create;simple effective way;simple;simple effective;article present simple;effective way;present simple", "pdf_keywords": ""}, "a4dd375c18709b1554249cc5cb88d8ba6acfea10": {"ta_keywords": "translation machine;translation systems;machine translation machine;speed translation process;translation machine translation;translation systems used;describes machine translation;machine translation systems;machine translation finally;translation process able;machine translation long;use machine translation;machine translation;machine translation standard;translation process;translation standard technology;technology machine translation;speed translation;question machine translation;used speed translation;translation standard;translation finally;translation long;translation finally seeing;translation long held;translation;large scale use;answer question machine;question machine;real use machine", "pdf_keywords": ""}, "a95400c70c4beb609c77cc500677b2f1ed852e8e": {"ta_keywords": "paraphrase detection;paraphrase detection generates;resolution paraphrase detection;coreference resolution paraphrase;multiple sentences coreference;semantic inference texts;semantic inference;sentences coreference resolution;sentences coreference;approach semantic inference;distractors answer choices;inference texts requires;answers semantically;utilize specific inference;multiple choice answers;phrase level distractors;answers semantically motivated;inference texts;resolution paraphrase;paraphrase;steps multiple sentences;inference steps multiple;motivated phrase level;correct answers semantically;answer choices;coreference resolution;inference steps traditional;semantically motivated phrase;sentence approach multiple;inference", "pdf_keywords": ""}, "63d99a61e798d7cb714f336a8d581ae2b75672ee": {"ta_keywords": "phonetic metric lexical;phonetic metric;phoneme discriminative representation;35 phonetic metric;phoneme discriminative;syntactic metric achieved;lexical metric;environment phoneme discriminative;lexical metric syntactic;gain lexical metric;phonetic;improvement 35 phonetic;metric lexical;metric lexical metric;syntactic metric;metric syntactic metric;metric syntactic;discriminative representation achieved;pseudo labels outputs;gain lexical;clustering outputs;generate pseudo labels;pseudo labels;lexical;35 phonetic;noisy environment phoneme;discriminative representation;clustering outputs final;discriminative;distance speakers", "pdf_keywords": "clustering language models;phoneme discriminative representation;phonetic metric lexical;spoken language model;language models train;phonetic metric;phoneme discriminative;clustering language;35 phonetic metric;model deep cluster;coding deep cluster;labels raw speech;language models;deep cluster method;data clustering language;train spoken language;deep cluster;models train spoken;language model;language model slm;improvement 35 phonetic;phonetic;raw speech signal;spoken language;symbols phoneme discriminative;syntactic metric achieved;lexical metric;discriminative representation achieved;lexical metric seven;gain lexical metric"}, "d4305b3bf233e5f192a5d17dde114b771b621d92": {"ta_keywords": "entanglement detection based;entanglement detection;approach entanglement detection;entanglement;approach entanglement;novel approach entanglement;use entropic entropies;entropic entropies;entropic;entropies;detection;based use entropic;use entropic;detection based;detection based use;paper;based;paper propose;novel approach;approach;propose novel approach;propose;based use;novel;paper propose novel;use;propose novel", "pdf_keywords": ""}, "de3c3eb590065a6d78ec8566161f8236ab2a7435": {"ta_keywords": "translation operations;translation server automatic;translational invariant water;translation operations domain;automatic evaluation translation;translation server;domain translation invariant;translation domain write;translation invariant;evaluation translation results;translation domain;write translation domain;domain translation;domain write translation;translation domain domain;domain domain translation;results translation server;translation invariant example;domain translational;domain translation invariance;translation results translation;server case translation;evaluation translation;translation rewrite;translation invariance domain;domain translational invariant;problem translation domain;translation results;translation invariance;translation rewrite problem", "pdf_keywords": ""}, "ffb562d3ac7d86b5c527863f5a3e72e1aa22a809": {"ta_keywords": "incentive mechanism prediction;information incentivize agents;incentivize agents;incentivize agents different;problem crowd sourcing;crowd sourcing multiple;incentive mechanism;group agents;crowd sourcing;heterogeneous agents private;group agents able;joint incentive mechanism;agents private information;rational agents private;optimal joint incentive;proportional number agents;elicit heterogeneous agents;joint incentive;agents private;sourcing multiple agents;multiple agents;behavior group agents;agents consider problem;incentive;cost prediction elicitation;private information incentivize;approach problem crowd;agents consider;agents;information incentivize", "pdf_keywords": ""}, "076b2ba158c35bd2941769864ce7455cf76ecd8e": {"ta_keywords": "peer review process;discussing peer review;peer review;reviewers argument discussing;reviewers argument;reviewers senior decision;present peer review;given paper reviewers;argument discussing peer;paper reviewers;paper reviewers senior;peer review backbone;reviewers senior;case reviewers argument;reviewers;review process;discussion decision making;review process design;biases present peer;influenced case reviewers;discussion decision;influence argument presented;case reviewers;discussing peer;senior decision makers;decision makers;decision making process;argument presented discussion;study influence argument;paper study influence", "pdf_keywords": "reviewers herding behaviour;consensus reviewers herding;reviewers begin discussion;reviewers herding;herding behaviour discussions;discussion opinion reviewers;herding behaviour discussion;choice reviewers herding;peer review discussion;review discussion takes;discussion management experiment;consensus reviewers;erroneous consensus reviewers;discussion dynamics;infer pre discussion;herding behaviour;discussion takes place;discussions participants;herding behaviour present;conference peer review;review discussion;review process peer;discussion management;conducted discussion management;conducted discussion;discussion initiators;pre discussion opinion;experiment identify herding;opinions discussion phase;peer review process"}, "80b747af8d86541cf53198519c8fa51109eed4f9": {"ta_keywords": "crowdsourced unsolicited recommendations;data augmentation semisupervised;unsupervised data augmentation;corresponding crowdsourced unsolicited;augmentation semisupervised;crowdsourced unsolicited;supervised;crowdsourced;popularity text classification;corresponding crowdsourced;sequence labeling tasks;augmentation semisupervised technique;consistency loss supervised;learning algorithm significantly;algorithm predicting;loss supervised;predictions observed unlabeled;data augmentation;examples corresponding crowdsourced;classification;sequence labeling;text classification;text classification open;algorithm predicting outcomes;algorithms xcite unsupervised;unsupervised;supervised learning;loss supervised learning;unsupervised data;enforcing consistency predictions", "pdf_keywords": "semi supervised augmentation;unsupervised data augmentation;sequence tagging datasets;word replacement augmentation;supervised augmentation;supervised augmentation method;labeled entity recognition;effective data augmentation;classi\ufb01cation sequence tagging;sequence tagging;data augmentation;data augmentation techniques;sequence tagging classi\ufb01cation;entity recognition;augmentation method labeled;sequence tagging hypothesize;context nlp tasks;nlp tasks;semi supervised;uda sequence tagging;entity recognition paper;supervised machine translation;present semi supervised;semi supervised machine;tagging datasets;investigate semi supervised;strategies sequence tagging;labeled entity;tagging datasets paper;data augmentation uda"}, "b033400e9a80915a928f4603582e5e8bf7656a85": {"ta_keywords": "unimodal baselines multimodal;assessing performance multimodal;performance multimodal;baselines multimodal;performance multimodal techniques;unimodal baselines;baselines multimodal domains;multimodal;unimodal approaches better;multimodal techniques;visual navigation bioinformatics;unimodal approaches;ablations recent datasets;multimodal domains;strength unimodal baselines;unimodal ablations;unimodal ablations recent;multimodal domains make;approaches better capture;unimodal;present unimodal ablations;navigation bioinformatics;datasets visual navigation;datasets visual;present unimodal;argue unimodal approaches;navigation bioinformatics showing;recent datasets visual;visual;performance published baselines", "pdf_keywords": "captioning ablate models;models propose multimodal;egocentric question answering;propose multimodal;crowdsourced language descriptions;visual inputs crowdsourced;crowdsourced natural language;multimodal;crowdsourced language;image captioning ablate;crowdsourced natural;multimodal domains bias;captioning;captioning ablate;descriptions navigation egocentric;image captioning;inputs crowdsourced natural;language vision;assessing performance multimodal;performance multimodal;routes image captioning;multimodal techniques;language vision proposed;ablating language vision;performance multimodal techniques;paired crowdsourced language;inputs crowdsourced;navigation question answering;crowdsourced;multimodal domains"}, "c0099a15bd3251083c62ebd47c9705a16309b974": {"ta_keywords": "zero crossing image;multiscale zero crossing;crossing image representation;crossing image;multiscale zero;zero crossing;novel multiscale zero;level early vision;multiscale;image representation;novel multiscale;early vision;present novel multiscale;image representation image;image processing tasks;representation image processing;image processing;representation image;crossing;vision;image;level early;tasks level early;processing tasks level;processing tasks;representation;early;tasks level;processing;zero", "pdf_keywords": ""}, "a16ae67070de155789a871cb27ecbf9eaa98b379": {"ta_keywords": "human evaluations text;evaluations text generated;nlg evaluation;nlg evaluation provide;natural language generation;nlg researchers improving;evaluations text;nlg researchers;evaluations play nlg;recommendations nlg;machine authored text;recommendations nlg researchers;provide recommendations nlg;text generated;domains human evaluations;improving human evaluations;untrained human evaluations;play nlg evaluation;authored text;natural language;gpt3 authored text;nlg;quickly training evaluators;text gpt3 authored;language generation;human machine authored;annotate text;annotate text gpt3;human evaluations;text generated state", "pdf_keywords": "machine generated evaluation;generated texts evaluated;machine generated texts;evaluation recognizing machine;machine generated text;evaluation recognizing;generated text collecting;collecting human evaluations;evaluations language models;evaluations language;learners evaluations end;machine authored text;generated text intrinsic;generated texts;learners evaluations;multitask learners evaluations;human evaluations;evaluations end users;human evaluations instead;generated evaluation;machinegenerated text;human evaluations state;texts evaluated;generated text reliably;generated evaluation used;human authored text;text models evaluated;identify machinegenerated text;machinegenerated text maintaining;authored text evaluated"}, "ece56ab633f11d1592a3d4f9386412d3f48fcf95": {"ta_keywords": "natural language argumentation;language argumentation solution;argument reasoning comprehension;argument analyze warrant;language argumentation;arguments highly contextualized;argument reasoning;reasoning based language;argumentation solution task;comprehend argument analyze;argumentation solution;comprehension human reasoning;language understanding reasoning;arguments news comments;human reasoning;contextualized warrants;comments reasoning crucial;argumentation;highly contextualized warrants;authentic arguments news;comprehend argument;reasoning comprehension task;human reasoning based;argument analyze;crucial natural language;task argument reasoning;reasoning comprehension;warrant reconstruction comprehend;premises arguments highly;news comments reasoning", "pdf_keywords": "natural language argumentation;natural language arguments;reasoning natural language;argument reasoning comprehension;language argumentation;language argumentation parts;argumentation parts;arguments annotated reasons;arguments annotated;argument reasoning;arguments annotated stance;discussed argumentation;arguments discussed argumentation;argumentation parts arguments;called argument reasoning;generated arguments annotated;argument summarization;language argumentation addition;reasoning comprehension tackles;argumentation addition provide;argumentation;026 arguments annotated;argumentation addition;argumentation scholars;argument summarization sentence;discussed argumentation scholars;language arguments discussed;logic natural language;useful argument summarization;user generated arguments"}, "2232808cf3161ca4c434126e35f47ee33c0c8219": {"ta_keywords": "explanations accuracy gains;evaluating explanations know;attribution methods;explanations accuracy;numerous attribution methods;attributions;classification question answering;evaluating explanations;question answering;evaluation attributions;attribution methods text;attributions methods;agnostic evaluation attributions;evaluation attributions methods;attribution;question answering observe;compare numerous attribution;state evaluating explanations;value explanations accuracy;numerous attribution;attributions methods purport;explanations serve;quantify value explanations;explain predictions;features aims explanations;explanations know;explanations serve ought;explanations;text classification;aims explanations serve", "pdf_keywords": "tasks learning explanations;learning explanations modeled;learning explanations;trained intermediate explanations;learning explanations question;explanations question answering;intermediate explanations performance;explanations performance serve;explanations utility student;answering tasks learning;explanations modeled;explanations utility;explanations modeled meta;explanations communicate information;explanations communicate;explanations performance;producing explanations;update explanation generation;producing explanations framework;explanations framework;explanation generation;explanations framework inspired;framework learning explanations;question answering tasks;tasks learning;value explanations utility;ensure different explanations;incorporates attention regularization;answering tasks;human reasoning"}, "228f2efe7b06b6db3b2c6c0a61d7b33daee1d641": {"ta_keywords": "sense disambiguation based;word sense disambiguation;disambiguation based automatically;sense disambiguation;disambiguation based;learn relevant semantic;lexical semantic resources;sentence synset;synset based learning;synset constituting sense;semantic similarity;lexical semantic;relevant semantic similarity;disambiguation;sentence synset constituting;different lexical semantic;synset based;semantic similarity given;sense target word;given sentence synset;semantic;synset;approach word sense;synset constituting;semantic resources;automatically induced synsets;word sense;lexical;synsets;semantic resources propose", "pdf_keywords": ""}, "301352755a94d7524312b7c7f2fab7d3fd3d334d": {"ta_keywords": "optimality dominance queries;dominance queries;dominance queries context;nets efficient dominance;propose aggregation;efficient dominance procedure;conditional preferences based;study optimality dominance;paper propose aggregation;optimality dominance;consider aggregation;aggregation;propose aggregation method;preferences based notion;conditional preferences;consider aggregation large;aggregation large;efficient dominance;probabilistic;model conditional preferences;aggregation method;aggregation method better;paper consider aggregation;notion conditional uncertainty;dominance procedure;introduce probabilistic;multi agent;random variables single;variables single random;dominance procedure used", "pdf_keywords": ""}, "035595ebf6821031a543ee1c30386a6230fc7a41": {"ta_keywords": "online speaker diarization;speaker diarization algorithm;speaker diarization;diarization process speaker;propose speaker tracing;process speaker regions;speaker tracing;called speaker tracing;representing speaker permutation;speaker permutation information;speaker tracing buffer;online diarization;frames representing speaker;online speaker;process speaker;speaker tracing able;self organizing;supervised self attention;speaker locations based;online diarization process;data online diarization;fully supervised self;speaker permutation;representing speaker;self organizing elements;novel online speaker;speaker regions;diarization algorithm based;supervised self;assigned speaker locations", "pdf_keywords": "speaker diarization based;speaker diarization widely;embeddings speaker diarization;speaker diarization crucial;approach speaker diarization;speaker diarization;speech segments speakers;diarization based deep;speaker diarization end;speech segments;assign speech segments;buffer speaker diarization;network embeddings speaker;ambiguity speaker diarization;embeddings speaker;pipeline locate speaker;assign speech;speaker tracing;diarization based;speaker permutation;diarization widely;end speaker permutation;segments speakers;telephone conversations;using speaker tracing;telephone conversations home;length using speaker;neural network embeddings;datasets callhome;datasets callhome csj"}, "8328508dc12c295165f997e02d74d00a42971c01": {"ta_keywords": "semantic parsing context;parsing context;parsing context received;parsing context code;typical context modeling;13 context modeling;context modeling;world semantic parsing;decoding semantic parser;effective context based;semantic parsing;frequent contextual;semantic parser;context modeling methods;context based learning;far effective context;challenging complex contextual;context based;semantic parser adapt;contextual phenomena far;decoding semantic;13 context;effective context;study context modeling;complex contextual;contextual;contextual phenomena fine;frequent contextual phenomena;recently semantic parsing;context code", "pdf_keywords": ""}, "d36e39aedd802aea4be1ea303c70dc56e97dbc3c": {"ta_keywords": "summary factually consistent;evaluation metrics summarization;answers summary factually;generated summaries;evaluation metric summarization;generates usable factually;factually consistent text;summarization propose automatic;generated summaries cnn;identifies factual inconsistencies;model generated summaries;usable factually consistent;factual inconsistencies generated;metrics summarization largely;generated summary;summarization datasets;summaries cnn dailymail;summary factually;metric summarization;metrics summarization;summarization largely insensitive;automatic automatic evaluation;metric summarization propose;inconsistencies generated summary;judgments factual consistency;existing automatic evaluation;summaries cnn;dailymail xsum summarization;factual consistency model;automatic evaluation", "pdf_keywords": "generated summaries summarization;detecting factual inconsistencies;inconsistency text summarization;automatically summarizing;automatically detecting factual;generating usable factually;generated summaries;generation tasks summarization;detect factual inconsistencies;ngrams automatic summarization;automatic summarization;automatically summarizing human;correctness crowdsourced situations;conditional text generation;conditionally generated texts;summarization dialogue;summarizing human judgments;factually consistent text;summaries summarization datasets;summarization datasets 2015;automatic summarization popular;summarization datasets;summarization dialogue large;model generated summaries;judgments correctness crowdsourced;summarization using sequence;factual inconsistencies generated;framework automatically summarizing;correctness crowdsourced;detecting factual"}, "6c6975750207f787c318627ff7cb63a649165a8d": {"ta_keywords": "intelligent tutoring agent;agent tutoring task;intelligent tutoring;tutoring agent;agent tutoring;representation agent tutoring;algorithm intelligent tutoring;tutoring task;tutoring task present;tutoring;tutoring agent sim;integrating learned display;learned display;learner support learning;learned display representation;expert humans learn;grammar learner support;grammar learner;free grammar learner;solving skills;learn process displays;problem solving skills;results learning complex;learning complex;humans learn;learner support;learner;student integrating learned;display representation agent;learning", "pdf_keywords": ""}, "1a671afdac8e7b759cf3b5ec7d03d485c76a989c": {"ta_keywords": "speech recognition asr;sequence sequence models;speech recognition tasks;speech recognition;automatic speech;sequence models;automatic speech recognition;stochastic sequence models;connectionist temporal classification;sequence refining outputs;neural sequence sequence;recognition asr;correction neural sequence;end automatic speech;error correction neural;different speech recognition;recognition asr framework;neural sequence;quantum error correction;sequence models shown;mask ctc outperforms;code quantum;sequence models class;sequence refining;new code quantum;generates sequence refining;temporal classification;outputs connectionist temporal;language processing tasks;code quantum error", "pdf_keywords": "end speech recognition;autoregressive model speech;end automatic speech;model speech recognition;automatic speech;speech recognition asr;automatic speech recognition;speech recognition;model speech;speech recognition framework;autoregressive model trained;end end speech;end speech;speech frames;prediction nonautoregressive end;speech recognition attracted;speech recognition based;speech frames output;mask prediction nonautoregressive;recognition asr model;input speech frames;non autoregression transducers;recognition asr;autoregressive end end;autoregression transducers;autoregressive end;non autoregressive end;models machine translation;prediction nonautoregressive;non autoregressive models"}, "38ff6cf441050a1db10df85ac0771ccc88dea748": {"ta_keywords": "efficient peer review;peer review systems;peer review framework;peer review;conference peer review;peer review settings;day peer review;review systems;strategyproof efficient peer;review systems designed;efficient peer;method conference peer;conference peer;reviewers final rankings;review framework;reviewers;influence reviewers;peer;paper conflicted reviewer;reviewer;review settings;review framework based;strategyproof efficient paper;single channel radio;review;influence reviewers final;highly scalable approach;conflicted reviewer;radio;simple highly scalable", "pdf_keywords": "peer review algorithm;peer review mechanism;peer review;conference peer review;peer review based;e\ufb03cient peer review;impossible peer review;peer review process;setting reviewer ranks;peer review prevalent;strategyproofness conference peer;reviewer ranks candidates;review algorithm divide;new peer review;reviewer ranks;review algorithm;unanimous weakly strategyproof;review process pairwise;voting theory social;unanimous weakly strategyproofness;review based notion;review mechanism;rank impossible peer;voting theory;strategyproofness majority;impart strategyproofness conference;pairwise unanimous;e\ufb03ciency voting theory;strategyproofness majority classical;measure e\ufb03ciency voting"}, "97ca917f66d60f5277651a74f233804b03cb5e3d": {"ta_keywords": "predicting morpheme boundaries;approaches predicting morpheme;predicting morpheme;morpheme boundaries;morpheme boundaries paper;non neural approaches;existing non neural;neural approaches predicting;non neural;convolutional neural networks;deep convolutional neural;neural approaches;neural networks;neural;deep convolutional;convolutional neural;neural networks outperform;morpheme;dynamics;dynamics level;networks;convolutional;deep;approaches predicting;predicting;dynamics level coupled;boundaries;level coupled;level coupled environment;networks outperform", "pdf_keywords": ""}, "49989dc4d77b9df775b284ab7682ba76c080be12": {"ta_keywords": "hidden markov models;learning hidden markov;classifying deformable objects;implement texture classification;object classifiers;object classifiers based;texture classification;hidden markov;parts models classifying;hidden hidden markov;classifiers;models hmms;texture classification algorithm;machine learning hidden;class object classifiers;models hmm;star classifiers;object classified;models classifying deformable;classifying;noncausal hidden markov;classifying deformable;hmm implement texture;classification;classifiers based;classified star classifiers;markov models hmm;models classifying;markov models hmms;classifiers follow", "pdf_keywords": ""}, "51d735419392dbe961c60bff7eee95388b8d6d3d": {"ta_keywords": "unsupervised grammar induction;induced word clusters;paper unsupervised grammar;unsupervised grammar;unlabeled dependency trees;linguistic inference;induce labeled dependencies;trees gold speechtagged;linguistic inference based;grammar induction;supervision frequent words;word clusters;gold speechtagged text;grammar induction used;induce unlabeled dependency;automatically induced word;approach linguistic inference;words cluster;speechtagged text;dependency trees gold;labeled dependencies automatically;dependency trees;gold speechtagged;labeled dependencies;induce labeled;linguistic classes;analysis linguistic classes;speechtagged;frequent words cluster;linguistic classes general", "pdf_keywords": ""}, "b26ca2bb882c2d3526fb4ac7f544fb87c39ded62": {"ta_keywords": "recognition images speech;gradient matching pursuit;kernel gradient matching;matching pursuit method;method automatic recognition;images speech;automatic recognition;automatic recognition images;images speech proposed;recognition images;gradient matching;speech proposed method;conventional kernel gradient;matching pursuit;pursuit method approximates;recognition;accuracy conventional kernel;pursuit method;kernel gradient;conventional kernel;optimal parameter vector;kernel;basis vectors;state art methods;pursuit;method approximates optimal;speech;proposed method outperforms;number basis vectors;approximates optimal", "pdf_keywords": ""}, "2cd7c3ed5a06c461b259694376820dcfcfbe94a9": {"ta_keywords": "constituency parsing inference;constituency parsing;task constituency parsing;parsing inference;parsers;external parsers;treebank surpassing prior;treebank;external parsers alternative;parsers alternative;parsers alternative conventional;treebank surpassing;output external parsers;discriminative neural models;parsing;parsing inference procedure;decode directly generative;penn treebank;penn treebank surpassing;generative neural models;directly generative models;generative neural;neural models recently;discriminative neural;directly generative;search space generative;23 penn treebank;used discriminative neural;neural models;generative models", "pdf_keywords": "search neural generative;generative parser;generative constituency parsers;generative parser achieves;contained generative parser;generative neural pruning;decode directly generative;loss probabilistic parsing;generative neural;neural generative;beam search neural;generative neural model;predicting word sentence;search neural;inducing modi\ufb01cation generative;directly generative;neural generative constituency;directly generative models;discriminative neural models;probabilistic parsing language;parsers lexical actions;generative models;parsing language modeling;probabilistic parsing;present generative neural;generative;training corpus;discriminative neural;generative model choe;constituency parsers lexical"}, "19a3af37df22c7c646cc99efad3af96cda6e80f0": {"ta_keywords": "translation trained attentional;neural machine translation;machine translation nmt;machine translation;machine translation task;wwmd machine translation;translation nmt model;networks help translation;translation trained;translation nmt;attentional encoder;multimodal nearest;language descriptions images;translation task;multimodal nmt model;multimodal nearest neighbor;attentional encoder decoder;trained attentional encoder;study multimodal nearest;multimodal nmt;present multimodal nmt;hiero translation trained;decoder neural machine;multimodal;nearest neighbor nnn;translation task 2018;nnn nearest neighbor;trained attentional;present multimodal;neural machine", "pdf_keywords": ""}, "51546584aa394d159edcc08f2412ae30dd316f6c": {"ta_keywords": "inside deep models;prediction depth;effective prediction depth;deep models;prediction depth fluid;deep learning;deep models showcase;faster networks learn;data point deep;prediction depth given;deep learning employs;model speed learning;allows improve prediction;speed learning;deep;input effective prediction;relationships prediction depth;depth fluid dynamics;networks learn easy;networks learn;point deep learning;improve prediction;accuracy early layers;point deep;speed learning data;depth fluid;converge faster networks;improve prediction accuracy;effective prediction;prediction accuracy early", "pdf_keywords": "adversarial input margin;input margin adversarial;prediction depth;margin adversarial input;margin adversarial;prediction depth consistent;adversarial input;prediction depth used;deep networks predict;adversarial;prediction depth given;prediction depth function;algorithms prediction depth;networks predict input;entropy predictions predictions;improve prediction accuracy;predictions early layers;points prediction depth;improve prediction;prediction accuracy;predict input margin;entropy predictions;prediction accuracy work;function entropy predictions;predictions predictions early;train deep networks;relationships prediction depth;predictions predictions;robust algorithms prediction;case prediction depth"}, "ca73cc17ca69fa0807e566c22c7c1711da916281": {"ta_keywords": "approximate nearest neighbor;nearest neighbor search;search nearest neighbor;nearest neighbor node;nearest neighbor;neighbor search;similarity searching vast;similarity searching;spaces similarity searching;search nearest;search high dimensional;neighbor node;neighbor node graph;structures approximate nearest;neighbor search high;approximate nearest;approximate search methods;searching vast range;approximate search;approach search nearest;nonmetric spaces similarity;data structures approximate;data sets approximate;sets approximate search;efficient high dimensional;estimating number tornadoes;nearest;tornado provide comparison;similarity;estimating size tornado", "pdf_keywords": ""}, "56501a3441c2074bbbbe31015d6d41c57d9d285b": {"ta_keywords": "paraphrastic sentence representations;paraphrastic sentence models;sentence models parallel;semantic textual similarity;monolingual semantic similarity;unsupervised semantic textual;training paraphrastic;training paraphrastic sentence;textual similarity significantly;sentence representations;sentence representations variety;sentence models;semantic similarity;bert based models;semantic similarity cross;state art paraphrastic;textual similarity;lingual semantic similarities;unsupervised semantic;method training paraphrastic;models like sentencebert;semantic textual;paraphrastic;semantic similarities;similarity cross lingual;paraphrastic sentence;like sentencebert reimers;semantic similarities bitext;outperforming bert based;like sentencebert", "pdf_keywords": "embeddings language paraphrase;paraphrastic sentence models;paraphrastic sentence representations;paraphrastic sentence embeddings;learning paraphrastic;learning paraphrastic sentence;learning inference paraphrastic;mining learning paraphrastic;training paraphrastic sentence;training paraphrastic;lingual semantic similarity;state art paraphrastic;inference paraphrastic;english semantic similarity;language paraphrase bilingual;paraphrase bilingual parallel;sentence embeddings translated;sentence embeddings language;inference paraphrastic sentence;paraphrase bilingual;similarity cross lingual;sentence embeddings;semantic similarity cross;paraphrastic;bases training paraphrastic;paraphrastic sentence;semantic similarity;unsupervised english semantic;cross lingual semantic;sentence models"}, "ce458be308f2c75edc53366272fa6e744fda7902": {"ta_keywords": "word sense disambiguation;unsupervised word sense;sense disambiguation;lexical semantic;lexical semantic resources;different lexical semantic;semantic similarity;lexical;sentence synset;word sense;semantic;different lexical;sentence architecture;disambiguation;semantic similarity given;sentence synset constituting;sense target word;sentence architecture present;unsupervised word;sense sentence;new unsupervised word;evaluation different lexical;sentences able relevant;operation semantic similarity;semantic resources present;sense sentence mogoznal;given sentence synset;semantic resources;sentences able;synset constituting sense", "pdf_keywords": ""}, "9195186cf44876d0d1d03b87756c464b760a7f4e": {"ta_keywords": "segmentation audio data;segmentation audio;audio segmentation;method segmentation audio;audio segmentation using;improved audio segmentation;segmentation using pyannote;pyannote audio toolkit;segmentation;pyannote audio;context modeling sequence;new method segmentation;using pyannote audio;audio toolkit;long context modeling;audio data;segments long context;sequence level information;method segmentation;audio toolkit merging;segmentation using;sequence level knowledge;tool distinguish sequence;modeling sequence level;context modeling;segments;multiple short segments;short segments;level information distillation;level knowledge distillation", "pdf_keywords": "machine translation;new machine translation;machine translation framework;long context modeling;machine translation ted;improvements translation performance;domain machine translation;large improvements translation;machine translation rare;translation performance;evaluation machine translation;translation framework;context modeling;context modeling self;context modeling propose;translation framework 2016;improvements translation;multi decoder architecture;multi decoder;decoder architecture transferring;sequence level knowledge;architecture multi decoder;decoder;decoder architecture;encoder;algorithm long context;teacher model;level knowledge distillation;multi multi decoder;translation ted domain"}, "4fd6488e38043d680c592170bf7f651c079d0e98": {"ta_keywords": "handoff cellular networks;mean throughput mobile;users handoff cellular;speed mobile users;throughput mobile static;handoff cellular;throughput mobile;poisson process mobile;users heterogeneous wireless;network speed mobile;cellular networks ubiquitous;mobile users handoff;cellular networks;downlink data rate;static mobile users;stations mobile users;heterogeneous wireless network;outage mobile users;average downlink data;mean throughput;data outage mobile;heterogeneous wireless;mobile users high;speed mobile;mobile static users;homogeneous network speed;optimize mean throughput;average downlink;outage mobile;wireless network jointly", "pdf_keywords": "mobile base stations;heterogeneous cellular networks;mobile base station;base stations mobile;cellular networks;base station networks;mobile users handoff;cellular networks combat;mobile base;stations mobile;stations mobile moving;handoff 2gps;planning heterogeneous cellular;static mobile users;throughput loss handoff;station networks based;wireless networks alleviate;performance mobile users;station networks;model mobile base;users generation wireless;macro base stations;static mobile;loss handoff 2gps;networks combat throughput;outage mobile users;users handoff analytical;optimization framework mobile;cellular;heterogeneous tier network"}, "4ab7b65e1a3b76eb3db064523c862f1325e04971": {"ta_keywords": "recognition systems speech;speech deficit similar;dependence speech deficit;speech deficit;speech recognition;human brain parkinson;automatic speech;automatic speech recognition;brain parkinson;systems speech;speech people pd;differences speech recognition;speech recognition systems;parkinson;speech recognition patterns;systems speech people;brain parkinson disease;hybrid learns recognize;parkinson disease;spoken language functional;learns recognize;functional dependence speech;deficit similar human;motor capabilities patients;hybrid learns;parkinson disease psd;spoken language;speech;speech people;art automatic speech", "pdf_keywords": ""}, "3f79b71b887d2ccb733926867a62f69902fcbdab": {"ta_keywords": "similarity ontologies vector;adaptive ontology mapping;adaptive ontology;structural similarity ontologies;similarity ontologies;ontologies vector;generic adaptive ontology;ontology mapping;ontologies vector space;constraint satisfaction solver;ontology mapping approach;ontologies;ontology;aggregation methods;based constraint satisfaction;adaptiveaggregation outperforms aggregation;aggregation;outperforms aggregation methods;satisfaction solver;aggregation methods paper;constraint satisfaction;information retrieval;linguistic structural similarity;retrieval techniques arti\ufb01cial;space model aggregate;structural similarity;retrieval techniques;outperforms aggregation;measure linguistic structural;aggregate themusing adaptive", "pdf_keywords": ""}, "7954b31ce1f6ad935808b7cf62c34bc118d20a9a": {"ta_keywords": "decision maker disagreement;empirical objective generalization;decision making competition;inter decision maker;decision maker large;maximizing empirical objective;decision maker;decisions faced context;probabilistic model decision;objective generalization;making decision maker;healthcare datasets recovering;maximizing empirical;competition decision making;effective decision;datasets recovering variation;healthcare datasets;decision present algorithm;effective decision making;causal effect decision;high inter decision;region maximizing empirical;probabilistic;model decision making;competition decision;make different decisions;model decision;objective generalization bound;decisions;drug related crimes", "pdf_keywords": "prediction outcome diagnosis;diagnosis decision tree;formalize causal inference;prediction outcome;outcome diagnosis decision;diagnosis decision;outcome region models;prediction prediction outcome;method predicting region;method predicting;diabetes examine treatment;causal e\ufb00ect decision;decision tree formalize;decision tree;outcome diagnosis;tree formalize causal;models apply metformin;diabetes;diabetes examine;supervised;predicting region;learning algorithm prediction;supervised learning algorithm;predicting;formalize causal;supervised learning;diagnosis gestational diabetes;propose method predicting;present supervised;type diabetes"}, "4bf5084d21f681c09409bd890daa4bf1c4f9b691": {"ta_keywords": "platelet reactivity;study platelet reactivity;using antiplatelet drug;antiplatelet drug administration;antiplatelet drug;platelet reactivity hpr;bose einstein condensate;phenomenon platelet function;platelet function;bec dilute bose;platelet function testing;measured using antiplatelet;using antiplatelet;bose gas confined;dilute bose gas;antiplatelet;study platelet;phenomenon platelet;platelet;dilute bose;retrospective study platelet;known phenomenon platelet;formation bose einstein;bose einstein;bose gas;formation bose;model formation bose;einstein condensate bec;periprocedural myocardial infarction;condensate bec dilute", "pdf_keywords": ""}, "c3490ec9b8f695bed2187fb4a4164b1509389ca8": {"ta_keywords": "oscillator coupled heat;dynamics mechanical oscillator;mechanical oscillator coupled;mechanical oscillator;oscillator coupled;coupled heat bath;oscillator;coupled heat;dynamics mechanical;dynamics;heat bath;theoretical investigation dynamics;investigation dynamics mechanical;heat;mechanical;bath;results theoretical investigation;theoretical investigation;present results theoretical;results theoretical;coupled;theoretical;investigation dynamics;present;article present;article present results;present results;article;results;investigation", "pdf_keywords": ""}, "7d94d4c6b2db490e08beabd2661df009f1a06d6c": {"ta_keywords": "thesaurus crowdsourcing;thesaurus crowdsourcing applications;like thesaurus crowdsourcing;create noun synsets;crowdsourcing applications present;book like thesaurus;crowdsourcing;crowdsourcing applications;noun synsets;open word book;noun synsets paper;thesaurus;kk words synsets;like thesaurus;words synsets;generating kk words;create noun;word book;open word;project create noun;word book like;kk words;noun;word;large open word;words;project learning;resource generating;synsets;synsets paper", "pdf_keywords": ""}, "02a757548da783d43ffcfd4b60f2cbb0ac71a4bc": {"ta_keywords": "fairness elicitation fairness;notion individual fairness;fairness elicitation;fairness constraints elicited;fairness metric task;subject fairness constraints;human subject fairness;framework fairness elicitation;fairness indirectly specified;fairness metric framework;fairness constraints;subjective fairness;individual fairness;introduce fairness metric;fairness introduce fairness;elicitation fairness indirectly;elicitation fairness;study subjective fairness;completely specified fairness;subjective fairness using;individual fairness proposed;specified fairness;fairness metric;subjective notions fairness;fairness revisit notion;fairness indirectly;fairness using human;notions fairness;specified fairness metric;fairness proposed", "pdf_keywords": "learning fairness constrained;metric fair learning;fairness constrained learning;learning metric fair;learning fairness;heuristic learning fairness;learning fairness speci;fair learning fairness;fairness constraints;fairness constrained;fair learning;study subjective fairness;fairness loss net;subjective fairness;fairness constraints particular;subjective fairness decision;problem metric learning;face fairness constraints;fairness decision making;unconstrained learning;harder unconstrained learning;unconstrained learning giving;fairness loss;metric learning goal;respect fairness loss;constrained learning;metric fair;learning metric;methods automate moral;metric learning"}, "f7247fefc9efb57ace33425a2981d6aba08da3b7": {"ta_keywords": "statistical dialogue framework;statistical dialogue management;statistical dialogue manager;dialogue manager statistical;dialogue management using;dialogue management;based statistical dialogue;conventional dialogue manager;annotated dialogue data;manager statistical dialogue;knowledge annotated dialogue;dialogue data transition;statistical dialogue;dialogue framework;dialogue data;dialogue manager;method statistical dialogue;dialogue evaluation;annotated dialogue;context dialogue evaluation;dialogue manager utilizes;dialogue evaluation present;conventional dialogue;dialogue;context dialogue;intention dependency graph;learning policy functions;converts conventional dialogue;markov decision process;functions context dialogue", "pdf_keywords": ""}, "23e42bc79f10234bdceef31441be39a2d9d2a9a0": {"ta_keywords": "rules knowledge base;knowledge base reasoning;logical rules knowledge;learning order logical;reasoning design neural;knowledge base;rules knowledge;knowledge base benchmark;tensor log inference;logic called tensor;differentiable logic;model differentiable learning;multiple knowledge base;developed differentiable logic;base reasoning design;differentiable learning lighthill;parameter structure learning;learning probabilistic;log inference tasks;differentiable logic called;learning probabilistic order;differentiable learning;base reasoning;requires learning parameters;probabilistic order logical;learning parameters;inference tasks;problem learning probabilistic;structure learning;order logical rules", "pdf_keywords": ""}, "06064617f152f5032137204aec739c0c82dbb836": {"ta_keywords": "challenge recognition topological;recognition topological;automatic speech recognition;speech recognition;recognition topological defects;speech recognition asr;speech recognition task;learning noisy;automatic speech;recognition asr challenging;distant microphone automatic;outcome speech recognition;recognition asr;microphone automatic speech;learning noisy noisy;computer aided recognition;predicting outcome speech;matter distant microphone;distant microphone;recognition task;recognition;noisy data;noisy noisy data;aided recognition aca;human brain predicting;aided recognition;brain predicting;microphone automatic;recognition task task;propose new probabilistic", "pdf_keywords": ""}, "14047a24b23d9e392776229f9d40bee9f8243e4c": {"ta_keywords": "energy efficient tracking;tracking motion particle;complexity dynamic sensor;efficient tracking;sensor activation tracking;gibbs sampling sensor;tracking motion;efficient tracking mechanism;dynamic sensor;sensor networks;sensor subset selection;dynamic sensor activation;sensor selection;sampling sensor subset;sensor subset;sensor networks cyberphysical;active sensor selection;method tracking motion;sensor selection problem;stochastic approximation learning;stochastic approximation;selection stochastic approximation;tracking;active sensors numerical;tracking time varying;frame motion particle;motion particle;tracking mechanism;sensors numerical;3d particle", "pdf_keywords": ""}, "0dd1b9ad5aeda250dc61f38cf7018e7a014e91c0": {"ta_keywords": "safety motor pool;traffic accident safety;impact traffic accident;accident safety;traffic accident;motor pool;accident safety motor;pool;safety motor;accident;impact traffic;safety;study impact traffic;traffic;motor;impact;paper present comprehensive;comprehensive study impact;present comprehensive;study impact;comprehensive;present comprehensive study;comprehensive study;study;paper;paper present;present", "pdf_keywords": ""}, "a67face220a88b6b36f3343a6a017a3536562d5b": {"ta_keywords": "speed guessing games;benefit guessing games;guessing games;neural networks presence;guessing games later;visual question answering;neural;study neural networks;guessing games prototypical;agents benefit guessing;artificial agents benefit;object representations;neural networks used;neural networks;experience learning;study neural;artificial agents;games prototypical instance;object representations previously;iterated experience learning;accuracy speed guessing;neural networks neural;answering self play;grained object representations;learning;networks neural;speed guessing;investigates artificial agents;question answering self;networks neural networks", "pdf_keywords": "dialogue recognition trained;encoder dialogue recognition;modal encoder dialogue;dialogue recognition;encoder dialogue;question guessing games;trained dataset conversations;modal dialogue;guessing games;multi modal dialogue;produce effective dialogues;modal dialogue work;dialogue;dialogue work;model trained guesswhat;dialogues;recognition trained;games gs ski;effective dialogues;guessing games specici;tasks guesswhat visual;trained guesswhat spiel;games generator;dataset conversations experience;dialogues use;dialogues use self;experience buffer dialogues;key gameplay performance;trained guesswhat;modal encoder"}, "970383c0a41d7ae1ec4b8abaa3033778203377b9": {"ta_keywords": "neural qa systems;neural qa;downstream neural qa;models synthetic corpus;noisy sentences;000 noisy sentences;question answering;question answering tasks;automatic speech;inputs machine translation;noise automatic speech;factoid question answering;synthetic corpus;noisy sentences evaluate;character recognition speech;recognition speech;recognition systems factoid;speech recognition create;unknown words empirically;synthetic corpus 500;speech recognition;speech recognition systems;decoding unknown words;language processing;automatic speech recognition;human corpora quizbowl;words empirically shown;machine translation;accuracy downstream neural;answering tasks fig", "pdf_keywords": "question answering models;machine translation answering;question answering;answering models;answering question answering;answering models propose;question answering questions;factoidquestion answering tasks;scale question answering;automatic speech;models synthetic corpus;factoidquestion answering;translation answering;approach automatic speech;systems factoidquestion answering;translation answering question;predict answer question;machine translation;answering tasks;new machine translation;synthetic corpus;answer question automatic;answering questions;questions immediately quizbowl;answering question;corpora quizbowl jeopardy;human corpora quizbowl;000 noisy sentences;synthetic data quizbowl;corpora quizbowl"}, "3193766c0439ff29a0a3d176628f8144d6e77231": {"ta_keywords": "walk social network;random walk social;outcome random walk;random walk;walk social;social network;probabilistic model predicting;propose probabilistic model;predicting outcome random;paper propose probabilistic;probabilistic;probabilistic model;walk;propose probabilistic;predicting outcome;predicting;model predicting outcome;outcome random;social;model predicting;random;network;outcome;model;paper propose;paper;propose", "pdf_keywords": ""}, "b38ec68c8bab031138606a9b00e9d817be3e1d22": {"ta_keywords": "jointly modeling links;topic models;topic models improve;link modeling jointly;models topic models;entity link modeling;modeling links text;text entities linked;links text entities;link modeling;entity links text;entity links;publications annotated proteins;modeling links;links text corpora;annotated proteins;entities linked;entity entity links;analysis text entity;link text information;text entities;entity link;corpus abstracts;corpus abstracts scientific;model datasets protein;entity entity link;supplemented corpus abstracts;induced topics;inspecting induced topics;category prediction proteins", "pdf_keywords": ""}, "3f256b31d446015d8cd0f9f3996009cdf2034c5e": {"ta_keywords": "speech recognition asr;monolithic multilingual asr;speech recognition;automatic speech recognition;identification speech recognition;speech recognition present;recognition asr;multilingual asr;architecture recognize speech;multilingual asr language;recognize speech automatically;model recognize language;asr language independent;based speech recognition;language recognize speech;recognize language recognize;hybrid attention connectionist;neural network architecture;automatic speech;recognition asr significantly;joint language identification;language independent neural;connectionist temporal classification;speech recognition paper;monolithic multilingual;developing asr systems;attention connectionist temporal;recognize speech 10;presents monolithic multilingual;asr language", "pdf_keywords": ""}, "c56aced0f0c5cfebefadb530cb08d736c3ac5c05": {"ta_keywords": "code summaries retrieval;code generation summarization;code summary generation;supplement code generation;code generation;code summaries;code code summaries;datasets code generation;code natural language;developers code summary;code summaries written;source code code;mimic developers code;framework generation summarization;code description pairs;generation summarization large;code summary;relevant code summaries;code generation java;source code;generation summarization models;generation summarization;collections source code;source code demonstrate;retrieves relevant code;developers code;summaries retrieval database;summary generation;code description;summary generation behavior", "pdf_keywords": "code generation summarization;code summarization attracted;generation code summarization;source code summarization;code summarization tasks;code summarization;summarization programming languages;source code generation;generation summarization java;generation summarization programming;summarization java python;summarization programming;generation summarization intrinsically;code generation;code generation experiments;python code generation;generation summarization;code generation code;summarization tasks source;summarization java;datasets code generation;source code;repositories question answering;usable source code;summarization attracted lot;source code short;java python promising;code associated text;summarization tasks;summarization intrinsically"}, "a4ce6cd06bc73d81651f7888efa4337fd82a60f0": {"ta_keywords": "classifier based electroencephalography;based electroencephalography eeg;user brain waves;electroencephalography eeg;based electroencephalography;electroencephalography;eeg signal user;eeg signal;signal user brain;electroencephalography eeg signal;performance spoken dialog;communication brain waves;brain waves time;eeg;disturbs communication brain;brain waves;brain waves including;unknown word perception;words disturbs communication;spoken dialog;brain waves used;unknown words disturbs;word perception significantly;analyze performance spoken;communication brain;desynchronization erd features;classifier based;classifier;words disturbs;spoken dialog user", "pdf_keywords": ""}, "04b364d56995de2228cb1acfb320a935cbcf4440": {"ta_keywords": "weakly supervised segmentation;weakly supervised training;quality weakly supervised;weakly supervised;weakly supervised settings;rule semantic segmentation;standard semantic segmentation;robust trust region;trust committee weakly;committee weakly supervised;deteriorate weakly supervised;semantic segmentation;supervised segmentation;semantic segmentation method;results robust trust;regularized losses improving;semantic segmentation ssa;deep learning;approach deep;new robust trust;new approach deep;supervised segmentation presented;trust region approach1;regularized losses;robust trust;robust trust trust;approach deep learning;improve quality weakly;regularized;trust region", "pdf_keywords": "weakly supervised segmentation;semantic image segmentation;context weakly supervised;supervised semantic segmentation;semantic segmentation;weakly supervised;weaklysupervised computer vision;weakly supervised semantic;supervised segmentation;propose deep convolutional;semantic segmentation particular;deep convolutional;supervised segmentation scribbles;convolutional network;approach weakly supervised;segmentation;deep convolutional network;gradient descent;image segmentation;semantic image;region network training;context backpropagation;unsupervised weaklysupervised;robust loss;segmentation particular majority;regularized losses improving;rule context backpropagation;context backpropagation highlight;segmentation scribbles;common robust loss"}, "fa774368fcf51cc0fa1bfda59b6a606e163c64b1": {"ta_keywords": "counting constraints;formulate counting constraints;counting constraints mildly;highly symmetrical counting;symmetrical counting problems;symmetrical counting;indoor environment;constraints mildly heterogeneous;problems exploit symmetry;cluttered indoor environment;counting problems exploit;indoor environment work;constraints;mildly heterogeneous systems;cluttered indoor;formulate counting;subject cluttered indoor;indoor;counting problems;constraints mildly;systems linear inequalities;counting;exploit symmetry;heterogeneous systems linear;heterogeneous systems;paper formulate counting;correct controllers systems;controllers systems;exploit symmetry synthesize;model control motion", "pdf_keywords": "discrete time switched;cycles counting constraints;000 dimensional switched;switched represented linear;dimensional switched;trajectories satisfy counting;counting constraints;switched states;switched states input;counting constraints special;synthesizing controlled trajectories;switched represented;counting constraints type;satisfy counting constraints;dimensional switched 210;cycles counting;counting constraints given;controlled trajectories 10;time switched states;input switched represented;dynamics discrete time;reduce discrete counting;counting problem feasibility;instances counting constraints;control aggregate state;controlled trajectories;discrete time;pertaining cycles counting;synthesizing open loop;states input switched"}, "9abf14d4f89bf6c297e1bbd637cd54e1a0335e71": {"ta_keywords": "ocr compositor attribution;bibliographic;manual judgements bibliographers;bibliographers;bibliographic task;bibliographers accuracy;judgements bibliographers;attribution clustering pages;bibliographers accuracy 87;bibliographic task relies;judgements bibliographers accuracy;attributions agree manual;ocr compositor;page paper;attributions;page paper present;historical printed document;printed document;analysis orthographic;type bibliographic;attribution;attribution clustering;predicts attributions;type bibliographic task;orthographic;orthographic variation;analysis orthographic variation;document;printed page paper;pages historical printed", "pdf_keywords": "bibliographers compositor attribution;attribution incorporates textual;text bibliographic analyses;text bibliographic;text text bibliographic;words compositor attribution;bibliographers compositor;bibliographic analyses;attribution incorporates;compositor attribution incorporates;used bibliographers compositor;compositor attribution traditionally;bibliographic;bibliographic analyses present;incorporates textual;textual visual sources;textual;possible compositor attribution;attribution traditionally;compositor attribution;bibliographers;designed compositor attribution;automate extraction text;compositor attribution long;model compositor attribution;attribution relies;compositor attribution important;compositor attribution relies;attribution important areas;sources evidence traditionally"}, "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4": {"ta_keywords": "crowdsourced model tuning;graph soft prompts;crowdsourced model;crowds crowdsourced model;learned backpropagation tuned;model dynamics driven;prompts learned backpropagation;method crowds;pendulum prompt tuning;model tuning;large models;models costly;learned backpropagation;crowdsourced;force pendulum driven;pendulum driven;crowds crowdsourced;acting pendulum prompt;large models costly;relevant large models;models;graph soft;driven pendulum driven;models exceed billions;parameters method crowds;driven pendulum;prompt tuning competitive;model tuning end;pendulum prompt;pendulum driven force", "pdf_keywords": "adapting language models;sentinels language models;language understanding tasks;trained language model;unsupervised multitask learners;contextualized word representations;representations language models;adapting language;text sentinels language;language models widely;multitask learners;language models;automatically tuning deep;models unsupervised multitask;word representations language;pre trained language;trained language;model language models;language models unsupervised;simpli\ufb01cation adapting language;machine translation;unsupervised multitask;word representations;tokens text sentinels;tuning deep convolutional;machine translation language;representations language;language model;examples prompt tuning;task description canonical"}, "1a3fcb1e2a416cbc79a011f1a1916aa53f7a2a09": {"ta_keywords": "perceived meaning stick;describing body language;meaning stick figure;stick figures intent;stick figure represented;postures essential social;describing body;emotional states describing;verbs used body;stick figure;dramatic action verbs;body language;stick figures;body language gained;figures intent verbs;meaning body postures;comprehending meaning body;stick figure simple;stick figure stable;dictionary stick figure;stable stick figure;emotion;figure stable stick;body postures;called stick figures;role emotional states;postures;meaning body;postures suggests;emotional states", "pdf_keywords": ""}, "e63c9eb5b623baad0a7805e839e5d9fabad37fce": {"ta_keywords": "generating detailed explanations;human readable explanations;question answering;multi hop reasoning;automated question answering;question answering systems;explanations multi hop;hop reasoning;multi hop inference;hop inference;hop reasoning explanation;answers natural language;hop inference problems;answering systems increasingly;explanations answers;answering systems;readable explanations answers;individuals automated question;knowledge experiments;explanation graphintelligence;explanations multi;choose alternative explanations;graphintelligence;challenging multi hop;detailed world knowledge;world knowledge experiments;retrieve answers natural;readable explanations;information retrieval model;information retrieval", "pdf_keywords": ""}, "ab8174a1f1810c1122f90649276a552d2eb1ccd4": {"ta_keywords": "highest order segmentation;order segmentation;segmentation;lowest order loss;loss highest order;order loss highest;order loss;loss highest;lowest order;relationship lowest order;loss;highest order;lowest;examine relationship lowest;order;relationship lowest;highest;paper examine;examine;paper examine relationship;paper;relationship;examine relationship", "pdf_keywords": ""}, "7618c65685c98fa88526555ae3f62cd5645066ad": {"ta_keywords": "predicting relation entailment;task predicting relation;relation extraction;relation entailment allows;relation entailment;answering relation extraction;relation entailment construct;predicting relation;relation entail;relation extraction summarization;examine relation entailment;brownian motion collective;relation entailment existence;question answering relation;relation hierarchies;entailment construct dataset;existing wikidata relation;wikidata relation hierarchy;wikidata relation;relation hierarchies enabling;coupled brownian motions;relation hierarchy;entailment;construct relation hierarchies;entailment allows;entailment existence relation;task predicting;collective motion described;entailment construct;relation formally define", "pdf_keywords": ""}, "6e78e32481218e9391a88e6d0e30c0062ae71bec": {"ta_keywords": "gesture style transfer;gesture generation style;embeddings speaker gestures;gesture generation;speaker gestures;speech gestures;approach gesture generation;gesture generation provides;performing gesture style;speech gestures naturally;introduce speech gestures;study gesture generation;gesture style;speaker gestures end;style transfer;style transfer multiple;style preservation challenge;gestures naturally;gestures;style embeddings speaker;generation style transfer;performing gesture;style preserving;style transfer introduce;gestures end;approach gesture style;style preservation propose;style preservation;gestures naturally timed;challenge style preservation", "pdf_keywords": "learns gesture style;gesture style transfer;speech gesture generation;learns gesture;embeddings speaker gestures;gesture generation style;gesture generation;gestures meaningful speech;speech gesture;approach learns gesture;gesture transfer;gesture style;speaker gestures;approach gesture transfer;gesture generation setting;speakers speech gesture;style transfer based;gesture transfer based;gestures;naturalistic looking gestures;study gesture generation;embodied virtual assistants;style transfer;generation style transfer;gestures meaningful;speaker gestures end;looking gestures;style transfer multiple;gestures end;style embeddings speaker"}, "f3bca263a92b69c6da872a9a3268f260ba43f690": {"ta_keywords": "discriminative rnn lnn;language model rnn;discriminative rnn;propose discriminative rnn;model automatic speech;speech recognition tasks;speech recognition task;continuous speech recognition;speech recognition;discriminative training;neural network language;training method rnn;model rnn;recurrent neural network;vocabulary continuous speech;method improves rnn;gram language model;improves rnn lm;method rnn lm;automatic speech;automatic speech recognition;rnn lnn;discriminative training method;model rnn recently;lm baseline recurrent;rnn lm baseline;rnn lm;method rnn;rnn lnn moves;baseline recurrent neural", "pdf_keywords": ""}, "53880036fb85cc737103c480c613e1912c416010": {"ta_keywords": "extract job postings;wrapper learning systems;job postings;wrapper induction extracting;wrapper learning;structured wrapper induction;extract job;allows wrapper learning;machine learning;real work learning;able extract job;induction extracting information;wrapper induction;extracting information;work learning;learning able extract;describes machine learning;job postings 500;structured wrapper;induction extracting;extracting information semi;machine learning able;master learning algorithm;learning algorithm;single master learning;learning systems;learning systems easily;wrapper;extracting;semi structured documents", "pdf_keywords": ""}, "a2f4731258830c76af7e3bdb96c4488823219585": {"ta_keywords": "frequency masking reverberant;time frequency masking;frequency masking;frequency masking method;masking reverberant environment;blind source separation;frequency masking apply;masking reverberant;stereo input speech;robustly noisy reverberant;robust automatic speech;speech recognition asr;signal robustly noisy;noise signal stereo;input speech recognition;separate noise signal;noise robust automatic;speech recognition;noise robust;asr using sparseness;present noise robust;masking method separate;underdetermined blind source;masking method;noisy reverberant environment;recognition using sparseness;method noisy reverberant;masking apply time;automatic speech recognition;signal robustly", "pdf_keywords": ""}, "341c72f55a89572915aa476db7f525c2e0b60eba": {"ta_keywords": "cluster like voxel;propose similarity measure;similarity measure based;similarity measure;like voxel patterns;voxel patterns;clustering;occurrence cluster;occurrence cluster like;clustering choosing;clustering choosing best;criterion occurrence cluster;clustering algorithms;propose similarity;cluster;similarity;cluster like;like voxel;evaluate clustering choosing;clustering algorithms used;evaluate clustering;voxel;satisfies requirements clustering;used evaluate clustering;requirements clustering algorithms;requirements clustering;correlation;importance analysis online;analysis online;algorithms", "pdf_keywords": "analyze similarity indices;similarity indices;detection clustering indices;clustering indices widely;clustering indices;measure similarity graphs;similarity indices satisfy;metric graph similarity;indices satisfy clustering;graph similarity mirkin;similarity mirkin metric;information theoretic indices;graph similarity;graph clustering;graph clustering propose;formally analyze similarity;similarity graphs;graphs graph clustering;analyze similarity;measure similarity;similarity graphs graph;cluster evaluation;pair counting indices;indices widely;cluster evaluation paper;indices widely used;used measure similarity;counting indices used;counting indices paper;clustering used various"}, "143183584a8ebaad93490f4550295a9cb6cf9817": {"ta_keywords": "scalable probabilistic logics;scalable probabilistic logic;probabilistic logic programming;probabilistic logics;introduction probabilistic logics;logic probabilistic;probabilistic logic;probabilistic logics statistical;probabilistic logics paper;probabilistic order logics;statistical relational learning;order logic probabilistic;logic probabilistic order;relational learning srl;logic machine learning;logics statistical relational;relational learning;probabilistic inference provide;advances scalable probabilistic;probabilistic inference;scalable probabilistic;inference order logic;statistical relational;logics statistical;logic programming;semantic parsing;learning methods probabilistic;gentle introduction probabilistic;semantic parsing information;classification semantic parsing", "pdf_keywords": ""}, "fc78af26fd7644867af1abb8fbf2c37b47ad8257": {"ta_keywords": "embedding word embeddings;embedding word embedding;word embedding;word embeddings;learning english embedding;english embedding space;english embedding;downstream lexicon induction;word embeddings general;embeddings languages distant;context embedding word;lexicon induction;embedding word;word embedding larger;embeddings languages;embedding larger context;learning multilingual;approach embedding word;multilingual learning;aligning embeddings languages;language induction;context efficient embedding;downstream lexicon;lexicon induction zero;impact downstream lexicon;language induction evaluation;languages language induction;embedding simpler context;context embedding;embeddings general context", "pdf_keywords": "multilingual word embeddings;crosslingual word embeddings;machine translation multilingual;embeddings machine translation;dictionaries multilingual;dictionaries multilingual word;creating dictionaries multilingual;unsupervised machine translation;word embeddings machine;training dictionaries languages;translation multilingual;multilingual word;translation machine;machine translation;dictionaries pivot language;multilingual;machine translation machine;use crosslingual word;crosslingual word;word embeddings;use crosslingual;annotated training dictionaries;dictionaries languages;dictionaries languages present;training dictionaries;translation machine trained;arbitrary language pairs;language pairs;translation multilingual settings;morphologically rich languages"}, "682660c7a014e806b924fdf1a2a3d999a9ac13cf": {"ta_keywords": "decoder architectures summarization;parses improve summarization;summarization progress neural;abstractive summarization progress;architectures summarization performance;abstractive summarization;neural language generation;abstractive summarization using;abstract meaning representation;architectures summarization;summarization using abstract;extend abstractive summarization;summarization performance;improve summarization results;summarization performance later;neural encoder decoder;summarization progress;improve summarization;summarization results;neural encoder;demonstrate parser based;summarization;progress neural encoder;work abstractive summarization;representation amr neural;parser based;summarization results 10;parser;encoder decoder architectures;parses parses", "pdf_keywords": "abstractive summarization progress;abstractive summarization using;abstractive summarization;based abstractive summarization;summarization progress neural;automatically producing summary;work abstractive summarization;summarization progress;summary source document;producing summary source;summarization using;summarization using information;summarization;producing summary;source document summarized;document summarized;guide nlg;amr based abstractive;paraphrasing aggregating compressing;approach guide nlg;paraphrasing aggregating;guide nlg stage;neural encoder decoder;nlg stage amr;process paraphrasing aggregating;document process paraphrasing;neural encoder;decoder architectures;document summarized recent;summarized recent work"}, "9d698e034d83eedc05237e629eaad1c0c4e5bbb9": {"ta_keywords": "learnability recursive logic;learnable recursive clause;efficient learnability recursive;learnable recursive;learnability recursive;consisting learnable recursive;recursive logic programs;free recursive logic;clause learnable;nonrecursive clause learnable;efficient learnability;clause learnable additional;logic programs polynomial;boundary efficient learnability;recursive logic;recursive programs constrained;programs consisting learnable;imply pac learnability;queries classes recursive;pac learnability classes;recursive programs;determinate clause learnable;classes recursive programs;pac learnability;recursive clause constant;free recursive;learnability classes present;learnability classes;clause programs consisting;learnability", "pdf_keywords": "recursive logic programs;learning recursive logic;learnability recursive logic;inductive logic programming;recursive logic;automatic logic programming;recursive clause learned;logic programs;logic programming;logic programming ilp;learnability recursive;programs automatic logic;learning recursive;algorithm learning recursive;logic programs automatic;closed recursive clauses;logic programming problems;program consisting recursive;recursive clause constant;ary recursive clause;recursive base clause;automatic logic;boundary learnability recursive;training data recursive;inductive logic;recursive clauses paper;non recursive clause;recursive clauses;data recursive;able equivalence queries"}, "bfe6d67ed1c9119f91774e62fe0f4f328830526e": {"ta_keywords": "neural conversation models;training neural conversation;conversation models;conversation data model;persona based conversation;conversation data speakers;conversation agent challenging;speaker roles modeled;speaker specific conversation;conversation agent;based conversation agent;speakers building persona;neural conversation;conversation models leverages;conversation data;specific conversation data;speaker roles;traits speaking styles;speakers traits speaking;data pertaining speaker;speaking styles;speaker speaker roles;leverages conversation data;representing individual speakers;conversation;precisely speakers traits;multi task learning;speakers traits;based conversation;speaker specific", "pdf_keywords": "translation based conversation;neural conversation models;training neural conversation;speaker conversational data;conversation models;conversational data;bilingual machine translation;conversation data;machine translation;conversational model;conversation model;general conversational model;conversation models leverages;individual speaker conversational;conversation triples multi;conversation based nonconversation;leverages conversation data;conversation data speakers;neural conversation;dataset conversations;conversational conversations;conversation triples;speaker conversational;conversation model able;bilingual machine;model conversation based;based conversation model;3turn conversational conversations;dataset conversations users;performance bilingual machine"}, "ca879ec1c04b94de274954dfd09dddfde6cbb4f3": {"ta_keywords": "perceptual age singing;voice characteristics manipulating;control singing voice;voice timbre control;change singer perceptual;singing voice conversion;convert singing voice;singing voice age;singing voice timbre;age singing voice;singing voice characteristics;voice timbre arbitrary;method control singing;singer perceptual age;sing expressively controlling;possible convert singing;control singing;proposed voice timbre;voice timbre varieties;controlling prosody voice;voice age singer;singing voice;convert singing;gender age singers;voice conversion;age singer perceived;voice timbre;singer perceptual;voice characteristics;age singing", "pdf_keywords": ""}, "bd6c708a535af588d90025a0e6cf17407bf65434": {"ta_keywords": "deception detection models;reviews attributions;fake hotel reviews;interact deception detection;deception detection;reviews attributions claimed;bert machine learning;model fresh reviews;set reviews attributions;attributing predictions features;trained distinguish genuine;techniques attributing predictions;propose new classifier;classifier designed predict;predictions features deemed;classifier;machine learning classifier;learning classifier;learning classifier designed;reviews goal;predicted class;predictions features;crowdsourcing study;participants interact deception;outcome interaction learner;reviews paper report;new classifier;hotel reviews paper;classifier designed;based local explanations", "pdf_keywords": "evaluating model explanations;deception detection task;predict outcome conversation;model explanations text;explanations text classi\ufb01cation;model predictions explanations;deception detection;evaluate explanation generation;automatically predict;paradigm deception detection;approach automatically predict;predictions explanations change;model bert based;predictions explanations;explanation generation;ofwords model bert;automatically predict outcome;predictive model trained;explanations change real;model explanations;models trained;explanation generation schemes;bag ofwords model;explanations text;model predictions propose;learning models trained;fake provide participants;crowdsourcing;model predictions;crowdsourcing study"}, "bf7481685e63b85ef2586de3f6098f1a5fbe0e2d": {"ta_keywords": "sampling surface water;surface water sampling;water sampling;water sampling based;sampler organic contaminants;contaminants surface water;spot concentrations river;samplers ambient monitoring;active sampler organic;concentrations river used;concentrations river;surface water;sampler organic;situ surface water;detect presence herbicides;situ sampling surface;active samplers ambient;samplers ambient;sampling surface;application situ sampling;trace contaminants surface;river used detect;contaminants surface;free active sampler;active sampler;range organic contaminants;monitoring trace contaminants;surface water different;surface water highly;sampler", "pdf_keywords": ""}, "72579f6ce4a413585445c4ef8c8c2fa63ea1b8bc": {"ta_keywords": "stochastic perturbations privacy;perturbations obfuscate private;private data;differential privacy;stochastic perturbations obfuscate;perturbations privacy;obfuscate private data;obfuscate private;differential privacy criterion;private information raw;private information;satisfies differential privacy;privacy;user private information;perturbations privacy users;infer privacy;private data sent;privacy criterion paper;privacy user private;privacy criterion;method infer privacy;infer privacy user;privacy users inference;perturbations obfuscate;privacy users;privacy user;stochastically perturbed network;user private;novel stochastic optimization;private", "pdf_keywords": "cloak aims learning;trained mutual information;privacy reduction mutual;privacy;privacy reduction;information estimator cloak;task propose cloak;images using trained;deep neural network;problem privacy;deep neural;problem privacy reduction;dnn model recognition;using trained mutual;target classification;recognition;efficiently black box;novel deep neural;target classification task;learning representations;classification task propose;noisy representations estimated;neural network dnn;network dnn;approach problem privacy;cloak framework sifts;accuracy target classification;recognition head;target prediction task;prediction task"}, "80edd01d46228fac7ec0cd14aea1666253b28f4d": {"ta_keywords": "voter preferences computationally;heuristics voter ignores;heuristics voter;voting environments;voting environments presented;voting environment;voter preferences;look heuristics voter;approval voting environments;votes collective decision;voting scenarios;approval voting scenarios;information voter preferences;agents vote choose;voting scenarios people;structure voting environment;heuristics single winner;winner approval voting;information voting;ignores information voting;candidate structure voting;voting scenarios missing;information voting profiles;voting profiles;structure voting;voting environment affects;world voting scenarios;agents vote;uncertain approval voting;preferences computationally", "pdf_keywords": "voting scenarios behavioral;heuristic voters use;heuristic voters;votes affect heuristic;approval elections behavioral;elections behavioral experiment;strategic vote use;election approval voting;important voting process;experiment people vote;complex voting scenarios;approval voting;voting scenarios;voting process;approval voting voting;multiwinner approval voting;winner approval elections;elections behavioral;voting multi winner;use approval voting;multi winner approval;strategic vote;vote truthfully situations;possible heuristic voters;lead election approval;approval voting multi;winner approval;approval voting important;voting truthfully;voting voting truthfully"}, "1da3a9c194a01c0bff7b6ecda79db9d673810bee": {"ta_keywords": "recurrent models transformer;transform recurrent models;recurrent neural networks;outperform recurrent models;transform recurrent;outperform recurrent neural;models outperform recurrent;recurrent models;recurrent neural network;recurrent models recurrent;approach transform recurrent;models recurrent;models recurrent models;outperform recurrent;recurrent neural;paper recurrent neural;transliteration paper recurrent;sequence models batch;performance morphological inflection;recurrent models paper;does outperform recurrent;recurrent sequence sequence;sequence sequence models;contrast recurrent sequence;sequence models outperform;transformer character;character level transduction;recurrent sequence;performance transformer character;recurrent", "pdf_keywords": "recurrent models transformer;language level transduction;transduction tasks grapheme;outperform recurrent models;transduction tasks character;phoneme conversion transliteration;text normalization novel;outperform recurrent;baseline transduction tasks;morphological level transduction;transduction models trained;model character morphological;character morphological level;conversion transliteration;historical text normalization;transliteration;recurrent models;text normalization;transduction tasks;character morphological;tasks character language;sequence models batch;does outperform recurrent;level transduction tasks;conversion transliteration paper;transliteration paper;normalization novel;character language;strong baseline transduction;grapheme phoneme conversion"}, "28e81f96eab94e99febcaaee00637825c8a3e664": {"ta_keywords": "machine learning process;machine learn;machine learning learn;machine learning;machine learn reason;approach machine learning;set machine learning;leverages machine learning;ability machine learn;learn given data;learning process;machine learning leverages;novel approach machine;learning leverages machine;learning;learning learn given;approach machine;data set machine;learn reason harnessed;learn reason;machine;learning learn;learn given;data set;data;set machine;ability machine;learn;process ability machine;society changing technology", "pdf_keywords": ""}, "faf494d0aa25a17aa25930ffb4c750fa59c44849": {"ta_keywords": "unsupervised tts embeddings;speaker embedding networks;tts speaker embedding;representations speaker verification;tts embeddings significantly;speaker embedding;tts embeddings;corresponding speaker embedding;tts embeddings work;embeddings combined speaker;speakertext speech tts;computes representations speaker;speaker encoder computes;speaker verification task;speaker encoder;speaker verification;embeddings significantly outperform;speech tts;representations speaker;embedding networks;speaker classification;speaker classification enhance;unsupervised tts;speech tts generates;embedding networks self;speakertext speech;embeddings significantly;speakertext;speaker embedding paper;text corresponding speaker", "pdf_keywords": "learns speaker representation;encoder learning speaker;learning speaker embedding;speaker embedding vector;embedding text speech;end speech recognition;speaker encoder learn;speaker encoder learns;encoder learns speaker;speaker recognition learns;unsupervised learning speaker;speaker embedding text;speaker representation robust;speaker embedding;speaker representation;help speaker encoder;speaker encoder;better embedding speaker;learning synthesized speech;embedding speaker;new speaker recognition;speaker recognition;learning speaker;synthesized reference speech;representation robust phonetic;hypothesize speaker encoder;learns speaker;embedding speaker speaker;synthesized speech;speech synthesized reference"}, "7efb1788b5e0fa3b4d9932722286ba1753b42f91": {"ta_keywords": "effective descriptiondriven dialog;simple effective descriptiondriven;descriptiondriven dialog state;language description driven;dialog state tracking;effective descriptiondriven;descriptiondriven dialog;schemata language description;information conversations completion;conversations completion;contained taskspecific ontology;taskspecific ontology schemata;description driven;purely schema descriptions;taskspecific ontology;conversations completion given;descriptiondriven;schema descriptions index;schema descriptions;convey semantics effectively;key information conversations;dialog state;task specifications higher;understanding task specifications;convey semantics;descriptions index picking;task specifications;schemata language;developers convey semantics;ontology schemata language", "pdf_keywords": "dialogue modelling;conversational agents task;domain conversational agents;task oriented dialogue;oriented dialogue modelling;effective descriptiondriven dialog;multi domain conversational;dialogue modelling paper;descriptiondriven dialog state;language description driven;dialog state tracking;domain conversational;conversational agents;descriptiondriven dialog;dialog state;natural language descriptions;simple effective descriptiondriven;conversational;oriented dialogue;effective descriptiondriven;dialog;language model capable;dialogue;description driven;language descriptions;language model;descriptiondriven;agents task oriented;answer question language;schema based language"}, "9688671a573651955c26d710c12617de26715e78": {"ta_keywords": "network storage code;distributed storage minimum;bound repair bandwidth;constructions regenerating codes;minimum storage regenerating;distributed storage systems;distributed storage;storage code;regenerating codes achieve;design distributed storage;storage code maximum;regenerating codes;bandwidth exact repair;network storage;storage regenerating;regenerating codes require;concept distributed storage;storage systems;minimum storage;repair bandwidth;storage minimum storage;new regenerating code;storage minimum;regenerating code;bound repair;repair bandwidth exact;regenerating code used;codes achieve cut;set bound repair;storage systems based", "pdf_keywords": "distributed storage codes;storage codes treatment;storage codes;regenerating codes achieve;regenerating codes achieving;architecture regenerating codes;regenerating codes;linear regenerating codes;regenerating codes does;implementation distributed storage;repair computer codes;storage nodes;consider distributed storage;distributed storage systems;storage systems;condition regenerating codes;distributed storage;regenerating code;new regenerating code;storage systems raid;regenerating code architecture;storage nodes having;systems raid oceanstore;code architecture regenerating;codes treatment;consists storage nodes;storage;storage considered;miser code capable;systems raid"}, "148efaba70165d9faef0dac28d5fa2538cfa662d": {"ta_keywords": "human ai collaboration;cognitive biases human;human artificial intelligence;ai collaboration assumptions;intelligence collaborative decision;biases human artificial;interplay cognitive biases;account cognitive biases;artificial intelligence collaborative;ai collaboration;cognitive biases;intelligence collaborative;human ai;cognitive biases confirmation;collaborative performance;optimal human ai;process team skimming;improve collaborative performance;collaborative decision;team skimming;collaborative decision making;biases human;human improve collaborative;bias availability bias;collaborative performance model;cognitive science;distorted cognitive biases;bias availability;cognitive science account;artificial intelligence", "pdf_keywords": "biases human ai;cognitive biases human;policies cognitive biases;cognitive biases;cognitive biases introduced;ai collaborative decision;cognitive biases con\ufb01dence;evaluate human ai;role cognitive biases;rationality judgment individuals;human ai collaborative;biases human;distorted cognitive biases;human ai team;behaviour collaborative decision;rationality judgment;ai collaborative;ai team performance;policies cognitive;human ai;decision making tasks;con\ufb01dence bias;accounting human behaviour;judgment individuals create;collaborative decision making;collaborative decision;biases con\ufb01dence bias;makers adjust decisions;baseline policies cognitive;cognitive tendencies decision"}, "a6219725a9ad2079536c091f02fda2d4da6d62ac": {"ta_keywords": "exact regenerating codes;regenerating codes;regenerates codes;storage repair bandwidth;regenerating codes meet;regenerating codes introduced;regenerating codes provided;reliability distributed storage;regenerates codes additional;erasure coding techniques;exact regenerating code;introduced regenerates codes;storage overhead code;code exact regenerating;erasure coding;scheme regenerating codes;regenerating codes addition;codes introduced regenerates;regenerating code;distributed storage systems;repair bandwidth distributed;distributed storage;regenerating code derived;codes addition erasure;bandwidth distributed storage;storage repair;addition erasure coding;regenerating node;systems minimizing storage;regenerating node failure", "pdf_keywords": ""}, "3c1001c04866647650216201feb54c927af3a05b": {"ta_keywords": "concept description language;language called concept;learning concept;concept description;called concept description;knowledge including constraints;concept learning;concept learning produces;knowledge based learning;learning concept learning;large concept description;describes learning;knowledge including;based learning concept;concept;called concept;knowledge based;incomplete theories theories;description language;theories incomplete theories;theories syntactically;paper describes learning;theories theories syntactically;knowledge;learning uniies problems;incomplete theories;types background knowledge;predicates used programming;approach learning uniies;eld knowledge based", "pdf_keywords": ""}, "60f0af1dbc2775a69f64e4351d969ac966659fb2": {"ta_keywords": "synset clusters graph;clustering synonymy graph;synonymy graph sparseness;graph based synset;global clustering synonymy;synset clusters;clustering synonymy;graph clustering;approach graph clustering;use synset clusters;graph clustering based;synset induction methods;clusters graph;clusters graph based;global clustering;graph sparseness;synset induction;graph sparseness substantially;based synset induction;performance synset induction;clustering;synonymy graph;performing global clustering;extracted synsets;input synonymy graph;quality extracted synsets;synsets;synset;synsets performing global;clusters", "pdf_keywords": "similarity synonymy dictionaries;synonymy dictionaries synset;semantic similarity synonymy;semantic similarity;synonym dictionaries widely;synonymy relation lexical;words synsets synonymy;synonym dictionaries;words synonymy relation;independent synonym dictionaries;synonymy dictionaries;synonymy synonymy dictionaries;similarity measures semantic;similarity synonymy;synonymy networks densely;synonymy networks;synonymy dictionaries dictionaries;dictionaries semantic relationships;knowledge based similarity;synonymy relation;synsets synonymy;words synonymy;word sense induction;synonymy graph correspond;methods synonymy networks;relation words synonymy;synonymy graph;input synonymy graph;semantic relationships;synonymy symmetric relation"}, "22616702da06431668022c649a017af9b333c530": {"ta_keywords": "automated fact checking;automated fact;survey automated fact;natural language processing;fact checking research;autoencoders natural language;fact checking;natural language;language processing;automating autoencoders natural;automating;unifying task formulations;checking research stemming;autoencoders natural;automating autoencoders;method automating autoencoders;language processing problem;research stemming;automated;language processing related;research stemming natural;paper survey automated;novel method automating;autoencoders;task formulations;statistical;survey automated;answer question probability;disciplines unifying task;method automating", "pdf_keywords": "automatic fact checking;automated fact checking;fact checking capable;fact checking task;developed fact checking;based fact checking;fact checking claims;fact checking veri\ufb01cation;fact checking work;fact checker;analysis automatic fact;natural language processing;fact checking;fact checker evaluate;automatic fact;survey automated fact;fact checking important;professionals fact checker;automated fact;learning based fact;processing nlp;claims expressed text;processing nlp related;consider fact checking;nlp requirements systems;text knowledge base;language processing nlp;natural language;industry automatic fact;checking claims expressed"}, "6b7f2f30840b0d72484784a15b3be670868a9f95": {"ta_keywords": "parsing crosslingual transfer;dependency parsing crosslingual;parsing crosslingual;tagging dependency parsing;speech pos tagging;interlingual latent embedding;languages especially annotated;lingual word embedding;syntactic analysis tools;tagging direct transfer;word embedding;discriminative models transfer;syntactic tasks speech;build syntactic analysis;dependency parsing;syntactic tasks;especially annotated;especially annotated target;syntactic analysis;crosslingual transfer;crosslingual transfer effective;improvement pos tagging;low resource languages;tagging dependency;cross lingual;parsing;cross lingual word;tagging direct;pos tagging direct;resource languages invertible", "pdf_keywords": "lingual dependency parsers;unsupervised dependency parsing;unsupervised cross lingual;embeddings learn interlingual;target language generative;interlingual latent embedding;language generative prior;crosslingual word embedding;inducing syntactic structure;lingual word embeddings;inducing syntactic;cross lingual dependency;language generative;dependency parsers;contextualized embeddings learn;lingual dependency;dependency parsing;novel unsupervised dependency;contextualized embeddings;monolingual setting supervision;syntactic structure monolingual;contextualized embeddings propose;word embeddings;word embeddings universal;advantage contextualized embeddings;technique inducing syntactic;word embedding;dependency parsers paper;features cross lingual;unsupervised dependency"}, "eebc1811c55c2e5e8b3b78d0b0382ad50f22e32a": {"ta_keywords": "adversarial fact verification;fact verification models;fact checking evidence;effective fact verification;fact verification model;fact checking;adversarial fact;verification natural language;checking evidence sources;10 adversarial fact;fact verification natural;evidence sources change;problem fact checking;evidence verify verification;supporting evidence training;evidence training;evidence training using;fact verification;natural language inference;written evidence verify;differences supporting evidence;supporting evidence;typical fact verification;claim evidence pairs;checking evidence;evidence verify;evidence sources;factual changes paper;000 claim evidence;official verification false", "pdf_keywords": "claims crowdsourced articles;leveraging factual;natural language inference;leveraging factual revisions;predicting truthfulness claims;method leveraging factual;factual revisions wikipedia;crowdsourced articles;crowdsourced articles present;automatic machine translation;truthfulness claims crowdsourced;claims crowdsourced;wikipedia create challenging;lexically similar factually;automatically predicting truthfulness;machine translation;language inference;language inference fact;wikipedia create;crowdsourced;dataset claims;machine translation important;revisions wikipedia create;tasks natural language;factual revisions;claims various topics;natural language;predicting truthfulness;answer question training;revisions wikipedia"}, "6dd1e4d97dbdb370a36c25f82a9a9baaa16c836c": {"ta_keywords": "coil virus glycoprotein;virus glycoprotein;virus glycoprotein presence;bacteriophage rabsocyanine glycoprotein;virus particles cell;virus particles oligomerization;vesicular stomatitis virus;transport virus particles;virus transport;stomatitis virus transport;virus transport virus;rabsocyanine glycoprotein;virus host cells;rabsocyanine glycoprotein gp;virus particles;viral oligomer ganglion;transport virus;coiled coil virus;oligomer ganglion protein;bacteriophage rabsocyanine;coil virus;ganglion protein;ganglion protein vggp;bacterium bacteriophage rabsocyanine;virus genome bacterium;virus genome;production virus particles;stomatitis virus;model vesicular stomatitis;viral oligomer", "pdf_keywords": ""}, "7e122cc1a62e2f30951e14b91811896e1866dd7c": {"ta_keywords": "symbolic music generation;symbolic music;trained spiking model;approach symbolic music;spiking model;antiferromagnetic afm order;pre trained spiking;trained spiking;long range antiferromagnetic;antiferromagnetic afm;spiking model investigate;music generation;range antiferromagnetic afm;antiferromagnetic;music generation based;range antiferromagnetic;music transformer;music transformer quality;spiking;performance music transformer;music;autoregressive models demonstrate;transformer quality sequences;autoregressive models;long sequences phenomenon;sequences models;performance music;learning algorithms;autoregressive;quality sequences models", "pdf_keywords": ""}, "c7af06170f3d81ab761873a4c1fe0af2736eb0a2": {"ta_keywords": "guessing accuracy conversational;accuracy conversational agents;predict ability dialogue;response affective interaction;dialogue systems;accuracy conversational;agents dialogue systems;affective interaction;conversational agents dialogue;emotion emotional responses;dialogue systems consider;emotional responses;emotional responses events;conversational agents;responses events elicit;action response affective;dialogue decide action;events elicit emotional;interaction paper emotion;elicit emotional triggers;emotional awareness interaction;automatic prediction;dialogue;conversational;emotion human cognition;dialogue decide;indonesian automatic prediction;incorporate emotion human;automatic prediction performance;analyze occurrence emotion", "pdf_keywords": ""}, "8dd3b88ac87372c9f4428029ac12288ff3405199": {"ta_keywords": "variability nlr atherosclerosis;lipid variability nlr;blood lipid variability;atherosclerosis variability;atherosclerosis variability blood;lipid levels atherosclerosis;progression atherosclerosis variability;lipid variability;variability blood lipids;nlr atherosclerosis associated;variability blood lipid;levels atherosclerosis studies;nlr atherosclerosis;levels atherosclerosis;blood lipid levels;atherosclerosis studies;lipid metabolism variability;blood lipids affects;atherosclerosis studies explored;atherosclerosis associated chronic;effects blood lipid;atherosclerosis associated;lipid levels;chronic inflammation lipid;cholesterol progression atherosclerosis;lipids affects neutrophil;atherosclerosis;variability hdl nlr;effect cholesterol;progression atherosclerosis", "pdf_keywords": ""}, "36b6abfb32ea56208a2858b558acbdd001c965e9": {"ta_keywords": "neural machine translation;machine translation generation;machine translation participants;translation generation;efficient machine translation;machine translation;translation generation held;bose einstein condensate;workshop neural machine;einstein condensate;second workshop neural;workshop neural;neural machine;einstein condensate confined;translation participants tasked;computational linguistics;translation participants;condensate confined harmonic;dynamics bose einstein;dynamics bose;bose einstein;computational linguistics article;condensate;investigation dynamics bose;neural;condensate confined;linguistics;computational;harmonic trap;translation", "pdf_keywords": "neural machine translation;translation generation;machine translation;machine translation important;machine translation generation;machine translation paper;translation generation pointing;noise neural machine;workshop neural machine;machine translation saw;neural mt;translation important task;noise neural;second workshop neural;neural machine;workshop neural;neural mt paper;structure domain adaptation;analysis models neural;models neural machine;ef\ufb01ciency neural mt;translation paper;domain adaptation;translation;adaptation data augmentation;knowledge neural;models neural;knowledge neural machine;types noise neural;neural"}, "47b6023808002dfde031c17b34dcb1b522d3b326": {"ta_keywords": "spring pendulum;spring spring pendulum;spring pendulum pendulum;pendulum pendulum period;pendulum period;dynamics spring spring;dynamics spring;pendulum period xmath0;study dynamics spring;pendulum pendulum;pendulum;pendulum pendulum pendulum;spring spring;spring;period xmath0;dynamics;period xmath0 days;xmath0 days;study dynamics;comprehensive study dynamics;xmath0;period;days;present;present results;comprehensive study;paper present results;paper present;present results comprehensive;study", "pdf_keywords": ""}, "743d1aae44a12fb37b743ec947fad41cba9831b8": {"ta_keywords": "text generation pragmatics;summarization generation structured;abstractive summarization generation;summarization generation;methods text generation;language generation tasks;generation pragmatics;model text generation;text generation;idea text generation;generation single text;text generation process;text generation based;language generation;generation structured;generation structured meaning;generation pragmatics imposed;standard language generation;generation tasks;abstractive summarization;pragmatic modeling;systems abstractive summarization;formulate language production;structured meaning representations;generate output text;generation tasks consider;consider pragmatic modeling;pragmatic modeling methods;generation based idea;summarization", "pdf_keywords": "text generation pragmatics;summarization tasks abstractive;representations abstractive summarization;tasks abstractive summarization;abstractive summarization structured;language generation tasks;abstractive summarization;summarization tasks;summarization structured meaning;focus conditional generation;summarization structured;conditional generation tasks;text summarization tasks;meaning representations summarization;abstractive text summarization;generation pragmatics;approaches text generation;approach abstractive summarization;text generation;generation meaning representations;representations summarization;structured meaning representations;language generation;representations summarization evaluate;generation tasks generation;generation tasks like;pragmatic reasoning;modeling distractors;explicit modeling distractors;summarization evaluate models"}, "ba5e3559a2d54bb0e8d7678c9905b4a77da63f71": {"ta_keywords": "reward based incentive;peer reward inversely;peer reward;incentivizes truthful responses;matches peer reward;based incentive schemes;incentive schemes;reward mechanism incentivizes;incentive scheme;based incentive scheme;based incentive;scheme administering reward;incentive scheme administering;incentive;reward group people;mechanism incentivizes truthful;administering reward based;reward based;truthful equilibrium;simple reward mechanism;incentivizes truthful;reward;reward mechanism;administering reward group;attractive administering reward;sra simple reward;simple reward;administering reward;reward evaluation;incentive schemes rebates", "pdf_keywords": "truthful peer prediction;peer prediction mechanisms;reputation systems;mechanism truthful peer;setting reputation systems;reputation systems online;peer prediction based;peer prediction;reciprocity information loss;truthful peer;incentives lying;practical setting reputation;strongly truthful peer;users investigate incentives;incentives lying various;reciprocity information;reputation;incentives;truthful equilibria;investigate incentives lying;evaluations strongly incentivized;propose mechanism truthful;mechanism truthful;setting reputation;prediction mechanisms result;reporting observations truthfully;truthful equilibria relationship;prediction mechanisms;mechanisms online service;peer"}, "52824fb6eb5d3b55fb6634c77dc80f5826964464": {"ta_keywords": "extracting specifications software;inductive logic programming;technique extracting specifications;extracting specifications;specifications software using;specification recovery;software technique extracting;specifications software;logic programming methods;used specification recovery;cases inductive learning;logic programming;inductive logic;specification recovery domain;demonstrated inductive logic;test cases inductive;inductive learning techniques;generating examples behavior;programming methods;programming methods successfully;inductive learning;world software technique;software technique;class test cases;cases inductive;real world software;technique generating examples;generating examples;programming;test cases", "pdf_keywords": ""}, "0cd693f1a1223f25e89c1f5efdedd7c3b7846691": {"ta_keywords": "parking arterial traffic;traffic parking;traffic parking city;proportion traffic;arterial traffic;traffic volume;arterial traffic volume;traffic probability;traffic searching parking;estimate proportion traffic;congestion average occupancy;traffic;traffic probability block;traffic vehicles depends;traffic volume data;traffic vehicles;volumes traffic probability;travel time traffic;volumes traffic;parking city percentage;utilize curbside parking;curbside parking arterial;queuing network;proportion traffic searching;relationship traffic parking;controlling congestion;parking high occupancy;new queuing network;curbside parking;time traffic", "pdf_keywords": "modeling urban parking;urban parking demand;parking scheduling;parking demand based;parking demand;parking scheduling pricing;new parking scheduling;urban parking;parking used predict;model curbside parking;curbside parking parking;parking considers;parking considers spatial;curbside parking;parking near destination;situations desirable parking;curbside parking considers;public curbside parking;parking;cruising parking;parking parking;cruising parking arise;exploring new parking;parking neighborhood;curbside parking used;desirable parking;street queues simulator;parking near;searching parking neighborhood;desirable parking near"}, "1f0524971c20a06d745ab784689eb8833435fde1": {"ta_keywords": "verification principle factuality;factuality form verification;aided verification;computer aided verification;topological insulator;topological insulator ti;verification principle;verification paper;verification human written;aided verification human;verification paper present;principle factuality;factuality form;principle factuality form;results verification principle;human written factoid;competition topological insulator;verification;verification human;results verification;factuality;form verification paper;written factoid;insulator ti submitted;participating systems paper;written factoid supported;form verification;insulator;topological;factoid supported", "pdf_keywords": "textual evidence retrieved;sentence representations training;human generated textual;entailment using lexicalized;evidence retrieved wikipedia;claim sentence representations;sentence representations;entailment documents;entailment documents submitted;textual evidence;tasks automatic information;evidence sentences;textual;present entailment documents;information extraction;generated textual claims;automatic information;generated textual;automatic information veri\ufb01cation;textual claims textual;evidence retrieved;entailment using;description task dataset;source scoring computer;combining evidence sentences;lexicalized features;using lexicalized features;evidence sentences claim;claims textual evidence;open source scoring"}, "68731c68773b117250f04509103031109b222d27": {"ta_keywords": "sentence level extractions;extractions translatingbased objective;extractions based corpus;extractions translatingbased;level extractions translatingbased;translatingbased objective extracting;relations text corpora;objective extracting entities;entity relation phrases;sentence level tuple;sentences based local;extracting entities relations;based corpus;extracting entities;based corpus level;text corpora;level tuple extractions;quality sentence level;knowledge bases supervision;corpora important task;objective extracting;corpus;tuple extractions based;sentence level;phrases individual sentences;individual sentences based;score sentence level;tuple extractions;framework distant supervision;text corpora important", "pdf_keywords": ""}, "fd8e176087335355ff5e81821a616d15ec8d3346": {"ta_keywords": "synthesizing training labels;unseen labels generalization;creating labeled training;synthesize training labels;training labels based;training labels;labeled training sets;labeled training;labels based indirect;unseen labels;plrm unseen labels;labels generalization;creating labeled;noisy supervision sources;labeled;training labels multiple;labels based;based indirect supervision;automatically synthesizing training;labels generalization bound;indirect supervision sources;weak indirectsupervision;labels;formulate weak indirectsupervision;weak indirectsupervision wis;label;plrm outperforming baselines;label spaces;image text classification;output label", "pdf_keywords": "probabilistic label relation;supervision unseen labels;label relations;label relations dependencies;models label relations;label relation model;label relation structure;label relation;labels based indirect;synthesizing training labels;develop probabilistic label;pairwise label relations;training labels based;indirect supervision sources;probabilistic label;propose label hierarchy;label relations paper;based indirect supervision;distinguishability label relation;training labels;label hierarchy based;label hierarchy;labels develop probabilistic;supervision sources;supervised;propose supervised;propose supervised learning;formulate weakirect supervision;labels based;supervised learning"}, "e34f9e9163b13de00707157feda6a8b853c5c82d": {"ta_keywords": "techniques synset deduplication;synset deduplication;synset deduplication including;deduplication problem automatically;deduplication;collaboratively created lexical;deduplication including;deduplication including machine;approach solve deduplication;deduplication problem;solve deduplication problem;solve deduplication;ones collaboratively created;created lexical resources;collaboratively created;based ones collaboratively;created lexical;techniques synset;machine crowd based;ones collaboratively;comparable expert based;lexical resources;collaboratively;lexical resources trending;different techniques synset;quality comparable expert;constructing resource;automatically quality comparable;crowd based ones;lexical", "pdf_keywords": ""}, "e1d35deec12d18e53ca97a3cf4071526ad47968d": {"ta_keywords": "pretrained language models;evaluation multiple linguistic;language models;attention configuration training;large pretrained language;multiple linguistic;machine learning tm;language models led;linguistic phenomena recent;pretrained language;linguistic knowledge encoded;transformation based machine;learning tm emerging;linguistic knowledge;linguistic;learning tm;learning;multiple linguistic phenomena;linguistic phenomena;kinds linguistic knowledge;machine learning;multi tasking;multi tasking focal;transformer lms;based machine learning;self attention configuration;language;kinds linguistic;improvements subcategories transformation;examining kinds linguistic", "pdf_keywords": ""}, "e2ebf18e0b88752bd3ff905d2fba74213dcd2c51": {"ta_keywords": "prediction electrolarynx speaking;real time prediction;prediction electrolarynx;sounds generated electrolarynx;electrolarynx speaking aid;type prediction electrolarynx;optrolaryngeal el speech;sounds help laryngectomees;electrolarynx based statistical;prediction f0 prediction;speech yielded prototype;generate excitation sounds;electrolarynx based;statistical f0 prediction;f0 prediction accuracy;generated electrolarynx based;speaking aid device;sounds generated;excitation sounds generated;prediction accuracy alleviate;electrolarynx speaking;sounding el speech;electrolarynx use prototype;f0 prediction;laryngectomees;speech propose method;prediction accuracy;prediction f0;help laryngectomees;help laryngectomees produce", "pdf_keywords": ""}, "595a79ca667258ca2a4f5e7775e95a0fb0a0f024": {"ta_keywords": "stochastically driven;stochastically driven systems;stochastic gradient method;stochastic gradient;simulation stochastically driven;free gradient play;monotone games complexity;gradient play strongly;strongly monotone games;gradient play;simulation stochastically;gradient method simulation;stochastically;unconstrained optimization;games complexity;present stochastic gradient;method simulation stochastically;stochastic;monotone games;games complexity paper;method unconstrained optimization;predict winner race;predicting outcome race;free gradient;motion body gravitational;derivative free gradient;body gravitational field;outcome race based;play strongly monotone;bodies gravity field", "pdf_keywords": "monotone games learning;learning algorithms games;learning called bandit;bandit feedback strongly;learning adversarial strategically;weights players convex;bandit feedback;players convex machine;algorithms games;strongly monotone games;games learning;adversarial strategically;called bandit feedback;monotone games;players convex;adversarial strategically generated;decision dependent games;algorithms games continuous;solutions learning adversarial;monotone games long;convex machine learning;cost functions learning;learning decision;gradient estimator weighted;learning adversarial;games learning tool;learning decision dependent;players arrive equilibrium;learning called;learning algorithms form"}, "36f7827bc344f9c2198dcb29732c525c68dc637a": {"ta_keywords": "cooperative games shapley;games shapley value;computationally tractable allocation;principled fair allocation;fair allocation;tractable allocation techniques;tractable allocation;schemes cooperative games;allocation techniques good;approximating shapley value;fair allocation broad;approximating shapley;cooperative games;games shapley;allocation techniques;allocation;shapley value relatively;euclidean games games;shapley value;shapley value tsg;euclidean games;allocation broad;synthetic euclidean games;prove approximating shapley;broad variety games;tours calculated scenarios;proxies shapley value;allocation broad variety;single vehicle transport;shapley value problem", "pdf_keywords": "routing games costs;coalitional games fundamental;vehicle routing game;supermodular coalitional games;cooperative game theory;problem game theory;coalitional games;cost allocations proxies;game theory present;routing game;routing games;games costs delivering;routing game game;game theory;delivery cost allocation;cost allocations;game game theory;theory game pair;cooperative game;game theory moat;game theory game;cost allocation;compare cost allocations;packing cost allocation;cost allocation problems;value supermodular coalitional;allocations proxies equal;games fundamental problem;allocations;framework cooperative game"}, "ead1e044d284f3deecd32c2d5cc89fe513195a0a": {"ta_keywords": "augmentation synonymy graph;synonymy graph augmentation;graph augmentation synonymy;synonyms input graph;investigated synonymy graph;synonymy graph connectivity;synonymy graph;approach synonymy graph;synonymy graph approach;graph based word;word sense induction;augmentation synonymy;graph clustering comparing;quality graph clustering;graph augmentation;synonyms input;graph clustering;potential synonyms input;synonym relation;investigated synonymy;synonyms;edges potential synonyms;induction investigated synonymy;graph connectivity;synonym relation implies;synonym;sense induction;graph connectivity graph;augmented input graph;datasets russian language", "pdf_keywords": ""}, "7e63be5285e6596fbbc6c56bc89f7b6fd8bbe8c5": {"ta_keywords": "model explanations effective;effective model debugging;model explanations;explaining model prediction;model debugging;explanations tools;explanations tools model;turning explanations tools;explanations effective model;explanations effective;primarily model predictions;deep network ineffective;model debugging response;based explanation methods;model debugging addition;diagnosing model contamination;model trained;model based explanation;identify mislabeled training;mislabeled training examples;explanation methods proposed;ineffective diagnosing model;diagnose mislabeled training;methods modify backpropagation;model predictions investigate;turning explanations;diagnosing model;tools model debugging;explanations;deep network", "pdf_keywords": "explanations categorize bugs;categorizing model bugs;categorize bugs;learning explanations categorize;machine learning explanations;categorize bugs office;dnns trained image;assess feature attribution;learning explanations;feature attribution;mislabelled training examples;similarity learning explainable;explanations categorize;attribution methods bugs;feature attribution methods;testing feature attribution;explanation approach dnns;approach dnns trained;learning explainable models;dnn model predicting;mislabelled training;bugs testing feature;visual similarity learning;learning explainable;model bugs data;dnns trained;model bugs;model trained;training examples;bug type"}, "bd8922f8cc8284553dc9e6db529af309298451fe": {"ta_keywords": "text train attention;train attention decoder;attention based encoder;attention decoder;decoder asc reinforcement;target text train;text train;reinforcement learning rl;attention decoder networks;data transcribed speech;transcribed speech languages;study backtranslation proton;train attention;backtranslation proton;speech languages;reinforcement learning;asc reinforcement learning;learning rl;decoder networks;attention based;use attention based;backtranslation proton end;transcribed speech nearby;transcribed speech;transcribed speech depend;using transcribed speech;data transcribed;study backtranslation;use attention;attention", "pdf_keywords": ""}, "25ddddbd0bd1cfebf1548b2ee91bb1bbd05fdff1": {"ta_keywords": "compositional tasks pretraining;multimodal transformer encodes;challenges neural systems;visual perception action;navigation neural agents;transformer encodes episode;multimodal transformer;neural agents dynamic;neural agents;interaction navigation neural;predicting outcome motor;perception action;neural;multimodal;compositional tasks;encodes episode history;based multimodal transformer;training synthetic instructions;perception based multimodal;neural systems;navigation neural;perception action propose;encoding history transformer;episodic transformer transforms;based multimodal;complex human instructions;tasks pretraining joint;transformer transforms petri;long sequence subtasks;encodes episode", "pdf_keywords": "captioning temporal prediction;recurrent architecture capable;propose recurrent architecture;recurrent architecture;data recurrent architecture;recurrent architecture able;recurrent based architecture;able learn recurrent;learn recurrent;tasks video captioning;video captioning temporal;language encoder trained;vision language navigation;learn recurrent baseline;agent language encoder;captioning temporal;recurrent data;data recurrent;train agent language;prediction train agent;camera observations language;recurrent data recurrent;recurrent baseline recurrent;expert demonstrations annotated;baseline recurrent data;challenging tasks video;temporal prediction train;vision language;benchmark trained;agent language"}, "0805cb1b26577f08f84190445992f7f0584e4742": {"ta_keywords": "question knowledge world;answer question knowledge;question knowledge;knowledge world;knowledge;novel approach answer;answer question;world;approach answer question;novel approach;novel;present novel approach;present novel;paper present novel;answer;approach answer;approach;paper;question;paper present;present", "pdf_keywords": ""}, "352ac73b7d92afa915c06026a4336927d550cec3": {"ta_keywords": "graph neural;novel graph neural;graph neural network;extracting information;neural;neural network generated;neural network;annotated dataset distantly;data extracted;novel graph;data data extracted;annotated dataset;datasets multi hop;extracted output data;input data;extracting information content;text input data;method extracting information;data extracted output;supervised datasets;human annotated dataset;propose novel graph;probe electric field;distantly supervised datasets;dipole moment device;dataset distantly supervised;distantly supervised;device axis electric;data data;quark gluon plasma", "pdf_keywords": "relational reasoning unstructured;convn model relational;relation extraction;improved relational reasoning;relation extraction variety;reasoning unstructured text;relational reasoning best;relational reasoning;relation extraction signi;graph neural;entities natural language;relational reasoning aims;model relational reasoning;novel graph neural;graph neural networks;parameters graph neural;predicting relationship entities;reasoning unstructured;reasoning unstructured inputs;reason entities relations;propose convolutional neural;relationship entities natural;natural language bag;improved relational;graph neural network;relational message;natural language;abstractly reason entities;relational;reasoning best models"}, "b53689b8c28353106f327f0981b108eb67816053": {"ta_keywords": "machine translation pbm;based machine translation;realized machine translation;syntactic preprocessing approaches;machine translation;based syntactic preprocessing;machine translation technique;translation machine translation;syntactic preprocessing;machine translation machine;formalism language translation;translation formalism language;englishjapanese pbmt syntax;use machine translation;translation technique improving;translation machine;syntax based smt;new translation formalism;preprocessing method englishjapanese;translation formalism;machine translation paper;translation technique;translation pbm;language translation;pbmt syntax based;method englishjapanese pbmt;translation pbm output;syntax based;rule based preprocessing;phrase based machine", "pdf_keywords": ""}, "21d45b4923ad165fbb6612e08d06f9d786f9b4cc": {"ta_keywords": "knowledge graph commonsense;commonsense knowledge graphs;graph commonsense models;graphs train commonsense;commonsense knowledge;commonsense models;symbolic knowledge distillation;commonsense model work;train commonsense models;commonsense general language;knowledge graphs train;commonsense models present;commonsense model;symbolic knowledge graph;general language models;commonsense models study;graph commonsense;models author commonsense;type commonsense model;author commonsense knowledge;language models;causal commonsense gpt;general language model;knowledge distillation;machine general language;knowledge graphs;causal commonsense;distill aspect commonsense;knowledge graph;language model teacher", "pdf_keywords": "knowledge graphs commonsense;commonsense knowledge graphs;knowledge commonsense;commonsense knowledge distillation;constructing commonsense knowledge;knowledge commonsense research;graphs commonsense leveraging;authored knowledge commonsense;commonsense knowledge;commonsense model corpus;symbolic commonsense knowledge;knowledge resulting commonsense;corpus commonsense inference;generated corpus commonsense;graphs commonsense;automatic knowledge graphs;commonsense inference;automatically constructing commonsense;commonsense inference types;automatic knowledge base;commonsense leveraging;automatic knowledge distillation;knowledge base completion;commonsense model automatically;commonsense model better;machine generated knowledge;knowledge graphs;trained knowledge source;automatic knowledge;symbolic knowledge distillation"}, "53e0abebd9aef5915f72147d3674596a0051748c": {"ta_keywords": "privacy data protection;data protection;data protection research;personalized artificial intelligence;privacy data;data protection context;privacy;evolving ai services;research privacy;rare earth ionization;research privacy data;ai services;resulting privacy;earth ionization rie;protection context personalized;levels resulting privacy;edge research privacy;privacy implications;ionization rie;ai services ii;protection research;resulting privacy implications;personalized artificial;directions data protection;earth ionization;enabling enhanced personalization;personalization better support;protection research especially;privacy implications facto;ionization", "pdf_keywords": "privacy integrity;privacy integrity issues;privacy privacy;privacy;questions privacy privacy;questions privacy;investigate privacy integrity;investigate privacy;following questions privacy;privacy privacy particular;privacy particular argue;systems addressing trust;privacy particular;data access rights;encryption;addressing trust issue;data access;argue personal data;personal data;rights storage protection;encryption reliably;trust issue;trust assumption;encryption reliably kept;addressing trust;approaches given trust;trust;ai propose categorization;access rights storage;article investigate privacy"}, "b6145cc19acfbec31373446a2dba210cc9b1eb7f": {"ta_keywords": "supervised relation extraction;distantly supervised relation;relation extraction;relation extraction propose;extraction relation extraction;relation extraction paper;concept instance extraction;supervised relation;approach distantly supervised;distantly supervised;instance extraction relation;small structured corpora;structured corpora;structured corpora sections;distantly labeled examples;instance extraction;extraction relation;relation arguments distantly;distantly labeled;relation;related tasks;supervised;corpora sections;extraction propose framework;corpora;related tasks concept;concept instance;identified correspond relation;corpora sections identified;correspond relation", "pdf_keywords": "supervised relation extraction;relation extraction;extraction relation extraction;relation extraction jointly;relation extraction particular;extraction entity mentions;extracting instances relations;distantly supervised relation;extraction jointly learning;small structured corpora;concept instance extraction;instance extraction relation;extraction entity;small structured corpus;supervised relation;structured corpora;structured corpus;text corpus;text corpus propose;ordinary text corpus;unstructured free text;extraction relation;structured corpus commonly;instance extraction;structured corpora sections;extraction jointly;corpus propose framework;relations de\ufb01ned freebase;corpus propose;information relatively unstructured"}, "181e1d4b08dc62237277a6a743576facd8c5e572": {"ta_keywords": "highly overlapped speech;speaker voice activity;voice activity;overlapped speech;voice activity detection;speaker diarization;speaker diarization highly;results speaker diarization;simulate real conversations;targeted speaker voice;speaker voice;conversations propose fusion;targeted speaker;conversations group humans;model conversations;voice;conversations;real conversations;promising results speaker;simple model conversations;speech;model conversations group;conversations group;results speaker;speaker;speakers targeted speaker;frame level systems;combine frame level;numbers speakers targeted;activity detection ts", "pdf_keywords": "meetings based diarization;speaker meetings based;meeting corpus;speaker meetings;multi speaker meetings;libricss meeting corpus;meeting corpus proposed;supervised diarization;speakers inspired supervised;diarization propose fusion;improve diarization performance;supervised diarization methods;long form recordings;diarization systems improved;inspired supervised diarization;diarization performance;approach multi speaker;based diarization propose;form recordings;recordings;corpus;improve diarization;diarization systems;diarization methods;diarization propose;diarization methods including;based diarization;speakers recording;corpus proposed;diarization methods like"}, "4236a5f650f5b7ced7512b5072a062b521220b31": {"ta_keywords": "predicting traffic;traffic speed prediction;predicting traffic speed;aim predicting traffic;semantic features traffic;transfer learning framework;transfer learning;features traffic;traffic patterns city;propose transfer learning;features traffic speed;traffic speed urban;traffic;traffic patterns;speed prediction;proposed kernel regression;traffic speed;analyze traffic;analyze traffic patterns;temporal semantic features;transportation smart city;intelligent transportation;intelligent transportation smart;areas recent supervised;kernel regression;recent supervised machine;speed prediction benefit;models proposed kernel;speed urban;speed urban areas", "pdf_keywords": ""}, "e25ce2a7b28699e1d57803ef977175937ce50923": {"ta_keywords": "annotated words;annotating particular words;annotated word;data annotated word;sentence based annotation;annotated word word;utilize corpus based;corpus based;annotating;natural language processing;annotation techniques;trained data annotated;utilize corpus;word segmentation;annotation;based annotation techniques;corpus based techniques;based annotation;effective natural language;corpus;number annotated words;sentence document fluid;annotation techniques paper;language processing;natural language;segmentation word based;data annotated;annotated;annotated words traditional;sentence level resources", "pdf_keywords": ""}, "0e532d1489d7420cff7ff8aa211ded08e7d57fe9": {"ta_keywords": "batch learning occurs;batch learning;utilize batch learning;data online learning;batch learning given;batch learning approach;online algorithms strongly;paradigm batch learning;functions online learning;statistical learning machinery;online algorithms;learning paradigm batch;statistical learning;convex loss functions;course statistical learning;algorithms strongly convex;loss functions online;online learning;learning approach possible;contrast batch learning;strongly convex loss;generalization ability convex;convex loss;learning approach;learning occurs sequence;investigating online algorithms;single optimal hypothesis;learning machinery;learning given data;single optimal", "pdf_keywords": ""}, "1109f787fc8d51feb3bae9bf6e1945dc4a1191e7": {"ta_keywords": "human artificial intelligence;artificial intelligence team;explanations winner outperformed;performance artificial intelligence;intelligence team working;intelligence ia algorithms;trust intelligence;proposal rejected agent;intelligence team;improvements explanations winner;artificial intelligence;artificial intelligence ia;intelligence help generate;trust intelligence help;demonstrate artificial neural;human centered intelligence;winner outperformed human;performance artificial neurons;rejected agent;artificial neural;agents;appropriate trust intelligence;rejected agent proportional;agent;reported improvements explanations;agents report results;intelligence develop explanatory;human artificial;artificial neural networks;decision making tasks", "pdf_keywords": "expert generated explanations;created expert explanations;expert explanations manually;expert explanations;explanations assess;explanations producing complementary;state art explanations;adaptive explanation switches;explanations informative instead;generated explanations developed;intelligence _adaptive explanation_;explanations assess quality;role explanations producing;predicting assistant;predicting assistant assistant;generated explanations;level conciseness explanations;explanations manually crafting;adaptive explanation;conciseness explanations;_adaptive explanation_ better;art explanations assess;explanations informative;explanations producing;strategy adaptive explanation;role explanations;explanations developed;explanations manually;multiple explanation styles;understand role explanations"}, "328a9fe143639810d6413c2cc901ec3afa6aa607": {"ta_keywords": "algorithm molecular dynamics;displacements molecular dynamics;molecular dynamics md;molecular dynamics;continuum model efficiently;displacements molecular;graphene;md displacements molecular;dynamics md simulations;dynamics md computational;graphene provide;single layer graphene;algorithm molecular;layer graphene;graphene provide sufficient;homogenized continuum model;discretized models;md simulations;layer graphene provide;new algorithm molecular;md computational method;md computational;molecular;continuum model;discretized models sign;materials lipids behavior;solid lpn model;surrogate md displacements;posedness conditions discretized;conditions discretized models", "pdf_keywords": ""}, "350e5f5a89cbb3a23442c9d0d3e59fc50d665dbb": {"ta_keywords": "electricity market formulate;electricity market;design electricity market;costs resulting dispatching;team ahead scheduling;scheduling;ahead scheduling problem;scheduling problem;ahead scheduling;scheduling problem constitutes;electricity;design electricity;offers design electricity;electricity eastern coast;electricity eastern;prices analysis;pricing compensation schemes;pricing;dispatching;clearing prices analysis;objective function pricing;fluctuations electricity eastern;function pricing;load costs resulting;energy offers design;pricing compensation;fluctuations electricity;units clearing prices;resulting dispatching units;energy offers", "pdf_keywords": ""}, "2226f5a13e3e9faac2e228e95175d3e612b52395": {"ta_keywords": "team function gender;leadership teleconference set;leadership teleconference;women basketball team;teleconferences monthly leadership;men women basketball;monthly leadership teleconference;basketball team function;women basketball;team function;officers teleconferences;basketball team;teleconferences;autonomous systems;leadership;teleconference;gender;diversity men women;addition officers teleconferences;autonomous;acm sigai leadership;basketball;officers teleconferences monthly;diversity men;teleconference set;autonomous systems present;team;teleconference set association;sigai leadership;understand diversity men", "pdf_keywords": ""}, "c102ac8c779ee0a53dc8e4ee20b4088ac2c7e186": {"ta_keywords": "speech recognition encoder;recognition encoder;speech recognition model;recognition encoder deep;speech recognition;encoder trained attention;chinese speech recognition;automatic speech recognition;automatic speech;encoder trained;encoder deep convolutional;encoder deep;neural network cnn;cnn motion;end automatic speech;state art encoder;art encoder;recognition model;deep convolutional neural;network cnn motion;cnn motion moving;recognition model paper;convolutional neural network;network cnn;attention based decoder;deep convolutional;encoder;trained attention based;art encoder present;neural network", "pdf_keywords": "end speech recognition;speech recognition deep;joint attention encoder;attention encoder;attention encoder decoder;speech recognition;model automatic speech;ctc attention based;automatic speech recognition;automatic speech;end end speech;speech recognition present;end speech;recognition deep convolutional;recognition deep;deep convolutional neural;mandarin chinese tasks;speech recognition paper;deep convolutional;methods automatic speech;use ctc attention;mandarin;attention based;ctc attention;convolutional neural network;spontaneous japanese mandarin;japanese mandarin;attention based methods;chinese tasks;encoder"}, "1144cc3e86b1cc4160aedddb085d7861d4b528dc": {"ta_keywords": "sampled softmax rnn;softmax rnn transducer;sampled softmax optimize;extend sampled softmax;softmax optimize memory;sampled softmax accuracy;sampled softmax;softmax rnn;apply sampled softmax;softmax accuracy baseline;softmax accuracy;softmax;impact sampled softmax;model rnn transducer;sampling vocabulary improve;softmax optimize;losses sampling vocabulary;rnn transducer advantages;rnn transducer;baseline model rnn;model rnn;vocabulary training saving;rnn transducer requires;speech recognition;automatic speech;ctc losses sampling;automatic speech recognition;sampling vocabulary;end automatic speech;training saving memory", "pdf_keywords": "sampled softmax architecture;propose sampled softmax;sampled softmax greatly;sampled softmax improve;extend sampled softmax;use sampled softmax;sampled softmax optimize;sampled softmax;softmax optimize memory;extensions sampled softmax;experimentally sampled softmax;sampled softmax example;softmax architecture automatic;softmax architecture;softmax improve baseline;speech recognition recurrent;softmax greatly;models automatic speech;end speech recognition;softmax;sampling vocabulary improve;losses sampling vocabulary;speech recognition sequence;architecture automatic speech;softmax optimize;softmax example wise;softmax improve;ctc loss sampling;speech recognition;subset automatic speech"}, "69e9a040ef633c60533843442529cc68c5f12932": {"ta_keywords": "algorithm computation eigenvectors;quadratic matrices embedding;matrices embedding dataset;matrix data embedding;similarity matrix data;computation eigenvectors quadratic;pairwise similarity matrix;eigenvectors quadratic matrices;matrices embedding;computation eigenvectors;similarity matrix;spectral methods;eigenvectors quadratic;spectral methods ncut;matrix data;used spectral methods;normalized pairwise similarity;eigenvectors;quadratic matrices;data embedding;spectral;pairwise similarity;embedding dataset;fast algorithm computation;widely used spectral;fast algorithm;effective cluster;embedding dataset low;ncut real datasets;dataset low dimensional", "pdf_keywords": ""}, "3dc4580a154df87f3a56aa3d16b00c5a935ebe15": {"ta_keywords": "reviewers detects citation;citation bias;citation bias venues;detects citation bias;citation reviewer;bias venues citations;does citation reviewer;citation reviewer work;right citation bias;cite prospective reviewers;evaluation scienti impact;citations play important;citations;detects citation;does citation;submission does citation;citations play;positively biased submission;venues citations;positively biased;citation;venues citations play;evaluation scienti;bias;biased submission;cause positively biased;prospective reviewers;bias venues;positive evaluation submission;bias venues large", "pdf_keywords": "citation bias peer;bias peer review;citation bias;test citation bias;citation bias present;cite potential reviewers;analysis citation bias;bias peer;2020 citation bias;bias present authors;peer review;conferences machine learning;conference machine learning;metrics assignment quality;reviewers conferences;evaluation submissions reviewers;reviewers conferences paper;submissions reviewers conferences;peer review propose;work peer review;papers cite;assignment quality;champion papers cite;deliberately cite;objective assignment quality;potential reviewers reviewers;potential reviewers;reviewers reviewers consciously;test citation;bias"}, "3cfdec4f1664fcdc20fd5a6d3f86e7b40cf19f70": {"ta_keywords": "length neural encoder;sequence length neural;length encoder decoder;summarization generate concise;length encoder;summarization generate;output sequence length;neural encoder decoder;decoder models neural;summaries desired length;text summarization generate;generate concise summaries;sequence generation tasks;right length encoder;neural encoder;encoder decoder outputs;length degrading summary;sequence generation;encoder decoder models;models neural encoder;encoder decoder;sequence length;length neural;encoder;controlling output sequence;quality summarization task;decoder models;summary quality summarization;decoder outputs;output sequence", "pdf_keywords": "summarization based recurrent;learning sentence summarization;model automatic summarization;sentence summarization based;able generate summaries;generate summaries;abstractive sentence summarization;automatic summarization source;summarization systems able;summarization systems;sentence summarization;generate summaries various;methods sentence compression;automatic summarization;document summarization;sentence compression;summarization source document;source document summarization;document summarization systems;sentence summarization topic;summarization source;recurrent neural networks;summarization based;focus sentence summarization;summarization;sequence length neural;sentence compression word;machine translation;shorter versions sentences;sentence summarization suggests"}, "7a070c558cdb9c525559d1ad48159551381750c9": {"ta_keywords": "question answering;question answering tasks;deep qa models;answering tasks gram;encoded knowledge deep;answering question text;modeling machine translation;sequence models encode;gram machines trained;tasks gram machines;knowledge deep neural;certain question answering;knowledge deep;deep qa;machine translation;end deep qa;tasks language modeling;gram machines;encode knowledge;deep neural;models encode knowledge;sequence sequence models;answer questions encoded;language modeling machine;answering tasks;questions encoded knowledge;answering question;machine translation certain;language modeling;sequence models", "pdf_keywords": "trainable answer answering;answering problems deep;answering tasks learning;answer answering;answer answering tasks;answer answer answering;framework answer answering;organize knowledge answer;answering treats schema;knowledge encoders;answer answering problems;knowledge answer questions;answer answering treats;answering tasks;deep knowledge representations;answer questions gram;answering problems;training answer questions;knowledge encoders trained;answering;knowledge bases;semantic parsing;knowledge representations;semantic parsing work;text symbolic knowl;answer questions present;answer questions;questions gram machines;symbolic knowl text;problems deep knowledge"}, "6516b800482100731f0eb348f678ad30799c839f": {"ta_keywords": "words emerge language;word emergence;predictive word emergence;word emergence support;factors language change;growth rates semantic;words emerge;diachronic corpora english;distributional semantics paradigm;distributional semantics;linguistic;language internal factors;language external factors;new words emerge;language change motivated;linguistic question;process new words;semantics paradigm factors;language change;diachronic corpora;semantic sparsity frequency;large diachronic corpora;rates semantic neighbors;factors semantic sparsity;language internal;formalized distributional semantics;semantic;factors language;semantic neighbors;semantic sparsity", "pdf_keywords": "linguistic literature neology;word embeddings quantify;word embedding space;word embeddings provide;word embeddings;word embedding;language change computational;growth hypothesize semantic;embedding space neologisms;neologisms later emerge;emerge word embeddings;semantically related words;hypothesize semantic neighborhood;languageinternal factors sparsity;areas word embedding;neologisms later;neologisms;words likely emerge;embedding paper linguistic;word vectors;semantic sparsity density;semantic neighborhood sparsity;approximate semantic sparsity;semantic neighborhoods new;semantic sparsity;determining semantic neighborhoods;density word community;factors language change;semantic neighborhood;linguistic"}, "ac03cf22e2a831ab030ae33b5ddf5f9864917a17": {"ta_keywords": "qst tomography protocol;quantum state tomography;state tomography qst;qst tomography;tomography qst;quantum computer;tomography qst tomography;study performance quantum;performance quantum computer;quantum computer based;performance quantum;tomography protocol competition;shot quantum;shot quantum state;quantum;single shot quantum;tomography protocol;quantum state;qst;state tomography;tomography;protocol competition marked;import competition;protocol competition;import competition recruits;competition;international import competition;competition recruits students;competition recruits;competition marked", "pdf_keywords": ""}, "0a7f95adbaf0e46c93b5f82c74a26f5874c861ac": {"ta_keywords": "model isolated traffic;isolated traffic ramp;isolated traffic;traffic ramp metering;controllers robust uncertainties;stochastic model isolated;ramp metering problem;ramp metering;safety lumped parameter;traffic ramp;controllers robust;robust uncertainties;stochastic model;proposed control law;considers uncertainty parameters;uncertainty parameters;distributed model;design controllers robust;given level safety;level safety;analysis stochastic model;shock rarefaction wave;level safety lumped;distributed model paper;traffic;control law paper;stochastic;control law;weak solution shock;uncertainty parameters shows", "pdf_keywords": ""}, "c5e4eafd85949e6aac9d8e98d5e03b2acf444046": {"ta_keywords": "questions adversarially;questions adversarially model;adversarial datasets worse;better adversarial datasets;adversarial data usually;trained adversarial;trained adversarial data;adversarially;adversarial datasets;perform better adversarial;adversarial datasets produce;adversarial;better adversarial;compose questions adversarially;adversarially model;models trained adversarial;analysis adversarial;unclear adversarial datasets;adversarial data;qualitative analysis adversarial;adversarial vs;model adversarial data;analysis adversarial vs;model adversarial;remains unclear adversarial;unclear adversarial;adversarial data collection;trained challenging datasets;adversarial vs standard;adversarially model loop", "pdf_keywords": "crowdsourcing data adversarial;researchers random adversarial;crowdsourcing data;crowdsourcing;question answering tasks;present crowdsourcing;research explores adversarial;question answering;explores adversarial;random adversarial;crowdsourcing platform;present crowdsourcing platform;explores adversarial data;adversarial;platform crowdsourcing data;platform crowdsourcing;random adversarial ii;paper present crowdsourcing;answering tasks;data adversarial;adversarial data;adversarial ii;adversarial data collection;crowdsourcing platform crowdsourcing;data adversarial data;adversarial data context;adversarial ii standard;machine comprehension;answering tasks assigning;answering"}, "575ac3f36e9fddeb258e2f639e26a6a7ec35160a": {"ta_keywords": "intermediate parsing training;supervised parsing high;parsing training;availing supervised parsing;dependencies treebanks transformer;intermediate parsing;supervised parsing;treebanks transformer;parsing high level;effect intermediate parsing;universal dependencies treebanks;spaces intermediate parsing;treebanks transformer results;parsing;transformer biaffine parsing;dependencies treebanks;high level semantic;explicit syntactic knowledge;parsing training ipt;intermediate parsing make;treebanks;syntactic knowledge;syntactic knowledge universal;parsing head;natural language understanding;parsing high;learn reason semantic;infuse explicit syntactic;language understanding;level semantic natural", "pdf_keywords": "transformer based parsers;supervised parsing high;availing supervised parsing;supervised parsing explicit;supervised parsing;intermediate dependency parsing;state art parsers;transformerbased biaf parser;parsing high level;usefulness supervised parsing;based parsers outperform;treebanks intermediate dependency;parsers;dependency parsing;parsers outperform;treebanks intermediate;based parsers;infusing structural language;language information treebanks;parsing;parser;information treebanks intermediate;parsing explicit;parser apply transformer;parsers terms;translation machine learning;treebanks;parsing dp training;relationship machine translation;parsers terms accuracy"}, "b437cc7c0ae672b188df078b5dd80f97e8dde978": {"ta_keywords": "word segmentation morphological;lexical learning speech;recognition machine translation;learning lexical units;models lexical learning;machine translation;learning lexical;lexical learning;word segmentation;recognize speech translate;machine translation conventionally;speech translate text;models lexical;language processing;machine translation present;segmentation morphological;speech translate;processing nonparametric bayesian;segmentation morphological analysis;gibbs sampling;lexical units raw;natural language processing;overview word segmentation;using gibbs sampling;lexical units;morphological analysis;annotation bottleneck;speech recognition;learning speech recognition;models using gibbs", "pdf_keywords": ""}, "69ba64b20d0a1849ef08d63c39bfafbaac909087": {"ta_keywords": "bandits artificial neural;multiarmed bandits artificial;ethics multiarmed bandits;bandits artificial;agent learns constrained;reward constraints;learns constrained policy;bandit setting allows;multiarmed bandits;online agent learns;reward constraints significantly;set reward constraints;extension multiarmed bandit;bandit setting;multiarmed bandit;multiarmed bandit setting;online exploration exploitation;online learning obeying;bandit;reward performance agent;agent learns;learns constrained;bandits;performance agent learns;agent learns set;reward feedback experiments;learned constraints;set reward;uses learned constraints;reward performance", "pdf_keywords": ""}, "40bbd3046f1fa86a50e526b3848b4f2bd3a1d873": {"ta_keywords": "polyelectrolyte solution lithium;dendrite li battery;li battery promising;lithium transfer;high lithium transfer;lithium batteries;safety lithium dendrite;lithium dendrite;lithium dendrite li;lithium salt designed;li battery;lithium salt immobilized;candidate generation lithium;generation lithium;lhc lithiated lithium;performance simple polyelectrolyte;lithiated lithium salt;li air cells;polyelectrolyte solution;solution lithium;lithium salt;polyelectrolyte;solution lithium salt;battery based battery;achieves high lithium;lithium batteries high;simple polyelectrolyte;generation lithium batteries;lithiated lithium;safety lithium", "pdf_keywords": ""}, "059f515bf53bcddeca031fd4a4071c911999a3c6": {"ta_keywords": "learn apparel invariant;apparel invariant person;apparel invariant feature;person clothing;person clothing change;apparel invariant;presents apparel invariant;apparel simulation gan;wearing similar clothes;invariant person representation;unsupervised apparel;clothing change;different clothes;mall person clothing;similar clothes;clothes;wearing different clothes;unsupervised apparel simulation;invariant feature representation;images person wearing;cloth embedding;persons wearing similar;person representation;clothing;learn apparel;target cloth embedding;clothing change different;person wearing different;apparel simulation;images person", "pdf_keywords": "supervised apparel invariant;apparel invariant feature;semi supervised apparelsimulation;semi supervised apparel;supervised apparelsimulation framework;learn apparel invariant;supervised apparelsimulation;supervised apparel;apparel invariant;unsupervised apparel;framework unsupervised apparel;reid feature extraction;apparel changing;wearing different clothes;wearing similar cloth;apparel changed reid;different clothes;unsupervised apparel changed;learn apparel;apparelsimulation framework learn;framework learn apparel;apparel changing different;reid feature;person reid tasks;apparelsimulation framework;apparel;clothes;invariant feature learning;learn discriminative feature;like apparel changing"}, "c96363c42bc8c465902c22b8c33c8704233f519e": {"ta_keywords": "code generation natural;code generation systems;benchmark code generation;code generation;adapting code generation;generation new languages;code generation new;development languages;language commands;natural programming languages;languages vow vowels;technology development languages;new languages;programming languages;languages applications;development languages propose;generation natural language;language commands extending;programming languages applications;benchmark code;new languages mitigate;challenges adapting code;adapting code;languages;art code generation;vowels;natural language commands;given code proficient;vowels recent burgeoning;languages propose", "pdf_keywords": "multilingual code generation;language code generation;multilingual data augmentation;pretrained multilingual encoder;multilingual code;machine translation multilingual;serves multilingual code;multilingual encoder;multilingual understanding code;multilingual encoder encode;multilingual encoder results;nl code generation;translation models encode;models pretrained multilingual;generation machine translation;translation monolingual inspired;lingual nl code;applying multilingual encoder;nl code models;translation monolingual;encode english;code generation challenging;train machine translation;challenging machine translation;addition multilingual training;translation multilingual data;multilingual training data;machine translation models;natural language code;translation multilingual"}, "70a321f12a655e305781e2de0ca9617d96e462c3": {"ta_keywords": "data aggregators competition;aggregators competition users;aggregators competition;aggregators game;aggregators game game;estimation strategic data;competition users data;data aggregators;strategic data;competition arises aggregators;statistical estimation strategic;aggregators;data non cooperative;central data aggregator;interactions data market;data aggregator;nash equilibrium social;aggregator statistical estimation;strategic data sources;estimates aggregator statistical;data aggregator design;aggregator statistical;aggregator;estimates aggregator;data market;aggregator design;multiple data aggregators;nash equilibrium;arises aggregators game;nash equilibria mechanisms", "pdf_keywords": "data markets game;data markets;data market derive;data markets strategic;game data buyers;model data markets;data market;markets game data;markets strategic data;structures data markets;data buyers data;arise data market;data buyers;strategic data sources;data buyers inef\ufb01ciencies;markets game;strategic data;buyers data sources;marketplace formulating game;information data buyers;pricing mechanisms;game data;designing pricing mechanisms;data sources existence;formulating game buyers;buyers data;game buyers compete;incentive mechanisms;marketplace formulating;game buyers"}, "bee52c51cbd37d0e48c3ea5f71a08f177d2aff73": {"ta_keywords": "training adaptive regularization;rate phoneme prediction;phoneme conversion structured;phoneme prediction requiring;conversion structured learning;phoneme prediction;adaptive regularization weight;adaptive regularization;based adaptive regularization;discriminative training;phoneme conversion based;online discriminative training;regularization weight vectors;structured learning;efficient training adaptive;phoneme conversion;discriminative training method;grapheme phoneme conversion;new structured learning;regularization weight;structured learning problem;regularization;binary classification;learning based margin;rate phoneme;approach grapheme phoneme;structured learning based;noisy grapheme phoneme;efficient training;learning applied g2p", "pdf_keywords": ""}, "7129b62be18487db5e9602e353bb10a4c79a9b92": {"ta_keywords": "neural reverse engineering;compiler optimizations neural;stripped binary systems;engineering stripped executables;reverse engineering;static analysis neural;stripped executables diverse;dynamics stripped binary;executables diverse assembly;available stripped executables;reverse engineering stripped;stripped executables;compiler optimizations;compiler;engineering stripped binaries;executables contain debug;arising compiler optimizations;patterns arising compiler;control flow graphs;stripped binary;stripped executables contain;analysis neural networks;binary systems;optimizations neural reverse;names stripped executables;executables;code patterns arising;problem reverse engineering;analysis neural models;combines static analysis", "pdf_keywords": ""}, "f4cf4246f3882aa6337e9c05d5675a3b8463a32e": {"ta_keywords": "vision language tasks;existing vision language;grounded visual language;vision language task;visual language;vision language;embodied vision language;visual language understanding;language existing vision;vision sequences actions;mapping natural language;language task datasets;interactive visual environment;language tasks;language instructions egocentric;instructions egocentric vision;embodied vision;visual environment;action space language;recent embodied vision;egocentric vision sequences;interactive visual;language tasks performs;vision sequences;visual;grounded visual;learning mapping natural;task datasets;natural language instructions;instructions egocentric", "pdf_keywords": "language robot actions;interactive agent trained;language robot;robots capable reasoning;human language robot;language directives robots;agent trained sequence;vision language;interactive agent;robot actions;sequence architecture robotics;vision language inputs;reasoning vision language;learned semantic;robot actions accomplishing;learned semantic parsers;robot;agent trained;capable reasoning vision;vision language experiment;computer vision language;robots;level natural language;crowdsourced language;robots capable navigation;robots capable;vision andlanguage navigation;crowdsourced language directives;andlanguage navigation tasks;architecture robotics long"}, "8f11643b42976433fc3a2ec19feef486929527a1": {"ta_keywords": "black hole motion;test particles gravitational;test particle gravitational;black holes studied;particle gravitational field;particles gravitational fields;massive black hole;gravitational field massive;particle gravitational;massive black holes;gravitational fields massive;particles gravitational;black holes;gravitational field;gravitational fields;black hole;motion test particles;gravitational;motion test particle;test particle;numerical investigation motion;test particles;hole motion test;fields massive black;field massive black;field massive;hole motion;holes studied;numerical investigation;fields massive", "pdf_keywords": ""}, "d9e56aa9f69e18c9d37799b86b50d36709cbf711": {"ta_keywords": "hadron collider lhc;bose einstein condensate;large hadron collider;einstein condensate bec;hadron collider;particle viscous incompressible;collider lhc;3d bose einstein;xmath1 transition;xmath0 xmath1 transition;lhc;einstein condensate;collider lhc took;cern large hadron;xmath1 transition dimensional;phase cern large;phase cern;test particle viscous;particle viscous;lhc took place;dimensional 3d bose;results phase cern;viscous incompressible fluid;structure xmath0 xmath1;study structure xmath0;collider;lhc took;viscous incompressible;condensate bec;xmath1", "pdf_keywords": ""}, "59a228f48a83eb0905391f7e454fde0eeb6680ee": {"ta_keywords": "lattices statistical framework;bayesian learning lexical;lattice gauge theories;simulation lattice gauge;lattices statistical;lattice gauge;bayesian learning;information lmi lattices;lmi lattices statistical;joint bayesian learning;quantum algorithm simulation;algorithm simulation lattice;handling lattice input;gibbs sampling;lattice input;finite state transducers;quantum algorithm;learning lexical;handling lattice;simulation lattice;lattice input propose;speech recognition;lattice;implementation quantum algorithm;algorithm use gibbs;learning lexical units;efficient handling lattice;use gibbs sampling;model automatic speech;lattices", "pdf_keywords": ""}, "8b7a8f9a27b8dc73a5b0b62ada14bbab047084fc": {"ta_keywords": "silent speech enhancement;statistical voice enhancement;speech enhancement;silent speech interface;voice conversion;voice enhancement based;statistical voice conversion;voice enhancement;speech interface;electrolaryngeal speech enhancement;virtual speech;speech produced alternative;speech enhancement paper;virtual speech recognition;speech interface nonaudible;voice conversion vc;vc silent speech;enhancement electrolaryngeal speech;speech enhancement electrolaryngeal;laryngectomees real time;time virtual speech;audible speech;speaking method laryngectomees;silent electrolaryngeal speech;speech recognition vv;nonaudible murmur;speech produced;statistical voice;speech recognition;focus nonaudible murmur", "pdf_keywords": ""}, "4100256a125d7b56cac693a436bba2b00fae3fa3": {"ta_keywords": "automatic audio captioning;audio captioning;audio captioning based;pretrained audio neural;captioned audio;captioned audio samples;incorporate audio embeddings;audio neural;audio embeddings;audio neural networks;training incorporate audio;pretrained audio;audio embeddings obtained;approach automatic audio;captioning based state;availability captioned audio;incorporate audio;automatic audio;language descriptions acoustic;embeddings obtained pretrained;captioning based;captioning;audio;obtained pretrained audio;trained models;input model training;audio samples model;indicate trained models;trained models significantly;model training", "pdf_keywords": ""}, "306c59458cebb35c2d520dd129f09d5c6cc2985f": {"ta_keywords": "xmath0 dimensional euclidean;eigenvalues hamiltonian written;xmath0 dimensional;mapped euclidian space;space mapped euclidian;mapped euclidian;eigenvalues hamiltonian;case eigenvalues hamiltonian;paper xmath0 dimensional;euclidean space mapped;hamiltonian written;hamiltonian;hamiltonian written terms;dimensional euclidean space;euclidian space;dimensional euclidean;euclidian space suitable;lagrange equations xcite;euclidean space;euler lagrange equations;space mapped;space suitable transformation;transformation case eigenvalues;analytically eigenvalue problem;xmath0;dimensional;solved analytically eigenvalue;numerically article presents;analytically eigenvalue;euclidian", "pdf_keywords": "learning paraphrasing textual;learning paraphrasing;paraphrasing textual;approach learning paraphrasing;models short paraphrases;paraphrasing textual entailment;paraphrases based vector;phrase based paraphrasing;short paraphrases based;paraphrasing tasks;paraphrasing tasks presented;based paraphrasing tasks;model short paraphrases;paraphrases based;entailment question answering;short paraphrases;short paraphrases \ufb01lling;paraphrasing;phrase embeddings;question answering systems;models phrase embeddings;paraphrase task term;paraphrases \ufb01lling gap;word phrase representations;paraphrases \ufb01lling;paraphrase task;embed phrases;goal embed phrases;paraphrases;question answering"}, "bba9b93ab8d9b98cd54001a5ba9673e513a35219": {"ta_keywords": "data recurrent neural;networks rnns;learning sequence data;recurrent neural networks;data recurrent;rnns;networks rnns powerful;multilayer perceptron trained;introduce multilayer perceptron;neural networks rnns;recurrent neural;models learning sequence;learning sequence;classification diagnoses training;clinical medical data;missing data recurrent;medical data;time series clinical;multilayer perceptron;diagnoses training model;sequence data specifically;perceptron trained;health information sensor;diagnoses training;classify 128 diagnoses;sequence data;rnns powerful increasingly;classification diagnoses;medical data especially;rnns powerful", "pdf_keywords": ""}, "cc352ea39f820c3effc40ce09369cf59afe361df": {"ta_keywords": "cloud based health;based health cloud;health cloud;implementing cloud based;implementing cloud;virtualized cloud;cloud computing platform;virtualized cloud computing;cloud computing;public cloud computing;integrate virtualized cloud;cloud services;cloud computing conducted;performance public cloud;cloud based;cloud;developed cloud based;method implementing cloud;public cloud;cloud computing vcnc;demonstrating cloud services;applications demonstrating cloud;developed cloud;end developed cloud;workload azure;demonstrating cloud;virtualized;electronic health health;context cloud computing;cloud services leveraged", "pdf_keywords": "health applications cloud;cloud based health;cloud based medical;hosting health applications;applications cloud;cloud services;health applications deployments;azure cloud;applications cloud computing;cloud computing environment;systems azure cloud;cloud variable workload;cloud computing environments;cloud computing;application systems azure;health application systems;public cloud computing;performance health application;cloud based;systems azure;environments developed cloud;cloud;demonstrating cloud services;developed cloud based;public cloud;azure;related cloud based;implementations health applications;deployments public cloud;cloud services leveraged"}, "589e651c69251ee20a89e075d015eb03b35cf17d": {"ta_keywords": "speech translation dual;speech translation;end speech translation;length beam translation;translation laser;deployment speech translation;automatic speech;speech translation st;translation laser beam;beam translation laser;translation dual decoder;speech recognition ar;beam translation;beam fast inference;automatic speech recognition;effective length prediction;translation quality;prediction methods speech;decoding speed;fast inference speed;speech recognition;translation quality compared;decoding automatic;competitive translation quality;text based translation;translation st systems;regarding decoding speed;improving inference speed;ar decoding automatic;decoding automatic recognition", "pdf_keywords": "translation experiments benchmark;automatic speech translation;speech translation task;machine translation experiments;speech translation sequence;speech translation e2e;machine translation;comparable translation quality;st speech translation;speech translation;improving inference speed;translation experiments;competitive translation quality;target text translation;translation sequence level;end speech translation;maintaining competitive translation;translation quality state;translation quality;translation quality compared;benchmark corpora;inference speed maintaining;benchmark corpora proposed;translation bypassing;automatic speech;inference speed;speed inference 2016;translation bypassing components;mapping source speech;benchmark corpora effectiveness"}, "d5123ab81f511027cbe11dc92d99e116fd193158": {"ta_keywords": "remote sensor energy;sensor energy transfer;sensor energy;energy harvesting;energy harvesting eh;variables energy harvesting;eh energy harvesting;harvesting eh energy;available energy buffer;temporal evolution wireless;choice energy buffer;remote remote sensors;remote sensors;single remote sensor;channels information receiver;remote sensor;channel state information;decision variables energy;energy buffer;energy transfer;energy transfer sink;node sampling policy;state information transmitter;energy buffer instantaneous;energy buffer used;sensitive choice energy;channels information;available energy;information transmitter;choice energy", "pdf_keywords": "channel state optimal;state optimal source;sink energy arrival;horizon markov decision;energy buffer instantaneous;available energy buffer;energy harvesting;markov decision process;poisson energy arrival;node sampling policy;threshold policy decision;energy harvesting eh;energy arrival;harvesting eh sensor;markov decision;energy arrival unit;state optimal;threshold policy involving;energy buffer;channel state probing;sampling policy shown;threshold policy;sampling policy;threshold policy achieves;threshold policy proposed;instantaneous channel quality;probed channel state;channel quality decision;instantaneous channel;exists threshold policy"}, "ddfd297531f56121b8383bd1eb2bb09189ab2e2b": {"ta_keywords": "language emphasis prediction;model speech translation;speech translation systems;focus speech translation;translation based neural;speech translation transfers;emphasis prediction;speech translation based;emphasis prediction measure;target language emphasis;speech translation;mapping emphasis languages;translation transfers linguistic;translation systems;linguistic content emphasis;model speech;short term memory;term memory lstm;language emphasis;transfers linguistic content;traditional speech translation;lstm neural;target language;emphasis information languages;model target language;emphasis languages;lstm neural networks;focus speech;memory lstm neural;new model speech", "pdf_keywords": ""}, "f0a498014c4ef67c0b72ceb18d95e0d25087fd57": {"ta_keywords": "neural machine translation;machine translation;machine translation systems;bidirectional translation tasks;translation systems;translation tasks;translation tasks proposed;10 improving decoding;softmax;softmax binary;softmax binary codes;codes combining softmax;japanese bidirectional translation;improving decoding;neural machine;achieve softmax;bidirectional translation;models achieve softmax;combining softmax binary;layer neural machine;combining softmax;layer logarithmic vocabulary;improving decoding speed;output layer neural;translation systems letter;english japanese bidirectional;logarithmic vocabulary size;softmax scores;japanese bidirectional;predicting binary code", "pdf_keywords": "neural machine translation;machine translation systems;novel machine translation;machine translation;bidirectional translation tasks;machine translation nmt;translation tasks;machine translation method;translation tasks proposed;translation systems;experiments translation tasks;domains machine translation;attention based models;models attention based;10 improving decoding;translation nmt models;translation systems usually;nmt models attention;models attention;softmax reducing memory;neural network language;softmax;instance bidirectional translation;attention;improving decoding;neural machine;softmax low;improving decoding speed;bidirectional translation;large vocabulary inputs"}, "88e2beccbc89b3e3dd793e2502b17c1fa551151d": {"ta_keywords": "propose distributed storage;distributed storage;distributed storage scheme;distributed storage approximated;point distributed storage;node repair bandwidth;tradeoff storage node;storage node;storage scheme;stored networks;data stored networks;tradeoff storage;repair bandwidth;reduced node stores;storage node repair;bandwidth point distributed;storage scheme data;stored nodes;storage approximated;distributed;storage approximated exact;data stored nodes;storage;characterize tradeoff storage;paper propose distributed;stored nodes data;nodes data recovered;bandwidth paper propose;propose distributed;network minimum bandwidth", "pdf_keywords": ""}, "f4c8539bed600c9c652aba76a996b8188761d3fe": {"ta_keywords": "improving performance crowdsourcing;performance crowdsourcing;machine translation;powerful machine translation;machine translation mt;performance crowdsourcing algorithms;crowdsourcing;chosen machine translation;machine translation technique;crowdsourcing algorithms;crowdsourcing algorithms letter;translation technique;shared task systems;translation technique successfully;performance lennard;models addressed new;translation mt;models addressed;task systems;improvements;machine;improve performance lennard;translation mt powerful;task systems compare;nmt models addressed;nmt models;translation;analysis improvements;improving performance;analysis improvements originate", "pdf_keywords": "neural machine translation;evaluate machine translation;machine translation techniques;machine translation;machine translation risen;translation techniques;translation techniques proposed;nmt systems empirically;neural models consistently;effectiveness adam training;strengthening nmt systems;service providers neural;neural machine;adam training;nmt systems;model ensembling improving;ensembling improving;techniques strengthening nmt;strengthening nmt;neural models;nmt;providers neural machine;adam training multiple;effectiveness adam;demonstrate effectiveness adam;chosen neural models;performance work empirically;ensembling improving bleu;performance shared evaluation;providers neural"}, "3e3254bce9c321310d2e9825ed52b30da9879173": {"ta_keywords": "speaker adaptation;speaker adaptation experiments;experiments speaker adaptation;improvement speaker adaptation;speech segment features;classification experiments speaker;utterance classification experiments;systems utterance classification;speech recognition;speaker adaptation baseline;representation speech segment;recognition systems utterance;utterance classification;speech recognition systems;performance speech recognition;speech segments;boa speech segments;approach representation speech;speech segment;speech segment represented;speech segments speech;representation speech;segments speech segment;systems utterance;improve performance speech;segments speech;state machines;speech;performance speech;state machines fsms", "pdf_keywords": ""}, "285c50d98dab741a82649b1abcaca8273cb8f253": {"ta_keywords": "phoneme conversion structured;conversion structured learning;phoneme prediction requiring;phoneme prediction;phoneme conversion;rate phoneme prediction;grapheme phoneme conversion;online discriminative training;structured learning;discriminative training;structured learning known;training adaptive regularization;discriminative training method;approach grapheme phoneme;structured learning based;approach structured learning;use structured learning;adaptive regularization weight;rate phoneme;online discriminative;grapheme phoneme;binary classification;adaptive regularization;learning based margin;regularization weight vectors;multiclass classification;efficient training adaptive;multiclass classification applied;regularization weight;conversion structured", "pdf_keywords": ""}, "55bdc4ad158e272ccf796ae52b0ab7086a834352": {"ta_keywords": "automated tutoring;automated tutoring systems;tutoring systems;affecting automated tutoring;tutoring systems making;tutoring;courses student modeling;model learning online;tutoring instruction;improve tutoring instruction;tutoring instruction strategy;improve tutoring;student modeling;discovers student models;instruction machine learning;student models;approach automatic learning;learning approach automatic;used improve tutoring;student models using;automatic learning;learning models used;automatic learning presented;learning models;automatically discovers student;systems making instructional;student model;good student model;student modeling key;making instructional", "pdf_keywords": ""}, "c3177616ad35ef7850ea1e62da1fa3be36943e8b": {"ta_keywords": "dialog response retrieval;neural network paraphrase;dialog pair database;paraphrase identification improve;based dialog model;distributed word representations;network paraphrase identification;dialog model;example based dialog;based dialog response;utilizing recursive neural;dialog response;response retrieval;paraphrase identification;word representations;query distributed word;dialog model requires;dialog pair;dialog;model dialog pair;pooling determine sentences;word representations employ;employ recursive autoencoders;based dialog;sentences arbitrary length;recursive neural network;model dialog;recursive neural;vocabulary oov database;recursive autoencoders", "pdf_keywords": ""}, "49a049dc85e2380dde80501a984878341dd8efdf": {"ta_keywords": "speech recognition labeled;learning speech input;representations speech audio;wav2vec;automatic speech;speech recognition;learning speech;feasibility automatic speech;speech audio;tuning transcribed speech;time learning;automatic speech recognition;wav2vec outperforms;speech input;semi supervised;recognition labeled data;best semi supervised;wav2vec outperforms previous;speech input latent;transcribed speech outperform;representations speech;supervised;time learning powerful;speech outperform;speech audio followed;powerful representations speech;data time learning;speech outperform best;problem learning speech;recognition labeled", "pdf_keywords": "representations speech learning;learns discrete speech;gumbel softmax;softmax;speech learning representations;latent speech representations;units gumbel softmax;speech representations;speech learning;gumbel softmax represent;trained contrastive task;learns convolutional;good representations speech;learns convolutional neural;softmax represent latent;model trained contrastive;speech units gumbel;representations speech;trained contrastive;contextualized word representations;contrastive task learning;speech representations paper;representations contrastive task;contextualized representations;contextualized representations model;representations model trained;build contextualized representations;softmax represent;learning good representations;speech audio"}, "5f1bbc96a22a630d3662b3fceb3160091e4bd814": {"ta_keywords": "voice activity detection;detection voice activity;nonstationary gaussian mixture;detection voice;noise voice activity;voice activity;gaussian mixture model;voice activity using;gaussian mixture;normalizing gaussian weights;learns nonstationary gaussian;gaussian weight normalization;model gmm speech;gmm speech signals;activity using gaussian;normalizing gaussian;frame normalizing gaussian;method detection voice;speech signals;estimates noise voice;gmm speech;gaussian weights;speech signals sequentially;based gaussian pruning;gaussian pruning;gaussian weights remaining;gaussian pruning weight;noise voice;nonstationary gaussian;mixture model gmm", "pdf_keywords": ""}, "bf0105bdd5b0dfc09580697739fb84590d031d0b": {"ta_keywords": "sim student simulations;student simulations simulated;student simulations;cognitive model sim;students cognitive model;model sim student;generated sim student;human students cognitive;generate cognitive model;model human students;sim student tool;study sim student;modeling real students;sim student performance;study sim;applicability sim student;sim student present;students cognitive;sim student;cognitive model generated;automatically generate cognitive;simulated simulated;students performance model;human students performance;generate cognitive;human students;performance human students;student performance model;simulated;simulated data good", "pdf_keywords": ""}, "8ec127925a8680928d546df7248963e772e07a5d": {"ta_keywords": "assigning tests adaptively;tests adaptively;policy assigning tests;tests adaptively based;optimal policy employees;estimate skill worker;optimal policy employer;tests candidate;assigning tests;estimate skill;method estimate skill;tests subject candidates;observe underlying skill;tests candidate set;skill estimated performing;worker skill estimated;skill worker noisy;tests;groups recruiting job;recruiting job candidates;interviews collating noisy;candidates employers;job candidates employers;directly analyze optimal;skill estimated;job candidates;outcomes groups recruiting;adaptively based results;worker skill;number tests candidate", "pdf_keywords": "optimal hiring policy;optimal hiring;discrimination hiring policy;choosing optimal hiring;hiring policy;problem discrimination hiring;discrimination hiring;randomized threshold policy;problem discrimination;optimal policy reject;consider problem discrimination;threshold policy optimal;policy reject candidates;discrimination;hiring policy employer;problem optimal policy;optimal policy;hiring policy employees;candidates decision rule;threshold policy;greedy policy;policy optimal;policy present randomized;form greedy policy;reject candidates fail;greedy policy present;reject candidates;randomized threshold;identically distributed skills;candidates decision"}, "a34954d9e36ea6c57743f55124a6ae444b951c2c": {"ta_keywords": "activation prediction neural;prediction neural;learned parameters network;model neural;activation prediction;activations training points;prediction neural network;model neural network;neural network excitatory;neural network;neural;training point learned;field performance lhc;pre activation prediction;simple model neural;performance lhc;neural network linear;training points weights;representer points training;propose neural;test point prediction;training points;point learned parameters;learned parameters;lhc;training point;predict set representer;network approach predict;insight dynamics network;propose neural network", "pdf_keywords": "deep convolutional;deep convolutional neural;convolutional neural networks;deep neural;context deep convolutional;theorem deep neural;convolutional neural;representer point explanations;neural network predictions;representer theorem deep;deep neural network;image datasets insights;explanations test points;convolutional;point explanations;automatically inferring;real time explanations;large scale models;in\ufb02uential model predictions;model predictions;representer points training;models image datasets;point explanations range;explanations range;context deep;neural networks;models image;neural;neural networks propose;training examples in\ufb02uential"}, "ce4eadb324026191c075f1af876403a847329d5b": {"ta_keywords": "learning based fixed;rule learning;tree rule learning;rule learning algorithms;set valued features;valued features;learning algorithms;valued features propose;learning algorithms easily;featurevector representation allows;decision tree;feature set strings;simple model learning;valued features particular;feature set;extended setvalued features;value feature set;categorization;featurevector representation;model learning;decision tree rule;length networks;model learning based;features;learning problems efficiently;categorization problems;learning problems;setvalued features paper;nominal features;setvalued features", "pdf_keywords": ""}, "63bc09c11a792abfcbb2d9e2809aa67929f09262": {"ta_keywords": "learning hyponym hypernym;semantic relations hypernymy;learning hyponym;word embeddings;hypernymy hyponymy widely;reasoning fluid dynamics;word embeddings inducing;hypernymy hyponymy;sense reasoning fluid;reasoning fluid;semantic relations;embeddings inducing relations;use word embeddings;hyponym hypernym;relations hypernymy hyponymy;semantic;inducing relations words;embeddings inducing;gpus semantic relations;hyponym hypernym projections;hyponymy widely;software learning hyponym;hyponymy;experiments russian language;hyponymy widely used;embeddings;description dynamics;relations hypernymy;natural language;gpus semantic", "pdf_keywords": ""}, "cfb1b39d1a6733f42cc5e8cfd60dc68cafa01d28": {"ta_keywords": "predicting position person;predicting position;structured prediction tasks;method predicting position;structured prediction;position person;natural language processing;prediction tasks;position person low;methods structured prediction;machine learning;processing machine learning;machine learning method;learning method predicting;natural language;relationship natural language;prediction tasks paper;machine learning methods;novel machine learning;based machine learning;learning methods structured;language processing machine;person low resource;language processing;method predicting;person low;position;predicting;learning method;learning methods", "pdf_keywords": ""}, "2b110fce160468eb179b6c43ea27e098757a56dd": {"ta_keywords": "generating syntactically adversarial;trained produce paraphrase;syntactically adversarial examples;syntactically adversarial;controlled paraphrase networks;generate adversarial examples;paraphrase systems pretrained;generate paraphrases;paraphrase networks;generate paraphrases follow;syntactically controlled paraphrase;paraphrase networks scpns;uncontrolled paraphrase systems;generate adversarial;produce paraphrase;scpns generate paraphrases;capable generating syntactically;use generate adversarial;paraphrase systems;generating syntactically;controlled paraphrase;adversarial examples;adversarial;baseline uncontrolled paraphrase;adversarial examples fooling;produce paraphrase sentence;paraphrase quality;decreasing paraphrase quality;robustness models syntactic;adversarial examples scpns", "pdf_keywords": "controlled paraphrase generation;able generate paraphrases;automatically generating paraphrases;generating paraphrases;paraphrase generation effective;generate paraphrases;paraphrase generation;controlled paraphrase network;generation paraphrases;generating paraphrases given;generation paraphrases introduce;generate paraphrases similar;paraphrase network;paraphrase generation given;paraphrase generation appropriate;syntactically controlled paraphrase;produce paraphrase;parse produce paraphrase;controlled paraphrase;demonstrate controlled paraphrase;produce paraphrase sentence;paraphrases similar input;paraphrases given target;paraphrases introduce;paraphrases;framework generation paraphrases;learning syntactically controlled;paraphrases similar;paraphrase sentence syntax;sentiment analysis entailment"}, "cf0860ab99c63cb7cbd5317fca7cf1fe70e8fb63": {"ta_keywords": "corpus virtual knowledge;entities corpus;knowledge bases;mentions entities corpus;knowledge base;existing knowledge bases;virtual knowledge base;reasoning virtual knowledge;entities corpus differentiable;knowledge base pretraining;consider task answering;corpus differentiable reasoning;knowledge base presented;questions using corpus;traverses textual data;virtual knowledge;task answering;textual data like;using corpus;question particular neural;corpus virtual;textual data;task answering complex;using corpus virtual;corpus;mentions entities;traverses textual;novel approach query;learning based gradient;textual", "pdf_keywords": "answer span extraction;question answering;indexed question answering;entities based deep;question answering particular;mentions entities corpus;large text corpus;entities corpus;queries extracted corpus;answering particular neural;answers queries extracted;text passages embedding;relations entities text;traverses textual data;reasoning latent entities;task answers queries;answer span;embedding entities;passages embedding entities;corpus encoded query;qa large text;answers queries;propose deep neural;approach answer span;latent entities based;relations mentions entities;entities text passages;textual data;textual data like;deep neural module"}, "024091a3c0223f27d6456b1a27db18fb08d41e5a": {"ta_keywords": "vocabularies neural network;language model;vocabularies neural;trained different translation;large vocabularies neural;network language model;neural network language;lmg translation tasks;language model note;translation tasks lmg;generating large vocabularies;translation tasks;large vocabularies;lipkin meshkov glick;different translation tasks;lmg model trained;model trained;translation tasks propose;vocabularies;meshkov glick lmg;better lmg translation;glick lmg model;performance lipkin meshkov;lmg translation;likelihood estimation;noise induced learning;maximum likelihood estimation;lipkin meshkov;meshkov glick;maximum likelihood", "pdf_keywords": ""}, "629323c5b9f7c64afac9300212538e488569bd1e": {"ta_keywords": "word embeddings ontology;embeddings ontology;presents ontology induction;ontology induction;dictionaries establishes semantic;embeddings ontology paper;structured lexical knowledge;synonym dictionaries establishes;lexical knowledge;establishes semantic relations;lexical knowledge synonym;knowledge synonym dictionaries;semantic relations structures;semantic relations;ontology;ontology induction approach;semantic;structured lexical;establishes semantic;word embeddings;presents ontology;synonym dictionaries;framework embedding word;extracts structured lexical;embedding word embeddings;embedding word;dictionaries establishes;atom harmonic oscillator;using word embeddings;lexical", "pdf_keywords": ""}, "33aa6c70eac0e4b7eb28d8386e5e4113fdd55203": {"ta_keywords": "question answering qa;question answering;automatic question answering;answering qa;given question answers;modular automatic question;automatic question;answer question probability;question answers;questions world;questions;question answers multiplechoice;answering qa regulation;answers multiplechoice questions;answering;qa;answer question;exam;cosmic microwave;answers multiplechoice;cosmic;history entrance exam;answer given question;answers;spectrum cosmic microwave;cosmic microwave background;questions world history;approach answer question;energy spectrum cosmic;entrance exam", "pdf_keywords": ""}, "2e820673ca861a9ece8d36f2b93793b5d2c7e1da": {"ta_keywords": "cryptanalysis quantum key;cryptanalysis quantum;framework cryptanalysis quantum;quantum key distribution;paper propose cryptanalysis;quantum key;cryptanalysis;propose cryptanalysis;cryptanalysis framework;propose cryptanalysis framework;cryptanalysis framework cryptanalysis;key distribution qkd;aerobicobicobic block ciphers;framework cryptanalysis;ciphers;ciphers used;block ciphers;key distribution;quantum;ciphers used improve;distribution qkd;block ciphers used;qkd;qkd present simple;distribution qkd present;qkd present;bird noisy environment;bird noisy;medical imaging systems;deterministically steering bird", "pdf_keywords": ""}, "49d415cf593be38c6cd97a183dadc7d7b48bab72": {"ta_keywords": "growth artificial intelligence;artificial neural;firm growth artificial;based forward backward;evidence artificial intelligence;faster employment growth;algorithms artificial neural;labor firms;labor firms results;innovations firm labor;neural networks parse;artificial intelligence;backward fbs;new force based;artificial intelligence entered;parse text patent;backward fbs code;concept force backward;forward backward fbs;artificial intelligence related;force based forward;employment growth;neural networks;text patent comparative;artificial neural networks;demand labor firms;faster revenue growth;force backward forward;automation effect;automation effect large", "pdf_keywords": ""}, "2225950d1d3e02bc0d88a0c78325d00e0122b576": {"ta_keywords": "deep learning separation;speech separation end;adapts separation recognition;speech separation;learning separation recognition;recognition overlapping speech;speech separation methods;methods speech separation;asr speech separation;learning separation;separation recognition components;separation recognition;speech recognition single;separation recognition overlapping;overlapping speech signals;components speech recognition;multiple components speech;overlapping speech;simultaneous speech signals;multiple simultaneous speech;separation methods deep;simultaneous speech;speech recognition;synergistically adapts separation;speech recognition asr;automatic speech;jointly trained deep;components speech;automatic speech recognition;separation", "pdf_keywords": ""}, "05fb5a180214bf092eeda30baf9f16fb6bd15727": {"ta_keywords": "modified speech parameter;temporal warping waveform;speech phoneme insertion;speech modified;speech modified speech;generating corrected speech;speech parameter;speech parameter sequence;duration speech phoneme;speech significantly degrades;warping waveform segments;warping waveform;control duration speech;speech phoneme;modified speech;modify durational patterns;temporal warping;quality corrected speech;corrected speech modified;waveform;non native speech;native speech significantly;native speech handled;duration speech;phoneme insertion;caused temporal warping;phoneme insertion observed;using modulation spectrum;native speech;non gaussian patterns", "pdf_keywords": ""}, "649c1148439a4e875dab414ba67bf8c80350af4a": {"ta_keywords": "neural semantic parser;semantic parsing code;semantic parser;semantic parsing;semantic parser maps;different semantic parsing;formal meaning representations;parsing code generation;syntax description language;neural semantic;parser;transition based neural;based neural semantic;parsing;abstract syntax description;parser maps natural;description language;abstract syntax specification;syntax description;parsing code;description language target;maps natural language;based abstract syntax;syntax specification;syntax target mr;parser maps;syntax specification corresponding;utterances formal meaning;language utterances formal;natural language utterances", "pdf_keywords": "semantic parsing code;parsing code generation;neural semantic parsing;syntax parser semantic;semantic parsing tasks;parser semantic;code generation semantic;semantic parsing model;semantic parsing;parser semantic parsing;language semantic parsing;abstract syntax parser;semantic machine translation;generation semantic machine;semantic parsing based;framework semantic parsing;syntax parser;generation semantic;parsing based hybrid;syntax description language;syntax target machine;parsing code;parser;abstract syntax description;present semantic parsing;parsing tasks;parsing;parsing model;parsing based;machine translation"}, "86d84c1c9b0a500f930696ab27c83a4b30477560": {"ta_keywords": "baselines paraphrased translations;paraphrastic sentence embeddings;learning paraphrastic sentence;learning paraphrastic;paraphrased translations simple;paraphrased translations;baselines paraphrased;para phrase corpora;sentence embeddings directly;paraphrastic;creating para phrase;cross lingual tasks;paraphrastic sentence;sentence embeddings;lingual tasks outperforms;methodology learning paraphrastic;lingual tasks;art baselines paraphrased;para phrase;phrase corpora;cross lingual;applied cross lingual;paraphrased;lingual;embeddings directly bitext;corpora;embeddings directly;translations;translations simple;translations simple simple", "pdf_keywords": "monolingual semantic similarity;paraphrastic sentence embeddings;bilingual sentence representations;similarity cross lingual;embeddings semantic textual;cross lingual similarity;sentence embeddings semantic;learning paraphrastic;sentence embeddings directly;sentence embeddings translated;learning paraphrastic sentence;semantic similarity cross;lingual similarity;semantic textual similarity;sentence embeddings;sentence encoder faster;creating paraphrase corpora;textual similarity semantic;embeddings semantic;sentence representations encode;lingual monolingual semantic;lingual similarity primary;similarity semantic similarity;representations encode sentences;semantic similarity;setting learning paraphrastic;performant sentence embeddings;embeddings translated;textual similarity;encoders monolingual"}, "65c53ed3575e160eb1e7d0a516353ba52de7e7e5": {"ta_keywords": "auctions bid leakage;leakage particular auction;bid leakage detection;sealed bid auctions;detect bid leakage;bid auctions;russian procurement auctions;auctions exposed bid;auctions exposed;auctions bid auctions;auctions bid;class auctions;procurement auctions 2014;class auctions bid;procurement auctions;new class auctions;bid auctions bid;auction bid;detect bid;auction;auctions;particular auction;bid leakage propose;bid leakage;bid leakage price;bidders extract;given bidders extract;given bidders;bid leakage particular;auction bid prices", "pdf_keywords": "probabilities bid leakage;leakage online auctions;probability bid leakage;bid leakage detection;bid leakage based;estimates bid leakage;bid leakage assumed;bid leakage online;bid leakage;auctions based hypothesis;post bid leakage;bid leakage used;russian procurement auctions;model bid leakage;predicting probability bid;estimating probability bid;procurement auctions;procurement auctions 2014;auctions method;bid leakage speci;probabilistic model bid;known advance auctions;auctions method provides;based hypothesis bids;auctions;auctions based;hypothesis bids;probability bid;probabilities bid;hypothesis bids timing"}, "a9e6222e71dd101d444b7192b3a0636c71edb0a4": {"ta_keywords": "ranking approach answering;corpus virtual knowledge;consider task answering;task answering;knowledge bases;virtual knowledge base;mentions entities corpus;knowledge base pretraining;existing knowledge bases;natural language inputs;presents ranking approach;presents ranking;knowledge base;answering question order;task answering complex;ranking;explores textual data;entities corpus;question order team;virtual knowledge;natural language;ranking approach;contextual representations;capable processing queries;index contextual representations;contextual representations mentions;answering question;paper presents ranking;textual data;inner product search", "pdf_keywords": ""}, "6536f36648d39f0f9f6105562f76704fcc0b19e8": {"ta_keywords": "persistent spatial semantic;representation language robot;language robot actions;tasks robotic agents;spatial semantic representation;robot actions long;robotic agents;spatial semantic;persistent spatial;propose persistent spatial;coherence entanglement;language robot;robot actions;agent performs hierarchical;coexistence coherence entanglement;representation language;robotic agents evaluate;semantic representation;demonstration coexistence coherence;coherence entanglement dimensional;robot;term tasks robotic;coherence;coherence time;tasks robotic;coherence obtain coherence;performs hierarchical reasoning;entanglement;expressive interface;robotic", "pdf_keywords": "integrating semantic voxel;environments robots trained;manipulation task learning;semantic voxel map;semantic voxel;decomposition semantic segmentation;semantic segmentation able;robots trained;semantic segmentation;robots trained training;home environments robots;task learning parse;robots increasingly;natural language commands;robotics;environments robots;representation hierarchical;approaches robotics autonomous;robots;scene set semantic;hierarchical representation;representation hierarchical representation;automatically segmenting scene;robotics autonomous;learn models following;learning parse;approaches robotics;challenge computer vision;learning parse natural;parse natural language"}, "2c0f2a03c3a427cc61359b5e2c31cfefe9850a31": {"ta_keywords": "unsupervised information extraction;information extraction;concept names clusters;information extraction method;clustering terms html;extracting sets entities;names clusters using;extracting sets;efficiently extract;method extracting sets;names clusters;concept instance pairs;clustering terms;approach clustering terms;web using unsupervised;efficiently extract large;concept names;unsupervised information;large corpus propose;propose method extracting;corpus propose method;assigning concept names;using unsupervised information;concept instance;novel approach clustering;terms html tables;large corpus;method efficiently extract;pairs large corpus;clusters using hearst", "pdf_keywords": "web documents clustering;extract semantic classes;term clustering;automatically extract semantic;term clustering combines;documents clustering;hypernyms semantic partitions;web websets retrieval;extract semantic;clustering technique web;semantic classes structured;term cluster;websets retrieval;structured web documents;semantic classes;extract information unstructured;web tables hyponym;classes structured web;tables hyponym patterns;corpora hypernyms semantic;semantic partitions;documents clustering series;unstructured lists web;websets retrieval webpages;semantic partitions terms;coordinate term clustering;search engines extract;hypernyms semantic;corpora hypernyms;lists web websets"}, "ed2cc779c7eb0004bd6dd50538a2cafca092c94f": {"ta_keywords": "spelling normalization historical;normalization historical texts;spelling normalization;manuscript normalization;problem spelling normalization;accuracy manuscript normalization;manuscript normalization raised;modern speech taggers;normalization historical;based descriptors normalization;speech taggers;processing modern speech;descriptors normalization normalized;descriptors normalization;character based descriptors;normalization;normalized;normalization normalized;normalization normalized data;normalized data pos;speech taggers present;normalization raised;normalized data;german texts;taggers present simple;word based character;historical german texts;texts regard processing;texts;use word based", "pdf_keywords": ""}, "5bcd9117899bc2c91db83532dcf587b9d8f8888b": {"ta_keywords": "electroluminescence spectrometer;performance electroluminescence spectrometer;electroluminescence;field performance electroluminescence;performance electroluminescence;spectrometer;external electric field;presence external electric;free speech law;electric field;speech law religion;external electric;free speech;electric;electric field performance;law religion paper;law religion;survey free speech;religion paper present;speech law;religion paper;field performance;effect presence external;field;law;religion;presence external;speech;external;effect presence", "pdf_keywords": ""}, "3d1cfefdbe40f7535ada772c260c192bb63bb9fe": {"ta_keywords": "document similarity tasks;textual supervision;relatedness provide textual;document similarity;similarity tasks;similarity tasks datasets;learning match aspects;source supervision sentences;aspects papers citations;similarity large scienti\ufb01c;textual descriptions cited;textual supervision used;learn structure textual;finding similarity large;textual;structure textual texts;structure textual;sentences text papers;form textual supervision;cited papers related;textual texts;source supervision;large scienti\ufb01c corpora;performance document similarity;supervision sentences text;aspect matching;descriptions cited papers;text papers cite;provide textual descriptions;finding similarity", "pdf_keywords": "similarity based deep;similarity leverage textual;similarity implicit textual;source document similarity;document similarity implicit;document similarity;similar sentence embeddings;enhance abstract similarity;source textual supervision;text supervised;abstract similarity based;sentence embeddings;terms contextualized sentences;textual supervision describing;contextualized sentences;trained text supervised;disentangled representations texts;implicit textual supervision;learning disentangled representations;abstract similarity;text supervised single;textual supervision;similarity computer vision;sentence embeddings used;cited sentence similar;abstract similarity leverage;leverage textual descriptions;contextualized sentences work;citation contexts;describing related"}, "5e74d4e041a25e7752a596e2891975df5ba65aa2": {"ta_keywords": "beamforming speech recognition;singlechannel speech enhancement;beamforming improve speech;microphones covariance prediction;beamforming speech;speech enhancement deep;singlechannel speech;mask microphones covariance;multi microphone speech;microphone speech;use singlechannel speech;beamforming method speech;mvdr beamforming speech;microphone speech databases;speech enhancement;beamformer focus speech;improve speech recognition;single mask microphones;speech noise spatial;microphones covariance;obtain speech noise;mask microphones;speech databases;measures speech recognition;speech recognition;speech noise;speech recognition recent;masks obtain speech;multi microphone;speech unknown acoustic", "pdf_keywords": ""}, "a0ab4106dabd6bc067c7b3e4db06807e2c0f6036": {"ta_keywords": "pretretrained language models;pretrained language models;magnitude pretretrained language;pretretrained language;pretrained language;outperforms pretrained language;pretraining given labeled;large general corpus;language models;corpus jointly optimizes;model outperforms pretrained;general corpus tlm;large scale pretraining;simple efficient learning;language modeling;efficient learning framework;corpus tlm;language modeling objective;general corpus;efficient learning;labeled task data;objective language modeling;corpus;language models terms;pretraining;pretraining given;learning;learning framework;labeled task;outperforms pretrained", "pdf_keywords": "training large corpus;large corpus;large scale language;resources majority nlp;resource entailment tasks;large corpus consisting;high resource entailment;scale language models;language models;task drivenlanguage modeling;language models resort;entailment tasks;domain adaptive machine;small task data;task drivenlanguage;corpus;nlp researchers afford;training large scale;nlp researchers;resource entailment;entailment tasks addition;texts larger batch;majority nlp researchers;job machine learning;english texts larger;framework task drivenlanguage;corpus consisting 160gb;domain adaptive;adaptive machine learning;drivenlanguage modeling tlm"}, "a4f2e6c38454c9e7b4068a456813d622b91f2663": {"ta_keywords": "xmath0 particle kurtosis;speech diadochokinetic ddk;speech diadochokinetic;particle kurtosis;particle kurtosis presented;norms speech diadochokinetic;kurtosis presented results;diadochokinetic ddk rates;kurtosis presented;diadochokinetic ddk;kurtosis;calculating kinesin kinetics;measurement xmath0 particle;diadochokinetic;kinesin kinetics;kinesin kinetics presence;measurement xmath0;kinetics presence anisotropy;compared results maidanoff;calculating kinesin;protocol measurement xmath0;results maidanoff;maskawa kkm test;method calculating kinesin;xmath0 particle;presence anisotropy;results maidanoff kobayashi;kinetics;kinetics presence;ddk rates", "pdf_keywords": ""}, "d408be961d0db8b97c0ca6b2fc7afd3c9dc914e7": {"ta_keywords": "mobility planning applications;platform mobility planning;mobility platform;mobility planning;available mobility options;modal mobility platformmobilytics;mobility planning application;mobile mobility;mobility options;extensible mobility;mobility platform state;available mobility;mobility platformmobilytics integrates;extensible mobility new;mobility platformmobilytics;mobile mobility platform;platform mobility;new mobility modes;state art mobility;integrates mobility providers;integrate available mobility;new mobile mobility;mobility;new mobility;platformmobilytics integrates mobility;mobility modes;mobility providers;mobility providers commuters;mobility modes new;multi modal mobility", "pdf_keywords": ""}, "4fe70c172cc38c2eb15103f0f1eac4e6766c60e6": {"ta_keywords": "voice activity detection;voice activity;new voice activity;gaussian pruning weight;based gaussian pruning;gaussian pruning;pruning weight normalization;activity detection method;activity detection;weight normalization;voice;method based gaussian;new voice;propose new voice;pruning weight;normalization;based gaussian;gaussian;pruning;detection method based;detection method;activity;detection;method based;weight;method;new;based;propose new;propose", "pdf_keywords": ""}, "efe9fe804f34b18524708b18293508191bda78eb": {"ta_keywords": "redundant hardware severe;redundancy tmr program;transient permanent faults;faults energy consumption;redundancy tmr;modular redundancy tmr;deactivate faulty components;consumption redundant hardware;permanent faults;core tmr provides;redundant hardware;presence permanent faults;tmr core;core tmr;consumption tmr core;introduce reactive tmr;tmr core tmr;permanent faults triple;reactive tmr;faults energy;hardware severe detriment;detect deactivate faulty;reactive tmr novel;hardware severe;faults;power consumption redundant;energy consumption tmr;tmr provides desirable;faulty components;faults triple modular", "pdf_keywords": ""}, "395044a2e3f5624b2471fb28826e7dbb1009356e": {"ta_keywords": "sentence embeddings supervised;paraphrastic sentence embeddings;pretrained sentence embeddings;supervision paraphrase database;sentence similarity;sentence embeddings based;supervised nlp tasks;sentence embeddings;perform sentence similarity;tasks sentence similarity;paraphrase database;similarity entailment sentiment;supervised nlp;sentence similarity entailment;embeddings supervised tasks;similarity entailment tasks;nlp tasks;entailment sentiment classification;experiments supervised nlp;embeddings supervised;short term memory;recurrent neural networks;nlp tasks sentence;term memory recurrent;paraphrase database ganitkevitch;word averaging models;embeddings based supervision;similarity entailment;sick similarity entailment;memory recurrent neural", "pdf_keywords": "learns sentence representations;word embeddings;sentence representations domains;sentence representations;word similarity tasks;embeddings generalize better;semantic textual similarity;embeddings improve;similarity entailment sentiment;learned ppdb embeddings;effectiveness word embeddings;embeddings improve performance;ppdb embeddings improve;similarity tasks semantic;textual similarity entailment;similarity entailment models;textual similarity;embeddings generalize;text similarity entailment;learns sentence;22 embeddings generalize;word embeddings paper;word compositionality fundamental;word similarity;recurrent neural network;results 22 embeddings;deep recurrent;embeddings able match;embeddings;deep recurrent neural"}, "14551d2bf2584bb1ea7ad69f9a64419bab82bb6e": {"ta_keywords": "sound event detection;features audio data;sound event;features audio;local features audio;presents sound event;foreground sound;sound foreground;foreground sound background;audio data effectively;sound foreground sound;background sound foreground;example background sound;background sound;audio data;background sound decomposed;background sound experimental;decomposed background sound;sound decomposed background;sound background;sound background sound;audio;learning data augmentation;data augmentation;sound experimental results;dcase2020 task4 dataset;sound decomposed;presents sound;event detection;semi supervised", "pdf_keywords": ""}, "469ad889bd628e2cf46424f7097c4830719ec740": {"ta_keywords": "vowel space estimates;automatic vowel space;estimation talker vowel;vowels estimated;talker vowel space;estimated handcorrecting vowels;vowels intelligible;ability vowels estimated;vowels estimated handcorrecting;intelligible point vowels;vowels intelligible point;vowel space representations;impractical vowels intelligible;vowel space using;vowel space;vowel space expansion;expanded vowel space;predictive ability vowels;vowel space making;estimate complexity speech;range vowel space;vowels difference talker;complexity speech recognition;analysis impractical vowels;automatic vowel;talker vowel;handcorrecting vowels measured;vowels measured;speech recognition;vowels measured carefully", "pdf_keywords": ""}, "d9944e13a38e5ca685985c9b5c050ec6d300e104": {"ta_keywords": "communication interactive multimedia;verbal communication interactive;interactive multimedia application;interactive multimedia;communication interactive;non verbal communication;multimedia application;multimedia application common;verbal communication;multimedia;interactive;non verbal;communication;artificial systems;verbal;natural artificial systems;feature natural artificial;application;systems;application common;application common feature;artificial;common feature natural;feature natural;natural;natural artificial;common feature;feature;non;common", "pdf_keywords": ""}, "ba602ea9aaecab5a3ad243211f110ae7db4cc66a": {"ta_keywords": "repeated risk minimization;risk minimization performative;risk maximization data;performative risk minimizers;performative risk maximization;risk minimization;minimization performative risk;maximization data driven;risk minimizers;risk maximization;minimizers performative risk;risk minimization perturbed;identifying bank loan;classifier;analyze repeated risk;data driven;mapping classifier data;data driven methods;approach classify;mapping classifier;classifier data;maximization data;novel approach classify;risk minimizers paper;bank loan;classifier data distribution;identifying bank;classify;classify closed loop;flows performative risk", "pdf_keywords": ""}, "ef6a4d8bf248944ca1d0cfdc107d3bb107f57bff": {"ta_keywords": "population animals action;behavior population animals;action external perturbation;population animals;external perturbation;perturbation;animals action external;animals action;behavior population;animals;behavior;population;study behavior population;action external;action;study behavior;paper study behavior;paper study;external;study;paper", "pdf_keywords": ""}, "6c68866e6486923d2e8b999de57d450c9d4febab": {"ta_keywords": "neural machine translation;machine translation nmt;speed human translation;machine translation;human translation using;improve translation quality;human translation;translation quality;outputs improve translation;translation nmt;translation rule tables;language translation rule;phrase based decoding;translation nmt used;decoding cost nmt;improve translation;translation rule;based language translation;translation quality different;language translation;phrase based soft;translation using phrase;decoding cost rerank;decoding algorithm phrase;address question neural;phrase based smt;soft forced decoding;translation using;question neural machine;rerank nmt outputs", "pdf_keywords": ""}, "c2b22a18ea2ed444c6c1f5bb27ab55bda2b44567": {"ta_keywords": "model speech translation;language emphasis prediction;speech translation systems;speech translation based;model speech;translation based neural;emphasis prediction conditional;emphasis prediction;new model speech;speech translation;paralinguistic information experiments;paralinguistic information;translation systems;traditional speech translation;model target language;prediction conditional random;networks traditional speech;target language;paralinguistic;target language emphasis;simulation neural;computer simulation neural;neural network trained;simulation neural network;conditional random fields;translation based;oblivious paralinguistic information;conditional random field;speech;neural network", "pdf_keywords": ""}, "b799d66c710dd82a1b925b9c31e55a0d2d99b624": {"ta_keywords": "urban dynamics;urban dynamics urban;dynamics urban;dynamics urban region;urban activities;population urban space;mobility dataset;life mobility dataset;rhythms urban activities;urban activities paper;mobility dataset highly;population urban;daily rhythms urban;hidden markov models;sharing hidden markov;real life mobility;size population urban;hidden markov;dynamics human activities;urban;life mobility;rhythms urban;mobility;urban region characterized;urban space;urban space evaluate;temporal dynamics human;markov models;region state sharing;urban region", "pdf_keywords": ""}, "0c39c0dc296a902e4a5eb85182209f7b9e6053b0": {"ta_keywords": "networks dynamic;networks dynamic ones;vertex centric programming;static graph optimization;static networks dynamic;graph optimization;expensive graph construction;static networks;strategies present vertex;networks;expensive graph;dynamic team dynamics;modeling network;deep learning;team dynamics models;training various dynamic;graph optimization techniques;vertex centric;team dynamics;deep learning dl;dynamic nns demonstrate;modeling network structure;trees graphs cavs;frameworks dynamic nns;graphs;static graph;performance cern phytoplankton;network structure;dynamic nns;graph construction preprocessing", "pdf_keywords": "dynamic deep learning;dynamic deep;train dynamic tensor;deep learning;ef\ufb01cient dynamic deep;graph execution optimizations;dynamic tensor address;dynamic tensor;deep learning cavs;graphs useful dynamic;vertex centric programming;graph execution;neural network architecture;deep convolutional neural;structure dynamic tensor;dynamic nns;dynamic nns implementation;tensor address challenge;dynamic tensor investigate;data structure dynamic;developing deep convolutional;training inference address;deep convolutional;convolutional neural;declaration execution graphs;convolutional neural network;developing deep;vertex centric;tensor address;neural"}, "57026b2d45fa59c6326b5a1d2e27626403f083ba": {"ta_keywords": "artificial intelligence ethics;ethics artificial intelligence;intelligence ethics;intelligence ethics general;artificial intelligence courses;artificial intelligence course;ethics artificial;students artificial intelligence;ethics general artificial;intelligence courses;intelligence course;intelligence courses note;artificial intelligence society;moral ethical;moral ethical philosophical;intelligence course recent;ethical philosophical;students artificial;ethics;artificial intelligence practitioners;intelligence society;ethical;ethical philosophical issues;understand moral ethical;impacts artificial intelligence;intelligence practitioners;issues artificial intelligence;intelligence;ethical philosophical impacts;ethics general", "pdf_keywords": "ai ethics;\ufb01cent intelligence ethics;intelligence ethics course;ethics based game;integrate ai ethics;intelligence ethics;ai ethics general;arti\ufb01cial intelligence course;ethics course;knowledge ethical;ethical theory;ethics;ways ethics;thinking ethics based;moral ethical;thinking ethics;ethical considerations propose;knowledge ethical theory;ways ethics work;ethical;moral ethical philosophical;ethics based;arti\ufb01cial intelligence;ethical philosophical;ethical considerations;students arti\ufb01cial intelligence;illustrates ways ethics;understand moral ethical;arti\ufb01cial intelligence practitioners;ethical theory certain"}, "653add540adae12491fade7e18ec4e1e4288b4a7": {"ta_keywords": "academic advising;decision theoretic advising;support academic advising;selection advising career;course selection advising;selection advising;advising career paths;advising career;advising;theoretic advising support;university student advisor;person advising;theoretic advising;student advisor;person advising conducted;advising support tool;advising conducted;advising support;student advisor relationship;replace person advising;students university california;support tool undergraduates;undergraduate students;course selection;university california berkeley;undergraduate students university;career paths software;concerning course selection;state university student;university student", "pdf_keywords": "academic advising;mathematical model advising1;academic advising present;guidance undergraduates;guidance undergraduates large;aspects academic advising;advise undergraduate students;advising;theoretic guidance undergraduates;model advising1;advisor recommendation based;advise undergraduate;staff advise undergraduate;experts advisors;academic advisor recommendation;domain experts advisors;productive person advising;advisor recommendation;person advising;students human advisors;advising1;advising present decision;model advising1 recent;advising1 recent years;user study encompassing;advising present;experts advisors developed;advising sessions;advising1 recent;user study"}, "6fe62b967376361d7cd55e1033ab968895841d67": {"ta_keywords": "learning text mining;deep learning text;text mining;concepts text data;learning text;fcm interpretable deep;concept discovery;focusing concept discovery;text explain business;concept discovery study;focused concept miner;concepts text;concept impact baselines;concepts text explain;classifiers compared interpretable;unknown concepts text;evaluate fcm text;fcm text data;text mining algorithm;interpretable deep learning;interpretability concept impact;interpretability accuracy tradeoff;blackbox classifiers article;interpretable deep;fcm interpretable;concept impact;concept correlational importance;corpus level concepts;discovery study interpretability;interpretability accuracy", "pdf_keywords": ""}, "29001ac04e61dfffb8e24ffd3e351ece12ce44af": {"ta_keywords": "speaker separation matching;independent speaker separation;speaker separation;framework speech enhancement;speech enhancement source;speech enhancement;phase representations phasebook;representations phasebook layers;phasebook layers;channel speaker independent;phase locked signals;enhancement source separation;learning framework speech;detection phase locked;phase reconstruction;phasebook layers used;waveguides phase mixture;phasebook;discrete phase representations;phase representations;channel speaker;optical waveguides phase;representations phasebook;waveguides phase;framework speech;additional phase reconstruction;single channel speaker;remove phase;source separation systems;corpus single channel", "pdf_keywords": "mask inference networks;separation speech enhancement;speech separation;approach speech separation;speaker separation based;softmax activation magnitude;softmax;speech separation speech;speech enhancement;softmax activation;softmax layer;multi speaker separation;mask inference;speech enhancement paper;convex softmax activation;learns magnitude;speaker separation;complex mask directly;magbook softmax layer;magbook softmax;complex mask;separation speech;softmax layer build;learns magnitude target;model complex mask;codebook magbook softmax;concept convex softmax;present deep learning;convex softmax;deep learning"}, "5dce0fd43a21825bebd8121fd0a28155d524c44c": {"ta_keywords": "death unrelaxed state;unrelaxed state sudden;state sudden;state sudden birth;sudden death unrelaxed;unrelaxed state;competition sudden death;death unrelaxed;birth new state;sudden death;competition sudden;new state;state;sudden birth;sudden birth new;study competition sudden;unrelaxed;competition;death;birth;birth new;sudden;paper study competition;study competition;new;paper study;paper;study", "pdf_keywords": ""}, "4b2d583e22f378f9104814d9f63cda411ddd5825": {"ta_keywords": "lexical sememe prediction;predict sememes words;lingual sememe prediction;sememe based linguistic;lingual lexical sememe;semantic space sememe;automatically predict sememes;sememes words languages;lingual lexical;sememes multi lingual;lexical sememe;predict sememes;cross lingual lexical;linguistic knowledge bases;prediction sememe based;low dimensional semantic;sememes words;prediction sememe;sememe prediction letter;sememe prediction aiming;sememe prediction;sememe prediction sememe;linguistic knowledge;lingual words;dimensional semantic space;lingual sememe;dimensional semantic;space sememe prediction;cross lingual sememe;multi lingual words", "pdf_keywords": ""}, "f1513d72cb5dd6d70541cce0da36b77467128d13": {"ta_keywords": "xmath1 heisenberg antiferromagnet;heisenberg antiferromagnet;heisenberg antiferromagnet square;antiferromagnet square lattice;antiferromagnet square;xmath1 heisenberg;xmath0 xmath1 heisenberg;antiferromagnet;electronic structure xmath0;lattice paper describes;heisenberg;lattice;lattice paper;structure xmath0;structure xmath0 xmath1;field described brownian;square lattice;square lattice paper;xmath0;gravity field described;electronic structure;described brownian motion;study electronic structure;xmath1;xmath0 xmath1;particle action gravity;particle action;described brownian;particle;action gravity field", "pdf_keywords": ""}, "838fbfd9066dbbac6c10059c5b183046fb1cd9d1": {"ta_keywords": "bayesian active learning;active learning;outperform active learning;deep active learning;active learning al;active learning shown;bayesian active;study deep active;active learning addressing;deep active;empirical study deep;luxury bayesian active;outperform active;learning;supervised;learning shown outperform;al process learning;learning natural;supervised learning;large scale empirical;acquisition functions supervised;process learning natural;process learning;learning natural language;learning addressing;shown outperform active;learning addressing multiple;functions supervised learning;study deep;learning al", "pdf_keywords": "deep active learning;active learning bayesian;sequence tagging tasks;prediction sequence tagging;bayesian active learning;bayesian deep;active learning addressing;learning bayesian deep;propose deep bayesian;bayesian deep learning;deep bayesian;sequence tagging;active learning;study deep active;network active learning;promising approach deep;nlp tasks models;deep active;active learning promising;learning addressing;deep bayesian convolutional;nlp tasks;bayesian active;learning addressing multiple;learning bayesian;approach deep;deep learning;bayesian convolutional neural;tasks restricted annotation;tagging tasks"}, "5693c74eb8ffde1490ba480fdc963f008243906a": {"ta_keywords": "sequence tagging tasks;sequence tagging;crowd annotations;annotating large annotators;large annotators;rapid tagging;annotations real time;annotation framework;crowd annotations real;consolidation crowd annotations;annotation framework sequence;sequence labeling tasks;annotating annotating large;new approach annotating;annotations;annotating;annotating annotating;framework sequence tagging;annotators;data annotation framework;data annotation;annotation;approach annotating annotating;tagging;rapid tagging recommendations;annotating large;sequence labeling;tagging tasks;approach annotating;ranging rapid tagging", "pdf_keywords": ""}, "a8239258abded4f08d1bf270c2e86662f4dc1760": {"ta_keywords": "skills decreases learning;differences prior knowledge;learning process affected;prior knowledge strong;prior knowledge learning;decreases learning;prior knowledge;learned skills decreases;weak prior knowledge;general prior knowledge;investigate learning;model learning called;investigate learning complex;decreases learning rate;student learning;learning called;prior knowledge encoding;specific prior knowledge;data student learning;learning process complex;learning complex skill;learning;learning complex;accuracy learned skills;accuracy learned;model learning process;learning rate significantly;learning process;student learning time;prior knowledge affect", "pdf_keywords": ""}, "a182a8a0678857df5c513d52469fa707c32e69ec": {"ta_keywords": "translation rules based;statistical machine translation;translation rules;machine translation;rule selection models;appropriate translation rules;machine translation smt;rule selection;based rule selection;space rule selection;baseline rule selection;dependent rule selection;rule selection csr;rules based sentence;rule selection contrast;better generalization keywords;words better generalization;compared nonminimal rules;words features model;rule selection paper;translation tasks;translation smt choose;different translation tasks;nonminimal rules;entropy based rule;rule selection mers;generalization keywords;train rule selection;models minimal rules;minimal rules", "pdf_keywords": ""}, "4e3935ef7da6bcbb202ec7f8b285c313cadcd044": {"ta_keywords": "question answering datasets;information seeking qa;seeking question answering;question answering;answering datasets;answering datasets usually;factoid type information;existing information seeking;qa tasks;models qa tasks;questions generic factoid;seeking qa dataset;qa dataset designed;paper answer question;answering questions;qa dataset;information seeking question;information seeking;qa tasks perform;seeking qa;database paper answer;perform answering questions;information extracted;document grounded information;grounded information seeking;existing models qa;factoid type;tasks perform answering;tools complex reasoning;usually contain questions", "pdf_keywords": "question answering dataset;question answering evidence;performance question answering;question answering;seeking question answering;answering evidence selection;answering dataset academic;questions answerable annotators;answer large corpus;answerable annotators;answering dataset;answers questions;answerable annotators asked;answering questions;answering evidence;generating answer text;answer text;answering;evidence required answering;answers questions answerable;required answering questions;large corpus text;questions answerable;answer text present;providing answers questions;reference evidence;evidence selection tasks;reference evidence considering;natural language query;references present information"}, "85e148ac629b1b38556c5fe5f8d657f2eb01a701": {"ta_keywords": "trade union iact;international trade union;2014 international trade;trade union;international trade;union iact;iact;analysis data collected;data collected summer;analysis data;2014 international;summer 2014 international;data collected;results analysis data;present results analysis;data;international;results analysis;collected summer 2014;analysis;union;summer 2014;present results;2014;paper present results;summer;results;collected summer;trade;collected", "pdf_keywords": ""}, "a997d6e253f08a3e589432c611d6d2a3097d7629": {"ta_keywords": "online research tool;tool research exchange;software open ended;collaborative online research;describes open source;integration openreview apis;openreview apis;online research paper;research tool designed;open source;open source software;research tool;research paper tool;tool research;online research;issues collaborative online;collaborative online;openreview apis large;paper tool research;collaborative paper;prototype http researchexchange;source software open;integration openreview;efficacy online research;http researchexchange;openreview;usability studies;apis;development efficacy online;item collaborative paper", "pdf_keywords": ""}, "3ec37205c9201fc891ab51da200e361fdc34bfb3": {"ta_keywords": "trained word embeddings;word embeddings;word embeddings representation;vocabulary tokens;representation vocabulary tokens;words added word;embeddings representation vocabulary;deep learning;calculating number words;performance deep;vocabulary tokens test;number words added;performance deep learning;study performance deep;deep learning algorithm;learning algorithm reading;representation vocabulary;number words;pre trained word;added word word;words added;embeddings;word word;vocabulary;words;trained word;word word use;reading book;word;reading", "pdf_keywords": "recurrent entity networks;2015 machine comprehension;pretrained word embeddings;comprehension machine learning;machine comprehension machine;embeddings pre trained;reading comprehension tasks;pre trained embeddings;translation machine embeddings;embeddings assigning pretrained;machine comprehension;machine comprehension work;embeddings propose deep;comprehension tasks based;comprehension tasks;trained embeddings initialization;comprehension machine;training recurrent neural;pre training recurrent;word embeddings;utilizing embeddings pre;training recurrent;trained embeddings;network machine comprehension;recurrent entity;machine comprehension attracted;recurrent neural network;machine embeddings propose;utilizing embeddings;reading comprehension"}, "cfad4dc5f1f7fcaf7ca318acf672ad92d47f8413": {"ta_keywords": "cool liquid crystal;liquid crystal transition;liquid crystal;crystal liquid;liquid crystal liquid;crystal liquid crystal;efficiency industry tries;inequality employment circumventive;efficiency industry;crystal transition temperature;efficiency;crystal transition;maryland fluid dynamics;industry;inequality employment;diaphragms used cool;cool liquid;employment;industry tries;employment circumventive;paradox;paradox present study;dynamics;employment circumventive practices;crystal;counter diaphragms;liquid;address inequality employment;state maryland fluid;5ths rule illustrate", "pdf_keywords": "discriminatory practices poset;ethical hiring goals;addressing discrimination;bias screening decisions;bias screening;ethical hiring;applicants based poset;addressing discrimination a\ufb03rmative;discrimination;reduce bias screening;discriminatory practices;legal ethical hiring;diversity addressing discrimination;discrimination a\ufb03rmative action;discriminatory;screening applicants based;evaluations job candidates;illegal discriminatory practices;hiring goals;job candidates;hiring goals face;poset approach screening;discrimination a\ufb03rmative;illegal discriminatory;applicants based;equal opportunity employment;approach screening companies;candidate job;approach screening applicants;selection propose poset"}, "f889723a4427e914e4e32547dfd0ca4996170180": {"ta_keywords": "voice conversion challenge;converted speech evaluate;converted speech;generate converted speech;predict prosody linguistic;voice conversion;directly predict prosody;predict prosody;speech evaluate;automatic speech;text speech;transcribe source speech;prediction ttp voice;speech evaluate methods;use automatic speech;voice conversion vc;representation target speaker;input text speech;latest voice conversion;source speech;ttp voice conversion;speech tts;prosody linguistic representation;transferring prosodic clues;transferring prosodic;text speech tts;source speech underlying;speech tts generate;target text prediction;text prediction ttp", "pdf_keywords": "predict prosody linguistic;directly predict prosody;predict prosody;prosody modeling speech;text prediction ttp;prosody modeling methods;utterances spt ttp;prosody linguistic representation;target text prediction;speech utterances spt;prosody modeling;investigate prosody modeling;prosody linguistic;modeling speech;modeling speech utterances;representation target speaker;utterances spt;prediction ttp;text prediction;speech utterances;prediction ttp evaluate;linguistic representation target;prosody;target speaker dependent;speech recognition;utterances;speech;paper investigate prosody;investigate prosody;spt ttp"}, "f4cca8ea79e26fa20a91c3d3b769c9f7b82a6207": {"ta_keywords": "spherical microphone array;virtual spherical microphone;spherical microphone;microphone array method;microphone array;microphone array discussed;microphone array simulated;extraction pinna spectral;microphone;spectral notches median;pinna spectral notches;implementation virtual spherical;plane virtual spherical;virtual spherical;spectral notches;sphere virtual spherical;method extraction pinna;median plane virtual;fourier bessel;notches median plane;fourier bessel series;extract ps nuggets;using fourier bessel;pinna spectral;efficient median plane;notches median;bessel series fbs;bessel series;sphere virtual;computationally efficient median", "pdf_keywords": ""}, "8c38bffc058d558e7c734032ba63942865e05ae4": {"ta_keywords": "query answering;query answering algorithms;class query answering;complex queries;queries;queries incomplete kbs;answering algorithms;performance complex queries;answering algorithms called;complex queries incomplete;kb entities;wide range queries;embedding kb entities;queries incomplete;query;answer question answer;entities;answering;class query;algorithms called kbs;approach embedding kb;answer question;kb entities paper;new class query;range queries;kb answer;kbs able answer;state kb answer;kbs;new approach embedding", "pdf_keywords": ""}, "e4a6bc3ac385b8982bbbe0a2a5ac0c79101ec979": {"ta_keywords": "overgenerality known phenomenon;overgenerality known;overgenerality;phenomenon statistical physics;statistical physics;known phenomenon statistical;phenomenon statistical;phenomenon;statistical;known phenomenon;physics;known", "pdf_keywords": ""}, "9bbeb4f0e48032df19f9f6a08839da5d2e60e8eb": {"ta_keywords": "microphone dispute;detection entanglement sound;speech recognition asr;microphone microphone dispute;microphone dispute dispute;speech recognition;microphones challenging;recognition presence microphone;discriminative training speaker;wer dnn discriminative;microphone microphone;stereo microphones challenging;noisy automatic speech;microphone;environment speech applications;entanglement sound environment;microphones challenging desirable;dnn discriminative training;automatic speech;training speaker adaptation;speaker adaptation;distant stereo microphones;automatic speech recognition;dnn discriminative;speech applications;presence microphone microphone;home environment speech;speaker adaptation combination;environment speech;speech applications develop", "pdf_keywords": ""}, "8f963beca679cb1129df0a944c6de4b126e20fd5": {"ta_keywords": "memory cell state;decoding proposed fusion;memory lstm present;memory lstm;memory cell;term memory lstm;memory cells;memory cells hidden;cells hidden states;lstm present;short term memory;presence white dwarf;white dwarf wd;cell state hidden;white dwarf;state seq2seq decoder;lstm;hidden state seq2seq;lstm present new;shallow fusion;use memory cells;shallow fusion baseline;fusion baseline;confined optical lattice;cell state;fusion;magnetic fields electron;dwarf wd sequence;decoding;focus memory cell", "pdf_keywords": "speech recognition transfer;end speech recognition;multilingual asr mlasr;corpus transfer learning;recurrent neural networks;multilingual asr;librispeech corpus transfer;learning deep fusion;lingual asr setup;translation inference attention;mono lingual asr;setup multilingual asr;deep fusion multi;corpus transfer;deep fusion;speech recognition;deep recurrent neural;transfer learning deep;lingual asr;machine translation inference;based deep recurrent;seq2seq model decoder;machine translation;deep recurrent;recognition transfer learning;translation inference;transfer learning;recurrent neural;librispeech corpus;asr mlasr base"}, "b2c47dd46bf7087b754aed45f06b6196cf2b1c28": {"ta_keywords": "imaging acute abdomen;pediatric imaging;pediatric imaging appear;topics pediatric imaging;abdomen chapters imaging;acute abdomen;imaging acute;acute abdomen chapter;associated acute abdomen;diagnostic imaging acute;abdomen;acute abdomen chapters;sonographic computed;acute abdomen providing;icmp special pediatric;diagnostic imaging;radiographic sonographic;abdomen chapter present;analysis radiographic sonographic;radiographic sonographic computed;treat acute abdomen;pediatric imaging translation;sonographic;imaging appear;imaging;aspects diagnostic imaging;sonographic computed tomography;abdomen chapter;tomography findings;imaging techniques", "pdf_keywords": ""}, "46d87d4614d9353f1b7d527333073ef9109bfaea": {"ta_keywords": "rankings item labelings;label item ranking;fast algorithm item;labeling faster;algorithm item labeling;labelings superior accuracy;item labeling faster;pick correct labels;label set candidates;item labelings superior;item ranking users;users pick label;item ranking;fast algorithm;ranking users based;ranking users;user rankings;pick label;user rankings item;labelings superior;ranking;labelings;algorithmically various discrete;labeling;labels items compare;algorithmically;pick label set;problem crowd sourced;item labelings;algorithms", "pdf_keywords": ""}, "82ae0d4b41046ccedb435ece08a61f198cf77bb9": {"ta_keywords": "generating editing sentences;manipulation text content;generate sentence accurately;approach manipulation text;manipulation text;generating editing;generate sentence;editing sentences;textual attributes;visualizing structured;efficiently manipulate structured;text content automatic;manipulate structured data;text content;given textual attributes;editing sentences given;visualizing structured data;idea structured data;sentence aim generate;writing style;textual;writing style work;text;manipulate structured;sentences given textual;structured data;problem visualizing structured;aim generate sentence;structured;idea structured", "pdf_keywords": "text generation;natural language generation;content generation;text generation systems;sentence planning surface;data text generation;content generation sidesteps;end natural language;content planning sentence;embodiment text style;text style transfer;content style experiments;sentence planning;style embodiment text;paradigm content generation;arbitrary exemplar sentences;content style;language generation;growing text style;text style;embodiment text;planning sentence planning;style transfer;exemplar sentences;natural language;style embodiment;learns weak supervisions;content;exemplar sentences describ;control content style"}, "bc1832e8b8d4e5edf987e1562b578bd9aa5e18a9": {"ta_keywords": "reverberation characteristics utterance;utterance data augmentation;speech recognizer;robustness speech recognizer;utterance data;characteristics utterance data;speech recognizer deployed;robustness speech;improve robustness speech;analysis noise reverberation;similarity acoustic;noise reverberation characteristics;respect similarity acoustic;noise reverberation;similarity acoustic conditions;reverberation characteristics;data collected mike;reverberation;characteristics utterance;data augmentation;recognizer;acoustic;collected mike experiment;data augmentation simple;acoustic conditions;analysis noise;utterance;detailed analysis noise;speech;noise", "pdf_keywords": ""}, "e212f788c701370af02b138d2a61e180cddfb138": {"ta_keywords": "multi target translation;language multiple target;target translation strong;target translation;second target language;target language;multiple target languages;translating single source;source language multiple;grammars handle multiple;target language provide;context free grammars;single source language;target languages;translation strong language;free grammars;language multiple;free grammars handle;translating single;method translating single;strong language model;language model;target languages paper;source language;rule extraction scoring;grammars;language model similar;multi target;translation strong;rule extraction", "pdf_keywords": ""}, "c5dfc5fe7102fd8647edd1c9483aded82557e544": {"ta_keywords": "subpopulations generating recourse;recourses interpretability explanations;optimizes correctness recourses;generating recourse;recourses interpretability;generating recourse summaries;model biases discrimination;discrimination developing tools;minimizing overall recourse;correctness recourses interpretability;undesirable model biases;analyse interpret predictive;predictive model vet;evaluation real world;interpretability explanations minimizing;recourses corresponding black;recourses offers meaningful;biases discrimination;correctness recourses;recourses corresponding;recurrence times;recourse costs entire;recourse costs;recurrence reduced;recurrence time;model biases;interpret predictive;overall recourse costs;overview recourses;overall recourse", "pdf_keywords": "counterfactual explanations leveraged;counterfactual explanations;datasets satisfy recourse;recourses interpretability resulting;recourses interpretability;correctness recourses interpretability;optimizes correctness recourses;optimize recourse correctness;population counterfactual explanations;minimizing overall recourse;interpretability resulting explanations;explanations minimizing overall;optimize recourse;correctness recourses;costs interpretability;costs interpretability explanation;explanations minimizing;recourse correctness;recourse correctness coverage;coverage costs interpretability;jointly optimize recourse;exploratory data analysis;resulting explanations minimizing;diagnose model biases;model biases discrimination;explanations leveraged;counterfactual;recourses offers meaningful;questions recourse coverage;recourse coverage"}, "90ed32fa521b9e85f1c9efe356619814a2e79961": {"ta_keywords": "priori knowledge state;priori knowledge;priori information;terms priori information;knowledge state expressed;knowledge state;expressed terms priori;priori;knowledge;terms priori;information;state expressed terms;state expressed;expressed terms;terms;state;expressed", "pdf_keywords": ""}, "a75869d69cc86f501939c237ae4711aa2885f6a6": {"ta_keywords": "translation meta learning;low resource translation;resource machine translation;resource translation meta;multilingual transfer learning;translation meta;resource translation;high resource language;machine translation;multilingual high resource;languages source tasks;outperforms multilingual transfer;low resource languages;machine translation proposed;tasks diverse languages;resource language tasks;language tasks;meta learning;multilingual transfer;proposed meta learning;agnostic meta learning;source tasks diverse;significantly outperforms multilingual;meta learning strategy;languages target tasks;resource language;resource languages;meta learning problem;outperforms multilingual;diverse languages target", "pdf_keywords": "neural machine translation;multilingual machine translation;competitive machine translation;resource machine translation;models multilingual translation;multilingual translation systems;machine translation;universal machine translation;machine translation recently;machine translation particular;translation systems multilingual;multilingual translation;machine translation important;translation systems;machine translation based;machine translation issue;task machine translation;multilingual machine;low resource neural;models multilingual;translation based second;translation important task;translation based;translation fraction training;meta learning algorithm;resource neural machine;systems multilingual machine;meta learning;initial models multilingual;multilingual"}, "18e70ad07561cf09a2d7f0da992a0e87a5e5c0a8": {"ta_keywords": "model speech recognition;speech recognition;model speech;new model speech;speech recognition based;recognition based;recognition;speech;model;propose new model;new model;paper propose new;paper;new;paper propose;based;propose new;propose", "pdf_keywords": ""}, "8cebfae7cd436241eb5c3442e687a913a75a5531": {"ta_keywords": "disambiguation methods slavic;sense induction disambiguation;word sense induction;methods slavic language;sense embeddings competition;slavic language shares;features slavic languages;based sense embeddings;methods slavic;sense embeddings;slavic languages rich;slavic languages;induction disambiguation methods;slavic language;sense induction;shares features slavic;languages rich morphology;disambiguation methods;features slavic;slavic;russian language;word sense;russian language multiple;languages explore;induction disambiguation;induction wsi russian;languages explore performance;language shares features;disambiguation;sense granularity explore", "pdf_keywords": "russian language semantic;crowdsourcing word sense;semantic models slavic;semantic models russian;word sense embeddings;semantic relatedness similarity;semantic similarity;semantic similarity 2016;semantic similarity tasks;important semantic similarity;measure semantic relatedness;sense induction disambiguation;similarity important semantic;language semantic relatedness;word sense induction;words word disambiguation;disambiguation systems;disambiguation systems 2016;word disambiguation;semantic relatedness;similarity based crowdsourcing;language semantic;crowdsourcing word;task semantic similarity;disambiguation methods;induction disambiguation systems;disambiguation methods proposed;evaluation semantic models;word disambiguation important;relatedness similarity based"}, "c6bb04f3d8000b7e800f6359082de39548c7da79": {"ta_keywords": "locality nonparametric language;structural locality nonparametric;locality information models;adding locality information;locality nonparametric;structural locality ubiquitous;locality features contribute;nonparametric language models;locality ubiquitous;utilizing structural locality;locality structure;analysis locality features;locality information;locality ubiquitous feature;locality features improve;locality features;adding locality;structural locality;contextual similarity metrics;neighborhoods structural locality;approach adding locality;analysis locality;domains demonstrate locality;demonstrate locality features;locality;locality structure paper;demonstrate locality;nonparametric language;perform analysis locality;topical clusters text", "pdf_keywords": "locality nonparametric retrieval;incorporate locality nonparametric;locality nonparametric;incorporating structural locality;locality non parametric;differences structural locality;attributes structural locality;terms locality helps;structural locality;structural locality particular;method incorporate locality;nearest neighbor retrieval;locality additionally;nonparametric retrieval;incorporate locality;combines structural locality;locality helps;structural locality non;structural locality propensity;locality propensity text;locality additionally indicative;locality;nonparametric retrieval module;structural locality affects;locality particular;locality non;use nearest neighbor;locality affects lms;locality helps improve;nearest neighbor"}, "97846070369f66c3080a0803be58e96963dec581": {"ta_keywords": "usage patterns twittersphere;usage patterns tweet;patterns twittersphere;pandemic used twitter;twitter;twitter report;twitter report results;twittersphere;used twitter report;cluster websites based;patterns twittersphere summer;cluster websites;used twitter;twittersphere summer;twittersphere summer 2014;data cluster websites;patterns tweet;view clustering;websites significantly;multi view clustering;tweet text occurs;different websites significantly;patterns tweet text;view clustering technique;views data cluster;cluster;tweet text;clustering information;clustering;tweet", "pdf_keywords": ""}, "08f6819e66318cd49cddefd5d690a752d1098da7": {"ta_keywords": "quantum gravity;study quantum gravity;divergent conceptualization claims;quantum gravity curved;cross domain classification;applied quantum gravity;xmath0 classical gravitational;divergent conceptualization;gravity curved spacetime;conceptualization claims;domain classification;quantum gravity form;properties lexical;spacetime short;domain classification shared;spacetime;classical gravitational field;shared properties lexical;curved spacetime short;gravity;classification shared properties;properties lexical level;argument based measured;classical gravitational;curved spacetime;conceptualizations;conceptualization claims different;gravitational field;gravity form xmath0;tool study quantum", "pdf_keywords": "argument mining;argument mining computational;argumentation analysis;argumentation analysis recent;manual argumentation analysis;web discourse persuasive;gaps argument mining;argumentative context paper;manual argumentation;discourse persuasive;discourse persuasive essays;argumentative context;reasoning arguments text;question argumentative context;argumentation;corpora web discourse;counterpart manual argumentation;web discourse;argumentative;persuasive essays online;reasoning arguments;document approach reasoning;persuasive essays;question argumentative;essays online comments;discourse;arguments text precisely;arguments text;answer question argumentative;comments wiki talk"}, "9f73c3f86026c21d0e5e55c70462952c6ada1175": {"ta_keywords": "selection selective backprop;dnns prioritizing examples;dnns prioritizing;networks dnns prioritizing;selection high priority;importance sampling;selective backprop;forward pass selection;pass selection selective;selective backprop new;backpropagation;prioritizing examples high;uses backpropagation;training deep;speed image prediction;introduces selective backprop;training deep neural;uses backpropagation compute;importance sampling approach;high priority events;selective backprop state;prioritizing examples;prediction 5x faster;pass selection;backpropagation algorithm;backpropagation compute;automatic selection high;update parameters backpropagation;parameters backpropagation;deep neural", "pdf_keywords": "backpropagations example selection;sampling techniques deep;training selective backprop;importance sampling;evaluation selective backprop;example selection;train backpropagations;deep learning;training batches selected;loss training examples;deep learning measurements;introduce selective backprop;approach train backpropagations;importance sampling approach;new machine learning;selective backprop;train backpropagations example;prioritizing high loss;high loss training;batches selected examples;learning learn recent;techniques deep learning;sampling technique prioritizing;learn recent loss;practical effective sampling;example selection introduce;training examples;backpropagations;examples training;training batches"}, "dc984ea8be018a0244b40468d13f7b734ab55bac": {"ta_keywords": "neural machine translation;machine translation nmc;machine translation;translation lexicons efficiently;attention vector nmt;probability word translation;human body translationally;nmt probability neural;translational motion human;discrete translation lexicons;translation nmc;body translationally;translation lexicons;translating translational motion;word lexical probabilities;lexical probabilities model;attention vector;lexicon probability;probability neural machine;calculate lexicon probability;probability neural;lexical probabilities;efficiently encode translations;method translating translational;using attention vector;lexicon probability word;translational motion;translation nmc makes;discrete translation;encode translations", "pdf_keywords": "neural machine translation;machine translation learns;machine translation;machine translation process;translation learns lexical;machine translation performed;predict translation probabilities;languages machine translation;translation content words;improving translation content;improving translation;machine translation important;predict translation;propose machine translation;translation learns;translation probabilities tokens;translating source words;method improving translation;translation probabilities;tasks machine translation;learns lexical probability;matrix predict translation;translation process machine;words neural machine;translation content;process machine translation;lexical probability matrix;translation process;translation important tasks;lexical probability"}, "0533ccdc4840eed0fe1769b5e78da912631be609": {"ta_keywords": "soliton nonlinear optical;optical solitons formed;optical solitons;optical solitons exist;optomechanics optical solitons;soliton nonlinear;modulational instability bose;bose einstein condensate;cation soliton nonlinear;instability bose einstein;instability bose;nonlinear optical airflowbres;nonlinear optical;solitons formed;nonlinear fiber optic;solitons;condensate bec nonlinear;bec nonlinear fiber;solitons formed balance;optic described bifurcation;soliton;solitons exist various;solitons exist;nonlinear schr\u00e4odinger;dispersion self phase;nonlinear fiber;equation nonlinear schr\u00e4odinger;nonlinear schr\u00e4odinger equation;cation soliton;photonic crystal photorefractive", "pdf_keywords": ""}, "4b73f4956c31cd10994c73b21e2c38a60a68d03e": {"ta_keywords": "assignment problem paper;finding egalitarian assignment;algorithms assignment problem;algorithm finding egalitarian;egalitarian assignment group;allocating papers referees;solving assignment problem;assignment problem sided;allocation problem;allocation;proportional multi winner;assignment problem;allocation problem context;weighted averages;weight assigned item;sided matching problem;use order weighted;weighted;sided matching;assignment group;allocation use order;problem sided matching;egalitarian assignment;order weighted averages;problem preferences agents;determined weight assigned;order weighted;preferences agents;algorithms assignment;weighted averages asas", "pdf_keywords": "maximal assignment paper;allocating papers referees;egalitarian maximal assignment;assignment rawlsian fairness;maximal assignment;utilitarian maximal assignment;assignment problem sided;assignment paper;allocation owa assignment;agent resource allocation;problem preferences agents;agent approve candidates;preferences agents;assignment paper present;paper assignment data;allocation owa;fairness concept maximizing;conference paper assignment;allocation;solving assignment problem;preferences agents reviewers;agents reviewers objects;algorithm owa assignments;sided matching;model approval voting;assignment settings tradeoff;assignment problem;allocating papers;assignment implement algorithm;owa assignment implement"}, "b131cf78363993e4126b2562a156bd9d046c8bc4": {"ta_keywords": "pivot language words;tree based translation;distinguish pivot language;pivot language;pivot target translation;pivot translation;approach pivot translation;word pair generation;pivot translation method;pivot language using;pivot language present;phrase combinations pivot;combinations pivot translation;approach pivot language;concept pivot language;pivot translation used;phrase tree based;based translation models;subtrees distinguish pivot;language words syntactic;target translation models;translation models;phrase tree;syntactic subtrees;translation models combines;language using syntactic;using syntactic subtrees;words syntactic;syntactic subtrees distinguish;combines source pivot", "pdf_keywords": ""}, "18e5fb8cec55a75b288a499c57d77ede541dc049": {"ta_keywords": "question answering benchmarks;question answering tasks;commonsense question answering;question answering;language models training;answering benchmarks;learning language models;results commonsense;answering benchmarks paper;answering tasks;tasks global knowledge;accuracy commonsense;empirical results commonsense;answering tasks data;constrained distractor sampling;task generating fair;generated external knowledge;resources language models;generating fair informative;structure task generating;task generating;tasks individual knowledge;distractor sampling strategies;tasks learning;commonsense;tasks learning utilize;language models;neural language modeling;distractor sampling;global knowledge graph", "pdf_keywords": ""}, "5b1bb1f6ed091dfd53adf7ebbcda2c48a3b67c2c": {"ta_keywords": "semantic frame induction;unsupervised semantic frame;unsupervised semantic;semantic frame;present unsupervised semantic;word context embeddings;verb clustering;verb clustering using;combining syntactical features;frame induction;syntactical features;embeddings role labeling;labeling combining syntactical;context embeddings;steps verb clustering;context embeddings role;languages present unsupervised;semantic;role labeling combining;combining syntactical;role labeling;predicting outcome unsupervised;clustering using word;unsupervised rheological event;frame induction paper;syntactical;using word context;outcome unsupervised;word context;datasets languages present", "pdf_keywords": "unsupervised semantic frame;embeddings role labeling;labeling verb roles;semantic frame role;frame role induction;word context embeddings;verb vectors enhanced;verb induction models;word sentence embeddings;context embeddings role;unsupervised semantic;semantic frame;features semantic frame;sentence embeddings models;sentence embeddings;verb roles approach;syntactical features semantic;verb roles;frame semantics;word sense induction;role labeling;models verb induction;frame semantics experimented;context embeddings;learning word context;approach unsupervised semantic;role labeling combined;word embedding;word verb vectors;word embedding models"}, "384bf224d91a1691c9e6384201483121e2e7ddab": {"ta_keywords": "subspace clustering known;clustering subspaces xmath0;exact subspace clustering;clustering subspace clustering;clustering subspaces;subspace clustering;clustering subspace;study clustering subspaces;subspace clustering subspace;clusters different subspaces;subspaces similarity measurements;subspaces similarity;clustering known;clustering known problem;community recovery hypergraphs;clustering;reliable clustering;different subspaces similarity;required reliable clustering;reliable clustering unknown;hypergraphs characterize;problem clusters;subspaces xmath0 dimensional;clustering unknown;clustering unknown paper;subspaces xmath0;hypergraphs;clusters;recovery hypergraphs characterize;paper study clustering", "pdf_keywords": ""}, "e01aa6f8ce625469b6f161d7ab9e61a60ac33798": {"ta_keywords": "online streaming codes;streaming codes communicating;consider streaming codes;streaming codes;streaming codes need;streaming codes fact;optimal online code;online streaming;streaming communication;latency streaming communication;low latency streaming;practice streaming codes;latency streaming;streaming communication settings;surprisingly online streaming;live video streaming;rate consider streaming;offline rate feasible;streaming;streaming codes variable;optimal offline rate;consider streaming;video streaming;optimal offline;codes communicating burst;attain optimal offline;rate optimal online;offline rate consider;online code;offline rate", "pdf_keywords": "online coding schemes;online coding scheme;optimal online code;streaming packet erasures;online coding;present online coding;online code constructions;message size sequences;streaming packet;coding schemes;surprisingly online coding;online code;new coding scheme;coding scheme;size message packets;present online code;coding scheme real;packet erasures;time streaming packet;coding schemes fact;code constructions present;real time streaming;rate new streaming;code constructions;coding scheme message;streaming;achieves lossless delay;new streaming model;consider streaming;scheme message size"}, "6b7004138ee2de5ec52e500cae4e65390e961e16": {"ta_keywords": "bounds kernel clustering;kernel clustering solution;regularization kernel clustering;kernel clustering;kernel clustering criteria;kernel clustering work;based kernel clustering;optimization kernel cut;kernel cut algorithm;bound optimization kernel;clustering solution optimization;closed contact surfaces;closed contact surface;optimization kernel;size contact surface;shape closed contact;contact surfaces;regularization kernel;surface closed contact;clustering solution;contact surface controlling;contact surface;contact surfaces explain;loop contact surface;kernel spectral bounds;contact surface addition;contact surface used;spectral bounds kernel;contact surface closed;clustering work", "pdf_keywords": ""}, "e0236106e51984e4ea6bbbd1fb5ce57abf3e4e5e": {"ta_keywords": "people images masks;mask maintaining social;mask maintain social;face mask;using face mask;images masks;social distance protocols;images masks collected;wearing mask;measure social distance;virus using face;social distance;maintaining social distance;mask;person wearing mask;masks;coronavirus goal;maintain social distance;new coronavirus goal;masks collected;risk transmitting virus;prevent transmission virus;mask maintaining;face mask maintaining;virus california institute;search virus california;social distance help;wearing mask maintain;coronavirus goal determine;distance protocols", "pdf_keywords": ""}, "af92dd61340808f3008a84ae57803bb4aa57d03b": {"ta_keywords": "dynamics avatar speech;avatar speech body;speech body pose;interactions form avatar;dynamics avatar;intrapersonal dynamics avatar;avatar speech;human operating avatar;behavior human observers;body pose interlocutor;behaviours gestures facial;audio body pose;pose conditioned audio;gestures facial;pose interlocutor audio;expressions body posture;body pose conditioned;model intrapersonal dynamics;avatars requires model;human observers;gestures facial expressions;model interpersonal dynamics;body pose needs;dyadic residual attention;pose interlocutor;facial expressions body;speech body;avatar fluid dynamics;verbal behaviours gestures;dynamics video", "pdf_keywords": "avatar communicating;verbal behaviours avatar;avatar pose forecasting;face interactions dyadic;user avatar communicating;dynamics real conversations;pose dyadic face;predicting facial expression;dynamics dyadic conversations;avatar communicating virtual;behaviours avatar;interactions dyadic conversation;behaviours avatar conditioned;face interactions;predicting facial;head pose dyadic;pose dyadic;face face interactions;dynamics interpersonal dynamics;dyadic conversations;model predicting facial;interpersonal dynamics;dynamics interpersonal;real conversations mixture;interpersonal dynamics analyzing;intrapersonal dynamics real;intrapersonal dynamics interpersonal;predict future pose;intrapersonal dynamics;pose forecasting participant"}, "5b1c0152bbb12ece2a8817c727e33e6d5c503065": {"ta_keywords": "distributed machine learning;distributed learning algorithms;distributed learning;blocks distributed learning;algorithms data shuffling;distributed machine;data shuffling matrix;shuffling matrix multiplication;distributed;approach distributed machine;data shuffling;shuffling matrix;faster uncoded matrix;coded shuffling;new approach distributed;coded shuffling used;worker node distributed;node distributed machine;node distributed;approach distributed;reduce communication bottlenecks;optimal coded matrix;coded matrix multiplication;uncoded matrix multiplication;faster uncoded;shuffling;matrix multiplication optimal;optimal task replication;blocks distributed;communication bottlenecks", "pdf_keywords": ""}, "c395595cf7be23f7d90cbca98d8c7861ebfd884d": {"ta_keywords": "opinion deconvolution crowdsourced;deconvolution crowdsourced annotators;deconvolution crowdsourced;crowdsourced annotators machine;crowdsourced annotators;opinion deconvolution;metrics crowdsourced;highly metrics crowdsourced;crowdsourcing current metrics;annotators machine learning;crowdsourced crowdsourcing current;metrics crowdsourced crowdsourcing;crowdsourced;crowdsourcing;crowdsourcing current;classifiers human facing;crowdsourced crowdsourcing;classifiers human;learning classifiers human;facing machine learning;classifiers;machine learning tasks;machine learning classifiers;learning classifiers;problem opinion deconvolution;annotators;machine learning classification;learning classification metrics;comment toxicity;annotators machine", "pdf_keywords": "annotators resolve disagreement;crowdsourcing;social computing tasks;crowdsourcing researchers;performance deployed crowdsourcing;crowdsourcing researchers proposed;deployed crowdsourcing researchers;label artificial intelligence;reasoning models performance;annotator disagreement making;clearer relying annotators;disagreement making task;social computing;annotators;deployed crowdsourcing;relying annotators;intelligence classifier;teams face reasoning;datasets social computing;artificial intelligence classifier;disagreement making;intelligence artificial intelligence;resolve disagreement;ml tasks;assumptions social computing;annotator disagreement;effort create evaluation;resolve annotator disagreement;reasoning models;computing tasks today"}, "37074b2b9cebd89e4a92d20f41eec7360e11fe5a": {"ta_keywords": "automatic speech;attention based automatic;attention speech processing;attention based models;speech recognition asr;based automatic speech;automatic speech recognition;speech recognition;speech processing;sentences faster;ar attention based;attention speech;sentences faster inference;coherent sentences faster;attention based;compared ar attention;speech recognition based;approach speech recognition;faster inference speed;recognition asr;faster inference;state art attention;ar attention;speech processing paper;inference speed;real time factor;new decoding strategy;accuracy compared autoregressive;recognition asr structure;inference speed compared", "pdf_keywords": "streaming speech recognition;prediction end speech;end streaming speech;recognition speech translation;speech recognition challenging;streaming speech;translation speech recognition;masked language model;speech recognition;recognition speech recognition;speech recognition speech;online asr recognition;recognition speech;decoder masked language;automatic speech recognition;speech translation automatic;automatic speech;translation automatic speech;ctc greedy decoding;asr recognition;machine translation speech;speech recognition combining;asr recognition low;speech end end;speech end;speech end speech;mask ctc models;attention based decoder;masked language;attention sequence sequence"}, "bd6018632a360cb567da8e50e1717ff526503845": {"ta_keywords": "stochastic decoders used;sequence models stochastic;models stochastic decoders;stochastic decoders;stochastic beam search;beam search skimming;search skimming;search skimming skimming;poisson search;decoding strategy sequence;decoders;sequence models;decoding strategy;conditional poisson search;sequence generation tasks;sequence generation;poisson search default;search default decoding;decoding;skimming skimming events;stochastic beam;skimming events;decoders used;beam search;stochastic;approach search kuramoto;models stochastic;default decoding strategy;sets sequence models;search kuramoto like", "pdf_keywords": "stochastic machine translation;sampling sequences replacement;samples nlp tasks;generates samples nlp;machine translation;machine translation algorithm;samples nlp;sampling sequences;algorithms sequence models;nlp tasks;decoding algorithms sequence;sequences replacement;sampling resulting decoding;prediction struc replacement;sequence models;sequence models solution;nlp tasks require;sequences replacement work;conditional poisson sampling;translation algorithm;sets sequence models;method sampling sequences;sequence models present;stochastic beam search;translation algorithm based;beam search stochastic;decoding algorithms;search stochastic beam;stochasticization conditional poisson;poisson sampling"}, "6494cd26511c076186673c9a636d21d1dfed8d5a": {"ta_keywords": "enhanced features student;features network training;features input speech;features student teacher;learning enhanced features;input speech enhancement;improving performance speech;performance speech recognition;self supervised learning;network learning enhanced;speech recognition shown;speech enhancement;learning enhanced;features student;shown speech enhancement;teacher network learning;training data;enhanced features network;speech recognition;network training;speech enhancement essential;noisy features input;speech enhancement implicitly;teacher network outputs;training used improve;mimic teacher network;improve performance neural;recognition speech recognition;features improved;recognition speech", "pdf_keywords": ""}, "c8f9313ce8416a7be079935d1cbb637705f75182": {"ta_keywords": "quality translation individuality;approach translation dictionary;estimation translation model;translation model probabilities;translation model;translation dictionary language;translations person language;construction translation dictionary;improvements quality translation;translation individuality;individuality using translation;translation dictionary using;translation dictionary;using translation dictionary;automatic construction translation;translation dictionary problem;thesaurus ngram statistics;translations person;quality translation;dictionary language model;statistics estimation translation;fact translations person;translation individuality paper;method translating sentence;using thesaurus ngram;estimation translation;translations;language model;translating sentence target;using translation", "pdf_keywords": ""}, "8c7628641450203b0aa959b5a69729ff906760ff": {"ta_keywords": "embeddings speaker overlaps;speaker wise embeddings;wise embeddings speaker;embeddings speaker;speaker diarization;approach speaker diarization;speaker diarization better;end neural diarization;cascaded approach speaker;decoder based attractor;attractors embeddings;attractors embeddings remedy;generating speaker wise;neural diarization;generating multiple attractors;speech activity detector;generating speaker;external speech activity;method generating speaker;products attractors embeddings;multiple attractors;speaker overlap handling;attractor calculation module;multiple attractors given;neural diarization eend;attractors given;results external speech;introduce encoder;speaker overlap;attractor calculation", "pdf_keywords": "end neural diarization;neural diarization comprehensive;neural diarization;speakers diarization based;speech recognition;evaluation speakers diarization;automatic speech recognition;automatic speech;speech recognition important;recognition speech;speakers diarization;neural diarization unknown;neural diarization method;speech recognition speech;recognition speech recognition;speakers diarization important;end neural;end end neural;diarization based;decoder based attractor;diarization comprehensive discussions;diarization comprehensive;diarization based use;15 speakers diarization;encoder;encoder decoder;diarization important;introduce encoder;encoder decoder based;diarization"}, "5aea95e1ae78a66474051a330ded374e199b658c": {"ta_keywords": "hoc networks;ad hoc networks;hoc networks particular;bioinformatics citation networks;networks paper proposes;networks paper;networks;citation networks paper;citation networks;paper propose network;graph structure;graph structure analogous;neighboring nodes;networks particular;track swimming arena;social bioinformatics citation;nodes;experiments social bioinformatics;neighboring nodes node;social bioinformatics;swimming arena analyze;depends graph structure;dynamics swimming track;dynamics swimming;mobile ad hoc;nodes node representation;swimming arena;networks particular range;propose network;study dynamics swimming", "pdf_keywords": "networks biological networks;citation networks represent;networks citation networks;biological networks;networks represent;biological networks citation;citation networks;biological citation networks;networks citation;networks biological;graphs social networks;networks represent important;citation networks majority;networks;node representation learning;social networks;nodes;networks \ufb01nancial networks;networks \ufb01nancial;graphs social;networks majority nodes;nodes individually;node representation;nodes individually biological;\ufb01nancial networks biological;\ufb01nancial networks;connected nodes;adaptive aggregation;graphs;selective adaptive aggregation"}, "564dec6eab6115ecd604f22738ce0b47777f6e17": {"ta_keywords": "robustly classify speech;speech proposed bayesian;acoustic classification;estimation clustering speech;approach acoustic classification;acoustic classification based;clustering speech recognition;speech recognition vbec;clustering speech;classify speech;bayesian predictive classification;classify speech proposed;continuous speech recognition;acoustic models robustly;speech recognition;classification based bayesian;bayesian estimation clustering;based bayesian predictive;speech recognition experiments;acoustic models;predictive classification;bayesian predictive;robustly classify;predictive posterior distribution;models robustly classify;accurate acoustic models;propose variational bayesian;tasks continuous speech;variational bayesian estimation;predictive posterior", "pdf_keywords": ""}, "8c5465eb110d0cab951ca6858a0d51ae759d2f9c": {"ta_keywords": "classifier learns rationale;learns rationale;text classifiers;classifier learns;text corpora neural;text classifiers semantic;learns rationale propose;propose classifier learns;semantic classifiers;classifiers semantic;semantic classifiers interpretable;classifier text classifiers;corpora neural networks;corpora neural;interpretable classifiers;interpretable classifiers paper;classifiers;classifiers interpretable;rationale extraction explore;class neural networks;rationale extraction;classifiers semantic classifiers;jointly training neural;class neural;classifiers interpretable classifiers;gradient based training;classifiers paper;classifier text;neural;classifier", "pdf_keywords": "sentiment prediction;sentiment prediction based;learning attitudes attributes;learning rationales;framework sentiment prediction;reinforce machine learning;sentiment;rationale extraction explore;rationale extraction;attributes multiaspect reviews;learning attitudes;attitudes attributes;2018 learning rationales;learning rationales time;extractive rationales;approach extractive rationales;extractive rationales allows;bayesian model predicting;gradient based training;predicting outcome;attitudes attributes multiaspect;attitudes;attention;reviews approach;predicting;framework sentiment;focusing interpretability;novel framework sentiment;machine learning;approach extractive"}, "2660dbba723573266edb2a0a4929e6847ae83212": {"ta_keywords": "speaker adaptation rnn;vector speaker adaptation;eigenvector speaker adaptation;speaker adaptation;language model fusion;winner vector speaker;adaptation rnn;vector speaker;language models;adaptation rnn tangles;language model;class language models;speaker adaptation presence;neural network recognize;ratio language model;voice italian test;encoder prediction network;eigenvector speaker;language models scratch;equivalent eigenvector speaker;recognition winner vector;train neural;train neural network;prediction network vectors;encoder prediction;integration encoder prediction;based recognition winner;neural network;prediction network;voice italian", "pdf_keywords": "automatic speech;speech recognition;speech recognition machine;end speech recognition;language model fusion;automatic speech recognition;model regularization speaker;softmax;external language model;speaker adaptation;regularization speaker adaptation;machine translation multiplicative;regularization speaker;speaker adaptation external;conversational speech collected;speech recognition paper;set conversational speech;retail automatic speech;language model;conversational speech;adaptation external language;conversational italian 900;softmax wouttanh;conversational italian;learning machine translation;softmax wouttanh wencht;machine translation;gu softmax;speech collected using;scripted dialogs"}, "94f22d7a8b48784b3d8975616e20d8028a08162f": {"ta_keywords": "crystal film pendulum;film pendulum described;film pendulum;dynamics film aluminum;liquid crystal film;pendulum described;film aluminum film;dynamics film;pendulum rotates;aluminum film;pendulum;analysis dynamics film;crystal thickness film;film aluminum;pendulum described hamiltonian;surface liquid crystal;pendulum rotates fixed;hamiltonian xmath22 pendulum;thickness liquid crystal;xmath22 pendulum rotates;aluminum film grown;xmath22 pendulum;liquid crystal thickness;crystal film;liquid crystal;thickness film xmath0;film xmath0 mm;thickness film;film grown surface;film thickness", "pdf_keywords": ""}, "d119cc4051ed1206a0dac963cd23a84acf77fea7": {"ta_keywords": "predicted accurately threshold;accurately threshold decisions;decision threshold loss;decision loss prediction;loss threshold decision;calibrated decision forecaster;threshold calibrated decision;decision forecaster;decisions based forecasted;threshold decisions decisions;decision loss predicted;accurate decision loss;forecaster forecasted probabilities;hospital scheduling decisions;loss prediction real;threshold decisions average;threshold decisions;forecasted probabilities;cutoff predicted accurately;decision threshold;forecasted outcome;decision forecaster forecasted;loss predicted accurately;estimate loss threshold;threshold decision threshold;loss prediction;scheduling decisions;ensure decision loss;threshold decision;based forecasted outcome", "pdf_keywords": "threshold decision forecasted;loss deployment threshold;decision thresholds cost;threshold calibration decision;deployment threshold calibration;predicting decision loss;decision loss deployment;decision thresholds;threshold loss functions;threshold loss;threshold loss function;reliability gap threshold;forecasted cdfs threshold;different decision thresholds;threshold calibration minimizes;decision forecasted cdfs;cdfs threshold loss;improved decision loss;threshold decision;reliability gap decision;different threshold loss;decision loss compared;tasks threshold calibration;gap decision loss;problem threshold calibration;faced threshold loss;calibration minimizes reliability;average decision loss;gap threshold decision;decision loss achieved"}, "6a9795853e5f39325deb0d916fe22d9e5a202a9f": {"ta_keywords": "dynamics;fluid dynamics;theorem fluid dynamics;littlewood richardson theorem;movement people shaken;richardson theorem fluid;book complex phenomenon;fluid dynamics video;people shaken bed;phenomenon described;movement;nature movement;movement people;version littlewood richardson;shaken bed;proofs paper;behavior darwinian pair;nature movement people;proofs paper present;behavior darwinian;people shaken;dynamics video;printed book proofs;theorem fluid;proof concept printed;book proofs;nature;behavior;printed version littlewood;littlewood richardson", "pdf_keywords": ""}, "5270b626feb66c8c363e93ba6608daae93c5003b": {"ta_keywords": "memorization generalization transformers;memorization knowledge modification;knowledge modification transformer;facts memorize new;improving memorization;transformer based language;facts memorize;memorization knowledge;old facts memorize;encoding factual knowledge;improving memorization generalization;memorize new;tasks improving memorization;language models;memorization;memorize new ones;transformers forget specific;knowledge modification;memorization generalization;stale knowledge;make transformers forget;language models shown;generalization transformers widely;updating stale knowledge;generalization transformers;based language models;transformers forget;factual knowledge vast;memorize;factual knowledge", "pdf_keywords": "factual knowledge transformer;language models achieve;language models serve;memorization language models;question answering;language models rely;domain question answering;language models;encode factual knowledge;models encode factual;knowledge transformer models;language models multitude;factual knowledge stored;language models encode;knowledge transformer;based language models;potential machine translation;transformer based language;nlp tasks fueled;models multitude nlp;machine translation;memorized sensitive information;question answering qa;ability language models;encode factual;nlp tasks;implicit memorization language;language models paper;memorization language;machine translation based"}, "7b99c51d562e33309a46601c846abbe72a65c6a4": {"ta_keywords": "trained language models;efficient embedding;train language models;demonstrate efficient embedding;efficient embedding based;datasets intermediate training;training fine tuning;trained language;pre trained language;datasets pre trained;fine tuning approaches;language models combinations;train language;language models;classification tasks;infeasible train language;fine tuned electrocardiogram;set classification tasks;embedding based methods;language models infeasible;best datasets intermediate;datasets outperform computational;intermediate training;embedding based;classification tasks multiple;embedding;shot fine tuning;fine tuning;intermediate training fine;sequence tagging tasks", "pdf_keywords": "transfer learning nlp;transfer learning considerably;transfer learning;transfer learning aspect;training transfer learning;transfer learning important;transfer learning diverse;based transfer learning;intermediate task training;address transfer learning;learning aspect transfer;aspect transfer learning;task training;target tasks;intermediately trained adapters;training transfer;11 target tasks;target task ne;adapter based training;trained adapters;task ne tuning;learning machine translation;vision tasks;based training transfer;task selection;target task;approach task selection;transferability;target tasks results;transferability set"}, "c47c8c2527bf2ca8339c342f44db2218a0cbcbbd": {"ta_keywords": "knowledge graph construction;extraction knowledge base;base knowledge graph;knowledge graph;knowledge graph identification;knowledge graph information;information extraction knowledge;knowledge base construction;knowledge graph introduce;text knowledgeledge graph;desired knowledge base;extraction knowledge;statistical relational learning;knowledge base knowledge;deriving knowledge text;problem knowledge graph;present knowledge graph;knowledge base;relational learning;results knowledge graph;information extraction;solution knowledge graph;knowledge text knowledgeledge;probabilistic soft logic;knowledgeledge graph;relational learning framework;knowledge text;graph information extraction;knowledgeledge graph identification;statistical relational", "pdf_keywords": ""}, "4eb22b488052c430170139c492674aa05512f7bf": {"ta_keywords": "shape optimization design;optimization design carried;optimization design;perform design optimization;design optimization software;die shape optimization;shape optimization;design optimization;perform optimization procedure;optimization software motion;perform optimization;optimization procedure;optimization procedure optimization;optimization software;optimization;optimization problem;optimiztion deforming force;forging obtained optimiztion;shape forging deforming;obtained optimiztion deforming;design multiple objective;optimiztion deforming;used perform optimization;optimization problem solved;net shape forging;procedure optimization problem;objective perform design;shape forging;procedure optimization;obtained optimiztion", "pdf_keywords": ""}, "f394c5101d7bfc3d8055f9391a83f7e2395dec4a": {"ta_keywords": "performance decision trees;learning algorithm parallelized;machines decision trees;decision trees;decision tree;decision trees comparable;programs parallelized;programs parallelized using;sequential programs parallelized;algorithm parallelized;algorithm parallelized versions;boosting using adaboost;parallelization code;performance classification;comparable performance classification;tree error reduction;performance classification problem;trees comparable performance;parallelization;classification problem boosting;contribution decision tree;parallelized;decision tree error;parallelized using;livermore parallelization code;parallelization code hand;vector machines decision;versions livermore parallelization;parallelized versions livermore;school run snowdrift", "pdf_keywords": ""}, "f41e6c832c9e0d5360b66ee7681d3b1ffd2d9c3d": {"ta_keywords": "learn hierarchical task;hierarchical tasks unified;hierarchical tasks;representation task structures;learning new tasks;hierarchical task structure;tasks unified;task learning sub;new tasks language;look task learning;hierarchical task;tasks language;learn hierarchical;task structures;learning representation task;task learning;stands hierarchical tasks;tasks;task learning representation;task structure;tasks unified transformers;new tasks;task structures enables;decomposed task learning;tasks language instructions;planning scene;look task;architecture decomposed task;planning scene navigation;manner learn hierarchical", "pdf_keywords": "learn hierarchical task;multi task deep;hierarchical tasks;hierarchical task structure;task deep;hierarchical task modeling;task deep neural;hierarchical task learning;level action planning;task planning;hierarchical task;planning task;presents hierarchical task;hierarchical tasks uni\ufb01ed;model task planning;action planning;task planning task;planning based hierarchical;structure task compositions;present hierarchical task;learn hierarchical;task compositions;task compositions language;sub goal planning;planning task paper;planning scene;task structure propose;learn new tasks;novel multi task;task learning"}, "c54ad6e29f3e516eecf0a72bd1f95b80e8617116": {"ta_keywords": "compressive phase retrieval;compressive phase;general compressive phase;measurements compressive phase;problem compressive phase;phase retrieval algorithm;phase uncertainty sparse;phase retrieval problem;phase retrieval;retrieval problem compressive;tackle general compressive;general compressive;problem compressive;sparse graph code;uncertainty sparse complex;number measurements compressive;compressive;measurements compressive;sparse complex;sparse complex vector;recover nonzero signal;sparse graph;optimal complexity memory;uncertainty sparse;degree sparse;recover global phase;left degree sparse;degree sparse graph;algorithm recover nonzero;sparse", "pdf_keywords": "compressive phase retrieval;phase retrieval sparse;codes compressive phase;signal recovery compressive;sparse signal recovery;recovery compressive sensing;compressive phase;retrieval sparse signal;compressive sensing low;compressive sensing;new compressive phase;graph based compressive;compressive sensing algorithm;recovering sparse complex;based compressive sensing;codes compressive sensing;compressive sensing layer;recovering sparse;sparse complex signal;graph codes compressive;recover sparse complex;measurements phase retrieval;recover sparse;phase retrieval algorithm;phase retrieval;recovery compressive;sparse signal;perfectly recover sparse;problem recovering sparse;phase retrieval used"}, "044b502e5a00b5eeff1dd078ea03f491ca2c37bf": {"ta_keywords": "phoneme recognition task;lecture transcription task;continuous phoneme recognition;phoneme recognition;lecture transcription;acoustic language models;systems training discriminative;discriminative training techniques;discriminative training;task wsj transcription;mit lecture transcription;transcription task;wsj transcription task;training discriminative training;recognition task wsj;training discriminative;transcription task mit;acoustic language;structural classification;similarity structural features;automatic repeat;timit continuous phoneme;structural classification based;repeat request asr;transducers used decoding;asr systems training;automatic repeat request;conventional acoustic language;transcription;language models", "pdf_keywords": ""}, "b9057dce43181a30aa3e0435c8ffc4c0b6f8f127": {"ta_keywords": "reasoning inference graphs;humans inference graphs;generate inference graphs;inference graphs support;inference graphs;inference graphs useful;automatically generate inference;inference graphs transfer;generate inference;graphs defeasible inference;machine reasoning;reasoning inference;defeasible inference task;reasoning constructing;humans inference;useful reasoning constructing;graphs useful reasoning;defeasible inference;human reasoning;reasoning defeasible reasoning;inference task;help human reasoning;reasoning mode reasoning;human reasoning defeasible;conjecture humans inference;inference;reasoning help human;defeasible reasoning;machine reasoning help;inference task probability", "pdf_keywords": "learning defeasible inference;defeasible inference based;graph defeasible reasoning;trained defeasible inference;knowledge defeasible reasoning;defeasible reasoning model;reasoning model trained;machine reasoning;defeasible inference;reasoning defeasible reasoning;inference graph;machine reasoning mode;inference graph defeasible;defeasible reasoning cognitive;reasoning constructing;inference based graph;defeasible reasoning;useful reasoning constructing;defeasible inference dataset;concept inference graph;supplement defeasible reasoning;reasoning model;defeasible reasoning defeasible;approach defeasible inference;reasoning mode reasoning;defeasible inference problem;reasoning cognitive science;defeasible reasoning important;reasoning defeasible;inference based"}, "b143ee344fe3af4169bde8af8b682a2835dae4a4": {"ta_keywords": "distributed agents dataset;task furnmove agents;agents learn collaborate;learn collaborate agents;decentralized action sampling;collaborate agents;collaborate agents achieve;distributed agents;design distributed agents;agents dataset;agents learn;training agents complete;autonomous agents learn;agents complete;agents achieve;actions autonomous agents;training agents;agents work;agents complete furnmove;furnmove requires agents;agents;agents dataset code;challenges training agents;existing tasks furnmove;autonomous agents;challenges synchronize actions;agents work piece;tasks furnmove;requires agents;furnmove agents", "pdf_keywords": "agents observe egocentric;objects simultaneously agents;agents observe;simultaneously agents;task agents observe;multiple agents communicate;agents away actions;object agent;multiple agents;independently agents central;agents communicate;agent learning;lifted object agent;decentralized agent learning;agents communicate synchronize;independently agents;algorithms guides agents;egocentric visual views;collaborative tasks;task agents;observe egocentric;novel task agents;simultaneously agents liftedobject;actor critic algorithms;agent learning model;collaborative tasks based;observe egocentric visual;agents;object agent locations;guides agents"}, "c3930cb34241a42e03ed02cbc83a3c87dddd60cc": {"ta_keywords": "story generation reward;generated stories scoring;crowdsource story continuation;automatically generated stories;metric stories generation;generated stories;learn generate stories;generate stories;story generation;evaluation metric stories;function story generation;crowdsource story;stories partial evaluation;stories scoring;stories generation;generate stories partial;story continuation systems;stories generation seek;stories scoring used;metric stories;feature function story;crowdsource;makes story interesting;story continuation;generation reward;stories partial;models predict annotations;predict annotations;human perception based;statistics human perception", "pdf_keywords": ""}, "c9d7b1f9b13d6ea4ff45b908285cc65af959cc5b": {"ta_keywords": "language probability word;probability word language;word probability;word probability word;probability word;proportional word probability;assign probability word;features language probability;language probability;probability word basis;probabilistic;propose new probabilistic;new probabilistic model;distribution number words;new probabilistic;probabilistic model;words language;probabilistic measure;words language considered;word language;attraction repulsion;probabilistic model predicting;word proportional word;probabilistic measure defined;single word proportional;competition attraction repulsion;word language spoken;proportional word;assign probability;word proportional", "pdf_keywords": ""}, "3050735eb35af3527276aa1952f79eb2483df3f0": {"ta_keywords": "conversation corpus;dialogue act recognition;labels conversation corpus;conversation corpus high;utterance level dialogue;analyzing conversations;annotating labels conversation;emotion classification;recognition emotion classification;emotion classification low;propose categorical dialog;categorical dialog act;act recognition emotion;categorical dialog;analyzing conversations new;dialogue;dialog act affect;conversations;labels conversation;dialog act;dialogue act;conversation;interact common language;level dialogue;dialog;level dialogue act;speakers mind emotions;recognition emotion;corpus high level;encountered analyzing conversations", "pdf_keywords": ""}, "a556914c1b32372d47a36f2826cbe143ddae95ca": {"ta_keywords": "taxonomy expansion;feature representations query;attachment task learns;learns feature representations;taxonomy expansion 11;task learns feature;study taxonomy expansion;self organized taxonomy;taxonomies new concept;representations query anchor;paths query terms;learns feature;query terms taxonomies;taxonomy based self;methods taxonomy expansion;benchmarks study taxonomy;query terms;taxonomies new;expand existing taxonomies;new concept terms;self propelled;views;taxonomy expansion problem;terms taxonomies;self propelled propelled;feature representations;taxonomy mini paths;multiview training;natural self supervision;task learns", "pdf_keywords": ""}, "d85c0032d7bb0bd220eb2df8ba6d2130bc87e79e": {"ta_keywords": "semi supervised training;supervised adaptation pseudo;adaptation pseudo labeling;semi supervised adaptation;semi supervised;pseudolabel method training;propose semi supervised;present semi supervised;dataset proposed pseudolabel;supervised training;supervised adaptation;adaptation using labeled;data joint speech;supervised;supervised training technique;proposed pseudolabel achieves;pseudolabel achieves;pseudo labeling;pseudolabel;proposed pseudolabel;iterative pseudolabel;pseudo labeling propose;labeled data;pseudo labeling end;propose iterative pseudolabel;neural diarization;using pseudo labeling;labeled unlabeled data;end neural diarization;pseudolabel achieves 37", "pdf_keywords": "speakers diarization semisupervised;semi supervised acoustic;unsupervised speaker diarization;speaker diarization based;extracted speaker diarization;speaker diarization;supervised acoustic models;unlabeled data speech;train semi supervised;semi supervised learning;data joint speech;semi supervised;data speakers diarization;supervised acoustic;audio joint speech;unsupervised speaker;introduced semi supervised;method unsupervised speaker;diarization semisupervised unsupervised;data speech;speaker diarization essential;speaker embeddings extracted;speakers diarization;data speech regions;semisupervised training;diarization semisupervised;embeddings extracted speaker;unlabeled data speakers;diarization unlabeled data;speaker embeddings"}, "571b4425498549c56c0828a824dc453ff6f482fc": {"ta_keywords": "gyromagnetic resonator delay;delay throughput;delay throughput optimality;medium access control;certain delay throughput;optical gyromagnetic resonator;performance magneto optical;magnetic field performance;resonator delay;performance magneto;field performance magneto;scheduling air showers;showers simulations protocols;resonator delay sensitive;delays lower achieved;magneto optical gyromagnetic;delay sensitive;gyromagnetic resonator;optical gyromagnetic;magneto optical;speed execution;method scheduling air;medium access;scheduling air;speed execution computer;delay sensitive nature;throughput optimality;delays;delay;simulations protocols", "pdf_keywords": ""}, "0823f2187eeed53be8fd452decf6ed9a6a6cd124": {"ta_keywords": "semantic parser;semantic parsing;dialog act tagging;semantic parser paper;semantic parsing present;referred semantic parser;natural language understandingcomponent;semantic analysis;analysis linguistic;phase semantic parsing;prototype natural language;follows semantic analysis;natural language;semantic;analysis linguistic data;linguistic;tagging phase semantic;linguistic data;act tagging;parsing;dynamics articulated articulated;described follows semantic;dynamics articulated;motion articulated vehicle;parser;motion articulated;act tagging phase;linguistic data document;language understandingcomponent;language understandingcomponent space", "pdf_keywords": ""}, "5de24203bf98ae7f4c514bc0bd2a310caa47a047": {"ta_keywords": "scheduling vehicles;vehicle scheduling;scheduling vehicles disruptions;multi agent reinforcement;solve vehicle scheduling;scheduling traffic virtually;agent reinforcement;agent reinforcement learning;vehicle scheduling problem;train flatland competition;agents need coordinated;scheduling traffic;challenging tasks agents;concerned scheduling vehicles;multi agent;trains flatland soccer;vehicle competition;railway networks;tasks agents need;scheduling;tasks agents;real time scheduling;agents need;trains;scheduling problem;train trains;train trains flatland;agents;time scheduling traffic;trains flatland", "pdf_keywords": "coordinating activities crowdsourcing;multi agent reinforcement;multi agent;crowdsourcing environments;agent reinforcement learning;agent reinforcement;crowdsourcing;activities crowdsourcing;activities crowdsourcing environments;clusters allowing agent;crowdsourcing environments propose;agents;single agent;observation agents;agent control;scheduling vehicles;agent occupy time;single agent control;based observation agents;flatland competition decentralized;presents multi agent;allowing agent occupy;networks scheduling vehicles;agent occupy;agent;allows single agent;scheduling vehicles disruptions;flatland competition;competition participants learn;participants learn schedule"}, "254d1b8cf247ae8b19e017f7ba758d670207ddda": {"ta_keywords": "detection acoustic waves;detection acoustic;network detection acoustic;based ultrasonic;ground based ultrasonic;ultrasonic waveguide;based ultrasonic waveguide;ultrasonic;ultrasonic waveguide bf;acoustic waves;speech processing systems;introduce beamforming network;beamforming network;speech processing;optimal beamforming;beamforming network perform;task speech processing;speech recognition asr;speech recognition;waveguide bf network;waveguide;optimal beamforming parameters;systems automatic speech;automatic speech recognition;acoustic;beamforming parameters frequency;waveguide bf;acoustic waves paper;spatial filtering optimal;introduce beamforming", "pdf_keywords": ""}, "1f5a1e959147e989e12846a5bd1d20234ef667d7": {"ta_keywords": "anticoagulants treatment bleeding;oral anticoagulants prothrombin;anticoagulants prothrombin;direct oral anticoagulants;effect anticoagulants treatment;oral anticoagulants;anticoagulants treatment;effect anticoagulants;concentration direct anticoagulants;anticoagulants prothrombin complex;anticoagulants determined;anticoagulants;study effect anticoagulants;anticoaggulants mortality differs;direct anticoagulants;direct anticoaggulants mortality;anticoaggulants mortality;treated direct anticoaggulants;direct anticoagulants determined;treatment bleeding patients;patients treated prothrombin;anticoaggulants;bleeding patients treated;concentrates vicinity bleeding;treatment bleeding;direct anticoaggulants;severe bleeding patients;anticoagulants determined 62;patients severe bleeding;hospitalized severe bleeding", "pdf_keywords": ""}, "148f055083666c72945eea79833a19494f5f57c0": {"ta_keywords": "probability particle;probability finding particle;use probability particle;particles state;particle given state;number particles state;probability particle given;particles state second;state second method;method use probability;proportional number particles;number particles;particle;finding particle;particles;based fact probability;particle given;state proportional number;fact probability finding;given state proportional;state proportional;fact probability;use probability;probability;probability finding;finding particle given;given state;equivalence approaches;equivalence approaches method;proof equivalence approaches", "pdf_keywords": ""}, "924ce584acc148be29ef905c228fda7fe552c0c2": {"ta_keywords": "prediction probabilistic logics;markov logic networks;learning markov logic;reasoning large knowledge;logic networks;challenge probabilistic logics;probabilistic logics share;large knowledge bases;probabilistic logics;large knowledge base;probabilistic logics reasoning;stochastic logic programs;logics reasoning large;learning kb inference;knowledge bases;logic networks allowing;faster learning markov;structure large knowledge;knowledge bases kbs;earlier relational learning;relational learning;stochastic logic;ranking learning proppr;knowledge base;relational learning algorithm;logic programs;logic programs biased;propositional representation query;inference leads improvements;logics share scalability", "pdf_keywords": "database probabilistic logics;logics probabilistic inference;machine learning prolog;probabilistic logics;probabilistic logics probabilistic;datalog logic probabilistic;logic probabilistic;probabilistic datalog logic;probabilistic inference powerful;logics probabilistic;probabilistic datalog;probabilistic logic;logic probabilistic logic;relational database probabilistic;probabilistic logic used;database probabilistic;probabilistic inference machine;learning prolog;task probabilistic datalog;inference machine learning;probabilistic inference;approximate inference;benchmark inference tasks;small benchmark inference;inference machine;learning prolog proceedings;approximate inference scheme;tasks probabilistic inference;programming tasks probabilistic;new probabilistic machine"}, "8c4d1e81c277f71cd9e3c9a0af356203c7948dca": {"ta_keywords": "automatic transcriptional;automatic transcriptional evaluation;approach automatic transcriptional;automatic speech;corpuscle investigate effectiveness;transcription correction task;el corpus corpuscle;transcriptional evaluation el;documentation automatic speech;corpus corpuscle;el corpus;corpus corpuscle investigate;corpus;mixtec el corpus;propose transcription correction;automatic speech recognition;transcription;propose transcription;transcription correction;speech recognition;human transcribers;corpuscle;corpuscle investigate;transcriptional evaluation;transcriptional;study human transcribers;transcribers;automatic repeatability;automatic repeatability combined;speech recognition proposed", "pdf_keywords": "endangered language corpus;asr automatic speech;automatic speech;automatic automatic speech;automatic speech transcription;recognition endangered language;model automatic speech;speech recognition asr;language corpus;language corpus yoloxo;automatic speech recognition;speech recognition;speech transcription task;speech transcription;endangered language documentation;corpus;bottlenecks endangered language;automatic recognition endangered;archiving corpora lexicons;recognition asr;endangered language;source endangered language;endangered language el;corpus yoloxo;recognition asr suggested;conventional hmm approaches;speech recognition demonstrate;fusion automatic recognition;language documentation trained;corpora lexicons"}, "c5ed3d1a2ce418610a6fc9b5520a4f845279969a": {"ta_keywords": "erasure coded resilience;model erasure coding;erasure coding;parity model neural;parity model erasure;coded resilience prediction;erasure coded queries;imparting erasure coded;erasure coded;coded resilience distributed;resilience distributed computation;parity models new;applicability parity models;parity models;coded resilience;parity;introduce parity models;erasure coding technique;enabling erasure coded;trained transform erasure;resilience prediction serving;applicability parity;prediction serving;prediction serving systems;predictions services;systems parity;parity models image;efficient resilience data;models prediction serving;transform erasure coded", "pdf_keywords": ""}, "657329c633709dd1ac34a30d57341b186b1a47c2": {"ta_keywords": "sparse attention models;sparse attention patterns;temporal sparse attention;sparse attention;learning sparse attention;sparse attention efficiency;based sparse attention;comparable sparse attention;attention models language;attention models;attention layers;attention patterns;self attention layers;temporal sparse;attention patterns avoid;attention efficiency;attention;windows self attention;learning sparse;approach learning sparse;sequence modeling;local temporal sparse;content based sparse;attention layers paper;memory attend content;self attention;language modeling;sequence modeling problems;attention efficiency gains;attention recently", "pdf_keywords": "sparse attention patterns;sparse attention;dynamic sparse attention;attention model;examples attention model;language modeling deeper;attention model various;deep sequential;presents deep sequential;attention patterns;sparse representation sequence;generative modeling tasks;deep sequential neural;attention model based;examples attention;strided attention based;attention cache;attention based;attention;use attention;representation sequence vectors;use attention cache;text image generation;attention patterns avoid;sequential neural;attention based means;pairs examples attention;generative modeling;attention cache introduced;image generation characterlevel"}, "ba4a34680e09e77984624c95f5245d91b54373f6": {"ta_keywords": "cross lingual learning;cross lingual generalization;multilingual multi task;evaluating cross lingual;lingual generalization capabilities;lingual transfer evaluation;languages tasks;cross lingual;bilingual representations;evaluation multilingual encoders;research cross lingual;transfer evaluation multilingual;introduce cross lingual;lingual generalization;lingual learning methods;multilingual encoders xtreme;multilingual encoders;lingual learning;mult bilingual representations;cross lingual transfer;transfer linguistic knowledge;transfer linguistic;40 languages tasks;evaluation multilingual;languages tasks paper;multilingual;multilingual multi;bilingual;linguistic knowledge diverse;lingual transfer", "pdf_keywords": "predicted languages crosslingual;multilingual translation encoder;crosslingual transfer learning;translation encoder trained;crosslingual setting benchmark;languages crosslingual transfer;predicted languages;lingual performance crosslingual;correctly predicted languages;languages crosslingual;automatically translated test;cross lingual performance;learn deep contextual;cross lingual;multilingual translation;performance crosslingual;performance crosslingual setting;translation encoder;deep contextual;automatically translated;present multilingual translation;predicted english correctly;translation based baselines;multilingual;predicted english;crosslingual transfer;incorrectly predicted english;crosslingual;structured prediction tasks;lingual performance"}, "927ff874d3ed9307356d256c31b79a0624b3c9d5": {"ta_keywords": "speech recognition integrate;acoustic model training;speech recognition;swimmer noisy environment;swimmer noisy;combination automatic speech;automatic speech recognition;home swimmer noisy;automatic speech;reverberation overlapping speakers;new challenge neutrino;measuring neutrinoless;acoustic model;enhancement acoustic model;asr systems;measuring neutrinoless double;challenge neutrino;model vb hmm;reverberation;asr systems achieve;channel weighted prediction;hmm based overlap;neutrino flavour oscillation;performance home swimmer;vb hmm based;posterior fusion diarization;neutrinoless double beta;challenge neutrino flavour;neutrino;guided source separation", "pdf_keywords": "speech recognition chime;speech diarization recognition;speech recognition;speech recognition asr;automatic speech diarization;automatic speech recognition;challenge automatic speech;speech recognition speaker;automatic speech;recognition chime 2020;approach speech recognition;learning automatic speech;speech recognition based;recognition chime;speech diarization;describes speech recognition;diarization recognition everyday;field automatic speech;recognition speaker diarization;recognition speaker;deep neural networks;speaker diarization;recognition asr;diarization recognition;respectively automatic speech;chime 2020 challenge;recognition based deep;recognition everyday home;speaker diarization important;novel machine learning"}, "c4efaeccd7f0d900b1df95dadf51bad74264f613": {"ta_keywords": "optimal manipulation quantum;manipulation quantum probabilistic;randomized rules assignment;randomized rules;prominent randomized rules;computation equilibrium;quantum probabilistic;computation equilibrium profiles;profiles nash equilibrium;manipulation quantum;assignment problem agents;rules assignment problem;quantum probabilistic serial;rule dynamics;algorithms optimal manipulation;action parity violating;nash equilibrium;rules assignment;parity violating;preference profiles nash;rule expected utility;manipulability rules;rule dynamics team;optimal manipulation;quantum;rule prominent randomized;fairness welfare properties;polynomial time;probabilistic serial rule;action parity", "pdf_keywords": "algorithms compute preference;preferences present algorithm;compute preference;houses probabilistic assignment;probabilistic assignment objects;compute preference pro\ufb01le;probabilistic assignment fundamental;probabilistic assignment;agents based preferences;algorithm sequential allocation;algorithm assignment objects;preference pro\ufb01le nash;problem probabilistic assignment;sequential allocation indivisible;based preferences objects;assignment problem probabilistic;sequential allocation;allocation indivisible houses;expected utility best;assignment objects agents;indifferent houses probabilistic;allocation indivisible;houses probabilistic;allocation;algorithm assignment;preferences case agents;assignment objects;preferences objects indifferent;consistent ordinal preferences;preference pro\ufb01le"}, "605bae6c397e4829dde7ff7b8ddb84782ec6e607": {"ta_keywords": "map influenza;comprehensive map influenza;influenza virus;map influenza season;influenza viral;virus comprehensive map;proteins mrnas influenza;influenza viral infection;interactions flumap host;flumap host factors;interactions flumap;mrnas influenza;influenza;influenza researchers studies;mrnas influenza common;discussion influenza researchers;community discussion influenza;defend influenza viral;understood flumap comprehensive;influenza common infectious;discussion influenza;flumap host;flumap comprehensive;flumap comprehensive pathway;caused influenza virus;influenza common;influenza researchers;functional interactions flumap;influenza season presented;influenza virus available", "pdf_keywords": ""}, "a18b49fae647ae08711c2384611b3537485e8408": {"ta_keywords": "interpreter quantum computer;interpreter interpreter quantum;interpreter quantum;simultaneous interpreters learning;experienced simultaneous interpreter;interpreters learning;simultaneous interpreter;experience simultaneous interpreters;simultaneous interpreters;interpreters learning process;simultaneous interpreter present;simultaneous interpreters perform;interpreter;interpreters;data simultaneous interpreters;interpreter interpreter;new interpreter interpreter;new interpreter;interpreters perform;automatic speech translation;present new interpreter;interpreter present;quantum computer;incorporating simultaneous interpretation;simultaneous interpretation data;speech translation;automatic speech;interpreter present new;approach automatic speech;make easier understand", "pdf_keywords": ""}, "417259d40d0d8b3ca7ebdcf811aa9f7814d5c0c5": {"ta_keywords": "saxophone model using;fingering filter saxophone;measurement saxophone;estimating pitch fingered;derived measurement saxophone;fingering reed parameters;measurement saxophone configured;saxophone model;filter saxophone model;saxophone;filter saxophone;method estimating pitch;estimating pitch;fingerings sample recordings;saxophone configured possible;estimating reed;source parameters fingering;saxophone configured;reed parameters estimated;estimating reed source;estimated given recording;parameters fingering;fingering reed;parameters fingering filter;problem estimating reed;performance fingering identification;sample recordings notes;recordings notes;fingering identification evaluated;recordings notes produced", "pdf_keywords": ""}, "4302e981e3ec118b68e0b3fcf1820b3f6ecfa988": {"ta_keywords": "nanotube cnt thermoelectric;argumentation quality viewed;argumentation quality;theory argumentation quality;carbon nanotube cnt;carbon nanotube;cnt thermoelectric device;nanotube cnt;walled carbon nanotube;cnt thermoelectric;argumentation theory practical;nanotube;argumentation theory;differently argumentation theory;theory argumentation;comparisons arguments practice;thermoelectric device;based theory argumentation;thermoelectric;arguments practice;differently argumentation;viewed differently argumentation;argumentation;comparisons arguments;arguments practice correlate;single walled carbon;walled carbon;relative comparisons arguments;quantitative assessment;theory practical assessment", "pdf_keywords": ""}, "15251fa3a3bcf695bf153d0856886cab9a3145ea": {"ta_keywords": "yixinl7 refactoring summarization;refactoring summarization;refactoring summarization experimentally;text summarization;text summarization summaries;view text summarization;summarization summaries combination;summarization experimentally perform;summarization experimentally;refer techniques reranking;summarization;summarization summaries;techniques reranking stacking;techniques reranking;reranking stacking approach;reranking stacking;ranking stacking;based ranking stacking;summaries combination;summaries;reranking;ranking stacking researchers;yixinl7 refactoring;problem based ranking;com yixinl7 refactoring;stacking approach;ranking;refactor provides unified;refactor provides;refactoring", "pdf_keywords": "summarization pretrained encoders;summarization pretrained;text summarization pretrained;summary selecting sentences;abstractive sentence summarization;performance text summarization;summarization systems;candidate summaries generated;summaries generated single;sentence summarization paper;summaries generated;summarization systems formulating;sentence summarization;text summarization;text summarization systems;ranking candidate summaries;summarization;search multi summary;train neural abstractive;candidate summaries;modern text summarization;summarization paper;summarization paper investigate;construct summary;construct summary selecting;selecting sentences source;candidate summary results;multi summary;neural abstractive sentence;sentences source document"}, "f9e3b7c6ca7d534694148bd0c7c37c1ef896a784": {"ta_keywords": "speech recognition asr;automatic speech recognition;speech recognition;automatic speech;speaker speech recognition;speech recognition mqa;end automatic speech;codeswitching utterances;recognition sequence acoustic;sequence acoustic features;recognition asr systems;including codeswitching utterances;multi speaker speech;codeswitching utterances paper;recognition asr;multiple languages modeled;monolithic neural network;using 10 languages;multiple languages;utterances paper end;multi speaker end;sequence acoustic;multilingual multi;multi multi speaker;multi speaker;utterances;languagedependent modules proposed;multilingual;using monolithic neural;neural network architecture", "pdf_keywords": ""}, "400e083a18ab94bbf45b0820693fb5035684dd7c": {"ta_keywords": "utterances goal computer;meaning recognition spoken;spoken utterances goal;recognition spoken utterances;spoken utterances;utterances goal;sentence algorithm used;utterances;computation linguistic research;sentence algorithm;computation linguistic;given sentence algorithm;recognition spoken;natural language;meaning recognition;interaction natural language;linguistic research article;linguistic research;linguistic;problem meaning recognition;computer algorithm capable;area computation linguistic;human computer interaction;algorithm capable;goal computer algorithm;computer algorithm;algorithm capable construct;human computer;computer interaction natural;algorithm used", "pdf_keywords": ""}, "71a85e735a3686bef8cce3725ae5ba82e2cabb1b": {"ta_keywords": "pipelines based deep;practical ml pipelines;multilayer ml pipelines;ml pipelines;ml pipelines based;ml pipelines using;deep learning;deep learning anml;new machine learning;learning anml pipeline;pipelines;modeling pipelines;underspecification modeling pipelines;pipeline underspecified;pipeline;pipelines based;modeling pipelines intended;pipelines intended real;training domain;pipelines intended;pipelines using examples;pipelines using;generation multilayer ml;based deep learning;based deep;performance training domain;pipeline underspecified return;deep;clinical risk prediction;machine learning video", "pdf_keywords": "prediction policy;prediction bias;prediction individual genetic;prediction prediction bias;risk disease genome;machine learning robustness;predictive performance;bias machine learning;prediction bias machine;prediction policy prediction;deep learning case;underspecification machine learning;predictive performance determined;deep learning;genetic risk;individual genetic risk;literature prediction policy;learning robustness;policy prediction prediction;policy prediction;genetic risk disease;machine learning speci\ufb01cally;prediction;prediction individual;predictive;prediction prediction;set deep learning;risk minimizers;unique risk minimizers;sections predictive performance"}, "dd961bb9e2a70f3819a13b13402fe585ae384226": {"ta_keywords": "pure nash equilibrium;nash equilibrium np;nash equilibrium;equilibrium np complete;nash equilibrium yields;fairness welfare properties;quality equilibria;quality equilibria exist;equilibrium np;indivisible goods agents;compute pure nash;assigning indivisible goods;equilibrium yields assignment;design equilibria;truthful profile probabilistic;profile pure nash;probabilistic serial rule;evaluate quality equilibria;randomized rule assigning;strategyproof present probabilistic;design equilibria cycle;good fairness welfare;equilibria exist ps;fairness welfare;equilibrium;indivisible goods;serial rule randomized;known good fairness;probabilistic serial;equilibria tension", "pdf_keywords": "fairness probabilistic assignment;procedural fairness probabilistic;fairness probabilistic;allocation rules strategyproof;probabilistic assignment objects;strategyproof agents incentive;pure nash equilibrium;agents incentive;allocation rules;ensure procedural fairness;procedural fairness;probabilistic serial mechanism;probabilistic assignment;nash equilibrium;strategic behaviour randomization;serial mechanism social;mechanism social welfare;manipulations nash dynamics;agents ps rule;possible manipulations nash;allocation natural way;probabilistic serial ps;manipulations nash;behaviour randomization;nash equilibrium conp;agents express preferences;consider probabilistic serial;allocation;probabilistic serial;allocation natural"}, "86d55c5a098689438ceb1d52bdd768da3b47f55f": {"ta_keywords": "kalman consensus filtering;centralized kalman filter;based kalman consensus;kalman filter centralized;distributed tracking algorithm;centralized kalman;consensus filtering stochastic;approach distributed tracking;distributed tracking problem;distributed tracking;kalman consensus;proposes distributed tracking;competetive centralized kalman;centralized tracking;filter centralized tracking;chain based kalman;tracking algorithm markov;consensus filtering;kalman filter;tracking algorithm;based kalman;tracking problem dynamic;sensor activation tracking;filtering stochastic approximation;filtering stochastic;tracking problem;tracking time varying;tracking;stochastic approximation;filter centralized", "pdf_keywords": "centralized tracking problem;wireless sensor networks;sensor networks;sensor networks contend;sensors wireless network;algorithms centralized tracking;centralized tracking;active sensors wireless;sensor subset selection;sensors wireless;person wireless sensor;wireless sensor;sensor activation tracking;communication tracking;selection problem sensors;sensors simultaneously resource;control communication tracking;sensing control communication;sensor subset;sensors simultaneously;dynamic sensor activation;algorithm based stochastic;tracking problem dynamic;based stochastic approximation;communication tracking paper;dynamic sensor;person wireless;performance sensing control;tracking problem;tracking time varying"}, "2c0ebf5479db7f76c1e15512676c16b9032343fb": {"ta_keywords": "automatic transmission gear;automatic automotive transmission;shifts automatic automotive;gear shifts automatic;transmission gear shifts;automatic transmission;method automatic transmission;shifts automatic;gear shifts;automotive transmission;transmission gear;automatic automotive;transmission;shifts;automatic;gear;automotive;new method automatic;method automatic;present new method;new method;method;present new;paper present;paper present new;paper;new;present", "pdf_keywords": ""}, "0d360a1256ccdfca58cf98d12243df8407fd442d": {"ta_keywords": "defenses attacks nlp;trained models attacks;attacks nlp;weights injected vulnerabilities;sentiment classification toxicity;attacks nlp seen;detection spam;toxicity detection spam;spam detection attack;models attacks;untrusted pre trained;models attacks possible;spam detection;tuning enabling attacker;detection spam detection;experiments sentiment classification;trained weights;pre trained weights;trained weights injected;sentiment classification;backdoors fine tuning;vulnerabilities expose backdoors;expose backdoors;pre trained models;pose security threat;detection attack widely;trained models;weights pose security;detection attack;security threat", "pdf_keywords": "learn adversarial attack;adversarial attack pre;learn adversarial;vulnerabilities pre trained;adversarial attack;adversarial;model learn adversarial;weights vulnerabilities;weights vulnerabilities exploited;poisoning weights vulnerabilities;detection spam;poisoning text classi\ufb01cation;spam detection attack;models expose backdoors;trained models poisoning;toxicity detection spam;possible attacker introduce;expose backdoors;spam detection;detecting preventing vulnerabilities;poisoning text;detection attack widely;similar vulnerabilities pre;detection spam detection;expose backdoors \ufb01ne;preventing vulnerabilities;vulnerabilities pre;attack computer;introduce similar vulnerabilities;preventing vulnerabilities demonstrate"}, "c14254fd285706e549d0dcc57ae74680164c9afc": {"ta_keywords": "inverse reinforcement learning;inverse reinforcement;modeling passengers decisions;problem inverse reinforcement;risk sensitivity reinforcement;reinforcement learning markov;learning markov decision;mdp modeling passengers;passengers decisions;markov decision processes;sensitivity reinforcement learning;probabilities rewards mdp;passengers decisions regarding;agent risk;reinforcement learning framework;reinforcement learning;markov decision;modeling passengers;transition probabilities rewards;agent risk sensitive;learning markov;probabilities rewards;sensitivity reinforcement;reinforcement;decision processes agent;rewards mdp;model risk sensitivity;model risk;risk;risk sensitivity", "pdf_keywords": "inverse reinforcement learning;reinforcement learning valuation;risk preference agent;risk sensitive reinforcement;predicting risk preference;predicting risk;formulate inverse reinforcement;risk metric decision;employs convex risk;inverse reinforcement;convex risk metrics;sensitive reinforcement learning;convex risk;reinforcement learning algorithm;reinforcement learning framework;autonomy learning;models human decision;risk metric;learning valuation;risk preference;reinforcement learning;learning valuation functions;introduce risk metric;risk metrics;autonomy learning accurate;present reinforcement learning;prospect theory risk;risk metrics models;learning policy best;prediction description formulate"}, "e10dba1d4a56a81429d6ec4c9b7bdc15ea75474b": {"ta_keywords": "security malicious sensor;detection secure estimation;malicious sensor detection;malicious sensor;attack unknown sensor;sensor detection secure;secure remote estimation;secure estimation;problem malicious sensor;secure estimation considered;secure estimation false;detection secure;detect injection attack;attack cyber physical;unknown sensor subset;sensor observations;using sensor observations;data injection attack;attack cyber;measurements shared fusion;injection attack cyber;observations multiple sensors;sensor subset developed;estimating performance fusion;sensor subset;security malicious;remote estimation;sensor detection;relevant cyberphysical systems;cyberphysical systems", "pdf_keywords": ""}, "5403fd71810d098e572d9bd0f9ec10e96d6b6336": {"ta_keywords": "graph signal processing;graph symmetrization methods;processing graph symmetrization;signal processing graph;graph symmetrization;based graph signal;graph signal;graph developed gsp;processing graph;algorithms structural information;linear programming;iteration linear programming;symmetrization methods;algorithms structural;complexity reduction;networks large state;symmetrization methods used;tools graph signal;linear programming employed;signal processing gsp;complexity networks;computational complexity networks;probability transition graph;graphs;dynamic programming techniques;graph;graph developed;standard algorithms structural;complexity reduction performance;proposed based graph", "pdf_keywords": ""}, "967b2d10b8b378f1da43fd4d9107826e540e1112": {"ta_keywords": "animations natural language;embedding language pose;motion animations;language pose;language pose application;human animation;joint embedding language;learns joint embedding;concepts motion animations;motion animations proposed;linguistic concepts motion;embedding language;present language pose;human annotated sentences;generating animations natural;approaches generating animations;virtual human animation;generating animations;animation;animations;planning joint embedding;language pose paper;animations natural;animations proposed;embedding space learned;animation robot motion;concepts motion;human annotated;accurate animations;multimodal", "pdf_keywords": "language pose translation;pose translation task;language pose learn;sentences poses mapped;language pose model;pose translation;language sentences poses;pose data humanannotated;joint language pose;sentences poses;learn joint embedding;language pose;propose language pose;pose model animation;language pose work;pose learn;space language pose;corpus 3d pose;integrates language pose;poses mapped human;pose learn joint;language pose jl2p;animation sequences video;model animation generation;pose;pose model;generate animation sequences;propose joint embedding;joint embedding;poses"}, "0bdf1f3b79f4df5d5e11af1ea00379e1461e22fa": {"ta_keywords": "partial dependence plots;dependence plots;plots usually;selection interesting pdps;interesting plots usually;pdp generalization;plots single feature;model partial dependence;select interesting plots;dependence plots pups;automating selection interesting;proposed pdp generalization;plots;usually limited plots;neural network changes;plots pups;automating selection;plots pups widely;usefulness proposed pdp;plots usually limited;interesting pdps;interesting pdps extend;method automating selection;model response neural;generative model partial;partial dependence;pdp generalization multiple;feature spaces;pdps;interesting plots", "pdf_keywords": ""}, "d95973f0f0d86b758154e9a5f3d7434430d7856c": {"ta_keywords": "heisenberg antiferromagnet presence;heisenberg antiferromagnet;spin heisenberg antiferromagnet;dynamics spin heisenberg;antiferromagnet presence;antiferromagnet presence external;antiferromagnet;study dynamics spin;spin heisenberg;dynamics spin;estimating position particle;position particle noisy;particle noisy environment;confined harmonic trap;particle noisy;harmonic trap;presence external magnetic;position particle;gas confined harmonic;external magnetic field;heisenberg;electron gas confined;dimensional electron gas;confined harmonic;external magnetic;magnetic field;spin;magnetic field propose;electron gas;particle", "pdf_keywords": ""}, "194c5644c49e9e1b87990439fae05c98ba8b4fbb": {"ta_keywords": "scientific information extraction;annotating scientific text;material superconductors shown;superconductor normal metal;scientific text;superconductivity materials understood;superconductors;superconductors shown fig;material superconductors;scientific text shallow;parent material superconductors;superconductor;superconductors shown;resource annotating scientific;annotating scientific;superconductivity materials;scientific understanding materials;semantic structure large;semantic structure;extraction systems superconductivity;shallow semantic structure;tornado usa xmath0;text shallow semantic;systems superconductivity materials;study occurrence tornado;scientific information;superconductivity;structure large scale;superconductor normal;understanding materials", "pdf_keywords": "materials annotated semantic;materials syntheses text;materials science semantic;sentences inorganic materials;natural language organic;synthesized materials annotated;semantic dependency parsing;synthesis sentences inorganic;language organic materials;extracting semantic;dataset synthesized materials;extracting semantic structures;annotated semantic relationships;annotated semantic structure;supervised entity tagging;semantics synthesis sentences;structures natural language;materials annotated;semantics synthesis;annotated semantic;semantic structures natural;semantic structure domain;materials syntheses;semantic relationships text;method extracting semantic;synthesizing inorganic materials;tagging models;sentences inorganic;entity tagging models;semantic structures"}, "03b68259f9e70d2007d40e5331c9ff31f2bb46b9": {"ta_keywords": "modeling human activity;proposes activity recognition;activity models;activity based data;activity recognition;adapts activity models;activity models end;activity recognition method;human activity based;activity using labeled;activity using;human activity;user activity using;user activity;activity based;automatically adapts activity;acceleration sensor data;activity;adapts activity;data collected crowd;sensor data;training data users;modeling human;end user activity;training data;approach modeling human;applying sensor data;sensor data method;unlabeled sensor data;sensor data obtained", "pdf_keywords": ""}, "e11d6a031d5f85f372b0fda3ab62ca4ce2d89f2c": {"ta_keywords": "quantum circuits based;simulation quantum circuits;quantum computing;based quantum computing;architecture simulation quantum;gate based quantum;quantum circuits;quantum computing capable;quantum circuit;simulation quantum;entanglement swapping control;idea entanglement swapping;based quantum;circuits based idea;rules rule generation;rule generation;optimizes combinational logic;scalable architecture simulation;design space quantum;entanglement swapping;design performance gate;circuits based;rule generation module;gate operations;space quantum circuit;quantum circuit reduced;performance gate based;automatically encodes rules;expert optimizes combinational;gate based", "pdf_keywords": ""}, "90fbeb4c871d3916c2b428645a1e1482f05826e1": {"ta_keywords": "quantum memories encoder;quantum state encoder;decoder architecture quantum;encoder class quantum;quantum memories;architecture quantum repeaters;memories encoder;class quantum memories;quantum repeaters;state art encoder;memories encoder outputs;implementation quantum;encoder decoder architecture;encoder decoders;encoder decoder systems;state encoder;encoder decoder;encoder decoder learning;architecture quantum;encoder;quantum repeaters paper;implementation quantum state;encoder decoders special;conventional encoder decoders;modular encoder decoder;novel encoder decoder;present modular encoder;conventional encoder;modular encoder;propose novel encoder", "pdf_keywords": "sequence sequence learning;sequence learning;end memory networks;recurrent neural networks;tasks image captioning;decoder model trained;encoder decoder learning;captioning results review;captioning source;image captioning source;captioning text;image captioning text;image captioning;captioning;recurrent neural;memory networks;code captioning;decoder learning;captioning text summarization;captioning source code;sequence learning based;learning based recurrent;captioning results;including image captioning;code captioning results;attentive encoder;performance attentive encoder;incorporating discriminative supervision;encoder decoders datasets;attention memory"}, "ddd74358d7e11535ee77e2c323dd662d115a0f20": {"ta_keywords": "learning robot policy;learning robot;objects training;trained augmented reality;observe objects training;objects training present;robot policy;trained augmented;learned map;quadcopter context learning;conditioned object grounding;robot;present learned map;learning map;motion object action;robot policy follow;learned map representation;object locations instructed;learning previously unseen;introduce shot language;problem learning robot;object action;method trained augmented;shot language;grounding method trained;learning map raw;object action external;object grounding;shot language conditioned;learning previously", "pdf_keywords": "vision language navigation;vision language;applicable vision language;natural language object;ground natural language;robot representation learning;train quadcopter drone;object annotations;object annotations commonly;trained augmented reality;drone able autonomously;robot representation;approach language grounding;propose robot representation;drone;introduce shot language;shot language;language grounding;trained augmented;quadcopter drone;object mentions;conditioned object grounding;object mentions observations;robot;exemplars identify objects;combines visual language;visual language;language object mentions;method trained augmented;quadcopter drone able"}, "97943a5dee3c6e36d01a6099acb9ec360ad0ee19": {"ta_keywords": "portmanteaus word formation;word formation;character level neural;neural sequence sequence;generation portmanteau;neural sequence;level neural sequence;exhaustive candidate generation;word lists improving;word formation phenomenon;word experiments approach;words combine new;word experiments;unsupervised word lists;task portmanteaus word;generation portmanteau propose;combine new word;portmanteaus word;words combine;candidate generation strategy;features portmanteau;new word experiments;features portmanteau task;word lists;portmanteau propose noisy;accuracy human evaluation;menudo women competition;sequence s2;methods generation portmanteau;neural", "pdf_keywords": "translation predicting portmanteaus;machine translation predicting;neural machine translation;predicting portmanteaus spoken;spoken language portmanteaus;translation predicting;language portmanteaus;machine translation model;portmanteaus spoken language;end neural machine;words generate portmantes;translation model predict;language model vocabulary;language model;model vocabulary words;trainable language;incorporate language model;machine translation;features phonetic similarity;words generate;use additional phonetic;additional phonetic information;end end neural;capture features phonetic;toend trainable language;input words generate;neural sequence tosequence;features phonetic;model predict portmanteaus;language portmanteaus frequent"}, "30f86d38f0660af5ea2e16d996434c72eee8c5ee": {"ta_keywords": "speech recognition pasr;automatic speech recognition;automatic speech;speech recognition;speech recognition based;approach automatic speech;end automatic speech;recognition based augmented;petawatt like device;recognition pasr;based augmented;recognition pasr paper;based augmented lagrangian;augmented lagrangian formalism;software;open source software;software platform;augmented lagrangian;open source;speech;augmented;automatic;source software;recognition based;petawatt;end end automatic;new open source;software platform named;lagrangian formalism;end automatic", "pdf_keywords": "speech recognition toolkit;speech processing toolkit;speech recognition asr;hybrid automatic speech;platform speech recognition;end speech recognition;end speech processing;automatic speech;hmm dnn systems;hybrid hmm dnn;end platform speech;automatic speech recognition;speech recognition;recognition speech processing;platform speech;speech processing;asr toolkits;recognition speech;toend speech processing;speech recognition speech;end automatic speech;speech recognition systems;speech processing named;source asr toolkits;dnn systems legacy;recognition asr based;asr toolkits experimental;hybrid hmm;recognition asr;open source asr"}, "36bca9d41de386fce5dce06999a45a802a7c4f41": {"ta_keywords": "combinatorially constrained ciphers;conditional preference networks;ciphers ccf nets;preference networks;constrained ciphers ccf;preference networks cp;random conditional preference;preferences method computationally;combinatorial networks;ccf nets;constrained ciphers;xmath0 nets uniformly;nets uniformly random;class combinatorial networks;classical ccf nets;acyclic xmath0 nets;ciphers ccf;modeling preferences;combinatorial networks called;ccf nets discuss;formalism modeling preferences;combinatorially constrained;xmath0 nets;modeling preferences method;networks called combinatorially;called combinatorially constrained;nets uniformly;networks cp nets;ciphers;conditional preference", "pdf_keywords": ""}, "0c5bfa2d4bb351a479073cb358c3ae6f7ecf0476": {"ta_keywords": "nlp libraries provides;language network detailed;nlp libraries;language network;based language network;used nlp libraries;nlp frameworks implementing;processing nlp;implementing natural language;nlp frameworks;language processing nlp;established nlp frameworks;processing nlp requires;annotators python;annotators python interface;natural language processing;annotations data structures;widely used nlp;natural language;language processing;corpora annotations;easily use annotators;corpora annotations data;nlp requires;access annotators python;annotators;established nlp;reading corpora annotations;annotations;nlp requires considerable", "pdf_keywords": ""}, "a2aa642db090b3aa28a44ccbc3c51fdb0be8335b": {"ta_keywords": "generalization neural parsers;treebanks neural parsers;domain treebanks neural;neural parsers substantially;neural parsers;representations neural parsers;neural parsers provide;benchmark treebanks neural;neural nonneural parsers;parsers substantially improves;improvement domain treebanks;neural parsers able;parsers able predict;state art parsing;domain treebanks;neural parsers zero;treebanks neural;benchmark treebanks;nonneural parsers;results benchmark treebanks;parsers substantially;parsers;nonneural parsers new;parsers new domains;parsers provide;parsers new;treebanks;parsers able;training trees corpus;parsers provide state", "pdf_keywords": "augmenting neural parsers;parsers augmenting neural;neural parser models;recent neural parsers;neural parsers incorporate;neural parsers;art neural parser;neural parser;neural parsers pre;encoder parser structured;structure neural parsers;neural parsers obtain;generalization neural parsers;parsers pre trained;encoder parser;neural constituency parsers;strong performance parsing;parsers augmenting;domain parsers augmenting;word embeddings improve;neural parsers recent;parser models;embeddings improve performance;encoder encoder parser;embeddings improve;deep neural;neural models rich;trained word embeddings;decoder neural models;rich representations language"}, "de5834305ea419c25b17f0c8d27bad6a5feb311a": {"ta_keywords": "language descriptions chess;chess commentary dataset;descriptions chess games;descriptions chess;chess commentary;generating natural language;generate commentary;scale chess commentary;natural language generation;commented given chess;commentary individual moves;language generation data;natural language descriptions;generate commentary individual;chess games;chess games propose;language generation;commentary frequently depend;pragmatic aspects game;methods generate commentary;chess;commentary dataset propose;given chess human;chess human study;commentary texts;commentary dataset;moves chess;natural language;large scale chess;commentary", "pdf_keywords": ""}, "47234fca1b14666d72bc5df0e2d911ff7cdea688": {"ta_keywords": "algorithms random hypergraph;hypergraph model weighted;random hypergraph model;random hypergraph;hypergraph model;hypergraph;nodes weights hyperedges;pairwise similarity information;polytime algorithms random;measures modeled nodes;weighted stochastic block;spectral clustering;spectral clustering popular;hyperedges respectively spectral;large number edges;binary edge weight;polytime algorithms;weights hyperedges;based pairwise similarity;respectively spectral clustering;weights hyperedges respectively;binary edge;popular algorithm partitions;partitioning graph;weighted stochastic;analysis polytime algorithms;modeled nodes;pairwise similarity;modeled nodes weights;nodes", "pdf_keywords": "hypergraph clustering algorithms;clustering weighted hypergraphs;hypergraph clustering;spectral clustering;spectral clustering weighted;develop hypergraph clustering;prior spectral clustering;weighted hypergraphs;algorithm subspace clustering;spectral clustering step;problem spectral clustering;hypergraphs size edges;subspace clustering;subspace clustering based;weighted hypergraphs size;hypergraphs;sketching algorithm subspace;hypergraph;hypergraphs size;clustering algorithms;clustering weighted;clustering step ii;clustering step;computationally ef\ufb01cient algorithms;clustering;ef\ufb01cient algorithms;compute edge weights;develop hypergraph;sample complexity computational;clustering algorithms preprocessing"}, "9a41111cf881b052555985bd8cf304ef9fc4f6d5": {"ta_keywords": "structured corpus;small structured corpus;structured corpus improve;distant labeling information;augmenting large corpus;large corpus coupling;large corpus;structure distant labeling;labeling information extraction;corpus improve performance;corpus coupling;corpus coupling constraints;distant labeling;document structure distant;corpus;labeling information;corpus improve;information extraction;information extraction suffers;labeling;label interaction identifying;analysis document structure;document structure;interaction identifying label;label interaction;inter label interaction;identifying label labels;noisy training data;structure distant;structured", "pdf_keywords": "entitycentric corpus oriented;entitycentric corpus;novel entitycentric corpus;supervised information extraction;corpus oriented method;label propagation extract;corpus oriented;distant supervised information;information extraction;extract objects relations;distant supervised;document relation;label propagation;supervised information;entitycentric;relation query extract;subject document relation;propose novel entitycentric;document relation query;relations given document;corpus;corpora sections;information extraction evaluate;uses label propagation;particular corpora;associated distant identifying;novel entitycentric;distant supervision approach;particular corpora sections;classical distant supervision"}, "c8a95217cde1bc893b230297250918818aa01dd7": {"ta_keywords": "mobile body underdamped;fluid dynamics video;fluid dynamics;movement mobile body;body underdamped environment;dynamics;dynamics video;movement mobile;underdamped environment;body underdamped;dynamics video movement;underdamped environment used;mobile body;movement;video movement mobile;underdamped;video movement;fluid;mobile;gain insight dynamics;insight dynamics;video;body;environment;environment used gain;environment used;gain insight;gain;used gain insight;used gain", "pdf_keywords": ""}, "71cdf94d13cc6c497dcc2dcb20893fe64cfaf62e": {"ta_keywords": "text generation;continuations text generation;text generation model;text generation desired;topics judged automated;guide text generation;generates fluent sentences;topics predicting;candidate topics predicting;word clusters;word clusters possible;topics predicting centers;automated metrics crowdsourced;based language models;chosen topics;text written far;self supervised;self supervised machine;language models aid;interactive writing;related chosen topics;language models;chosen topics large;text written;components self supervised;centers word clusters;set candidate topics;new interactive writing;crowdsourced;chosen topics judged", "pdf_keywords": "automated story generation;story generation deep;story generate continuations;story generation;story generate;generate continuation narrative;representations automated story;continuations text generation;text generation model;text generation;automatic generation text;generation text;stories event representations;generation narrative stories;developments story generate;generating text;generation narrative;generation deep neural;continuation narrative way;generation text conditional;text promotes narrative;automated story;narrative stories event;model generation narrative;continuation narrative;event representations automated;method generating text;generation deep;interactive writing assistants;generating text promotes"}, "edb49aa423afc210facec998277923c4b75e4648": {"ta_keywords": "crse4 antiferromagnetically;properties crse4 antiferromagnetically;crse4 antiferromagnetically frustrated;magnetic properties crse4;magnetic coupling crse4;crse4 chains structural;phase transitions zncr2se4;antiferromagnetic phase transitions;transitions zncr2se4 studied;antiferromagnetic phase transition;coupling crse4 chains;diffraction cr3 spin;structural antiferromagnetic phase;crse4 chains;coupling crse4;structural antiferromagnetic;antiferromagnetic phase;zncr2se4 studied;antiferromagnetically frustrated spin;structural properties spinelncr2se4;transitions zncr2se4;occurs antiferromagnetic phase;properties spinelncr2se4 studied;spin heisenberg antiferromagnet;properties crse4;zncr2se4 studied using;spinelncr2se4 studied;magnetic interaction cr3;110 structural antiferromagnetic;crse4", "pdf_keywords": ""}, "4e749b2e0728044af44d50a708fc99d49359ea0b": {"ta_keywords": "sequence models attention;sequence models decoding;transduction artificial neural;encode structural linguistic;sequence sequence models;character level transduction;models decoding;sequence models;models attention;performance unsupervised character;models attention paper;decoding;models designed encode;artificial neural;encode structural;unsupervised character level;structural linguistic;unsupervised learning;unsupervised character;neural;script language translating;script language;unsupervised tasks testbeds;models decoding time;languages;unsupervised learning scenario;structural linguistic knowledge;unsupervised tasks;attention;state sequence sequence", "pdf_keywords": "unsupervised machine translation;neural model multilingual;multilingual language modeling;machine translation tasks;unsupervised models monolingual;model multilingual language;machine translation;translation tasks decipherment;model multilingual;translation tasks;unsupervised sequence transduction;models monolingual data;language modeling;models monolingual;related language translation;language translation;sequence transduction models;language modeling paper;monolingual data;monolingual data leipzig;multilingual;multilingual language;base models decoding;learning varying alignment;unstructured sequences transduction;models decoding;writing systems russian;unannotated input texts;train unsupervised models;related language"}, "9700940262cd5e797ab81eee464c3b3a16295cba": {"ta_keywords": "dynamic speech enhancement;based speech enhancement;speech enhancement approaches;speech enhancement scheme;speech enhancement;reverberation speech enhancement;speech enhancement preprocess;capabilities speech enhancement;speech enhancement known;new dynamic speech;dynamic speech;speech recognizer model;dynamic capabilities speech;model based speech;speech recognizer;noise reverberation speech;automatic speech;automatic speech recognition;known automatic speech;speech features acoustic;speech recognition performs;speech recognition;enhancement known automatic;interconnect speech recognizer;speech features;reverberation speech;features acoustic model;mismatch speech features;static mismatch speech;adaptive training scheme", "pdf_keywords": ""}, "0d2a1c0724743de0cb74463466b075598ba36c45": {"ta_keywords": "position woman hospital;hospital location woman;woman hospital location;location woman hospital;woman hospital;position woman;hospital location;location woman;relationship position woman;position;relationship position;examine relationship position;woman;hospital;location;examine relationship;relationship;paper examine relationship;examine;paper examine;paper", "pdf_keywords": ""}, "f01f4808263ecfa221f856c34d3420166dbf5930": {"ta_keywords": "classifier trained;classifier;neural network;detector car navigation;neural networks;recurrent neural networks;performance classifier trained;status detector car;short term memory;neural network recurrent;performance classifier;car navigation;network recurrent neural;detector car;neural networks long;confusion status detector;classifier trained feedforward;particle environment interaction;study performance classifier;particle interaction strength;trained feedforward neural;network recurrent;feedforward neural network;feedforward neural;particles interaction strength;car navigation proactively;particle;interaction particle environment;interaction strength particle;driver", "pdf_keywords": ""}, "ff7b5379641875be7357766af0b1e2bd55c74cc8": {"ta_keywords": "transformer information corpus;information retrieval;information retrieval accomplished;information corpus encoded;learns text;learns text text;models corpus sizes;retrieval;search index dsi;interplay models corpus;corpus encoded parameters;models corpus;memory differentiable search;corpus encoded;information corpus;demonstrate information retrieval;paradigm learns text;retrieval accomplished single;study documents identifiers;retrieval accomplished;corpus sizes transformer;documents identifiers represented;dual encoder models;directly relevant docids;search index;text model;differentiable search index;encoder models;text model maps;documents identifiers", "pdf_keywords": "atomic identi\ufb01ers indexing;document retrieval autoregressive;retrieval autoregressive entity;autoregressive entity linking;sequence sequence learning;document retrieval;identi\ufb01ers indexing;semantic identi\ufb01ers;semantics associated document;entity linking;units document retrieval;sequence learning;automatically create identi\ufb01ers;document structured;sequence learning sutskever;capture information semantics;sequence seq2seq approach;document document structured;generates identi\ufb01ers;indexing training;indexing;document structured way;retrieval;focus semantic identi\ufb01ers;indexing units document;structured way search;direct indexing;unstructured atomic identi\ufb01ers;inputs2targets indexing strategy;use direct indexing"}, "3cd4ae1cac866f853bb3276d215cff18df371b67": {"ta_keywords": "decoding aerobicobicobic;noisy speech recognition;mbr decoding aerobicobicobic;decoding aerobicobicobic support;discriminative feature transformation;speech recognition promising;discriminative languagemodeling dlm;speech recognition;discriminative feature;discriminative languagemodeling;decoding asr;discriminant feature transform;augmented discriminative feature;feature transform efficient;feature transform;decoding;recognition promising results;recognition promising;method discriminative languagemodeling;reverberated noisy speech;risk mbr decoding;neural networks reverberated;based binary masking;aerobicobicobic support alignment;noisy speech;feature transformation;mbr decoding asr;deep neural;recognition;binary masking", "pdf_keywords": ""}, "ceef266c59698999c9283a0cda852d8bc1ce27ea": {"ta_keywords": "isotropy embedding;isotropy embedding space;extent isotropy embedding;learn word representation;trained contextual word;word representation experiments;isotropy enhancements;pre trained contextual;isotropy enhancement;existing isotropy enhancement;isotropy enhancements propose;contextual word representation;result isotropy enhancements;trained contextual;isotropy enhancement methods;makes existing isotropy;representation pre trained;elongated directions embedding;embedding space changes;directions embedding space;existing isotropy;embedding space contrast;isotropy;word representation pre;directions embedding;extent isotropy;word representation;network learn word;embedding;neural network learn", "pdf_keywords": "pre trained embedding;word representations embedding;trained embedding space;trained embedding spaces;embedding space transfer;trained embedding;deep embedding;deep embedding models;deep nlp models;performance deep embedding;embedding models contextual;linguistic knowledge deep;embedding spaces pre;contextualized word representations;knowledge deep nlp;deep nlp;embedding models;trained language models;embedding space models;changes embedding space;word representations;embedding spaces used;embedding;changes embedding;representations embedding;embedding space;embedding spaces;spaces pre trained;representations embedding spaces;nlp models pre"}, "b719fc66b173f8e9e0624317bb00abf10a4d5606": {"ta_keywords": "temporal segmentation basketball;basketball game video;segmentation basketball;game video segmentation;video shot basketball;segmentation basketball game;video shows basketball;segmentation video shot;boundary detection video;edge detection video;detection video shot;frame extraction video;segmentation video;game video shots;temporal segmentation video;video segmentation;detection video;shows basketball game;video image;extraction video;segmentation based video;shows basketball;frames edge detection;game video;video image frame;video segmentation based;basketball game using;based video image;image frame extraction;detection rate video", "pdf_keywords": ""}, "18ef33a6e040b49ba475e586202932cecbafba0d": {"ta_keywords": "event influence generation;generate event influences;event influence detection;reasoning events tracking;relevance generations reasoning;generation based event;reference relevance generations;generations reasoning events;event influences generated;question answering;influence distance reasoning;influence generation based;relevance generations;based event influence;question answering wiqa;influence generation;events tracking influences;event influence;reasoning events;influence detection;performance question answering;event influences;models generate event;based event;language models generate;generate event;influences generated eigen;context nature influence;reasoning chain;events", "pdf_keywords": "context event extraction;event in\ufb02uence generation;generate event in\ufb02uences;event extraction;context relationship events;language models generate;task generating;studied event understanding;generation event in\ufb02uences;models generate event;generating novel event;novel task generating;generate event;event extraction paper;generate targeted in\ufb02uences;events distance reasoning;task generating novel;event understanding context;large corpus based;events;trained language models;event based;based background reasoning;reasoning chain propose;generation event;corpus based wiqa;event in\ufb02uences;background reasoning;reasoning chain;event understanding"}, "e5acad5bba23a8c3a9f7cd24f7694ab10357ebc7": {"ta_keywords": "separation performance reverberant;speech separation recognition;reverberant multi speaker;separation recognition singlechannel;speech dereverberation separation;reverberant acoustic waveguides;dereverberation beamforming speech;beamforming speech recognition;reverberant acoustic;speaker speech separation;speech separation;reverberant multi;performance reverberant multi;recognition singlechannel multichannel;recognition singlechannel;single photon entangled;performance reverberant;photon entangled;multi speaker;beamforming speech;anechoic reverberant acoustic;photon entangled states;entangled states end;speech dereverberation;channel e2 asr;dereverberation separation performance;multi channel e2;multi speaker speech;reverberant;entangled states", "pdf_keywords": "beamforming speech recognition;speech recognition improved;good speech enhancement;voice activity detection;speech enhancement performance;speech recognition;speech enhancement;voice activity;recognition speech;speech recognition speech;dereverberation beamforming speech;subnetworks including voice;recognition speech recognition;speaker reverberant;speaker condition optimized;automatic speech recognition;beamforming speech;automatic speech;deep learning framework;framework automatic speech;use voice activity;deep learning;multi speaker reverberant;reverberant multi speaker;deep neural networks;multichannel multi speaker;speaker reverberant condition;including voice activity;recognition based deep;use voice"}, "4b9b7240ef9b6bc442044684ed5646ef02897d87": {"ta_keywords": "planning competition domain;strategy moshinsky moriya;winning strategy moshinsky;planning competition;domain planning competition;strategy moshinsky;strategy based competition;winning strategy based;competition winning strategy;winning strategy;competition domain;competition domain exhibits;mmo model competition;strategy based;advising domain planning;moshinsky moriya mmo;model competition;based competition;domain planning;model competition winning;based competition mmo;strategy;planning;competition mmo mno;study competition winning;competition mmo;competition;theoretical study competition;tournament baikal;international tournament baikal", "pdf_keywords": ""}, "c3aa698b562e91f78a042b938ffce1877b6e859c": {"ta_keywords": "people brought contact;count number people;contact interacted;brought contact interacted;elegant way count;number people;number people brought;count number;way count number;brought contact;way count;count;contact;interacted;people brought;number;people;simple elegant;simple elegant way;present;elegant way;paper present;present simple elegant;paper present simple;present simple;brought;elegant;paper;simple;way", "pdf_keywords": ""}, "728a6850882a0d8ef5551949cc2baee1e1667cd8": {"ta_keywords": "optimal bribery schemes;optimal bribery;finding optimal bribery;bribery schemes voting;bribery schemes;bribery problem;motion cases bribery;bribery problem easy;bribery;schemes voting;cases bribery problem;schemes voting domains;cases bribery;agents preferences represented;complexity finding optimal;swimmer crowded environment;motion swimmer crowded;variables agents;set variables agents;computational complexity;swimmer crowded;optimal;variables agents preferences;swimmer motion cases;voting domains;computational;dynamics swimmer motion;computational complexity finding;finding optimal;candidate set cartesian", "pdf_keywords": ""}, "df8ae2068d17d969db6ab2d27108776e99413975": {"ta_keywords": "knowledge solve nli;natural language;text graph based;techniques text graph;text graph;graph based models;graph text graph;graph text;text graph text;structured knowledge;natural language mammals;team based analysis;explanation natural language;external knowledge solve;use structured knowledge;graph based;large scale datasets;textual information;analysis large scale;textual;structured knowledge received;knowledge solve;learning based;techniques text;use textual information;external knowledge;lipkin meshkov;models discuss;use textual;based analysis large", "pdf_keywords": "crowdsourcing textual entailment;entailment crowdsourcing textual;entailment crowdsourcing;approach entailment crowdsourcing;crowdsourcing textual;text models graph;graph concepts attention;wordnet;network models entailment;textual entailment;sources including wordnet;word attention models;text graph based;textual entailment important;techniques text graph;text models;models entailment;crowdsourcing;graph text graph;text graph;concepts attention model;conceptnet;novel approach entailment;entailment important tasks;attention models widely;textual content;text graph text;attention models;lstm graph concepts;external knowledge solve"}, "a7abd783de8d21d640e41d31ec89f2c1caec4e42": {"ta_keywords": "unsupervised learning wadyswadysm;interpretable knowledge free;knowledge free interpretability;predictions human readable;unsupersupervised knowledge free;disambiguation wsd knowledge;interpretable knowledge based;new unsupervised learning;unsupervised learning;interpretable knowledge;word sense disambiguation;sense disambiguation wsd;free interpretability predictive;tend interpretable knowledge;unsupersupervised knowledge;knowledge free;completely unsupersupervised knowledge;interpretable word sense;sense disambiguation;sense representations disambiguous;word sense inventories;free interpretability;sense representations;sense predictions human;predictions presented tool;learning wadyswadysm wsd;unsupervised safe;wsd knowledge based;providing interpretable word;dynamics video unsupervised", "pdf_keywords": "word sense disambiguation;sense disambiguation word;sense disambiguation;disambiguation word senses;word sense representations;unsupervised disambiguation word;unsupervised disambiguation;sense disambiguation bridges;unsupervised disambiguation learns;semantics word sense;word sense central;cluster word features;lexical semantics word;lexical semantic network;word senses based;word sense;hypernyms word cluster;lexical semantic;word cluster;lexical semantics;disambiguation word;text word sense;computational lexical semantics;word senses;approach word sense;disambiguation learns;particular word senses;senses based cluster;word cluster present;unsupervised knowledge free"}, "58961f0ea3291ddab697fbe5be999a0793b0efaf": {"ta_keywords": "storage overhead quantum;lower bound storage;overhead quantum erasure;quantum erasure channel;erasure codes provide;erasure codes;storage overhead codes;bound storage;overhead codes existence;quantum erasure;decodability subsets unnecessary;decodability subsets;use erasure codes;bound storage overhead;codes existence;randomized construction decodability;storage overhead distributed;distributed storage;overhead distributed storage;codes existence codes;erasure channel;construction decodability subsets;distributed storage systems;existence codes;decodability;subset nodes decode;satisfy decodability;bound randomized construction;codes certain subsets;codes provide tolerance", "pdf_keywords": ""}, "f75e691daae9133941c9a083e319b39bd837d456": {"ta_keywords": "embeddings entity alignment;knowledge embeddings entity;entity alignment joint;joint knowledge embeddings;entity alignment improve;alignment joint knowledge;improvements entity alignment;knowledge graph completion;entity alignment;entity alignment aims;entities wikipedia links;knowledge embeddings;entities wikipedia;alignment improve knowledge;improve knowledge graph;approach entity alignment;link entities counterparts;embeddings entity;link entities;aligned entities;knowledge graph;information entities wikipedia;encodes entities relations;knowledge graphs;multiple knowledge graphs;knowledge embeddings experiments;entities counterparts;entities relations;entities counterparts multiple;aligned entities paper", "pdf_keywords": ""}, "80a085a79ac6cee94f21d21ab8ca302458c4e131": {"ta_keywords": "privacy based loss;tradeoff privacy accuracy;privacy accuracy provider;asymmetric tradeoff privacy;loss accuracy cloud;privacy accuracy propose;privacy accuracy;cloud hosted inference;accuracy cloud hosted;tradeoff privacy;accuracy cloud;services raises privacy;privacy;communicated data cloud;novel privacy;cloud;novel privacy based;dnns shows shredder;data cloud;hosted inference services;shredder learns additive;trusted data;hosted inference;accuracy provider trusted;loss function exploits;privacy based;proposes novel privacy;privacy concerns experimentation;shredder learns;raises privacy", "pdf_keywords": "deep private feature;dataset differential privacy;accuracy tradeoff privacy;private feature extraction;privacy accuracy controlled;privacy accuracy;differential privacy;novel privacy measure;tradeoff privacy accuracy;privacy preserving;privacy protection;privacy measure;novel privacy protection;quantify privacy;private feature;privacy;proposed quantify privacy;propose novel privacy;novel privacy;privacy measure based;shredder maximizes privacy;presents novel privacy;privacy management;differential privacy originally;maximizes privacy;maximizes privacy minimal;privacy protection mechanism;privacy preserving datamining;new approach privacy;deep private"}, "464b47a6a395fa1338e230254965cf5f669e715c": {"ta_keywords": "movement animals;model babbling basins;babbling basins based;animals involved movement;model babbling;probabilistic model babbling;movement animals crowded;video movement animals;polysynthetic languages interestingly;polysynthetic languages;babbling basins;neural machine translation;neural machine;dynamics;fit polysynthetic languages;machine translation;basins based character;fluid dynamics video;behavior animals;animals crowded environment;languages;neural;fluid dynamics;languages interestingly;dimensional model biological;dynamics video movement;dynamics video;model biological systems;systems fluid dynamics;movement", "pdf_keywords": ""}, "ca57443fcb87f03267fccee162a4924c56062c6f": {"ta_keywords": "position pedestrian crowded;track position pedestrian;pedestrian crowded environment;pedestrian crowded;position pedestrian;pedestrian;track position;method track position;crowded environment;track;crowded;method track;position;new method track;method;new method;present new method;paper;paper present;environment;new;paper present new;present;present new", "pdf_keywords": ""}, "d3304b926cfcd91110bd5ba01db21d26ce5fca2d": {"ta_keywords": "learning general paraphrastic;paraphrastic sentence embeddings;human written sentences;machine translation;explore machine translation;sentence embeddings;machine translation output;sentence embeddings setting;paraphrastic;general paraphrastic;training sets languages;written sentences finding;general paraphrastic sentence;sentences;paraphrastic sentence;written sentences;sentences finding;embeddings;language pairs;data language pairs;language pairs data;languages;language;learning general;learning;data language;generate large training;large training sets;problem learning general;translation output", "pdf_keywords": "paraphrasing text corpus;machine translation;machine translation train;paraphrasing bilingual parallel;automatically embedding paraphrasing;paraphrasing bilingual;machine translation techniques;machine translation architecture;generated paraphrase corpus;embedding paraphrasing text;embedding paraphrasing;paraphrases language encoders;corpus sentential paraphrases;consider machine translation;paraphrase corpus;sentential paraphrases language;improve generated paraphrase;paraphrase corpus explore;machine translation based;paraphrasing text;bilingual parallel corpora;use machine translation;sentential paraphrases;methods machine translation;paraphrases language;improve translation quality;translation score sentences;variety machine translation;text corpus sentential;translation use machine"}, "305a1251a68fb16835876d8c99de498472c0cd8f": {"ta_keywords": "coded computation emerging;coded computation significantly;enables coded computation;coded computation schemes;coded computation scheme;coded computation;known coded computation;code proposed locality;coded computation lens;locality codes;view coded computation;best known coded;locality codes rederives;art coded computation;computation lens locality;schemes coded computation;known coded;approach enables coded;coded;distributed computing;scale distributed computing;enables coded;coding;computation emerging;high performance computing;state art coded;muller code proposed;coding theory;distributed computing applies;proposed locality based", "pdf_keywords": ""}, "3332dc72fbe3907e45e8a500c6a1202ad5092c0f": {"ta_keywords": "trained speaker mixtures;source separation deep;speaker mixtures deep;speaker mixtures;separation deep learning;surprisingly speaker mixtures;spectrogram input mixtures;speaker mixtures improve;mixtures multiple speakers;separation based deep;deep clustering;model trained speaker;called deep clustering;mixtures deep neural;segmentation separation;speaker independent model;deep clustering train;sources speech noise;trained speaker;based deep clustering;mixtures held speakers;source separation;segmentation separation based;clustering train deep;separation deep;single channel mixtures;channel mixtures multiple;input mixtures;mixtures improve signal;channel mixtures", "pdf_keywords": "separation deep learning;segmentation audio separation;segmenting speech mixtures;source separation deep;audio segmentation;segmentation audio;separation spectral clustering;image audio segmentation;image segmentation audio;separate speaker mixtures;deep clustering_;segmenting speech;speech mixtures based;speech mixtures;method segmenting speech;separation deep;audio separation;audio separation spectral;acoustic source separation;framework deep clustering_;deep clustering_ present;speaker mixtures;training mixtures speakers;speakers deep neural;mixtures speakers deep;speaker mixtures despite;separate speaker;deep network;source separation;deep learning"}, "6e7cfed8815cce163efac9d17b1109849c050c6b": {"ta_keywords": "extraction data web;heuristics extraction data;extraction data;data web page;data web;heuristics extraction;present heuristics extraction;web page;extraction;web;data;page;heuristics;present heuristics;present", "pdf_keywords": ""}, "7038b181f776e9cd587d4d61cb68692fdac8ec26": {"ta_keywords": "signalized traffic flow;control signalized traffic;traffic signal parameters;signalized traffic;timing traffic flow;traffic signal control;signalized intersections dynamic;determine traffic signal;cascading congestion study;cascading congestion;timing traffic;traffic flow data;traffic signal;infer traffic signal;phase timing traffic;determine traffic;traffic flow;network signalized intersections;congestion study;traffic flow networks;intersections dynamic molecular;status determine traffic;infer traffic;congestion study single;traffic;congestion;knowledge traffic signal;method infer traffic;dynamic molecular dynamics;signalized intersections", "pdf_keywords": "signalized traf\ufb01c networks;applications signalized traf\ufb01c;measurements signalized traf\ufb01c;monitoring forecasting systems;signalized traf\ufb01c;monitoring forecasting;event monitoring forecasting;properties signalized traf\ufb01c;koopman operator framework;spectral decomposition koopman;representing oscillatory dynamics;measurements signalized;based spectral decomposition;operator framework;signalized;matrix based spectral;operator framework studying;data streams;based spectral;applications signalized;signalized traf\ufb01c suggests;decomposition koopman operator;spectral decomposition;operator matrix based;representing oscillatory;forecasting systems;time event monitoring;koopman operator paper;koopman operator;problems signalized traf\ufb01c"}, "1e58c9d1153d2f25d94b3a12b785bd7abe43bd1c": {"ta_keywords": "semi supervised;semi supervised learning;semi supervised classification;used semi supervised;activity recognition datasets;human activity recognition;activity recognition;learned autoencoder;contexts deep learning;deep learning;change point detection;learning contexts deep;large datasets sequences;supervised;supervised learning contexts;recognition datasets;framework semi supervised;learned autoencoder obtain;supervised learning;datasets sequences;autoencoder obtain improved;labelling segmentation expensive;datasets sequences high;classifiers sequential data;superior learned autoencoder;classification setting sequential;supervised classification;classifiers sequential;sequential data unsupervised;sequences coupled labeled", "pdf_keywords": "semi supervised;semi supervised learning;sequential data semisupervised;semi supervised classi\ufb01cation;activity recognition datasets;used semi supervised;semisupervised learning;semisupervised learning promising;data semisupervised learning;semi supervised techniques;activity recognition;human activity recognition;representations semi supervised;data semisupervised;unsupervised change point;exploited semisupervised context;semisupervised context;segmenting unlabeled sequence;settings semi supervised;unsupervised change;unlabeled sequence easily;semisupervised context context;easily exploited semisupervised;approach segmenting unlabeled;supervised classi\ufb01cation;supervised;unlabeled data labeled;segmenting unlabeled;change point detection;labeled data"}, "5bad092098ba7400e19468a06cb8b238c43b7637": {"ta_keywords": "fermilab tevatron collider;tevatron collider presented;tevatron collider;fermilab tevatron;round fermilab tevatron;results round fermilab;collider presented;round fermilab;collider;fermilab;tevatron;final results round;final results;results round;final;presented;results;round", "pdf_keywords": ""}, "c81bb5ff79e8c7f65a3e28b7ba52d90deaa32fde": {"ta_keywords": "web search engine;web pages web;code switching;search engine;web pages;code switching online;topology web success;collaborative editing;web pages code;web search;pages code switching;language choice collaborative;web;topology web;collaborative editing process;collaborative;choice collaborative editing;web success;pages web;online community;pages likely web;effect code switching;web success web;web pages likely;success web search;capture language choice;topological properties web;code switching shown;switching online community;search engine section", "pdf_keywords": ""}, "f26f17ec49f2593bcc926051394871480a80c0c2": {"ta_keywords": "lingual word embedding;monolingual embedding;lingual word similarity;monolingual embedding spaces;word embedding;expresses monolingual embedding;bilingual lexicon induction;embedding vectors languages;lingual word;word embedding based;bilingual lexicon;cross lingual word;difficult language pairs;lingual;set bilingual lexicon;language pairs word;language pairs;morphologically rich languages;approach cross lingual;cross lingual;data set bilingual;lexicon induction cross;induction cross lingual;embedding spaces probability;word similarity;vectors languages;vectors languages benchmark;mappings difficult language;word pairs;embedding vectors", "pdf_keywords": "bilingual embedding;bilingual embedding word;method bilingual embedding;crosslingual word embedding;lingual word embedding;bilingual lexicons monolingual;learning bilingual lexicons;unsupervised machine translation;bilingual lexicons;unsupervised crosslingual word;translation method bilingual;unsupervised crosslingual;augmented monolingual corpora;lexicons monolingual corpora;word embedding methods;monolingual corpora;monolingual corpora challenging;embedding word representation;present unsupervised crosslingual;word embedding;lexicons monolingual;cross lingual word;translation method learns;learning shared embedding;word embedding proposed;unsupervised lexicon induction;embedding space words;models learning bilingual;embedding word;cross lingual"}, "28028458d75bf9281200389a880741eb6d06a3a4": {"ta_keywords": "abstract datalog specifications;reconstruct abstract datalog;datalog specifications examples;inductive logic programming;abstract datalog;logic programming;logic programming methods;datalog specifications;specification recovery;specifications extracted nearly;datalog;used specification recovery;specifications extracted;specifications examples operation;programming;specification recovery fluid;large software;large software combination;real world software;programming methods;enable specifications extracted;behavior large software;grendel2 improve performance;specifications examples;programming methods successfully;hypotheses outputs specifications;software;software demonstrate;shelf inductive logic;grendel2 improve", "pdf_keywords": ""}, "436380dd75d8ff3f2debb29913bd2fe8dde0b684": {"ta_keywords": "source separation results;source separation;improvement source separation;phase speech mixture;sources phase speech;speech mixture;speech mixture used;matrix factorization;decomposition spectral magnitudes;reconstructing target speech;complex matrix factorization;matrix factorization problem;aspect target speaker;phase speech;magnitude mixture mixtures;method decomposition spectral;decomposition spectral;separation results demonstrated;mixture used reconstructing;magnitude mixture;separation results;speaker fluid dynamics;phase magnitude mixture;target speaker presence;interfering speaker fluid;speech signal complex;mixture mixtures utilized;speaker presence interfering;target speech signal;speaker presence", "pdf_keywords": "separation using factorization;source separation using;complex matrix factorization;matrix factorization;channel source separation;improvement source separation;source separation results;source separation;factorization complex data;matrix factorization jointly;method source separation;source separation presence;using factorization complex;factorization complex;factorization;factorization jointly utilizes;using factorization;speaker multi channel;multi channel source;factorization jointly;separation using;method complex matrix;speaker multi;grid corpus;competing speaker multi;separation results;complex matrix;converts complex matrix;matrix maintaining integrity;separation results demonstrated"}, "6992f54509c139455c3cffa9b0e4ae5c19ebff82": {"ta_keywords": "air contact network;multilingual response air;air air contact;air contact;network sudden change;response air air;contact network sudden;sudden change network;network sudden;contact network;air air;response air;network connectivity;multilingual response;network;connectivity;air;study multilingual response;change network;change network connectivity;multilingual;comprehensive study multilingual;study multilingual;sudden change;sudden;contact;response;paper report results;change;results comprehensive study", "pdf_keywords": ""}, "30c6be4c7f549a2ec7328d24ecc0a54fbf90d41c": {"ta_keywords": "optimal controllers reduced;optimal controllers;finding optimal controllers;optimal control policies;policies wireless networks;control policies wireless;optimal control;determine optimal policies;reduced dimensions markov;optimal policies;develop optimal control;dimensions markov decision;policies wireless;markov decision processes;policy iteration;markov decision;iteration policy iteration;controllers reduced dimensions;wireless communication networks;iteration policy;wireless networks;controllers reduced;dimensions markov;optimal;optimal policies moderate;value iteration policy;determine optimal;finding optimal;policy iteration employed;policies moderate complexity", "pdf_keywords": ""}, "4da018847a0f44378e6a1ded93fee672a3c7c370": {"ta_keywords": "speech recognition asr;speech recognition;speech translation faster;recognition asr machine;recognition asr;automatic speech recognition;automatic speech;systems automatic speech;mask predicting connectionist;translation faster robust;temporal classification ctc;connectionist temporal classification;asr machine learning;end speech translation;predicting connectionist temporal;mask ctc performance;encoder;classification ctc mask;enhance encoder network;model mask ctc;inference speed recognition;mask ctc;end robotic;end robotic based;classification ctc;mask ctc achieves;end end robotic;asr machine;end end speech;encoder network", "pdf_keywords": "new training decoding;end speech translation;end automatic speech;training decoding;machine translation architecture;automatic speech;speech translation tackle;training decoding methods;ctc adopts encoder;mlm decoder;inference keeping decoding;speech recognition mask;inference mask ctc;end end speech;speech recognition;mlm decoder handle;end speech;speech translation;enhance encoder architecture;encoder;translation architecture end;encoder architectures introduce;methods mlm decoder;automatic speech recognition;machine translation;tokens mask ctc;encoder architectures;encoder architecture;state art encoder;tokens inference keeping"}, "abaf39dc9d1b156ddf387230611f5102378d052c": {"ta_keywords": "observed textual variants;sequence model attention;ocr post correction;texts large corpora;textual variants bootstrapping;noisily observed textual;observed textual;textual variants;exploits repeated texts;large corpora source;repeated texts;approach ocr post;repeated texts large;textual;large corpora;texts;source evidence decoding;evidence decoding;corpora;novel approach ocr;human annotation training;ocr post;model human annotation;model attention averaging;training correction;corpora source noisy;corpora source;approach ocr;ways training correction;post correction", "pdf_keywords": "learning post correction;automatic text correction;text correction;multi input attention;text sequences exploit;exploiting duplication corpus;post correction framework;input attention combination;duplication corpus;training correction;human annotation exploiting;input text sequences;post correction;single input text;duplication corpus addition;ways training correction;input attention;text sequences;attention combination simple;texts large corpora;training attention combination;exploits repeated texts;attention combination;automatic text;annotation exploiting duplication;text correction paper;corpus;inputs training attention;model human annotation;corpus addition"}, "63bfe58735f44b0af24da3c2cb6b1651b001b83c": {"ta_keywords": "complexity secret sharing;distributed secret sharing;algorithm secret sharing;secret sharing networks;threshold secret sharing;coding distributed secret;distributed secret;secret sharing;secret sharing low;secret sharing network;communication complexity secret;protocol transmission secret;problem secret sharing;secret sharing dealer;eavesdropping based distributed;secret participants protocol;secure network coding;protocol nodal eavesdropping;secret sharing important;sharing networks satisfies;protocols secure multiparty;transmission secret participants;secure multiparty computation;generalization cryptographic protocols;distributed protocol transmission;cryptographic protocols large;nodal eavesdropping based;sharing low communication;complexity secret;algorithm secret", "pdf_keywords": ""}, "5d69380565aa258bfa54005c9ba05e30675be227": {"ta_keywords": "entity classification;clueweb page datasets;pages ontologies;page ontology clueweb;ontology clueweb;page ontology;web page ontology;web pages ontologies;entity classification task;ontology clueweb page;semantic drift seeded;semantic drift;ontologies;topic concept hierarchies;respectively entity classification;page dataset analyze;web page dataset;pages ontologies xmath0;rank web pages;classification tasks;semi supervised classification;supervised classification tasks;hierarchical classification tasks;page datasets;supervised classification;hierarchical classification;methods hierarchical classification;page dataset;classification;concept hierarchies", "pdf_keywords": ""}, "62d6ccd01c2e022a385add5e689b4561b0fbfd88": {"ta_keywords": "compute speaker embeddings;speaker embeddings;speaker embeddings neural;network speaker diarization;neural network speaker;speaker diarization;speaker diarization important;compute speaker;method compute speaker;network speaker;speech applications;overlapped speech;embeddings neural network;speech applications aims;baseline standard diarization;speaker;step speech applications;embeddings neural;diarization systems;deal overlapped speech;diarization datasets;standard diarization systems;diarization systems achieve;embeddings;vector baseline;speech;vector baseline outperforms;diarization important preprocessing;diarization;results diarization datasets", "pdf_keywords": "trained overlapped speech;overlapped speech segment;diarization overlapped speech;overlapped speech regions;detecting overlapped speech;speech segment proposals;automatic speaker diarization;speaker diarization promising;speaker diarization based;speaker diarization;speech regions diarization;joint speech segment;overlapped speech;learning diarization overlapped;speech segment proposal;framework speaker diarization;speech segment;speaker embedding extraction;diarization based deep;segment proposal speaker;automatic speech;speak diarization achieves;proposal speaker embedding;diarization based overlapped;speak diarization;speaker embedding;overlapped speech important;integrate automatic speech;speech recognition;speaks speak diarization"}, "9ba545841b837fa077579290e252eb00351ebeb0": {"ta_keywords": "communication compression;communication compression strategy;based compression gradient;novel communication compression;federated learning;distributed order methods;compression gradient;federated learning like;compression gradient differences;strategy based compression;compression strategy;biased gradient estimator;communication complexity bounds;compression strategy based;important federated learning;oracle communication complexity;prove communication complexity;class stochastic differential;second order algorithms;gradient estimator;stochastic differential;communication complexity;based compression;communication complexity paper;loss functions;gradient estimator key;algorithms marina better;local loss functions;compression;algorithms marina", "pdf_keywords": "distributed federated learning;methods federated learning;compression distributed federated;distributed methods federated;federated learning;propose distributed stochastic;distributed methods;distributed algorithm;distributed stochastic non;distributed algorithm communication;distributed stochastic;federated learning communication;communication compression distributed;federated learning provide;compression distributed;propose distributed algorithm;study distributed methods;distributed federated;paper propose distributed;non convex optimization;algorithm communication compression;propose distributed;convex optimization methods;distributed;convex optimization;communication compression;stochastic non convex;learning provide convergence;methods federated;study distributed"}, "ea1f61270480a8dec54ec571c0e6ce116d096241": {"ta_keywords": "sound event detection;polyphonic sound event;detection polyphonic sound;detection polyphonic;sound event multilabel;segment sound event;event multilabel classification;sound event;method detection polyphonic;sound event postprocessing;detecting segment sound;polyphonic sound;propose sound event;polyphonic;event multilabel;event detection;hidden markov;multilabel classification;short term memory;hidden markov motor;memory recurrent neural;recurrent neural;recurrent neural network;event detection method;multilabel classification problem;convolutional bidirectional long;network hidden markov;segment sound;frame wise detection;multilabel", "pdf_keywords": ""}, "598321d9c3eb5c035b449e19e539b6fa04b3802a": {"ta_keywords": "billion crawled urls;cce4corpus largest publicly;general web crawl;crawl date billion;web crawl;billion crawled;cce4corpus largest;date billion crawled;crawled;crawled urls;web crawl date;largest publicly available;cce4corpus;crawl date;general web;largest publicly;crawl;available general web;largest;urls;web;publicly available general;date billion;billion;publicly available;publicly;available general;general;date;available", "pdf_keywords": ""}, "8f1f43408baf1ccb0ec3e7985592326c83ee276d": {"ta_keywords": "semantic similarity retrieval;similarity retrieval response;similarity retrieval;syntactic semantic similarity;retrieval techniques syntactic;semantic similarity;demonstrate movie similarity;cosine similarity retrieval;statistical machine translation;similarity retrieval tf;chat oriented dialog;machine translation approaches;retrieval response generation;machine translation;movie similarity;movie twitter scripts;movie similarity good;retrieval techniques;building chat oriented;phrase based smt;retrieval tf idf;retrieval response;techniques syntactic semantic;oriented dialog investigates;twitter scripts demonstrate;translation approaches;dialog investigates combined;approaches building chat;using movie twitter;based cosine similarity", "pdf_keywords": ""}, "07a9f47885cae97efb7b4aa109392128532433da": {"ta_keywords": "coding cross attention;simpler efficient attention;attention head hardcoded;efficient attention;attention connects decoder;learned cross attention;cross attention;attention based models;encoder decoder trained;efficient attention based;decoder trained;cross attention head;attention based;cross attention connects;encoder important selfattention;encoder;selfattention translational motion;attention;models encoder decoder;models encoder;encoder decoder;decoder encoder important;attention head;encoder important;decoder encoder;based models encoder;position body translational;decoder trained fixed;translational motion;decoder", "pdf_keywords": "attention crucial translation;neural machine translation;improving multiheaded attention;machine translation architecture;decoder self attention;crucial translation quality;bleu machine translation;multiheaded attention;machine translation systems;machine translation;translation systems;machine translation brings;translation systems focus;attention cross attention;hard coded attention;translation architecture;singleheaded cross attention;compared cross attention;learned attention heads;translation quality;cross attention contrast;attention grammatical characterlevel;attention heads;attention heads instead;head self attention;cross attention;self attention cross;translation quality compared;multi headed attention;attention distributions"}, "8afbc4188be9e9452ce1fe868ebe217179d36793": {"ta_keywords": "noisy speech spectra;speech spectra;speaker normalization;speaker normalization method;new speaker normalization;speech spectra represented;stochastic approach speech;approach speech recognition;speech recognition;speech noise sources;speech exemplars noise;predict acoustic;approach predict acoustic;noisy speech;speech noise;individual speech noise;predict acoustic condition;spectrum clean noisy;clean noisy spectra;clean speech exemplars;parameters new acoustic;speech recognition based;speakers acoustic;spectrum used recognition;noisy spectra clean;speech exemplars;speakers acoustic waveguides;condition speakers acoustic;acoustic condition speakers;noise sources reconstruction", "pdf_keywords": ""}, "3426fadf73a5ce418486e640b26b3d2470d932b5": {"ta_keywords": "multilingual adversarial speech;multilingual multilingual adversarial;multilingual adversarial;adversarial speech recognition;recognizing multilingual speech;language adversarial classification;multilingual speech findings;multilingual speech;adversarial speech;paired language adversarial;language adversarial;adaptation multilingual end;recognizing multilingual;language independent encoder;target pretraining languages;adaptation multilingual;pretraining languages dimensions;pretraining languages;languages dimensions phonetics;trained 100 languages;end speech recognition;method recognizing multilingual;adversarial classification objective;languages multilingual multilingual;languages multilingual;multilingual multilingual;adversarial classification;multilingual;100 languages multilingual;speech recognition models", "pdf_keywords": "pretraining multilingual asr;objectives multilingual pretraining;pretraining objectives multilingual;language adversarial pretraining;languages multilingual pretraining;pretraining multilingual;multilingual pretraining useful;multilingual pretraining;model pretraining languages;explore pretraining multilingual;multilingual asr models;pretraining languages diverse;adversarial pretraining objectives;phoneme language adversarial;pretraining languages baseline;adapted target language;multilingual transfer;multilingual speech dataset;pretraining languages;multilingual speech;auxiliary objectives multilingual;multilingual knowledge transfer;corpora multilingual transfer;multilingual asr;target language;adversarial pretraining;language target speakers;particularly pretraining languages;auxiliary phoneme language;objectives multilingual end"}, "8de431e0e62653711136836642af38179731c2f0": {"ta_keywords": "storage proposed codes;coded distributed storage;secure storage;secure storage based;security secure storage;erasure coded distributed;secure code;secure code providing;security data presence;non secure code;erasure coded;proposed codes optimal;coding schemes repair;ensure security data;storage based information;distributed storage systems;storage systems;coded distributed;bounds storage network;operations erasure coded;matrix codes;information stored storage;distributed storage;codes optimal;security data;stored storage proposed;product matrix codes;codes based optimal;lower bounds storage;matrix codes paper", "pdf_keywords": "distributed network coding;secure capacity distributed;distributed storage;distributed storage systems;approach distributed storage;storing data securely;data securely cloud;availability erasure codes;capacity distributed storage;network coding based;secure capacity;network coding;erasure codes considered;capacity achieving codes;cloud based storage;erasure codes;data securely;security passive eavesdroppers;securely cloud;storage systems present;storage services;eavesdropper possess bounded;adversaries passive eavesdroppers;metric network coding;storage systems;securely cloud based;establish secure capacity;passive eavesdroppers;storage systems case;based storage services"}, "fdf1aec2da3597010c31138159574b1016019f73": {"ta_keywords": "computational social choice;preference reasoning computational;reasoning computational social;preference reasoning lead;preference reasoning;computational social;research computational social;choice preference reasoning;social choice preference;voting resource allocation;group decision making;social choice group;driven research computational;choice preference;understanding social choice;social choice;social choice problem;voting resource;reasoning computational;relationship preference reasoning;choice group decision;data driven;voting;preference;choice group;research computational;computational;group decision;computer scientists toolkit;data building human", "pdf_keywords": ""}, "538deb39d57bef62833c492a56c796a2bafa340f": {"ta_keywords": "active learning classifier;machines active learning;classifier based active;active learning;instances active learning;vector machines active;active learning using;based active learning;active learning provide;video active learning;efficiency active learning;active learning experiments;new active learning;active learning support;learning support vector;support vector machines;entity recognition;named entity recognition;learning classifier;classifier;entity recognition treated;vector machines stm;annotation task;learning classifier based;aided annotation task;vector machines;classifier based;learning using linear;annotation task task;aided annotation", "pdf_keywords": ""}, "63e7e3b16e03da62a2c535ac9cfccfa3ae48b292": {"ta_keywords": "teamwork crowdsourced decision;teamwork crowdsourced;team artificial intelligence;making teamwork crowdsourced;ai collaborations;ai collaborations artificial;collaborations artificial intelligence;crowdsourced decision making;intelligence ai collaborations;crowdsourced decision;human artificial intelligence;team artificial;crowdsourced;human centered optimization;collaborations artificial;decision making teamwork;ai;artificial intelligence ai;performance team artificial;intelligence ai;definition team utility;intelligence ai practitioners;team utility;definition team;artificial intelligence;making teamwork;artificial intelligence discuss;non linear tasks;team utility argue;ai practitioners", "pdf_keywords": "human ai teaming;ai teaming human;human ai teams;ai teaming;human ai collaboration;performance human ai;ai collaboration;ai teams;teaming human;ai teams paper;teaming human overseer;human ai;directly optimized team;ai collaboration propose;process human ai;optimized team performance;optimize collaborative;model optimize collaborative;optimized team;ai optimization;motivated human ai;teaming;teammate human;possible teammate human;optimize collaborative process;ai best possible;guides team performance;accurate ai best;work ai optimization;ai optimization problems"}, "b2baf9e053c32abfb3c8658b9bc6d6790ae671cb": {"ta_keywords": "eye movement detection;eye tracking features;eye tracking;eye movement features;svvm random forests;using eye tracking;eye movement animals;support vector machines;features detection;vector machines svvm;eye movement;movement detection;features detection paper;gaze duration word;approach utilizes gaze;movement detection systems;novel eye movement;random forests;machines svvm;vector machines;reading using eye;gaze duration;detect unknown words;utilizes gaze duration;features paper eye;gaze;svvm;tracking features;performance eye movement;improve detection", "pdf_keywords": ""}, "15931520cce546bbf19b4cebeb4161c4debeabe7": {"ta_keywords": "agent manipulate ballot;winner approval voting;decisions using voting;outcome voting;outcome voting way;generally manipulate vote;better outcome voting;employing voting;employing voting rules;voting scenarios;multi winner approval;behaviors multi winner;using voting scenarios;manipulate vote;approval voting scenarios;manipulate ballot;elections employing voting;data multi winner;votes choosing candidates;manipulate vote obtain;votes choosing;ballot consisting approvals;tallying votes choosing;winner approval;voting av agent;approval voting;collective decisions using;manipulate ballot submit;vote obtain better;using voting", "pdf_keywords": "forecast human votes;real world voting;predictions approval voting;voting scenarios optimal;complex voting;complex voting scenarios;votes reasonable accuracy;winner approval voting;human votes;heuristic voters use;people vote realistic;human votes reasonable;voting set methods;heuristic voters;vote realistic;faced complex voting;voting scenarios;world voting;multi winner approval;approval voting elections;votes reasonable;people vote;possible heuristic voters;voting data;approval voting;vote realistic settings;voting work;understanding people vote;heuristics accuracy;heuristics accuracy model"}, "931cbd9d689e9fd6bd91f4e8e1dbdd7fbb6df9de": {"ta_keywords": "recognize speech mixtures;speech separation;target speech recognition;speech recognition;automated speech recognition;alternative speech separation;inserting speech separation;automatic speech recognition;speech recognition called;automatic speech;speech separation mechanism;called speech separation;automated speech;speech features sequence;sequence speech features;speech separation mitigate;speech recognition asr;mixture based speech;end automatic speech;speech recognition results;speech mixtures;extended recognize speech;e2e automatic speech;speaker mixture based;speech features;combine target speech;recognition called speech;speech recognition e2e;speech enhancement diarization;speech mixtures inserting", "pdf_keywords": ""}, "a445adf335aa5212f929f67c1ca56a62c221b43a": {"ta_keywords": "unknown unknowns predictive;unknowns predictive models;unknowns predictive;predictive model unknowns;discovering unknown unknowns;unknowns given predictive;discovering unknown;blind search algorithms;blind searches based;identifying unknowns;problem discovering unknown;identifying unknowns called;blind search;method identifying unknowns;discovery unknown unknowns;identify unknowns;predictive models;blind searches;unknown unknowns various;strategy discovering unknown;problem informed discovery;unknowns called;informed discovery;observation blind search;blind blind searches;unknown unknowns;unknowns called blind;informed discovery unknown;problem discovering;model unknowns occur", "pdf_keywords": "bandit algorithm systematically;armed bandit algorithm;discovering unknown unknowns;bandit algorithm;bandit problem propose;searching unknown unknowns;unknown unknowns predictive;discovering unknown;unknowns predictive;unknowns predictive models;bandit problem;unknowns partitions leveraging;training predictive;discover unknown unknowns;systematically searching unknown;unknowns given predictive;armed bandit problem;systematically searching;algorithms partitioning search;training predictive model;unknown unknowns arise;problem discovering unknown;multi armed bandit;bandit;discovery unknown unknowns;algorithm systematically searching;discovering;unknowns arise;partitioning search;used training predictive"}, "947750c717a5fbd17fc52758322d1ca201c4c6bc": {"ta_keywords": "entity recognition tweet;tweet classification;named entity recognition;entity recognition;recognition tweet classification;recognition tweet;nlp researchers;efforts nlp researchers;nlp researchers create;word segmentation;stricken area tweets;area tweets;disasterstricken;efforts nlp;systems word segmentation;tweets;nlp;area tweets massive;word segmentation named;people disasterstricken;describes efforts nlp;safety people disaster;segmentation named entity;tweets massive;tweets massive highly;earthquake;100 people disasterstricken;organized information source;coast earthquake;people disasterstricken area", "pdf_keywords": ""}, "c45a23e7c565169c5a55898683aceac458c116bb": {"ta_keywords": "rnns speech enhancement;speech separation recognition;chime speech separation;networks rnns speech;speech recognition;speaker adaptive training;data augmentation speaker;advanced speech recognition;dnn based acoustic;extraction advanced speech;speech enhancement language;rnns speech;augmentation speaker adaptive;speech enhancement;speech recognition used;speech separation;speaker adaptive;chime challenge robust;rnn language models;recognition challenge chime;chime speech;3rd chime speech;networks rnns;neural networks rnns;recurrent neural networks;augmentation speaker;short term memory;based acoustic models;acoustic models large;microphone", "pdf_keywords": ""}, "2b0aa68ef2c1773642ca91627a4fc03f536cc5fc": {"ta_keywords": "walled carbon nanotube;carbon nanotube;nanotube;magnetic field optical;demonstration effect magnetic;effect magnetic field;field optical properties;effect magnetic;optical properties single;single walled carbon;magnetic field;optical properties;magnetic;field optical;walled carbon;properties single walled;present experimental demonstration;optical;carbon;experimental demonstration;experimental demonstration effect;single walled;properties single;present experimental;demonstration effect;properties;effect;single;paper present experimental;field", "pdf_keywords": ""}, "f762ce106b37728df1126375981a02a589e0497c": {"ta_keywords": "stochastic gradient descent;propose stochastic gradient;stochastic gradient based;randomized coordinate descent;stochastic gradient;gradient descent;proximal stochastic gradient;tricks variance reduction;mini batch sampling;perturbation propose stochastic;reduction importance sampling;importance sampling mini;sampling mini;variants proximal stochastic;sampling mini batch;batch sampling quantization;sampling fluid dynamics;sgdin randomized coordinate;sgdin randomized;coordinate descent;gradient descent tt;video motion swimmer;proximal stochastic;sampling quantization;propose stochastic;importance sampling;stochastic;sgd framework;batch sampling;motion swimmer", "pdf_keywords": "distributed learning gradient;stochastic gradient descent;stochastic distributed learning;optimization stochastic distributed;distributed learning;gradient descent;distributed optimization;distributed optimization regularized;stochastic gradient;gradient descent methods;descent methods gradient;learning gradient quantization;gradient descent sgdin;methods gradient descent;introduces stochastic gradient;proximal stochastic gradient;gradient descent paper;learning gradient;unbiased estimator gradient;gradient unbiased estimator;gradient estimation;estimator gradient;free optimization stochastic;compute gradient unbiased;loss minimization;expensive compute gradient;regularized loss minimization;gradient quantization;optimization regularized loss;gradient quantization variance"}, "ec084dac14a069da2e924ff7f3d5d2fb75b9b39a": {"ta_keywords": "sentiment sufficient dimension;sentiment variables;sentiment variables research;sentiment sufficient;regression coefficients prior;multinomial logistic regression;dimensional logistic regression;connected sentiment variables;inference regression coefficients;joint inference regression;developed multinomial logistic;regression coefficient efficient;multinomial logistic;prior scale text;inference regression;coefficients prior scale;regression high dimensional;coefficients prior;straightforward framework sentiment;reduction text data;connected sentiment;high dimensional logistic;framework sentiment sufficient;sentiment;independent laplace priors;forms connected sentiment;provide sample prediction;logistic regression;dimensional logistic;sample prediction studies", "pdf_keywords": "modeling text sentiment;sentiment regression;multinomial predictor distributions;sentiment regression context;text sentiment;text sentiment increasingly;based multinomial likelihood;text given sentiment;quantifying partisanship weighted;multinomial likelihood framework;literature quantifying partisanship;regression based multinomial;multinomial likelihood;based multinomial predictor;multinomial predictor;based multinomial;analyzed multivariate likelihood;partisanship weighted;problem sentiment regression;analysis based multinomial;data particular multinomial;sentiment increasingly;partisanship weighted term;quantifying partisanship;particular multinomial;dimensional logistic regression;multinomial;text analysis;sentiment increasingly popular;multinomial distribution xi"}, "bc251481aa5566b1e86a8dbd0417cdf858205e3b": {"ta_keywords": "graphene nanoribbons gnrs;graphene;graphene nanoribbons;properties graphene nanoribbons;properties graphene;transport properties graphene;detecting false alarms;nanoribbons gnrs;false alarm detection;fact false alarm;detection based fact;nanoribbons gnrs end;detecting false;nanoribbons;false alarms;false alarms false;alarms false;veracity considering patterns;fact based model;false alarm;detection based;alarms false alarm;detecting;shared patterns fake;patterns fake news;patterns facts methods;fact based methods;false alarm rate;patterns fake;alarm detection", "pdf_keywords": "fake news detection;integrates fake news;fact based models;news detection experiments;news detection;models binary classification;learning patternand fact;fact based;preferences improving detection;fake news;learning models preferences;heterogeneous graph convolution;graph convolutional network;binary classification;singlepreference models;realworld datasets preference;based models binary;patternand fact based;binary classification task;realworld datasets;facts;pattern fact based;detection performance singlepreference;datasets preference models;fact;graph convolutional;learn model preferences;facts address challenges;patterns facts address;experiments realworld datasets"}, "dc2f6f092fa04e334dfe2e8592b6d597e00b97ca": {"ta_keywords": "generate hypernymy extraction;helpful extraction hypernyms;extraction hypernyms;hypernymy extraction;hypernymy extraction terms;quality hypernymy extraction;hypernymy extraction mixture;distributionally induced semantic;distributional semantics using;aware semantic classes;distributional semantics;using distributional semantics;induced semantic classes;sense aware semantic;semantic classes;semantic classes helpful;noisy hypernymy relations;semantic classes filtering;semantic classes using;aware semantic;generate hypernymy;method generate hypernymy;hypernyms;using induced semantic;induced semantic;classes helpful extraction;semantic;hypernymy relations;semantics using induced;inducing sense", "pdf_keywords": "distributional semantics hypernyms;semantics hypernyms;sense aware semantic;distributional semantics;lexical semantic resources;semantics hypernyms building;distributional semantics using;distributional semantics present;hypernyms using distributional;resources distributional semantics;hypernymy extraction terms;using distributional semantics;lexical semantic;aware semantic classes;hypernymy extraction;semantic resources distributional;hypernyms hyponyms widely;noisy hypernymy relations;extracted hypernyms;automatically extracted hypernyms;aware semantic;induced text hypernyms;semantic classes;text hypernyms hyponyms;induced semantic classes;enriching lexical semantic;hypernyms hyponyms;quality hypernymy extraction;common hypernyms;semantic class"}, "02a83a01d6236149e4ead01e202b2453f9590e9e": {"ta_keywords": "fairness confusion tensor;optimize accuracy fairness;accuracy fairness objectives;designing fair classifiers;fair classifiers;group fairness class;group fairness incompatibilities;fairness objectives elements;group fairness notions;fairness class fairness;accuracy fairness;accuracy fairness majority;fair classifiers demonstrate;group fairness;incompatibilities group fairness;nanotube cnt thermoelectric;fairness objectives;fairness incompatibilities;class fairness notions;fairness class;including group fairness;carbon nanotube;class fairness;fairness incompatibilities group;carbon nanotube cnt;walled carbon nanotube;fairness notions;fairness notions expressed;fairness confusion;fairness notions measure", "pdf_keywords": ""}, "c263507db2c15a8b2e3c955bda7b3c29a1ebd106": {"ta_keywords": "chatbots facilitates penalising;live chatbots diverse;chatbots diverse domains;chatbots;live chatbots;chatbots diverse;chatbots facilitates;queries received chatbots;received chatbots facilitates;created live chatbots;received chatbots;complexities imbalanced datasets;data intentent detection;intentent detection systems;intentent detection;machine learning classifier;imbalanced datasets;training data intentent;imbalanced datasets containing;classifier;unintended patterns training;real user queries;diverse domains dataset;machine learning;learning classifier;training data;detection systems real;detecting;learning classifier saturates;intent unintended correlations", "pdf_keywords": "scope intent detection;task oriented dialogue;dialogue systems;dialogue systems gained;oriented dialogue systems;intent detection;intent detection single;scope intent;platforms like dialog\ufb02ow;personal assistants automated;oriented dialogue;assistants automated;scope accuracy systems;dialogue;like dialog\ufb02ow;intents;intents test sets;dialog\ufb02ow;like dialog\ufb02ow luis;assistants automated customer;scope accuracies systems;task oriented;dialog\ufb02ow luis rasow;versions training;problem scope intent;dialog\ufb02ow luis;personal assistants;subset versions training;scope accuracies;intent"}, "9bd170248355047067f05349d57110cc8e4de5cf": {"ta_keywords": "web crawl common;general web crawl;crawl common crawl;coupled crawl crawl;crawl general web;common crawl;web crawl;coupled crawl;crawl crawl web;crawl web crawl;crawl common;crawl web;common crawl general;crawl crawl;crawl general;crawl;general web;web;coupled;common;general", "pdf_keywords": ""}, "9952fe8cbd09e4fc89dc7d76595d138e36c7d7b5": {"ta_keywords": "ranking model transferring;neural ranking models;neural ranking;based neural ranking;ranking models;ranking models world;training pseudo labels;compared transfer learning;annotated queries;transfer learning training;annotated queries produce;tournaments paper report;compare transfer learning;ranking model;number annotated queries;queries produce competitive;ranking;high annotation costs;transfer learning;evaluation ranking;tournaments;evaluation ranking model;training data;tournaments paper;training data important;learning training pseudo;wide tournaments;wide tournaments paper;world wide tournaments;report evaluation ranking", "pdf_keywords": "shot transfer learning;labels transfer learning;compared transfer learning;transfer learning promising;training pseudo labels;zero shot transfer;training transfer learning;transfer learning;learning transfer learning;learning transfer;transfer learning based;shot training transfer;collections training pseudo;pseudo labels transfer;transfer learning transfer;transfer learning reduce;transfer learning limited;shot transfer;zero shot;annotated queries;supervised training directly;approach zero shot;learning based pseudo;shot transfer evaluated;large collections training;annotated queries produce;non neural learning;labels transfer;number annotated queries;training pseudo"}, "1f76ee8472ec3a41511540f62e4676317df14ea5": {"ta_keywords": "age control singing;singing voice conversion;perceptual age singing;control singing voice;convert singing voice;voice characteristics manipulating;change singer perceptual;voice timbre control;singing voice age;singing voice timbre;age singing voice;voice conversion regression;singing voice characteristics;possible convert singing;singer perceptual age;voice timbre arbitrary;method control singing;convert singing;control singing;sing expressively controlling;voice conversion;voice age singer;voice conversion proposed;singing voice;gender age singers;age singing;voice timbre varieties;proposed voice timbre;controlling prosody voice;voice timbre", "pdf_keywords": ""}, "5a5fb155b5fc518389a7fe67b55271e143ad695d": {"ta_keywords": "community recovery hypergraphs;hypergraphs community recovery;community recovery graphs;recovery hypergraphs community;recovery hypergraphs studied;recovery graphs;hypergraphs community;recovery graphs popular;recovery hypergraphs;hypergraphs studied;hypergraphs;limit community recovery;hypergraphs studied paper;nodes hyperedge;censored block model;generalized censored block;clustering;limit community;nodes hyperedge corrupted;theoretic limit community;community recovery;clustering core;clustering core problem;problem community recovery;graphs;hyperedge corrupted bernoulli;randomly chosen hyperedges;noise data clustering;clustering received;data clustering", "pdf_keywords": "community detection;community detection binary;community recovery graphs;problem community detection;social networks computational;include crowdsourced clustering;crowdsourced clustering;crowdsourced clustering group;recovery graphs;clustering;algorithm subspace clustering;sequences social network;subspace clustering;clustering paper case;networks computational;model community recovery;subspace clustering paper;clustering paper;social networks;communities;domains social networks;recovery graphs received;social network;clustering group;complexity exact recovery;minimal sample complexity;algorithms provide information;sampling based algorithm;propose sampling;networks computational biology"}, "17c7c92db1f4ace842f9db6b44bfce264308b628": {"ta_keywords": "latent code learning;labeled constituency parsing;train latent code;outside recursive autoencoders;phrase vectors deep;recursive autoencoders;learned phrase vectors;autoencoders;constituency parsing;unsupervised labeled constituency;code learning;constituency parsing method;latent code;labeled constituency;code learning model;vectors deep;vectors deep inside;parsing;unsupervised labeled;labels using learned;phrase vectors;human annotations outperforms;disentangled parts effectively;disentangled parts;train latent;based unsupervised labeled;supervised;propose supervised learning;requires human annotations;separation disentangled parts", "pdf_keywords": ""}, "753fd6952c9f06f3bbd46e37129acc3f7a984896": {"ta_keywords": "text generation systems;learns retrieve relevant;retrieve relevant passages;textual knowledge corpus;text generation;retriever relevant passages;retrieve passages textual;knowledge corpus;simple guide retriever;guide retriever learns;guide retriever input;elbo text generation;retriever generator;textual knowledge;output retriever relevant;overall output retriever;passages textual knowledge;guide retriever;corpus paper retriever;retriever generator maximizing;standard retriever generator;retrieve passages;retrieve relevant;textual;output retriever;generator learn;learns retrieve;generator maximizing evidence;retriever retrieve passages;knowledge corpus paper", "pdf_keywords": "question answering;generate utterances conversations;domain question answering;trained generative sequence;generating wikipedia summarizing;question answering questions;generate utterances;tasks arbitrary text;answering questions;answering;jointly trained generative;trained generative;generative sequence sequence;utterances conversations;generative sequence;utterances conversations open;summarizing long;generating wikipedia;computer generate utterances;summarizing long sequences;conversations open ended;wikipedia summarizing long;long sequences challenging;end end tasks;answering questions propose;sequences challenging task;utterances;outputs challenging creates;conversations open;generative"}, "7fa3a5318ac45b2fd93a0130f0ceba9995ffa3c0": {"ta_keywords": "linguistic differences similarities;cross linguistic differences;cross linguistic;lingual natural language;entities finding similar;cross cultural similarities;complexity cross linguistic;cultural similarities;cultural differences named;common cross lingual;cultural similarities proposed;slang languages;differences named entities;finding similar;terms slang languages;linguistic differences;differences similarities common;cross lingual;mining cross cultural;similarities common;natural language;cultural differences based;cross lingual natural;linguistic;finding similar terms;cross cultural differences;differences named;understanding cultural differences;similar terms slang;differences similarities", "pdf_keywords": ""}, "8306e4a566e2b1279d5d67b40facc8e1e345c4e3": {"ta_keywords": "convergence distributed gradient;theory gradient descent;gradient descent excellent;networks error feedback;gradient descent;effective networks generalization;networks generalization;distributed gradient based;complex networks method;networks generalization established;descent gradient descent;complex networks;dynamics complex networks;effective networks;effective networks error;networks method;distributed gradient;descent gradient;contractive compression operators;gradient descent gradient;networks;prediction structure dynamics;gradient based optimization;contractive compression;concept effective networks;communication compression strategies;communication compression;compression strategies;compression operators;compression operators work", "pdf_keywords": "distributed optimization;distributed optimization method;new distributed optimization;distributed optimization framework;present distributed optimization;training stragglers;stochastic gradient descent;framework training stragglers;gradient descent;optimization framework training;called gradient descent;stragglers;training stragglers paper;gradient descent paper;gradient descent reduces;based stochastic gradient;distributed;stochastic gradient;machine learning framework;optimization framework;optimization;new distributed;new machine learning;distributed version;stragglers paper propose;optimization method;optimization method called;distributed version heavy;present new distributed;learning framework"}, "bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713": {"ta_keywords": "language phoneme recognition;phoneme recognition;phonetic annotation;phoneme recognition fine;accuracy human speech;recognition based phonetic;based phonetic annotation;tuning phoneme representation;improving speech recognition;speech recognition low;human speech recognition;phonetic annotation incorporates;speech recognition context;speech recognition;multilingual recognition;recognition fine tuning;fine tuning phoneme;language phoneme;tuning phoneme;human speech;appear language phoneme;phonemes appear language;phoneme representation;phonetic;phoneme representation important;multilingual recognition based;method multilingual recognition;based phonetic;recognition low resource;improving speech", "pdf_keywords": "phoneme recognition benchmark;phoneme recognition;present phoneme recognition;particular language phones;phonetic annotation;phoneme recognition using;phone recognition;language present phoneme;recognition based phonetic;phone recognition new;based phonetic annotation;multilingual recognition;reasonable phone recognition;language phones;phonetic annotation incorporates;language phones sounds;recognition new language;multilingual recognition based;correspond particular phoneme;phonemes appear language;language dependent allophone;allophone annotation method;method multilingual recognition;allophone annotation;focus phoneme recognition;using allophone annotation;recognition using allophone;adapt phonemes sounds;adapt phonemes;language independent allophones"}, "c64843e51f24773c895511ba9befa8a9bc4924a9": {"ta_keywords": "perceptual age singing;perceptual ages singing;age singing voices;age person voice;ages singing voice;age singing;age singer speech;singing voice characteristics;perceived age singer;ages singing;voice characteristics arbitrary;voice characteristics;processing statistical voice;statistical voice conversion;singing voice prosodic;statistical voice;singer speech analysis;voice prosodic features;singing voices;convert singing voice;person voice;voice ratio singers;singing voice;singing voices corresponds;voice technique;voice conversion;age singer;voice prosodic;possible convert singing;voices produce", "pdf_keywords": ""}, "8ef0ca924ae88ac2bb2803c49589722b52efc5b4": {"ta_keywords": "corpus social affective;presents corpus social;corpus social;social affective interactions;corpus corpus eukaryotic;corpora containing social;emotion occurrences social;corpus eukaryotic;conversations real emotion;paper presents corpus;containing social interactions;human social interactions;affective interactions english;corpus corpus;emotionally rich corpora;corpus;presents corpus;affective interactions;social interactions;corpus eukaryotic cells;real emotion occurrences;use corpus;annotated corpus;photonically annotated corpus;human social;filled corpus;annotated corpus corpus;social affective;constructing corpora;containing natural conversations", "pdf_keywords": ""}, "192fc995631e3443bb7f291a971089bd06e61017": {"ta_keywords": "human annotation science;annotation science;human annotation;annotation science question;interface human annotation;annotation;annotation process;annotation process used;classification new questions;reasoning types;knowledge reasoning types;knowledge reasoning;body dynamics battle;types knowledge reasoning;dynamics body;classification schema;questions;participants;labels annotation;types knowledge;labels annotation process;respective knowledge reasoning;knowledge reasoning required;body dynamics body;body dynamics;classification schema proposed;reasoning;new questions;classification new;interface human", "pdf_keywords": ""}, "39ffb5e9f2f36df42ef8ea010499e484c913e79e": {"ta_keywords": "approach spelling normalization;spelling normalization combines;spelling normalization;use spelling normalization;spelling normalization map;historical language corpora;language corpora;translated corpora recently;translated corpora;corpora;corpora recently;language corpora paper;create corpora;corpora recently growing;map historical wordforms;spelling noisy;normalization;normalization algorithms;normalization map historical;corpora exist;normalization combines;growing historical language;spelling noisy environment;historical wordforms modern;historical wordforms;dynamics spelling noisy;corpora paper;combines different normalization;normalization algorithms evaluates;create corpora exist", "pdf_keywords": ""}, "26f427d2b27828f2893e95344342570699e9c589": {"ta_keywords": "storage erasure codes;distributed storage erasure;erasure code data;encoded code maintaining;erasure codes;new erasure code;erasure codes used;erasure code;data encoded code;distributed storage;distributed storage systems;code data encoded;maintaining desired decodability;stored distributed storage;encoded code data;scale distributed storage;storage erasure;code data stored;storage systems;mds convertible codes;storage systems provide;achieve code conversions;convertible codes broad;storage;code conversions;desired decodability properties;desired decodability;code data;code conversion;data encoded", "pdf_keywords": ""}, "5c333f11431d1f0d04ced62b712c8d05ebac0891": {"ta_keywords": "speech enhancement;speech enhancement critical;speech enhancement algorithm;signal speech enhancement;novel speech enhancement;lagging speech enhancement;speech signal diffusion;speech enhancement paper;noises estimated speech;observed noisy speech;noisy speech;potential speech synthesis;speech synthesis;noisy speech signal;estimated speech signal;estimated speech;gaussian real noises;real noises estimated;human subject noisy;synthesis lagging speech;speech synthesis lagging;speech signal speech;signal speech;noises estimated;speech signal;strong potential speech;potential speech;user oriented audio;generative models;training generative models", "pdf_keywords": "automatic speech synthesis;speech synthesis;speech synthesis based;model automatic speech;speech signal diffusion;audio synthesis;speech enhancement;generation audio synthesis;automatic speech;image generation audio;novel speech enhancement;generative models;speech data noise;noisy speech;reverberant speech clean;generative models investigate;speech data;noisy reverberant speech;speech enhancement algorithm;generation audio;observed noisy speech;reverberant speech;generative models shown;generalization capabilities speech;generative;generative approaches;representative generative models;capabilities speech data;speech clean;justi\ufb01cation deep learning"}, "c82854b8d4c715da141d34c73bf9bda67adf307c": {"ta_keywords": "analyzing political blogs;political blogs;sentiment polarity;sentiment polarity comments;assign sentiment polarity;political blogs share;polarization news;polarity based topics;media political blogs;polarization news stories;sentiment polarity used;communities polarization;cause political blogs;political blogs comment;political blogs form;polarity comments serve;polarity comments;beliefs sentiment polarity;different communities polarization;assign sentiment;models predict polarity;blogs share opinion;communities polarization causes;social media political;predict polarity based;predict polarity;predict polarization responses;political beliefs sentiment;predict polarization;topics blog", "pdf_keywords": ""}, "30a30781c66c758e8e59cdb00c368f3add99768b": {"ta_keywords": "unsupervised speaker representations;network discriminative speaker;augmentation adversarial training;unsupervised speaker;recognition models speaker;augmentation adversarial;speaker representations;discriminative speaker information;propose augmentation adversarial;robust speaker recognition;speaker recognition;speaker information invariant;trained invariant augmentation;discriminative speaker;speaker recognition models;train robust speaker;work unsupervised speaker;speaker representations focused;invariant augmentation network;models speaker labels;adversarial training;models speaker;speaker information;learn acoustic characteristics;augmentation network;information invariant augmentation;acoustic characteristics network;self supervised models;embedding learning;robust speaker", "pdf_keywords": "speaker embedding extractor;speaker embedding;augmentation adversarial training;augmentation adversarial;speaker recognition propose;propose augmentation adversarial;addition speaker embedding;speaker recognition;model speaker recognition;learning method speaker;prevent speaker embedding;unsupervised automatic speaker;dealt speaker recognition;face images speech;approach speaker recognition;automatic speaker veriability;augmentation classi\ufb01er gradient;adversarial training;embedding extractor learning;speaker veriability model;speaker recognition years;method speaker recognition;recognition propose augmentation;voice activity responses;network model speaker;images speech;adversarial training aat;speaker veriability;speaker recognition based;adversarial"}, "84370f2fa3ac21ed3c6a30144fbdb377157b8853": {"ta_keywords": "preference reasoning tasks;preferences agents answering;preference reasoning;conditional preferences based;key preference reasoning;preferences based conditional;conditional preferences;preferences agents;model conditional preferences;qualitative preferences expressed;reconcile qualitative preferences;qualitative preferences;based conditional povms;preferences expressed;agents answering dominance;preferences based;answering dominance queries;represents preferences agents;conditional povms;dominance queries;propose probabilistic;agents answering;best represents preferences;probabilistic model conditional;preference;probabilistic uncertainty;use probabilistic uncertainty;answering dominance;probabilistic;use probabilistic", "pdf_keywords": ""}, "d66110a315f5f216b42d99cfafec31e8e30a03ea": {"ta_keywords": "target speaker neural;distribution target speaker;speaker neural network;speaker neural;neural network probability;network probability distribution;target speaker;probability distribution target;network probability;estimating probability distribution;estimating probability;problem estimating probability;distribution target;distribution;speaker;probability distribution;neural network;distribution given;distribution given xmath0;probability distribution given;neural;problem estimating;estimating;probability;network;study problem estimating;label eqn;target;eqn;right label eqn", "pdf_keywords": ""}, "05d1e21f5f0c8209cc125f2e9ccd3a62d6479114": {"ta_keywords": "document similarity;similarity pairs documents;document similarity metrics;assessing similarity;pairs documents based;assessing similarity pairs;method assessing similarity;kingdom document similarity;similarity pairs;similarity metrics widely;similarity metrics;information retrieval;documents based information;similarity;document classification;based information retrieval;information retrieval present;pairs documents;used document classification;document classification summarization;documents based;retrieval;retrieval present simple;records different hospitals;classification summarization;retrieval present;documents;document;widely used document;classification", "pdf_keywords": ""}, "f9409302c4d8201481fe65675bc6f0fa32e01df7": {"ta_keywords": "individual based probability;based probability distribution;probability distribution xmath0;distribution xmath0;distribution;based probability;probability distribution;distribution probability distribution;distribution xmath0 shown;distribution probability;probability distribution probability;individual based;theory individual based;probability;individual;theory individual;xmath0;xmath0 shown;xmath0 shown fig;introduction theory individual;based;brief introduction theory;introduction theory;theory;introduction;brief introduction;present brief introduction;shown fig;fig;fig fig", "pdf_keywords": ""}, "59d3f6a14e20efdf54216188e227e58a351237e5": {"ta_keywords": "fingerprint gan generated;fingerprint gan;disentangle fingerprint gan;fake image attribution;fake image detection;generative model fake;gan generated images;representation fake image;recognition gans provide;recognition gans;automatic recognition gans;binary fake image;gan generated;model fake image;gans provide clues;superior fake image;image attribution based;gans provide;fake image;image attribution;binary fake;gans;representation fake;image attribution performance;gated automatic recognition;gan;generated images;method binary fake;generative;model fake", "pdf_keywords": "interpretable gerprint gan;fake image attribution;gan propose machine;accuracy fake image;model fake image;fake images;gan;origin fake images;fake images simultaneously;gerprint gan;fake image;network model fake;accuracy fake;gerprint gan propose;image attribution;model fake;image attribution propose;gan propose;convolutional network;deep convolutional;in\ufb02uences accuracy fake;convolutional network model;deep convolutional network;image attribution closed;open world testing;origin fake;generated images;propose deep convolutional;attribute origin fake;generated images attribute"}, "207c64b36fbd6accf7067366a251d071e8dd03a7": {"ta_keywords": "species whirl vascular;vascular flora southern;whirl vascular flora;floristic affinity semif;flora southern hemisphere;vascular flora;examine floristic affinity;taxonomic properties vertebrate;vertebrate species whirl;semiflexible bidoublet pellet;floristic affinity;taxonomic;species whirl;flora southern;taxonomic properties;bidoublet pellet;examine floristic;bidoublet pellet order;study taxonomic;results study taxonomic;whirl vascular;study taxonomic properties;properties vertebrate species;flora;species;vascular;vertebrate species;floristic;semif semiflexible bidoublet;southern hemisphere", "pdf_keywords": ""}, "4f6d64eec6eaa38177ae45ad6315cf25d1535294": {"ta_keywords": "conversational question answering;conversation history modeling;selection conversation histories;conversation histories conversational;encode conversation history;conversation histories;conversation task;conversational search;prediction essential conversation;conversation history positional;essential conversation task;question answering;conversation history;conversation task dialog;histories conversational;conversational search demonstrate;dialog act prediction;selection conversation;answer prediction essential;encode conversation;answer prediction;history attention mechanism;method encode conversation;setting conversational search;role conversation history;soft selection conversation;players based helpful;mtl answer prediction;based helpful answering;conversation", "pdf_keywords": "conversations machine comprehension;conversational search systems;conversation task;conversation task dialog;functional conversational search;conversational search;handling conversation history;conversations machine;conversation history;dialog act prediction;conversation history advantage;essential conversation task;conversational personal assistant;role conversations machine;history attention mechanism;prediction essential conversation;architecture conversational search;conversational search emerging;tasks natural language;model conversation mod;traditional machine comprehension;task dialog act;model conversation;machine comprehension popular;comprehension popular tasks;dialog act;model architecture conversational;conversation mod eling;task dialog;answer prediction essential"}, "e8135016ff3bd33ace936e50247fd650fcc58a7a": {"ta_keywords": "neural machine translation;highest translation evaluation;translation nmm models;machine translation;machine translation nmm;translation evaluation;translation evaluation scores;attentional neural machine;translation nmm;achieves highest translation;translation lexicons improve;lexicons improve probability;discrete translation lexicons;translation based;translation lexicons;translation based achieves;international lhc;attentional neural;new translation based;proton collision international;highest translation;discrete translation;collision international lhc;submission attentional neural;model proton;lhc;translation;use discrete translation;neural machine;international lhc ilc", "pdf_keywords": "neural machine translation;machine translation polish;machine translation;machine translation proposed;paper machine translation;translation lexicons improve;translation task;translation task workshop;translation lexicons minimum;paper translation task;translation lexicons;discrete translation lexicons;paper translation;translation polish;incorporating discrete translation;scienti\ufb01c paper translation;translation polish english;workshop asian translation;translation proposed;discrete translation;translation proposed recent;translation;use discrete translation;lexicons improve probability;translation 2016 effective;asian translation;asian translation 2016;lexicons minimum risk;polish english;approaches attention based"}, "7847419becbc04596b79f804f844cf9719e875ea": {"ta_keywords": "learning demonstrations;approach learning demonstrations;unannotated demonstrations parsing;learning demonstrations matching;demonstrations parsing;demonstrations parsing sequences;hierarchical policies demonstrations;learning hierarchical policies;unannotated demonstrations;natural language commands;primarily unannotated demonstrations;sparse natural language;action sequences goals;policies demonstrations using;high level instruction;level instruction;high level actions;level subtask descriptions;descriptions generate sequences;model action sequences;learning hierarchical;action sequences;policies demonstrations;high level subtasks;instruction following models;level actions;subtask descriptions;level instruction sequences;low level subtask;generative model action", "pdf_keywords": "modular multitask reinforcement;level subtask descriptions;demonstrations sparsely annotated;subtask descriptions;subtask compositional description;subtask descriptions descriptions;multitask reinforcement learning;learning hierarchical policies;labeling subtask compositional;unannotated demonstrations automatically;hierarchical policy representations;multitask reinforcement;subtask compositional;high level subtask;hierarchical policies demonstrations;level action sequences;action sequences goals;segmenting subtasks labeling;segmenting subtasks;labeling subtask;level subtask;automatically segmenting subtasks;low level actions;subtasks labeling;unannotated demonstrations;reinforcement learning mir;subtasks;learning hierarchical;level actions;generative model action"}, "ad734a3f530a6c338af6bf2bf678e5af05477c1a": {"ta_keywords": "algorithm secret sharing;threshold secret sharing;distributed algorithm secret;secret sharing general;secret sharing;secret sharing dealer;generalization cryptographic protocols;problem secret sharing;secure multiparty computation;algorithm secret;cryptographic protocols large;protocols secure multiparty;efficient generalization cryptographic;sharing general networks;satisfying propagating dealer;secret sharing important;sharing dealer;byzantine agreement algorithm;efficient distributed algorithm;shares secret;generalization cryptographic;shares secret participant;communication complexity algorithm;efficient distributed;networks satisfying propagating;threshold secret;shamir threshold secret;protocols large;secure multiparty;networks satisfies propagating", "pdf_keywords": ""}, "11154216ca898590e7b2f0339587e3378c2c646c": {"ta_keywords": "quantifying driving confidence;driving confidence uses;drivers confidence driving;psychology driving confidence;drivers driving confidence;drivers confidence;driving confidence psychology;driving confidence guide;confidence driving;driving confidence;driving psychology;psychology based driving;difference drivers confidence;important driving psychology;psychology driving;driving psychology driving;driving psychology based;confidence driving conditions;confidence driving warning;quantifying driving;method quantifying driving;driving warning information;confidence guide drivers;technology driving psychology;method drivers confidence;affect drivers driving;driving warning;driving simulation technology;based driving simulation;vehicle technology driving", "pdf_keywords": ""}, "87951cea6573eed827986371a35025e478d3c184": {"ta_keywords": "optimization variational inequalities;variational inequality seg;max optimization variational;based variational inequality;optimization variational;variational inequalities problems;variational inequalities;variational inequality;inequality seg generalization;sine gordon inequality;gradient based variational;introduce stochastic gradient;convergence sde;stochastic gradient;convergence sde introduce;min max optimization;inequality seg;gordon inequality;max optimization;sde introduce stochastic;stochastic gradient based;based variational;variational;assumptions convergence sde;optimization;equation sde;sde approach relies;equation sde approach;stochastic differential;sde approach", "pdf_keywords": "nonmonotone convex optimization;guarantees stochastic optimization;hamiltonian gradient methods;stochastic hamiltonian gradient;sums stochastic hamiltonian;convex optimization;convex optimization problems;stochastic optimization;gradients players losses;adversarial nets paper;adversarial nets;weighted sums stochastic;monotone vips learning;stochastic optimization algorithms;generative adversarial nets;learning quadratic games;convergence guarantees stochastic;minimizing weighted sum;hamiltonian gradient;problem generative adversarial;nonmonotone convex;methods smooth games;guarantees stochastic;gradients players;vips learning quadratic;stochastic hamiltonian;sums stochastic;class nonmonotone convex;minimizing weighted;generative adversarial"}, "dfa7bdea128b899d348ed32a84a7ccb1da4340e4": {"ta_keywords": "dialog systems;robust dialog systems;short term memory;dialog response based;generate dialog response;term memory lstm;building robust dialog;dialog systems using;generate dialog;based dialog long;example based dialog;memory lstm neural;lstm neural networks;based dialog baselines;memory neural network;robust dialog;dialog response;term memory neural;representation user utterance;paraphrase identification retrieval;dialog long;lstm neural;dialog;sentences arbitrary length;memory neural;based dialog;retrieve generate dialog;memory lstm;dialog long short;lstm", "pdf_keywords": ""}, "3675958405f3ad1633d565efa36b4eb3004bcf59": {"ta_keywords": "live streaming upload;file video streaming;streaming live streams;optimized video streaming;streaming techniques video;streaming techniques;video compression;live streams;streams live streams;video streaming;streaming systems;streaming;streaming upload;video compression significantly;video streaming techniques;technique video compression;live streaming;streaming upload solution;file streaming;video streaming systems;video streaming important;streams live;streaming live;single file streaming;applications streaming;streaming systems significant;live streams watched;streaming important;delays video streaming;vantage live streaming", "pdf_keywords": ""}, "0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b": {"ta_keywords": "lattice language models;neural lattice language;english neural lattice;language models;language modeling tasks;language modeling;multiple language modeling;language models utilize;language model;new language modeling;language modeling paradigm;lattice language;language model allows;multiword lexical items;multiword lexical;intuitions including polysemy;incorporate linguistic intuitions;english neural;better perplexity models;lexical items language;modeling tasks english;linguistic intuitions including;granularities introduce language;lexical items;linguistic intuitions;baseline chinese model;polysemy existence multiword;including polysemy;embeddings able improve;linguistic", "pdf_keywords": "neural language models;translation compositional embeddings;token lexical units;units language modeling;lattice language models;multi token lexical;compositional embeddings words;machine translation compositional;token lexical;lattice language model;embedding words learning;neural lattice language;language models;language modeling humans;language modeling;language models particularly;embedding words;embeddings words;multiple embedding words;segment machine translation;polysemy neural language;task language modeling;billion word corpus;lexical units language;phrases multiple embedding;machine translation;embeddings words powerful;language models used;word corpus chinese;learning language model"}, "4786e10003655be97feee21b9d9894a88a62885f": {"ta_keywords": "multilingual transfer nlp;transfer nlp model;massively multilingual transfer;transfer nlp;transfer based unsupervised;multilingual transfer;supervision target language;target language;unsupervised truth inference;target language paper;resource target language;migration dynamics bidirectional;model source languages;bidirectional bidirectionally forced;bidirectionally forced semiflexible;nlp model source;nlp model;bidirectional;bidirectional bidirectionally;bidirectionally forced;techniques modulating transfer;transfer particularly distant;source languages;particularly distant languages;modulating transfer;unsupervised truth;source languages applied;distant languages;distant languages propose;modulating transfer based", "pdf_keywords": "crosslingual transfer supervised;unsupervised crosslingual transfer;multilingual transfer model;multilingual transfer;propose multilingual transfer;language specici\ufb01c transfer;transfer supervised;unsupervised crosslingual;transfer language;transfer language specici\ufb01c;crosslingual transfer;transfer supervised transfer;target language bayesian;supervised transfer;presents unsupervised crosslingual;transfer model learns;low resource supervised;supervised transfer model;labelling target language;learns quality transfer;quality transfer language;learning techniques transfer;transfer model trained;language bayesian;resource supervised baseline;data target language;resource supervised;target language;dataset word embeddings;noisy annotations outperforms"}, "4a06bfa86cbccdf5e55dcec3505cdc97b8edb288": {"ta_keywords": "grammatical type raising;grammars generalized composition;composition type raising;grammars generalized;grammatical type;study grammars generalized;generalized composition type;type raising;deal grammatical type;composition type;grammars;generalized composition;composition bounded degree;generalized composition bounded;eisner normal form;account generalized composition;paper study grammars;study grammars;normal form;extension deal grammatical;bounded degree extension;type raising paper;composition bounded;composition;generalized;degree extension;grammatical;raising;bounded degree;type", "pdf_keywords": ""}, "622f980030f766e5eb3989f36eea4459ccc948bf": {"ta_keywords": "adaptation techniques speech;mllr incremental adaptation;incremental adaptation;novel incremental adaptation;proposed incremental adaptation;incremental adaptation techniques;incremental adaptation framework;temporal changes speaker;speech recognition;acoustic models quickly;regression mllr incremental;incremental adaptation involves;speech recognition aimed;changes speaker speaking;direct adaptation approaches;mllr incremental;adjusting acoustic models;changes speaker;time variant acoustic;adaptation framework;acoustic characteristics temporal;adaptation framework based;interpretation adaptation approaches;adaptation transformation parameters;unified interpretation adaptation;adaptation approaches presented;techniques speech recognition;adaptation approaches;indirect adaptation transformation;acoustic models", "pdf_keywords": ""}, "ae06bc1e8e67c27b89329ebcfe61b71625d853f6": {"ta_keywords": "explanations perform best;gradient based explanations;explainability techniques human;explanations perform;based explanations perform;explainability techniques efforts;diverse explainability techniques;new explainability techniques;assigned explainability techniques;diverse explainability;assigned explainability;explainability criteria;explainability techniques;based explanations;human annotations salient;scores assigned explainability;explainability criteria used;reviewed explainability techniques;text classification tasks;explainability phenomenon;explainability techniques downstream;explainability;new explainability;list explainability criteria;annotations salient input;classification tasks;explainability phenomenon exists;tasks compare saliency;explainability techniques paper;list explainability", "pdf_keywords": "machine learning explainability;learning explainability;learning explainability techniques;based explanations explainability;explainability techniques gradient;gradient based explanations;explainability automatic;evaluating explainability heavily;explainability techniques propose;evaluating explainability;studies evaluating explainability;explanations explainability;explainability techniques;explainability methods;explanations explainability important;models explainability methods;models explainability;different explainability techniques;explanations best models;explainability automatic measurement;based explanations best;explainability heavily differ;based explanations;investigate different explainability;explainability;explainability heavily;explainability important research;based explanations simpli\ufb01cation;explainability methods attempt;simpli\ufb01cation based explanations"}, "43896ea7d488100d135645fbb4be6e7eb2e7f4e2": {"ta_keywords": "valued features learning;features learning systems;set valued features;dog nominal features;nominal features;feature vector representation;vector representation feature;valued features;set valued feature;valued features propose;feature vectors;feature set strings;tree rule learning;features learning;categorization;valued feature;representation feature vector;value feature set;representation feature;categorization problems;rule learning;valued features particular;rule learning algorithms;decision tree;feature vector feature;length feature vectors;decision tree rule;feature vectors real;vector representation allows;valued feature color", "pdf_keywords": ""}, "adb80a4190fdb6a019d57f18ae072ca93f494b7e": {"ta_keywords": "noisy speech recognition;discriminative training model;sequence discriminative training;discriminatively low rank;dnn acoustic models;speech recognition tasks;discriminative training;modeling speech;speech recognition effective;speech recognition;reverberated speech recognition;modeling speech based;effective noisy speech;training low rank;combination discriminative training;rank sequence discriminative;discriminative training used;low rank trained;rank model deep;discriminant training low;approach modeling speech;acoustic models outperform;network dnn acoustic;low rank model;model low rank;low rank approximations;low rank approximation;discriminatively low;discriminant training;rank trained model", "pdf_keywords": ""}, "f3762141fd64bee8d09e55ad4c83057cd4e002d4": {"ta_keywords": "coalition utility learning;utility learning;utility functions agents;utility learning method;utility function agents;utility function agent;agents estimated utilities;correlations learning utilities;learning utilities noncooperative;estimated utility;correlation coalition utility;coalition utility;estimated utility function;study utility functions;learning utilities;utility functions;game correlation coalition;correlated utilities utility;utilities correlated optimize;incentivizing efficient;estimated utilities correlated;sum estimated utility;utility function;utilities noncooperative agents;utility;leveraging correlations learning;incentivizing efficient shared;function agent weighted;highly correlated utilities;correlated utilities", "pdf_keywords": ""}, "388d41b99c9c0867301f345c65877a2796225ead": {"ta_keywords": "trained separation speaker;pairs speech recognition;mixed speech recognition;speech recognition asr;speaker pairs speech;speech recognition;speech recognition task;target speaker utterances;speaker automatic speech;automatic speech;separation speaker pairs;separation speaker;pairs speech;performance speech recognition;automatic speech recognition;function target speaker;multiple speakers speech;recognition based speaker;speaker mixed speech;target speaker asr;speaker utterances monaural;speech various signal;speaker asr;speaker utterances;sample target speaker;using speaker mixed;speaker asr baseline;speakers speech;utterances monaural mixture;transcribes target speaker", "pdf_keywords": "mixed speech recognition;talker speech recognition;multi talker speech;accuracy multi speaker;accuracy multi talker;multi speaker mixed;speaker mixed speech;architecture multi speaker;using speaker mixed;multi talker;interference speaker accuracy;speaker asr accuracy;multi speaker;target speaker asr;speech recognition;speaker asr;maximize interference speaker;speech recognition limited;speech recognition evaluated;interference speaker;separating target speaker;talker speech;speaker mixed;multi speaker conditions;target speaker speech;speaker accuracy regularize;mixed speech;speaker accuracy;speech various signal;mixed speech various"}, "7a8f8109e65ed9a6048859681a825eb5655e5dd2": {"ta_keywords": "word embeddings training;sentence embeddings;trained word embeddings;computing sentence representations;sentence representations;sentence classification;word embeddings;method sentence classification;sentence classification based;sentence representations pre;sentence embeddings solid;aim sentence embeddings;sentence length explore;embeddings training;sentence length;evaluation sentence length;sentences;methods computing sentence;sentences gain random;embeddings;embeddings training paper;representations pre trained;looking modern sentences;modern sentences;pre trained word;air conditioning;sentences gain;trained word;classification based evaluation;computing sentence", "pdf_keywords": "sentence embeddings trained;word embeddings transfer;trained word embedding;sentence embedding models;embeddings transfer tasks;word embeddings capture;embeddings trained;learned sentence encoder;sentence encoder architectures;sentence embeddings;word embedding sentential;word embeddings;word embedding;tasks sentence embeddings;embeddings transfer;embeddings trained unsupervised;sentence embedding;embedding sentential embeddings;nonrandom sentence embedding;sentential embeddings;relationship word embeddings;representation predict linguistic;sentence encoder;set word embeddings;embedding sentential;sentential embeddings powerful;models sentemic embeddings;predicting linguistic information;embedding models sentemic;embedding models"}, "03058f9a39d37a8bee635969eed227d59bbc8152": {"ta_keywords": "stochastic variational inference;gradient based stochastic;solution gradient memory;gradient memory;stochastic variational;based stochastic variational;gradient memory efficient;inference latent stochastically;latent stochastically differential;variational inference;stochastically differential;memory computation gradients;stochastically differential equations;variational inference latent;captured neural networks;stochastic differential;method stochastic differential;solvers derive stochastic;stochastic differential equations;latent stochastically;generalize method stochastic;computation gradients;stochastically;derive stochastic differential;motion captured neural;stochastic differential equation;method stochastic;stochastic;dimensional motion capture;derive stochastic", "pdf_keywords": "gradientbased stochastic variational;gradientbased stochastic;approach gradientbased stochastic;stochastic variational inference;learning stochastic gradients;stochastic gradients;latent stochastically differential;stochastically differential;based stochastic variational;stochastic variational;gradient based stochastic;learning stochastic;stochastic gradients respect;prediction stochastic motion;inference latent stochastically;stochastically differential equations;latent sde models;marginalizing latent sde;sde models arbitrary;method learning stochastic;prediction stochastic;latent sde;latent stochastically;sde models;stochastic motion;prediction outcome sde;approach prediction stochastic;variational inference scheme;variational inference;variational inference latent"}, "4eb62ee328ceac9976c72bca65570d73ca0e8b64": {"ta_keywords": "single sex marriage;marriage single sex;sex marriage single;example single sex;sex marriage;marriage single;single sex partner;single sex;partner change marriage;change marriage status;marriage;marriage status;change marriage;sex partner change;partner change;example single;single;simple example single;sex partner;sex;partner;simple example;present simple example;present simple;status;change;example;simple;present", "pdf_keywords": ""}, "30a6a5614727017e7d7981f87df57d17713501a0": {"ta_keywords": "bandit framework matching;demand bike sharing;personalized incentives recommendations;incentives users preferences;matching incentives users;bandit framework;greedy matching paradigm;matching incentives;multiarmed bandit framework;propose greedy matching;personalized incentives;greedy matching;propose multiarmed bandit;incentives recommendations;greedy matching algorithm;bike sharing;framework matching incentives;bike sharing platform;demand bike;incentives recommendations improve;incentives users;time propose greedy;design personalized incentives;propose greedy;greedy;theoretical bounds regret;multiarmed bandit;domains greedy matching;incentives;bandit", "pdf_keywords": "bandit problem combines;algorithm online bandit;online bandit problem;bandit problem;bandit problem agent;bandit strategy theory;bandit problem paper;optimal incentives user;bandit strategy;novel online bandit;online bandit;semi bandit model;present semi bandit;seeks incentive maximizes;incentive maximizes;semi bandit setting;algorithm bike sharing;bandit model leverages;armed bandit problem;incentive maximizes expected;semi bandit;optimal incentives;bandit model;bandit setting work;multi armed bandit;bandit setting;ideas greedy matching;bandit;developed semi bandit;armed bandit strategy"}, "357ff26120dd220d7132f8083697d54b007ef260": {"ta_keywords": "usability speech processing;speech processing universal;speech processing tasks;speech processing community;supervised learning ssl;universal performance benchmark;performance benchmark superb;lightweight prediction;challenge leaderboard benchmark;ssl representations competitive;lightweight prediction heads;benchmark toolkit;benchmark superb;representation learned ssl;wide range speech;promising ssl representations;speech processing;shared model benchmark;range speech processing;benchmark toolkit present;model benchmark;benchmark performance shared;model benchmark performance;specialized lightweight prediction;leaderboard benchmark toolkit;tasks self supervised;accessibility superb tasks;task specialized lightweight;benchmark performance;performance benchmark", "pdf_keywords": ""}, "9f24b8f93ed00a1592e02fdb0edf5ebf0d8752ff": {"ta_keywords": "assignment papers reviewers;accuracy peer review;maximize review quality;automated assignment papers;reviewers conference peer;conference peer review;peer review;objective maximize review;peer review process;maximize review;automated assignment;papers reviewers;accuracy peer;peer review design;assignment algorithms overcomes;review quality disadvantaged;various assignment algorithms;assignment algorithms;quality papers analysis;experiments peer review;subjective score model;papers reviewers conference;assignment algorithm based;proposed assignment algorithm;assignment algorithm;subjective score;review quality;analysis accuracy peer;papers accepted algorithm;review design assignment", "pdf_keywords": ""}, "6a4deeb40aed8a4d56c8d9401c94b6c7a769e8c3": {"ta_keywords": "information retrieval;rhetorical structure elements;linguistics machine learning;information retrieval task;linguistics machine;known information retrieval;scientific literature search;retrieval task document;analysis rhetorical structure;retrieval;rhetorical structure;retrieval task;computational linguistics machine;literature search;query document introduce;computational linguistics;task scientific literature;query documents;query documents drawn;linguistics;expert annotated test;annotated test collection;analysis rhetorical;scientific literature;field document covers;drawn computational linguistics;documents drawn computational;annotated test;query document;relevant documents large", "pdf_keywords": "faceted literature search;text matching causal;similarity purpose facets;document similarity methods;approach faceted literature;document similarity;faceted literature;field faceted literature;documents similar;document facet example;inference observational text;computing papers similarity;documents similar input;set document similarity;similarity methods;papers similarity;similarity methods paper;document facet;retrieve documents similar;text matching;similar input document;observational text data;linguistics machine learning;use text matching;input document facet;facet example;matching causal inference;papers similarity purpose;similarity;literature search based"}, "356ea9b29101ec6974a7a97b62266b0e7e58d6bf": {"ta_keywords": "3d reinforcement learning;dimensional 3d reinforcement;3d reinforcement;reinforcement learning rl;model euler lagrange;reinforcement learning;lagrange el equations;learning rl model;euler lagrange el;rl model euler;el equations;euler lagrange;dimensional 3d;reinforcement;learning rl;lagrange el;lagrange;3d;rl model;performance dimensional 3d;model euler;el;rl;equations;learning;performance dimensional;model;dimensional;euler;study performance dimensional", "pdf_keywords": ""}, "9e0d3161b13481418b7e85e3a691d23d67cf1e68": {"ta_keywords": "object detectors correlation;images leak privacy;privacy leaking image;object detectors;sharing images social;leaking image detection;pretrained object detectors;images social media;correlation appropriate images;detectors correlation objects;object detectors modeling;automatically identify images;images social;identify images leak;privacy leaking;detection spatially temporally;correlated features image;privacy leakage dynamically;privacy leakage;detectors correlation;privacy;sharing images;correlation self attention;detectors modeling correlation;leak privacy;detection spatially;practice sharing images;detection;identify images;privacy paper describes", "pdf_keywords": "privacy spread images;drag privacyleaking image;online image sharing;privacy online image;drag privacyleaking;privacyleaking image;image sharing;images leak privacy;privacyleaking image detection;aware graph convolutional;image sharing brings;called drag privacyleaking;image detection privacy;annotate sharing images;privacy leaking image;images social media;image social media;sharing images;sharing images contain;shared images overcome;shared images;share images;risk shared images;detection privacy;graph convolutional network;adaptively image social;images social;share images leak;new privacy;privacy risk shared"}, "a3c9d1c5e403f35e5694778b86832f0f9a7d87e6": {"ta_keywords": "graph indexing speech;acoustically similar query;search large speech;indexing speech model;fast finding speech;utterance gmm search;indexing speech;large speech data;similarity search large;finding speech model;fast similarity search;similarity search;finding speech;closest query graph;speech model acoustically;similarity search present;speech data;model acoustically similar;speech models;speech model;speech data set;speech model set;acoustically similar;neighborhood graph indexing;similar closest query;set speech models;speech models paper;similar query model;large set speech;fast similarity", "pdf_keywords": ""}, "77910e51a40d17157fc798325d06edfa6cff18d6": {"ta_keywords": "programming language api;nl code generation;code generation;language python natural;programming language python;python natural language;knowledge nl code;programming language;purpose programming language;writing code explore;code generation aims;quantum information processing;programming qa;code explore;code generation automatically;generate code;language python;language api;code explore effectiveness;natural language;entanglement entropy;language api documentation;generate code general;online programming qa;general purpose programming;entanglement entropy entropy;quantum;code generation testbed;python natural;based entanglement entropy", "pdf_keywords": "knowledge code generation;external knowledge code;code generation models;domain code generation;learning parse database;automatically external knowledge;existing api documentation;external knowledge resources;learning parse;natural language intents;propose retrievalbased;knowledge code;api documentation;code generation;model learning parse;code generation task;representations natural language;api documentation actual;propose retrievalbased sampling;existing api;api;programming task generating;manually curated dataset;augmentation retrieval data;natural language;api documentation turning;resources existing api;retrieval data sampling;inductive logic programming;executable meaning representations"}, "948b68677c4f3bcbb1bae7f1d4e1fd5a103f03d4": {"ta_keywords": "enhancement automatic speech;speech enhancement automatic;optimize speech enhancement;objectives speechechanical tweezer;automatic speech;incorporating estimation speech;speech enhancement;speechechanical tweezer;speech enhancement quality;estimation speech distortion;improves speech enhancement;estimation speech;optimize speech;enable optimize speech;speech enhancement application;objective improves speech;metric automatic speech;approach automatic speech;improves speech;speechechanical tweezer swt;jointly optimize speech;speech recognition asr;automatic speech recognition;speech recognition;speech distortion;learning extend beamforming;learning extend;speech recognition based;speech distortion factor;enhancement automatic", "pdf_keywords": ""}, "5edaab1fa078a5c468e3fb26d267ca49be32e70e": {"ta_keywords": "preference elicitation aggregation;preference elicitation;efficient randomly asking;given question prediction;framework preference elicitation;question prediction;information criteria efficient;question prediction accuracy;criteria efficient randomly;elicitation aggregation;predict expected information;based idea probability;propose probabilistic framework;propose probabilistic;expected information gain;randomly asking questions;elicitation aggregation plackett;elicitation method based;criteria efficient;estimate cost elicitation;association determined probability;propose framework elicitation;cost elicitation method;idea probability finding;expected information;probabilistic framework predict;luce model features;designed information criteria;probabilistic framework;randomly asking", "pdf_keywords": "elicitation hotel preferences;questions preference elicitation;preference elicitation information;eliciting preferences candidates;preference elicitation widely;directly eliciting preferences;eliciting preferences;preference elicitation;questions probabilistic elicitation;eliciting preferences goal;hotel preferences given;preference elicitation adapted;elicitation adapted ranking;elicitation questions probabilistic;probabilistic elicitation questions;hotel preferences;probabilistic elicitation;preferences goal elicitation;elicitation information criteria;preferences candidates infeasible;elicitation hotel;problem eliciting preferences;information criterion elicitation;framework elicitation hotel;elicitation questions widely;criterion elicitation questions;framework preference elicitation;decision directly eliciting;preferences candidates;criterion elicitation"}, "98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de": {"ta_keywords": "named entity recognition;entity recognition english;entity recognition;entity recognition models;lingual entity linking;exhaustive entity gazetteers;neural named entity;gazetteers exist languages;gazetteers lists entities;entity gazetteers exist;_soft gazetteers incorporates;entity gazetteers;english knowledge bases;method _soft gazetteers;_soft gazetteers;models use gazetteers;lingual entity;entity linking;cross lingual entity;models cross lingual;gazetteers incorporates ubiquitously;named entity;english data;recognition english data;use gazetteers lists;gazetteers exist;use gazetteers;resource languages challenging;traditional named entity;english data designing", "pdf_keywords": "gazettesers neural models;neural models gazetteer;gazettesers neural;lingual entity linking;language soft gazetteer;named entity recognition;entity recognition models;entity recognition;soft gazetteer features;neural named entity;method soft gazetteers;called gazettesers neural;soft gazetteers recently;soft gazetteers incorporates;soft gazetteers;gazetteer features trained;soft gazetteer;entity linking;present soft gazetteer;english knowledge bases;named entity mention;gazetteer based features;entities called gazettesers;linking entity linking;bases wikipedia neural;lingual entity;entity linking el;entity linking entity;cross lingual entity;models gazetteer based"}, "55b61befce42280c3d57331121c7d349dd8be4cf": {"ta_keywords": "simultaneous speech translation;speech translation systems;speech translation technology;speech translation;translation systems;evaluation simultaneous speech;accuracy simultaneous speech;speech translation beginning;inherent speech translation;translation systems consideration;translation technology;translation technology attempts;simultaneous speech;delay inherent speech;translation beginning translation;speed accuracy simultaneous;translation end explicit;better speed accuracy;beginning translation end;translation beginning;accuracy simultaneous;speed accuracy systems;reduce delay inherent;reduce delay;translation end;speech;speed accuracy;translation;evaluation simultaneous;beginning translation", "pdf_keywords": ""}, "a949ba38194ad43c86925acec6705b434d5a920f": {"ta_keywords": "detect false positives;false positive detection;positives false positive;positive detection test;false positives;false positives false;automatically detect false;false positive;detect false;positive detection;positives false;automatically detect;detection test;method automatically detect;detect;detection;positives;method automatically;automatically;test;positive;new method automatically;false;method;new method;present new method;present new;paper present new;paper present;paper", "pdf_keywords": ""}, "22b6e88a2f234fc5646f6239f9040a776e841a97": {"ta_keywords": "bilingual lexicon corpus;bilingual lexical entries;monolingual bilingual lexical;induction bilingual lexicon;bilingual lexical;bilingual lexicon;english translations alignment;high precision corpora;lexicon corpus phonemic;translations alignment algorithms;monolingual bilingual;induction bilingual;translations alignment;investigate induction bilingual;aligned english translations;precision corpora;corpora;corpus;corpus phonemic transcriptions;lexical entries learnt;corpus phonemic;lexicon corpus;bilingual;transcriptions sentence aligned;track endangered languages;translations;precision corpora having;lexical entries;phonemic transcriptions sentence;transcriptions sentence", "pdf_keywords": ""}, "9abd13caa32b1a90e32462a884a512f8666e80cc": {"ta_keywords": "parsing results propose;natural language queries;language queries contextual;parsing results context;queries contextual;dependent semantic parsing;queries contextual information;context query processing;analysis parsing results;semantic parsing;parsing results;follow query analysis;approach analysis parsing;analysis parsing;parsing;methods parsing results;language queries;semantic parsing proven;query processing;query processing proposed;perform follow query;methods parsing;parsing proven important;follow query;context query;dependent natural language;results context query;query analysis;context dependent semantic;query analysis aiming", "pdf_keywords": "answering dialogue experiments;answer answering dialogue;contextindependent semantic parsing;answering dialogue;natural language queries;independent semantic parsing;language queries contextual;task natural language;advances contextindependent semantic;approach answer answering;dependent natural language;semantic parsing;queries contextual;sentence level embedding;answer answering;natural language processing;context independent semantic;queries contextual information;semantic parsing propose;processing sentence level;dialogue experiments;single natural language;contextindependent semantic;independent semantic;dialogue;restate follow queries;semantic parsing translates;language queries;follow queries;parsing"}, "3c8853d4ae3ad2633c47e840a48951d62b64a5b4": {"ta_keywords": "view graph learning;graph learning;extraction graph sparsification;graph multi view;graph sparsification;multi view graph;graph learning adaptive;graph learning methods;graphs single view;graph learning seeks;graph sparsification graphs;conventional graph learning;sparse graph;multi view learning;view graph;constructs sparse graph;factor extraction graph;sparse graph accordingly;label propagation latent;view learning;sparsification graphs;sparsification graphs play;reliable graph multi;latent factor extraction;graphs single;view learning baselines;graph multi;constructing graphs single;label propagation promising;graphs", "pdf_keywords": ""}, "df29486c04eafd004f2f0816e84c798783802cdf": {"ta_keywords": "transliteration related languages;lingual transfer typologically;morphological inflection;related languages grapheme;languages grapheme phoneme;transfer language data;languages grapheme;transliteration;lingual transfer;transliteration related;cross lingual transfer;morphological inflection paper;transfer language;task morphological inflection;use transliteration related;language data;cross lingual;use transliteration;related languages;grapheme phoneme conversion;typologically related languages;lingual;language data language;language leads accuracy;points cross lingual;explore use transliteration;function given language;given language;related languages proven;languages", "pdf_keywords": ""}, "298a68153859303ee70b3ef1525ee9c7031e32f5": {"ta_keywords": "personalized dialogue generation;quality personalized dialogue;dialogue generation chit;chit dialogue systems;personalized dialogue;dialogue generation;chat dialogue systems;chit chat dialogue;chit chat conversation;chat dialogue;dialogue systems experiments;dialogue systems;quality chit chat;chat conversation;dialogue systems interlocutors;chit dialogue;dialogue;conversation;chit chit dialogue;chit chat;traditional chit chat;chit messages;exchange chit messages;automatic metrics human;generation chit;generation chit chit;metrics human evaluations;enhance quality personalized;quality personalized;quality chit", "pdf_keywords": "dialogue generation;dialogue generation based;approach dialogue generation;mutual persona perception;dialogue;approach dialogue;persona perception;persona perception works;mutual persona;human like responses;based mutual persona;new approach dialogue;persona;mimicking style human;modeling understanding interlocutors;human like;style human like;style human;generation based;generation based mutual;like responses;human;modeling understanding;generation;mimicking style;interlocutors;explicitly modeling understanding;understanding interlocutors;mimicking;responses leaving understudied"}, "251a80dd4126fed3d6ae64f00dc24479f0ba5662": {"ta_keywords": "player knockout tournament;knockout tournament;competes tournament;tournament player wins;player competes tournament;tournament;simulation player knockout;knockout tournament player;player menudo tournament;tournament paper present;tournament paper;competes tournament paper;menudo tournament;tournament player;player knockout;competition player menudo;results competition player;check player wins;player competes;player wins championship;competition player;simulation player;knockout;championship;competition;player wins;computer simulation player;player wins time;results competition;wins championship", "pdf_keywords": ""}, "46dab5eb9c11bd49893e2dafa7d1b720a0aa2b3d": {"ta_keywords": "reading comprehension tasks;comprehension tasks achieving;word character level;tag prediction;representation word level;performance reading comprehension;word level character;comprehension tasks;reading comprehension;word level;character level representations;tag prediction present;combine word character;media tag prediction;word character;dynamically combine word;concatenation scalar weighting;reading;representations using concatenation;improve performance reading;combine word;representations using;scalar weighting propose;words case speed;novel gating;scalar weighting;fine grained gating;grained gating;representations based;performance reading", "pdf_keywords": ""}, "e92677eb974a2814d57de54e2c3733cbd92e2c00": {"ta_keywords": "hierarchical cern lhc;distributed computing;efficient parallel decoding;performance hierarchical cern;class distributed computing;decoding cost computing;distributed computing used;computing systems distributed;low complexity computation;systems distributed computing;parallel decoding;distributed computing systems;parallel decoding reducing;world distributed computing;hierarchical cern;hierarchical coding scheme;cost computing time;reducing decoding costs;cern lhc;hierarchical computational;lhc;proposed hierarchical coding;propose hierarchical coding;consider hierarchical computational;computation time;hierarchical coding;hierarchical computational structure;cost computing;lhc hlc run;cern lhc hlc", "pdf_keywords": "descent distributed computing;distributed computing;complexity distributed computing;propose distributed computing;distributed computing architecture;architecture distributed computing;distributed computing systems;coded distributed;computing architecture distributed;data coded distributed;parallel decoding algorithm;develop parallel decoding;parallel decoding;distributed gradient descent;parallel decoding reducing;architecture distributed;coded distributed n2;low complexity distributed;ef\ufb01cient parallel decoding;coding scheme hierarchical;descent distributed;decoding cost computing;hierarchical computing;complexity distributed;gradient descent distributed;proposed hierarchical coding;hierarchical coding;distributed n2 racks;hierarchical computing different;distributed"}, "0e9e334e2647307f8fa7f9937d93f3ca9095e351": {"ta_keywords": "optimisticgradient method hamiltonian;harmonic oscillators extragradient;oscillators extragradient method;operators optimisticgradient method;operators optimisticgradient;oscillators extragradient;extragradient method;update operators optimisticgradient;method hamiltonian gradient;cocoercivity update operators;extragradient method popular;optimisticgradient method;hamiltonian gradient method;monotone lipschitz vip;saddle point variational;hamiltonian gradient;optimisticgradient;method hamiltonian;rate change quantum;variational inequalities problems;variational inequalities;lipschitz vip additional;lipschitzness jacobian operator;change quantum;point variational inequalities;extragradient;change quantum state;lipschitz vip;update operators;rate monotone lipschitz", "pdf_keywords": "convex optimization;convex optimization linear;approach convex optimization;gradient descent ascentive;cocoercivity stochastic gradient;new approach convex;weighted gradient descent;optimization linear operators;ascentive weighted gradient;descent ascentive weighted;approach convex;gradient descent;descent gradient descent;stochastic gradient descent;variational inequalities problems;descent gradient;descent ascentive;point variational inequalities;gradient descent proof;variational inequalities;convex;gradient descent gradient;optimization linear;proximal point approximations;convex concave;cocoercivity stochastic;ascentive weighted;convex concave saddle;optimization;convergence unweighted action"}, "75ba422d90c488b1388345865e0525208331bb3d": {"ta_keywords": "nlp tasks privacy;privacy preserving;tasks privacy preserving;privacy preserving strategies;different privacy preserving;privacy regime;privacy regime requires;lipids nlp tasks;task privacy regime;privacy;pattern task privacy;lipids nlp;analysis different privacy;typical lipids nlp;different privacy;tasks privacy;task privacy;performance nlp models;improving performance nlp;nlp models;performance nlp;private approaches;private approaches solving;nlp models presence;nlp tasks;nlp tasks varying;non private approaches;solving nlp tasks;approaches solving nlp;neural models paper", "pdf_keywords": "model differential privacy;differential privacy improved;differential privacy;hypothesize differential privacy;differential privacy large;task differential privacy;differential privacy paradigm;finetuning differential privacy;differential privacy model;privacy preserving learning;privacy model differential;differential privacy noise;differentially private stochastic;behavior differential privacy;models varying privacy;private stochastic gradient;differentially private;networks differentially private;varying privacy;privacy preserving;privacy improved finetuning;privacy improved;privacy paradigm dwork;privacy;different privacy preserving;privacy paradigm;privacy large;varying privacy regimes;private stochastic;privacy regimes paper"}, "9d3e33875ec39001e72313fb919f66242ee97880": {"ta_keywords": "discovery raw speech;discovery linguistic;surrounding discovery linguistic;discovery linguistic units;words language orthography;unsupervised discovery;linguistic units subwords;subwords words language;language orthography;words language;problem unsupervised discovery;unsupervised discovery raw;subwords words;movement animals noisy;linguistic;raw speech;linguistic units;subwords;raw speech paper;units subwords words;speech;discovery;speech paper;noisy environment motion;animals noisy;model movement animals;movement animals;motion animals;animals noisy environment;motion animals modulated", "pdf_keywords": "automatic learning spoken;unsupervised learning spoken;speech image retrieval;language speech image;translation speech image;speech image;image speech image;language semi supervised;using speech image;speech data;learning spoken;speech image speech;raw speech data;like units speech;speech image relatively;speech image associations;learning spoken language;speech data based;units speech;image speech;resource language speech;project speech translation;semi supervised training;rosetta project speech;language raw speech;spoken language raw;units speech signal;semi supervised;word discovery metrics;speech translation"}, "7d7469e059c6890c24d42931c697df835329f26a": {"ta_keywords": "estimating noise mixture;noise mixture model;noise mixture;peak distribution noise;estimating noise;estimate noise;noise multi peak;way estimating noise;estimate noise propose;distribution noise;noise suppression method;noise suppression;distribution noise deal;noise model;mmse estimate noise;nonstationary noise multi;new noise suppression;noise multi;noise model complex;distribution gaussian fluctuations;signal model noise;nonstationary noise;multi peak distribution;gaussian fluctuations proposed;model noise model;noise deal noise;gaussian fluctuations;optimization observed signal;model noise;based gaussian distribution", "pdf_keywords": ""}, "f7f6160d4e9e3bf7f36bacbc9f15e916a6f226de": {"ta_keywords": "multichiral speech enhancement;end speech recognition;multichannel noisy speech;multichichannel noisy asr;enhancement speech recognition;speech recognition architecture;speech enhancement;adequate speech enhancement;speech recognition single;components multichiral speech;speech enhancement ability;enhancement speech;speech enhancement speech;speech significantly improves;evaluation speech quality;noisy speech data;multichiral speech;speech recognition;speech recognition component;stage speech recognition;speech data;speech quality;noisy asr tasks;architecture multichichannel noisy;arm rippled speech;utility automatic speech;speech quality recently;multichichannel noisy;speech data available;automatic speech recognition", "pdf_keywords": ""}, "99c87e16c56b8a113124779734951f11bd662d5d": {"ta_keywords": "energy consumption social;game building energy;energy efficient behavior;consumption social game;energy consumption building;encouraging energy efficient;voting based mixture;design voting;design voting based;building energy efficiency;continuous game characterize;social game designed;energy consumption;behavior building occupants;consumption building;energy efficiency model;efficient behavior building;utility maximizers;efficiency model occupants;occupants utility maximizers;utility maximizers utility;model design voting;encouraging energy;building energy;agents continuous game;using nash equilibrium;continuous game;voting based;game building;addition social game", "pdf_keywords": "energy consumption building;energy consumption propose;utility maximizers play;energy consumption;game centered hvac;occupants behave energy;occupants utility maximizers;reduction energy consumption;utility maximizers;consumption building model;formulate utility estimation;consumption building;utility estimation;utility estimation problem;social game formulate;overall energy consumption;implement social game;encouraging energy;savings social game;computing nash;consumption propose social;reducing overall energy;utility functions convex;game social game;designed encouraging energy;estimating occupant utility;method computing nash;game inducing building;computing nash equilibrium;social game designed"}, "4697ef43450f173e12b1e22b77e976dc56fdf5fe": {"ta_keywords": "adversarial attack considered;adversarial attack;neural network adversarial;network adversarial attack;semiflexible bidimensional swimmer;security deep neural;adversarial;network adversarial;box attacks mnist;attacks mnist;white box attacks;swimmer proposed algorithm;complexity security deep;bidimensional swimmer;swimmer;security deep;attacks;attack experimental;box attacks;attack experimental results;deep neural;type attack experimental;bounds various recovery;deep neural network;attack types;type attack;bidimensional swimmer proposed;neural network;attack;information type attack", "pdf_keywords": "compressive sensing;novel compressive sensing;compressive sensing based;face detection security;object recognition covid;basis pursuit;l0 norm attacks;norm attacks gradient;basis pursuit combat;detection security;pursuit combat l0;propose novel compressive;recognition covid;attacks gradient based;compressive;reconstructed compressed images;norm attacks;gradient based attacks;attacks spectral domain;novel compressive;l0 attacks spectral;attacks spectral;using reconstructed compressed;compressed images;attacks gradient;detection security surveillance;effectively classifying uncompressed;vehicles face detection;combat l0 attacks;versions basis pursuit"}, "ce3d6673b7eebdd0198940316d18e383e9597c9a": {"ta_keywords": "learning contextual bandits;bandits bound achieved;bandits bound;contextual bandits;contextual bandits help;results optimal regret;armed bandits bound;bandits help loss;optimal regret mathcal;optimal regret;bandits;multi armed bandits;bandits help;regret mathcal min;armed bandits;adversarial versus stochastic;better bound;lower bounds various;learning rounds;optimal;surprising results optimal;better bound non;learning rounds provide;standard better bound;sqrt achievable predictors;results optimal;bounds various;achievable predictors;lower bounds;various settings adversarial", "pdf_keywords": "regret contextual bandits;learning contextual bandits;contextual bandits extent;contextual bandits;adversary regret min;contextual bandits help;prediction expected regret;adversary regret;ided adversary regret;adversarial setting stochastic;reduce regret contextual;bandits help loss;setups adversarial stochastic;adversarial setting contexts;adversarial stochastic environments;adversarial stochastic;adversarial setting;bandits extent;known consider adversarial;regret upper bound;regret contextual;consider adversarial setting;algorithm matching regret;various setups adversarial;bandits;bandits extent non;setups adversarial;adversarial;contexts losses predictions;bandits help"}, "29b3a609f2b5cb10cffb80a6aaf96413a4a9998e": {"ta_keywords": "iflow neural compression;generative compression;generative compression algorithms;deep generative compression;lossless compression using;lossless compression;state art compression;compression using normalizing;compression algorithms significantly;invertible flow transformations;data compression;neural compression;flow transformations new;codecs terms compression;compression;flow transformations based;compression ratio iflow;flow transformations;compression algorithms;compression ratios propose;compression using;compression ratios quicker;achieving high compression;tool data compression;normalizing flows;compression retrieval;discuss lossless compression;neural compression shown;using normalizing flows;fluid dynamics video", "pdf_keywords": "neural compression;neural compression techniques;state art compression;learning machine translation;compression performance benchmarking;approaches deep probabilistic;deep probabilistic;machine translation;compression;variety neural compression;deep probabilistic models;compression performance;comparing compression;new machine learning;iflow comparing compression;comparing compression performance;approaches deep;compression techniques;compression ratios coding;machine translation paper;based normalizing \ufb02ows;compression ratios;normalizing \ufb02ows 11;normalizing \ufb02ows;models based normalizing;art compression;compression techniques paper;benchmarking;benchmarking datasets;deep"}, "24ee54c8d5a01197e015d40be4277cfbb727394f": {"ta_keywords": "bus routes sharing;planning bus routes;plan bus routes;bus routes;planning bus;design bus routes;bus routes consideration;lanes planning bus;driven plan bus;plan bus;routes sharing bike;bus route suitable;sharing bike lanes;bus route;plan city transportation;designed bus route;bus routes paper;routes consideration sharingbikes;bikes emergence sharing;extend bus routes;sharing bikes area;bus routes constraint;sharing bikes emergence;bus;passengers ride sharing;bike lanes planning;emergence sharing bikes;share common parking;directness bus routes;sharing bikes", "pdf_keywords": ""}, "ea5cfce90444b17b36da07840b2f0cafb54ab0a7": {"ta_keywords": "automatic deception detection;deception detection;deceptive corpus japanese;higher deception detection;deception detection accuracy;deception propose dialogue;deceptive corpus;detect deception;automatic deception;deception detection used;collect deceptive corpus;attempt detect deception;detect deception perform;deception asking questions;deceptive conversational partner;deceptive conversational;deception asking;deception salient humans;deception perform actions;paper automatic deception;deception;perform higher deception;features deception;deception propose;learn signs deception;signs deception asking;deception perform;features deception salient;unveil deceptive conversational;signs deception propose", "pdf_keywords": ""}, "e3a1b2a19356dc685d78630ae2a8852ad6c86200": {"ta_keywords": "pruning proximity information;proximity aware ranking;retrieval allows efficiently;indexing word pairs;repeater based entanglement;entanglement generation experiments;pruning proximity;proximity information;entanglement generation;moderately loaded retrieval;mild pruning proximity;entanglement;retrieval;words certain distance;based entanglement generation;loaded retrieval;quantum repeater;aware ranking functions;quantum repeater based;loaded retrieval allows;retrieval allows;phrase queries promptly;performance quantum repeater;proximity information appropriate;close pairs words;aware ranking;quantum;occurrences close pairs;performance quantum;indexing word", "pdf_keywords": ""}, "5e24aa9fdf5466e96d314dfcde973fccec02995d": {"ta_keywords": "single pass learning;batch learners single;batch feature selection;batch learners;pass online learning;pass learning algorithms;pass learning;batch feature;online learning methods;learning competitive batch;learning online learning;learners single pass;online learning;line learning online;feature selection;used batch feature;learning online;advantages line learning;single pass online;line learner single;feature selection methods;svm;passes training data;feature;linear svm;training line learner;features fly;select features fly;comparable linear svm;algorithm feature selection", "pdf_keywords": ""}, "e2854bf66ed86a5dc74183bae5fde18e65699833": {"ta_keywords": "neural network hmm;speech recognition extend;memory hidden markov;speech recognition;hidden markov;hidden markov model;short term memory;sound event detection;markov chain hmm;polyphonic tasks;field speech recognition;network hidden markov;performance field speech;hidden markov chain;network hmm achieves;polyphonic sound event;monophonic polyphonic tasks;chain hmm multi;network hmm;chain hmm;hmm multi label;markov model hybrid;polyphonic;field speech;approaches monophonic polyphonic;recognition extend hybrid;markov;camera motion pedestrian;sound event;markov model", "pdf_keywords": ""}, "21c9c624bc328686cef4bb1f80a786a5027d8886": {"ta_keywords": "scientific information extraction;information scientific documents;content citation graph;structure content citation;information extraction;citation graph referential;information extraction tasks;context citation graph;organized criticality sciie;citation graph;information extraction presence;scientific documents potential;criticality sciie automatically;end information extraction;document context citation;cited papers;content citation;referential links citing;sciie automatically extracting;scientific documents;links citing;scientific text;extracting document level;raw scientific text;citation graph lead;improve literature search;links citing cited;key information scientific;method extracting document;text representations leveraging", "pdf_keywords": "citation aware sciie;annotated citation graph;information extraction citationie;dataset annotated citation;citation graph information;citation graph content;explore citation aware;source citation graph;citation aware;extraction citationie model;extraction citationie;annotated citation;citation graph;citationie;graph content citing;enables explore citation;explore citation;citationie model;cited documents;structure citation graph;2014 imagenet classification;use citation graph;citing cited documents;large convolutionality kernels;imagenet classification challenge;existing sciie dataset;content citing cited;citationie model leverage;scienti\ufb01c information extraction;content citing"}, "50c651e9f94f9d4927a726af0ef44818179d87da": {"ta_keywords": "translation semantic parsing;semantic parsing problem;semantic parsing;semantic parser;semantic parser parser;adapted semantic parser;semantic parsing fundamental;semantic parsing context;machine translation semantic;parser;parser parser;parsing;translation semantic;parser parser competitive;theory machine translation;structured meaning representation;process semantic parsing;machine translation task;parsing problem;parser competitive;machine translation;parsing fundamental;parsing fundamental problem;parsing context;machine translation powerful;parser competitive state;parsing problem deriving;problem machine translation;representation natural language;translation powerful machine", "pdf_keywords": ""}, "cd0702deabaa8b7ccfba077f89dcc24e48ae1d47": {"ta_keywords": "documents subtopic retrieval;subtopic retrieval using;evaluating subtopic retrieval;subtopic retrieval utility;subtopic retrieval;performing subtopic retrieval;subtopic retrieval problem;track subtopic retrieval;problem subtopic retrieval;retrieval problem subtopic;relevance ranking;subtopic retrieval accounting;baseline relevance ranking;relevance ranking data;marginal relevance ranking;relevance ranking strategy;subtopics query topic;document ranking;ranking dependent documents;documents subtopic;retrieval methods;dependent documents ranking;retrieval utility;utility document ranking;document ranking dependent;documents ranking;outperform baseline relevance;retrieval poses challenges;traditional retrieval methods;retrieval using", "pdf_keywords": ""}, "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d": {"ta_keywords": "conformal prediction prediction;prediction prediction conformal;prediction conformal;conformal prediction;prediction conformal field;prediction heads;classification regression tasks;extension conformal prediction;prediction heads intermediate;confidently accelerating inference;accelerating inference;prediction prediction;prediction;additional prediction heads;accelerating inference large;regression tasks;classification regression;classifier develop novel;additional prediction;processing nlp;inference large expensive;approach classification regression;natural language processing;classifier;processing nlp propose;classifier develop;nlp;classification;computational effort;nlp propose novel", "pdf_keywords": "calibration inference pipelines;quantifying uncertainty prediction;con\ufb01dent adaptive transformers;prediction deciding;allows predict outcome;prediction consistent;uncertainty prediction deciding;training conformal calibration;uncertainty prediction;conformal calibration inference;prediction;allows predict;inference pipelines;prediction consistent small;predicting;prediction deciding additional;specify training conformal;adaptive transformers;calibration inference;predict outcome certain;adaptive;predict;quality predictions;inference pipelines summarized;process allows predict;capable quantifying uncertainty;predict outcome;multilayered machine learning;model agnostic;free model agnostic"}, "77cd3ae8a0b9ef6865d5324a4d62280e6f7a1053": {"ta_keywords": "gan augmentation trained;cycle gan augmentation;gan augmentation;augmentation trained;augmentation trained map;data augmentation;augmentations chime task;augmentation methods;label augmentation;automatic speech;data augmentation using;use augmentation methods;augmentation methods thier;label augmentation provided;augmentation using;combine augmentations chime;use augmentation;augmentation provided pretrained;augmentations chime;pseudo label augmentation;augmentation;results augmentation;speech noisy recordings;cycleconsistent generative adversarial;augmentation using text;end automatic speech;speech recognition e2easr;augmentation method;augmentation method individually;automatic speech recognition", "pdf_keywords": "speech domain adaptation;gan augmentation trained;gan augmentation;augmentation methods train;augmentation methods unsupervised;cycle gan augmentation;augmentation trained;unsupervised speech domain;propose augmentation methods;augmentation trained map;data augmentation conventionally;data augmentation;augmentation methods effective;label augmentation;pseudo label augmentation;augmentation methods;augmentation using text;label augmentation provided;data augmentation using;answer augmentation model;proposed augmentation methods;augmentation provided pretrained;methods unsupervised speech;answer augmentation;augmentation methods thier;use augmentation methods;augmentation using;text speech;cycleconsistent generative adversarial;speech domain"}, "3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c": {"ta_keywords": "neural semantic parser;semantic parser reranking;semantic parsing tasks;2016 semantic parsing;semantic parsers achieved;existing neural semantic;semantic parsing;parsing tasks improving;semantic parsers;semantic parser;parser reranking best;parser reranking;test semantic parsing;semantic parser test;parsers achieved impressive;neural semantic;based semantic parsers;parsers achieved;semantic parsing considers;strong baseline parser;parsing tasks;parser test semantic;parsers;competitive neural semantic;parsing;parser absolute 2015;parser;2015 2016 semantic;2016 semantic;performance existing neural", "pdf_keywords": ""}, "89d15c9de3608157ff746af7368556149b50e037": {"ta_keywords": "headline generation language;headlines language modeling;language models improve;words language modeling;language modeling;headline generation;language models;based headlines language;enables language models;language models work;language modeling downstream;language modeling paper;headlines language;language modeling methods;textual context sememes;sememes minimum semantic;generation headline based;generation language modeling;robustness language models;tune language models;application headline generation;language models argue;atomic semantic;headline based headlines;sememe level semantics;word level manipulation;necessarily atomic semantic;atomic semantic units;headline based;semantics words language", "pdf_keywords": "neural language modeling;neural language;language models;new language modeling;cnnns sememes;networks cnnns sememes;language modeling downstream;sememesense word generation;language modeling;language models recent;language modeling architectures;language modeling architecture;softmax;softmax models automatic;cnnns sememes naturally;easy use softmax;architectures simple lstm;softmax models;lstm cells easy;build linguistic;tasks machine translation;lstm;robustness language models;experiments language modeling;machine translation;simple lstm cells;simple lstm;advances rnns;annotate lexical sememes;word generation"}, "9dbd86f089c2132dc46d316750d9786d60d5d720": {"ta_keywords": "taggers annotated data;tag taggers annotated;taggers annotated;web based annotation;based annotation tool;annotation tool;annotated data;annotation process;approach tag taggers;new approach annotation;annotation;annotation process allows;annotated data present;based annotation;manual annotation;annotation historical;tag taggers;manual annotation historical;annotation historical nonstandard;annotation tool manual;approach annotation process;approach annotation;annotated;taggers;tool manual annotation;language data;nonstandard language data;language data present;tag;evaluation token boundaries", "pdf_keywords": ""}, "6f1ca0249eafa36a5762ac53f6ba2a4ee2133456": {"ta_keywords": "giga speech performance;performance giga speech;speech performance improved;speech performance;speech recognition corpus;audio unsupervised training;giga speech;000 hours audio;speech recognition;english speech recognition;audio suitable supervised;dubbed audiobooks transcribe;hours audio;dubbed audiobooks;hours audio variety;audio;performance giga;hour total audio;analysis performance giga;labeled audio;audio streaming;audio unsupervised;labeled audio suitable;services dubbed audiobooks;quality labeled audio;audiobooks transcribe;total audio unsupervised;corpus;recognition corpus;recognition corpus 10", "pdf_keywords": "speech recognition toolkits;domain speech recognition;generating speech recognition;speech recognition corpus;multi domain speech;speech recognition emerging;speech recognition corpora;domain speech;machine learning speech;automatic speech recognition;automatic speech;learning speech recognition;tasks automatic speech;pipeline generating speech;speech recognition provide;speech recognition speech;speech recognition;pika automatic speech;creating gigaspeech corpus;recognition speech;recognition speech recognition;generating speech;train domain acoustic;popular speech recognition;speech recognition asr;gigaspeech corpus;domain acoustic;audio suitable supervised;domain acoustic model;learning speech"}, "22b7a7c9faa8f340520ae1418c9cf8d960aaeec0": {"ta_keywords": "neuro symbolic reasoner;symbolic reasoning systems;models symbolic reasoning;neural symbolic;neuro symbolic knowledge;symbolic knowledge based;symbolic reasoning;symbolic reasoner closely;symbolic reasoner;neuro symbolic;neural symbolic methods;language models symbolic;symbolic knowledge;knowledge based reasoning;symbolic reasoning used;logics computationally;reasoning systems;reasoning systems based;barrier neural symbolic;paper symbolic reasoning;logics computationally powerful;reasoning based;reasoning based set;neural language;based reasoning based;based reasoning;modern neural language;models symbolic;symbolic;reasoner closely integrated", "pdf_keywords": ""}, "eeec05fc11b2e0b40b3b0800bc50930e240cafeb": {"ta_keywords": "prerequisite structure corpus;textual information sources;textual information source;structure textual corpora;textual corpora;textual information;textual corpora analyzing;domain textual information;structure corpus;structure textual;textual;analyze structure textual;corpora analyzing;textual description;prerequisite structure generalizing;provides complete textual;domain textual;corpus;containing corpora;open domain textual;structure corpus experimental;complete textual description;complete textual;textual description open;corpora;predicting prerequisites;method predicting prerequisites;structure generalizing labels;information sources;prerequisite structure", "pdf_keywords": ""}, "525b7f73744f5650391be4678d6d51ddaf23ed72": {"ta_keywords": "nonlinearities statistics differential;errors model;nonlinearity nonlinearities statistics;nonlinearities statistics;nonlinearities;orthogonal errors model;nonlinearity;relationship nonlinearity nonlinearities;investigate relationship nonlinearity;relationship nonlinearity;nonlinearity nonlinearities;nonlinear;mistakes particular;errors model standard;orthogonal errors;mistakes trip;mistakes woman trip;particular number mistakes;non orthogonal errors;mistakes trip remote;statistics differential equations;numbers mistakes trip;linear nonlinear;number mistakes particular;non linear nonlinear;non linear;mistakes particular item;statistics differential;mistakes;number mistakes", "pdf_keywords": ""}, "d338bcd1e34a8259e123465203b05c5bf21aa12a": {"ta_keywords": "erj speech recognition;model erj speech;erj synthetic speech;speech synthesized erj;prosody erj voices;erj voices based;speech preserving speaker;erj acoustic model;acoustic model erj;english speech preserving;erj speech;erj voices;preserving speaker individuality;synthesized erj acoustic;speech preserving;erj acoustic;correct prosody erj;synthetic speech preserving;english speech synthesized;model speaker dependent;prosody erj;preserving speaker;speaker dependent acoustic;synthetic speech;individual japanese speakers;speech synthesized;speech recognition;framework model speaker;model speaker;speech recognition problem", "pdf_keywords": ""}, "095bc69eddbf73fabf58a929d2be9a99c1b533a6": {"ta_keywords": "preference reasoning techniques;preference reasoning establishing;preference reasoning;recommendation systems;applicability preference reasoning;recommendation systems kidney;library preferences;preflib library preferences;library preferences new;preferences;computational social choice;preferences higher;preferences higher hope;preferences new online;preferences new;preflib library;preference;facets preference reasoning;data based simple;preflib preflib library;computational social;library;data based;ranging recommendation systems;exchanges preferences;high quality data;library invite community;computer emergence;resource online shopping;exchanges preferences higher", "pdf_keywords": ""}, "85099e075880a4844f3de77006a80c73daf99a4c": {"ta_keywords": "surface realisation engine;word ordering phenomena;describes surface realisation;surface realisation;word order phenomena;word ordering;realisation engine german;realisation engine;focus word ordering;word order;syntax german implementation;describes surface;ordering phenomena features;paper describes surface;gravitational coverage;surface;gravitational coverage demonstrated;german based simplenl;german implementation;ordering phenomena;phenomena features syntax;syntax german;features syntax german;order phenomena;order phenomena discussed;gravitational;based simplenl aspects;realisation;based simplenl;simplenl aspects word", "pdf_keywords": ""}, "f7ce4c7ec30c846cc122393deee98f1eacd24049": {"ta_keywords": "recurrent neural network;dialogue state tracker;short term memory;neural network rnn;long term memory;networks paper recurrent;memory neural networks;recurrent neural;network rnn;representations word embedding;dialogue state;word embedding;memory networks;embedding propose dialogue;memory neural;rnn used predict;paper recurrent neural;network rnn used;term memory lrm;embeddings words;word representations;utterances dialogue;rnn;dialogue participants input;outputs dialogue state;embeddings words word;term memory neural;utterances dialogue participants;approach embeddings words;dialogue state current", "pdf_keywords": ""}, "bd24b47165407a8b2d32016645ca71f7c9213636": {"ta_keywords": "classify email verbnoun;learning classify email;useful classify email;classify email;classify email according;email verbnoun;email verbnoun pairs;email speech;email speech acts;text classification;typical email speech;approach text classification;classification based text;existing text classification;text based categorization;text classification methods;text classification based;task tracking;describing typical email;information speechanical demonstrations;deliver information speechanical;task tracking tools;information speechanical;automatic classification;text based;useful classify;learning classify;email according intent;automatic classification taxonomy;classification", "pdf_keywords": ""}, "b54d49cdf57abf7f7e7bcc0e946f450fd807f829": {"ta_keywords": "inverse reinforcement learning;inverse reinforcement;expert agent demonstrations;expert demonstrations approaches;problem inverse reinforcement;demonstrations given knowledge;expert demonstrations;knowledge expert demonstrations;agent demonstrations given;agent demonstrations;likelihood observing demonstrations;reinforcement learning irl;demonstrations approaches problem;reinforcement learning;demonstrations approaches;observing demonstrations;estimating reward;estimating reward function;demonstrations present algorithm;likelihood expert agent;expert agent policy;reinforcement;behavior control task;agents structured environment;action feature constraints;agent policy demonstrated;expert agent;structured environment seek;observing demonstrations present;explains expert agent", "pdf_keywords": "learning constraints demonstrations;learning constraints;constraint learning;constraint learning irl;constraints demonstrations;constraints demonstrations important;constraint inference fundamental;constraint inference;constraints soft reward;work constraint learning;action feature constraints;constraints directly;learning straints demonstrations;cation learning constraints;penalties constraint inference;feature constraints directly;constraints;expressed hard constraints;constraints soft;approach reinforcement learning;hard constraints;constraint sets maximize;constraints arise;reinforcement learning;feature constraints;maximize probability agent;hard constraints soft;constraints arise safety;constraint best;approach reinforcement"}, "1a2410823486613e327892f05b38d3070f2d712c": {"ta_keywords": "class random networks;random networks;algorithm compute eigenvalues;networks;compute eigenvalues eigenvectors;eigenvalues eigenvectors large;eigenvectors large class;eigenvectors large;large class random;compute eigenvalues;eigenvectors;eigenvalues eigenvectors;efficient algorithm;simple efficient algorithm;efficient algorithm compute;class random;eigenvalues;algorithm;random;simple efficient;algorithm compute;large class;efficient;present simple efficient;class;paper;compute;paper present simple;paper present;large", "pdf_keywords": ""}, "3bb1e24eb3429f807397833105d1e137d9927767": {"ta_keywords": "sequence labeling tasks;active sequence labeling;automatic sequence labeling;sequence labeling;sequence labeling methods;perform sequence labeling;sequence labeling uses;annotations perform sequence;sequence labeling method;leveraging human annotations;resource sequence labeling;extra labeled sequences;labeling tasks;generating extra labeled;entityrecognition event detection;human annotations perform;labeled sequences;human annotations;annotations propose simple;labeled sequences iteration;experiments named entityrecognition;human annotations propose;annotations perform;annotations propose;annotations;samples active learning;improve label efficiency;automatic sequence;labeling;labeling tasks experiments", "pdf_keywords": "active sequence labeling;sequence labeling tasks;sequence labeling datasets;sequence labeling challenging;sequence labeling;sequence labeling low;sequence label generation;entity recognition ner;framework sequence labeling;sequence label;deep active learning;experiments sequence labeling;named entity recognition;technique sequence labeling;entity recognition;sequences labels based;model sequence label;sequences labels;semi supervised deep;sub sequences labels;labeling tasks;deep neural sequential;propose deep active;generate sentences token;supervised deep;mixup active sequence;labeling challenging task;deep active;labeling challenging;active sequence"}, "17c9a0f1a287c08bb2c1c1df47fa51ce1e428c4e": {"ta_keywords": "backend speech recognition;microphone speech recognition;multi microphone speech;based multi microphone;multi microphone;speech recognition;microphone speech;speech recognition based;microphone;recognition based multi;backend speech;new backend speech;backend;new backend;recognition based;speech;based multi;recognition;presents new backend;multi;paper presents;paper presents new;paper;new;presents new;presents;based", "pdf_keywords": ""}, "a064010cf6fe594b2506a8fecd16dc0040211daa": {"ta_keywords": "multilingual encoder;multilingual transfer nmt;purpose multilingual encoder;multilingual transfer;multilingual encoder paper;machine translation low;translation low resource;multilingual encoding;neural machine translation;effectiveness multilingual transfer;propose multilingual encoding;multilingual encoding method;multilingual;multilingual data beneficial;machine translation;low resource languages;languages multilingual;multilingual data;nmt models translate;languages multilingual data;resource languages multilingual;translation low;models translate lrl;decoder word embedding;improve effectiveness multilingual;general purpose multilingual;propose multilingual;effectiveness multilingual;models translate;translate lrl", "pdf_keywords": "multilingual neural machine;word embedding;gram based embedding;character sensitive embedding;neural machine translation;multilingual neural;translation soft decoupled;machine translation soft;word embedding method;decoder improves translation;multilingual transfer target;multilingual transfer;multilingual character sensitive;improves translation accuracy;propose word embedding;limiting multilingual transfer;embedding method decoder;framework multilingual neural;translation accuracy low;character gram based;multilingual character;translation soft;hrls similar representations;words hrls similar;translation accuracy;machine translation;soft decoupled encoding;demonstrate multilingual character;ef\ufb01cient character gram;improves translation"}, "2fbb75d7947808698f1554e4d400ec5ecb5ef998": {"ta_keywords": "improving answer selection;answer selection task;answer selection model;answer selection;answer selection long;adapted answer selection;dataset answer selection;reading comprehension dataset;reading comprehension document;reading comprehension;choice reading comprehension;span prediction;predictions portions documents;attention multiple choice;performing span prediction;span prediction models;comprehension dataset;span prediction model;long summaries;performance long summaries;applied span prediction;selection long documents;attention multiple;narrativeqa reading comprehension;summaries narrativeqa reading;comprehension dataset answer;multiple choice reading;long summaries narrativeqa;direct attention multiple;comprehension document", "pdf_keywords": "reading comprehension models;reading comprehension tasks;reading comprehension based;realistic reading comprehension;comprehension models;current reading comprehension;comprehension models present;comprehension tasks;improving answer selection;reading comprehension;choice reading comprehension;answer selection task;models answer selection;performance reading comprehension;comprehension based multiple;answer selection model;comprehension models answer;answer selection;comprehension tasks require;answer selection long;summaries long answers;comprehension based;reading comprehension rc;long answers questions;predictions portions documents;comprehension rc models;multiple choice reading;long documents summaries;questions occur summaries;answering questions longer"}, "c7c93601b52b1bcc68ec1f8b2c77c54f1b358ab9": {"ta_keywords": "pairwise comparison models;similarity models crowdsourcing;comparison models pairwise;pairwise comparison data;pairwise similarity models;pairwise comparison;models pairwise similarity;pairwise similarity;test pairwise comparison;relationship pairwise comparison;comparison models;models crowdsourcing;models crowdsourcing long;ratings converted comparisons;comparison data;crowdsourcing long;models pairwise;twosample test pairwise;distributed similar ratings;comparisons;crowdsourcing;similarity models;crowdsourcing long standing;test pairwise;similar ratings converted;study pairwise;comparison;comparison data provided;question comparison data;converted comparisons", "pdf_keywords": ""}, "e59adee86b666ad76164b3446cfee5068a15e5c9": {"ta_keywords": "fault tolerance nns;fault tolerance abft;deep learning;fault tolerance algorithm;soft errors spacecraft;nn layer neural;deep learning experiment;neural network architecture;layer neural networks;neural network;approach fault tolerance;based fault tolerance;neural networks;fault tolerance;algorithm based fault;neural networks increasingly;outcome deep learning;learn noisy data;layer neural;proving fault tolerance;errors spacecraft;performance neural network;nn layer;prone unreliability spacecraft;tolerance nns;performance neural;set soft errors;architecture able learn;neural network predicting;able learn noisy", "pdf_keywords": "inferenceoptimized gpus;inferenceoptimized gpus particular;increase inferenceoptimized gpus;nn inference gpus;inference gpus;fault model forvidiavidia;gpu;gpus;particular gpu;faults tolerance nn;headcount particular gpu;gpus particular;gpus particular unique;inference gpus lead;faults tolerance;hardened redundant processing;based faults tolerance;abft_ tensor cores;imparting fault tolerance;fault tolerance abft1;gpu paper;gpu paper present;particular gpu paper;tensor cores;model forvidiavidia;forvidiavidia;tolerance nn inference;fault tolerance linear;tensor cores radiation;tolerance linear algebra"}, "e6fa88f4af68aa7be4ae91940892eee52571997c": {"ta_keywords": "shot object detection;detecting rare objects;crucial shot object;classes crucial shot;meta learning promising;outperforms meta learning;little shot object;shot object;detectors rare classes;object detection task;benchmarking simple little;meta learning;object detection;task meta learning;new benchmarks;meta learning methods;approach detecting rare;benchmarking simple;rare objects examples;rare objects;detecting rare;benchmarking;simple little shot;detectors rare;benchmarks fine tuning;current benchmarks;crucial shot;benchmarks doubles accuracy;benchmarks;existing detectors rare", "pdf_keywords": "meta cnn;meta cnn class;fsrw meta cnn;shot object detection;meta learning;shot image dataset;shot object;present meta learning;cnn class;cnn;shot image;meta learning based;new benchmark datasets;shot train wild;cnn class speci\ufb01c;meta learner;object detection;performance shot train;shot train;approach shot object;benchmark datasets;approach shot image;stage meta learner;object detection introduce;performance shot;base object detection;object detection model;approach shot;new benchmark;contrast shot object"}, "db0a3ce9f315f650fe5220101c5677778de39fee": {"ta_keywords": "machine translation reordering;phrase based preordering;parser machine translation;discriminative parser machine;translation reordering using;learning discriminative parser;unsupervised syntax induction;translation reordering;discriminative parser;syntax induction;machine translation;accuracy factored parse;factored parse trees;parser machine;parser;propose unsupervised syntax;parse trees;syntax induction method;unsupervised syntax;parse;reordering using;learning discriminative;reordering accuracy;factored parse;parallel text;method learning discriminative;aligned parallel text;reordering accuracy factored;parallel text present;measures reordering accuracy", "pdf_keywords": ""}, "06f4de06fc37576e1e381cd76e375d57852047b9": {"ta_keywords": "translation multi task;neural machine translation;machine translation multi;machine translation achieves;machine translation;translation achieves remarkable;translation multi;multi task learning;transformer trained;shared task french;text neural machine;text neural;clean text neural;transformer trained clean;english dataset higher;learning algorithm transformer;translation achieves;task learning;task french english;vanilla transformer trained;task french;french english dataset;trained clean text;text performance known;robustness neural machine;robustness neural;transformer;multi task;text performance;domain text performance", "pdf_keywords": ""}, "cd3595f65519e4af6bcd073790ac32acdafadf55": {"ta_keywords": "shot image generation;domain adapting appearance;adapting appearance target;domain shot image;target domain shot;image generation;adapting appearance;pretraining human faces;source domain adapting;domain shot;image generation aims;domain adapting;source domain pretraining;domain pretraining human;shot image;domain pretraining;video motion body;fluid dynamics video;appearance target analyze;domain available training;human faces;faces fluid dynamics;dynamics video;training examples;video motion;adapting;adapting diversity source;motion body action;acting body;body action external", "pdf_keywords": "generative style transfer;shot generative learning;shot generative;propose shot generative;domain generative adversarial;generative adversarial networks;networks gans;networks gans deep;generative models learn;generative adversarial;convolutional generative models;deep convolutional generative;gans deep;gans deep convolutional;shot image generation;target domain generative;generative style;generative learning;convolutional generative;domain generative models;generative;generative models currently;adversarial networks gans;generative models;domain generative;adaptation propose shot;propose generative style;generative learning method;gans;style transfer"}, "6887537de3655a25c75bf4d0833f51e72331bdad": {"ta_keywords": "audio signal phase;phase signal audio;audio signal detecting;detect phase signal;enhanced audio signal;detecting noise signal;signal audio signal;signal extended audio;detect phase;signal detecting noise;detecting noise;audio signal converted;extended audio signal;audio signal;audio signal presented;method detect phase;signal phase signal;signal estimated phase;noisy audio signal;signal audio;phase signal estimated;signal detecting;phase signal;signal phase;enhanced audio;converted enhanced audio;noise signal environment;phase difference signal;extended audio;converting noisy signal", "pdf_keywords": ""}, "a8fc183c089bd596ccc48b3d666f8814e1b41e55": {"ta_keywords": "large generative code;generative code model;generative code;program synthesis;inference comment generation;program synthesis left;trained generate code;program synthesis benchmarks;perform program synthesis;standard program synthesis;comment generation;generate code;generation editing infilling;generation editing;large generative;generate code files;incoder trained generate;generative;trained generate;right generation editing;incoder unified generative;model large generative;comment generation variable;unified generative;code model;code model able;generative model perform;synthesis benchmarks;type inference comment;synthesis benchmarks comparison", "pdf_keywords": "learning program synthesis;program synthesis editing;program synthesis generating;program synthesis;programmers present generative;automatic generation language;generative language;generative languages;generative languages bidirectional;right generative languages;generative language model;shot program synthesis;learning code;machine learning code;approach program synthesis;generation language models;synthesis editing;present generative language;learning code assistance;generation comment generation;language model generates;synthesis editing present;automatic generation;generation language;comment generation;generative;aid human programmers;human programmers;model automatic generation;language models recently"}, "9a43dda4b01dde5d513c431564098e4d8794a7a5": {"ta_keywords": "large sentiment datasets;sentiment datasets;word cluster features;occurrence sentiment analysis;word occurrence sentiment;sentiment datasets outperforms;document level sentiment;sentiment analysis;method large sentiment;word cluster;level sentiment analysis;sentiment analysis article;sentiment analysis task;large sentiment;occurrence sentiment;new semi supervised;semi supervised method;semi supervised;clusters words;clusters words represented;adding word cluster;level sentiment;cluster features;cluster features features;exploit clusters words;semantic spaces based;sentiment;semantic spaces;represented semantic spaces;semantic spaces computed", "pdf_keywords": ""}, "2f201c77e7ccdf1f37115e16accac3486a65c03d": {"ta_keywords": "networks called stochastic;sde guard adversarial;adversarial;guard adversarial;adversarial examples;guard adversarial examples;adversarial examples inspiration;reliability deep learning;game adversary model;noise network;networks presence noise;pretrained networks;dynamics complex networks;noise network dynamics;stochastic;adversary model;game adversary;adversary model propose;random walks;learning systems wild;complex networks;networks;called stochastic;stochastic differential;sum game adversary;attacks improving accuracy;described stochastic;random walks propose;dynamics described stochastic;applied pretrained networks", "pdf_keywords": "training adversarial examples;adversarial examples learn;training adversarial;trained adversarial examples;adversarial training;trained adversarial;adversarial training method;adversarial examples created;adversarial examples;networks adversarial examples;ensemble adversarial training;adversarial examples paper;adversarial;introduces ensemble adversarial;method training adversarial;neural networks adversarial;learn robustness adversarial;adversarial attacks;ensemble adversarial;network trained adversarial;robustness adversarial attacks;networks adversarial;robustness adversarial;adversarial attacks paper;trained models image;trained models;examples learn robustness;stochastic activations sampling;performance deep;deep residual learning"}, "136235d2a3dc4f1c995eaf977aec9c42114da850": {"ta_keywords": "task morphological reinflection;shared task morphological;crosslingual variation morphosyntactic;task morphological;morphosyntactic features;morphological reinflection;morphosyntactic features report;morphological reinflection focuses;morphological;inflect multiword lemmas;crosslingual variation;variation morphosyntactic features;recognition human forms;language families;13 language families;diversity crosslingual variation;multiword lemmas conduct;multiword lemmas;typological diversity crosslingual;languages;sigmorphon shared task;languages achieving;iteration sigmorphon;languages achieving 90;language families resourced;model automatic recognition;lemmas data predictions;human forms;iteration sigmorphon shared;performance majority languages", "pdf_keywords": ""}, "8a09c90f6e9a3f6c3b172e5059c7af47f528f66b": {"ta_keywords": "text visually appealing;text visually;learn latent space;text;make text visually;context natural language;make text;emergence creative context;learn latent;emergence creative;natural language;creative context;represent letters;visually appealing memorable;represent letters cliparts;model emergence creative;latent space represent;approach learn latent;creative context context;use visual cues;approach make text;space represent letters;reinforce message approach;unsupervised approach learn;learning winner;reliably recognize word;latent space;natural language processing;theme outputs creative;context theme word", "pdf_keywords": "letter clipart imagery;visualizing letters cliparts;word theme generative;theme generative adversarial;word theme recognition;architecture visualizing letters;visualizing letters;learn representation visually;letters cliparts figure;similar letter clipart;clipart imagery;letter clipart;visual representation text;appearance letter clipart;visual representation;recognition speci words;reinforcement word typography;theme generative;representation visually similar;learn representation;word theme education;clipart imagery present;representation visually;letters cliparts;generative;able learn representation;visually similar letter;generative adversarial nets;typography given word;word typography"}, "1022696090666eab5c82ebc07d63c0de2fca2521": {"ta_keywords": "similarity textual identifiers;inductive classification world;classification systems similarity;textual identifiers fastest;similarity based reasoning;joins based similarity;identifiers fastest;inductive classification;classification world;inductive classification systems;world wide web;classification world wide;identifiers fastest way;similarity based;nearest neighbor generalization;based similarity;classification;based similarity textual;data world wide;soft joins based;nearest neighbor;relational database;data world;classification systems;mature inductive classification;similarity textual;textual identifiers;evaluates inductive classification;soft joins;web using data", "pdf_keywords": ""}, "71124b00b873e85aa55b07100cd5b492e5b1d73d": {"ta_keywords": "integrity protection scheme;integrity message authentication;integrity protection data;propose integrity protection;integrity protection;authenticated secret sharing;secure distributed storage;data integrity;achieve data integrity;data integrity certification;data integrity message;secret sharing;data owner secret;integrity check data;authenticated secret;shamirs secret sharing;owner secret sharing;stored trusted;authentication code mac;protection data realized;secure distributed;items stored trusted;authentication;authentication code;universal2 hash;integrity certification data;password authenticated secret;message authentication code;message authentication;protection data", "pdf_keywords": "authentication data integrity;verifiable secret sharing;secure data transmission;ensures data integrity;data integrity party;storage authentication data;based verifiable secret;storage authentication;ensure data integrity;transmission storage authentication;data integrity;secure data;theoretically secure data;data integrity certain;hiding distributed data;data integrity end;secret sharing scheme;distributed data backup;verifiable secret;sharing scheme ensures;protection integrity authenticity;data transmission storage;authentication data;distributed storage information;function based verifiable;secret sharing;scheme data integrity;protection integrity;distributed storage;sharing scheme"}, "eb7a64195ef4a268f79fa6740f128387f2696c65": {"ta_keywords": "agents maximize reward;reward environment learn;maximize reward environment;chaotic oscillators ensure;reward environment;reinforcement;reinforcement learning maximize;demonstrations reinforcement;actions reward maximizing;chaotic oscillators;learning maximize environmental;coupled chaotic oscillators;contextual bandit orchestrator;reward maximizing;demonstrations reinforcement learning;ensemble coupled chaotic;agent able learn;reinforcement learning;agents maximize;ways autonomous cyber;constraints demonstrations reinforcement;cyber physical agents;allow agents maximize;maximize reward;autonomous cyber;reward maximizing constrained;environmental rewards agent;maximize environmental rewards;chaotic;bandit orchestrator allows", "pdf_keywords": ""}, "6cddfbed35c46937588bd9d6b846ca2855953cea": {"ta_keywords": "attentional encoder;speech translation lattices;encoder attentional;attentional encoder decoder;swimming sequence work;swimming sequence;outcome swimming sequence;sequences posterior probabilities;encoder attentional encoder;segmenter speech;neural sequence sequence;used encoder attentional;word segmenter speech;neural sequence;bias term attention;tree language encoder;predict outcome swimming;segmenter speech tagger;animals word segmenter;language encoder tree;tagger speech recognizer;posterior probabilities stream;probabilities stream models;speech recognizer;language encoder;scores neural sequence;speech recognizer segmenter;model dynamics swimming;attention mechanism;alternative sequences posterior", "pdf_keywords": "labeled lattice encoders;lattice encoders conditioned;trained lattice;lattice encoders;translation performed lattice;trained lattice classi\ufb01cation;lattice tosequence model;lat2seq trained lattice;attentional encoder decoder;lattice tosequence;propose lattice tosequence;attentional encoder;machine translation;labeled lattice;lattice classi\ufb01cation task;data machine translation;handling lattices considerable;handling lattices;lattice data;lattice scores model;decoder models;lattice scores;work attentional encoder;performed lattice data;encoder decoder models;lattice data postulate;lattice;machine translation performed;trained sequential data;edge labeled lattice"}, "58a2e825884bc86e650fffafb86a2833117852c5": {"ta_keywords": "predict position body;body mind ranking;pre trained models;predicting location body;trained models ptms;pre trained model;tuned predict position;predict position;body model;body body model;trained models;trained model;position body;deep learning;cornerstone deep learning;pt machine tuning;position body body;tuned predict;fine tuned predict;trained model hubs;tuning heterogeneous pt;body mind;body body mind;body model fine;heterogeneous pt machines;machine tuning;location body body;tuning weight model;train pre trained;tune pt machine", "pdf_keywords": "learning transferability;transfer learning;model deep transfer;deep transfer;learning transferability important;deep transfer learning;transfer learning based;pre trained models;visual models training;transfer pre trained;trained model deep;trained models;pre trained model;trained models learning;generative visual models;trained model;tuning pre trained;learning generative visual;models learning generative;model deep;models training;generative visual;learning generative;visual models;models learning;models training examples;trained models experiments;transferability important task;important machine learning;pre trained"}, "e6cec3044688f1701b4b72b4b2189f215abc3759": {"ta_keywords": "crowdsourced evaluation tasks;crowdsourced evaluation;performance crowdsourced evaluation;crowdsourcing types evaluation;tasks crowdsourcing;performance crowdsourced;similar tasks crowdsourcing;crowdsourced;evaluate performance crowdsourced;crowdsourcing;tasks crowdsourcing types;crowdsourcing types;evaluation tasks propose;tasks like labeling;evaluation tasks like;evaluation tasks;tasks propose reward;generate truthful;like labeling;propose reward mechanisms;like labeling images;labeling images grading;truthful output agreement;generate truthful output;labeling;knowledge underlying generating;types evaluation tasks;used generate truthful;images grading assignments;truthful output", "pdf_keywords": ""}, "549df5fc83c382cbdf633dc782fa67bf2f983f2c": {"ta_keywords": "train nonlinear mixtures;nonlinear neural networks;nonlinear neural;nonlinear mixtures;use nonlinear neural;nonlinear mixtures binary;networks train nonlinear;neural networks linear;harmonic oscillators deep;oscillators deep neural;train nonlinear;nonlinear;networks linear activations;use nonlinear;harmonic oscillators;oscillators deep;damped harmonic oscillators;demonstrate use nonlinear;neural networks train;damped harmonic;binary mixtures propose;neural networks;networks linear;deep neural networks;deep neural;linear activations;linear activations used;binary mixtures;dynamics;mixtures binary", "pdf_keywords": ""}, "f61886d138497431cbeaa7bb73051bfb7a745026": {"ta_keywords": "scene graph generation;context scene graph;objects context contextual;contextual context scene;contextual supervision;contextual contextual;context contextual contextual;holistic contextual supervision;context contextual;linguistic structures captions;contextual supervision intuitively;contextual;captions benefit scene;scene graph;contextual contextual contextual;captions relations;contextual contextual context;contextual context;structures captions;holistic contextual;captions benefit;captions relations individual;task holistic contextual;context subjects objects;structures captions benefit;context scene;captions;multimodal;multimodal data;approach scene graph", "pdf_keywords": "scene graphs supervision;scene graph generation;annotating video captions;crowdsourced image annotations;automatically annotating video;generating scene graphs;image level supervision;annotating video;captions training;image annotations extensive;scene graphs;scenes based crowdsourced;scene graph;reasoning visual scenes;image annotations;approach scene graph;video captions;video captions text;automatically annotating;crowdsourced image;leveraging phrasal sequential;supervision form captions;guide scene graph;captions text accompanying;generating scene;graphs supervision;vision language module;visual scenes;annotating;trained phrasal structure"}, "275aaa20ba853c40a461f224eefbf06730bf03a9": {"ta_keywords": "confined optical lattice;random walk algorithm;point random walk;optical lattice;stochastic algorithm;stochastic algorithm compute;simple gradient based;gradient based algorithm;optical lattice paper;walk algorithm;rn gradient function;dynamics single particle;stationary point random;function rn gradient;minimized simple gradient;gradient descent;particle confined optical;rn gradient;simple gradient descent;lattice paper propose;gradient function minimized;random walk;propose simple gradient;walk algorithm polynomial;gradient descent based;algorithm smooth function;propose stochastic algorithm;gradient based;single particle confined;algorithm smooth", "pdf_keywords": "learning algorithm nonconvex;stochastic gradient descent;gradient descent;negative curvature algorithm;gradient descent algorithm;algorithm nonconvex loss;algorithm escaping saddle;nonconvex loss functions;algorithms escaping saddle;based gradient descent;stochastic gradient;gradient loss function;algorithms based gradient;algorithm nonconvex;gradient descent shown;curvature algorithm algorithms;curvature \ufb01nding algorithm;gradient loss;curvature algorithm;gradient based algorithm;descent algorithm;descent algorithm escaping;present stochastic gradient;robust gradient;loop robust gradient;hessian;hessian vector;based negative curvature;hessian vector product;using negative curvature"}, "e6924d247b56980260e4c68dbc51b947409e4764": {"ta_keywords": "particle action gravity;particle moves;particle moves gravitational;particle motion;local fpu dynamics;charged particle action;motion particle described;ulam fpu model;fpu dynamics;fermi pasta ulam;particle action;motion particle;motion charged particle;particle motion particle;fpu dynamics study;pasta ulam fpu;case particle moves;performance fermi pasta;gravity field manipulated;motion gravity field;performance fermi;brownian motion gravity;motion gravity;motion charged;moves gravitational field;particle described;particle;charged particle;action gravity field;fermi pasta", "pdf_keywords": "gradient methods convex;convex programming;local gradient minimizes;optimal local gradient;solving convex programming;convex programming problems;local gradient methods;local stochastic decoupling;gradient minimizes;methods convex strongly;stochastic decoupling;propose local stochastic;gradient methods;distributed federated training;analyzing local gradient;stochastic decoupling method;methods convex;local gradient;gradient minimizes number;local stochastic;decoupling method minimizing;method solving convex;learning models;machine learning models;federated training supervised;minimizes;convex strongly convex;optimal local;supervised machine learning;convex regimes distributed"}, "a1fc0041ef89ed5371317c8e2cc5effa8f38ae48": {"ta_keywords": "bayesian network structure;network structure learning;bayesian network;causal discovery;research bayesian network;search local clusters;large graphs;causal discovery methods;scale large graphs;nodes;accuracy causal discovery;structure learning;local search strategy;network structure;local clusters;large graphs numerical;neighbors hops superstructure;hundreds nodes;discovery methods rely;scales hundreds nodes;discovery methods;structure learning focuses;hundreds nodes high;variable neighbors hops;clusters;assumption exact search;local clusters formed;nodes high;discovery;variable neighbors", "pdf_keywords": "causal discovery observational;causal discovery;distributions causal discovery;causal discovery relying;methods causal discovery;assumption causal inference;consider causal discovery;causal inference;method causal inference;causal inference presence;causal inference based;estimation method causal;assumption causal;methods causal;based methods causal;faithfulness assumption causal;probability distributions causal;method causal;causal;distributions causal;variables selection bias;discovery observational data;paper consider causal;inference presence latent;consider causal;discovery observational;inverse covariance estimation;super structure estimation;inference based faithfulness;dag inverse covariance"}, "0a227a21172f7344ad911aeefc40ae4ec82d7cac": {"ta_keywords": "statistical metaphor processing;metaphor modeling nlp;metaphor using nlp;study metaphor nlp;metaphor modeling;metaphor nlp;new metaphor modeling;metaphor modeling approach;metaphor processing approaches;metaphor processing approach;metaphor processing;modelling metaphor using;problem metaphor modeling;metaphor research;proposes statistical metaphor;modelling metaphor;metaphor research workshop;metaphor using;results study metaphor;metaphor important research;makes metaphor;language case metaphor;case metaphor research;metaphor;cognitive linguistics automatic;makes metaphor important;study metaphor;statistical metaphor;cognitive linguistics;metaphor important", "pdf_keywords": ""}, "178f424d0f156cbf5b35eb241fc00b27a0a3808b": {"ta_keywords": "term memory lstm;modeling speech recognition;speech recognition;short term memory;speech separation recognition;combination enhanced speech;automatic speech recognition;speech recognition achieved;automatic speech;chime speech separation;effective modeling speech;memory lstm;modeling speech;memory lstm recurrent;enhancement automatic speech;memory recurrent neural;term memory recurrent;approach speech recognition;speech recognition based;enhanced speech;recurrent neural networks;recurrent neural network;lstm;term memory propose;speech enhancement automatic;lstm recurrent neural;term memory;lstm recurrent;memory recurrent;chime speech", "pdf_keywords": ""}, "317ed59456d76b500a7eb63b181df9e8b795976b": {"ta_keywords": "urban parking dynamics;queuing game model;parking dynamics;queueing game;queuing game;queueing game drivers;concept queuing game;model parking urban;models traffic flow;framework urban parking;explore queueing game;traffic flow driven;simulated models traffic;parking dynamics based;parking urban;parking urban centers;traffic flow;urban parking;neighbors model parking;queues network;models traffic;queues overlay game;queueing rules;queueing;parking;based queueing;traffic;queue;based queueing rules;queues", "pdf_keywords": "congestion occupancy;congestion occupancy relationship;congestion;nash equilibrium;game nash equilibrium;queue \ufb02ow network;socially optimal user;nash equilibrium far;equilibrium socially ef\ufb01cient;network model games;user selected equilibrium;social optimal;parking optimizes social;queue \ufb02ow;equilibrium socially;queue;equilibrium cases socially;socially optimal solution;selected equilibrium socially;equilibrium particular information;socially optimal;information price parking;present queue \ufb02ow;equilibrium far social;social optimal especially;price parking optimizes;parking optimizes;information limited game;\ufb02ow network model;network model"}, "56823e326f2515f73662b176054fbee0895e0c44": {"ta_keywords": "request assistant;usefulness assistant users;assistant navigation;usefulness assistant;web forms;request specialist expertise;assistant users;handle request assistant;update request specialist;demonstrate usefulness assistant;web services;request specialist;web based service;expertise handle request;based service web;service web;requests users;request assistant deployed;service web services;describes web based;web forms large;requests;web services used;thousands forms navigate;describes web;forms navigate;building assistant navigation;paper describes web;assistant;assistant users forced", "pdf_keywords": ""}, "7891ec1d8ba2abf238326dc6e8862cc4431a6f5c": {"ta_keywords": "heuristic placing relay;impromptu relay placement;relay placement;wireless network deployment;radio propagation;sequential placement policies;lattice path markov;optimal sequential placement;radio propagation possible;placing relay distance;relay placement problem;random lattice path;model impromptu relay;deployment multihop wireless;path markov evolution;path markov;placement policies structure;network deployment operative;relay distance;relay distance previously;network deployment;impromptu relay;placed relay;propagation possible;sequential placement;cost markov decision;placing relay;propagation;impromptu deployment multihop;cost markov", "pdf_keywords": ""}, "cdf5eb63e9c2434073e811aba50ae80ede9d15f6": {"ta_keywords": "crowdsourcing clue webpages;retrieval web pages;approach retrieval web;retrieval web;documents based crowdsourcing;ranking documents based;ranking documents;new approach retrieval;based crowdsourcing clue;clue webpages;retrieval passage retrieval;question answering systems;approach retrieval;web pages based;clue web pages;method ranking documents;crowdsourcing clue;pages based web;question answering;passage retrieval;answering systems;web pages fluid;web pages web;based crowdsourcing;webpages;crowdsourcing;passage retrieval important;web pages;pages need web;web pages need", "pdf_keywords": ""}, "1afe82d34c182d43cbcc365d26e704058aa32351": {"ta_keywords": "model speaker model;density model speaker;model speaker;model integrated voice;performance speaker model;speaker model;voice conversion;integrated voice conversion;mixtures speakers model;voice conversion experimental;parameter generation algorithm;speaker model second;speakers model mivc;number mixtures speakers;combines parameter generation;parameter generation;mixtures speakers;speakers model;integrated voice;performance speaker;improved performance speaker;optimization approach generate;speaker;voice;model optimization;optimize model structure;generate model;proposes parameter generation;model optimization approach;optimize model", "pdf_keywords": ""}, "2c871df72c52b58f05447fcb3afc838168d94505": {"ta_keywords": "knowledge neurons edit;knowledge neurons;self organized criticality;leverage knowledge neurons;directed percolation;transition directed percolation;neurons edit;knowledge fine tuning;organized criticality;knowledge stored pretrained;directed percolation model;factual knowledge stored;recalling factual knowledge;neurons express fact;storage knowledge pretrained;neurons edit update;transition occurs critical;percolation model transition;knowledge pretrained transformers;percolation;knowledge stored;pretrained language models;knowledge pretrained;percolation model;neurons;neurons express;occurs critical;organized criticality sc;criticality;storage knowledge", "pdf_keywords": "knowledge neurons;knowledge neurons update;knowledge neurons express;knowledge neurons shared;knowledge neurons described;utilize knowledge neurons;knowledge stored pretrained;set knowledge neurons;knowledge neurons considering;retain knowledge neurons;identify knowledge neurons;knowledge pretrained;erase knowledge pretrained;knowledge stored;knowledge attribution;pretrained language models;knowledge pretrained transformers;calculate knowledge attribution;knowledge attribution method;knowledge attribution scores;factual knowledge stored;knowledge;prompts knowledge attribution;calculate knowledge;language models answer;prompt calculate knowledge;identify knowledge;language models;erase knowledge;attribution scores neurons"}, "26c2aad87810418b09e0f5b80352dd4d2536afe3": {"ta_keywords": "automated social skills;social skills trainer;social skills attempt;social skills experiments;social skills;communication skills human;sst developing dialogue;improve social skills;social interaction;social communication skills;learns social communication;learns social;developing dialogue;computer based training;skills human;social interaction acquire;person computer based;automated social;acquire social skills;sst performed human;skills human agent;interaction acquire social;trainer learns social;humans performing sst;skills experiments;train improve social;named automated social;communication skills;skills attempt automate;interaction avatar", "pdf_keywords": ""}, "07cedc7899497f2f4ee6f4736e03b78accb47b74": {"ta_keywords": "propose semi supervised;semi supervised learning;semi supervised;data semi supervised;supervised learning network;learning network data;supervised learning;supervised;relational neighbor classifier;learning network;learning method labeled;baseline semi supervised;video supervised learning;supervised learning method;labeled data semi;supervised learning algorithm;neighbor classifier;dynamics video supervised;learn unlabeled data;video supervised;labeled data;authoritative instances training;neighbor classifier wvrn;classification;achieve classification;benchmark datasets labels;classifier;relational neighbor;unlabeled data semi;weighted vote relational", "pdf_keywords": ""}, "d4d26ccbf1e64e725b5bffc08ab28a72e271facb": {"ta_keywords": "production propagation air;propagation air air;propagation air;air air hybrids;air hybrids;air air;flow directed network;air hybrids identify;directed network;directed network structure;air;directed flow;production propagation;network structure directed;flow directed;directed flow directed;flow directed flow;model production propagation;structure directed flow;dynamics directed flow;xdts biases;directed flow defined;art xdts biases;propagation;network structure;xdts high proportion;xdts high;structure directed;challenges xdts;state art xdts", "pdf_keywords": ""}, "26d5c7ad2778c77a1b8734dceb34fe38a1179e2f": {"ta_keywords": "prediction malicious apps;malicious apps hmm;malicious apps;malicious applications app;generate malicious app;malicious app;applications malicious app;malicious app space;malicious app spaces;malicious applications;apps hmm model;possible malicious applications;various malicious applications;applications malicious;malicious applications malicious;training prediction malicious;prediction malicious;malwares detection;new malwares detection;apps hmm;propose new malwares;malwares detection called;malwares;new malwares;model various malicious;apps;model generate malicious;android real;apps obtaining;generate malicious", "pdf_keywords": ""}, "9045bf2a9c1e2b9621c69c57f991d10880e91f18": {"ta_keywords": "efficient randomized reductions;efficient randomized;randomness clique;randomized reductions need;space efficient randomized;randomized reductions;input randomness clique;randomness clique problem;reducibility statistical problems;algorithms planted clique;hardness host statistical;access randomness;computational hardness host;computational hardness;infer computational hardness;repeatedly access randomness;secret leakage variants;statistical problems sparse;finding randomness;secret leakage useful;polynomial time algorithms;access randomness use;secret leakage;finding randomness input;secret leakage helpful;appropriate secret leakage;algorithms planted;statistical problems planted;sparse pca;randomized", "pdf_keywords": "randomized logspace algorithms;randomized logspace emerged;randomized logspace;desired randomness harvesting;randomness harvesting;decades randomized logspace;randomized logspace_;logspace algorithms provably;present randomized logspace;access randomized logspace_;access randomness;logspace algorithms;randomness harvesting goals;randomized logspace_ planted;randomness multiple access;access randomness multiple;multiple access randomness;complexity problem derandomizing;access randomized;logspace algorithms used;achieve desired randomness;desired randomness;multiple access randomized;logspace emerged;logspace emerged widespread;provably polynomial time;randomness;randomized;solvable present randomized;randomness multiple"}, "24a2f68cf81ba3ee55e7a87d0770374ab8e99858": {"ta_keywords": "learnability recursive logic;recursive logic programs;learnable equivalence queries;logic programs recursive;recursive programs learnable;free recursive logic;programs learnable equivalence;learnability recursive;programs recursive logic;logic programs;learnability function free;eecient learnability recursive;recursive logic;logic programs models;queries class learnable;learning boolean functions;learnable equivalence;hard learning boolean;learning boolean;logic programs popular;computationally diicult learning;schemes analyze learnability;learnability function;class learnable programs;analyze learnability function;learnable programs;programs learnable;learnability;recursive programs;class recursive programs", "pdf_keywords": ""}, "5ed4b17a4b7932619f0969e1f5acae76e90f7bdd": {"ta_keywords": "utterances nested queries;utterance predicting queries;predicting nested queries;neural semantic parsers;predicting queries;query generation;semantic parsers;semantic parsers usually;nested queries large;query generation problems;nested queries;predicting queries different;parsers;complicated utterances nested;parsers usually;query generation problem;utterances nested;sql query generation;long complicated utterances;neural semantic;parse long complicated;parse;utterance predicting;complicated utterances;queries large search;novel question decomposer;queries;parsers usually fail;components utterance predicting;layers neural semantic", "pdf_keywords": ""}, "feb403bb5a064ab68b2db655b80a7417f7cfc9f3": {"ta_keywords": "relational learning;relation tree based;novel relational learning;relational learning method;propose relational learning;relational learning approach;features modeling relational;relation tree;features relational data;features relational;hidden variables relational;modeling relational data;called relation tree;modeling relational;learning hidden variables;pairwise features relational;dependencies markov networks;relational mns;relational data;variables relational data;relational data achieves;class relational mns;treermn efficient hidden;tree based rmn;relational mns rmns;novel relational;learning hidden;relational;relational data proposed;rmn treermn efficient", "pdf_keywords": ""}, "2fb44f1317bc51a1e011a5a44d817ad9104e29e8": {"ta_keywords": "privacy meets nlp;privacy applications nlp;differential privacy;differential privacy applications;privacy amplification;differential privacy various;privacy amplification downstream;dataset differential privacy;differential privacy meets;constructing differentially private;properties differential privacy;private auto encoder;differentially private;problem privacy amplification;differentially private auto;point differential privacy;privacy satisfy certain;users privacy satisfy;utterances privatized easily;privacy satisfy;encoder text rewriting;dimension utterances privatized;privacy;problem privacy;privacy various;protecting users privacy;privacy applications;users privacy;utterances privatized;privacy various scenarios", "pdf_keywords": "differential privacy;differential privacy differential;privacy differential privacy;privacy dp differentially;private differential privacy;differentially private autoencoder;differential privacy dp;local differential privacy;privacy differential;differential privacy computer;privacy preserving algorithm;privacy preserving;differentially private;new privacy preserving;rn level privacy;problem privacy enforcement;present new privacy;dp differentially private;private autoencoder;differentially private differential;private differential;privacy computer data;privacy enforcement;algorithm diagonally private;private autoencoder text;level privacy protection;new privacy;privacy dp;level privacy;privacy enforcement paper"}, "b3848d32f7294ec708627897833c4097eb4d8778": {"ta_keywords": "language models trained;safety factual grounding;improving model safety;models trained;model grounding robots;neural language models;crowdworker annotated data;tuning annotated data;fine tuning annotated;tuning annotated;grounding robots;grounding robots model;language models;model human;trained computer;generate responses grounded;crowdworker annotated;robots;behavior model human;trained 56 words;robots model;safety using metric;classifier fine tuned;model safety;quantify safety;model human brain;neural language;small crowdworker annotated;human values filtering;quantify safety using", "pdf_keywords": "dialogue evaluation crowdsourced;dialogue questions crowdsourced;crowdsourced activities dialogue;dialogue models widely;dialogue models;model dialogue evaluation;dialogue models powerful;evaluation crowdsourced;dialogue evaluation;dialogue quality evaluation;evaluation crowdsourced crowdsourced;activities dialogue models;answer dialogue questions;answer dialogue;life dialogue models;crowdsourced environments encoding;questions crowdsourced environments;model dialogue quality;crowdsourced environments;propose model dialogue;questions crowdsourced;model dialogue;crowdsourced activities;crowdsourced crowdsourced activities;dialogue quality;crowdsourced crowdsourced;crowdsourced;dialogue;dialogue questions;approach answer dialogue"}, "17c5e16d16585a01fbfd90ff39f6799952675b21": {"ta_keywords": "network conversational bilingual;speech recognition monolingual;model bilingual mandarin;proposed model bilingual;bilingual speech recognition;conversational bilingual;conversational bilingual speech;model bilingual;factorized final bilingual;bilingual speech;final bilingual output;utterances purely monolingual;recognition monolingual;bilingual output;bilingual mandarin;recognition defining monolingual;recognition monolingual code;bilingual output code;comprise bilingual speech;bilingual speech encompasses;monolingual code switched;monolingual code switch;monolingual code;tasks comprise bilingual;bilingual;monolingual sub tasks;final bilingual;bilingual mandarin english;likelihoods monolingual code;monolingual types", "pdf_keywords": "multilingual speech recognition;bilingual asr task;bilingual speech recognition;recognize monolingual utterances;multilingual multilingual speech;formulations bilingual asr;multilingual speech;bilingual asr;factorization bilingual task;conditional factorization bilingual;bilingual speech;monolingual asr conditional;multilingual language model;monolingual utterances;bilingual task;bilingual task logically;monolingual asr;utterances different languages;factorization bilingual;need recognize monolingual;monolingual based approach;cs monolingual asr;multilingual language;recognize monolingual;cover bilingual speech;switched utterances languages;recognize monolingual intrasententially;multilingual multilingual;language monolingual based;propose multilingual language"}, "c204d40384d39c59cd7249bde4cd8615972acaac": {"ta_keywords": "self propelled swimmer;dynamics unwrapping machine;propelled swimmer noisy;propelled swimmer;dynamics unwrapping;swimmer noisy environment;model dynamics unwrapping;topological invariant xmath0;robustness machine translation;swimmer noisy;study robustness machine;robustness machine;performance self propelled;swimmer;noisy environment study;translation systems;topological invariant;unwrapping machine;translation presence noise;competition topological invariant;self propelled;machine translation presence;invariant xmath0;translation systems ability;dynamics;study competition topological;paper propose machine;machine translation systems;machine translation;ability machine translation", "pdf_keywords": ""}, "024aa0b78e2a29d07533ee1c6e3b2e875ae45618": {"ta_keywords": "estimating influences speakers;speaker based hypothesis;quantum computer;influences speakers conversation;quantum computer propose;word use speaker;performance quantum computer;speakers conversation;word distribution;computer propose probabilistic;conversation data;speakers conversation data;general word distribution;performance quantum;environment performance quantum;influences speakers;word use depends;speaker based;propose probabilistic model;word distribution absence;propose probabilistic;probabilistic model estimating;speakers earlier word;quantum;use speaker based;probabilistic model;probabilistic;conversation;conversation data multiple;estimating influences", "pdf_keywords": ""}, "ffc211476f2e40e79466ffc198c919a97da3bb76": {"ta_keywords": "collective dynamics based;multi agent reinforcement;dynamics collective;agent reinforcement learning;train decentralised policies;policies exploit centralised;learning joint action;agent reinforcement;based multi agent;multi agent;dynamical dynamics collective;method train decentralised;decentralised policies centralised;study collective dynamics;collective dynamics group;based observation collective;consistency centralised decentralised;policies centralised;strategy extracting decentralised;centralised decentralised policies;collective dynamics;dynamics collective mapped;extracting decentralised policies;interacting agents;observation collective dynamics;starcraft ii micromanagement;guarantees consistency centralised;exploit centralised learning;observation collective;agent values guarantees", "pdf_keywords": "decentralised reinforcement learning;multi agent reinforcement;agent reinforcement learning;functions decentralised reinforcement;decentralised reinforcement;reinforcement learning architecture;learning agents controls;learning agents;agent reinforcement;learning decentralised policies;reinforcement learning recently;micromanagement problem starcraft;based multi agent;multi agent;ii learning agents;task heterogeneous agents;reinforcement learning;novel multi agent;reinforcement learning methods;decentralised micromanagement problem;decentralised micromanagement;agent action value;heterogeneous agents propose;agents controls;agent action;decompositions agent action;agents controls individual;learning decentralised;focus decentralised micromanagement;value functions decentralised"}, "a6b431df3b3d40c98d8d623cab559a9cddd41662": {"ta_keywords": "shot generalizability dialogs;shot transfer learning;model zero shot;flow;zero shot transfer;zero shot;generalizability dialogs;zero shot generalizability;dialog policies training;dialogs;solutions neural models;transfer learning;challenge dialog research;dialogs rigorously;solutions neural;dialog;challenge dialog;generalizability dialogs rigorously;zero shot settings;learning paper zero;dialog systems;unseen tasks;shot generalizability;dialog systems unseen;fluid dynamics video;paper zero shot;learning;dialogs rigorously proved;major challenge dialog;controlling flow", "pdf_keywords": "schema attention model;shot generalization taskoriented;dialog generation;schema attention;shot transfer learning;unseen dialog tasks;dialog tasks;data driven dialog;zero shot end;domain dialog generation;zero shot generalization;generalization taskoriented dialog;attention model;introduces schema attention;attention;zero shot;end dialog;dialog context;driven dialog;dialog;underperform unseen dialog;dialog generation paper;paradigm zero shot;dialog called schema;dialog tasks domains;implicitly memorize dialog;memorize dialog;shot generalization;shot end;zero shot transfer"}, "62763dbdd47f144c73663b6c6b5d95caeb318e43": {"ta_keywords": "permutation rank model;richer permutation rank;proposed permutation rank;low rank models;rank models;low rank assumptions;permutation rank;low rank approximated;approximated low rank;low rank matrix;matrix low rank;rank model estimator;rank model enforces;rank matrix paper;rank model matrix;rank assumptions avoided;negative rank model;rank approximated low;rank models standard;negative rank;rank assumptions;non negative rank;low rank;rank model;rank approximated;classical low rank;rank matrix;rank;clustering structural heterogeneity;structural descriptors", "pdf_keywords": "rank matrix completion;online matrix completion;permutation rank decomposition;rank decomposition;matrix completion;matrix completion problem;matrix completion goal;low rank matrix;rank decomposition consider;matrix decomposition based;matrix model crowd;optimal low rank;permutation rank approach;algorithm matrix decomposition;matrix decomposition;rank matrix;model crowd labeling;matrix entries partially;models matrix decomposition;crowd labeling;singular valuethresholding algorithm;matrix decomposition considers;low permutation rank;online matrix;structured matrix entries;crowd labeling based;reconstruct structured matrix;matrix entries;rank approach;permutation rank"}, "bd49e66af9755e6138967eba6aeb37d8190d2b4f": {"ta_keywords": "natural language natural;natural language;language natural language;explanation natural language;language natural;natural language used;labeled data extraction;natural languages;used natural language;natural language explanations;labeled data inductive;labeled data;uses labeled data;representations natural languages;pairs spouses text;natural representations natural;create natural representations;spouses text paper;natural representations;spouses text;data extraction;language explanations;data inductive bias;uses labeled;inductive bias couples;extracting pairs spouses;data extraction uses;labeled;data inductive;language explanations article", "pdf_keywords": "relation extraction tasks;relation extraction;natural language inference;models relation extraction;answer extraction;answer natural language;semantic parsing bert;answer answer extraction;natural language explanations;ofconcept relation extraction;fact models trained;semantic parsing;language inference datasets;existing natural language;prediction answer answering;language explanations richer;patterns semantic parsing;language inference;ontologies modern neural;bert patterns semantic;natural language;answering queries text;answer extraction answer;programmatic explanations;language explanations;generative models trained;relying fact models;answer answering;tasks answer answering;answer answering queries"}, "5e51edfcef2b28594c63cce97c08752dfd438af0": {"ta_keywords": "generative models grapheme;discriminative learning methods;structured online discriminative;discriminative learning;online discriminative learning;phoneme g2p conversion;online discriminative;conversion based phoneme;outperform conventional generative;generative models;conventional generative models;models grapheme phoneme;g2p conversion task;grapheme phoneme g2p;phoneme g2p;method g2p conversion;models grapheme;g2p conversion based;learning methods;g2p conversion;discriminative;support vector machines;ways regularization;conventional generative;regularization;phoneme error rate;generative;grapheme phoneme;vector machines allowing;vector machines", "pdf_keywords": ""}, "eebfece29b7a5c2202f1ec53ef49d6fdb75ce0ea": {"ta_keywords": "updates neuronal computations;neuronal computations;temporal functions learned;learned online backpropagation;learned synaptic weights;neuronal computations depend;based updates neuronal;parameters learned synaptic;neuron recover parameters;inputs presynaptic neurons;learned synaptic;neurons integrated;neural computations;neuron;updates neuronal;presynaptic neurons integrated;teacher neuron;online backpropagation time;backpropagation time;learning neural computations;online learning neural;online backpropagation;neural computations ungraded;neurons;neuron recover;simulated neural;neuron present;electrophysiological properties synaptic;parameters teacher neuron;intrinsic parameters learned", "pdf_keywords": "multilayer spiking neural;rule multilayer spiking;multilayer spiking;spiking neural networks;spiking neural;student neuron;teacher student neuron;online learning neural;online backpropagation;neuron;learning rule multilayer;functions online backpropagation;online backpropagation time;backpropagation time;learning brain;student neuron key;spike time teacher;neuron key;synaptic weights;neural computations;gradient based learning;learning neural computations;based learning brain;learning neural;backpropagation;backpropagation time relying;idea synaptic weights;spiking;synaptic weights intrinsic;learn complex temporal"}, "e31efa7295e5d6681607ed8ef9c45300d64227aa": {"ta_keywords": "multi winner approval;approval voting scenarios;winner approval voting;approval voting agent;heuristics multi winner;voting behavior multi;decisions using voting;behavior multi winner;cherenkov approval voting;voting agent vote;approval voting;manipulation heuristics multi;agent vote candidates;agent vote;voting rules multi;emergence multiple winnerings;multiple winnerings;voting agent;voting scenarios;manipulation heuristics;winner approval;voting scenarios complete;group voting;optimal manipulation heuristics;heuristics multi;winnerings group voting;voting behavior;collective decisions using;group voting rules;rules multi winner", "pdf_keywords": "voting behavior uncertain;model user voting;voting behavior;voting mixed human;user voting behavior;vote truthfully agents;setting voting truthfully;voting pattern consistently;voting truthfully results;voting pattern;generally manipulate vote;computer setting voting;user voting;consistently majority participants;manipulation heuristics;manipulation heuristics adaptive;approval voting;manipulate vote;voting mixed;approval voting mixed;voting truthfully;follow voting pattern;setting voting;effectiveness heuristics;heuristics adaptive;vote obtain better;effectiveness heuristics situations;optimal manipulation heuristics;participants did vote;manipulate vote obtain"}, "99c4007b1f6cb905788479db7fc886168f05e57c": {"ta_keywords": "rnns automatic speech;networks rnns;improvements dnn;improvements dnn end;neural networks rnns;fountain new backpropagation;performance dnn;recurrent deep neural;deep neural;dnn architecture;neural network dnn;dnn architecture 2nd;performance dnn architecture;speech recognition;networks rnns automatic;deep neural networks;wer improvements dnn;rnns;deep representations;propose recurrent deep;chime challenge track;new backpropagation;dependency deep representations;temporal dependency deep;dnn model;recurrent deep;evaluate performance dnn;new backpropagation time;stochastic gradient descent;rnns automatic", "pdf_keywords": ""}, "c783e1fb3ce8514f981925ee590c00884660ee4e": {"ta_keywords": "causally masked languageimage;masked languageimage models;conditioned text captioning;masked language models;causal masked language;languageimage models;captioning zero shot;masked languageimage;captioning;text captioning;languageimage models large;language models;causal masked;hyperlinks image tokens;image tokens;captioning zero;hypertext;causally masked;text captioning zero;new causally masked;common causal masked;masked language;languageimage;conditioned text;hypertext markup;text;text hypertext;causally masked approach;text hypertext markup;images unconditionally conditioned", "pdf_keywords": "entity linking causally;like dall captioning;summarization entity linking;shot summarization entity;captions ms coco;captioning zero shot;causal language modeling;zero shot summarization;dall captioning;language modeling causally;entity linking task;causally masked multimodal;entity linking;linking entity disambiguation;large corpus structured;captioning;end entity linking;truth captions;linking causally masked;shot summarization;images hypertext links;entity linking entity;dall captioning zero;linking entity;ground truth captions;captions;truth captions ms;captioning zero;generative models trained;trained large corpus"}, "80b92f762e116d4513da27792822897ca3915247": {"ta_keywords": "strong privacy guar021;photoassociative neural networks;networks pnns;privacy guar021 antees;baseline privacy bounds;strong privacy;privacy preserving 022;privacy guar021;privacy bounds 019;privacy bounds;certain privacy preserving;improves baseline privacy;015 optimizers nlp;privacy preserving;networks vnnns;paper certain privacy;pnns based use;provides strong privacy;graph convolutional networks;networks pnns based;neural networks pnns;baseline privacy;neural networks vnnns;privacy;optimizers nlp datasets;certain privacy;pnns based;representation learning documents;large number photoassociative;antees graph convolutional", "pdf_keywords": "privacy graph nlp;preserve privacy graph;privacy graph;privacy preserving model;new privacy preserving;privacy preserving;differential privacy;idea differential privacy;strong privacy measures;techniques differential privacy;privacy preserving gcns;privacy preserving techniques;guaranteeing strong privacy;preserve privacy;differential privacy org;strong privacy;models protect textual;new privacy;paper present privacy;keeping strict privacy;modeling choices privacy;propose new privacy;choices privacy preserving;privacy;differential privacy dp;privacy measures;privacy org;private variant keeping;prevent information leaks;present privacy"}, "3d5b51fc30ffacdcc8424618555accb36756ccc9": {"ta_keywords": "objective function search;randomized derivative free;design randomized derivative;free algorithm;unconstrained minimization;derivative free algorithm;randomized derivative;iteration complexity;analyze iteration complexity;algorithm;stepsize selection schemes;finding objective function;function search;algorithm analyze iteration;unconstrained minimization problem;iteration complexity paper;consider unconstrained minimization;minimization;free algorithm analyze;minimization problem smooth;finding particle binary;iterated function propose;analyze iteration;objective function;randomized;algorithm analyze;function search problem;selection schemes;method finding objective;estimating number particles", "pdf_keywords": "method strongly convex;strongly convex problems;stp method strongly;numerical optimization;unconstrained minimization;strongly convex;unconstrained minimization problem;problems numerical optimization;consider unconstrained minimization;minimization problem smooth;convex problems;optimization;stp method;search methods directional;convex problems paper;parallel version stp;minimization;minimization problem;method strongly;methods directional type;direct search methods;convex;paper propose parallel;rn direct search;methods directional;search methods;solving problems numerical;propose parallel version;propose parallel;stp"}, "845aad7b99f48526fe003c775836091521624471": {"ta_keywords": "crowdsourcing data;crowdsourcing data general;crowdsourcing;problem crowdsourcing;problem crowdsourcing data;dataset collaborative lexicography;study problem crowdsourcing;collaborative lexicography;collaborative lexicography projects;based naive bayes;lexicography projects wiktionary;bayes classifier weighted;naive bayes classifier;wikipedia expert;bayes classifier;naive bayes;wikipedia expert built;topic prediction hat;dataset collaborative;classifier weighted;model based naive;lexicography projects;provided dataset collaborative;prediction hat lipkin;lexicography;projects wiktionary;just wikipedia expert;competitors traditional semantic;knowledge bases;based naive", "pdf_keywords": ""}, "a3cd9c4f8fa52c5e23885c2f82931d7e0f7d4b45": {"ta_keywords": "dispensing data drug;data drug;dispensing powder drugs;files binary mixture;drugs employing dimensional;binary mixture xmath0;dispensing data;checking dispensing powder;necessary dispensing data;powder drugs employing;mixture xmath0;mixture xmath0 particles;powder drugs;xmath0 particles developed;data drug generic;dispensing powder;symbology 2d barcode;dimensional barcode symbology;employing dimensional barcode;dimensional barcode;barcode symbology;xmath0 particles;barcode symbology 2d;2d barcode;binary;binary mixture;database files binary;files binary;new checking dispensing;linear barcode", "pdf_keywords": ""}, "697e6eecb0e77ba56c685bb99b221d959739d13b": {"ta_keywords": "automatic geo tagging;geo tagging;geo tagging images;geo tagging consider;task geo tagging;detailed location models;location models;location data propose;providing location based;tags automatic geo;global location data;location data;location models providing;estimating location city;estimating location;location estimation;location based;location estimation multi;recognition movement tagged;location based services;method estimating location;tagging;tagging images;models providing location;providing location;latent dirichlet allocation;movement tagged;tags determining;textual tags determining;automatic geo", "pdf_keywords": ""}, "e42b3ead5ff04adfa95c87e0180561f0c3ba4af4": {"ta_keywords": "reinforcement learning control;learning control;learning control systems;reinforcement learning rl;reinforcement learning;algorithm learns convex;vanilla reinforcement learning;learning policy;optimization problem policy;learns convex;learning policy learned;learned policy;tasks reinforcement learning;learns convex combination;reinforcement learning variety;algorithm learns;problem learning policy;control tasks reinforcement;policy learned policy;learned policy propose;proposed algorithm learns;benchmark control tasks;policy learned;benchmark control;state action constraints;focuses reinforcement learning;outperforms vanilla reinforcement;learning rl powerful;reinforcement;problem learning", "pdf_keywords": "vertex policy network;novel vertex policy;constraints policy network;unconstrained policy optimization;vertex policy;policy optimization algorithms;deep reinforcement learning;policy optimization;learned control policies;policy optimization algorithm;policy network;policy network architecture;learn policy continuous;deep reinforcement;safe exploration continuous;policy continuous markov;propose deep reinforcement;exploration learned control;guarantees safety exploration;reinforcement learning framework;safety exploration learned;propose reinforcement learning;reinforcement learning algorithm;exploration continuous action;literature policy optimization;safety constraints policy;reinforcement learning;continuous markov decision;performs unconstrained policy;unconstrained policy"}, "e54a4e49917eb3da18c2f239be70a68fbd3274c3": {"ta_keywords": "peer review;analysis peer reviews;peer reviews;example peer review;peer review present;peer review process;peer reviews ii;analysis peer;peer paper study;context peer review;users technical programming;peer;peer paper;documentation debt prevalent;reviews ii documentation;work decide peer;decide peer;packages reviewed;peer good;technical programming;users technical;decide peer good;example peer;documentation debt;candidate peer paper;rr users technical;technical debt;review process;paper study performance;good candidate peer", "pdf_keywords": "peer review packages;documentation peer reviews;documentation peer;organize peer review;peer reviews analyze;wear software engineering;technical usability debt;peer review process;peer review study;peer review;technical debt software;open peer review;usability debt development;software conducted study;peer reviews paper;peer reviews;review packages;debt software conducted;prevalent peer reviews;engineering conducted peer;reviews analyze debt;software engineering conducted;software paper;taxonomy technical debt;conducted peer review;software conducted;software engineering;usability debt;github;software"}, "59d225fcb08ce66935e0285a9936ee158c4fdb97": {"ta_keywords": "explanations form entailment;entailment trees approach;multistep entailment trees;entailment trees;entailment steps facts;entailment steps;entailments baselines;multistep entailments baselines;multipremise entailment steps;entailments;approach generate explanations;form entailment trees;tree multipremise entailment;generate explanations;entailments baselines offering;multistep entailment;entailment;multipremise entailment;multistep entailments;dataset multistep entailments;entailment trees tree;richer systematic explanations;form entailment;generate explanations form;systematic explanations goal;contain multistep entailment;debugging reasoning paper;systematic explanations;fragment textual evidence;debugging reasoning", "pdf_keywords": "automatic entailment trees;generate deep entailment;automatic entailment;textual entailment steps;approach automatic entailment;entailments generated using;entailments generated;textual entailment;deep entailment trees;generate entailment;entailment trees approach;generate entailment tree;goal generate entailment;generate explanations;approach generate explanations;entailment datasets;deep entailment;entailment trees;entailment tree;premise textual entailment;entailment tree including;entailment datasets available;generate explanations form;answer answering tasks;entailment trees shown;trees answer answering;entailment trees answer;entailment steps use;entailment steps;entailments baselines offering"}, "deedb9b61a01d686b28e6034770fccc142e77fab": {"ta_keywords": "nlp tasks predictors;nlp predictor;processing nlp predictor;nlp predictor used;nlp experiment;nlp model perform;nlp tasks;score nlp experiment;experimenting different nlp;evaluation score nlp;nlp experiment given;nlp model;plausible judgments nlp;different nlp tasks;predictions unseen languages;natural language processing;judgments nlp model;processing nlp;nlp;language processing nlp;natural language;judgments nlp;analysis natural language;models predict evaluation;different nlp;score nlp;produce meaningful predictions;predict evaluation;meaningful predictions unseen;meaningful predictions", "pdf_keywords": "bilingual lexicon induction;evaluation machine translation;learning bilingual lexicon;bilingual lexicon;machine translation;machine translation standard;learning bilingual;automatic evaluation machine;comprehensive evaluation lexicographic;lexicon induction;lexicon induction important;automatic evaluation;evaluation lexicographic properties;evaluation lexicographic;records learning bilingual;bilingual;set children language;languages spoken set;children language;datasets evaluation;evaluation machine;children language united;translation standard;method automatic evaluation;translation standard practice;datasets evaluation likely;language united;languages domains;translation;languages"}, "4cfbd97a5b42695697f70a9f28ee29711f6ca433": {"ta_keywords": "features adversarial attacked;features adversarial;detect situations trained;saliency map predicted;real world driving;deep learning driven;similar features adversarial;adversarial attacked images;trustworthy prediction demonstrate;trustworthy prediction;make trustworthy prediction;datasets driving scenarios;prediction task aware;adversarial;saliency provide learning;self driving;learning driven safety;network saliency;self driving cars;adversarial attacked;saliency map;driving datasets driving;datasets driving;learned prediction;detect novel scenarios;driving scenarios;able detect situations;saliency;world driving datasets;driving datasets", "pdf_keywords": ""}, "10e88416035a8a3cbef0e65f8967df650abd0a00": {"ta_keywords": "word sense disambiguation;unsupervised word sense;semantic similarity;learn relevant semantic;sentence synset;sense disambiguation;relevant semantic similarity;lexical semantic resources;sparse embeddings;sense disambiguation paper;synset based learning;sentence synset constituting;lexical semantic;synset constituting sense;problem sparse embeddings;semantic similarity given;disambiguation;different lexical semantic;given sentence synset;unsupervised word;sparse datasets according;sparse datasets;synset based;semantic;synset;synset constituting;disambiguation paper;semantic resources;present unsupervised word;lexical", "pdf_keywords": "word sense disambiguation;disambiguation word embeddings;unsupervised word disambiguation;word disambiguation based;disambiguation based word;sense disambiguation;word disambiguation;sense disambiguation generalpurpose;synonymy dictionaries architecture;based synonymy dictionaries;disambiguation word;word sense inventory;synonymy dictionaries;disambiguation based;sense disambiguation whd;sense disambiguation important;lexical ontology sense;based disambiguation word;word vector dataset;available based disambiguation;disambiguation;word sense;disambiguation generalpurpose;lexical ontology;based disambiguation;senses polysemous words;wordnet based algorithms;word embeddings;wordnet;uses lexical ontology"}, "4fffa5245d3972077c83614c2a08a47cb578631e": {"ta_keywords": "supervised approaches speech;acoustic language models;acoustic language model;approaches speech representation;speech representation learning;speech representation;data self supervised;self supervised approaches;self supervised;state art wav2vec;unsupervised clustering;supervised;unsupervised clustering algorithm;self organizing algorithm;language models;combined acoustic language;acoustic language;wav2vec libri;wav2vec;language models able;combination acoustic language;prediction location person;lexicon input sound;cluster labels;learn combined acoustic;language model;organizing algorithm prediction;cluster labels unlabeled;representation learning;clustering", "pdf_keywords": "supervised speech representation;self supervised speech;unsupervised learning speech;learning speech representations;speech audio representations;supervised learning speech;speech representation learning;speech representations;speech representation;supervised speech;learning speech audio;audio representations;speech representations use;learning speech;speech audio;speech recognition;automatic speech;unsupervised learning;automatic speech recognition;self supervised learning;framework automatic speech;models unsupervised learning;audio representations long;continuous latent representations;loss information quantization;loss self supervised;semi supervised;bert like prediction;semi supervised iterative;supervised iterative pseudo"}, "520e82c0f35a14ecf78b93de3673bb8b2a3212fc": {"ta_keywords": "trained temporal linking;temporal linking;trained temporal;pre trained temporal;predict temporal;involved timeline extraction;timeline extraction distantly;temporal linking systems;timeline extraction;predict temporal behavior;approach predict temporal;temporal;neural;train neural;temporal behavior human;entity involved timeline;timeline;involved timeline;distantly supervised;temporal behavior;extraction distantly supervised;distantly supervised approach;human brain;presents novel neural;systems train neural;train neural network;neural network architecture;brain goal order;human brain goal;supervised", "pdf_keywords": ""}, "0115d5d37f7cdc7b8d2147c0bb348e714432e899": {"ta_keywords": "channel speech enhancement;speech enhancement se;use speech enhancement;speech enhancement;identification performance speech;speech enhancement step;speech recognition;new language recognition;performance speech recognition;single channel speech;language recognition lala;language recognition evaluation;knowledge audio domain;knowledge audio;video domain adaptation;channel speech;audio preprocessing language;language recognition;spectrogram instead adaptation;speech recognition systems;analysis language recognition;speech using noisy;dnn based single;requires knowledge audio;audio domain;domain adaptation;noise speech;dnn based;magnitude person speech;audio preprocessing", "pdf_keywords": ""}, "cc2c3df6b09166c54e670d347bfe26dae236ac73": {"ta_keywords": "multiview semi supervised;semi supervised exploratory;learn dynamics multi;semi supervised;semi supervised learning;supervised exploratory;learn dynamics;exploratory learning;automatic knowledge;supervised exploratory learning;exploratory learning framework;used learn dynamics;supervised learning;automatic knowledge base;supervised;supervised learning incomplete;learning framework;abstract problem multiview;pendulum used learn;multi view;multiview semi;general multiview semi;general multiview;learning incomplete;knowledge base construction;argue automatic knowledge;multi view multi;learning framework solving;view motion pendulum;knowledge base", "pdf_keywords": ""}, "f7979c6690562c5f8bf700e3fd184c4d1df0a54c": {"ta_keywords": "lingual entity linking;neural entity linking;cross lingual entity;lexical resources bridge;improves entity linking;language zero shot;bilingual lexical resources;lingual entity;entity linking accuracy;entity linking models;target languages resources;linking assume bilingual;low resource languages;heavily bilingual lexical;low resource language;entity linking;pivot language train;source target languages;target language;bilingual lexical;neural entity;resource languages transfer;target languages;lowresource language;languages transfer;languages resources scarce;mention source language;entity linking maps;target language experiments;source lowresource language", "pdf_keywords": "crosslingual entity linking;lingual knowledge linking;language crosslingual entity;crosslingual entity;cross lingual knowledge;entity linking experimental;source language crosslingual;approach cross lingual;language crosslingual;entity linking;low resource languages;knowledge linking;knowledge linking wiki;language low resource;resources source languages;lingual knowledge;linking wiki knowledge;cross lingual;crosslingual el systems;resource language pairs;low resource language;resource language language;enable cross lingual;resources source language;resource languages;entity linking method;cross lingual el;linking wiki;resource languages various;monolingual crosslingual el"}, "3b0a1a10d8f7496226635c5c3b8475fcd10d890d": {"ta_keywords": "latency serving requests;scheduling redundant requests;performance redundant requests;systems latency serving;sending redundant requests;redundant requests maximally;redundant requests achieves;redundant requests;optimal redundant requesting;redundant requests evaluated;latency serving;requests maximally scheduling;latency performance redundant;redundant requests primary;redundant requests help;serving requests;scheduling redundant;serving requests potentially;redundant requesting;requests way distributed;maximally scheduling redundant;requests achieves optimal;optimal average latency;systems latency;latency latency performance;arbitrary arrival processes;serve requests;redundant requesting policies;reduced sending redundant;latency performance", "pdf_keywords": ""}, "f826381aea632791b6007e427a9587c11b239b6a": {"ta_keywords": "exploration deep learning;learning agents rewards;exploration deep;learning agents;randomly exploring agent;based thompson sampling;efficiency exploration deep;thompson sampling spiking;agents rewards sparse;rewards sparse action;learn schedule exploration;thompson sampling;tasks make learning;randomly exploring;exploring agent;approach learning noisy;sampling spiking replay;improves efficiency exploration;exploration large parameter;introduce exploration;learning feasible;schedule exploration large;dialogue tasks;dynamics randomly exploring;sparse action;deep learning agents;schedule exploration;make learning feasible;learning noisy;harvest dialogue tasks", "pdf_keywords": ""}, "f07a326e21395f025a87b2d77cac7e8ca502f002": {"ta_keywords": "textual entailment based;textual entailment;analysis textual entailment;entailment based use;entailment based;language understanding models;knowledge data augmentation;entailment;language understanding;textual;domain knowledge;incorporating domain knowledge;surgery acl ligaments;meniscus surgery acl;domain knowledge data;surgery acl;analysis textual;knowledge data;acl ligaments;specialized domains medicine;specialized medical domain;acl ligaments results;models specialized medical;ligaments;medical domain paper;approach analysis textual;medical domain;ligaments results;meniscus surgery;domains medicine", "pdf_keywords": "domain adaptation explore;domain adaptation;achieve domain adaptation;knowledge data augmentation;dataset medical domain;inference dataset medical;dnn 2016 language;language inference dataset;domain medical data;natural language inference;incorporating domain knowledge;deep neural;data medical domain;domain medical;natural language understanding;domain knowledge;domain knowledge data;medical domain;language understanding model;lack domain medical;domains medicine;specialized domains medicine;data augmentation powerful;mt dnn 2016;augmentation using umls;dataset medical;model mt dnn;domains medicine approach;data augmentation;language inference"}, "d95aafa571e9cb6795cc28ecf257ead123664e3c": {"ta_keywords": "segmentation energies;kernel cut algorithms;mrf segmentation energies;segmentation energies benefit;mrf segmentation;clustering regularization;using mrf segmentation;clustering regularization models;propose kernel cut;segmentation;dimensional image features;clustering high dimensional;cut algorithms;new segmentation;cut algorithms based;kernel spectral;linear kernel spectral;solvers clustering regularization;kernel cut;propose new segmentation;high dimensional image;new segmentation model;regularization energies;pairwise clustering;segmentation model;regularization;kernel spectral bound;image features;segmentation model combining;common regularization energies", "pdf_keywords": "kernel embeddings;regularization weight kernel;motion features decompositions;approach embedding kernel;segmentation motion features;isometric data embeddings;embedding kernel;embedding kernel kernels;kernels kernel embeddings;segmentation restoration;kernel embeddings new;image segmentation restoration;segmentation motion;segmentation restoration registration;regularization;embedding;embeddings;features decompositions;data embeddings;segmentation;method segmentation motion;weight kernel;features decompositions allow;embeddings new approach;promising approach embedding;motion modelization;stereo motion modelization;vision image segmentation;image segmentation;approach embedding"}, "250e4a8f5155f1f9f60b2dee3e8da8024338db4d": {"ta_keywords": "dirichlet distribution sentiment;sentiment prediction;sentiment labels reviews;distribution sentiment labels;sentiment labels;approach sentiment prediction;document level sentiment;sentiment prediction based;sentiment analysis rely;sentiment target;sentiment analysis;level sentiment analysis;sentiment labels use;distribution sentiment;level sentiment;approach sentiment;labels reviews target;movie review datasets;context sentiment target;assume sentiment labels;global context sentiment;entropy classifier;labels reviews;novel approach sentiment;sentiment;review datasets outperforms;sentiment target assume;supervised learning;context sentiment;reviews target", "pdf_keywords": ""}, "8eda71ecad19cdef6092e76276eba48312ec7063": {"ta_keywords": "driven event recognition;probabilistic neural;event recognition;interactions modeled gaussian;interaction modeled gaussian;gaussian noise;interaction dense retrievals;resonance dr models;event recognition systems;class probabilistic neural;probabilistic neural network;gaussian white noise;dense retrievals drrs;effect gaussian noise;gaussian noise gw;modeled gaussian potential;noise gw dynamic;modeled gaussian;dense retrievals;neural network models;dynamics dr models;probabilistic;event driven;gaussian potential dm;representation representations work;modeled gaussian white;online event driven;driven event;recognition systems qualitative;interactions modeled", "pdf_keywords": "retrieval models 2016;retrieval models;investigating query representations;analyzing document representations;topic representations;query representations;topic representations mechanisms;stage retrieval models;document representations;level topic representations;retrieval;document representations leaves;query representations future;analyzing document;focuses analyzing document;performance stage retrieval;paper investigate words;investigate words;leaves investigating query;query based;investigating query;words dr models;query based discrete;representations leaves investigating;representations future research;attention different parts;stage retrieval;attention different aspects;aspects input extract;representations design"}, "e0c66240239263f16159eef166a391d3939ae2d5": {"ta_keywords": "reading comprehension;reading comprehension require;reading comprehension examples;does reading comprehension;answer tuples reading;address reading comprehension;topology human body;comprehension examples consist;comprehension examples;questions passages predict;comprehension;predict corresponding answers;topology human;comprehension require;tuples reading;reading;combines information questions;leaderboard;human body consists;paper topology human;human body;tuples reading does;leaderboard dominance;human body controlled;does reading;body human body;reading does reading;leaderboard dominance basic;questions;vying leaderboard", "pdf_keywords": "reading comprehension tasks;encoders reading comprehension;model reading comprehension;comprehension tasks;comprehension tasks based;reading comprehension popular;reading comprehension;learning model reading;comprehension popular;reading;self attention paper;attention;comprehension;model reading;attention paper;comprehension popular rc;global self attention;question passage information;self attention;deep learning;present deep learning;encoders reading;passage information;model encoders reading;attention paper present;deep learning model;extract information;models proposed tasks;learning;leaderboard"}, "3105b5863d4597058bf51aeda40db53394075784": {"ta_keywords": "complexity problem reduced;complexity problem;output game uncertain;complexity;input output game;output game;game uncertain;paper complexity problem;paper complexity;xmath0;factor xmath0;reduced factor xmath0;factor xmath0 case;xmath0 case;xmath0 case input;problem reduced;game;problem reduced factor;case input output;uncertain;input output;case input;problem;input;factor;reduced factor;reduced;output;case;paper", "pdf_keywords": ""}, "446efa0bcf3528b51332a12495cb56784dd8bad3": {"ta_keywords": "classification deep transfer;embeddings graphs trained;transfer learning;deep transfer learning;proposed transfer learning;deep transfer;transferred different embeddings;transfer learning mainly;vectors task transferable;transfer learning framework;embeddings graphs;embeddings language pretrained;features learned graphs;tasks word embeddings;graphs trained;different embeddings graphs;learned graphs;latent relational graphs;learned graphs generic;embeddings;graphs trained approaches;classification deep;word embeddings;graphs generic transferred;language pretrained convolutional;transferable tasks word;image classification deep;relational graphs capture;transfer unary features;convolutional features learned", "pdf_keywords": "features graphs trained;graphs trained;learned graphs;deep transfer;learned graphs generic;graph learning;graphs trained approach;existing transfer learning;latent graph learning;graph representations;graph learning called;deep transfer learning2;hierarchical graph representations;deep multi layer;deep multi;transfer learning;transfer learning2 learn;multi layer deep;based deep transfer;demonstrate learned graphs;unsupervised latent graph;feature based deep;transfer learning2;layer deep convolutional;decoupling graphs features;deep convolutional neural;features graphs;graphs features hierarchical;learn versatile structures;propose deep multi"}, "549dae68d04eefad88885c64a4d946205e524b79": {"ta_keywords": "document clustering sentiment;document embedding;vectors document embedding;clustering sentiment;document clustering;document representations;embeddings benefit text;document embedding stable;based document representations;embeddings;specifically document clustering;clustering sentiment classification;tf idf classification;probabilistic explanation embeddings;embedding;traditional nlp tasks;nlp tasks;sentiment classification paper;nlp tasks specifically;explanation embeddings;basis topic sentiment;idf classification;sentiment classification;representations traditional nlp;embeddings benefit;document representations traditional;traditional nlp;explanation embeddings benefit;topic sentiment;idf classification basis", "pdf_keywords": "document embeddings persistence;embeddings text classi\ufb01cation;sentiment classi\ufb01cation tasks;sentiment topic classi\ufb01cation;document embeddings;document categorization;based document embeddings;word embedding widely;categorization based document;word embedding;document word embedding;document categorization based;sentiment classi\ufb01cation;embeddings text;embeddings persistence;approach document categorization;utility embeddings text;embedding widely;embedding widely used;embeddings;vectors document word;word vectors document;sentiment topic;text classi\ufb01cation tasks;embeddings persistence diagrams;widely used embedding;representations case sentiment;categorization based;categorization;embedding technique computer"}, "35c710f5fdacc71a675832f6beaa2dbfe301d0ce": {"ta_keywords": "dialog systems efficiently;dialog systems;based dialog systems;based dialog;example based dialog;propose uncertainty sampling;dialog;propose random selection;dialog systems creating;based dialog popular;domains complexity utterances;alternatives random selection;uncertainty sampling;uncertainty sampling strategies;active learning;construction dialog systems;complexity utterances;dialog popular;specifically propose uncertainty;propose active learning;random selection;strategies selecting inputs;complexity utterances low;dialog popular option;random selection domains;inputs present human;utterances low paper;alternatives random;utterances;idea sound speed", "pdf_keywords": ""}, "25efc17ba82ba4af29f2e03868de74e1ea66d025": {"ta_keywords": "multilingual text video;multilingual instructional video;multimodal embeddings multilingual;multilingual multimodal embeddings;embeddings multilingual multimodal;learns contextual multilingual;multilingual multimodal pre;contextual multilingual multimodal;multilingual multimodal;introduce multilingual multimodal;multimodal pre training;embeddings multilingual;text video search;bilingual text image;vision language models;multilingual instructional;new multilingual instructional;learns contextual;improves video search;multimodal embeddings;training vision language;specifically focus multilingual;contextual multilingual;instructional video dataset;large margin multilingual;multilingual text;transfer vision language;text video model;bilingual text;multilingual", "pdf_keywords": "multilingual text video;videos introduce multilingual;multilingual instructional video;multimodal machine translation;transferrability multilingual multimodal;multilingual multimodal pre;multimodal pre training;multilingual multimodal;trained multilingual annotations;present multilingual multimodal;multilingual multimodal machine;proposed multilingual multimodal;multilingual pre training;trained multilingual;multilingual annotations zero;performance trained multilingual;multilingual annotations;lingual transferrability multilingual;multimodal pre;new multilingual instructional;multilingual instructional;deep contextual cross;annotations zero shot;instructional video dataset;multimodal;machine translation model;transferring video text;transferrability multilingual;large dataset videos;transfer proposed multilingual"}, "e60b88313fad52c1ef8dd02b482785651d09ad66": {"ta_keywords": "dependent density states;density states tdos;states tdos quantum;density states;tdos quantum;tdos quantum presence;time dependent density;quantum presence disorder;approximated gaussian distribution;approximated gaussian;dependent density;gaussian distribution;quantum;distribution real line;gaussian distribution paper;line approximated gaussian;distribution real;quantum presence;gaussian;distribution;uniform distribution real;states tdos;uniform distribution;distribution paper propose;density;note uniform distribution;distribution paper;tdos;disorder;time dependent", "pdf_keywords": "depth networks approximated;networks approximated depth;approximated depth networks;sized depth networks;deep networks depth;networks depth networks;size depth network;depth networks;depth network;depth networks weighted;product networks depth;depth networks poly;hilbert spaces deep;networks universal approximators;networks approximated;networks approximated exponential;small depth network;sum product networks;depth network proof;networks depth;neurons large approximated;exponential size depth;neurons exponential expressive;approximated depth;depth network extend;spaces deep feedforward;product networks;required neurons exponential;multilayer feedforward neural;neural networks studied"}, "a1c5af2a531c64f1c06e806d7986cd878ec3c33a": {"ta_keywords": "explanations decision making;explanations decision;human interpretability explanations;impact explanations decision;interpretability explanations;interpretability explanations seldom;model fraud analysts;explanations designed;proposing new explanations;choose explanation financial;generate model explanations;fraud detection task;fraud detection;new explanations designed;model explanations;explanations seldom evaluated;general popular explainers;fraud analysts;fraud analysts research;popular explainers;model fraud;explanations having specific;explanation methods real;real world fraud;model explanations having;explanations seldom;investigate impact explanations;explanation methods;explanations;explanations designed generate", "pdf_keywords": "predictive fraud task;learning explanations decision;machine learning explanations;performance predictive fraud;explanations decision making;predictive fraud;evaluate predictive performance;tasks fraud detection;learning explanations;explanations decision;predictive performance;human performance predictive;evaluate predictive;fraud task;performance predictive;evaluation machine learning;real tasks fraud;methods human decision;human interpretability;explanations promote better;study evaluate predictive;predictive performance machine;fraud detection;human interpretability gap;evaluation methods human;tasks fraud;fraud task paper;explanations promote;human decision making;fraud detection attempt"}, "4fa4e39ade763085a75146392b997b7d4da49725": {"ta_keywords": "contextualized weak supervision;contextualized highly label;weakly supervised text;contextualized highly;new contextualized highly;classification based contextualized;supervision fully contextualized;create contextualized corpus;contextualized corpus;word create contextualized;fully contextualized;contextualized representations word;contextualized;researchers contextualized corpus;new contextualized;fully contextualized existing;contextualized weak;label indicative keywords;supervised text;indicative keywords disambiguates;grained weakly supervised;contextualized corpus utilized;using contextualized weak;contextualized existing;contextualized corpus paper;create contextualized;based contextualized weak;using contextualized;contextualized representations;context free", "pdf_keywords": "contextualized weakly supervised;contextualized weak supervision;harvesting contextualized corpus;contextualized corpus extensive;create contextualized corpus;contextualized corpus;contextualized corpus speci\ufb01cally;novel contextualized weakly;weakly supervised classi\ufb01cation;contextualized representations word;weak supervision text;word create contextualized;interpretations harvesting contextualized;contextualized weakly;weakly supervised;contextualized;occurrences based contextualized;using contextualized weak;contextualized weak;document classi\ufb01cation learning;contextualized representations;using contextualized;supervision text classi\ufb01cation;contextualized representations based;create contextualized;label keywords;based contextualized representations;source weak supervision;propose novel contextualized;harvesting contextualized"}, "462e36e5e296900c80dcd36173340f9c29e36c80": {"ta_keywords": "bayesian model selection;determining tree structure;determining structure tree;tree structure;based variational bayesian;tree structure valid;approach determining tree;determining tree;bayesian approach criterion;variational bayesian;variational bayesian approach;structure tree;bayesian model;propose bayesian model;paper propose bayesian;model selection;bayesian approach determining;bayesian approach;bayesian;criterion based variational;tree;valid determining structure;propose bayesian;structure tree paper;model selection criterion;structure valid determining;selection criterion based;determining structure;tree paper propose;based variational", "pdf_keywords": ""}, "f05741b65a1d644f2fae4c654dae315a7451ee85": {"ta_keywords": "text network exploration;heterogeneous topic web;explore text networks;heterogeneous web topics;text networks;text networks present;topic web text;topic atlas;investigate text network;web topics;life text networks;topic atlas exhibit;topic web demonstrate;metrics text network;topic web;text networks showing;text network;exhibit heterogeneous topic;web text network;topic web quantified;heterogeneous topic;named topic atlas;text network associating;documents represented edges;task text network;web topics allows;explore text;way explore text;relationships heterogeneous topic;constructing heterogeneous web", "pdf_keywords": "heterogeneous topic web;text network exploration;topics wordtopic doctopic;topics wordtopic;topic web relationships;types topics wordtopic;topic web composed;text networks;model heterogeneous topic;exhibit heterogeneous topic;topic web;heterogeneous topic;text networks demand;called heterogeneous topic;related topic groups;available text networks;topicatlas exhibit heterogeneous;idea heterogeneous topic;track related topic;life text networks;topics;topic groups;framework text network;topic web demonstrate;topicatlas;propose probabilistic generative;wordtopic doctopic;named topicatlas;topicatlas exhibit;text network"}, "fe54832083f65eade8e2847627d330a24df22488": {"ta_keywords": "multi channel electroencephalogram;channel electroencephalogram eeg;channel electroencephalogram;electroencephalogram eeg;electroencephalogram eeg signal;electroencephalogram;signal simple electroencephalogram;simple electroencephalogram;electroencephalogram eegg;simple electroencephalogram eegg;eeg signal;electroencephalogram eegg used;eeg signal simple;covariance matrices frequency;noise removal single;noise removal;background noise removal;eeg;estimate covariance matrices;estimate covariance;covariance matrices;related potentials recorded;potentials recorded multi;method estimate covariance;matrices frequency bin;potentials recorded;background noise;covariance;method background noise;event related potentials", "pdf_keywords": ""}, "d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5": {"ta_keywords": "pluriconnected text generation;text generation;architecture text generation;text generation plm;generation complex networks;generate complex networks;text generation important;generated text;pretrained language models;pluriconnected text;language models plm;generation plm generalization;structure network;called pluriconnected text;architecture text;language models;generation single letter;networks;generate;plm architecture text;text called pluriconnected;generation complex;structure used generate;generation plm;network structure network;architecture generation;paradigm pretrained language;properties generated text;complex networks;plm paradigm deep", "pdf_keywords": "text generation relevance;text generation;text generation tasks;text generation researchers;provide text generation;ful\ufb01ll text generation;machine translation summarization;summarization dialogue systems;translation summarization dialogue;machine translation;summarization dialogue;translation summarization;generation tasks plms;properties text generation;language improve generation;generation relevance;generation researchers synthesis;generation tasks;generation relevance faithfulness;summarization;dialogue systems survey;improve generation quality;generation quality;dialogue systems;art machine translation;natural language;improve generation;natural language accurately;generation researchers;dialogue"}, "a2221b03211408ac2db0559b9a54c1d72b5f560c": {"ta_keywords": "transformers music annotation;supervised learning music;learning music annotation;music annotation tasks;music annotation;annotation tasks musicoder;music acoustic data;labeled data musicoder;music annotation critical;musicoder acoustically based;music acoustic encoder;genre classification auto;transformer musicoder;data musicoder;unlabeled music;musicoder acoustically;tasks musicoder acoustically;bidirectional transformers music;models music;learning music;massive unlabeled music;unlabeled music acoustic;music genre classification;based transformers musicoder;bidirectional transformer musicoder;understand music;transformers musicoder outperforms;data musicoder universal;transformer musicoder new;musicoder new self", "pdf_keywords": "music acoustic encoder;music representation;music acoustic data;learns acoustic features;musicoder machine learning;musical representation learning;representation unlabeled music;collect musical representation;music representation unlabeled;powerful music representation;assigning music clip;acoustic encoder based;machine learns acoustic;acoustic encoder;learns acoustic;musical representation;automatically assigning music;descriptions music clip;musicoder acoustics based;masked reconstruction musicoder;trained acoustic frames;acoustic frames musicoder;acoustic features video;assigning music;musicoder machine;training framework speech;acoustic data;unlabeled music acoustic;frames musicoder acoustics;musicoder learn"}, "74e9053d6f44f4507bd40bbea999ee65f0cbefb2": {"ta_keywords": "predicting importance word;iterative word reduction;word reduction methods;word reduction;word reduction method;removes important word;word complexity way;important word input;word complexity;importance word word;word pathologies neural;method predicting importance;word input;art word reduction;iterative word;predicting importance;word word;terms word complexity;important word;importance word;proposed iterative word;word input proposed;neural model predictions;word;word word pathologies;iteratively removes important;method predicting;tuning model interpretable;new method predicting;make difficult predict", "pdf_keywords": "adversarial examples neural;adversarial examples;relationship adversarial examples;neural models overcon\ufb01dence;examples neural models;neural models input;adversarial;examples neural;relationship adversarial;examples input reduction;reduced examples input;visual question answering;explore relationship adversarial;reduces input sentence;generating negative examples;issues neural models;neural models;input reduction uncovers;models input reduction;prediction input reduction;neural models visual;nonsensical reduced examples;approach neural models;unimportant words input;consistently reduces input;words lowering model;reduces reduced examples;examples input;model overcon\ufb01dence input;reduced examples"}, "01a21d74fb7414404851872f23cdca42243ab6a8": {"ta_keywords": "training convolutional;dukemtmc reid datasets;training convolutional neural;networks cnn;reid datasets;cnn;batch related convolutional;trained feature extraction;transfer learning;neural networks cnn;used transfer learning;pre trained feature;trained feature;networks cnn based;cnn based;cnn based use;person identification reid;mini batch training;method training convolutional;reid datasets paper;progressive transfer learning;identification reid applications;transfer learning widely;convolutional neural networks;convolutional neural;batch training;improve reid model;reid model performance;batch training propose;identification reid", "pdf_keywords": "batch related convolutional;progressive transfer learning;learning method batch;transfer learning;benchmark datasets demonstrate;trained model;data distribution training;trained model train;training data;distribution training;dukemtmc reid datasets;image classi\ufb01cation benchmark;pre trained model;transfer learning pt;classi\ufb01cation benchmark datasets;lifelong learning method;bias mini batch;batch improve;benchmark datasets;lifelong learning;convolutional cell;novel lifelong learning;related convolutional cell;mini batch improve;trained model integrating;learning pt network;distribution training data;batch improve stability;learning;training data propose"}, "c2dd1c332f65fea3a66f4a982428f31ce1a9dc70": {"ta_keywords": "web search engine;exempliied search engines;text search engines;information retrieval;web based information;information retrieval model;search engine integrates;eecient search engines;search engines;retrieval model exempliied;search engine;called information retrieval;like information web;retrieval model;information retrieval ir;web search;information sources easily;information multiple web;search engines like;information web;models information sources;knowledge integration systems;database like information;showing information sources;information sources common;information web whirl;knowledge integration;sources easily modeled;current knowledge integration;information sources", "pdf_keywords": ""}, "eca07d2b351d81719b33c913a87c63d6930ee7f5": {"ta_keywords": "carbon nanotube cnt;walled carbon nanotube;nanotube cnt reduced;carbon nanotube;nanotube cnt;nanotube;single walled carbon;cnt reduced;cnt reduced factor;walled carbon;failure rate single;compared case cnt;rate single walled;case cnt;cnt;failure rate;carbon;single walled;xmath0 compared case;xmath0 compared;reduced factor xmath0;factor xmath0 compared;letter failure rate;factor xmath0;failure;rate single;xmath0;walled;reduced factor;reduced", "pdf_keywords": ""}, "3a95fab610d5ff49fbdb7a4d8760b02c51df0013": {"ta_keywords": "anonymizing clinical notes;privacy clinical notes;improve privacy clinical;secure clinical texts;privacy clinical;anonymizing clinical;technique anonymizing clinical;differential privacy;improve privacy;tasks introduce privacy;differential privacy demonstrate;differential privacy used;anonymization;paper differential privacy;privacy;private health information;privacy technique anonymizing;based differential privacy;anonymization technique;introduce privacy;anonymizing;sensitive data family;privacy used improve;notes guarantees private;technique anonymizing;privacy technique;privacy used;privacy demonstrate utility;sensitive data;introduce privacy technique", "pdf_keywords": "anonymizing clinical texts;automatically anonymizing clinical;anonymizing patient consultation;anonymizing clinical;data use nlp;automatically anonymizing;clinical texts;anonymizing patient;texts clinical notes;method anonymizing patient;clinical notes rich;nlp research;use nlp;method automatically anonymizing;clinical texts clinical;novel method anonymizing;neural networks clinical;texts clinical;patient consultation notes;recurrent neural networks;clinical notes;notes rich information;consultation notes based;anonymizing;tokens;use nlp research;learning classification tasks;based use recurrent;method anonymizing;use recurrent neural"}, "76b95833fd0e242896d231abdea8dc01a167c7a6": {"ta_keywords": "gaussian process inference;sparse gaussian process;gaussian process regression;gaussian process prior;process prior drift;gaussian processes;method gaussian processes;gaussian process;propose gaussian process;using gaussian process;estimation drift markov;gaussian processes develop;based sparse gaussian;sparse gaussian;bayesian approach estimation;observations using gaussian;prior drift function;drift markov chain;drift markov;unobserved latent dynamics;prior drift;inference method gaussian;estimation drift;process inference;regression propose gaussian;approach estimation drift;process inference method;latent dynamics observations;process prior;markov chain based", "pdf_keywords": "modeling paleoclimatic changes;approach modeling paleoclimatic;modeling paleoclimatic;estimating drift function;estimating drift;gaussian process prediction;dynamical polynomial drift;variational approach modeling;model gaussian process;polynomial drift function;inference sparse observations;heuristic estimating drift;drift function;drift function double;variational approach;polynomial drift;propose variational approach;drift function paper;sparse observations;gaussian process;inference sparse;approximate expectation maximization;model gaussian;observation models widely;double model gaussian;process prediction observation;prediction observation models;method inference sparse;paper propose variational;prediction observation"}, "a660429b77e932af1c1d7d3f0554f4b17c044082": {"ta_keywords": "characteristics terrorist groups;terrorist groups related;similar terror groups;groups participating terrorist;terrorist groups results;islamist jihadist groups;terrorist groups;investigate characteristics terrorist;number terrorist groups;network includes terrorist;includes terrorist groups;terror groups;clusters similar terror;jihadist groups;terror groups using;terrorist actions investigating;terrorist attacks;terrorist attacks occurred;jihadist groups hold;characteristics terrorist;behaviors islamist jihadist;participating terrorist actions;data terrorist attacks;complex networks based;terrorist actions;complex networks;common behaviors islamist;analysis complex networks;includes terrorist;behaviors islamist", "pdf_keywords": "terrorism comparisons entropy;patterns terrorist global;terrorist network;partite terrorist network;terrorist network spite;research terrorism comparisons;terrorism comparisons;patterns terrorist;frameworks research terrorism;jihadist terrorism united;terror groups;islamist jihadist terrorism;research terrorism;population terror groups;jihadist terrorism;terrorism united;terror groups active;hidden patterns terrorist;terrorism united states;terrorism;multi partite terrorist;terrorist global;terrorist global scenario;relationship islamist jihadist;islamist jihadist;population terror;known population terror;nationalist groups;partite terrorist;terrorist"}, "8512718bafa447f9b433da9e809215dfc28b6b28": {"ta_keywords": "predict performance nearest;grained performance prediction;reliability performance prediction;performance prediction;performance prediction methods;predict performance;analysis performance prediction;performance prediction task;performance prediction necessity;performance predictors;calibration performance prediction;performance measures accuracy;performance prediction model;performance nearest;performance predictors holistic;estimating performance;approach predict performance;estimating performance performing;performance nearest neighbor;examine performance predictors;grained performance measures;task estimating performance;reliability analysis performance;understand reliability performance;reliability performance;prediction methods future;prediction task estimating;performance measures;examine performance;prediction necessity perform", "pdf_keywords": "performance prediction nlp;prediction nlp tasks;prediction nlp;improving performance prediction;grained evaluation metrics;predicted performance;performance prediction;task machine translation;performance prediction models;predicted performance paper;machine translation;performance prediction scenarios;nlp tasks;machine translation mt;consider performance prediction;reliability performance prediction;grained evaluation;tensor regression;tensor regression based;\ufb01ne grained evaluation;evaluation metrics;predicting reliability performance;nlp tasks task;interval predicted performance;measuring performance scenarios;large parallel corpora;predicting \ufb01ne grained;natural language processing;prediction scenarios holistic;measuring performance"}, "84702b091af8842b6bbe457e5435c343a9824693": {"ta_keywords": "learn noisy environment;animals able learn;learn noisy;model behavior animals;able learn noisy;behavior animals able;behavior animals;noisy environment;animals;noisy;animals able;simple model behavior;simple model;model behavior;behavior;able learn;environment;present simple model;model;learn;simple;present simple;paper;paper present simple;paper present;present;able", "pdf_keywords": ""}, "54e7209e692ca4f5c85f0e68df34040b3cfa8bad": {"ta_keywords": "low coded computation;distributed computation matrix;matrix coded computation;coded computation;distributed computing scheme;distributed computation;coded computation introduce;method distributed computation;sparsity encoded matrix;coded computation framework;distributed computing;zeros reduce computational;propose distributed computing;redundancy distributed computing;encoded matrix coded;computation matrix multiplication;matrix coded;reduce computational overheads;computation introduce large;encoded matrix;computation matrix;distributed computing systems;low coded;computational overheads;relatively low coded;long dilation matrices;computational overheads maintaining;reduce computational;bound sparsity propose;dilation matrices optimal", "pdf_keywords": ""}, "1acbfc7d3e245bd3146e9e24eae7550aa2d03482": {"ta_keywords": "activation matrices neural;rank breathe normalization;matrices neural;neural network directly;breathe normalization;matrices neural network;network topology training;neural networks paper;topology training performance;neural network;deep neural;activation matrices;breathe normalization bn;pre activation matrices;train deep neural;topology training;neural;neural networks;deep neural networks;normalization;networks;train deep;rank breathe;training performance understood;rank pre activation;network topology;training performance;related rank breathe;dumbbell like trap;normalization bn", "pdf_keywords": "initialization deep networks;random initialization deep;normalized convolutional neural;initialization deep neural;batch normalized convolutional;normalized convolutional;deep networks spectral;initialization deep;optimization performance deep;deep feedforward;direction random initialization;performance deep feedforward;deep networks;networks recover training;networks initialized;deep neural;performance deep;deep neural networks;deep feedforward neural;convolutional neural networks;networks spectral instabilities;deep learning;random initialization;networks spectral;convolutional neural;batch normalized;random matrices;learning based invariance;normalized;new approach deep"}, "796f29cee975603c7a1469df1eb21ed5142ecff5": {"ta_keywords": "literary evidence retrieval;retrieval complex literary;retrieval baselines humanities;lexical semantic similarity;task literary evidence;semantic similarity matching;information retrieval baselines;literary linguistic;complex literary linguistic;semantic similarity;evidence retrieval;literary linguistic phenomena;dataset 78 literary;evidence retrieval models;literary evidence;information retrieval;excerpt literary analysis;literary quotations use;retrieval models;literature claim;literary quotations;78 literary quotations;retrieval models given;retrieval baselines;literature;literature claim statistically;task literary;pretrained information retrieval;literary analysis;retrieve quoted passage", "pdf_keywords": "retrieval literary quotes;retrieval literary;database literary quotes;literary quotation embeddings;large database literary;passage retrieval;quotations collecting largescale;retrieving evidence literary;model retrieval literary;passage retrieval guu;dense passage retrieval;embedding scholarly claims;quotations collecting;quotations pretrained retriever;quotation embeddings;database literary;quotation embeddings use;quotes primary sources;scholarly excerpts;literary quotations pretrained;deep neural retrieval;scholarly excerpts directly;literary claims quotations;excerpts directly quote;neural retrieval model;literary claims relic;78k scholarly excerpts;embedding scholarly;claims quotations collecting;candidate literary quotations"}, "d10e410765699a75628a1437b93f0d0fc3dc0aa6": {"ta_keywords": "semi supervised learning;semi supervised;algorithm semi supervised;labeled seed instances;learning propagating labels;labeled instances training;labels labeled seed;fully supervised algorithms;labeled seed;supervised learning propagating;unlabeled instances graph;seeds large datasets;labeled instances;seed instances unlabeled;propagating labels labeled;ranking seeds large;ranking seeds;fully supervised;supervised learning;supervised;rank ranking seeds;easy obtain labels;algorithms require labeled;label best known;labeled;high rank ranking;highly accurate classification;propagating labels;generating high rank;training data instances", "pdf_keywords": ""}, "48aa33ad92566cb60ef348ffa438e4712f618b03": {"ta_keywords": "occlusal transillumination reflectance;infrared swir reflectance;swir reflectance occlusal;reflectance occlusal transillumination;reflectance measured;swir reflectance;transillumination reflectance depth;transillumination reflectance;cross polarization reflectance;transillumination images lesions;measured occlusal transillumination;reflectance occlusal;reflectance measured function;reflectance depth measurements;reflectance depth;polarization reflectance;reflectance dielectric;occlusal transillumination images;sensor reflectance measured;lesions swir images;polarization reflectance dielectric;permittivity sensor reflectance;images lesions tooth;sensor reflectance;developed clinical probe;reflectance;clinical probe capable;clinical probe;transillumination transmittance lesion;swirir occlusal transillumination", "pdf_keywords": ""}, "3fb78bee6cb39588a1a4cbb4e0abce5e362aa130": {"ta_keywords": "optimized mirror descent;mirror descent;improved regret bounds;adversarial high;mirror descent emerged;completely adversarial high;regret bounds;completely adversarial;adversarial;easier loss sequences;optimized mirror;provide regret bounds;sequence completely adversarial;regret bounds obtained;adversarial high level;regularization;mirror;regret bounds terms;loss sequences provide;loss sequences;high dimensional data;bounds obtained loss;easy data careful;data careful choices;loss sequence;data careful;dimensional data sets;adapt easier loss;obtained loss sequence;loss sequence completely", "pdf_keywords": ""}, "e6ffeb4b9d808d6c9b8d388a7cbb431ac96bf194": {"ta_keywords": "fluid dynamics video;rigid body action;external force controlled;motion rigid body;motion rigid;fluid dynamics;force controlled;force controlled adjusting;dynamics video;rigid body;controlled adjusting position;dynamics;video motion rigid;action external force;dynamics video motion;adjusting position body;external force;motion;position body center;adjusting position;position body;controlled adjusting;rigid;video motion;body action external;force;center mass;body action;body center mass;body center", "pdf_keywords": ""}, "99053e3a708fc27709c9dab33110dc98b187c158": {"ta_keywords": "reasoning reasoning crowds;reasoning knowledge robust;annotate gold reasoning;gold reasoning programs;trained financial models;reasoning powerful tool;reasoning programs;reasoning crowds introduce;finance knowledge complex;finance knowledge;numerical reasoning knowledge;reasoning crowds;acquiring finance knowledge;humans acquiring finance;robust numerical reasoning;deep questions financial;reasoning knowledge;numerical reasoning powerful;gold reasoning;large corpus financial;financial data aiming;financial data;questions financial data;knowledge robust numerical;automate analysis large;reasoning reasoning;knowledge robust;reasoning programs ensure;reasoning understanding;approach reasoning reasoning", "pdf_keywords": "learning reasoning paths;learning learns answer;learning reasoning;reasoning programs;learning framework answerability;learns answer;programs answer questions;reasoning programs answer;fail learning reasoning;retrieve supporting facts;learns answer \ufb01nancial;answerability transparency;reasoning paths;executable reasoning programs;reasoning processes;answerability;programs answer;machine learning learns;answerability transparency \ufb01nancial;text mining increasingly;reasoning paths conduct;generate executable reasoning;answer questions fintabnet;reasoning processes propose;framework answerability transparency;answer questions;learn program structures;learning learns;text mining;framework answerability"}, "0acbdcac9edf74cc2c1e98bd59e301c9300977d0": {"ta_keywords": "sequence tagging tasks;sequence tagging;crowd annotations;annotating large annotators;large annotators;rapid tagging;annotations real time;annotation framework;crowd annotations real;consolidation crowd annotations;annotation framework sequence;sequence labeling tasks;annotating annotating large;new approach annotating;annotations;annotating;annotating annotating;framework sequence tagging;annotators;data annotation framework;data annotation;annotation;approach annotating annotating;tagging;rapid tagging recommendations;annotating large;sequence labeling;tagging tasks;approach annotating;ranging rapid tagging", "pdf_keywords": ""}, "ee2e171d6a897ee5d0b0bde2d5f2548b52d3a840": {"ta_keywords": "tutoring meta cognitive;students meta cognitive;student meta cognitive;effective tutoring meta;tutoring meta;meta cognitive skills;meta cognitive help;students learn cognitive;meta cognitive;meta cognitive cognitive;effective tutoring;meta cognitive framework;employ meta cognitive;cognitive help;skills effective tutoring;cognitive skills;cognitive skills effective;performance meta cognitive;students meta;effect meta cognitive;help students learn;learn cognitive;learn cognitive skills;tutoring;student meta;cognitive help context;post test tutored;assistant anti cognitive;results students meta;tutored sim student", "pdf_keywords": ""}, "9633928f72cda45d102fb6740291d47137d0a5ca": {"ta_keywords": "attack federated learning;backdoor attacks training;malicious data training;training phase malicious;attacks training;federated learning systems;mitigate backdoor attacks;federated learning;model federated learning;attacks training phase;backdoor attacks;prediction attack;prediction attack success;stage federated learning;model recognition security;mitigate backdoor;security human navigation;method mitigate backdoor;subset data backdoor;malicious data;data backdoor;systems using malicious;data training;attack federated;data backdoor patterns;navigation systems validation;backdoor patterns;recognition security;clients attack federated;malicious clients attack", "pdf_keywords": "attacks federated learning;backdoor attacks federated;federated learning;federated pruning methods;called federated learning;pruning neurons client;training sharing datasets;federated pruning;federated learning enable;attacks federated;mitigate backdoor attacks;distributed way pruning;propose federated pruning;backdoor attacks;way pruning neurons;backdoor attacks experiments;private dataset solve;private dataset;pruning neurons;clients private dataset;model backdoor attacks;federated model backdoor;novel machine learning;track attack success;attacks experiments;model training sharing;neuron pruning;able track attack;mitigate backdoor;reduce average attack"}, "7731e3dec97c48498b585408d44615346ade144a": {"ta_keywords": "language variation trends;social group language;language variation;glossaries draw sociolinguistic;language similarity used;language similarity;language variation internet;group language deviates;variation internet social;sociolinguistic theories;groups communities highly;sociolinguistic;connect language variation;draw sociolinguistic theories;groups communities;sociolinguistic theories connect;group language;communities highly distinctive;used groups communities;draw sociolinguistic;internet social groups;trends community behavior;distinctive language medium;community behavior;communities;language deviates;communities highly;xmath0 language similarity;work language variation;language deviates norm", "pdf_keywords": "variation reddit communities;reddit communities;communities reddit;communities reddit subreddit;patterns subreddit communities;identify communities reddit;subreddit usage patterns;subreddit communities;usage patterns subreddit;semantic variation reddit;language community;subreddit glossaries word;subreddit glossaries;subreddit communities strongly;subreddit subreddit glossaries;communities distinctive language;specific word subreddit;reddit communities users;word subreddit;communities discuss;word sense induction;communities discuss particular;language community particular;correlated subreddit usage;networks word sense;word usage patterns;patterns subreddit;community speci\ufb01c language;communities users;glossaries subreddit"}, "b116e5044fe047fc48307795af1f3e11b3a9401c": {"ta_keywords": "statistical machine translation;machine translation bsm;machine translation;machine translation terms;alignments using variational;word alignments;word alignments using;bayesian statistical machine;gradient gpa bayesian;problem word alignments;moses machine translation;translation bsm problem;dynamics semiflexible polymers;translation bsm;bayesian statistical;model parameters swimmer;gpa bayesian;polymers contact heat;gpa bayesian approaches;variational bayes improves;semiflexible polymers contact;statistical machine;translation terms overall;variational bayes;translation terms;apply bayesian statistical;heat bath modeled;semiflexible polymers;using variational bayes;parameters swimmer moves", "pdf_keywords": ""}, "4b18303edf701e41a288da36f8f1ba129da67eb7": {"ta_keywords": "zero shot learning;zero shot learning_;shot learning;shot learning approach;domain adaptation;shot learning consists;shot learning_;shot learning based;shot learning_ sophisticated;layers network learned;domain adaptation methods;idea zero shot;networks weights layers;network learned;approach zero shot;weights layers network;zero shot;network learned given;networks weights;complex networks weights;casting domain adaptation;networks;learning_ sophisticated approaches;environment zero shot;layers network;learning bound generalisation;learning recognise new;datasets present simple;learning;learning based idea", "pdf_keywords": "zero shot learning;shot learning semantic;adaptation zero shot;shot learning;domain adaptation zero;shot learning based;shot learning provide;domain adaptation;shot learning zsl;approach zero shot;zero shot;shot learning machine;adaptation domain adaptation;study zero shot;learning semantic visual;domain adaptation domain;domain adaptation methods;semantic visual attributes;comprised zero shot;learning semantic;methods domain adaptation;adaptation domain;adaptation methods domain;casting domain adaptation;adaptation zero;training inference;ef\ufb01cient training inference;description leveraging;semantic visual;visual attributes"}, "b29dd2c50da0dc4589eafac58007f6be7e13c501": {"ta_keywords": "modeling indoor scenes;modeling indoor;indoor scene understanding;geometric modeling indoor;geometry indoor environments;helpful indoor scene;indoor scene;3d geometry indoor;objects helpful indoor;identify objects kitchen;indoor scenes;geometry indoor;indoor environments;scene bayesian geometric;identifying scene bayesian;objects kitchen;room layout estimation;indoor scenes powerful;scene bayesian;bayesian geometric modeling;identify objects;modeling geometry location;simultaneously identifying scene;identifying scene;modeling geometry;room layout;scene understanding;location specific objects;recognition based geometry;kitchen simultaneously identifying", "pdf_keywords": ""}, "873b83326ad1f98549beb85bdb130a40a61e1f9b": {"ta_keywords": "ranking string probability;highest probability answers;multiple choice tasks;scoring functions ranking;choice tasks;choice tasks simply;forms compete probability;answer highest probability;scale language models;scoring functions;uncalibrated scoring functions;shot performance gpt;highest probability answer;choice datasets improved;probability problematic surface;large scale language;probability answers;language models;introduce scoring;compete probability mass;ranking string;ranking;scoring function directly;surface forms compete;zero shot performance;highest probability;probability answers question;choice datasets;selecting answer highest;perform multiple choice", "pdf_keywords": "question answering;generative language models;predict answer choice;book question answering;question answering paper;generative language;language models know;language models;unsupervised semantic plausibility;unsupervised semantic;generative models;probabilities contextual;domain generative language;know language models;generative models used;propose generative model;predicting probability;raw probabilities contextual;answering paper;generative;language models used;propose generative;predicting probability speci\ufb01c;generative model;semantic plausibility;method unsupervised semantic;particular generative models;predict answer;answering;uncertainty know language"}, "9fe579e54712ba82c4f1c93e46409613f592df16": {"ta_keywords": "eigenvalue problem large;hermitian matrices;solution eigenvalue problem;class hermitian matrices;eigenvalue problem;matrix good approximation;exact solution eigenvalue;large class hermitian;solution eigenvalue;xmath0 matrix good;xmath0 matrix;eigenvalue;hermitian;class hermitian;matrices;matrix good;matrix;approximation exact solution;xmath0;good approximation exact;approximation exact;good approximation;approximation;problem large class;exact solution;large class;solution;problem large;problem;class", "pdf_keywords": ""}, "178d51c35c03e3ccaae2409c32a3c2001cefe7eb": {"ta_keywords": "incremental estimation;incremental model adaptation;incremental estimation algorithm;derive incremental estimation;kalman filter algorithm;kalman filter;filter algorithm posterior;incremental model;model adaptation approach;model adaptation;algorithm posterior distributions;based posterior distributions;new incremental model;solution kalman filter;posterior distributions model;binary mixture model;model derive incremental;bayesian approaches;mixture model derive;evolutions posterior distributions;mixture model;bayesian;propose bayesian;estimation algorithm based;propose bayesian approach;conventional bayesian;conventional bayesian approaches;algorithm posterior;bayesian approach;estimation algorithm", "pdf_keywords": ""}, "415d4231cab5ddee73e2ed536d033d5c31f24b4a": {"ta_keywords": "information extraction biomedical;extracting semantic information;entity recognition biomedical;extracting semantic;semi supervised bootstrapping;named entity recognition;information extraction;supervised bootstrapping;supervised bootstrapping approach;ontology seeds automatically;method extracting semantic;semantic information biomedical;entity recognition;open information extraction;extraction biomedical text;ontology seeds;text extraction web;learning terms biomedical;extraction web;biomedical text extraction;recognition biomedical entities;information biomedical text;initial ontology seeds;seeds ontology;bootstrapping approach learn;seeds ontology category;new bootstrapping algorithm;text extraction;learning terms;using learned lexicon", "pdf_keywords": ""}, "8163c4010fc103343518d49db5974577593972f6": {"ta_keywords": "automatic deception detection;deception detection;higher deception detection;deception detection used;deception detection accuracy;detect deception;automatic deception;deception propose dialogue;deceptive corpus;attempt detect deception;deceptive corpus japanese;collect deceptive corpus;detect deception perform;deceptive conversational partner;deceptive conversational;paper automatic deception;deception asking questions;deception salient humans;deception perform actions;features deception;features deception salient;deception asking;deception;unveil deceptive conversational;learn signs deception;deception propose;perform higher deception;catch potential liar;deception perform;signs deception asking", "pdf_keywords": ""}, "fa5c7406d09af3f06a3a7ead49975e3ee90ed584": {"ta_keywords": "share autonomy human;shared autonomy explains;shared autonomy;humanrobot language;autonomy human;agents share autonomy;viewing humanrobot language;robotic agents share;autonomy explains efficiency;communicating robot;autonomy explains;make instructions robotic;instructions robotic agents;share autonomy;communicating robot dramatically;humanrobot;environment communicating robot;humanrobot language lens;autonomy;viewing humanrobot;robotic agents;robot;instructions robotic;crowded environment communicating;robotic;lens shared autonomy;communicating robot taxing;autonomy human leverage;robot dramatically;dynamics human", "pdf_keywords": "robots language communication;robots language;human robot collaboration;interact robots language;robot collaboration;robot collaboration context;shared autonomy robots;task robot butler;task robot;understanding robot needs;robots humans interact;robot butler knowledge;shared autonomy agents;humans interact robots;improve understanding robot;robot needs workload;robot needs;butler knowledge robotic;human robot need;interact robots;autonomy robots;robots humans;robot need;understanding robot;knowledge robotic;individual strengths robots;robot butler;robot need handle;strengths robots humans;robot"}, "9527352b925f9fa36c40966ed755afd22301b0aa": {"ta_keywords": "preferences ethical principles;preferences ethical;preferences decision makers;important preferences ethical;preferences decision maker;decision based preferences;decision ethical complies;ethical complies constraints;decision ethical;subjective preferences decision;decision makers properties;preferences optimization criteria;preferences decision;preferences outcomes;principles measure preferences;subjective preferences;preferences outcomes paper;complies constraints priorities;evaluate preferences;constraints decision context;constraints priorities;impact decision ethical;ethical principles measure;method evaluate preferences;constraints priorities given;set ethical principles;based preferences optimization;ethical moral norms;override subjective preferences;measure preferences", "pdf_keywords": ""}, "74c80622b91894efbe4ae9ce1428e4d699b05516": {"ta_keywords": "distributed convex optimization;methods distributed convex;dual stochastic gradient;stochastic gradient oracle;stochastic gradient;distributed convex;dual stochastic;primal dual stochastic;convex optimization;convex optimization problems;point solution stochastic;method rate convergence;gradient oracle methods;solution stochastic differential;solution stochastic;stochastic differential;gradient oracle;oracle methods distributed;iteration sequence optimal;stochastic differential equation;methods distributed;optimization problems networks;convergence iteration;convergence terms duality;stochastic;rate convergence;analysis convergence iteration;optimal point;sequence optimal point;convergence iteration sequence", "pdf_keywords": ""}, "c00e4564ea054c14c83cb564af6c37e47c8ab367": {"ta_keywords": "sensor activation rate;estimation constraint sensor;activation rate estimation;activation rate markov;sensor sensor activation;estimating activation rate;estimation based lagrange;sensor activation;sensor sampling;constraint sensor activation;sensor sampling comparable;uniform sensor sampling;active sensor subset;sensor subset tracking;complete sensor observation;bound complete sensor;electromagnetics em algorithm;rate markov chain;sensor subset;estimating activation;em algorithm;performance uniform sensor;sensor observations;active sensor;mse estimation constraint;rate estimation based;dimension sensor observations;rate estimation;sensor observation;complete sensor", "pdf_keywords": "centralized tracking markov;optimal sensor subset;selection sensor networks;complexity active sensor;state estimation smart;sensor subset minimizes;activating optimal sensor;tracking markov chain;sensor networks active;networks active sensor;active sensor selection;estimation constraint sensor;optimal sensor;state estimation;sensor selection algorithm;tracking markov;active sensor subset;sensor networks;sensor subset selected;control state estimation;sensor subset;estimation smart grid;consider sensor selection;subset selected tracking;em state estimation;estimation smart;sensor selection;algorithm centralized tracking;consider centralized tracking;state estimation dominated"}, "98ef0db84e62aef969629264c9de1f4d0013f3b9": {"ta_keywords": "learned multiple tasks;carbon nanotubes swnts;nanotubes swnts empirically;carbon nanotubes;multi task learning;nanotubes;nanotubes swnts;task learning;knowledge multiple tasks;walled carbon nanotubes;learned multiple;task learning methods;tuning multi task;task learning sequential;knowledge composition classifier;representations learned multiple;exploit representations learned;composition classifier effectively;multiple tasks;multi task;composition classifier;representations learned;learning;catastrophic forgetting;catastrophic forgetting difficulties;learning algorithm leverages;suffer catastrophic forgetting;design fabrication single;learned;fabricate adapters", "pdf_keywords": "task transfer learning;multi task learning;multi task transfer;transfer learning adapters;task learning separating;tuning multi task;transfer learning;multi task;task learning;goal transfer learning;propose multi task;task transfer;approach multi task;trained different tasks;learning adapters;task learning catastrophic;pitfalls multi task;task learning approach;transfer learning pan;task learning paper;problem multi task;learning adapters able;information adapters trained;knowledge relevant task;task adapterfusion;target task adapterfusion;tasks;different tasks;target task;supports target task"}, "43953a051b6518f32fc37734cfc49942baeac5a1": {"ta_keywords": "spectral parameters utterances;spectral variation utterances;statistical voice conversion;speech spectral variation;sample converted speech;conversion statistical voice;speech spectral;converted speech speech;input speech spectral;speaker spectral variation;converted speech;statistical voice;generate original prosodic;predicting prosodic parameter;voice conversion;speech speech sample;speech sample;predicting prosodic;prosody input speech;original prosodic features;method predicting prosodic;voice conversion technologies;prosodic features input;speaker spectral;prosodic features;intra speaker spectral;speaker imitating prosody;parameter differences utterances;imitating prosody input;variation utterances", "pdf_keywords": ""}, "4077c1986f32817801b3082ce8dde514424f71a1": {"ta_keywords": "crowdsourcing synset cleansing;quality synsets thesaurus;crowdsourcing synset;synsets thesaurus created;synsets thesaurus;workflow crowdsourcing synset;experts crowdsourcing crucial;crowdsourcing crucial;crowdsourcing crucial resource;synset cleansing;experts crowdsourcing;thesaurus created collaboratively;non experts crowdsourcing;quality synsets;synset quality;synset quality evaluated;novel workflow crowdsourcing;crowdsourcing;crowdsourcing approach;crowdsourcing approach showing;workflow crowdsourcing;evaluated using crowdsourcing;using crowdsourcing;synset;using crowdsourcing approach;thesaurus created;synsets;high quality synsets;collaboratively non experts;thesaurus", "pdf_keywords": ""}, "2344cca985dd4e2e2519838b2353b5c295e73036": {"ta_keywords": "machine comprehension model;neural machine comprehension;machine comprehension;comprehension model evaluated;comprehension model;ai2reasoning challenge arc;ai2reasoning challenge;retrieval methods return;present arc corpus;sentences irrelevant answering;return sentences;sentences human selected;return sentences irrelevant;arc corpus short;arc corpus;comprehension;naive information retrieval;competition ai2reasoning challenge;ai2reasoning;retrieval;methods return sentences;information retrieval;2017 competition ai2reasoning;irrelevant answering query;human selected sentence;sentences human;demonstrate naive information;challenge arc report;information retrieval methods;corpus", "pdf_keywords": "question types knowledge;knowledge types reasoning;machine comprehension dataset;comprehension dataset;types knowledge reasoning;natural language questions;answer question types;machine comprehension;question types;knowledge reasoning;reading speci\ufb01c knowledge;speci\ufb01c knowledge corpus;reasoning types;train machine comprehension;knowledge reasoning required;knowledge types;various types knowledge;types knowledge human;types knowledge;knowledge corpus;speci\ufb01c knowledge;knowledge human need;dataset natural language;importance answering questions;types knowledge types;questions arc dataset;answer various questions;comprehension dataset contains;knowledge human;various questions"}, "05fb1eea6381ccd21bde53495c7707546aa234c7": {"ta_keywords": "memory bilstm neural;entanglement single quantum;entanglement single;entanglement;short term memory;paper entanglement single;single quantum;paper entanglement;bilstm neural;term memory bilstm;quantum;term memory;memory bilstm;bilstm neural network;comprehensive word representations;representations multi channel;memory present results;single quantum enhanced;word representations;memory present;word representations multi;memory;neural;shared memory;shared memory present;quantum enhanced;multi channel information;neural network;presence shared memory;representations", "pdf_keywords": ""}, "3132a18a441ab6067066e4d4d85608b058c9ed33": {"ta_keywords": "change point detection;change points time;detection alarms;predictive models;detection alarms use;predictive models used;autoencoder based;predictive predictive models;autoencoders learn;false detection alarms;predictive predictive;autoencoders;predict;autoencoder based methodology;models used predict;changes time series;crowd sudden changes;predict response crowd;changes time;predictive;used predict;used autoencoders learn;autoencoder;based changes time;employ autoencoder based;change points;point detection cpt;change point score;time art predictive;time invariant", "pdf_keywords": "peak detection based;peak detection;time invariant features;time series data;frequency invariant features;propose autoencoder based;time frequency invariant;detection based autoencoders;changes time series;based autoencoders;rock strati\ufb01cation outliers;autoencoder based loss;novel autoencoder based;propose novel autoencoder;propose autoencoder;autoencoder based;time series analysis;time series;overestimated propose autoencoder;mean time series;time series correspond;importance time series;invariant features prediction;data streams;method peak detection;novel autoencoder;autoencoders;features prediction changes;time frequency;use time frequency"}, "91e605a125f64207a242693d0dc1c862080f6c27": {"ta_keywords": "learning speech phoneme;automatic phoneme recognition;phoneme recognition;speech phoneme lattices;phoneme lattices translations;manual phonemic transcription;improve automatic phoneme;phoneme recognition method;phonemic transcription;speech transcribed experiments;automatic phoneme;phonemic transcription possible;language manual phonemic;translations speech transcribed;speech phoneme;lattices translations speech;learning speech;bilingual lexical entries;slow manual transcription;bilingual lexical;useful bilingual lexical;experiments demonstrate phoneme;speech transcribed;phoneme lattices;translations improve automatic;manual transcription;manual transcription possible;demonstrate phoneme;translations speech;language easily acquired", "pdf_keywords": ""}, "b953a582cc79c33054b295c20c1201e8d5bd8243": {"ta_keywords": "intelligent tutoring;intelligent tutoring systems;student models;quantum information processing;processing based entanglement;instruction intelligent tutoring;students interact fluid;finds student models;student models using;tutoring systems;student model;tutoring;new approach quantum;tutoring systems propose;quantum;learning related problems;approach quantum information;entanglement;approach quantum;good student model;student model model;students;students interact;entanglement swapping quantum;human students interact;based entanglement;student behavior patterns;quantum information;based entanglement swapping;swapping quantum", "pdf_keywords": ""}, "d9d0d908e3f652ee350f4919d4c2ab972ada1ca4": {"ta_keywords": "useful coreference models;coreference systems underperform;coreference core nlp;coreference models;coreference systems;existing coreference data;state art coreference;future coreference research;active learning annotation;text data coreferences;learning annotation pipeline;coreference models coreference;coreference data;coreference research;coreferences entities;useful coreference;coreference research primary;existing coreference;maximally useful coreference;learning annotation;data coreferences;data coreferences entities;coreferences entities paper;coreferences;coreference data lack;core nlp;models coreference core;coreference core;data future coreference;coreference", "pdf_keywords": ""}, "f18ec4e0bce2e4d847954c9692959d88ba8a9b66": {"ta_keywords": "secure estimates vehicle;malicious compromised vehicles;vehicle v2x communications;compromised vehicles goal;secure remote estimation;ensure security sensing;security sensing;compromised vehicles;security sensing systems;cooperative intelligent transport;v2x alliances security;estimate security;security v2x alliances;security v2x;ability estimate security;analysis security v2x;observations enabling secure;enabling secure estimates;vehicles goal;security analysis security;estimates vehicle v2x;systems v2x networks;malicious observations enabling;estimate security parameters;v2x networks systems;v2x communications essential;security analysis;alliances security analysis;v2x communications;ensure security", "pdf_keywords": ""}, "5e27712db641bc8f16c510292f7fd5440acd563d": {"ta_keywords": "stacked graphical learning;learning stacked graphical;classification collective classification;collective classification widely;collective classification based;collective classification;collective classification collective;approach collective classification;classification collective;machine learning stacked;learning stacked;concept collective classification;collective classification usually;collective classification powerful;expensive collective classification;used collective classification;instances stacked graphical;related instances stacked;graphical learning;graphical learning meta;classification relational datasets;classification relational;propose stacked graphical;stacked graphical;instances stacked;used classification relational;meta learning scheme;graphical learning stp;classification widely;learning meta learning", "pdf_keywords": ""}, "73e868f74376814a4c08eca6ce043fe7c7aefeed": {"ta_keywords": "personalized pagerank measure;similarity nodes graph;personalized pagerank;measure similarity graphs;similarity nodes;similarity measure graph;personalized pagerank widely;fields personalized pagerank;graphs based similarity;based similarity nodes;pagerank measure;similarity graphs;pagerank;tree structured markov;using personalized pagerank;pagerank measure paper;pagerank widely;similarity graphs based;probabilities tree structured;mrfs computation similarity;similarity vertices graph;nodes based graphwalks;pagerank widely used;structured markov random;measure graph nodes;graph nodes based;marginal probabilities tree;similarity measure;graph nodes;based graphwalks", "pdf_keywords": ""}, "f6160c3196288b9e435dc6f86024f56e6b5ab722": {"ta_keywords": "utilitarian maximal allocations;computing allocations fair;fair allocations compute;fair maximize utilitarian;allocations fair maximize;fair allocations;prop1 fair allocations;allocations fair;computational problems utilitarian;maximal allocations;tractable fairness concepts;maximal allocations decide;problems utilitarian maximal;tractable fairness;fairness concepts envy;maximize utilitarian;complexity computing allocations;maximizes utilitarian welfare;computing allocations;envy freeness proportionality;maximizes utilitarian;maximize utilitarian social;utilitarian maximal;allocations;compute maximizes utilitarian;allocations decide exists;allocations compute maximizes;focus tractable fairness;utilitarian welfare paper;allocations decide", "pdf_keywords": ""}, "579095d50eab27ace24a1ea0e97af2f70191dc7c": {"ta_keywords": "subgraphs sentences text;subgraphs labels sentences;matching subgraphs sentences;subgraphs sentences;subgraphs labels;labels subgraphs;match labels subgraphs;subgraphs;stacked graphical model;labels subgraphs labels;matching subgraphs;subcellular location image;mining data text;based matching subgraphs;text data stacked;stacked graphical;data stacked graphical;introduce stacked graphical;labels sentences paper;text images journal;sentences text paper;graphical model;graphical model match;labels sentences;mining data;data stacked;subcellular;biology combination text;text images;sentences paper", "pdf_keywords": ""}, "0c61265a4325df4b97389f92e5e4f5df412f8e97": {"ta_keywords": "localization ultrasonic;localization ultrasonic signals;tool localization ultrasonic;partial discharge localization;localization partial discharge;discharge localization model;discharge localization;discharge location power;location power transformer;joint localization method;partial discharge location;galvanic joint localization;accurate localization partial;efficient accurate localization;discharge location;method partial discharge;methods partial discharge;localization method;joint localization;ultrasonic waves used;localization method used;localization accuracy;ultrasonic signals semiconductors;localization model proposed;accurate localization;ultrasonic waves;ultrasonic signals;ultrasonic;acoustic electrical;propagation impact localization", "pdf_keywords": ""}, "46a2960e409c39901c1efd07a6adfc5f26e22ee8": {"ta_keywords": "email leaks;cases email leaks;data mining;novel data mining;email messages;email clients;provided data mining;email leaks 47;additions email clients;email;mistakes addressing email;permanent additions email;email clients type;accidental sending;real cases email;data mining techniques;additions email;cases email;addressing email messages;accidental sending message;data mining technique;message unintended recipient;detection 15 users;unintended recipient;addressing email;mining techniques;used detection;sending message;detection;mining techniques people", "pdf_keywords": ""}, "d6a7d2e9f2caf3e8eb615580f4ee8329ff9a271d": {"ta_keywords": "parking bulk traffic;simulation parking crowd;crowd interdependent queues;simulation parking;capacity queues drivers;queues advantageously modeled;parking comparing rate;data simulation parking;parking crowd interdependent;queue available space;available parking space;parking comparing;capacity queues;vehicle parking lots;finite capacity queues;approximated parking;cruising curbside parking;vehicle parking;queues drivers;parking space;occupancy approximated parking;interdependent queues;available parking;parking bulk;approximated parking transaction;traffic;parking lots;queues drivers arrive;popular vehicle parking;interdependent queues advantageously", "pdf_keywords": ""}, "4d01d6b445077ad0f1c9d85af93f9ed9239f3c33": {"ta_keywords": "automatic tagging texts;tagging texts;tagging texts written;automatic tagging;method automatic tagging;tagging;spelling normalization;phenomenon spelling normalization;preprocess texts;applying pos tagger;spelling normalization used;tagger;used preprocess texts;modern german corpora;tagger trained modern;pos tagger;preprocess texts applying;german corpora;texts applying pos;origin phenomenon spelling;pos tagger trained;tagger trained;phenomenon spelling;corpora;texts;texts applying;texts written 15th;texts written;normalization used preprocess;modern german", "pdf_keywords": ""}, "189e6bb7523733c4e524214b9e6ae92d4ed50dac": {"ta_keywords": "neural sequence taggers;sequence tagging tasks;sequence taggers;sequence tagging;performance sequence tagging;sequence taggers source;tagging tasks;taggers source task;annotations pos tagging;problem transfer learning;transfer learning neural;transfer learning;learning neural sequence;tagging;training data fluid;task plentiful annotations;pool used learn;tagging tasks unclear;pos tagging;annotations recent experiments;taggers;annotations;tagging penn treebank;model swimming;taggers source;neural networks perform;learning neural;plentiful annotations;available annotations;annotations pos", "pdf_keywords": "annotations transfer learning;neural sequence taggers;annotations transfer;based transfer nlp;transfer nlp;available annotations transfer;linguistic annotations weak;transfer domain adaptation;taggers source task;sequence taggers;supervision transfer learning;linguistic annotations;annotations pos tagging;propose transfer learning;annotations weak supervision;transfer nlp paper;sequence taggers source;domain adaptation;additional linguistic annotations;microblogs pos tagging;tagging microblogs pos;transfer learning languages;pos tagging microblogs;tagging microblogs;learning cross lingual;transfer learning framework;problem transfer learning;approach transfer learning;transfer learning;tagging"}, "58b628792d3eb22a034a871ed3cf373afe591928": {"ta_keywords": "codes hadoop hdfs;erasure codes repairable;hadoop hdfs;hadoop hdfs compare;distributed storage systems;distance distributed storage;hdfs compare;distributed storage;hdfs;codes hadoop;new codes hadoop;hdfs compare currently;hdfs present;hdfs present novel;erasure codes;storage systems;codes repairable;storage systems large;erasure codes used;recently erasure codes;hdfs module;family erasure codes;version hdfs;storage overhead replicated;hdfs module uses;replication provide reliability;modified version hdfs;codes repairable offer;solomon codes standard;high storage efficiency", "pdf_keywords": "distributed erasure codes;codes logarithmic locality;codes locality;repairable codes performance;nonlinear codes locality;local repairable codes;repairable codes;logarithmic locality clusters;erasure codes;randomized explicit codes;logarithmic block locality;logarithmic locality;exact regenerating codes;regenerating codes;new distributed erasure;distributed erasure;regenerating codes logarithmic;locality log dis;block locality log;theoretic bounds code;locality log;repairable storage systems;code distance;code distance linear;e\ufb03ciently repairable storage;bounds code distance;mds codes paper;block locality;codes paper;codes logarithmic block"}, "58174f5bb9f9815b52a99fa03ec42f2b44f2d550": {"ta_keywords": "production semantic lexicons;semantic lexicons large;semantic lexicons;production semantic;automatic set instance;lexicons large corpus;semantic class;lexicons;web based set;lexicons large;semantic;automatically outputs instances;semantic class input;automatic set;named asia automatic;takes semantic class;finding instances;finding instances set;benchmark problems english;study production semantic;large corpus;instances set given;corpus;language independence;instances set;takes semantic;asia automatic set;demonstrating language independence;instances;web resources easily", "pdf_keywords": ""}, "2bafaffe45ba66685c87e2d0a598222a9a68ae13": {"ta_keywords": "statistical analysis linguistic;distribution number words;number words language;words language described;words language;linguistic;linguistic resources;linguistic resources available;analysis linguistic;language described;analysis linguistic resources;language;number words;language described following;social media data;internet social media;statistical;social media;algorithm calculation probability;words;gathering representation approaches;data gathering representation;probability distribution;distribution;internet social;probability;results statistical;probability distribution number;calculation probability distribution;calculation probability", "pdf_keywords": ""}, "108ec3512cdf2e89ba3067f5b10eaa4a96df9347": {"ta_keywords": "speaker adaptation;speaker adaptation experiments;acoustics directional statistics;speech recognition;transfer vector adaptation;speech recognition efficient;continuous speech recognition;accurate transfer vectors;vector adaptation;transfer vectors compared;transfer vectors;representation transfer vectors;vector adaptation letter;transfer vector;conventional transfer vector;transfer vector obtain;field transfer vector;directional statistics basis;directional statistics posterior;acoustics directional;transfer vector decomposed;performance acoustics directional;inference directional statistics;improving performance acoustics;directional statistics;estim transfer vector;vocabulary continuous speech;recognition efficient coarse;coarse class estimation;directional statistics method", "pdf_keywords": ""}, "b575d272036740e03fcf67d64db969557843f629": {"ta_keywords": "space representations tweets;representations tweets learning;representations tweets;user annotated hashtags;tweets learning;annotated hashtags;tweets learning complex;annotated hashtags associated;hashtags associated posts;composition model tweet2vec;predicting user annotated;model tweet2vec;hashtags;hashtags associated;tweet2vec;tweets;character composition model;traditional nlp;new approach nlp;nlp approaches present;model tweet2vec finds;traditional nlp approaches;tweet2vec finds;tweet2vec finds vector;nlp approaches;character composition;user annotated;propose character composition;vector space representations;nlp", "pdf_keywords": "hashtag embeddings conversations;embeddings entire tweets;predicting hashtags tweets;embedding entire tweets;tweets hashtag prediction;tweets character vector;representations tweets;bidirectional hashtag embeddings;space representations tweets;hashtag embeddings;representations tweets propose;predicting hashtags;present tweet2vec character;hashtag prediction social;tweet2vec composing;predicting word embeddings;tweet2vec composing constituent;approaches predicting hashtags;tweet2vec character;social media tweet2vec;hashtags tweets;tweets characters;characters present tweet2vec;hashtag prediction;entire tweets characters;tweets hashtag;prediction social media;tweet2vec outperforms word;tweet2vec outperforms;tweet2vec"}, "d34712c217046ccf8063efe083fbb1e6cbfc0340": {"ta_keywords": "distributed caching capable;distributed caching;caching capable achieving;example distributed caching;caching capable;caching;servers paper describes;write load servers;content delivery networks;content delivery network;large cdn content;servers;content delivery;reduces write load;erasure coding architecture;servers paper;distributed;users thousands clusters;clusters large cdn;large distributed;cdn content delivery;cdn large distributed;content users thousands;performance presence redundancy;rebalance write load;logs 2000 clusters;month long logs;delivery networks cdns;deliver world web;load servers paper", "pdf_keywords": ""}, "f8e580fcf34ee6da50989bbde685634018cbe224": {"ta_keywords": "dialog state tracking;alignment utterances labels;alignment utterances;utterances labels;problem alignment utterances;attention based tracker;state tracking challenge;tracking challenge dstc5;advanced dialog state;5th dialog state;dialog state;advanced dialog;tracking challenge;trackers elaborated english;dialog;decoder architecture attention;compared rule trackers;word sequence set;labels combining attention;present advanced dialog;utterances;combining attention based;rule based trackers;5th dialog;attention based;challenge includes;input word sequence;designed 5th dialog;challenge includes encoder;rule trackers", "pdf_keywords": ""}, "8a6d2e134b3b2df6291af8e36e126ae55d50649c": {"ta_keywords": "recurrent averaging network;inspired averaging learning;gated recurrent averaging;recurrent architecture;recurrent averaging;learning general paraphrastic;recurrent architecture called;new recurrent architecture;averaging learning;motivated learning models;learning general;biologically motivated models;gated recurrent;training sentence pairs;problem learning general;averaging learning analyze;learning models evidence;motivated learning;motivated models;learning models;biologically motivated learning;transfer learning;simple transfer learning;paraphrastic sentence embeddings;introduce new recurrent;sentence embeddings;inspired averaging;network inspired averaging;new recurrent;sentence embeddings revisiting", "pdf_keywords": "learning sentence embeddings;paraphrastic sentence embeddings;word embeddings machine;performance word embeddings;embed semantic representations;word embeddings long;word embeddings;embed semantic;sentence embeddings;trained noisy paraphrase;sentences semantic compositionality;semantic representations sentences;sentence embeddings modify;relationship word embeddings;semantic textual similarity;way embed semantic;noisy paraphrase pairs;short term memory;sentence embeddings paper;lstm improve performance;embeddings long short;embeddings machine learning;semantic representations;representations sentences semantic;embeddings long;improved universal paraphrastic;semantic compositionality;textual similarity datasets;semeval semantic similarity;semantic compositionality received"}, "3911b13a61a3a57674cc8c70c760f545de8aeea2": {"ta_keywords": "payoff truthful equilibrium;truthful equilibrium approximately;peer reward inversely;truthful equilibrium;strong incentive properties;mechanism strong incentive;expected payoffs agents;peer reward;truthful equilibrium close;matches peer reward;equilibrium approximately optimal;incentive properties;payoffs agents symmetric;transactions symmetric equilibrium;expected payoff truthful;information loss mechanism;agents symmetric equilibria;propose reward mechanism;incentive;reward inversely;symmetric equilibrium;strong incentive;monotonic information loss;incentive properties applicable;reward response evaluation;reward response;reward mechanism strong;reward mechanism;reward inversely proportional;business transactions symmetric", "pdf_keywords": ""}, "f335e2256b31d9458c10c61e60bb8bed9dcaf1d9": {"ta_keywords": "learning navigate visual;vision language navigation;learning navigate;dataset learning navigate;new vision language;vision language;models vision language;navigate visual;language navigation;navigate visual environment;learns;approach learns;learns effectively;approach learns effectively;language navigation called;navigate;navigation;visual environment;learns effectively limited;language navigation allowing;visual environment following;language instructions challenging;language ambiguity improve;navigation allowing easy;visual;learning;navigation called limeo;natural language instructions;existing models vision;training paradigm learn", "pdf_keywords": "joint instruction reasoning;single instruction learning;instruction learning;instruction reasoning;architecture joint instruction;instructions navigation combined;agreement instruction variants;instruction learning total;instruction reasoning vln;view learning;single instruction;instruction variants;multiple related instructions;multi view learning;related instructions navigation;joint instruction;instructions different views;traditional single instruction;instructions navigation;multiple instructions;view learning applied;multiple instructions different;retrieval;information retrieval;leverages multiple instructions;language ambiguity improve;learning;instruction;instruction variants paper;mutual agreement instruction"}, "5f23482a8c06ca1ae3e4577e3fdd9213884dac85": {"ta_keywords": "complexity probabilistic lighthouses;probabilistic lighthouses studied;probabilistic lighthouses;complexity probabilistic;lighthouses studied extensively;lighthouses studied;lighthouses;probabilistic;complexity;extensively;studied extensively;studied", "pdf_keywords": ""}, "1e638d235a512cc76d00713639259540342c6fbe": {"ta_keywords": "classification subtask;assisted classification subtask;classification subtask subtask;relation extraction classification;semantic relation extraction;subtask paper describes;relation extraction;relation extraction model;subtask subtask paper;end relation extraction;subtask subtask;semantic relation;concept candidate embeddings;subtask;subtask subtask subtask;subtask paper;work semantic relation;recent work semantic;encoding attention;work semantic;level encoding attention;extraction classification scientific;embeddings;semantic;extraction classification;classification;candidate embeddings;computer assisted classification;assisted classification;classification scientific papers", "pdf_keywords": ""}, "c07fdc95bbf533f8709f8e39c069c1e22b73a7dc": {"ta_keywords": "polyelectrolyte polymers widely;polyelectrolyte polymers;polyelectrolyte;polymers widely used;polymers widely;polymers;blocks natural artificial;natural artificial;artificial;natural artificial systems;artificial systems;building blocks natural;used building blocks;building blocks;blocks natural;systems;widely used building;blocks;widely used;building;used building;natural;widely;used", "pdf_keywords": ""}, "26c65dad79da20aa67df21a6c10e509a964f0841": {"ta_keywords": "transition entity linking;entity linking;reference knowledge base;entries reference knowledge;entity linking integrates;reference knowledge;transition entity;entity;document refers fluid;validation follows entailment;knowledge base;phase transition entity;refers fluid;liquid crystal;entity mentions;cluster entity mentions;entailment formulation evaluates;entity mentions correspond;entailment formulation;entailment;slot filler validation;correspond entries reference;refers fluid dynamics;answer based evidence;cluster entity;entries reference;filler validation;linking;follows entailment;2d liquid crystal", "pdf_keywords": ""}, "d4d25eaa373087ac80810d79afff863ef1bae3c3": {"ta_keywords": "person wheelchair movements;wheelchair movements;seat activity wheelchair;movement wheelchair;wheelchairs periodic pressure;feedback wheelchair users;activity wheelchair user;wheelchair users clinicians;feedback wheelchair;activity wheelchair;valuable feedback wheelchair;manual wheelchairs;wheelchair users;study movement wheelchair;wheelchair user;movement wheelchair presence;person wheelchair;wheelchair;wheelchair movements mean;score person wheelchair;wheelchairs;wisat manual wheelchairs;wheelchair presence;manual wheelchairs periodic;wheelchair presence ischial;seat movement scores;wheelchairs periodic;ulcers seat movements;characterize seat activity;movement scores predicted", "pdf_keywords": ""}, "a4cd428d196bf041c22592216f15246b98b91915": {"ta_keywords": "maruyama yamaguchi theorem;proof irreversibility maruyama;irreversibility maruyama yamaguchi;yamaguchi theorem;irreversibility maruyama;maruyama yamaguchi;proof irreversibility;irreversibility;simple proof irreversibility;yamaguchi;maruyama;theorem;present simple proof;simple proof;proof;article;article present simple;article present;present simple;simple;present", "pdf_keywords": ""}, "8a94106364576f0aa79dccfb30f0536514408249": {"ta_keywords": "ranking datasets optimization;rankers learning;feature based ranking;baseline rankers learning;ranking functions recently;dierent ranking datasets;based ranking functions;ranking datasets;ranking functions;rankers learning eective;baseline rankers;ranking;rankers algorithms;rankers algorithms based;ranking performance;ranking performance present;overall ranking;linear rankers algorithms;overall ranking performance;dierent ranking;labeled document pairs;based ranking;linear rankers;rankers;deteriorate overall ranking;algorithm generation rank;document pairs;pairwise preference framework;art baseline rankers;restricted linear rankers", "pdf_keywords": ""}, "abcaec70b463ed925c29180437ed581c971952cf": {"ta_keywords": "dialogue quality adaptive;dialogue systems dialogue;dialogue systems;dialogue example databases;response candidates dialogue;spoken dialogue systems;improving dialogue quality;systems dialogue modeling;dialogue modeling;improving dialogue;response utterances;select response utterances;dialogue corpora plural;dialogue modeling important;response utterances examples;example based dialogue;responses utterance spoken;responses utterance;use plural responses;dialogue corpora;dialogue quality;plural responses properly;plural appropriate responses;appropriate responses adaptive;plural responses;systems dialogue;response plural appropriate;appropriate response plural;multiple response candidates;choosing appropriate responses", "pdf_keywords": ""}, "f46a3a5dc70a70292175e6c7ad505b8206cb070c": {"ta_keywords": "speech recognition asr;recognition asr challenging;speech recognition;microphone automatic speech;automatic speech;automatic speech recognition;recognition asr;distant microphone automatic;xcite distant microphone;asr challenging task;distant microphone;chime challenge initiative;asr challenging;asr systems;microphone;performance asr systems;performance asr;task specific challenge;microphone automatic;chime challenge;2nd chime challenge;reverberation present summary;challenge identify best;reverberation;challenge results including;evaluate performance asr;challenge initiative designed;computer science challenge;specific challenge identify;summary challenge results", "pdf_keywords": ""}, "1756376bf7cf0d0a7bec881d663b57907a361ecf": {"ta_keywords": "incremental editing structured;generate tree edits;editing structured;incremental editing;editing structured data;represent tree edits;tree edits;edits given tree;tree edits imitation;model incremental editing;novel edit encoder;edit encoder learning;abstract syntax trees;automatic editing;syntax trees computer;syntax trees;tree edits given;generate edited;method automatic editing;generate edited program;approaches generate edited;proposed models editing;editing sequential;partially edited data;code edit datasets;automatic editing partially;editing sequential data;editor imitate;editing;models editing sequential", "pdf_keywords": "proposed edit encoder;learning represent edits;structural edit encoder;novel edit encoder;edit encoder learning;automatic post editing;represent edits;represent edits imitation;editing translations human;editor better;post editing translations;editing;editing translations;editor;allows editor robust;editor robust;post editing;approaches generate edited;editor signi\ufb01cantly improves;edit encoder;structural edit;targeted edit actions;semantics intended edits;generate edited;editor better capture;edits imitation;code edit;propose structural edit;edits imitation learning;translations human orderings"}, "5eba3e525056cac6112cf0b13b62d86ba66661d9": {"ta_keywords": "resource syntactic analyzers;active learning al;view training analysis;training instances annotating;training analysis;syntactic analyzers;minimize annotation cost;active learning;annotating;low resource syntactic;annotation;minimize annotation;syntactic analyzers speech;predictions current heuristics;distribution active learning;useful training samples;samples minimize annotation;uncertain representative training;learning al;learning al xcite;select useful training;resource syntactic;representative training instances;annotating instances;training instances;instances annotating;annotation cost;view training;training analysis demonstrating;speech pos taggers", "pdf_keywords": "named entity recognition;entity recognition;entity recognition pos;automatic language understanding;annotated different languages;learning extractors unlabeled;language experts annotate;learning extractors;automatic language;learning unlabeled text;oracle automatic language;languages empirically;experts annotate tokens;learning unlabeled;language understanding learning;different languages empirically;languages empirically demonstrate;annotate tokens;approach named entity;language understanding;understanding learning unlabeled;tagging;tagging crucial step;annotate tokens need;named entity;extractors unlabeled text;annotate;language understanding al;active learning al;experts annotate"}, "84bc74d875e748aa0f11ac0c5e3000b16484b053": {"ta_keywords": "generating entailment preserving;systems adversarial evaluation;systems adversarial;preserving entailment altering;entailment preserving;adversarial;preserving entailment;adversarial evaluation construct;adversarial evaluation;entailment altering perturbations;generating entailment;resilience systems adversarial;simple adversarial;entailment preserving entailment;method generating entailment;adversarial attacks;simple adversarial attacks;adversarial attacks systems;entailment altering;collection simple adversarial;shared task fever2;fever2 shared task;entailment;verification shared task;extraction verification shared;fact extraction verification;fact extraction;newly perturbed instances;task fever2;perturbed instances", "pdf_keywords": "adversarial example generation;extraction veri\ufb01cation adversarial;adversarial example;adversarial method generating;adversarial;present adversarial;paper present adversarial;machine translation semantic;adversarial method;learning machine translation;annotators fever used;present adversarial method;annotators fever;veri\ufb01cation adversarial;translation semantic matching;evidence retrieval;generating probable sentences;adversarial attacks;fever2 shared task;semantic matching networks;instances hypothesize annotators;machine translation;adversarial attacks applying;veri\ufb01cation adversarial attacks;hypothesize annotators fever;generating negative examples;fever2;semantic matching;annotators;implementation fever2"}, "ecab8208e5182d4b3b0d6183928e816301d2366d": {"ta_keywords": "regularization elastic net;elastic net regularized;regularization elastic;study regularization elastic;classifier elastic net;net regularized models;regression classifier elastic;classifier elastic;net regularized;regularized models;regularization;efficiently training linear;updates study regularization;elastic net reg;compute elastic net;study regularization;regularized;elastic net;elastic net periodic;algorithm compute elastic;compute elastic;training linear;stochastic gradient descent;sparse rod elasticity;regularized models means;gradient descent;efficiently training;decreasing learning rates;elasticity closed form;elastic", "pdf_keywords": "online learning regularized;learning regularized loss;learning regularized;elastic net regularization;regularized loss;regularization updates sgd;regularized loss functions;regularization updates;net regularization;regularization runs;regularization;dense regularization updates;net regularization runs;regularization runs approximately;faster dense regularization;regularized;stochastic gradient descent;regularization 2000 times;net regularization 2000;online optimization;gradient descent sgd;gradient descent;dense regularization;regularization 2000;sgd fobos learning;ing22 regularization;online optimization non;regularization shown;\u21131 ing22 regularization;learning rate"}, "02cc92287c6614b6a2aa982007471f16b3450013": {"ta_keywords": "natural language actions;learning spatial relations;relation natural language;actions small vocabularies;natural language;language simple actions;attributes learning spatial;spatial relations actions;language actions plans;language actions;knowledge bases;robot visually;learning spatial;context simulated worlds;visual attributes learning;visual attributes;vocabularies hard coded;robot;knowledge bases wikipedia;table robot visually;actions context simulated;coded abstract concepts;extracting logical representations;vocabularies;small vocabularies;linking concepts;human interactions abstract;large scale knowledge;attributes learning;robot sees", "pdf_keywords": ""}, "ddc502b6c0d08fefe5b77639e4737cd8c7bce25c": {"ta_keywords": "extracting web pages;extracting web;structure web;method extracting web;structure web appropriate;web pages web;web pages;structure recognition;information structure recognition;particles binary probability;structure recognition methods;finding particle binary;wrapper web;pages web;types structure web;pages web pages;wrapper web page;particle binary;binary probability distribution;particles binary;web appropriate background;web page;maintaining wrapper web;background information structure;binary probability;pages;structure meaningful;methods recognizing;web;particle binary defined", "pdf_keywords": ""}, "ab42ad9698386cc15a30a8c7885fa82b260f537b": {"ta_keywords": "parking demand quantified;similar parking demand;parking demand;curbside parking demand;parking decisions analyze;parking demand propose;parking decisions;drive parking decisions;drive parking;parking;factors drive parking;parking performance based;parking performance;zones similar parking;curbside parking;search parking performance;similar parking;gaussian mixture models;properties curbside parking;cruising search parking;demand quantified spatial;repeatability gaussian mixture;search parking;parking garage;gaussian mixture model;members parking;develop gaussian mixture;gaussian mixture;mixture models;mixture models investigate", "pdf_keywords": ""}, "3abcd0ffc54c3a16c9dc5e5d3ea59eaa43070127": {"ta_keywords": "machine learning preventing;designing machine learning;machine learning algorithm;machine learning algorithms;machine learning intelligent;machines machine learning;learning algorithms exhibit;learning algorithm design;learning preventing undesirable;designer machine learning;machine learning;machine learning approaches;standard machine learning;using machine learning;learning algorithm;application machine learning;learning algorithms ubiquitous;learning algorithms;learning intelligent machines;algorithm design burden;algorithms exhibit undesirable;undesirable behavior intelligent;behavior intelligent machines;learning algorithms used;learning preventing;machines important task;learning algorithms simplifies;algorithms simplifies safe;user designer algorithm;algorithm computer", "pdf_keywords": "machine learning circumvents;regression classification algorithms;machine learning;regression classification;probabilistic constraints attempting;approach machine learning;probabilistic constraints;satisfy probabilistic constraints;classification;classification algorithms;trained data;consider problem predicting;advertising problem based;probabilistic constraints solution;predicting returned intelligent;classification algorithms important;attempting optimize algorithms;requiring satisfy probabilistic;optimize algorithms;algorithms widely;trained data analysis;predicting;behavior regression classification;algorithms;solution online advertising;user trained data;probabilistic;optimize algorithms seldonian;constraints attempting optimize;optimize"}, "37241cdc693b9c2daf49557f18c1ad6a15247239": {"ta_keywords": "image binarization based;document binarization;document binarization scheme;present document binarization;approach image binarization;image binarization;binarization based;state art binarization;binarization based decomposition;binarization;binarization scheme;art binarization;binarizing;color document images;binarization scheme intended;binarization solutions;decomposition color document;art binarization solutions;binarizing range;consistently binarizing;thresholding;consistently binarizing range;thresholding method;binarization solutions consistent;niblack thresholding method;binarizing range degraded;color document;niblack thresholding;popular niblack thresholding;degraded color document", "pdf_keywords": ""}, "f66a17836380c0c79c1b42a9219cf8fde6524287": {"ta_keywords": "answers unstructured text;unstructured text;text semi structured;unstructured text corpora;documents unstructured text;corpus entity;entity labeled documents;labeled documents unstructured;rnn language model;corpus entity labeled;precise answers unstructured;text corpora;semi structured knowledge;knowledge form corpus;rnn language;form corpus entity;answers unstructured;documents unstructured;structured knowledge;language model entity;language model;corpus;hybrid rnn language;entity occurrence;semi structured;text corpora used;form corpus;structured knowledge form;unstructured;labeled documents", "pdf_keywords": ""}, "a817785f0100f3fadc5c1203974d151d5b093310": {"ta_keywords": "quantum computing;quantum computing based;quantum computer;quantum computation;quantum computation used;quantum computer capable;idea quantum computation;demonstration quantum computer;approach quantum computing;reality virtualization;classical computation parallel;reality virtualization library;performance virtual observatory;virtual reality virtualization;virtual observatory;virtualization;virtual observatory virtual;observable virtual reality;virtualization library;computation parallel;speed classical computation;computation parallel execution;single computation cooperatively;machine virtual;observatory virtual observator;virtual observator;virtual machine;quantum;observator virtual machine;virtual reality", "pdf_keywords": ""}, "9336a2ff833d0b4bc914e2282ad04e19d27bc2be": {"ta_keywords": "grid ground state;xmath0 heisenberg antiferromagnet;grounding grid cause;ground grid ground;antiferromagnet square lattice;grounding grid;ground grid locally;copper ground grid;grid cause ground;heisenberg antiferromagnet square;injected grounding grid;grounded grid;ground grid;grid ground;connected ground grid;heisenberg antiferromagnet;grounded grid affect;antiferromagnet square;substation secondary;intelligent substation secondary;substation secondary equipment;intelligent substation;substation;ground state properties;antiferromagnet;copper ground;equipment connected ground;points grounded grid;cause ground potential;effect copper ground", "pdf_keywords": ""}, "4d5f9a0aba65ba6294c543ba5e6108e6d690f133": {"ta_keywords": "domain learn extractors;learn extractors generalize;learn extractors;annotated data;generate text articles;extractors generalize;extractors generalize section;domain learn;labeled examples desired;labeled examples;knowledge supplement domain;including unlabeled text;annotated;documents including unlabeled;target domain learn;text articles;labeled data related;extractors;knowledge supplement;set annotated data;researchers labeled examples;text articles given;labeled data;text able generate;domain insensitive features;transfer knowledge supplement;method generate text;set annotated;generate text;generalize section document", "pdf_keywords": ""}, "c69da8266e2f3f67febf22b8f2bf91623346d283": {"ta_keywords": "vaccine related tweets;vaccination prevalent tweets;prevalent tweets covid;identify retweeted websites;tweets covid;tweets posted covid;retweeted websites identify;techniques identify retweeted;websites shared vaccine;tweets covid 19;retweeted websites;prevalent tweets;social media identify;study viral behavior;identify retweeted;dissemination health information;viral behavior;shared tweets;likely shared tweets;conversations twitter;viral behavior group;related tweets;tweets posted;present study viral;tweets;19 conversations twitter;twitter;social media;shared social media;related tweets posted", "pdf_keywords": ""}, "cd9bfa6266cab4bf4b04c82746a5b650f83b57e4": {"ta_keywords": "nonconvex optimization problems;nonconvex optimization;class nonconvex optimization;non convex problems;dynamics paper nonconvexity;rates nonconvex optimization;methods nonconvex;intra cellular dynamics;methods nonconvex problem;efficiently global minimizer;convergence rates nonconvex;cellular dynamics paper;general non convex;minimization alpha weakly;non convex;cellular dynamics;intra cellular;global minimizer exploiting;nonconvex problem;quasi converx functions;order methods nonconvex;minimizer exploiting structure;nonconvex;consider class nonconvex;minimizer exploiting;inter intra cellular;global minimizer;class nonconvex;optimization problems minimization;nonconvex problem solved", "pdf_keywords": "non convex optimization;nonconvex optimization;methods nonconvex optimization;nonconvex optimization problems;learning gradient descent;gradient descent known;gradient descent;algorithms non convex;descent methods nonconvex;convex optimization;gradient descent methods;problems gradient descent;learning gradient;machine learning gradient;present gradient descent;guarantees optimization algorithms;stochastic optimization;gradient descent aligns;optimization problems gradient;optimization algorithms non;vision particular nonconvex;convex problem formulations;deep linear;guarantees optimization;learning convolutional;learning convolutional models;optimization algorithms;matrix factorization recovery;non convex;non convex problem"}, "d385d8563192569b229bde762fcd4d57ce2b3ee2": {"ta_keywords": "language model incorporate;language model;ontological category information;language model based;word occurrence ontological;proposed language model;domain specific knowledge;ontological;specific knowledge domain;occurrence ontological;knowledge domain;ontological category;occurrence ontological category;knowledge domain approach;definitions train language;experiments proposed language;model definitions train;train language model;information like word;proton collision fermilab;model definitions;proposed language;definition generation task;model based architecture;domain approach improves;domain specific information;collision fermilab;category information;generation task experiments;like word occurrence", "pdf_keywords": ""}, "63d7e40da7f0d37308b8e97fca4a14a26a6b52ea": {"ta_keywords": "statistics fish;analysis statistics fish;artificial intelligence ificial;aquatic competition;artificial intelligence;statistics fish culled;aquatic competition present;artificial neural;intelligence based artificial;ificial intelligence based;neural networks artificials;intelligence ificial intelligence;artificial intelligence ia;artificial neural networks;based artificial neural;53 artificial intelligence;approach artificial intelligence;intelligence ificial;ificial intelligence;data quality;stakes aquatic competition;networks artificials;data quality undervalue;fish;neural networks;quality undervalue data;affecting data quality;dynamic cascades;undervalue data quality;intelligence based", "pdf_keywords": "data high stakes;data practices high;ai practices;data cascades quality;intelligence data;intelligence data science;gaps ai practitioners;data practices;data high;ai practices like;data reflective real;artificial intelligence data;ai practitioners;conventional ai practices;impact data cascades;labelling conventional ai;data cascades;data cascades long;techni resentative data;stakes ai interviews;ai;presented data cascades;data reflective;high stakes ai;undervaluing data high;ai practitioners trained;ai interviews 53;effects ai models;ai interviews;undervaluing data"}, "7707af52b3e19bfd3fc07c2be5aed044e5d7953a": {"ta_keywords": "trained persistent homology;homology based loss;persistent homology based;topological loss;persistent homology;use persistent homology;topological loss functions;homology based;non topological loss;networks neuronal;road networks neuronal;reconstructions road networks;persistent homology ph;networks trained persistent;improving performance homology;homology;images neuronal;networks trained;neuronal processes microscopy;topology data;networks neuronal processes;neural networks segment;training deep;deep networks delineate;preserve connectivity;images neuronal processes;microscopy;topology data sets;aerial images neuronal;networks segment", "pdf_keywords": "persistent homology based;object persistent homology;persistent homology;use persistent homology;topological data;approach persistent homology;persistent homology shown;topological features;\ufb01eld topological data;topological data analysis;loss improves topological;topological features object;study topological features;homology study topological;improves topological;topological correctness segmentation;topological structure data;improves topological correctness;homology based;persistent homology ph;topological;computing persistence;computing persistence diagrams;topological structure;study topological;describing comparing topological;approach computing persistence;homology based machine;homology;persistence diagrams"}, "f74ccbc8988b7f0b847c480d4e8bea3082f4f931": {"ta_keywords": "planning;planning planning;train reward predictor;called planning planning;reward predictor;called planning;search planning practice;search planning;algorithm called planning;forcement learning;reward predictor conditional;planning practice;trained generative;effective search planning;planning deploy trained;trained generative model;generative model decision;planning practice expensive;forcement learning algorithm;deep forcement learning;train reward;sample efficient deep;learning;planning planning deploy;bias bias;reward;bias;bias known;interplay bias bias;efficient deep", "pdf_keywords": ""}, "3f5b7fcb6fc50ba80318ab959f3d63253cd0ef6b": {"ta_keywords": "acoustic event detection;classifier acoustic event;events classification;interdependence events classification;events classification conventional;multiple binary classifiers;new classifier acoustic;classifier acoustic;event detection;event detection based;acoustic event;detection based probabilistic;binary classifiers;neural networks multiple;binary classifiers linear;classification conventional grouping;classifier;classifiers;classification;classifiers linear layer;convolutional neural networks;learning convolutional;new classifier;learning convolutional neural;detection based;classifiers linear;convolutional neural;networks multiple binary;conditional independence;based probabilistic", "pdf_keywords": "acoustic event detection;acoustic event;events introduction acoustic;introduction acoustic event;proposes acoustic event;method acoustic event;event detection classi\ufb01er;event detection;event detection based;surveillance convolutional recurrent;learning method acoustic;situation sounds recorded;sounds recorded;event detection aed;monitoring surveillance convolutional;detection classi\ufb01er;networks crnns;sounds recorded technology;detection classi\ufb01er chains;surveillance convolutional;detection based multiple;neural networks crnns;detecting recognizing;acoustic;introduction acoustic;events;convolutional recurrent;recurrent neural networks;automatically detecting recognizing;detection based"}, "e6beab7c192d7fb04c8bfb0886464fd719cd3421": {"ta_keywords": "autoencoders outperforms;asymmetric autoencoders outperforms;predicting accuracy;autoencoders outperforms previous;autoencoders;asymmetric autoencoders;method asymmetric autoencoders;machine learning characterized;confidence predicting accuracy;learns threshold model;practical method learns;target domain accuracy;methods predicting;predicting target domain;method learns threshold;machine learning;general identifying accuracy;domain accuracy;methods predicting target;method learns;learns threshold;predicting accuracy fraction;world machine learning;identifying optimal predictor;identifying accuracy just;predicting;predicting target;investigate methods predicting;optimal predictor;real world machine", "pdf_keywords": ""}, "d25c4bf23b4b951f2417e4a8a44574c99608e9d7": {"ta_keywords": "sinusoidal wave wavelets;wavelets;wave wavelets;wavelet transform;linear chirplet transform;analysis seismic data;continuous wavelet transform;frequency analysis seismic;wavelet;seismic data using;wavelet transform cwt;seismic data;chirplet transform;time frequency analysis;chirplet transform presented;linear chirplet;time frequency representation;continuous wavelet;stft continuous wavelet;wave wavelets paper;chirplets base function;general linear chirplet;time frequency spectrum;chirplet transform lct;frequency analysis;frequency representation preferably;presented linear chirplet;wavelets paper;using chirplets;wavelets paper present", "pdf_keywords": ""}, "4a160efbe80c38cd5eb2f92c7c095b49b113397d": {"ta_keywords": "code generation retrieval;generation code retrieval;code generation;better code generation;code generation models;code natural language;prefer code generation;based code generation;code generation code;code retrieval;generation retrieval code;generation retrieval developer;developers prefer code;learning based code;code retrieval vice;code quality program;generation code;method generation retrieval;development better code;retrieval developer assistants;software development;code quality;demonstrates developers;apis;especially dealing apis;learning method generation;great software development;demonstrates developers prefer;apis unfamiliar libraries;software development involves", "pdf_keywords": "code generation machine;code generation results;code retrieval;code generation;automatically generating code;code retrieval ones;developers willing use;code directly programming;code search;developers write code;chosen code retrieval;directly programming;software developers;web developers;programming;code search plugin;plugin web developers;software developers write;directly programming languages;generating code;result code generation;developer experience;developer productivity;developers willing;developers;web developers willing;generate retrieve code;developer;improve developer productivity;programming languages"}, "e7ce1b01d2928514710bba044ac2af758c975d99": {"ta_keywords": "equilibrium congestion;costs network congestion;congestion costs multiplicative;selfish routing game;network congestion;underestimates congestion costs;congestion costs;increased equilibrium congestion;equilibrium congestion present;multicommodity selfish routing;selfish routing;underestimates congestion;urban transportation networks;transportation networks;congestion decreases equilibrium;underestimate costs network;type underestimates congestion;congestion;network congestion decreases;transportation networks perform;simulations parking traffic;favorable network conditions;routing game;costs network;routing game types;network costs;congestion present simple;favorable network;network costs significantly;equilibrium overestimation costs", "pdf_keywords": "users uncertainty asymmetries;parking users uncertainty;social cost equilibrium;equilibria uncertainty;users uncertainty;uncertainties user heterogeneity;equilibrium socially optimal;equal equilibria uncertainty;nash equilibrium socially;users uncertainty helps;nash equilibrium;price anarchy;multiplicative model uncertainty;uncertainties network performance;cost equilibrium;case nash equilibrium;uncertainty asymmetries beliefs;equilibria uncertainty paper;equilibrium socially;price anarchy ratio;socially optimal solution;uncertainty;uncertainties user;socially optimal;availability information dispersal;add users uncertainty;uncertainties;uncertainties network;model uncertainty;uncertainty paper provide"}, "caf40157a7a1d72ae3a6946169c992d8c973b743": {"ta_keywords": "gingival recession gingivalent;recession gingivalent inflammation;model gingival recession;recession gingivalent;gingival recession;gingivalent inflammation areas;periodontics;periodontics faced challenge;gingivalent inflammation;gingival recession current;mathematical model gingival;current practice periodontics;practice periodontics;practice periodontics faced;presence gingival recession;periodontics faced;model gingival;gingiva defined mucogingival;attached gingiva;gingiva defined;present periodontium;periodontium providing therapy;attached gingiva defined;periodontium;problems present periodontium;gingivalent;gingival;gingiva;present periodontium providing;band attached gingiva", "pdf_keywords": ""}, "1b57ffe73ae95f339015c174ec574b59f99ea553": {"ta_keywords": "information sifted representations;features maximizes utility;sifted representations symmetric;sifted representations;features maximizes;cloak greatly;state cloak;subset features maximizes;nonconducive features;cloak;mutual information sifted;depends rank representations;cloak novel approach;addition cloak greatly;based perturbation maximization;representations;specified state cloak;maximizes utility user;maximizes utility;perturbation maximization;nonconducive features note;function mutual information;state cloak novel;search subset features;rank representations;mutual information;adversaries ability learn;feature space;addition cloak;infer nonconducive features", "pdf_keywords": ""}, "c6048cd0b1368be0e62633ef723f9d691323102c": {"ta_keywords": "model speaker clustering;mixture models;scale mixture models;mixture model speaker;speaker clustering;mixture model highly;gaussian mixture model;mixture models assume;model dynamics speech;mixture model;speaker clustering noisy;scale mixture model;gibbs sampling;multi scale mixture;gibbs sampling iterative;gibbs sampling powerful;gaussian mixture;blocked gibbs sampling;dynamics speech;represented gaussian mixture;based mixture nonstationary;environment based mixture;speech noisy environment;mixture represented gaussian;clustering noisy;dynamics speech noisy;clustering noisy data;estimation improved clustering;mixture represented;mixture nonstationary", "pdf_keywords": ""}, "40e292d16168fcb8ac87c20682b827ad17a999dd": {"ta_keywords": "app able predict;personalized mobile apps;mobile apps;behavior app app;app app;walk network;random walk network;apps;behavior app;concept random walk;random walk recent;random walk;app;random walk generalization;generalization random walk;outcome random walk;behavior single app;walk generalization random;proliferation personalized mobile;app paper propose;personalized mobile;single app;mobile;walk network method;walk recent;walk generalization;network probability walker;app app able;app paper;walk", "pdf_keywords": ""}, "6332d5bb0e6af89471ffc6157e3816c029b3ae83": {"ta_keywords": "durability pem football;durability pem;parameters durability pem;pem football;dynamic load cycle;durability;operating parameters durability;load cycle;parameters durability;load cycle operating;dynamic load;pem;influence dynamic load;cycle operating parameters;cycle operating;dynamic;football;cycle;load;operating parameters;operating;parameters;influence dynamic;study influence dynamic;study;study influence;influence", "pdf_keywords": ""}, "8b3c0dd95167d4d63161038493a691ee5cdc76b3": {"ta_keywords": "similarity sentences model;monolingual sentence matching;sentence matching;similarity sentences;model similarity sentences;sentences model trained;text simplification based;text simplification;sentences model;problem text simplification;similarity;idea monolingual sentence;model similarity;simplification based idea;aligned dataset;aligned dataset experiments;model adapting;structure model similarity;adapting;adapt model adapting;sentences;introduce neural network;hand aligned dataset;high performance aligning;monolingual sentence;simplification based;rescoring adaptation improve;rescoring adaptation;idea monolingual;based idea monolingual", "pdf_keywords": "word embedding widely;word embeddings machine;word embedding;trained word embeddings;word embeddings;learn semantic similarity;word embeddings pre;word embedding promising;translation word embeddings;word embeddings built;trigram word embedding;word embeddings obtained;simpli\ufb01cation word embedding;word embedding initialization;task word embeddings;semantic similarity;word similarity;word similarity task;performance word similarity;semantic similarity scores;paper word embeddings;embeddings built wikipedia;method word embedding;neural language model;similarity scores gram;use word hashing;word hashing;embeddings pre trained;nlp tasks;different nlp tasks"}, "e4de1009eb7b3524bf7d19bdcebced80035a47cf": {"ta_keywords": "asynchronous neighbor discovery;neighbor discovery protocol;neighbor discovery problem;neighbor discovery;apply neighbor discovery;neighbors1 network nodes;based asynchronous neighbor;asynchronous neighbor;network nodes codeword;active neighbors1 network;neighbors1 network;discovery protocol;active neighbors1;discovery protocol internet;discovery problem;nodes codeword;protocol internet search;asynchronous group testing;set active neighbors1;scheme apply neighbor;network nodes;neighbors1;neighbor;discovery;nodes codeword length;group testing scheme;nodes;decoding complexity log;decoding complexity;novel asynchronous group", "pdf_keywords": ""}, "dbb159b288930c6be32c2d5b91373ca1e341e633": {"ta_keywords": "speech feature enhancement;stereo based speech;enhancement using dictionary;vocabulary noisy speech;speech feature;noisy speech recognition;based speech feature;dictionary learning;speech recognition;using dictionary learning;speech recognition task;feature enhancement;middle vocabulary noisy;feature enhancement using;noisy speech;vocabulary noisy;stereo based;proposes stereo based;stereo;based speech;using middle vocabulary;proposes stereo;enhancement using;paper proposes stereo;speech;using dictionary;middle vocabulary;dictionary;recognition;recognition task", "pdf_keywords": ""}, "2f3ec666ba50c6a9ce74abad6a5127ea38a05bca": {"ta_keywords": "erasure codes efficient;nodes erasure decoders;erasure codes;codes network nodes;traditional erasure codes;failures erasure codes;erasure codes termed;erasure codes used;erasure decoding;erasure decoders;regenerating codes;regenerating codes presented;class erasure codes;codes network;erasure decoders constituent;erasure decoding constituent;problem erasure decoding;termed regenerating codes;nodes erasure;storing data network;decoding constituent codes;codes termed regenerating;node failures erasure;decoders constituent codes;codes used retaining;constituent codes network;coding schemes;stored network;data stored network;encoded using codes", "pdf_keywords": "distributed data storage;type distributed storage;distributed storage systems;distributed storage proposed;distributed storage network;storage networks presented;distributed storage;storage networks;data storage networks;present distributed storage;storage network;storage network data;twin codes nodes;framework distributed storage;storage systems present;erasure coding reduce;erasure coding;storage systems;data storage;codes nodes partitioned;ef\ufb01cient node repair;distributed data;erasure codes;uses erasure coding;codes nodes;storage proposed;erasure codes termed;node repair;storage network facilitates;reconstruction node repair"}, "8f2182846d5d4cfbc216b5e4c00411e021dc4776": {"ta_keywords": "sensor data recurrent;data recurrent neural;networks rnns;learning sequence data;data recurrent;recurrent neural networks;time series clinical;clinical medical data;rnns;health information sensor;medical data;neural networks rnns;networks rnns powerful;classification diagnoses training;recurrent neural;learning sequence;multilayer perceptron trained;extracting health information;models learning sequence;classify 128 diagnoses;diagnoses training model;classification diagnoses;sequence data;health information;medical data especially;sequence data potentially;diagnoses training;multilayer perceptron;time series observations;neural network", "pdf_keywords": "multilabel recurrent neural;propose multilabel recurrent;multilabel recurrent;learns predict outcome;classify 128 diagnoses;recurrent neural networks;learns predict;training multilabel;deep learning;training multilabel classi\ufb01cation;predict outcome critical;diagnoses training model;recurrent neural network;approach training multilabel;regularize recurrent connections;multilabel classi\ufb01cation diagnoses;model multilabel time;multilabel time series;model multilabel;recurrent connections;predictive machine;use recurrent neural;multilabel classi\ufb01cation tasks;diagnoses training;recurrent neural;predict outcome;predictive machine learning;classi\ufb01cation diagnoses training;architecture learns predict;use recurrent"}, "bcffee102a99f726ddfe765906babb01b8226269": {"ta_keywords": "tetradecylindrical tetrate rna;tetrate rna;tetrate rna rna;rna rna cross;rna cross;rna cross section;rna;rna rna;tetracyanoquinodimethane tetradecylindrical tetrate;tetrathiafulvalene tetracyanoquinodimethane tetradecylindrical;effect tetrathiafulvalene tetracyanoquinodimethane;tetracyanoquinodimethane tetradecylindrical;tetrathiafulvalene tetracyanoquinodimethane;tetracyanoquinodimethane;tetradecylindrical tetrate;effect tetrathiafulvalene;tetrathiafulvalene;tetrate;tetradecylindrical;study effect tetrathiafulvalene;cross;cross section;letter present results;letter;results experimental;present results experimental;letter present;results experimental study;experimental study;experimental", "pdf_keywords": ""}, "322ef476e90a487c8f9797bece7799b69af9e5c1": {"ta_keywords": "coded matrix multiplication;coded computation scheme;propose coded computation;dimensional coded matrix;coded matrix;product coded matrix;coded computation;new coded computation;high dimensional coded;coded computation involving;study coded computation;ldpc codes achieve;codes computing;codes computing product;matrix multiplication hdd;large matrix multiplication;mdc codes computing;separable mdc codes;redundancy distributed computing;multiplication matrices large;dimensional coded;codes achieve linear;coded scheme achieves;coded scheme;product coded scheme;distributed computing;ldpc codes;decoding complexity allowing;decoding complexity;multiplication hdd framework", "pdf_keywords": ""}, "822395760906f4940df68aa33925b6bf9123bac2": {"ta_keywords": "ecosystem parkinsonian parkinsoninsoninsonian;ecosystem parkinsonian;ecosystem ecosystem parkinsonian;parkinsoninsoninsonian association competitions;parkinsonian parkinsoninsoninsonian;parkinsonian parkinsoninsoninsonian association;parkinsonian;parkinsoninsoninsonian association;parkinsoninsoninsonian;cherenkov light collection;cherenkov light;data mining;solution data mining;2016 international cherenkov;light collection;comprehensive study ecosystem;popular tool data;international cherenkov light;cherenkov;data mining problem;tool data science;international cherenkov;data science solve;data science;study ecosystem ecosystem;study ecosystem;competitions popular tool;results comprehensive;ecosystem ecosystem;mining problem", "pdf_keywords": ""}, "24d28783f6061bd1e91fb60417ac8b3646305a49": {"ta_keywords": "blackboard communication components;systems information extraction;components declarative control;weight blackboard communication;information extraction systems;extraction systems described;large scale systems;blackboard communication;information extraction;scale information extraction;sequencing component;automatically sequencing component;scale systems information;extraction systems;pinning rod;control automatically sequencing;large scale information;declarative control automatically;extraction feature computation;systems information;like classification extraction;sequencing component level;systems described based;automatically sequencing;inflexible pinning rod;information extraction include;architecture large scale;blackboard;classification extraction;weight blackboard", "pdf_keywords": ""}, "c8648d04f52e49167d1a4443a4830709cf3331ff": {"ta_keywords": "computing game theoretic;game theoretic solutions;stackelberg strategies games;algorithms computing game;strategies defender constructed;computing game;stackelberg strategies;game theoretic;strategies defender;strategies games;pure strategies defender;number pure strategies;strategies games showing;pure strategies;multiple targets;real world security;algorithms;study stackelberg strategies;polynomial time cases;multiple targets exponential;presence multiple targets;placement checkpoints;polynomial time;hard xmath0;xmath0;games showing computed;defender;targets;defender constructed;strategies", "pdf_keywords": "computing equilibria twoplayer;equilibria twoplayer;agents game theory;game theory;game theory provides;equilibria twoplayer normal;normal form games;optimal action team;finding optimal action;optimal strategy \ufb01nding;optimal action;finding optimal strategy;optimal strategy;computing equilibria;act optimally;problem computing equilibria;interested agents game;agents game;equilibria;strategy \ufb01nding defender;act optimally domain;games paper resolve;twoplayer normal form;form games;means act optimally;finding optimal;problem finding optimal;optimal;np hardness;self interested agents"}, "651468a69da74dab716cebbd179a5cbb8e672c14": {"ta_keywords": "learning demonstrations;learning demonstrations challenging;learning suboptimal demonstrations;cavity learning demonstrations;algorithm learn demonstrations;demonstrations struggle learning;learning demonstrations lfd;dimension learning demonstrations;learn demonstrations noisy;sparse optimization;solving sparse optimization;demonstrations challenging task;learn demonstrations;optical cavity learning;optimization problems sparse;solve sparse problems;sparse optimization problems;solving sparse;algorithm solving sparse;sparse environments;problems sparse environments;sparse problems;used solve sparse;solve sparse;sparse;demonstrations noisy;cavity learning;speed optical tweezers;demonstrations training;hyperparameters handcrafted", "pdf_keywords": "learning demonstrations;learns human demonstrations;imitation learning demonstrations;lifelong reinforcement learning;learning demonstrations silfdin;reinforcement learning demonstrations;demonstrations learn;demonstrations learn policy;human demonstrations learn;lifelong reinforcement;lifelong lifelong reinforcement;new deep reinforcement;initialized demonstrations preserved;deep reinforcement;adversarial demonstrations;lifelong learning;adversarial demonstrations paper;expert demonstrations alleviating;deep reinforcement learning;expert demonstrations;imitation learning;reinforcement learning modi;demonstrations preserved;lifelong lifelong learning;self imitation learning;scheduling in\ufb02uence demonstrations;initialized demonstrations;learns;reinforcement learning;bene\ufb01t expert demonstrations"}, "1cb7015c0a8015c65844876459809ecac917ec02": {"ta_keywords": "speech enhancement se;multi channel acoustic;channel acoustic;speech enhancement;channel acoustic model;acoustic model;acoustic model single;acoustic;tal speech enhancement;learning noisy;learn noisy;learn noisy data;multi channel input;learning noisy noisy;input multi channel;channel input;used learn noisy;multi channel;extended noisy datasets;noisy data extended;single multi channel;data extended noisy;speech;noisy data noisy;noisy noisy data;noisy datasets;extended noisy;noisy data;trained se architecture;noisy data method", "pdf_keywords": ""}, "899055ad2f0863cf1931c41f04da8b1dd7382607": {"ta_keywords": "lipid profiles risk;lipid variability;lipids lipid variability;variability lipid profiles;lipid variability multivariate;visit variability lipid;severity lipid profiles;lipid profiles varied;variability lipid;assess association lipids;lipid cholesterol;densitylipoprotein lipid cholesterol;lipid profiles;lipoprotein cholesterol;density lipoprotein cholesterol;lipoprotein total cholesterol;lipoprotein cholesterol ldl;low density lipoprotein;densitylipoprotein lipid;high densitylipoprotein lipid;lipid cholesterol non;association lipids lipid;density lipoprotein;severity lipid;level severity lipid;cholesterol;risk factors cardiovascular;cholesterol univariate multivariate;relationship lipoprotein;cholesterol ldl", "pdf_keywords": ""}, "173c73077a421680f12576524e85dff4b890c17e": {"ta_keywords": "projected wasserstein distance;wasserstein distance sample;wasserstein distance;distance projected distributions;kernel projected wasserstein;projected wasserstein;distance sample;distance projected;computing distance function;computing distance;estimates develop kernel;distance function nonasymptotic;nonasymptotic uncertainty quantification;algorithms computing distance;maximizes distance projected;projected distributions;distance sample test;quantification empirical estimates;nonasymptotic uncertainty;uncertainty quantification empirical;function nonasymptotic uncertainty;distance;projected distributions contrast;distance function;existence long range;dimensionality efficiently;empirical estimates;kernel projected;wasserstein;develop kernel projected", "pdf_keywords": "robust wasserstein distance;projection robust wasserstein;wasserstein distance riemannian;distance projected distributions;wasserstein distance;robust wasserstein;distance riemannian optimization;projected distributions propose;distance functions empirical;projected distributions;kernels design tests;divergence algorithm prediction;testing based weighted;riemannian optimization;riemannian optimization propose;empirical distributions constructed;data distributions hard;weighted divergence algorithm;projection robust;empirical distributions;data distributions;wasserstein;based kernels design;empirical estimates statistical;projector based kernels;distributions constructed observed;distance projected;based kernels;weighted weighted divergence;functions empirical distributions"}, "4ae15dbb068cc962b39dca07d87b22fe5dcd5f6a": {"ta_keywords": "cryptography generate private;metric inferential privacy;privacy metric inferential;quantum cryptography generate;quantum cryptography;using quantum cryptography;new privacy metric;generate private information;inferential privacy;privacy metric;privacy metric assumes;electricity smart grid;generate private;new privacy;introduce new privacy;smart grid smart;smart grid;privacy;electricity smart;meters smart grid;price electricity smart;smart grid operations;cryptography generate;grid smart meters;private information user;private information;ability infer private;tradeoff smart grid;grid smart;smart meters power", "pdf_keywords": "tradeoff performance privacy;privacy metric inferential;metric inferential privacy;performance privacy;inferential privacy;privacy consumers electric;privacy metric;model smart grid;privacy recommendations;smart grid operations;new privacy metric;propose new privacy;performance privacy consumers;privacy recommendations sound;tradeoff smart grid;privacy;smart grid;privacy consumers;new privacy;follow privacy recommendations;energy usage data;information collection policies;grid operations data;want follow privacy;follow privacy;consumers electric utilities;collection policies analyze;thermostatically controlled loads;electric utilities want;utilities want"}, "513552a56668279d6cd0857a4399fe8a63d92145": {"ta_keywords": "effect crowding behavior;crowding behavior;pedestrian crowded environment;crowding behavior single;crowding effect;crowding effect caused;effect crowding;effect caused crowding;investigate effect crowding;pedestrian crowded;single pedestrian crowded;crowded environment;case crowding effect;crowding;behavior single pedestrian;caused crowding;crowded environment case;case crowding;environment case crowding;crowded;single pedestrian;pedestrian;behavior;behavior single;effect;effect caused;investigate effect;environment case;environment;paper investigate effect", "pdf_keywords": ""}, "8113f05360c6483e52b3e261fc9efce671e0aaa6": {"ta_keywords": "meeting transcription automatic;overlapping speech diarization;speaker speech recognition;trained separation diarization;automatic speech;speech recognition diverse;recognition speech;separation diarization recognition;automatic speech recognition;speech recognition;speech diarization;recognition speech recognition;speech recognition speech;meeting transcription;speech recognition popular;multi speaker speech;overlapping speech;decade automatic speech;diarization recognition components;transcription automatic subtitle;diarization recognition;trained separation module;transcription automatic;applications meeting transcription;problem overlapping speech;independently trained separation;separation diarization;speaker speech;trained separation;subtitle generation experiments", "pdf_keywords": "speech separation based;frame speech separation;separation diarization recognition;trained separated audio;speech separation;diarization trained separated;best diarization separation;overlapping speech diarization;performs separation diarization;separation network trained;separated audio;separation diarization asr;trained separation module;diarization separation;speech diarization automatic;diarization automatic speech;separation diarization;multi frame speech;speaker speech recognition;separated audio streams;diarization separation 45;speech recognition;speech diarization;automatic speech recognition;multi channel separation;channel separation network;automatic speech;speech recognition effectively;trained separation;separation network"}, "29437d98b9e6f45bef7029f3ce1237b8b284464f": {"ta_keywords": "text generation focused;text generation;sentences soft templates;way stylistic control;stylistic control using;stylistic control;control writing styles;data text generation;improving content fidelity;stylistic;writing styles;hybrid attention copy;content fidelity style;existing sentences soft;writing styles approach;control writing;new way stylistic;way stylistic;text;sentences soft;learns weak supervisions;attention copy;hybrid attention;improving content;templates develop neural;content fidelity lacking;content fidelity;fidelity style control;explicit control writing;recent neural approaches", "pdf_keywords": ""}, "d63edef60d674408819bb015b64b7f42470e151b": {"ta_keywords": "molecular generative model;design molecular generative;crnn models novo;molecules controlled generative;molecular generative;models novo molecule;crnn models;generative model gaining;generative model generation;train crnn models;novo molecule design;models novo;controlled generative way;rna molecules achieving;controlled generative;generative model;model generation;model generation control;design molecular;novo molecule;propose generative model;molecule design molecular;generative way;rna molecules;method train crnn;rna;generative;generating compounds;molecule design;drug like molecules", "pdf_keywords": "learning based molecular;drug molecular design;molecular generative modelling;model targeted molecule;molecular properties prediction11;molecular generation models;targeted molecule generation;aided drug design;binding affinity prediction14;protein binding pocket;generation molecules targeting;drug molecular;based molecular generation;ligand complex datasets;information molecular generation;molecule generation guided;molecular design;molecular generative;molecular generation;molecule generation;computer aided drug;molecular design example;drug design;drug design cadd;process molecular generative;targeted molecule;field drug molecular;pocket information molecular;generative model targeted;protein binding"}, "1ddf9d306ae27113f55ea3d4eee12c8441235656": {"ta_keywords": "3d water ice;water ice interaction;ice interaction;ice interaction presence;dimensional 3d water;water ice;3d water;translation server;ice;translation server automatic;dynamics dimensional 3d;external magnetic field;magnetic field;external magnetic;presence external magnetic;dynamics;result translation server;automatic evaluation translation;magnetic;study dynamics dimensional;theoretical study dynamics;dimensional 3d;dynamics dimensional;study dynamics;server automatic evaluation;automatic evaluation server;translation result translation;evaluation translation;3d;evaluation translation result", "pdf_keywords": ""}, "b03feec6f5b898484fdfdc3cd12f084afbe77036": {"ta_keywords": "pairs rank learning;rank learning pairwise;rank learning;ranking datasets optimization;feature based ranking;ranking datasets;pairs rank;based ranking functions;ranking functions based;ranking functions;learning pairwise preferences;ranking;ranking performance;overall ranking;different ranking datasets;overall ranking performance;ranking performance present;outlying pairs rank;labeled document pairs;linear rankers;based ranking;deteriorate overall ranking;rankers;learning pairwise;different ranking;restricted linear rankers;linear rankers experiments;pairwise preference framework;based pairwise preference;algorithm generation rank", "pdf_keywords": ""}, "f388c2be45e4415fcb59cf43a3b29463cf7e7940": {"ta_keywords": "fact checking used;particular fact checking;fact checking;addressed fact checking;statements fact checked;fact checking likely;fact checked journalists;natural language processing;using statements fact;data task check;language processing tasks;fact checked;statements fact;perform natural language;language processing;natural language;task check;checked journalists;task check given;dataset using statements;data task;checked journalists available;check given task;statistical;analysis statistical;need addressed fact;checking likely use;statistical properties data;fact;journalists", "pdf_keywords": ""}, "9352dfd127dcfce8013eb350e0229cc72b9bd203": {"ta_keywords": "attention augmented wgan;tunneling microscopy;scanning tunneling microscopy;self augmented attention;augmented attention;attention weights;self attention augmented;fully convolutional network;tunneling microscopy stm;improved super resolution;convolutional network;module attention weights;attention;attention augmented;convolutional network extract;super resolution;microscopy;augmented attention saa;augmented wgan;module attention;encoder decoder network;scanning tunneling;benchmark datasets demonstrate;scale features reconstruct;arpes scanning tunneling;attention weights produced;super resolution model;new super resolution;super resolution ssr;fully convolutional", "pdf_keywords": ""}, "4104d632d1cff0c9314cde344e2b1da06e662c5b": {"ta_keywords": "recognition speech;semi supervised training;automatic speech;speech recognition speech;semi supervised;speech text data;data quantity speech;automatic speech recognition;speech recognition;approach automatic speech;unsupervised semi supervised;speech text;quantity speech text;recognition speech recognition;improvements semi supervised;supervised;supervised training;dynamics component bose;supervised training models;recognition;supervised training using;speech text modalities;bose einstein;unsupervised semi;atoms lattice;speech recognition based;bose gas confined;atoms lattice laser;optical lattice;speech", "pdf_keywords": ""}, "7e43dad7fbae3a7db47adc6b89c76acbd2fb225f": {"ta_keywords": "hierarchical knowledge base;instructional articles documenting;domain hierarchical knowledge;procedures based wiki;hierarchical knowledge;instructional articles;knowledge base;procedures inherently hierarchical;instructional video retrieval;110k instructional articles;tasks instructional video;articles documenting steps;tasks instructional;hierarchical;domain hierarchical;knowledge base kb;inherently hierarchical require;inherently hierarchical;wiki website containing;open domain hierarchical;downstream tasks instructional;hierarchical require;wiki;wiki website;articles documenting;constructing kb procedures;based wiki;based wiki website;kb procedures inherently;instructional video", "pdf_keywords": "contextualized embeddings procedures;hierarchical semantic;contextualized embeddings;domain hierarchical semantic;videos textual goal;video embeddings linked;embeddings procedures inherently;procedures deep neural;expressive contextualized embeddings;embeddings procedures;hierarchical semantic web;embeddings linked data;video embeddings;relevant videos textual;deep neural;videos textual;embeddings linked;textual goal visually;retrieving relevant videos;codifying procedures deep;decomposing video embeddings;semantic;textual goal;embeddings;procedures deep;expressive contextualized;contextualized;learning procedures text;semantic web provides;semantic web"}, "15a6c3d32ae1daefba3c4b40146de8efdf16ec8d": {"ta_keywords": "simulation motion human;computer simulation motion;simulation motion;motion human subject;human subject noisy;motion human;computer simulation;subject noisy environment;simulation;motion;noisy environment;subject noisy;results computer simulation;human subject;noisy;human;subject;computer;environment;present results computer;paper present;results computer;paper;present;paper present results;present results;results", "pdf_keywords": ""}, "950c2c041db52c416e49fb0945078f6463c501b8": {"ta_keywords": "iterative control incentive;model energy consumption;control incentive design;estimating consumer utility;consumption model energy;incentives estimating consumer;energy consumption patterns;control incentive;based aggregate consumption;incentive design estimation;energy consumption model;utility learning;consumer utility function;consumption data utility;consumers driven feedback;disaggregated consumption;designing incentives estimating;design incentives based;consumer utility;consumption patterns consumers;aggregate consumption;disaggregated consumption data;aggregate consumption data;novel energy consumption;consumption data propose;utility learning disaggregation;estimation utility learning;design incentives;incentive design;energy consumption", "pdf_keywords": "designing incentives energy;incentives energy users;energy disaggregation utility;incentives energy;energy disaggregation algorithm;propose energy disaggregation;energy disaggregation convex;energy disaggregation problem;energy disaggregation;utility company incentives;disaggregation convex optimization;signals energy disaggregation;aggregated power consumption;incentives consumers estimating;incentives consumers;framework energy disaggregation;designing incentives;design incentives consumers;estimating utility functions;consumers estimating utility;utility functions propose;power consumption signals;disaggregation utility company;incentives designed model;design incentives;algorithm design incentives;consumption signal propose;solution energy disaggregation;power consumption signal;estimating utility"}, "f637d061704579531a8b8e03ef6e8331ba117490": {"ta_keywords": "estimating pairwise;estimating pairwise probabilities;computationally achievable adaptivity;optimally adaptive closely;context estimating pairwise;relative oracle estimator;pairwise distance adaptivity;optimally adaptive;known optimally adaptive;adaptive closely;optimal computationally achievable;regularized squares estimator;estimator adapts;adaptive closely related;squares estimator known;introduce adaptation index;adaptation index compares;entanglement pairs people;pairwise pairwise entanglement;adapt context estimating;optimal computationally;achievable adaptivity;distance adaptivity index;adaptation index;estimating;computationally achievable;risk estimator oracle;good indicator pairwise;adaptive;pairwise entanglement", "pdf_keywords": ""}, "7626f73c3b013b5b7bf293c1cc22d2835b6579b3": {"ta_keywords": "speech synthesis models;speech synthesis;class speech synthesis;context based speech;synthesis models based;synthesis models;based speech recognition;speech recognition;based speech;synthesis;models based context;context based;based context based;speech;new class speech;class speech;models based;models;based context;context;recognition;based;new class;propose new class;class;paper propose new;paper propose;propose new;paper;new", "pdf_keywords": ""}, "bf0b66e0e328df1df42b075422c8fecdd95736c0": {"ta_keywords": "tutored problem solving;tutor learning;facilitate tutor learning;learning tutoring;learning tutoring limited;learning agent;learning tutored problem;tutoring;tutor learning tutoring;learning tutored;learning agent called;tutoring limited;tutor;facilitate tutor;compared learning tutored;effectively facilitate tutor;tutor students;algebra learning linear;tutor learning paper;learns learn teaching;teaching learning;solving students;game like learning;student learns;algebra learning;influence tutor learning;student learns learn;solving students use;tutoring limited benefits;tutored problem", "pdf_keywords": ""}, "4ffca5d623950e2396089e7fc1621b4a477436cb": {"ta_keywords": "text generation;way stylistic control;text generation focused;stylistic control;sentences soft templates;stylistic control using;control writing styles;writing styles;data text generation;stylistic;improving content fidelity;content fidelity style;writing styles approach;way stylistic;new way stylistic;control writing;text;fidelity style control;existing sentences soft;content fidelity;content fidelity lacking;style control;fidelity style;sentences soft;balances content fidelity;neural approaches;styles;improving content;writing;explicit control writing", "pdf_keywords": ""}, "97e033d79b6aebab1927ab9232afa8268e198481": {"ta_keywords": "distributed caching video;video caching systems;caching video demand;demand video caching;video caching;distributed caching;caching video;based distributed caching;video caching video;caching systems;caching systems based;cache communication;cache communication aspects;desirable tradeoffs cache;caching;cache users topology;tradeoffs cache;cache content;cache content placement;cache;cache cache communication;content placement cache;tradeoffs cache touser;placement cache;scalable vod;cache cache;placement cache users;video demand;network codes;cache users rate", "pdf_keywords": ""}, "f5ca46585818771e64ee9449c930748fbee35cba": {"ta_keywords": "explainable nlp models;natural language feedback;interactive refines explanations;structured explanations;feedback natural language;structured explanations paper;nlp models reasoning;interactively correct explanation;class explainable nlp;form structured explanations;explainable nlp;models reasoning tasks;correct explanation structures;natural language;reasoning tasks;defeasible reasoning;structures natural language;refines explanations;explanation structures;reasoning tasks support;nlp models;language feedback;defeasible reasoning reduced;explanation structure;explanations given reasoning;simple explanation structure;feedback introduce interactive;getting human feedback;models reasoning;refines explanations given", "pdf_keywords": "inference natural language;natural language feedback;explanation generation;explainable nlp models;advancements explanation generation;improving explainable nlp;reasoning human feedback;generates defeasible reasoning;interactive systems nlp;training dialogue systems;generates defeasible inference;adapt user utterances;embeddings learning language;nlp models;language feedback;learning training dialogue;approaches nlp wide;defeasible inference graphs;defeasible reasoning human;approaches nlp;word embeddings learning;dialogue systems adapt;language feedback approach;presents natural language;nlp wide ranging;relevant approaches nlp;natural language;defeasible reasoning inferential;answering questions retraining;inference graphs directly"}, "97db55b196cf0c768644a392a7e6c79d1c65207e": {"ta_keywords": "speech enhancement frame;online speech enhancement;prediction speech;frame online speech;overlapped frame prediction;prediction speech recognition;frame deep neural;speech enhancement task;analysis prediction speech;frame prediction;reverberant speech enhancement;speech enhancement;frame prediction technique;enhancement frame deep;predicting current frame;prediction technique deep;network dnn predicts;learning based frame;online speech;dnn predicts;tournament based predictive;enhance performance neural;predicting outcome tournament;speech recognition;reverberant speech;frame deep;deep neural;current frame online;frame short time;based frame online", "pdf_keywords": "speech enhancement frame;online speech enhancement;enhancement speaker separation;speech enhancement task;speech enhancement speaker;speech enhancement;monaural speech enhancement;overlapped frame prediction;speaker separation;frame online speech;enhancement frame deep;enhancement speaker;frame deep neural;learning online speech;enhancement frame;frames necessary overlap;mixtures based overlap;speaker separation paper;based overlap add;overlapped frame;online speech;overlap add;proposes overlapped frame;frame prediction;frame prediction technique;simulated monaural speech;overlap add instead;learning based frame;necessary overlap add;overlap add mechanisms"}, "45ce9fce4a4eea9f72688885182aee0c84786fab": {"ta_keywords": "domains musical transcriptions;musical transcriptions;musical transcriptions present;translate musical compositions;method translating music;method translate musical;samples domains musical;translating music musical;domains musical;musical compositions interpreted;translating music;musical compositions;compositions musical compositions;compositions musical;musical compositions musical;translate musical;musical instruments genres;instruments genres;music musical instruments;musical instruments;domain wavenet autoencoder;compositions interpreted domain;wavenet autoencoder;methods translating whistling;autoencoder;wavenet autoencoder shared;instruments genres styles;instruments;compositions interpreted;transcriptions", "pdf_keywords": "musical transcriptions;music automatic composition;automatic composition music;transcription music;tasks transcription music;transcription music automatic;domains musical transcriptions;musical domains mozart;composition music train;musical transcriptions work;musical segments mozart;domains mozart;domains mozart 46;mozart;automatic voice translation;symphonies;automatic composition;mozart 46 symphonies;segments mozart;composition music;trained set musical;segments mozart 46;translation using deep;46 symphonies;musical segments;mozart 46;classical musical domains;symphonies conducted;46 symphonies conducted;arbitrary classical musical"}, "782a50a48ba5d32839631254285d989bfadfd193": {"ta_keywords": "entity representations human;entity representations;creating entity representations;new approach embeddings;approach embeddings;grained entity types;type embeddings;fine grained entity;language processing entities;embeddings robust;type embeddings biologically;embeddings biologically plausible;natural language processing;embeddings;corresponding type embeddings;grained entity;embeddings biologically;approach embeddings parameter;processing entities text;automatically inferring person;trained models representations;models representations vectors;automatically inferring;representations human readable;embeddings parameter free;confidence typing model;type set learning;typing model;entities text typically;models representations", "pdf_keywords": "supervised entity typing;embedding words entities;typing based embedding;entity representations;entity typing dataset;entity typing based;entity representations possible;entity typing;approach entity typing;based embedding words;embedding words;type set learning;tasks entity representations;words entities;embedding approach benchmark;types considered embeddings;entity types;trained supervised entity;domains entity types;similarity sparse trainable;supervised entity;words entities continuous;entities continuous space;embeddings;embeddings post hoc;entity types typically;similarity sparse;entities;wikipedia categories;based embedding"}, "d26683135c70d7b2a61ce5f70fb49b4fa22cf9c4": {"ta_keywords": "sampling arbitrary ranking;learning preference distributions;models pairwise preference;arbitrary ranking distributions;pairwise preference data;preference distributions;ranking distributions conditional;preference distributions critical;learning preference;pairwise comparisons alternatives;pairwise preference;ordinal rankings experiments;arbitrary ranking;pairwise evidence computationally;rankings experiments;ranking distributions;rankings experiments real;arbitrary pairwise comparisons;nonparametric estimation existing;preference data;model pairwise evidence;existing learning inference;pairwise evidence;pairwise comparison data;pairswise evidence;approach learning preference;likelihood learning mallows;pairwise comparisons;learning inference methods;nonparametric estimation", "pdf_keywords": "ranking models;ranking models introduce;generative model ranking;preference distribution;real ranking data;used ranking models;preference distribution allows;model ranking web;ordinal preferences based;ranking data;preferences develop probabilistic;ranking web;distribution real ranking;ranking restaurants;real ranking;ordinal rankings;inferences drawn preferences;population preference distribution;ranking web pages;model ranking;ordinal rankings paper;ranking restaurants clearly;ranking;dinner ranking;ordinal preferences;dinner ranking restaurants;rankings;commonly used ranking;pairwise preferences commonly;blocks ordinal rankings"}, "205d67dfe0112df846bc4b221fa2665b0434d441": {"ta_keywords": "explanation origin phenomenon;origin phenomenon;explanation origin;phenomenon;present explanation origin;article present explanation;origin;explanation;present explanation;article;article present;present", "pdf_keywords": ""}, "7df95dceaba3f4fb45e2b9de29caf7fbce20e25c": {"ta_keywords": "xmath1 transition superconducting;iron pnictide superconductors;superconducting state iron;transition superconducting state;transition superconducting;superconducting state;pnictide superconductors;superconductors;superconducting;xmath1 transition;xmath0 xmath1 transition;state iron pnictide;observation xmath0 xmath1;observation xmath0;xmath1;report observation xmath0;xmath0 xmath1;state iron;iron pnictide;xmath0;pnictide;iron;transition;state;report observation;report;observation", "pdf_keywords": ""}, "83a2582b94aeaaa97b2f52af8d827d28dc4690bf": {"ta_keywords": "communicating person stuttering;stuttered speech recognition;person stuttering process;person stuttering;recognition silent pauses;fluency speech;normal fluency speech;pauses stuttered speech;speechanical communication;speechanical communication communication;stuttering;fluency speech paper;stuttering process;stuttered speech;silent pauses;speech recognition;silent pauses stuttered;stuttering process affects;xmath0 speechanical communication;speechanical;pauses stuttered;recognition silent;measures effectiveness speech;effectiveness speech;clustering algorithm provide;speech;xmath0 speechanical;means clustering algorithm;clustering algorithm;speech paper propose", "pdf_keywords": ""}, "930445d9cda71d6ff857e69aa5bb4b1bef7d31e5": {"ta_keywords": "nonstationary reinforcement learning;discounted reinforcement learning;drifting nonstationary reinforcement;dynamic regret bounds;reinforcement learning algorithms;nonstationary reinforcement;dynamic regret bound;propose discounted reinforcement;reinforcement learning reward;discounted reinforcement;regret bounds optimal;reinforcement learning;non stationary policy;low dynamic regret;learning reward;dynamic regret;learning reward functions;stationary policy;learning algorithms drifting;enjoy dynamic regret;algorithms drifting nonstationary;regret bounds;regret bound parameter;stationary policy various;reward functions state;reinforcement;regret bound;optimal non stationary;reward functions;drifting nonstationary", "pdf_keywords": "dynamic regret bound;regret bound optimal;inventory planning censored;dynamic regret;achieve dynamic regret;regret decomposition autonomous;regret decomposition;inventory planning;regret bound parameter;new regret decomposition;terms dynamic regret;particular dynamic regret;autonomous inventory systems;approach inventory planning;decomposition autonomous inventory;inventory systems stationary;autonomous inventory;planning censored demand;approach inventory;regret bound;inventory systems;inventory;regret bound demonstrate;demand based nonparametric;censored demand based;stationary demand distributions;performance learning;knowing variation budgets;new regret;stationary demand"}, "d3231772937a2182b2377d028417245c49868dd1": {"ta_keywords": "neural machine translation;machine translation;neural sequence models;minimum translation length;machine translation mcmc;bias shorter translations;minimum translation;translation length;shorter translations;translation mcmc experiments;unsatisfactory report neural;shorter translations present;neural models;report neural machine;neural machine;translations;inference procedure neural;neural networks perform;report neural;neural sequence;problem translations;procedure neural sequence;translation length root;translations present;sequence models;neural networks;translation;translation mcmc;procedure neural;neural", "pdf_keywords": "neural machine translation;machine translation;machine translation beam;translation beam search;head translation task;search length normalization;translation task;exact search length;exact decoding;length bias neural;model bias shorter;reducing search errors;exact search extended;search length;translation task 2016;present exact decoding;search ubiquitous decoding;neural models properly;decoding algo rithm;bias shorter hypotheses;hypotheses beam search;reduces search errors;suggests problem translations;decoding algo;arbitrary length models;ubiquitous decoding algo;translation beam;shorter hypotheses;neural models;length models report"}, "6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b": {"ta_keywords": "scaling laws trained;scaling training;scaling training loss;study scaling training;explaining scaling laws;connects scaling laws;scaling laws tight;scaling laws;network scaling exponents;framework explaining scaling;explaining scaling;limited scaling;size training dataset;large dataset resolution;related scaling regimes;parameters network scaling;power law scaling;scaling regimes;related scaling;law scaling;limited scaling exponents;dataset resolution limited;network scaling;scaling exponents depend;architectures datasets large;scaling;dataset resolution;identify related scaling;resolution limited scaling;law scaling relations", "pdf_keywords": "kernel learning curves;linear kernel learning;scaling properties convolutional;learning curve theory;learning curve;learning curves gaussian;kernel ridge regression;kernel learning;learning curves;intrinsic dimension;theory deep neural;data dimensionality;scaling laws neural;kernel ridge;scaling loss consequence;meta learning neural;convolutional neural networks;dimensionality image datasets;scaling laws learning;dimensionality;dimensionality data;scaling loss;linear model kernel;dimensionality image;neural networks wild;intrinsic dimension possible;dimensionality data manifold;deep neural networks;meta learning;universal scaling"}, "4e1b16fd719354b0a9e92075be66c85d4b95082c": {"ta_keywords": "ability perform task;body ability perform;body ability;properties body ability;structural properties body;perform task;task specific tasks;specific tasks;tasks;perform task specific;task specific;body;properties body;ability perform;structural properties;task;structural;ability;relationship structural properties;properties;relationship structural;investigate relationship structural;perform;relationship;paper;specific;paper investigate;paper investigate relationship;investigate relationship;investigate", "pdf_keywords": ""}, "713844009469478141671c53a3b73cd12caf9df0": {"ta_keywords": "dynamics coupled pendulum;coupled pendulum;coupled pendulum pendulum;pendulum oscillators pendulum;pendulum pendulum oscillators;pendulum oscillators;oscillators pendulum;oscillators pendulum described;pendulum driven external;temperature dynamics coupled;pendulum driven;described pendulum driven;pendulum described pendulum;pendulum pendulum;described pendulum;pendulum described;temperature dynamics;pendulum;effect temperature dynamics;oscillators;dynamics coupled;effect temperature;temperature;study effect temperature;dynamics;driven external force;coupled;external force;driven external;external", "pdf_keywords": ""}, "810420af4fa5f3ed932724aea5f7b66d3bd592b2": {"ta_keywords": "weight allocation online;learning rules weight;allocation online weight;online weight allocation;describes concept resource;allocation online;rules weight allocation;new concept resource;concept resource;learning rules;weight allocation;machine learning;weight allocation systems;concept based discovery;learning methods address;concept resource directories;machine learning methods;learning methods;based discovery;approach learning rules;keeping resource;use machine learning;using machine learning;learning methods construct;rules weight;allocation;resource;discovering discovering new;systems learning methods;specific topic fluid", "pdf_keywords": ""}, "e5e74d312679eae8f2a2943e16f2efebcb5cc50f": {"ta_keywords": "wind speed changes;surface wind speed;wind speed;near surface wind;surface wind;wind;reanalysis near;speed changes observed;reanalysis near surface;speed changes;reanalysis;presents reanalysis near;summer;paper presents reanalysis;observed course summer;course summer;changes observed;summer 2014;course summer 2014;presents reanalysis;near surface;speed;changes observed course;changes;observed;surface;2014;near;observed course;paper presents", "pdf_keywords": ""}, "e35357ac461a669fe7e4b877ee1fad0dfda26303": {"ta_keywords": "style transfer languages;languages style transfer;style transfer methods;style transfer task;transfer languages style;crowdsource formality annotations;style transfer using;style transfer;inference style extraction;style transfer method;new style transfer;previous style transfer;style transfer new;modeling stylistic;style labelled corpus;style extraction;sentences inference style;sentence target style;method modeling stylistic;modeling stylistic difference;style extraction work;style labelled corpora;study style transfer;stylistic difference paraphrases;formality annotations;attempted style transfer;annotations 4000 sentence;attribute transfer tasks;approach attribute transfer;annotations", "pdf_keywords": "unsupervised style transfer;style transfer;style transfer scalar;improves style transfer;style transfer able;style transfer model;new style transfer;unidirectional style transfer;style controlled translation;crowdsourced datasets;models crowdsourced datasets;use supervised translation;attribute transfer tasks;attribute transfer;supervised translation;models crowdsourced;learn unsupervised style;crowdsourced datasets use;crowdsourced;performance models crowdsourced;paraphrases style vector;supervised translation objective;unsupervised style;translation objective style;translation data encourages;transfer able learn;propose style encoder;results attribute transfer;style encoder based;translation data"}, "e97c5b206c1f308b821917bc2f584b5f1faad547": {"ta_keywords": "estimators rely ranking;ranking miscalibrations;ranking miscalibrations results;testing ranking cardinal;induced ranking;ranking cardinal scores;scores induced ranking;consistent induced ranking;induced ranking popular;rely ranking;testing ranking;scores arbitrary adversarially;algorithm ranking miscalibrations;ranking cardinal;ranking;problem ranking cardinal;adversarially chosen miscalibration;new ranking algorithm;ranking algorithm;bias scores;cardinal scores arbitrary;new ranking;induced ranking paper;ranking algorithm ranking;information cardinal scores;cardinal scores induced;ranking popular;scores arbitrary;ranking popular approach;problem ranking", "pdf_keywords": "ranking items googol;deterministic ranking estimator;ordinal ranking estimator;ranking items;ranking estimator achieves;deterministic ranking;ranking estimator;true ranking items;ranking;propose ordinal ranking;ranking better;rankings goal;ordinal rankings goal;estimator true ranking;ordinal ranking;consistent ordinal rankings;ranking better given;ordinal rankings;rankings;present deterministic ranking;rankings goal paper;consider problem ranking;problem ranking better;ranking estimator true;crowdsourced microtasks;problem ranking;googol propose ordinal;true ranking;cardinal scores elicited;crowdsourced microtasks workers"}, "2d3fcbaf28e650471b942f221c5fa3c178b1b72a": {"ta_keywords": "convex unconstraint optimization;accelerated directional search;unconstraint optimization;constraint optimization;unconstraint optimization problem;gradient free methods;accelerated gradient free;convex unconstraint;directional search method;factor constraint optimization;propose accelerated directional;accelerated gradient;nesterov method;directional search;constraint optimization problem;optimization;result accelerated gradient;accelerated directional;make nesterov method;consider convex unconstraint;gradient free;optimization problem mathbb;constraint;nesterov method times;optimization problem;unconstraint;optimization problem approach;method times faster;log factor constraint;make nesterov", "pdf_keywords": ""}, "b345057638e60eee581fea6c7110a98e3b9ebe61": {"ta_keywords": "topics text streams;streams semantic scan;text streams semantic;massive text streams;semantic scan;semantic scan event;semantic scan new;semantic scan alternative;scan event detection;reviews semantic scan;monitoring emerging events;detecting monitoring emerging;emerging topics text;identifies anomalous text;compare semantic scan;events text streams;monitoring free text;trend detection task;event detection;anomalous text patterns;text streams;streams semantic;semantic scan state;business trend detection;event detection characterization;events massive text;monitoring emerging;text streams methods;semantic scan ss;trend detection", "pdf_keywords": "topic modeling;topic models incorporated;contrastive topic modeling;topic models;topic modeling online;emerging topics using;contrastive topic model;topic model;using topic models;topic model online;extensions topic models;topic models compare;newly emerging topics;characterize disease outbreaks;emerging topics;topics using;topics using topic;topic assignment spatial;identify emerging events;topics;model online topic;disease outbreaks propose;topics time;disease outbreaks;geographical disease surveillance;outbreaks propose novel;propose semantic scan;semantic scan;contrastive topic;detecting emerging"}, "2038086c604f1f8841d086cd5cc6052e546ffc24": {"ta_keywords": "3d spgispeech model;3d spgispeech;dimensional 3d spgispeech;spgispeech model;spgispeech model presence;performance dimensional 3d;spgispeech;dimensional 3d;3d;external load;performance dimensional;presence external load;study performance dimensional;performance;model;load;model presence;model presence external;dimensional;paper study performance;external;study performance;presence external;paper study;paper;presence;study", "pdf_keywords": ""}, "9850d2b41c6c5be039649d6422306121b760169d": {"ta_keywords": "oblivious update algorithms;communication oblivious updates;oblivious updates generic;oblivious updates;codes update protocols;distributed storage;update protocols;distributed storage systems;distributed computer systems;oblivious update;interim distributed storage;updated nodes requiring;bounds communication oblivious;codes oblivious update;systems storage nodes;distributed distributed computer;distributed distributed;distributed computer;data updated nodes;update algorithms;data interim distributed;lower bounds communication;storage nodes;truly distributed distributed;updated nodes;update protocols stale;constructing truly distributed;explicit codes oblivious;distributed;communication oblivious", "pdf_keywords": ""}, "71bcdfe5b6be3a0d08ce4bde45acdfd0f738e2f7": {"ta_keywords": "inverse reinforcement learning;based inverse reinforcement;inverse reinforcement;problem inverse reinforcement;based inverse risk;inverse risk;inverse risk sensitive;risk sensitivity reinforcement;modeling passengers decisions;reinforcement learning markov;learning markov decision;risk sensitive reinforcement;sensitivity reinforcement learning;sensitive reinforcement learning;reinforcement learning framework;reinforcement learning algorithm;reinforcement learning;new reinforcement learning;reinforcement learning method;probabilities rewards mdp;passengers decisions;mdp modeling passengers;gradient based inverse;learning markov;markov decision processes;transition probabilities rewards;passengers decisions regarding;probabilities rewards;markov decision;modeling passengers", "pdf_keywords": ""}, "1843c91e9692484b574ef40961f1d0443a56ddf4": {"ta_keywords": "reward based incentive;matches peer reward;rewarding game theoretic;simple incentive mechanism;incentive mechanism;peer reward inversely;based incentive schemes;incentive mechanism obtaining;incentive schemes;rewarding game;simple rewarding game;peer reward;propose simple incentive;based incentive;reward based;incentive schemes rebates;reward answer evaluation;incentive;simple incentive;administering reward based;simple rewarding;agent gets reward;approach administering reward;bayes nash equilibrium;reward;nash equilibrium;reward answer;reward inversely;evaluation matches peer;answer mechanism agent", "pdf_keywords": ""}, "4c0a915b9389e6489753a968085ee12833131d0a": {"ta_keywords": "proofs correct paper;optical lattice tutorial;confined optical lattice;optical lattice;2d electron gas;rejection scientific papers;lattice tutorial discuss;2d electron;electron gas confined;challenges peer review;scientific papers;lattice tutorial;peer review;dimensional 2d electron;lattice;scientific papers means;gas confined optical;peer review outline;problem rejection scientific;electron gas;proof correctness;proof correctness proof;correctness proof proof;community peer review;correctness proof;proof correct proofs;based proof proof;peer;electron;approach proof correctness", "pdf_keywords": ""}, "df53aabeca68a8c0076f7e110f2cc7df7d010e7a": {"ta_keywords": "multi source localization;multi talker multiple;multi talker;talker multiple;talker multiple partners;scheme multi talker;source localization scheme;source localization;localization scheme multi;localization scheme;localization;multi multi source;talker;multi source;new multi multi;multi multi;multiple partners;presents new multi;new multi;multi;scheme multi;partners;multiple;scheme;source;paper presents;paper;presents;new;presents new", "pdf_keywords": "talker speech recognition;speech multi channel;speech recognition source;multi talker speech;speaker speech recognition;multimodal audio mixtures;multi speaker speech;speech recognition asr;predict location speakers;multi talker;speech recognition;multimodal audio;speech multi;audio mixtures multi;multi source localization;multi speaker;integration multimodal audio;multi source audio;asr mimo speech;simultaneous multi talker;frontend automatic speech;speech recognition taking;automatic speech recognition;speech recognition work;automatic speech;source localization classify;source audio mixture;recognition source localization;audio mixture;audio mixture integration"}, "b03c7ff961822183bab66b2e594415e585d3fd09": {"ta_keywords": "neurons attention powerful;neurons attention;concept negative liposome;negative liposome;parallel attention;negative liposome nl;parallel attention head;attention heads;neural models focus;attention mechanisms parallel;percentage attention heads;mechanisms parallel attention;large percentage attention;multiple attention;cortical neurons attention;multiple attention mechanisms;generalization negative lipids;attention mechanisms;attention head;allowing neural models;neural models;liposome;percentage attention;neural;negative lipids;attention head focusing;liposome nl;allowing neural;attention;attention powerful", "pdf_keywords": "pruning attention heads;inference attention heads;pruning attention;language inference attention;attention based machine;attention heads;attention heads individually;attention heads deep;masking attention heads;machine translation model;inference attention;large percentage attention;effect pruning attention;machine translation;network machine translation;attention heads removed;attention;percentage attention heads;heads deep convolutional;masking attention;method masking attention;new attention based;attention based;machine translation 2018;translation model able;translation model;based machine translation;attention heads paper;percentage attention;test performance"}, "315432474166ff7abbc6351e8ff07fcccbd68458": {"ta_keywords": "misinformation websites shared;quality misinformation websites;misinformation terms tweets;misinformation websites;websites contain misinformation;quality misinformation sources;urls high quality;misinformation sources analyze;news sources connecting;urls shared twitter;information websites twitter;tweets containing websites;web health news;tweets containing urls;quality urls extensively;health news sources;misinformation sources;low quality urls;news sources common;websites twitter;quality urls shared;higher rate tweets;quality urls;urls low quality;urls extensively shared;sources connecting community;twitter;news sources;low quality misinformation;tweets", "pdf_keywords": "misinformation network news;spread misinformation online;misinformation abounds twitter;misinformation spread social;misinformation network;concern spread misinformation;spread misinformation;quality misinformation network;misinformation spread;online ideological news;misinformation online;media platforms misinformation;misinformation online low;news sources connecting;study misinformation spread;ideological news exposure;news organizations websites;health news sources;web health news;quality misinformation;online news organizations;low quality misinformation;promotes political misperceptions;news information network;online news;network news information;online ideological;news exposure promotes;sources connecting community;quality websites emerged"}, "92acaf505a9c738e56ed70759e8d0062f3c520d6": {"ta_keywords": "speech recognition asr;stranded rna;stranded rna rna;network neural end;input long recording;single stranded rna;rna;speech recognition;asr single neural;recognition asr systems;automatic speech;rna rna;realizes audio segmentation;segmentation audio;segmentation audio deal;audio segmentation non;recognition asr;automatic speech recognition;long recording;audio segmentation;connectionist temporal classification;streaming input long;issue segmentation audio;length token sequence;autoregressive automatic repeat;single neural network;neural end end;practical automatic speech;audio;network neural", "pdf_keywords": "speech recognition automatic;automatic speech recognition;integrate audio segmentation;recognition automatic speech;speech recognition;audio segmentation decoding;audio segmentation non;audio segmentation;speech recognition segmentation;automatic speech;speech recognition promising;attention automatic speech;asr single neural;autoregressive asr single;classi\ufb01cation automatic speech;autoregressive asr;single neural network;paper audio segmentation;propose integrate audio;non autoregressive asr;integrate audio;segmentation non autoregressive;single neural;combined single neural;audio;segmentation decoding combined;self attention automatic;segmentation decoding;recognition segmentation oracle;attention automatic"}, "f75d05e759447c2aedb7097728f29f9a520d9bc1": {"ta_keywords": "transformer language models;sentence level prediction;long range context;language models trained;context improve prediction;language models;language models including;range librarians know;long range librarians;tokens language models;sentence level;transformer language;range librarians;range transformer language;tokens models improves;prediction tasks;prediction tasks ways;range context improve;discourse level information;level prediction tasks;range context remain;copied distant context;trained short truncated;range context;long long range;improves predictions;models improves predictions;librarians know expect;models trained short;discourse level", "pdf_keywords": "question answering;model answer answering;answering question answering;sequence level predictions;language models results;language model predict;answer answering;answer answering question;language models;question answering paper;transformer language models;token sequencelevel improvements;long range discourse;context token sequencelevel;sequencelevel improvements long;sequencelevel improvements;range discourse structure;language models using;answering paper;language model;answering question;sequence level;long range context;prevents language models;sequence sequence level;model predict word;predict word;answering;token sequencelevel;masked language model"}, "1dfa71ecab0c25c5fdd6b2df83a41e944ffa5d58": {"ta_keywords": "models based multinomial;based multinomial distribution;tractability bayesian models;bayesian models;statistical models text;models based poisson;bayesian models used;statistical models;multinomial distribution;tractability bayesian;binomial distributions desirable;based multinomial;bayesian;binomial distributions;negative binomial distributions;words higher frequencies;analytic tractability bayesian;present statistical models;models text;women business transaction;based poisson;frequencies occurrence sensible;multinomial;frequencies occurrence;poisson negative binomial;occurrence;men women business;models text treat;text treat words;distributions desirable", "pdf_keywords": ""}, "0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d": {"ta_keywords": "nonparametric machine learning;benchmark domain adaptation;nonparametric neural language;performance nonparametric neural;models nlm learn;nearest neighbors language;learning nlm algorithms;domain adaptation;learning nlm;domain adaptation datasets;comparable performance nonparametric;performance nonparametric nonlinear;machine learning nlm;nonparametric neural;nlm algorithms effective;nlm learn predictive;performance nonparametric;nonlinear machine learning;learn predictive distributions;models nlm;nlm algorithms;neighbors language model;benchmark domain;language models nlm;inference speed retaining;neural language models;memorizing training datapoints;nonparametric machine;nearest neighbors;analysis performance nonparametric", "pdf_keywords": "large scale retrieval;machine translation model;neural network retrieval;model machine translation;rnn based language;datastores language models;machine translation;retrieval large datastore;translation based model;large datastores language;parametric machine translation;machine translation based;retrieval based;retrieval large;network retrieval adaptor;retrieval;translation model;retrieval adaptor;recurrent neural network;network retrieval;network rnn based;language models;neural network rnn;models require retrieval;retrieval operation;require retrieval large;network rnn;model recurrent neural;performance retrieval;rnn based"}, "83cbe142d445a521aefa11acbd184e176085e7c7": {"ta_keywords": "trusting voters suspicious;voters negative information;trusting voter anomalous;model election dynamics;suspicious voters types;election dynamics based;trusting voters;voters suspicious;suspicious voters;voters suspicious voters;multi agent influence;voter anomalous updates;model election;election dynamics;agent influence diagrams;agent influence;behavior observed voters;trusting voter;voters trusting voters;voters trusting;possibility deceptive information;simple model election;voter anomalous;voters types information;observed voters negative;deceptive information;deceptive information paper;observed voters;generalize bayesian networks;bayesian networks", "pdf_keywords": "trusting voters;trusting voters suspicious;voters behave intuitively;trusting voters behave;voting strategy model;voters trusting voters;voters trusting;easy trusting voters;trusting voters trusting;votes skeptical voter;voting strategy;votes skeptical;voters suspicious voters;skeptical voter influenced;voters suspicious;skeptical voter;suspicious voters;behavior votes skeptical;voters types information;model vote behavior;suspicious voters types;mock election data;mock election;voters behave;model vote;election data relevant;election data;voters types;vote behavior;model classes voters"}, "6a79ff7465d8249d9c8a50fa5f2e0a3e308b436d": {"ta_keywords": "domain adaptation retrieval;dense retrieval approaches;results dense retrieval;dense retrieval approach;retrieval approaches;dense retrieval;domain specialized datasets;retrieval approaches overcome;domain adaptation;domain robust training;retrieval tasks yield;retrieval approach;adaptation retrieval tasks;retrieval tasks;retrieval;improved search results;patterns web domains;retrieval approach points;improved search;search patterns;search patterns web;approach search patterns;domains representative;adaptation retrieval;domain specialized;scenario domain adaptation;specialized datasets;cantly improved search;data available domains;wide range domains", "pdf_keywords": "retrieval architecture crowdsourced;domain adaptation retrieval;deep retrieval;deep retrieval architecture;deep retrieval model;novel deep retrieval;particular deep retrieval;crowdsourced crowdsourcing tasks;crowdsourcing tasks;adaptation retrieval tasks;domain speci retrieval;crowdsourcing tasks particular;domain adaptation;retrieval datasets;speci retrieval datasets;dense retrieval;retrieval datasets investigate;adaptation retrieval;crowdsourced crowdsourcing;pseudolabels cross encoder;retrieval tasks yield;retrieval architecture;retrieval tasks;retrieval model achieves;architecture crowdsourced crowdsourcing;crowdsourced;architecture crowdsourced;crowdsourcing;retrieval model;retrieval"}, "3e3f55cb25b919c4e8158195fd3ce2f23cfa7723": {"ta_keywords": "language prior inference;counterfactual inference framework;inference language prior;counterfactual inference;proposed counterfactual inference;language bias;language bias sensitive;language prior;inference language;bad language bias;language bias particular;counterfactual response;counterfactual response bouncer;modal knowledge vision;bias sensitive vqa;performance language bias;demonstrate proposed counterfactual;knowledge vision language;modal knowledge;search counterfactual response;search counterfactual;disentangle good language;counterfactual;good language context;language context;multi modal knowledge;vision language;bias sensitive;inference framework;learning multi modal", "pdf_keywords": "counterfactual reasoning causal;counterfactual inference framework;counterfactual inference;propose counterfactual inference;counterfactual reasoning;reasoning causal inference;modal reasoning experimental;causal inference inspired;reasoning causal;multi modal reasoning;causal inference;question answering;inference vqa systems;modal reasoning;inference inspired;inference vqa;inference;strategies causal graphs;works counterfactual reasoning;answer question answering;vqa works counterfactual;causal graphs;causality based interpretation;learning semantic;causal graph direct;inference framework;counterfactual;visual explanations scene;nie inference vqa;causality based"}, "7b29f45df975ed1e4c3864b6ab4483f11086aa76": {"ta_keywords": "trained word embeddings;word embeddings;embeddings surprisingly effective;word embeddings proven;performance embeddings surprisingly;word embeddings used;reduction performance embeddings;performance embeddings;embeddings surprisingly;language analysis tasks;performance natural language;embeddings;embeddings used;embeddings used help;nmt tasks;embeddings proven;natural language analysis;language analysis;natural language;embeddings proven invaluable;trained word;help nmt tasks;ground state energy;energy potts written;nmt tasks presence;xmath0 particle presence;pre trained word;model state potts;state energy potts;energy potts", "pdf_keywords": "training multilingual machine;translation pre trained;pre training multilingual;trained word embeddings;machine translation pre;multilingual machine translation;pre trained embeddings;word embeddings significantly;training multilingual;embeddings significantly improve;trained embeddings effective;ensure word embeddings;embeddings source languages;word embeddings;multilingual machine;monolingual data desirable;word embeddings source;embeddings effective similar;translation pairs especially;monolingual data;bilingual data;trained embeddings used;trained embeddings;use monolingual data;effective similar translation;effective use monolingual;word embeddings proven;single language lowresource;translation pre;similar translation pairs"}, "7a684045afae2ccf40338ff07b8fa429bad93a57": {"ta_keywords": "web corpus;web corpus 10;pages web corpus;large web corpora;billion crawled;crawled;process commoncrawl corpus;commoncrawl corpus;corpus 10 billion;web crawl;billion crawled urls;web corpora;general web crawl;corpus;commoncrawl corpus 2000;crawled urls highly;million pages web;web corpora containing;meaningful representation corpus;large web;date billion crawled;crucial nlp tasks;representation corpus human;nlp tasks;corpus human brain;representation corpus;corpus 2000 cpu;corpus 2000;crawl;corpus 10", "pdf_keywords": ""}, "fc5d79301a0876201c95954a764ec374b8eb236e": {"ta_keywords": "parallel domain corpus;neural machine translation;domain corpus;pseudo domain corpus;domain parallel sentences;extract domain lexicon;translation monolingual domain;domain target sentences;domain lexicon;domain corpus performing;strong translation baselines;machine translation;parallel sentences improving;domain lexicon construct;domain specific words;lexicon induction extract;monolingual domain target;machine translation nmt;domain corpus paper;lexicon induction;perform lexicon induction;perform lexicon;specifically perform lexicon;trained domain;effect highly lexicalized;pre trained domain;highly lexicalized nature;highly lexicalized;monolingual domain;translation baselines", "pdf_keywords": "adaptation machine translation;unsupervised machine translation;sentences domain adaptation;supervised machine translation;machine translation monolingual;machine translation model;machine translation able;word translation parallel;machine translation;translation unadapted nmt;translation monolingual data;domain adaption lexicon;domain adaptation machine;corpus copying monolingually;parallel indomain corpus;domain adaptation;corpus copying;indomain corpus copying;monolingual data unsupervised;word translation unadapted;translation model;translation parallel data;million sentences domain;integrate word translation;translation model semi;translation able adapt;translation parallel;domains largest corpus;corpus subtitles medical;adaption lexicon induction"}, "75abecb4568366d89e89c3c9d39574b9c1c028a5": {"ta_keywords": "nlp tasks;certain nlp tasks;paperbrowser navigational;paperbrowser navigational functionalities;navigational functionalities paperbrowser;nlp biomedical text;processing nlp;nlp tasks named;processing nlp biomedical;text highlighting task;named entity recognition;natural language processing;nlp biomedical;events improves navigational;entity recognition;language processing nlp;performance certain nlp;highlighting events improves;entity recognition anaphora;text technology facilitate;improves navigational;highlighting task evaluation;appraise paperbrowser navigational;facilitate tasks;language processing;human computerinteraction paper;biomedical text;text highlighting;using text highlighting;natural language", "pdf_keywords": ""}, "ec9367ab933a142124eecd3232fe2d933d93a144": {"ta_keywords": "motion newborn;motion newborn baby;video motion newborn;fluid dynamics video;fluid dynamics;dynamics video;dynamics;baby affected presence;newborn baby affected;newborn baby;rigid body;dynamics video motion;newborn;motion;baby affected;presence rigid body;affected presence rigid;presence rigid;rigid;video motion;fluid;affected presence;baby;video;presence;body;affected", "pdf_keywords": ""}, "51c2321244b0a489970e1b52c59b049fdcc5cd46": {"ta_keywords": "automatic translation;automatic translation evaluation;manual machine translation;translation evaluation metrics;machine translation;manual automatic translation;team machine translation;machine translation paper;machine translation mt;translation evaluation;translation paper study;translation mt tool;metrics clqa accuracy;interior knowledge bases;knowledge bases;translation paper;build knowledge bases;evaluation metrics clqa;translation;words performance;metrics clqa;knowledge bases based;information source language;source language;topology earth;translation mt;evaluation metrics;knowledge bases limited;frequency words performance;words performance team", "pdf_keywords": ""}, "bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d": {"ta_keywords": "language grounding embodiment;truly contextual language;language grounding;meaningful natural language;contextual language;contextual language understanding;contextual foundations language;foundations language grounding;linguistic communication;social nature language;natural language;grounding embodiment;utterances meaningful natural;contextual social;linguistic;text corpora deeply;makes utterances meaningful;grounding embodiment social;truly contextual;linguistic communication relies;language successful linguistic;embodiment social interaction;contextual;relate language;nature language;language understanding;natural language processing;language physical world;successful linguistic communication;contextual foundations", "pdf_keywords": "axioms based multimodal;referencing natural language;perception cross referencing;learning semantic axioms;contextualization language human;semantic axioms;purposeful contextualization language;learning semantic;contextualization language;language human experience;linguistic representations;semantic;pre linguistic representations;contextualization;extralinguistic events;natural language;extralinguistic events requirement;contextual information;purposeful contextualization;incremental purposeful contextualization;linguistic;linguistic representations generalize;attach extralinguistic events;contextual;cross referencing natural;objects words evoke;semantic axioms based;multimodal;multimodal perception cross;cross referencing"}, "6b98bef930182a848c027dece1bfb58ca706449d": {"ta_keywords": "characters subwords;subword extraction;characters subwords paper;sequence characters subwords;speech recognition;crowding dynamics animals;leverages pronunciation information;propose subword extraction;subword extraction method;end speech recognition;pronunciation information word;animals walking noisy;dynamics animals walking;speech recognition systems;pronunciation information;subwords;walking noisy landscape;animals walking;method leverages pronunciation;subwords paper;dynamics animals;walking noisy;pronunciation;subwords paper investigate;subword;noisy landscape propose;animals;leverages pronunciation;landscape propose subword;crowding dynamics", "pdf_keywords": "model speech recognition;speech recognition toolkit;utilizing pronunciation dictionary;toend speech recognition;end model speech;speech recognition;end asr based;pronunciation dictionary aligner;learns phonetically meaningful;phonetically meaningful sub;access pronunciation information;utilizing pronunciation;sub word segmentation;pronunciation information work;pronunciation information;method learns phonetically;words speech tasks;sub word modeling;word segmentation;access pronunciation;word segmentation methods;model speech;pronunciation dictionary;learns phonetically;based information pronunciations;speech recognition based;phoneme pairs encoder;word modeling;end end asr;direction utilizing pronunciation"}, "8d7db1b1290e5d6f802e9f1075ef197cb55d754f": {"ta_keywords": "semif semiflexible bidoublet;semiflexible bidoublet;semiflexible bidoublet season;bidoublet season 2012;semif;xmath0 invariant semif;bidoublet season;directed polymers;polymers contact heat;semif semif;directed polymers contact;semif semif semiflexible;single decoding;employ single decoding;decoding fully continuous;framework built janusrecognition;semif semiflexible;janusrecognition toolkit paper;polymers contact;models stage multipass;semi continuous models;built janusrecognition toolkit;decoding;single decoding fully;multipass combination framework;study directed polymers;decoding fully;bidoublet;semi;invariant semif semif", "pdf_keywords": ""}, "acbf4f9a4457cf2884e6018e4653519beef2833a": {"ta_keywords": "complex pentamer glycoproteins;pentamer epithelial cells;function pentamer epithelial;pentamer glycoproteins;pentamer epithelial;pentamer glycoproteins required;potent vaccine antigen;potent vaccine;cells potent vaccine;pentameric complex pentamer;vaccines;vaccine antigen;effective vaccines;pentamer components guinea;vaccine;development effective vaccines;vaccine antigen article;complex pentamer;demonstrated pentameric complex;guinea pig cmv;effective vaccines critical;studies demonstrated pentameric;vaccines critical;pentamer;demonstrated pentameric;pentameric complex;antigen;function pentamer;vaccines critical public;pentamer components", "pdf_keywords": ""}, "790d3503fa95ec32f04c280bd9a52fef6bf1e874": {"ta_keywords": "differencing schemes macroscopic;finite differencing methods;differencing methods macroscopic;models traffic flow;finite differencing schemes;macroscopic models traffic;finite difference schemes;differencing schemes;finite differencing;difference schemes based;comparing finite differencing;microscopic models traffic;upwind forward differencing;differencing methods;models traffic;analysis finite difference;difference schemes;forward differencing;traffic flow consistent;traffic flow;finite difference;certain finite differencing;forward differencing lax;schemes macroscopic models;methods macroscopic models;traffic flow given;macroscopic models;differencing lax friedrichs;macroscopic models better;differencing", "pdf_keywords": ""}, "0fcfa0ef253a81c103854e1dc123d90e7310a0e1": {"ta_keywords": "private deep learning;fairness privacy tradeoffs;differentially private deep;differential privacy disparate;differential privacy;demonstrated differential privacy;better fairness privacy;fairness privacy;privacy tradeoffs;advances differentially private;privacy tradeoffs context;privacy;privacy disparate;privacy disparate impact;differentially private;private deep;tradeoffs context fairness;private;achieving better fairness;fairness xmath0 tradeoffs;fairness xmath0;context fairness xmath0;better fairness;context fairness;learning demonstrated differential;motor activity accuracy;maskawa gkm rule;fairness;deep learning;deep learning demonstrated", "pdf_keywords": "differentially private learning;privacy machine learning;privacy based deep;differential privacy;data differential privacy;using differential privacy;private learning;differential privacy machine;differentially private;differential privacy important;impact differential privacy;differentialial privacy;private learning mechanisms;differentialial privacy used;private aggregation;implications differentially private;private aggregation teacher;privacy;called private aggregation;privacy machine;new privacy;present new privacy;new privacy based;pate private aggregation;privacy important privacy;privacy based;privacy used contexts;privacy used;important privacy;privacy issue machine"}, "634bbe75c34b82e664f1e9f083314b5bdb6ba187": {"ta_keywords": "model removal electroencephalogram;removal electroencephalogram;electroencephalography;analyze electroencephalogram;electroencephalography data;electroencephalogram;artifacts electroencephalography data;removal electroencephalogram eeg;electroencephalogram eeg event;electroencephalography data method;artifacts electroencephalography;signals collecting eeg;analytically analyze electroencephalogram;analyze electroencephalogram eeg;electroencephalogram eeg;electroencephalogram eeg artifacts;eeg event related;eeg event;eeg data intentionally;eeg artifacts electroencephalography;collecting eeg;eeg artifacts;collecting eeg data;eeg data;event signal quantum;signal quantum;signals elicited;signal quantum described;eeg;multiple signals elicited", "pdf_keywords": ""}, "79a6f290cfe8652575e7bb65cfed519bca8f3bd3": {"ta_keywords": "motion particle viscous;particle viscous fluid;particle viscous;viscous fluid;model motion particle;motion particle;viscous;simple model motion;model motion;particle;motion;fluid;present simple model;simple model;model;present;present simple;simple", "pdf_keywords": ""}, "9cda754187545c3cc8f9e9f134c08707269d0fae": {"ta_keywords": "speech pre training;predicting outcome speech;speech text pre;prediction response speech;attention models pre;text pre training;language pre training;speech text data;text speech;unifying speech text;incorporating speech text;speech text;self attention models;encoder unlabeled speech;text speech understanding;approach text speech;unsupervised pre training;outcome speech recognition;pre trained models;data pre trained;models pre trained;attention models;response speech recognition;speech recognition test;speech translation;function speech pre;unsupervised language pre;speech recognition;unlabeled speech demonstrate;speech understanding", "pdf_keywords": "models speech translation;speech translation tasks;multimodal self supervised;wav2vec speech translation;speech translation lm;modal models speech;learns representations modalities;speech translation language;text speech representations;learning text speech;speech images;speech recognition unsupervised;multimodal self;wav2vec speech;speech translation;models speech;translation lm fusion;speech representations;generative language modelling;speech representations particularly;fusion self supervised;approach speech translation;text speech images;modal representation alignment;mono modal models;text speech;w2v bert;including w2v bert;tuning wav2vec speech;learns representations"}, "b50d03ecd9f2055b32451e3c04138a0da07b0f69": {"ta_keywords": "automatically speaking online;activity participant speaking;continuously captures utterances;monitoring conversations ongoing;monitoring conversations;participant speaking;speech recognition activity;captures utterances face;recognize automatically speaking;captures utterances;activity real time;speaking online;poses speaker using;automatically speaking;utterances face poses;analyzer monitoring conversations;face poses speaker;conversations ongoing group;recognition activity participant;poses speaker;real time analyzer;participant speaking laughing;speaking laughing watching;conversations;online manner meeting;speaking online manner;microphone;watching circumstances meeting;advanced audio;recognition activity", "pdf_keywords": ""}, "2bd54adb3b5588281396a4b5dae7db09496b2c61": {"ta_keywords": "superconducting phase transition;temperature superconducting phase;superconducting phase;temperature superconducting;high temperature superconducting;rubidium doped silicon;xmath0 state potts;transition single crystal;crystal rubidium doped;phase transition;state potts model;single crystal rubidium;phase transition single;superconducting;doped silicon;crystal rubidium;xmath0 state;rubidium doped;statistics xmath0 state;single crystal;phase;potts model;silicon;replica symmetric mean;rubidium;mean field theory;state potts;doped silicon paper;replica symmetric;observation high temperature", "pdf_keywords": "question answering tasks;question answering;question categorization crowdsourced;multilingual answer model;answer question categorization;answer model trained;tasks answer answering;answering answer translation;domain question answering;answer answering;answer length multilingual;documents contain answers;answer answering answer;translation answer matching;answering tasks;categorization crowdsourced;answer model;crowdsourced categories;crowdsourcing approach answer;answering tasks answer;categorization crowdsourced categories;answer matching;trained ontonotes;model trained ontonotes;split 590 sentences;trained ontonotes study;crowdsourced categories paper;length multilingual answer;answering answer;answer translation"}, "5e657bc8097c12649d027ca3c16ff7d37df1354d": {"ta_keywords": "multilingual machine translation;multilingual neural machines;training multilingual machine;optimized training multilingual;training multilingual;multilingual neural;languages optimized training;multilingual machine;languages training data;machine translation mt;machine translation;sampling resourced languages;translate multiple languages;study multilingual neural;multilingual;languages training;training sets languages;translation mt models;sets languages training;languages optimized;study multilingual;languages faced imbalanced;multiple languages;sets languages mt;resourced languages increase;languages mt;neural machines performance;multiple languages faced;translate multiple;translation mt", "pdf_keywords": "multilingual machine translation;translation tasks multilingual;multilingual model training;multilingual neural machine;tasks multilingual training;multilingual training sets;model translation tasks;simultaneously multilingual training;multilingual training multiple;performance multilingual machine;performance multilingual training;translation tasks;multilingual training propose;multilingual data best;multilingual training;machine translation model;achieve multilingual training;multilingual neural;neural machine translation;multilingual training rich;multilingual model;propose multilingual neural;performance multilingual;translation model translation;data multilingual model;multilingual machine;achieve multilingual;optimizes usage multilingual;problem multilingual training;multilingual training common"}, "302ae0d991d62dee82b63530b487a50469810af4": {"ta_keywords": "complex spatial actions;interpretable spatial operations;complex spatial pragmatic;mapping natural language;interpretable spatial;language complex systems;spatial actions;instructions complex spatial;spatial operations rich;language complex;3d spatial operations;propose new neural;complex 3d spatial;spatial pragmatic;spatial operations;spatial pragmatic interpretations;3d spatial;complex spatial;inventory interpretable spatial;neural architecture achieves;spatial;natural language descriptions;descriptions require complex;spatial actions block;language instructions complex;neural;neural architecture;natural language;natural language instructions;new neural architecture", "pdf_keywords": "action understanding learn;actions computer vision;visualizing scenes actions;task visualizing scenes;landmarks learning language;action understanding;scenes actions;scenes actions computer;grounding natural language;end trainable neural;trainable neural;visualizing scenes;little landmarks learning;task visualizing;computer vision addition;landmarks learning;trainable neural architecture;computer vision;10 000 actions;000 actions;actions;model task visualizing;concepts subtle movements;scenes;neural;learning language;actions computer;learning model task;action;vision"}, "a73d83e50b5687455336a2adce32a069c77ba163": {"ta_keywords": "dynamics mechanical oscillator;oscillator absence external;mechanical oscillator absence;mechanical oscillator;magnetic field dynamics;oscillator absence;external magnetic field;external magnetic fields;effect external magnetic;oscillator;field dynamics mechanical;absence external magnetic;external magnetic;dynamics mechanical;field dynamics;dynamics;magnetic field;magnetic fields;magnetic;mechanical;investigate effect external;effect external;external;fields;absence external;investigate effect;field;effect;paper investigate effect;absence", "pdf_keywords": ""}, "13608821aa3b369526221182dfbd3a8842549652": {"ta_keywords": "duplex radios relays;relay nodes cost;radios relays;multi relay channel;relay channel;relay nodes;relay channel study;radios relays decode;deployment relay nodes;placement relays;presented relays clustered;forward relaying;cost adding relay;multi relay;relaying presented relays;relays clustered;relaying presented;forward relaying presented;formulas multi relay;duplex radios;relaying;decode forward relaying;relays decode forward;presented relays;node relay;deployment relay;large number relays;problem placement relays;relays clustered close;relays decode", "pdf_keywords": "optimal power allocation;power allocation maximizes;wireless relay networks;power allocation;relay networks;relay networks various;wireless communication multiple;deployment wireless relay;wireless relay;placement relay nodes;relay nodes;optimal power;total power constraint;finding optimal power;power constraint problem;power constraint;allocation maximizes achievable;wireless communication;networks large terrains;model wireless network;networks spatial multiplexing;placement relay;sensor networks large;power constraint constraint;multiple access networks;wireless network;allocation maximizes;sum power constraint;access networks spatial;impromptu deployment wireless"}, "81e684d01bbfb1f4143bb2ffea36cc4791f0530c": {"ta_keywords": "speech enhancement preprocessing;aerial speech recognition;discriminative training acoustic;channel speech enhancement;speech recognition asr;asr automatic speech;multi channel speech;speech recognition;automatic speech;aerial speech;recognize human speech;automatic speech recognition;reverberation time estimation;training acoustic models;speech recognition task;speech enhancement;criteria aerial speech;channel speech;speech recognition techniques;human speech;reverberation;recognition asr automatic;method reverberation;acoustic models;recognition asr;acoustic models including;art automatic speech;reverberation time;techniques discriminative training;discriminative training", "pdf_keywords": ""}, "14dddd1d8cb2e8c5f4e9998fef84e715cb321ac9": {"ta_keywords": "trust artificial intelligence;intelligence ai trust;ai trust humans;ai trust;trust artificial;trustworthy artificial intelligence;trust intelligence;ai model trust;trust intelligence present;trust user artificial;warranted trust intrinsic;trust intrinsic reasoning;trustworthy artificial;trust intrinsic;trust humans;relationship trust artificial;trust precisely nature;trust implicit;cognitive mechanism trust;trust trust;causes warranted trust;trust precisely;warranted trust;mechanism trust;trust implicit explicit;interpersonal trust;design trustworthy artificial;model trust implicit;trust;warranted unwarranted trust", "pdf_keywords": "trust human ai;trust artificial intelligence;trust ai;gaining trust ai;trust artificial;artificial intelligence trusted;formalizing trust;notion trust artificial;formalization notion trust;definition trust human;trust ai paper;trust human;ai using formalization;interested formalizing trust;intelligence trusted;interpersonal trust;trust defined;interpersonal trust defined;definition trust;ai integrated society;trust defined sociology;user ai;organizational trust;user ai using;human ai;trust;artificial intelligence ai;discuss interpersonal trust;notion trust;model organizational trust"}, "941e42ee75fc2bf07078bcfbd14bdf9ca7fe99ff": {"ta_keywords": "training language models;trained language models;learning cross lingual;lingual word embeddings;monolingual language models;language models textual;training monolingual language;models textual training;improve language models;word embeddings;textual training;language models;documentary linguistics;language models support;language models robust;bilingual lexicons;textual training data;languages bilingual lexicons;training language;task documentary linguistics;bilingual lexicons available;word embeddings preliminary;documentary linguistics propose;cross lingual word;training monolingual;trained language;models textual data;unperturbed models textual;cross lingual;lexicons improve language", "pdf_keywords": ""}, "981152cb3f1e11c9cee6af2275b57ef79c621934": {"ta_keywords": "diversity text generation;text generation;based text generation;text generation propose;probability human text;algorithm nucleus sampling;sampling paper neural;improved diversity text;text generation suffers;weighting sampling nucleus;nucleus sampling outperform;resemblance human text;nucleus sampling;sampling nucleus;sampling nucleus sampling;diversity generated samples;search based decoding;human text;diversity text;heuristic sampling;propose heuristic sampling;network based text;repetitive repetitive candidates;trained language model;probability weighting sampling;generated samples achieving;search repetitive repetitive;weighting sampling;generated samples;sampling method inspired", "pdf_keywords": "predicting distribution words;prediction human language;inverse probability sampling;sampling method predicting;sampling;language style distribution;probability sampling;propose heuristic sampling;resemblance human text;distribution words text;heuristic sampling;discouraging sampling;trained language models;new stochastic sampling;sampling methods propose;stochastic sampling;sampling according;traditional stochastic sampling;inverse probability weighting;discouraging sampling according;sampling methods;distribution repetitive candidates;probability sampling algorithm;candidates natural language;stochastic sampling methods;distributions increase diversity;propose probabilistic machine;algorithm generating human;stochastic sampling algorithm;heuristic sampling method"}, "1ea337ac24503d9da8dd9bbf98aac0bfd5920834": {"ta_keywords": "statistical machine translation;machine translation perform;machine translation;speaking style transformation;transcripts speech;transcripts speech recognition;creating clean transcripts;faithful transcripts speech;transforming faithful transcripts;transcripts propose model;machine translation approach;translation approach machine;transcripts perform detailed;transcripts perform;clean transcripts;clean transcripts perform;approach machine translation;words transformations;clean transcripts human;transcripts;transcripts human;transcripts clean transcripts;clean transcripts propose;transcripts clean;transcripts propose;words transformations paper;model faithful transcripts;omitted words transformations;faithful transcripts clean;insertion punctuation correction", "pdf_keywords": ""}, "b8b5b95a0471e0553a0e6cd5086f384cf0f4d4d8": {"ta_keywords": "speech translation s2st;speech speech translation;speech translation;predicting linguistic meaning;translate paralinguistic information;predicting linguistic;translates speech languages;describes machine translation;machine translation module;emphasis sequence transcription;machine translation;translate emphasis information;speech synthesis;describes speech synthesis;estimated emphasis sequence;translates speech;model translate emphasis;language emphasis sequence;speech synthesis module;translated emphasis sequence;target language emphasis;technology translates speech;speech tags;speech languages;translation model;emphasis sequence target;speech tags speech;tags speech speech;method predicting linguistic;communication translation model", "pdf_keywords": ""}, "0f2ea810c16275dc74e880296e20dbd83b1bae1c": {"ta_keywords": "attention web search;gated attention web;attention web;answer selection model;accurate answer selection;answer selection;answering cloze style;cnn daily mail;recurrent neural network;task cnn;query embedding;implementing gated attention;attention;recurrent neural;task cnn daily;answering;query embedding intermediate;attention mechanism based;gated attention;interactions query embedding;benchmarks task cnn;query specific representations;neural network document;questions documents effectiveness;architecture novel attention;attention mechanism;questions documents;answering cloze;representations tokens document;problem answering", "pdf_keywords": "text comprehension learning;text comprehension tasks;model text comprehension;gated attention layers;deepmind deep;rnn encoder decoder;attention layers;present deepmind deep;deepmind deep learning;approaches text comprehension;rnn encoder;comprehension learning;using rnn encoder;learning recurrent;relational learning recurrent;text comprehension;representations using rnn;using gated attention;learning recurrent models;attention layers various;comprehension tasks;gated attentions model;deepmind;present deepmind;using rnn;comprehension learning phrase;reading comprehension;text comprehension called;recurrent models visual;reading comprehension based"}, "e58edbeb41f3d2d24832e6e3abb94baac754e3f7": {"ta_keywords": "text summarization assessing;summarization assessing;automateated evaluation metrics;summarization assessing reliability;tasks text summarization;evaluation metrics older;automatic metrics using;text generation tasks;text summarization;evaluation metrics stand;summary level evaluation;evaluation metrics;method text summarization;automatic metrics;evaluation method text;automateated evaluation;text generation;letter model evaluation;metrics using scoring;generation tasks text;systems automateated evaluation;text summarization paper;evaluation mail;evaluation mail xmath0;development text generation;evaluation single letter;reliability automatic metrics;manual evaluation;model evaluation mail;results evaluation", "pdf_keywords": "summarization systems cnn;evaluating summarization metrics;summarization metrics;meta evaluating summarization;summarization metrics including;evaluating summarization;summarization models;summarization evaluated;summarization models require;dataset machine summarization;summarization evaluated using;text summarization conduct;summarization conduct;summarization systems;rnns meta evaluation;abstractive summarization systems;machine summarization important;development summarization models;summarization important task;text summarization;machine summarization;setting machine summarization;summarization paper propose;summarization;summarization important;used text summarization;human judgment dataset;extractive abstractive summarization;abstractive summarization;learning rnns meta"}, "0180c56bfbfb21243f8605e4c6f6aab2779d3ef0": {"ta_keywords": "user explanation leveraging;natural language explanations;language explanations optimal;explanation leveraging existing;policy based explanation;explanations end user;based explanation techniques;explanation leveraging;generate salient explanations;domain specific explanations;based explanation ported;trust user explanation;explanations generated;language explanations;language explanations used;specific explanations generated;natural language model;explanations optimal;builders natural language;mappings natural language;explanations generated case;natural language;domain knowledge base;user explanation;case based explanation;explanations optimal action;salient explanations;domain knowledge;explanation logic allowing;salient explanations end", "pdf_keywords": ""}, "197fcdfe05d0892ee7b4a98ef6fa74dfbcd14b48": {"ta_keywords": "crowdsourcing guarantee convex;functions crowdsourcing guarantee;computation crowdsourcing;functions crowdsourcing;human computation crowdsourcing;objective functions crowdsourcing;computation crowdsourcing involves;crowdsourcing guarantee;convex inference axiomatic;guarantee convex inference;crowdsourcing;convex inference;ensure convexity inference;crowdsourcing involves joint;crowdsourcing involves;convexity inference;convexity issue human;propose convex objective;convex objective;human computation;convexity inference problem;inference ground;convex function objective;computation human computation;support vector machines;computation human;human computation human;convex objective function;inference ground truth;guarantee convex", "pdf_keywords": "crowdsourcing based axioms;human computation convex;crowdsourcing model guarantee;natural assumptions crowdsourcing;axioms optimization convex;human computation axioms;assumptions crowdsourcing;guarantee convexity inference;inference human computation;assumptions crowdsourcing model;ensure convexity inference;crowdsourcing model;theory crowdsourcing;crowdsourcing;axioms optimization;convexity issue human;convexity inference;guarantee convexity;inference human;theory crowdsourcing based;convexity optimization;present theory crowdsourcing;certain axioms optimization;computation convex;convex objective;model guarantee convexity;human computation;impossible ensure convexity;computation axioms unfortunately;ensure convexity"}, "18c00a9b1e6fde799ec5100cf0b1f37c306d061f": {"ta_keywords": "web search based;approach web search;information retrieval;web queries;search based;web search;web queries posed;web search rules;topic independent query;complex structured queries;search based idea;traditional information retrieval;pages integrating information;queries posed topic;idea web search;independent query methods;structured queries;information extracting;search rules;retrieval assumed queries;fraction web queries;information retrieval assumed;relevant information extracting;search rules paper;extracting data pages;search;retrieval;query methods;statistical learning methods;statistical learning", "pdf_keywords": ""}, "5c283474bbb4838160410e24d33ce89ebaf32c07": {"ta_keywords": "speaker clustering clustering;approach speaker clustering;speaker clustering;speaker clustering problem;suitable speaker clustering;bayesian approach speaker;gaussian mixture model;gaussian mixture;clustering method;variational bayesian approach;scale gaussian mixture;based variational bayesian;propose clustering method;clustering method based;clustering;variational bayesian;mixture model;clustering clustering;paper propose clustering;clustering clustering accuracy;clustering accuracy;approach speaker;mixture model suitable;propose clustering;clustering problem;model suitable speaker;speaker;method based variational;clustering problem investigate;multi scale gaussian", "pdf_keywords": ""}, "2acc25a01a7ab7cd6b1a75d534ad29ea7d26f92d": {"ta_keywords": "search subtopic retrieval;evaluating subtopic retrieval;subtopic retrieval based;subtopic retrieval using;approach subtopic retrieval;retrieval based subtopic;subtopic retrieval;performing subtopic retrieval;relevance ranking;subtopic retrieval problem;baseline relevance ranking;relevance ranking data;marginal relevance ranking;strategy subtopic retrieval;subtopic retrieval accounting;relevance ranking strategy;retrieval methods mixture;subtopic search;subtopics query topic;search subtopic;document ranking;based subtopic search;outperform baseline relevance;retrieval methods;ranking dependent documents;subtopic search subtopic;retrieval based;dependent documents ranking;documents ranking;document ranking dependent", "pdf_keywords": ""}, "0d3baef146655c5727452ccc0dd680d21d92ae4e": {"ta_keywords": "model consumer heterogeneity;model consumer behavior;hierarchical gaussian process;sparse model consumer;consumer behaviors vary;consumer behaviors;activities consumer behaviors;consumer behaviors marketing;consumer behavior separately;propose hierarchical gaussian;marketing activities consumer;model consumer;consumer heterogeneity field;gaussian process approach;behaviors marketing variables;proposed hierarchical gaussian;consumer behavior;hierarchical gaussian;relationship consumer behaviors;gaussian process method;model called consumer;consumer heterogeneity;gaussian process;consumer heterogeneity terms;called consumer heterogeneity;paper model consumer;behaviors marketing;activities consumer;data sparse model;distributions flexible models", "pdf_keywords": ""}, "0735fb79bf34698c1df4461a05ed51c232c412e4": {"ta_keywords": "transformer trained memorize;transformer encoder attention;language transformer trained;learned transformer;transformer trained;encode task transformer;recurrent neural networks;conceivably learned transformer;transformer encoder;programming language transformer;learned transformer map;encoder attention;language transformer;components transformer encoder;task transformer;neural networks finite;transformer encoder form;recurrent neural;model transformer encoder;attention feedforward computation;transformer related complexity;encoder attention feedforward;transformer;trained memorize task;task transformer related;encode task;state machines;necessary encode task;finite state machines;neural", "pdf_keywords": "transformers trained sequences;transformer expressiveness;matrices transformer expressiveness;rnns abstract computational;transformers trained;networks rnns abstract;transformer expressiveness important;networks rnns;particular transformers trained;trained sequences;neural networks rnns;recurrent neural networks;transformer architecture abstracts;rnns;trained sequences length;rnns abstract;practice particular transformers;transformers fully;machine translation;transformer architecture;transformers;input sequences arbitrary;matrices transformer;machine translation particular;transformers fully recognize;particular transformers fully;transformer;operations sequences selection;particular transformers;transformer network"}, "6fea118a29d78340ae26c465ff06e80e55efbe3b": {"ta_keywords": "continual learning;continual learning learn;directed continual learning;jointly learns;model jointly learns;learning learn incrementally;learn incrementally process;learns;learning;learn incrementally;learns provide best;jointly learns provide;learns provide;fluid understand goals;learning learn;model motion;motion particle viscous;goal directed continual;model motion particle;particle viscous;absorb information incrementally;information incrementally;particle viscous fluid;simple model motion;viscous fluid understand;directed continual;fluid understand;colliders summer 2012;motion;colliders", "pdf_keywords": "comprehension question answering;question answering models;question answering;model question answering;models text comprehension;answering models;question answering consumes;model answer prediction;question answering qa;answer prediction;incremental reading context;incremental reading;standard question answering;text comprehension;incremental models text;question answering care;learn text incrementally;text incrementally;text comprehension use;answering consumes text;answering models reason;problem incremental reading;answering;context language comprehension;text incrementally introduce;answering consumes;answer prediction paper;answering qa;introduce incremental models;incremental model answer"}, "92731a953ad063eab1bc90dc541fb956f147a6ba": {"ta_keywords": "food preferences motivating;reasoning restaurant software;preferences motivating;food preferences;reasoning restaurant;motivating foodies;preferences motivating examples;preference based;preference representation reasoning;personal preference based;restaurant software seminal;preference representation;restaurant software;representation reasoning restaurant;used food preferences;preference handling;particularly motivating foodies;preference based presentation;preference handling used;introduce concept caloric;restaurant;concept caloric;consumed single participant;preference;problem personal preference;art preference representation;measure energy consumed;motivating foodies authors;concept caloric curve;menus seating", "pdf_keywords": ""}, "0791fe161d947d1e4d3af279b261155b88bc9ddf": {"ta_keywords": "optimized clinical prediction;clinical prediction tasks;clinical time series;clinical prediction;benchmark mortality prediction;model ensembles predictions;model ensembles;postulated temporal clustering;ensembles predictions;state art prediction;invariant temporal clustering;ensembles predictions based;ensemble;temporal clustering;prediction tasks;ensembles;temporal clustering invariance;prediction task outperforms;ensemble mre;networks optimized clinical;mre model ensembles;temporal clustering experiments;multiresolution ensemble mre;efficient data augmentation;sequences multivariate clinical;mortality prediction task;multivariate clinical time;multiresolution ensemble;ensemble mre model;propose multiresolution ensemble", "pdf_keywords": "benchmark mortality prediction;benchmark clinical prediction;propose data augmentation;date deep learning;healthcare time series;usefulness temporal clustering;clinical prediction tasks;data augmentation;medical time series;data augmentation schemes;mortality prediction task;data medical time;temporal clustering;exploit temporal clustering;data augmentation operator;data augmentation exploit;benchmark mortality;mortality prediction;augmentation exploit temporal;demonstrate data augmentation;benchmark tasks hospital;demonstrate temporal clustering;e\ufb03cient data augmentation;temporal clustering invariance;task date deep;healthcare data;map benchmark mortality;data augmentation procedure;mimic iii dataset;clinical prediction"}, "8d35230fec724398bed3f5939e9fa6a94f55a785": {"ta_keywords": "learning differential privacy;private machine learning;differentially private algorithms;differential privacy privacy;differential privacy;consider differential privacy;differential privacy popular;private datasets;data private datasets;differentially private;learned differentially privately;large differentially private;differentially private machine;data privacy;private algorithms;private algorithms frequently;data privacy preserved;data private;functions differentially private;non differentially private;private algorithms explore;privacy privacy;differentially privately;privacy preserving;privacy;privacy preserving machine;machine learning differential;privacy privacy preserving;powerful definitions privacy;private datasets number", "pdf_keywords": "private machine learning;learning models privacy;data mining privacy;learning di\ufb00erential privacy;private algorithms;di\ufb00erentially private algorithms;mining privacy;private data;private data leakage;algorithms di\ufb00erentially private;differential privacy output;differential privacy;protection private data;privacy privacy;private algorithms paper;privacy output;private data release;privacy preserving;models privacy;privacy privacy preserving;di\ufb00erentially private data;privacy;di\ufb00erential privacy privacy;privacy preserving machine;mining privacy concerns;learned differentially privately;involves differential privacy;privacy output used;models privacy important;di\ufb00erential privacy"}, "6eae6230ae277b6915706ec05241c8db6b9fab86": {"ta_keywords": "nitride ring resonators;ring resonators algorithm;xmath0 aluminum nitride;aluminum nitride nitrosymmetric;resonators algorithm;nitride ring;nitride nitride ring;metric xmath0 aluminum;resonators algorithm based;crystal rubidium library;ring resonators;similarity search library;nitride nitrosymmetric nitride;nitrosymmetric nitride nitride;similarity search;nitride nitrosymmetric;new similarity search;nitrosymmetric nitride;estimating distance points;single crystal rubidium;distance points;estimating distance;metric xmath0;distance;distance points plane;aluminum nitride;crystal rubidium;xmath0 aluminum;transition single crystal;measured xmath0 xmath1", "pdf_keywords": ""}, "8872e32284467fcbeadd1edd2f11aff077de4ccf": {"ta_keywords": "ripperk machine learning;existing rule learning;proposed rule learning;rule learning;rule learning systems;algorithm ripperk competitive;rule learning algorithm;algorithm ripperk;large noisy datasets;learning algorithm large;noisy datasets;learning algorithm achieves;resulting algorithm ripperk;process noisy datasets;noisy datasets containing;noisy datasets paper;machine learning;ripperk competitive c4;benchmark problems existing;ripperk competitive;machine learning algorithm;ripperk machine;learning algorithm;benchmark problems;22 benchmark problems;algorithm large diverse;ripperk;learning systems computationally;benchmark;predict outcome competition", "pdf_keywords": ""}, "47442ea4c28d631a9d46a9c23454684b834e49ea": {"ta_keywords": "coreference trigger word;dataset biomedicallanguage processing;new dataset biomedicallanguage;dataset biomedicallanguage;novel approach coreference;biomedicallanguage processing;approach coreference trigger;coreference trigger;distributional semantics corpus;coreference;semantics corpus;biomedicallanguage processing 25;approach coreference;semantics corpus introduce;corpus vocabulary captures;corpus vocabulary;unsupervised event neural;essential distributional semantics;corpus;distributional semantics;biomedicallanguage;event neural;corpus introduce;corpus introduce novel;original corpus vocabulary;trigger word;event neural network;original corpus;vocabulary captures;trigger word refers", "pdf_keywords": ""}, "df873bde0b44e543634d109a7a8b1ba7dfaa8187": {"ta_keywords": "learns latent ontological;components learned ontology;hierarchy latent semantic;latent semantic relationships;topic models;knowledge bases;models topic models;existing knowledge bases;topic models infer;latent ontological structure;learned ontology;model corpus web;jointly modeling text;learned ontology paper;semantic relationships;structure predefined ontology;concept hierarchy latent;modeling text latent;latent concept hierarchy;semantic relationships entities;text existing knowledge;predefined ontology extracts;latent ontological;ontology extracts facts;latent semantic;facts corpus;knowledge bases kbs;ontological structure;identifies facts corpus;model corpus", "pdf_keywords": ""}, "22dd93fe1a0b8e9cb83eaff6e2ecca0cd6693294": {"ta_keywords": "speech recognition routine;person speech recognition;speech recognition;speech recognition els;automatic speech recognition;detecting speech;automatic speech;detecting speech segments;softmax;pre softmax;measure speed sound;end automatic speech;method detecting speech;sound turbulent;pre softmax output;ctc pre softmax;sound turbulent turbulent;speech segments;sound sound turbulent;softmax output;recognition routine prediction;speech segments simple;recognition routine;softmax output experimental;speed sound;speed sound sound;speech region vavad;end person speech;person speech;neural network", "pdf_keywords": "voice activity detectors;voice activity detector;voice activity detection;recognition voice activity;voice activity;neural networks audio;external voice activity;training external voice;speech recognition;audio segmentation essential;speech recognition voice;networks audio segmentation;conventional voice activity;automatic speech;recognition voice;automatic speech recognition;audio segmentation;approach automatic speech;external voice;attention based encoder;complexity automatic speech;speech recognition based;networks audio;short term memory;hybrid ctc attention;activity detectors;ctc attention framework;voice;activity detectors proposed;activity detection integrated"}, "9d332ad27bfce66ee725b413aa07bd93c355efdf": {"ta_keywords": "language augmentation framework;new language augmentation;language augmentation;robustness natural language;performance cellular automata;augmenter new language;cellular automata based;language processing enhancing;models natural language;language models;cellular automata;natural language models;language processing;present nl augmenter;automata;automata based;nl augmenter new;nl augmenter;data augmentation;language models assessed;augmentation framework;automata based robutstststam;noise data augmentation;natural language processing;data augmentation important;natural language;augmentation framework supports;robustness evaluation models;new language;robustness evaluation", "pdf_keywords": "generation learning paraphrastic;natural language tasks;evaluation nlp tasks;natural language generation;machine translation;nlp tasks;machine translation multilingual;learning paraphrastic sentence;paraphrastic sentence embeddings;learning paraphrastic;transforming input sentences;language generation learning;nlp tasks labeled;evaluation nlp;present machine translation;language generation;tasks language models;development evaluation nlp;translation multilingual natural;language tasks;data natural language;sentence embeddings;translated sentences collect;input sentences translated;multilingual natural language;generation learning;sentence embeddings used;language tasks language;evaluating models syntaxgym;generate diverse"}, "8057a5e7bcb0be7059a6e632124bc861b533c794": {"ta_keywords": "train speech recognition;response speech recognition;speech recognition external;speech recognition task;winner speech recognition;speech recognition;sequential discriminative training;train speech;approach train speech;discriminative training;speech recognition based;learns predict;learns predict best;predict winner speech;recurrent neural network;matrix adaptation;covariance matrix adaptation;matrix adaptation strategy;chime challenge chime4;challenge chime4;challenge chime4 developed;chime challenge;channel track task;recognition based cross;algorithm learns predict;learning algorithm learns;training objective function;based cross entropy;4th chime challenge;algorithm learns", "pdf_keywords": ""}, "88167f36dced91c279162d68af7225f2b4e2091c": {"ta_keywords": "natural language downstream;language downstream tasks;language downstream;masked language models;transferred natural language;pre training structured;pre trained unstructured;language models corpora;human language data;pre trained data;benchmarks pre trained;language models;language data;training structured data;language data gives;training structured;trained unstructured data;non human language;trained non english;trained unstructured;performance pre trained;corpora certain features;language end pre;trained data contributes;pre trained;models corpora;masked language;uncover pre trained;based masked language;human language", "pdf_keywords": "natural language downstream;downstream natural language;pretraining natural language;language downstream tasks;language downstream;trained transformer language;language pre training;models pretrained data;learns certain linguistic;pre training structured;transformer language model;transferred natural language;pretrained data;linguistic features pre;natural language tasks;features pre trained;models pretrained;pre trained model;pre training data;language tasks;pre trained transformer;low resource language;language tasks results;language model;language pre;pretrained data particular;resource language pre;transformer language;training structured data;training structured"}, "f752bf6f8c1502b8cb58aa1483ef598f9fc0d44c": {"ta_keywords": "polynomially stable networks;generate nets;generate nets given;generating polynomial time;nets given;stable networks;nets given number;stable networks article;nets;nodes;xmath0 hat tournament;polynomial time polynomially;networks;polynomially stable;example generate;polynomial time;generating polynomial;universal coefficient xmath0;random set xmath0;pellet example generate;generating;generate;number nodes;networks article present;universal universal coefficient;given number nodes;universal coefficient;tournament alternating;tournament alternating sign;networks article", "pdf_keywords": ""}, "5ebe542ee1a7eab7aad8e36ed53dbdd7ebd98c8d": {"ta_keywords": "encapsulation auto encoders;auto encoders multi;auto encoders;representation given scene;deep neural;toconvolutional neural networks;encoders multi dimensional;core computer vision;deep neural networks;use deep neural;encoders multi;computer vision;encoders;neural networks;computer vision problem;multi dimensional representation;neural networks core;objects involved scene;problem neural networks;concept encapsulation auto;neural networks 1_;encapsulation auto;scene efficient;toconvolutional neural;neural;neural networks based;given scene efficient;dimensional representation;alternative toconvolutional neural;vision problem infer", "pdf_keywords": ""}, "73484141ca58d9714ac592e3667de416322b51eb": {"ta_keywords": "wavelet transform;crossing representation wavelet;representation wavelet transform;wavelet transform practical;signal reconstruction;reconstructing image single;representation wavelet;wavelet transform domain;reconstructing image;domain signal reconstruction;method reconstructing image;reconstructed image;reconstruct original signal;signal zero crossing;wavelet;original signal zero;reconstructed image good;zero crossing representation;signal reconstruction letter;multiresolution analysis zero;multiresolution analysis;representation proposed reconstruct;transform domain signal;analysis zero crossing;shown reconstructed image;multiresolution;signal zero;signal processing;new zero crossing;iterative linear transformation", "pdf_keywords": ""}, "29263fa3632951be0ca617988d7c9ce651e74393": {"ta_keywords": "multilingual translation models;performance multilingual encoders;multilingual machine translation;trained machine translation;translation models outperform;multilingual encoders;multilingual encoders decoders;translation models;machine translation models;outperform multilingual machine;models outperform multilingual;machine translation model;multilingual translation;accuracy multilingual training;multilingual machine;translation model;machine translation mt;performance multilingual;results multilingual translation;outperform multilingual;multilingual training;translation mt systems;decoders initialized multilingual;machine translation;speed accuracy multilingual;translation models terms;compare performance multilingual;languages models low;multilingual training correlation;translation model different", "pdf_keywords": "multilingual machine translation;machine translation multilingual;translation multilingual training;translation neural machine;machine translation neural;translation neural;arrays multilingual training;translation multilingual;machine translation mt;multilingual machine;neural machine translation;machine translation;multilingual training promising;multilingual training essential;languages predictive model;multilingual training;propose multilingual machine;translation based machine;multilingual training key;pairs multilingual training;multilingual;based machine translation;arrays multilingual;languages predictive;translation mt systems;resource languages predictive;machine translation based;language pairs multilingual;different multilingual;pairs multilingual"}, "8b1be80cc1fabcd9ccea76d9a8830e2b07e71f0c": {"ta_keywords": "barflies barfly beeradvocate;barflies barfly;patterns barflies barfly;barflies;patterns barflies;explore patterns barflies;barfly beeradvocate;introduction raspberries;barfly beeradvocate review;brief introduction raspberries;barfly;fruit vegetable beer;dynamics state potts;beryllium ruthenium generate;ruthenium generate similarly;raspberries;vegetable beer;introduction raspberries popular;creation fruit;magnetic;ruthenium generate;dynamics;dynamics state;beryllium ruthenium;presence external magnetic;recipe creation fruit;beer;vegetable beer article;raspberries popular blend;raspberries popular", "pdf_keywords": ""}, "76b36a059c0d8d66a1bf910de32b34dba19482fa": {"ta_keywords": "synchronous decoding;synchronous decoding algorithm;blockwise synchronous decoding;stitch decoding;block decoder;block decoder propose;proposed decoding;based block decoder;proposed decoding algorithm;decoding;decoding algorithm achieves;achieved block synchronization;encoder decoder;decoding algorithm hybrid;propose block synchronization;based block synchronization;decoding algorithm;decoder;encoder;block synchronization achieved;latency streaming;encoder decoder automatic;synchronization achieved;decoder automatic;continuous block synchronization;problem stitch decoding;latency streaming style;synchronization achieved block;block synchronization;synchronization", "pdf_keywords": "decoder automatic speech;speech recognition previous;synchronous decoding algorithm;speech recognition;synchronous decoding;recognition speech recognition;blockwise synchronous decoding;speech recognition speech;recognition speech;automatic speech;automatic speech recognition;decoder automatic;encoder decoder automatic;blockwise encoder decoder;decoding;approach automatic speech;decoder;encoder;repeated tokens decoding;synchronous beam search;encoder decoder;decoding algorithm;speech recognition paper;propose blockwise encoder;english aishell mandarin;aishell mandarin;decoding algorithm does;aishell mandarin csj;tokens decoding;blockwise encoder"}, "caabc3d0c5ece9d44fb2216a347362d4609934c1": {"ta_keywords": "large language models;programming languages large;code natural language;language models lmms;language modeling release;language models;programming languages targeted;programming languages;new model polycoder;language modeling;code synthesizing;results programming languages;large class programming;model polycoder;large language;gpt architecture trained;class programming languages;model evaluation length;synthesizing code;synthesizing code natural;natural language modeling;code lms;model polycoder base;models open source;languages large language;code synthesizing code;mainly natural language;languages large;polycoder;trained 249gb code", "pdf_keywords": "repositories language models;benchmarks language models;models trained github;language code generation;language models recently;code natural language;language models widely;code modeling;language models;trained github;trained github repositories;natural language code;code modeling identify;github repositories language;code generation;benchmarks language;lingual corpus code;code downstream tasks;repositories language;bidirectional models trained;intrinsic benchmarks language;polycoder extrinsic evaluation;source model trained;corpus code exploding;code synthesizing;tasks code modeling;language code;large open source;open source model;code downstream"}, "3a8129e6fe3ad9bc3a51e44da32424e38612e4cc": {"ta_keywords": "tensorlog reasoning uses;called tensorlog reasoning;tensorlog reasoning;logics large knowledge;belief propagation;inference logics;inference logics based;tensorlog;probabilistic deductive database;process belief propagation;belief propagation context;belief propagation bp;called tensorlog;logics large;logics based logics;database called tensorlog;logistic logics;logics based;based logics;knowledge deep gradient;logical theories containing;logics logics;logics logics logics;approach inference logics;logics;deductive database;large knowledge bases;knowledge bases;probabilistic deductive;logistic logics ttensor", "pdf_keywords": "learning reasoning databases;learn probabilistic logic;probabilistic deductive database;probabilistic deductive databases;reasoning databases;probabilistic logic programs;reasoning deep learners;probabilistic logics;deductive databases;deductive database;reasoning databases uncertainty;deductive databases tuple;deep learning reasoning;probabilistic logic;called tensorlog reasoning;learning reasoning;learning reasoning systems;inductive logic programming;probabilistic inductive logic;tensorlog reasoning;knowledge base inference;probabilistic model logic;database called tensorlog;logical reasoning deep;reasoning systems probabilistic;order probabilistic logics;tensorlog reasoning performed;deductive database called;data probabilistic deductive;logic programming"}, "891fd2690a21f29b2ab54ee2249261d93c8cbc5c": {"ta_keywords": "crowdsourcing worker answers;crowdsourced labels;crowdsourcing train;crowdsourcing;crowdsourced;fast obtain crowdsourced;using crowdsourcing train;crowdsourced labels suffer;crowdsourcing train machine;obtain crowdsourced;obtain crowdsourced labels;using crowdsourcing;crowdsourcing worker;provide using crowdsourcing;setting crowdsourcing;stage setting crowdsourcing;setting crowdsourcing worker;machine learning tasks;errors crowd sourcing;answering randomly stage;crowd sourcing;crowd sourcing workers;inadvertent errors crowd;improving quality labeled;labeled data seek;answering randomly;noisy reference answer;reference answer goal;errors crowd;refrain answering randomly", "pdf_keywords": ""}, "0ce6db2fb8c691ff8a89bd01f379ce92b1d248d0": {"ta_keywords": "contagion classification soft;contagion classification;models contagion classification;topical retrieval classification;probabilistic models contagion;retrieval classification;modeling text;topical retrieval;retrieval classification tasks;appropriate topical retrieval;informative words rare;classification soft clustering;introduce probabilistic models;clustering based poisson;modeling text implicitly;classification soft;classification;bayesian methods predicting;probabilistic models;soft clustering;classification tasks;informative words;soft clustering based;introduce probabilistic;approaches modeling text;words rare;words rare introduce;relate sentiment;statistical learning;clustering", "pdf_keywords": ""}, "6994b9860248aea10f8b8bac74e87afd3fcdc842": {"ta_keywords": "google sets terms;language set expansion;google sets;web google sets;superior google sets;sets named entities;google sets paper;benchmark sets languages;structured documents;expanding sets named;structured documents written;semi structured documents;markup language;expansion using web;written markup language;human language set;markup language human;method expanding sets;documents written markup;expanding sets;set expansion;benchmark sets;sets languages;set expansion refers;named entities;36 benchmark sets;set expansion using;does set expansion;sets paper propose;distance animals", "pdf_keywords": ""}, "49f9afa4d0405019d01b55529ce4167380acc103": {"ta_keywords": "speech modification capable;input speech waveform;manipulating phoneme sounds;speech waveform;speech production mapping;mapping articulatory speech;developed speech modification;degradation synthetic speech;synthetic speech;speech articulatory inversion;articulatory speech production;speech modification;performing speech articulatory;speech caused modeling;filter input speech;vocoderbased waveform generation;speech articulatory;articulatory speech;speech waveform time;waveform modification;speech production;synthetic speech caused;vocoderbased waveform;waveform generation framework;waveform generation;phoneme sounds manipulating;direct waveform modification;manipulating unobserved articulatory;method manipulating phoneme;sequentially performing speech", "pdf_keywords": ""}, "e1bb329621de73d08c47beae9b5439a1c244eb1a": {"ta_keywords": "augmentations novelty detection;novelty detection;suited novelty detection;various novelty detection;novelty detection essential;novelty detection scenarios;novelty detection designing;contrastive learning visual;learning visual;contrastive learning;augmentations novelty;contrastive learning methods;learning visual representations;conventional contrastive learning;representation suited novelty;shifted augmentations novelty;new detection;training scheme contrasts;propose new detection;various novelty;learning representation;novelty;success contrastive learning;positive video clip;learning representation suited;datasets inspired recent;new detection scheme;positive video;visual representations propose;detection", "pdf_keywords": "learning distribution detection;detection semantic ood;contrastive learning distribution;semantic ood samples;contrastive learning features;detection semantic;contrastive learning;unlabeled semantic ood;distribution detection;learning distribution;novel contrastive learning;characteristic contrastive learning;power contrastive learning;contrastive learning extracts;settings contrastive learning;simclr contrastive learning;supervised learning uncertainty;contrastive learning method;contrastive learning prior;self supervised learning;learning features projection;learning uncertainty;distribution detection problems;novel detection;terms ood detection;learning uncertainty robustness;semantic ood;novel detection score;augmentations distribution shifting;learning features"}, "931a103258c96a1230dc5c7e38a1cd0b095b9d62": {"ta_keywords": "model word segmentation;model continuous speech;continuous speech phoneme;learning language model;learn language model;speech phoneme;large vocabulary speech;speech phoneme lattice;language model continuous;gram language model;language model noisy;language model directly;word segmentation;word segmentation based;simple language model;language model;language model prior;language model construction;learning model word;model prior linguistic;language model novel;yor language model;robustly learn language;new language model;vocabulary speech;continuous speech;learning word;learning word boundaries;vocabulary speech paper;learning language", "pdf_keywords": ""}, "a5690b0a514a7cbc913871e41e54c9ad4f6362db": {"ta_keywords": "task machine translation;translation task machine;machine translation task;machine translation;machine translation performed;automatic evaluation reliable;translation task;judgment automatic evaluation;mt machine translation;automatic evaluation;machine translation mt;evaluated submissions human;mismatch human judgments;ble machine translation;human judgment automatic;submissions human judgment;translation mt machine;evaluation reliable;automatic evaluation ble;reliable manual evaluation;correlated automatic evaluation;human judgments highly;translation performed;human judgments;translation;judgment automatic;reddit task predicting;manual evaluation;evaluated submissions;evaluation", "pdf_keywords": "machine translation tasks;neural machine translation;machine translation;machine translation mt;translation tasks;particular machine translation;translation mt systems;translation tasks submitted;errors informal language;translation mt;evaluation campaign neural;informal language linguistic;improve models robustness;grammatical errors informal;translation;computational linguistics;conference computational linguistics;campaign neural machine;improve nmt robustness;human quality news;errors informal;neural machine;neural models facto;improvements following challenges;variations grammatical errors;challenges improve nmt;quality news domain;quality news;models robustness noisy;campaign neural"}, "6aecc93c2d61da073b70dec19795172ca1ff3405": {"ta_keywords": "counterfactually invariant predictors;causal inference counterfactual;predictors access counterfactuality;inference counterfactual invariance;inference counterfactual;counterfactuality examples machine;learning approximately counterfactually;counterfactual invariance outof;counterfactual invariance;data counterfactual invariance;counterfactuality;counterfactual invariance powerful;counterfactual invariance implies;counterfactual;data counterfactual;tools causal inference;counterfactuality examples;model text classification;text classification check;counterfactually invariant;connect counterfactual invariance;changes sentiment predictor;structure data counterfactual;text classification;sentiment predictor;counterfactually;text classification powerful;access counterfactuality examples;tools causal;causal inference", "pdf_keywords": "causal representation learning;language data counterfactual;causal inference formalize;models counterfactual invariance;causal inference;data counterfactual invariance;tools causal inference;approach causal representation;models counterfactual;causal representation;learning language data;counterfactual invariance;counterfactual invariance fundamental;unknown counterfactual invariance;data counterfactual;counterfactual invariance optimal;set counterfactual invariance;causal structures;causal;counterfactual;counterfactual invariance shown;causal structures used;tools causal;use tools causal;predicting relationships movies;set counterfactual;effects unknown counterfactual;learning language;language data;approach causal"}, "dfd4beb1ecf70b07eb4a52e6ae58f3357e66f478": {"ta_keywords": "crowdsourced speech recognition;noised examples training;speech recognition;presence crowdsourced speech;crowdsourced speech;person presence crowdsourced;recognition person;recognition;speech recognition paper;matched representations layer;data augmentation;recognition person presence;crowdsourced;models background noise;method recognition person;presence crowdsourced;matched representations;perform data augmentation;recognition paper hypothesize;artificially noised examples;examples training;representations layer;noised examples;examples training set;data augmentation adding;domain noise;coerce matched representations;adding artificially noised;representations layer harden;noise performance", "pdf_keywords": "robust recognition noisy;recognition noisy data;noise data augmentation;adversarially trained convolutional;recognition noisy;trained convolutional;recognition deep learning;neural networks robust;recognition deep;training adversarially trained;training adversarially;adversarially trained;noise invariant representations;networks robust recognition;learning convolutional;propose deep convolutional;trained convolutional neural;deep learning;network cnn;deep convolutional;speech recognition deep;learning convolutional neural;inductive bias regularization;deep convolutional neural;background noise datasets;noise datasets;robust recognition;machine learning convolutional;adversarially;bias regularization"}, "a427334e296b6be27c3a9c7d6b942d6468e487b8": {"ta_keywords": "deployment sensors relays;sequential deployment wireless;relay placement;wireless sensors relays;relays line forest;optimal sequential deployment;deployment wireless sensors;relay placement problem;wireless relay;deployment wireless sensor;sequential deployment sensors;radio propagation forestlike;sensor networks situations;impromptu deployment wireless;formulate relay placement;wireless relay network;sensors relays;sensors relays line;deployment algorithms;deployed network objective;connected network deployment;deployment wireless;network deployment;efficient deployment algorithms;relay network;multihop wireless relay;relay;sequential deployment;network objective;propagation forestlike environments", "pdf_keywords": ""}, "823956ee7b994735f3605f426a71e7f85d86f1d4": {"ta_keywords": "semantic frame induction;unsupervised semantic induction;semantic frame;scale semantic frame;verb class clustering;semantic induction;frame induction;unsupervised semantic;semantic induction paper;perform unsupervised semantic;frame induction problem;frame based dataset;frame induction perform;frame derived dataset;dataset frame derived;cast frame induction;class clustering task;triclustering problem generalization;web scale semantic;based dataset frame;dataset frame;scale semantic;generalization clustering;clustering task;class clustering;results frame based;frame based;semantic;clustering;problem generalization clustering", "pdf_keywords": "semantic frame induction;unsupervised semantic frame;unsupervised frame induction;frame induction approach;framenet construction unsupervised;entailment frame induction;semantic frame;frame induction methods;frame induction based;unsupervised semantic;frame induction;textual entailment frame;frame induction techniques;language processing frame;answer extraction;methodology frame induction;question answering textual;frame induction important;development unsupervised semantic;extraction question answering;learning machine translation;entailment frame;answering textual;present frame induction;answering textual entailment;unsupervised frame;framenet construction;answer extraction question;automatizing process framenet;machine translation"}, "6eb5029dabd60eb47fddebb5919c613d399fddc6": {"ta_keywords": "discriminative feature transformation;linear discriminant analysis;propose discriminative feature;discriminative feature;noisy speech recognition;discriminant analysis lda;continuous speech recognition;based sequential discriminative;discriminant analysis;paper propose discriminative;speech recognition;improve discriminability maximizing;speech recognition tasks;performance combination discriminative;sequential discriminative criterion;linear discriminant;lda training proposed;speech recognition corpus;discriminative criterion;combination discriminative feature;discriminability maximizing;discriminative criterion computed;improve discriminability;sequential discriminative;problem linear discriminant;discriminability maximizing ratio;standard lda training;lda training;recognition corpus spontaneous;propose discriminative", "pdf_keywords": ""}, "edca37b2004861513c54e7e97b64d4e00e72003f": {"ta_keywords": "determinate clauses learnable;learning indeterminate clauses;clauses learnable;clauses learnable primary;learning logic programs;learning logic;learning indeterminate;depth determinate clauses;logic programs;logic programs examples;method learning logic;clause language constant;horn clause language;indeterminates equivalent learning;clause language;determinate clauses;clauses indeterminates;programs examples recursive;recursive constant depth;indeterminate clauses;learning dnf present;examples recursive;clauses indeterminates equivalent;learning dnf;examples recursive constant;indeterminate clauses indeterminates;problem learning single;equivalent learning dnf;learnable primary technical;single horn clause", "pdf_keywords": ""}, "49775f20431c4a605c5dcc7111c9fe785bf00c62": {"ta_keywords": "probabilistic state reweighting;limitations policy gradient;policy gradient;markov decision processes;policy gradient pg;markov chain monte;markov decision;probabilistic state;movement pedestrian crowd;track movement pedestrian;policy learn risk;modeled markov;movement pedestrian;modeled markov chains;state distributions overcome;implementation probabilistic state;markov;markov chains;pedestrian crowd;monte carlo mcmc;markov chain;markov chains markov;pedestrian;probabilistic;uses state distributions;carlo mcmc methods;chains markov;research markov decision;research markov;risk sensitive policies", "pdf_keywords": "learning policy gradient;policy gradient based;policy gradient;policy gradient algorithm;risk gradient descent;critic reinforcement learning;plugged policy gradient;risk gradient;coherent risk gradient;gradient based risk;gradient plugged policy;pg learn risk;finding policy maximizes;policy maximizes dynamic;algorithm finding policy;learning policy;reinforcement learning;policy maximizes;gradient descent ascent;actor critic reinforcement;risk maximization function;critic reinforcement;risk maximization;gradient descent;reinforcement learning algorithm;learn risk;based risk maximization;method learning policy;policies propose algorithm;finding policy"}, "4350ce87dd3ec067f1e583ad415f71ef4ba6075e": {"ta_keywords": "dependency parser japanese;japanese dependency parsing;dependency parsers trained;parser japanese trained;dependency parsers;based dependency parser;parser japanese;dependency parsing;dependency parser;parsers trained;dependency parsing approach;partially annotated corpora;annotated corpora;parsers trained fully;word based dependency;annotated corpora allowing;trained partially annotated;trained fully annotated;parsers;japanese dependency;experiments japanese dependency;parsing;parsing approach allows;parser;linguistic resources reduction;art dependency parsers;fully annotated data;partially annotated;available linguistic;domain adaptation", "pdf_keywords": ""}, "598e2d69d3573adae1f0e3bbe54d10c43f48e0b0": {"ta_keywords": "stochastic gradient;stochastic gradient method;algorithm gradient noise;stochastic optimization;proximal stochastic gradient;probability stochastic optimization;noise efficiency gradient;stochastic algorithms;stochastic optimization distributional;study stochastic optimization;optimization distributional drift;stochastic optimization problems;stochastic algorithms iterate;guarantees stochastic algorithms;drift stochastic tracking;gradient noise;efficiency proximal stochastic;gradient based optimization;efficiency gradient gradient;algorithm gradient;convergence guarantees stochastic;concept drift stochastic;optimization algorithm gradient;efficiency gradient;stochastic tracking performative;proximal stochastic;stochastic tracking;drift stochastic;tracking performative prediction;guarantees stochastic", "pdf_keywords": "online learning gradient;drift stochastic optimization;learning gradient drift;approximation online learning;stochastic gradient methods;gradient drift stochastic;dependent stochastic gradient;stochastic gradient;learning distributional drift;distributional drift learn;decision dependent learning;stochastic gradient method;stochastic optimization;optimization distributional drift;optimization stochastic approximation;stochastic approximation online;stochastic optimization distributional;decision dependent stochastic;learning decision dependent;stochastic optimization stochastic;optimization stochastic;dependent learning;online learning distributional;proximal stochastic gradient;learning gradient;gradient methods learning;drift learn data;gradient drift;learning decision;stochastic approximation"}, "22f93927c487e0e0e0d2844489423bcb5d21b45c": {"ta_keywords": "participants xmath0 dimensional;number participants xmath0;total number participants;number participants;xmath0 dimensional euclidean;participants xmath0;distribution total number;xmath0 dimensional;distribution total;dimensional euclidean space;probability distribution total;numerical study probability;distribution;dimensional euclidean;dimensional;total number;study probability distribution;xmath0;probability distribution;participants;euclidean space;numerical study;study probability;results numerical study;numerical;results numerical;euclidean;present results numerical;probability;space", "pdf_keywords": ""}, "19803adec3b97fb2e3c8097f17bf33fabf311795": {"ta_keywords": "machine weak supervision;entanglement entropy quantum;entanglement entropy;compute entanglement entropy;weak supervision labeled;generated weak supervision;entropy quantum;supervised fine tuning;trained lm machines;using weak supervision;entanglement;compute entanglement;fully supervised;performance fully supervised;fully supervised fine;quantum;weak supervision;weak supervision problem;method compute entanglement;weak supervision work;supervised fine;trained lm;tuning pre trained;pre trained lm;supervised;machines using weak;supervision labeled;contrastive self training;supervision labeled data;entropy quantum presence", "pdf_keywords": "semi supervised regularized;semi supervised;semi supervised learning;supervised regularized;accelerate contrastive learning;propose semi supervised;weakly labeled data;semi transductive learning;recognizing weak labels;expensive nlp tasks;supervised regularized self;nlp tasks cheap;contrastive learning;weakly labeled;framework contrastive learning;obtain weakly labeled;data expensive nlp;contrastive learning collecting;sample reweighting regularization;contrastive representation learning;datasets short contrastive;pretrained language model;weak labels text;weak labels;regularized self training;weak supervision sources;contrastive self training;tuning pretrained language;nlp tasks;expensive nlp"}, "53dc99155c52979e311a403571f1b1d57ff73b48": {"ta_keywords": "model displacement tracking;learning model displacement;displacement tracking;displacement tracking porcine;predict displacement;workflow predict displacement;predict displacement field;automated tracking;modeling;microstructure learned data;track movement animals;constitutive modeling;tracking;automated tracking used;displacement field based;model displacement;operator learning model;leaflet model based;learning model;learned data;loop automated tracking;material microstructure learned;neural operator learning;element analysis based;microstructure learned;operator learning;track movement;leaflet model;data driven;model based stochastic", "pdf_keywords": "learning material model;predict mechanical;predict mechanical responses;approach predict mechanical;modeling mechanical;alternative modeling mechanical;physics guided neural;material responses prediction;neural operator learning;modeling mechanical response;operator learning model;tissue biomechanics;operator learning;learning material;material model;learned solution operators;soft tissue biomechanics;guided neural operator;learning model integrates;measurements learned solution;mechanical response biological;learning model;material model latex;measurements learned;tissue biomechanics particular;mechanical responses soft;operator learning approach;partial physics knowledge;build neural operator;mechanical responses"}, "6bca949b7ce69d6a43120d75e65f43d4c5a80ed4": {"ta_keywords": "incentive systems;incentive design;incentive systems wide;incentive design core;design incentive systems;state art incentive;approach design incentive;design incentive;incentive;art incentive design;intelligent infrastructure commerce;communities economics control;economics control;art incentive;economics control theory;core communities economics;communities economics;intelligent infrastructure;resource constraints;infrastructure commerce;economics;designing mechanisms;exogenous uncertainties dynamic;self interested parties;machine learning;exogenous uncertainties;infrastructure commerce present;interested parties avoiding;environments resource constraints;uncertainties dynamic environments", "pdf_keywords": ""}, "0ee468b9b709a2610c4b574d67218e7960350224": {"ta_keywords": "neural machine translation;machine translation experiments;augmentation text based;machine translation;experiments translation datasets;translation experiments;augmentation strategy nmt;translation experiments translation;data augmentation text;idea replacing words;augmentation text;replacing words;replacing words source;data augmentation strategy;new data augmentation;data augmentation;data augmentation policy;translation datasets;augmentation strategy;alternatives word dropout;random words corresponding;target sentences random;experiments translation;augmentation;neural machine;qubits quantum probability;sentences random words;augmentation policy;methods data augmentation;word dropout", "pdf_keywords": "augmentation machine translation;neural machine translation;effective machine translation;experiments translation datasets;translation datasets;augmentation text based;novel data augmentation;machine translation;data augmentation text;machine translation vulnerable;datasets experiments translation;propose data augmentation;data augmentation;machine translation source;machine translation nmc;augmentation text;translation datasets different;methods data augmentation;strong baselines datasets;data augmentation algorithm;translation source target;propose augmentation machine;augmentation;english german dataset;alternatives word dropout;augmentation machine;augmentation algorithm encourages;effective lowresource datasets;word dropout experiments;2015 englishvietnamese effective"}, "4b22503d6da9ff3222d94106cc7425ea4fea43af": {"ta_keywords": "stanford chinese parser;chinese parser strong;sentence dependency parsing;big challenge parsers;parsers trained;parser trained;chinese parser;challenge parsers trained;parser trained domain;formulate dependency parsing;dependency parsing;parsers;parser strong;challenge parsers;malt parser trained;parsing;parsers trained traditional;parser;token sentence dependency;strong malt parser;new treebank;extraction question answering;treebank;present new treebank;parser strong malt;dependency parsing problem;answering machine translation;sentence dependency;machine translation proposed;machine translation", "pdf_keywords": ""}, "1fa69608666e66452df56b1f71282def7ac16035": {"ta_keywords": "voting theory;vote manipulation;voting theory literature;analysis voting rules;constraints vote manipulation;theoretical analysis voting;vote manipulation problem;voting theory identified;paradoxes associated voting;choice voting theory;agent voting rules;bribery manipulation problems;information agent voting;voting agents strategically;analysis voting process;agent voting;complexity bribery manipulation;voting agents;analysis voting;voting rules;bribery manipulation;complexity bribery;voting process context;voting process;social choice voting;voting scenario;problem seeks voting;seeks voting agents;computational complexity bribery;associated voting", "pdf_keywords": ""}, "2f7c03f0d3c6f51728e925a874c49a25559cc6b3": {"ta_keywords": "answering retrieval;answering retrieval performance;question answering retrieval;question answering;academic paper answering;improve question answering;paper answering;paper answering complex;answering compositional questions;predicting answers model;answering compositional;dochopper answer compositional;sentence embedding document;questions long documents;novel retrieval based;predicting answers;novel retrieval;answers model;evidence predicting answers;document mixes retrieved;retrieval based;embedding document mixes;retrieval method dochopper;answer compositional questions;present novel retrieval;reading long documents;retrieval;performance long documents;answering complex questions;answer compositional", "pdf_keywords": ""}, "c13c400d1f481863d57ec265d296b0a08ec77876": {"ta_keywords": "speech recognition asr;segmentation input audio;non autoregressive decoding;segmentation non autoregressive;realizing asr segmentation;loss automatic speech;asr segmentation input;asr segmentation;autoregressive decoding algorithm;recognition asr systems;autoregressive decoding;recognition asr;automatic speech;speech recognition;automatic speech recognition;connectionist temporal classification;nonautoregressive transformer train;autoregressive decoding generates;temporal classification ctc;autoregressive transformer promising;non autoregressive transformer;segment segmentation non;decoding algorithm segment;segmentation non;segmentation input;autoregressive transformer;ctc neural networks;decoding;like quantization compression;quantization compression", "pdf_keywords": ""}, "0a2ba7b1c05062d2bb7cd35e218fe08d6ea29488": {"ta_keywords": "graph structure;networks timeline structural;underlying graph structure;timeline structural graph;structure graph;quantum information networks;structure graph represented;graph structure graph;structural graph;social networks timeline;structural graph present;social networks;structure documents topology;structure extended similarity;graph walk paradigm;quantum communication networks;characterizing quantum communication;content social networks;investigate relationship structure;information networks;representing underlying structure;relationship structure documents;topology underlying graph;analyzing characterizing quantum;networks timeline;characterise quantum;lazy graph;analyze characterise quantum;graph represented;lazy graph walk", "pdf_keywords": ""}, "fd54706252a094d592feadf53a0a3ffed4af9295": {"ta_keywords": "robotic speech recognition;speech recognition challenge;robotic speech;speech recognition real;speech recognition promoting;speech recognition;field speech recognition;speech recognition rsc;microphone;division robotic speech;chime challenge competition;chime challenge;automatic speech recognition;3rd chime challenge;competition automatic speech;automatic speech;processing automatic speech;far field speech;talking tablet device;microphone array;field speech;channel microphone;talking tablet;recognition rsc challenge;person talking tablet;recognition challenge;channel microphone array;recognition challenge findings;acoustics division robotic;challenge competition automatic", "pdf_keywords": ""}, "142407d3cb61067e88d385f95ae238c74b19d554": {"ta_keywords": "gender group distribution;gender social group;gender group;gender social;occurrence particular gender;particular gender group;occurrences given gender;given gender social;gender;analyze behavior sentiment;sentiment analysis;behavior sentiment;particular gender;xmath1 ratio type;xmath0 xmath1 ratio;xmath1 ratio;behavior sentiment context;sentiment context;social group;context sentiment analysis;given gender;sentiment;xmath0 xmath1;sentiment context sentiment;xmath1;group distribution members;xmath0;dependence xmath0 xmath1;social group strong;group", "pdf_keywords": ""}, "75983c55a489d526427fe399ce2670376168a2f0": {"ta_keywords": "paraphrase database quantum;domain paraphrase database;extracting paraphrased information;paraphrase database;tool extracting paraphrased;paraphrase categories domains;extract paraphrases;generate paraphrases;extract generate paraphrases;extracting paraphrased;generate paraphrases paraphrases;paraphrased information language;paraphrases paraphrases available;extract paraphrases large;paraphrases available;general domain paraphrase;domain paraphrase;paraphrased information;paraphrases paraphrases;paraphrase categories;method extract paraphrases;paraphrases;phrases domains;paraphrases large;phrases domains freely;paraphrasing powerful tool;paraphrases large class;paraphrases available creative;large class paraphrase;class paraphrase categories", "pdf_keywords": ""}, "5bc188b4ab7b27649236fad6a686b2cfe6368219": {"ta_keywords": "document topic modeling;topic modeling presented;topic modeling;paper commonsense knowledge;commonsense knowledge based;average agglomerative clustering;commonsense knowledge;agglomerative clustering;word distributions;particular word distributions;document topic;paper commonsense;word distributions making;agglomerative clustering providing;word occurrence particular;knowledge based algorithm;word occurrence;depend word occurrence;clustering;algorithms paper commonsense;occurrence particular word;clustering providing improved;clustering providing;features clustered;features clustered using;commonsense;texts length composition;making effective texts;knowledge based;algorithm document topic", "pdf_keywords": ""}, "d8d49cc56b303d6ed0e821f8593e2f7acd1b4fb4": {"ta_keywords": "sound event detection;audio feature sequence;sound event class;sound event;optimized sound event;audio feature;event detection dcase2020;paper sound event;detection dcase2020 task4;postprocessing optimized sound;semi supervised;convolution networks;context information audio;score paper sound;supervised;convolution networks efficiently;event detection;audio;information audio feature;semi supervised learning;teacher semi supervised;wise convolution networks;dcase2020 task4;supervised learning;detection dcase2020;data augmentation postprocessing;data augmentation;optimized sound;depth wise convolution;information audio", "pdf_keywords": ""}, "a61ef7be5b5c9fbc6654f7c17fa595976652416b": {"ta_keywords": "statistics kidney transplants;statistics kidney;analysis statistics kidney;kidney transplants performed;kidney transplants;transplants performed;transplants;strength particles;relative strength particles;interaction strength particle;kidney;transplants performed fig;strength particles surrounding;strength particle;strength particle surrounding;interaction strength;compared mechanisms currently;compared mechanisms;particle surrounding medium;particles surrounding medium;particles;statistical;surrounding medium xcite;paper present statistical;varying relative strength;mechanism;simple mechanism;particle;particles surrounding;mechanisms currently", "pdf_keywords": ""}, "ff1a1e39a94b9ca31e6013d12bc2d27f7a31567c": {"ta_keywords": "multiple decoders attention;head attention models;decoders attention;decoders attention integrates;head attention model;model multiple attentions;attention model multiple;attention models capture;multi head attention;attention models;attention model;integrated single attention;attention based encoder;end speech recognition;multiple attentions;multiple attentions calculated;multi head decoder;attentions calculated integrated;attention integrates outputs;contexts attention based;attention based;attentions calculated;head attention;speech recognition;attention model paper;head decoder;single attention;head decoder end;speech recognition extension;recognition performance ensemble", "pdf_keywords": "head attention models;head attention model;end speech recognition;multi head decoder;multi head attention;speech recognition multi;head decoder;multi head decoders;attention based encoder;head decoder end;automatic speech;speech recognition extension;speech recognition;head decoders;recognition multi head;attention models capture;voice search;speech recognition experimental;automatic speech recognition;attention models;attention model propose;attention model;approach automatic speech;head attention;search voice;contexts attention based;voice search voice;head decoders proposed;capture different speech;novel attention based"}, "3b4b5e72a2f84d079d0d1d825309c2f6ded76539": {"ta_keywords": "speaker adaptation;methods speaker adaptation;speaker adaptation experiments;transfer vector estimation;training transfer vectors;adaptation data transfer;improve speech recognition;recognition performance adaptation;adaptation data;training algorithms transfer;vector estimation gaussian;accurate transfer vectors;speech recognition;transfer vectors simple;algorithms transfer vector;transfer vectors;transfer vector;vector estimation;speech recognition performance;focus transfer vector;adaptation experiments proposals;vectors simple training;transfer vector decomposed;data transfer vector;transfer vectors small;significantly improve speech;class estimation direction;likelihood methods speaker;adaptation technique based;improve speech", "pdf_keywords": ""}, "77e5c4fa595466aa51d29327a60f9d4af4436876": {"ta_keywords": "learns incrementally;learning algorithm andebl;learning algorithm called;mistake bounded learnability;bounded learnability;ebl learns incrementally;perceptron learning;learnability able predict;learns incrementally ia;bounded learnability able;learns;perceptron learning algorithm;learning algorithm;learnability;learning;based learning algorithm;learning algorithm output;cover based learning;alternative learning algorithm;learnability able;based learning;algorithm output learning;output learning;extension perceptron learning;output learning longer;ebl learns;learning longer;learning longer expressed;fluid motion;fluid dynamics video", "pdf_keywords": ""}, "3c0d4dc4237934e37467f4ede3af859bcb140abf": {"ta_keywords": "neural networks crowd;scene context knowledge;networks crowd counting;context convolutional neural;visualizing scene context;local scene context;larger context convolutional;context convolutional;scene context;crowd counting;convnets large scale;scene context essential;global scene context;crowd counting problem;rnnns based;networks rnnns;scene context paper;local scene;networks rnnns based;networks convnets;networks convnets large;visualizing scene;convnets;rnnns based recurrent;global scene;networks crowd;attention informed context;token attention;neural networks rnnns;scene extensive experiments", "pdf_keywords": "supervised crowd counting;crowd counting approaches;crowd counting methods;approach crowd counting;crowd counting based;rely crowd counting;crowd counting using;crowd counting point;method crowd counting;crowd counting;supervision crowd counting;point supervised crowd;count size crowd;crowd counting problem;art crowd counting;supervised crowd;counting based convolutional;crowd present image;crowd density maps;counting using vision;crowd counting propose;prediction crowd density;point supervision crowd;counting point supervision;crowd present;approach crowd;crowd;crowd density;prediction crowd;method crowd"}, "9f49ed155d7575d181d16dd5bc92b754cae0bea9": {"ta_keywords": "subspace methods blind;rank constrained optimization;methods blind identification;formulated rank constrained;rank constrained;subspace methods;identification multiple input;blind identification multiple;identification problem formulated;output linear systems;blind identification;convex relaxation;convex relaxation practical;constrained optimization admits;identification problem;optimization admits convex;constrained optimization;linear systems;admits convex relaxation;multiple output linear;extension subspace methods;problem formulated rank;identification multiple;output formulation;output formulation applied;applied output formulation;subspace;methods blind;feasible measure inputs;formulated rank", "pdf_keywords": ""}, "91a2d496553cfee2b66906f704b8e3d081e2d1bf": {"ta_keywords": "inductive logic programming;fault detection;logic programs;fault detection compare;logic programming ilp;augment logic programs;logic programs variation;logic programming;tolerant fault detection;inductive logic;logic inductive;inductive logic inductive;learn counting clauses;logic inductive statement;evaluate inductive logic;methods predicting fault;approach fault tolerant;fault tolerant fault;fault tolerant;proof inductive logic;learn counting;predicting fault;predicting fault density;programming ilp methods;counting clauses augment;augment logic;classes counting clauses;users learn counting;programming ilp;clauses augment logic", "pdf_keywords": ""}, "2bbb33ab8124e5078ec39e821a25c24c20a31b9b": {"ta_keywords": "crowdsourced audio;organization crowdsourced audio;crowdsourcing technology users;make crowdsourcing technology;crowdsourced audio transcription;crowdsourcing technology;make crowdsourcing;crowdsourcing;crowdsourcing second workshop;crowdsourcing transition;helping crowdsourcing transition;workshop crowds science;helping crowdsourcing;crowdsourced;crowdsourcing second;crowdsourcing crowdsourcing;organization crowdsourced;summer school crowdsourcing;crowdsourcing transition art;face make crowdsourcing;school crowdsourcing;crowdsourcing crowdsourcing second;crowds science;describes organization crowdsourced;crowd science workshop;school crowdsourcing crowdsourcing;workshop crowds;crowds science organized;second workshop crowds;results crowd science", "pdf_keywords": ""}, "0e96925c57b3325e7e37c1964b518e9276024cbf": {"ta_keywords": "estimating speed sound;speed sound sound;sound noisy environment;speed sound;method estimating speed;sound sound noisy;sound noisy;estimating speed;noisy environment;sound sound;new method estimating;noisy;method estimating;estimating;sound;speed;method;new method;present new method;new;environment;present new;paper;paper present new;paper present;present", "pdf_keywords": ""}, "7c2ff8fa0d24ed712e4bc2dbdb370a1cd62c965b": {"ta_keywords": "1_ 25 125;1_ 25;125 25;25 125;25 125 25;125;1_;25", "pdf_keywords": ""}, "3d846cb01f6a975554035d2210b578ca61344b22": {"ta_keywords": "semi supervised learning;learning embeddings;based graph embeddings;graph embeddings;semi supervised;inductive transductive learning;transductive learning;distantly supervised entity;graph embeddings large;present semi supervised;learning embeddings given;entity classification;text classification distantly;classification distantly supervised;supervised entity extraction;transductive learning present;graph develop transductive;supervised entity;distantly supervised;graph instances train;method learning embeddings;instances train embedding;supervised learning;embeddings large diverse;extraction entity classification;supervised learning framework;classification given graph;entity classification given;classification distantly;text classification", "pdf_keywords": "semi supervised embedding;deep learning graph;learning predicting graph;predicting graph context;supervised embedding;learning graph;learning graph edges;supervised embedding method;semi supervised learning;feature vectors embedding;embedding method deep;propose semi supervised;novel semi supervised;predicting graph;embeddings input features;semi supervised;embeddings;graph context effective;graph laplacian regularization;graph context;skipgram like models;embedding;vectors embedding;improvement unsupervised learning;context effective graph;vectors embedding viewed;unsupervised learning;deep learning;embedding method;like models graphs"}, "b6de9d0ca42a03967287aa7abfd59479e086a35a": {"ta_keywords": "structured knowledge bases;generate structured knowledge;knowledge bases;generating structured knowledge;structured knowledge base;knowledge base;knowledge bases kbs;constructing structured knowledge;supervision available ontological;automatically constructing structured;knowledge base team;automatically constructing;short natural language;natural language;knowledge base demonstrate;propose semi supervised;available ontological constraints;research automatically constructing;structured knowledge;automatically constructed;available ontological;natural language definitions;generating facts;ontological constraints short;semi supervised;semi supervised learning;generating facts populate;generating structured;generate structured;way generating structured", "pdf_keywords": ""}, "d39478dd8d825bbd6c963d6a5ef2cee6857f6c21": {"ta_keywords": "turbulent flow argumentation;argumentation computationally;argumentation computationally addressing;online introduce argumentation;argumentation multifaceted communication;natural argumentation;argument quality;addressing argument quality;argumentation pros;deal argumentation computationally;based argumentation pros;simple natural argumentation;argumentation based argumentation;argument reasoning dealing;argumentation multifaceted;argumentation pros cons;introduce argumentation multifaceted;argumentation based;computationally addressing argument;argumentation arguments;attempts deal argumentation;argument quality understanding;flow argumentation based;natural argumentation equality;argumentation equality natural;based argumentation;argument reasoning;introduce argumentation;flow argumentation;argumentation", "pdf_keywords": ""}, "358d7d6333d3edd530e37efd8004cb9da8cfd5d4": {"ta_keywords": "video captioning;noise video captioning;knowledge extracted videos;evaluation multimodal models;videos submitted structured;sentence level annotations;labeling visual action;evaluation multimodal;captioning;video captioning short;annotated open vocabulary;quantitative evaluation multimodal;multimodal models;instructional cooking videos;visual action detection;structured procedural learning;multimodal models analysis;role labeling visual;semantic role labeling;manually annotated;video clip sentence;action detection;cooking videos;knowledge extracted structured;clip sentence level;level annotations;annotations;note structured knowledge;multimodal;manually annotated open", "pdf_keywords": "video captioning;visual action detection;vocabulary narrative videos;action detection;understanding instructional videos;framework video captioning;activity knowledge engine;human activity knowledge;video captioning paper;labeling visual action;human instance action;narrative videos;action detection perform;video context;activity knowledge;videos noisy transcripts;learning framework video;instructional videos present;narrative videos noisy;knowledge multimodal information;human activities;knowledge multimodal;visual action;instructional videos;captioning;procedural knowledge multimodal;task dataset extracting;human activity;activities;semantic role labeling"}, "adf726bdcdddacee1c70d911b8f84b6a16841a32": {"ta_keywords": "reviews products service;customer reviews products;summarizing customer reviews;reviews products;review aspects product;type textual reviews;textual reviews;customer reviews;textual reviews existing;prominent review aspects;review aspects extensive;reviews existing systems;review aspects;reviews existing;lhc;conventionally prominent review;reviews;lhc conventionally prominent;prominent review;end lhc;number prominent review;day end lhc;product type textual;review;end lhc conventionally;products service based;lhc conventionally;analyzing summarizing customer;lfpc hmpc;products service", "pdf_keywords": ""}, "0dff2b00fd6e8e7b5f3c0707b0e51e3628988420": {"ta_keywords": "privacy aware text;privacy aware translation;sensitive information instead;sensitive information publishing;leak sensitive information;semantics fluency adversarial;sensitive information;aware text rewriting;privacy aware;new privacy aware;based adversarial;task explore privacy;privacy;text observed fairness;new privacy;task based adversarial;adversarial;propose new privacy;based adversarial training;adversarial training;training approximate fairness;quantum;retains better semantics;adversarial training approximate;explore privacy aware;way protect data;distinguish classical quantum;classical quantum;fluency adversarial;protect data", "pdf_keywords": "text rewriting protecting;aware text rewriting;automatic text rewriting;privacy aware text;privacy aware rewriting;rewriting protecting sensitive;obfuscating gender;applications obfuscating gender;rewriting text automatic;sensitive attributes text;privacy aware translation;providers rewriting text;text rewriting framework;obfuscating gender political;protecting sensitive attributes;text rewriting helps;text rewriting;sensitive information data;rewriting text;text data behalf;race propose privacy;protect sensitive information;leakage sensitive information;rewriting protecting;research challenge privacy;sensitive information;sensitive attributes;aware rewriting;obfuscating;human annotators"}, "694c0c5a4d0176e29bb85e1b9ca8ea84075fbbbb": {"ta_keywords": "transformations hmms rnns;hmms rnns theoretical;rnns theoretical derivations;rnns theoretical;rnns present simple;softmaxes;architectural transformations hmms;rnns;softmaxes iii;special case rnns;case rnns;speech induction;placement softmaxes iii;softmaxes iii use;speech induction despite;transformations hmms;placement softmaxes;rnns present;hidden representations;case rnns present;algorithm neural;ii placement softmaxes;hmms rnns;representations hidden representations;representations xmath0 rna;parts speech induction;hidden representations xmath0;algorithm neural networks;representations hidden;neural networks", "pdf_keywords": ""}, "194a1e5f9af0ea00b22def879d90b926187fbb64": {"ta_keywords": "3d particle cell;2d granular gas;dimensional 3d particle;3d particle;dimensional 2d granular;particle cell;particle dimensional 2d;particle cell pic;2d granular;granular gas;particle dimensional;single particle dimensional;cell pic model;evolution single particle;dimensional 3d;particle;single particle;performance dimensional 3d;dimensional 2d;granular;cell;2d;cell pic;pic model study;3d;dimensional;model study evolution;gas;pic model;performance dimensional", "pdf_keywords": ""}, "c02da00857c33fa39b115c0eb6c655ff6cf96878": {"ta_keywords": "dynamics water ice;water ice interaction;ice interaction liquid;ice interaction;vapor phase water;dynamics water;liquid vapor phase;interaction liquid vapor;study dynamics water;phase water;water ice;interaction liquid;vapor phase;liquid vapor;ice;theoretical study dynamics;liquid;dynamics;phase;vapor;water;theoretical study;study dynamics;present results theoretical;results theoretical study;interaction;results theoretical;theoretical;paper present;paper", "pdf_keywords": ""}, "ca201db9980e49647feedf39eb30b19f074bf68a": {"ta_keywords": "end speech recognition;transcribed earnings calls;endtoend neural transcription;professionally transcribed earnings;accuracy speech text;speech text stt;transcription fully formatted;transcribed earnings;speech text;learning task acoustic;transcription fully;transcription;neural transcription fully;end end speech;professionally transcribed;endtoend neural;task endtoend neural;text stt machine;accuracy speech;neural transcription;hours professionally transcribed;stt machine learning;end speech;speech recognition;task acoustic models;head accuracy speech;earnings calls;earnings calls achieving;transcribed;text stt", "pdf_keywords": "end transcription english;end transcription;transcriptionist multilingual speech;end end transcription;audiobooks transcriptionist multilingual;corpus automatic speech;end neural transcription;transcription machine learning;automatic speech;speech corpus automatic;formatted speech recognition;audiobooks transcriptionist;speech automatic speech;use audiobooks transcriptionist;speech automatic;fully formatted speech;speech corpus;task transcribing transcribed;transcription english;transcription fully formatted;transcription english propose;models spgispeech corpus;approach automatic speech;transcribing transcribed earnings;transcription machine;automatic speech recognition;multilingual speech corpus;predict complete english;speech recognition machine;transcription fully"}, "48530f3d6425f2f150f07ccdd61ba951951a0a7d": {"ta_keywords": "bilingual models massively;individual bilingual models;bilingual models;tasks domain adaptation;universal machine translation;massively multilingual model;domain adaptation;models massively multilingual;multilingual model;translation fine tuning;domain adaptation demonstrate;domain adaptation problem;multilingual model language;experiments domain adaptation;machine translation;multilingual competition;massively multilingual;model language pairs;individual bilingual;multilingual competition problem;multilingual;bilingual;machine translation fine;ii multilingual competition;efficient approach adaptation;language pairs;adaptation;gap individual bilingual;tuning target task;ii multilingual", "pdf_keywords": "machine translation deep;translation deep learning;machine translation promising;domain adaptation multilingual;multilingual machine translation;universal machine translation;neural machine translation;adaptation multilingual nmt;machine translation architecture;machine translation models;translation architecture learns;translation models;adaptation multilingual;translation machine translation;translation deep;machine translation;machine translation machine;translation machine;translation architecture;machine translation propose;bilingual models massively;massively multilingual model;translation models evaluate;models massively multilingual;learning domain adaptation;adapting large scale;translation promising approach;individual bilingual models;multilingual nmt;bilingual models"}, "e3a85c5defe60f1f394fc4e7245fc071a249cf5b": {"ta_keywords": "behavioral models occupants;learning parameters behavioral;learn behavioral models;building occupants control;occupants control;efficient learn behavioral;occupants control management;energy efficient learn;models occupants building;learn behavioral;quantum body;occupants energy efficient;building occupants energy;models occupants;occupants energy;learning parameters;integrate building occupants;behavioral models;sensing;behavioral model;quantum body presence;occupants building;building occupants;quantum;sensing actuation;efficient learn;states dos quantum;occupants;behavioral;parameters behavioral model", "pdf_keywords": ""}, "c99e050b83360e5cbeee8fd2957aaab5b31aa638": {"ta_keywords": "memory language models;accurate language models;measure memory language;language models;language models capture;language models presented;building accurate language;language models including;language models use;language processing calibration;generative sequence model;memory language;entropic memory language;models including lstms;discrepancies generative sequence;term discrepancies generative;lstms transformers;lstms transformers emph;accurate language;natural language processing;language processing;generative sequence;lstms;including lstms transformers;generations drift dramatically;entropy rates generations;sequence model;memory;challenge natural language;including lstms", "pdf_keywords": "memory language models;language models generating;language models;term memory language;language models calibrating;language models mutual;term memory estimates;language model;term memory;memory language;language language model;generative sequence model;models generating text;long term memory;present language model;predict context;language model used;language model function;predict context used;used predict context;language model paper;box language model;memory estimates;information models predictions;properties language models;discrepancies generative sequence;memory estimates provide;term discrepancies generative;black box language;generative sequence"}, "7ad913d1c6eddbdad1ab4571ab91f00f055ab735": {"ta_keywords": "limitation rna network;rna networks;rna network;regular rna networks;limitation rna;fast recurrence attention;long form speech;overcome limitation rna;rna network known;speech recognition stiff;speech inputs;speech recognition;rna;speech inputs generalized;speech input;performance speech recognition;machine translation tasks;form speech inputs;recognition long;sr long form;parallelized regular rna;regular rna;architecture recognition long;sequence transduction;speech inputs paper;translation tasks improved;form speech input;modeling machine translation;sequence transduction model;recurrence attention", "pdf_keywords": "fusion network speech;model longform speech;attention recurrence fusion;network speech recognition;longform speech;recurrent neural networks;cnn architecture long;speech recognition;speech outperforming conformer;parallelized regular rnn;attention recurrence;attention better parallelized;network speech;recurrence fusion network;speech outperforming;speech recognition addition;speech frame sequence;fast recurrence models;recurrent neural;12 recurrent neural;network cnn;networks cnns widely;longform speech demonstrate;long form speech;form speech recognition;neural network cnn;network cnn architecture;cnn architecture;networks cnns;form speech outperforming"}, "40947612162cc4644f9489721ec1ca94fe7e765c": {"ta_keywords": "russian semantic relatedness;russian semantic;semantic relatedness native;computing semantic relatedness;semantic relatedness terms;purpose russian semantic;thesaurus russian thesaurus;evaluation semantic relatedness;semantic relatedness list;semantic relatedness;thesaurus russian;russian thesaurus;distributional thesaurus russian;word similarity;russian thesaurus used;native speakers crowdsourced;word word similarity;relatedness native speakers;problem semantic relatedness;speakers crowdsourced speech;semantic relatedness complementing;crowdsourced speech;crowdsourced speech recognition;word similarity ij;speakers crowdsourced;distributional thesaurus;thesaurus;computing semantic;relatedness native;thesaurus used", "pdf_keywords": "russian semantic similarity;semantic relatedness russian;russian language semantic;relatedness russian words;russian semantic;thesaurus russian language;relatedness semantic similarity;dataset russian semantic;distributional thesaurus russian;semantic relatedness systems;thesaurus russian;semantic similarity;training semantic relatedness;semantic similarity present;language semantic relatedness;semantic relatedness semantic;evaluation semantic relatedness;semantic relatedness;relatedness russian;semantic similarity extensively;relatedness semantic;semantic relatedness important;similarity word sense;world semantic relatedness;word collection russian;text similarity word;distributional thesaurus;russian words;distributional thesaurus used;language semantic"}, "69379f55de081938ae9d8b91ef549542ed78f5f0": {"ta_keywords": "external force controlled;force controlled external;fluid dynamics video;fluid dynamics;force controlled;action external force;motion rigid body;motion rigid;external force;rigid body action;dynamics video;controlled external control;external control;dynamics video motion;dynamics;video motion rigid;controlled external;rigid body;external control parameter;motion;video motion;body action external;action external;force;rigid;control;controlled;control parameter;fluid;body action", "pdf_keywords": "segmentation speaker embedding;speaker diarization promising;segmentation speaker;speech segmentation speaker;speaker diarization framework;speaker diarization;embedding diarized speaker;diarized speaker diarization;automatically recognizing spoken;speaker embedding diarized;speaker diarization presented;new speaker diarization;speaker embedding;recognizing spoken;vision speech enhancement;integrate speech segmentation;enhancement denoising speech;speaker diarization active;speech enhancement denoising;denoising speech enhancement;speech segmentation;denoising speech;recognizing spoken conversation;speech enhancement;speakers diarization machine;diarized speaker;computer vision speech;speakers diarization;speakers diarization important;conversation crowdsourced videos"}, "d74c5b5ed8eb467dc7f313b70a08880fcd74c39d": {"ta_keywords": "user assessment electronic;local government systems;assessment local government;assessment electronic filing;government systems;assessment electronic;electronic filing;information systems;user assessment;residents local governments;survey tax conducted;findings user assessment;tax survey;websites local governments;electronic filing payment;evaluation residents local;local government;users use information;tax conducted;national tax survey;systems websites local;local governments;survey tax;local governments present;tax survey tax;information systems websites;electronic;local residents results;useful evaluation residents;target information systems", "pdf_keywords": ""}, "625764f8e3e1334ffbfe5b3139e555499e6df4d5": {"ta_keywords": "natural language website;natural language processing;entity recognition;entity recognition text;website update requests;text classification tasks;process natural language;tasks natural language;estimating reliability web;processing nlp;robustness handling request;text classification;sequence entity recognition;performance using corpus;natural language;analyze requests posted;language processing nlp;processing nlp studied;requests information experimentally;reliability web;using corpus generated;update requests;analyze requests;requests information;language website update;update requests semi;classification tasks;understanding requests information;language processing;classification tasks natural", "pdf_keywords": ""}, "b5241fcbfbf30f6fd8ff1ae19d947dd2ca23244f": {"ta_keywords": "automatic event extraction;event extraction;event extraction paper;discovery aggregation events;effectiveness automatic event;aggregating events web;dictionary events survey;automatically acquired events;acquiring aggregating events;events web;events web data;emotion provoking events;aggregation events;automatic event;dictionary events;constructed dictionary events;events;aggregation events provoke;events present;aggregating events;concerned discovery aggregation;event;events survey;discovery aggregation;events present simple;events perform evaluation;events perform;automatically acquiring aggregating;acquired events perform;provoking events present", "pdf_keywords": ""}, "21c39ce886dc38dd2006ea25d6bd1eff4cdba0b8": {"ta_keywords": "predict blog;political blogs;predict blog users;online political blogs;comment prediction;model comment prediction;political blogs provide;comment prediction task;task predict blog;blogs;blogs provide;blog users;blog;blog community;discussion models discover;discussion models;discussions online political;blogs provide qualitative;blog community verbal;comments author;contents blog community;post comments author;users leave comments;comments given post;reactions post comments;contents blog;discussions online;comments author version;model discussions online;blog users leave", "pdf_keywords": ""}, "51321a60f5ec2c80253394ef86e8b5fcc768f52a": {"ta_keywords": "textual names databases;identifier names scalable;names scalable adaptive;databases exploiting similarities;names databases;similarities textual names;names databases exploiting;matching identifier names;integrate textual names;names scalable;clustering matching identifier;exploiting similarities textual;textual names;identifier names;data integration identification;techniques clustering matching;real world entities;textual names used;names used objects;clustering matching;entities;matching identifier;objects different databases;databases;techniques clustering;datasets;datasets shows adaptive;data sets;clustering;world entities", "pdf_keywords": ""}, "6954a6bb9d6f3e365b26b694c963ae1d62a03444": {"ta_keywords": "effective context modeling;long text modeling;context representations;global context representations;contexts transform token;context modeling;contexts transform;text modeling performance;context modeling linear;global contexts transform;text modeling;model global contexts;long text;achieve effective context;fastformer efficient;better long text;use additive attention;global context;transform token representation;additive attention;effective context;token representation based;fastformer efficient existing;contexts;performance long sequences;datasets fastformer efficient;context;global contexts;propose fastformer fastformer;additive attention mechanism", "pdf_keywords": "fastformer natural language;additive attention based;models nlp bert;additive attention handle;based additive attention;additive attention;propose additive attention;trained language models;attention based transformer;attention based;attention handle long;models nlp;language models nlp;attention;pre trained language;attention handle;nlp bert;sequence effectively captured;language models;ad cvr prediction;fastformer natural;collected bing ads;fastformer;pytorch version fastformer;trained language;summarization tasks;clicked ads transformer;text summarization tasks;natural language generation;bing ads"}, "03006aefccdd0c5c6736ab11ed574d02ba1cc086": {"ta_keywords": "neural machine translation;augmentation methods translation;machine translation;low resource japanese;monolingual data;resource japanese;resource japanese english;use monolingual data;models language pairs;machine translation nmt;alleviate issues translation;model motion heavy;monolingual data help;motion heavy;motion heavy body;language pairs;force motion heavy;use monolingual;heavy body action;body action external;body data augmentation;data augmentation methods;possible use monolingual;models language;data augmentation;improvements semi supervised;especially syntactically;monolingual;languages;translation make possible", "pdf_keywords": "statistical machine translation;bilingual lexicon induction;resource machine translation;induce word translations;translation word reordering;machine translation;translation low resource;improve translation low;sentences data augmentation;translations words target;machine translation word;bilingual lexicon;methods bilingual lexicon;sourceordered target sentences;approach machine translation;trained word embeddings;goal improve translation;machine translation creates;improve translation;translation low;machine translation smt;word translations;target sentences data;semi supervised;translation creates sourceordered;propose semi supervised;lexicon induction pre;translations words;lexicon induction;word embeddings"}, "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269": {"ta_keywords": "organized critical systems;organize critical systems;open loop automata;automata;self organized critical;critical systems introduce;loop automata;self organize critical;critical systems;automata present;loop automata present;synthesizing programs;automata present new;organized critical;systems able self;correctness synthesizing programs;organize critical;design self organized;able self organize;synthesizing programs docstrings;self organize;systems introduce;self organized;critical systems scs;systems;systems introduce model;writing capabilities fluid;organized;design self;systems able", "pdf_keywords": "code generation models;code generation;code generation systems;code generation context;tool code generation;using code generation;impacts code generation;code generation work;capable generating code;predictive code model;generating code;generating code incorporated;possibility code generation;predictive code;code model;code samples;code completion model;neural code completion;code samples automatically;code completion;train predictive code;training tool code;language models train;code model 2015;generation context codex;symposium software testing;correctness code samples;neural code;software testing analysis;tool code"}, "b5a667bf189a0cfda22bac702d97b601ae6adb6f": {"ta_keywords": "robust adversarial distribution;adversarial distribution;robustness strategically adversarially;adversarial distribution shifts;distributionally robust decision;robust adversarial;simultaneously robust adversarial;strategically adversarially;strategically adversarially generated;learning problems gradient;adversarially generated data;gradient information readily;adversarial;algorithms convex minimization;adversarially;adversarially generated;distributionally robust;gradient information;robust decision;robust decision dependent;decision dependent learning;problems gradient information;proposed approach learns;solve distributionally robust;max optimization emerging;convex minimization;min max optimization;convex minimization problems;learns models;dependent learning problems", "pdf_keywords": "optimistic gradient descent;robust risk minimization;learning robust optimization;gradient descent ascent;optimistic gradient;studied optimistic gradient;risk minimization;risk minimization problems;decisiondependent distributional robust;convex concave min;robust optimization;convex convex duality;min max optimization;gradient descent;stochastic algorithms min;distributional robust risk;learning robust;convex duality;propose gradient free;convex duality paper;machine learning robust;distributional robust;max optimization;constrained convex concave;concave min max;min max objective;robust risk;constrained convex;max optimization problems;adversarial model"}, "26cc9e13a7a76e3cf5f9885d08cdafabd6fbd7ec": {"ta_keywords": "realistic tournament data;statistics tournaments using;analyze statistics tournaments;statistics tournaments;random tournament model;generate realistic tournament;tournament data paper;tournament data;tournament model;random tournament;condorcet random tournament;sets tournaments average;realistic tournament;tournament solution sets;soccer tennis tournaments;tournament solution;data soccer tennis;tournaments average;tournaments using;sets tournaments;tournaments using real;tournaments;tournaments average number;tournament;tournaments paper;solution sets tournaments;tennis tournaments paper;tournaments paper introduce;tennis tournaments;problem tournament solution", "pdf_keywords": "tournament graph statistics;realistic tournament data;pairwise tournament datasets;tournament graphs;tournament datasets;tournament datasets paper;tournament graphs tournament;tournaments computational research;tournament data;tournaments computational;relationship tournament graphs;tournament graph;tournament datasets soccer;tournament data comsoc;graphs tournament;graphs tournament graph;random tournament model;tournament model;understand tournaments computational;pairwise tournament;tournaments create pairwise;datasets soccer tennis;tournaments;understand tournaments;tournaments using;use pairwise tournament;input understand tournaments;condorcet random tournament;computational research tournaments;generate realistic tournament"}, "ac46562e61cfef6213a915bbb80d1a1a2901542a": {"ta_keywords": "recommending conference paper;extracting information web;extract information web;web pages paper;paper propose recommendation;information web pages;technical paper recommendation;paper recommendation;recommendation service;propose recommendation service;recommendation service researchers;recommending conference;information web;information web page;problem recommending conference;paper recommendation ask;information sources evaluate;information sources;conference paper submissions;pages paper propose;paper submissions reviewing;extracting information;multiple information sources;web pages;web pages web;pages paper;extract information;pointers journal papers;paper submissions;pages web", "pdf_keywords": "reviewer information sources;reviewer information web;gathering reviewer information;submissions recommendation algorithms;reviewers recommender systems;reviewer information;queries compare recommendation;combining information sources;reviewers recommender;paper reviewer information;information formulating recommendation;reviewer paper contents;information sources combining;papers reviewers recommender;information web recommendation;sources combining information;compare recommendation algorithms;gathering reviewer;procedure gathering reviewer;submissions recommendation;information sources using;actual submissions recommendation;reviewer paper;recommendation framework provides;recommendation algorithms;web recommendation framework;information content queries;recommendation framework;conference reviewing;recommender systems use"}, "559fdae33f0b7733b80a7dbcb902c79598a0d26e": {"ta_keywords": "covariates treatments adversarial;treatments adversarial;treatments adversarial manner;adversarial;effect estimators transtee;propose network trained;water erenkov tweezer;adversarial manner;treatment effect estimation;associated treatments transformers;estimators transtee demonstrate;network trained;estimators transtee;tweezer;aimed estimating causal;treatment effect estimators;effect estimation;aimed estimating;effect estimation tae;vision experiments transformers;language computer vision;tweezer used;estimation problems datasets;estimating causal effects;effect estimators;inductive biases effective;treatments transformers emerged;demonstrate inductive biases;treatments transformers;estimating causal", "pdf_keywords": "attention weights;attention weights learning;based attention weights;learning balanced covariate;inference based attention;embeds covariates treatments;balanced covariate representations;weights learning balanced;based attention;learning balanced;weights learning;transtetransformer simultaneously embeds;attention;covariate representations;covariate representations address;embeds covariates;multimodal processing;simultaneously embeds covariates;covariates treatments scalable;ptr work inspirations;representations;learning;weights tr ptr;ptr work;study balancing weights;treatment effect estimation;multimodal processing particular;multimodal;inspirations transformer;transformers"}, "bf63276c90a803fe0d069ce0a3a4a8236e756363": {"ta_keywords": "comics deep neural;narratives conveyed stylized;stylized artwork dialogue;artwork dialogue comic;comic book panels;visual narrative;comics deep;context visual narrative;visual narrative combination;predict narrative character;dialogue comic;artwork dialogue;dialogue comic book;driven narratives conveyed;narratives conveyed;narrative character centric;conveyed stylized artwork;comics;models predict narrative;narrative character;conveyed stylized;predict narrative;content natural images;driven narratives;deep neural;closure driven narratives;comic book;vision language tasks;dialogue;narrative combination explicit", "pdf_keywords": "context comics;multimodality contextual;context comics just;text oil paintings;multimodality contextual understanding;panels context comics;comics scanned;character recognition;paintings answering questions;comics traditionally;multimodality;paintings answering;answering questions artwork;predict narrative character;comics just visual;impact multimodality contextual;work comics;comics traditionally focused;computational work comics;comics;work comics traditionally;recognizing text;multimodal tasks;multimodal;collections comics scanned;comics scanned public;character recognition ocr;narrative character centric;language vision understanding;oil paintings answering"}, "83165cf62e62a013c2bad61c98120ccb9a0087ae": {"ta_keywords": "intelligent simple easy;intelligent systems simple;intelligent intelligent simple;intelligent simple;tutorial peer review;easily implementable rules;easily implementable;make intelligent systems;simple easily implementable;intelligent systems;peer review;easy implement;make intelligent intelligent;tutorial peer;implementable rules tutorial;easy implement way;intelligent intelligent;simple easy implement;simple architecture;peer review backbone;make intelligent;easy use simple;way make intelligent;systems simple easily;implementable rules;unfairness paper;rules tutorial discusses;bias unfairness paper;simple easy use;peer", "pdf_keywords": ""}, "e8deeebc7ff6315115f01fd70a343d62db202888": {"ta_keywords": "crowdsourcing approach matching;crowdsourcing;lexical resource;representing hyponymic hypernymic;crowdsourcing approach;presents crowdsourcing approach;presents crowdsourcing;specific lexical resource;lexical resource paper;semantic glossaries;paper presents crowdsourcing;hyponymic hypernymic relations;semantic semantics represented;semantic;semantics represented;synsets representing hyponymic;semantic semantics;fk particular semantic;domain specific lexical;particular semantic semantics;specific lexical;semantics;representing hyponymic;lexical;natural language;particular semantic;hyponymic hypernymic;language processing domain;relations natural language;natural language processing", "pdf_keywords": ""}, "b778a7c4001898a1c3888577154d747522f16db4": {"ta_keywords": "adversarial loss functions;adversarial loss divergence;adversarial loss;functions adversarial loss;valid adversarial loss;component functions adversarial;adversarial;functions adversarial;valid adversarial;functions valid adversarial;loss functions;loss divergence like;loss divergence;loss functions valid;probability particle mixture;mixture probability distribution;probability particle ensemble;loss functions perform;particle mixture asymptotically;types loss functions;mixture asymptotically;particles binary mixture;ensemble particles probability;ensemble particles;mixture probability;binary mixture probability;particle ensemble;ensemble;mixture;binary mixture", "pdf_keywords": "adversarial losses generative;losses generative adversarial;divergence minimization adversarial;discriminator adversarial loss;minimization adversarial losses;adversarial losses;adversarial loss;adversarial losses applied;adversarial training objectives;adversarial loss using;adversarial training;generative adversarial;discriminator adversarial;propose generative adversarial;generative adversarial networks;generative adversarial network;designing adversarial training;space adversarial losses;data space adversarial;minimization adversarial;losses generative;adversarial;adversarial network;adversarial networks;designing adversarial;adversarial network model;adversarial networks machine;relationship discriminator adversarial;generative;space adversarial"}, "0453bab552e83f19dd6ba12061949f128fa9b045": {"ta_keywords": "nonparametric bayesian clustering;ssl supervised learning;bayesian clustering;learning ssl supervised;bayesian clustering algorithm;supervised multiclass;semi supervised multiclass;supervised multiclass learning;classes exploratory learning;known semi supervised;classes learning;ssl supervised;exploratory learning ssl;semi supervised;multiclass learning;multiclass learning methods;supervised learning;based nonparametric bayesian;clustering;exploratory learning;nonparametric bayesian;supervised;supervised learning process;learning methods robust;classes exploratory;learning ssl;numbers classes learning;clustering algorithm;ssl based nonparametric;learning methods", "pdf_keywords": "semisupervised learning ssl;semi supervised learning;propose semi supervised;ssl exploratory learning;classes learning exploratory;semisupervised learning;semi supervised;clustering generalization ssl;present semi supervised;semi supervised machine;exploratory learning extension;supervised learning;supervised machine learning;classes learning;exploratory learning;learning exploratory;learning exploratory learning;nounphrase classi\ufb01cation tasks;performance exploratory learning;supervised;exploratory learning addresses;clustering generalization;supervised learning method;semisupervised;constrained clustering generalization;learning extension;generalization ssl exploratory;learning viewed subtype;learning ssl popular;learning algorithm automatically"}, "b36dc8db9930a785edd55ca30328ace2896523e6": {"ta_keywords": "semantic annotation experiment;annotation tools semantic;semantic annotation gather;annotation semantic;semantic annotation;annotation semantic analysis;semantic annotation semantic;annotation gather teams;semantic annotation rules;results semantic annotation;shared semantic corpora;shared semantic;annotation gather;problematic semantic annotation;set semantic annotation;annotation tools;semantic corpora;trends semantic annotation;annotation;annotation experiment carried;semantic corpus;annotation experiment;standardized shared semantic;semantic corpora paper;semantic;standardized semantic corpus;annotation rules;tools semantic;semantic analysis;tools semantic languages", "pdf_keywords": ""}, "87eece8d39d1e25ba87550be8b01af32738cbf2c": {"ta_keywords": "talker speech recognition;talker automatic speech;multi talker asr;speech recognition asr;talker asr architecture;multi talker speech;speech recognition;talker multi talker;speech recognition proposed;speech recognition systems;multi talker;progress single talker;talker compared basic;talker multi;single talker speech;single talker automatic;talker asr;mixed speech datasets;multi talker compared;recognize overlapped speech;talker automatic;speech datasets;multi talker multi;automatic speech recognition;monaural multi talker;channel multi talker;automatic speech;end multi talker;talker speech;multi talker single", "pdf_keywords": ""}, "e2ece7ea0924b4f95f65587973118bea9a44a3d2": {"ta_keywords": "similarity measure java;develop similarity measure;relationship text corpus;measure java classes;similarity measure;develop similarity;measure java;similarity;text corpus statistics;text corpus;corpus;corpus statistics;term relationships entities;classifier;examine relationship text;relationships entities software;term relationships;java;entities software;java classes;classifier f1 score;java classes using;classifier f1;relationship text;entities software domain;detection coordinate term;effective algorithm finding;validation accuracy dataset;classes using distributional;software domain cross", "pdf_keywords": "nlp software engineering;modeling software code;approach grounding language;grounding language;grounded relation discovery;classes semantic similarity;interesting code taxonomy;semantic similarity relations;nlp software;code taxonomy;statistical language models;similarity measure java;semantically similar;work nlp software;semantic similarity;text entities underlying;statistical language;classes semantic;likely semantically similar;software code;semantically similar 32;relation discovery;shared classes semantic;language models;grounding language acquisition;idea grounded relation;software engineering;researchers statistical language;contexts likely semantically;relation discovery domain"}, "bbc7e533e5bfb388af1afd85bfb7ba17330cae76": {"ta_keywords": "dc pumped superconductors;pumped superconductors operate;pumped superconductors;shore power supply;dc dc pumped;dc currents high;dc currents;dc pumped;superconductors operate;voltage shore power;high voltage shore;strategy water waves;transformer dc;transformer dc component;high cascade current;superconductors operate presence;dc bias transformer;generation dc dc;water waves;bias transformer dc;superconductors;control strategy dc;cascade current eliminated;voltage shore;currents high voltage;dc dc;dc component high;generation dc;dc component;shore power", "pdf_keywords": ""}, "92cee1e209f2d9a311416b0d9fd8a49b0fbe7df2": {"ta_keywords": "filters collaborative learning;learned filters collaborative;collaborative learning transferring;learning transferring;learns filter documents;filters learned users;learning transferring retraining;transfer learned filters;filters collaborative;transfer learned;learned users collaborative;direct transfer learned;learns filter;symbolic learning methods;filters learned;learning methods diverse;text categorization;improving generalization performance;collaborative learning;learning methods direct;generalization performance collaborative;classifier learned;training classifier learned;symbolic learning;learned classifier;learned filters;transferring retraining information;text categorization investigate;retraining information pool;used text categorization", "pdf_keywords": ""}, "956aa64b0d5f5802b98bf551d5bab8993b114fd0": {"ta_keywords": "faster similarity metric;duplicate free similarity;tfidf similarity;tfidf similarity appropriate;compactly faster similarity;similarity function tfidf;good similarity functions;propose new similarity;similarity metric wellsuited;modification tfidf similarity;faster similarity;similarity metric;nearest neighbor classification;function tfidf similarity;new similarity;free similarity metric;widely used similarity;new similarity function;free similarity;similarity;tfidf similarity paper;similarity functions crucial;neighbor classification;similarity functions;average good similarity;similarity metric computed;similarity function reduces;used similarity function;similarity function;nearest neighbor", "pdf_keywords": ""}, "f52f7964febd6d6d72aa23505b50d33e1d4ce0aa": {"ta_keywords": "novel labeling rules;labeling rules data;weakly supervised learning;weakly supervised;labeling rules;supervised learning wl;scarcity nlp tasks;templates weakly supervised;discovering novel labeling;rule based learning;labeling rule set;label scarcity nlp;fully supervised models;labeling rule;rules data improve;gaps fully supervised;nlp tasks;scarcity nlp;boosting identify;nlp tasks manually;supervised learning;rule templates weakly;fully supervised;machines rule templates;supervised;boosting identify large;supervised models;novel labeling;labeling;supervised models paper", "pdf_keywords": "interactive rule discovery;learn rule based;rule annotation comprehensive;rule representation capturing;based rule discovery;rule discovery;rule discovery module;rule discovery framework;labeling rules enhance;rule annotation;prompting interactive rule;rule based prediction;interactive weakly supervised;learn rule;interactive rule;labeling rules;rule generation;rule discovery explored;interactive learning automatically;rule templates prompting;semantics rule generation;easy annotate interactive;model learn rule;learning automatically discover;process rule annotation;learning automatically;rule representation;rule generation iterative;prompt based rule;rule proposal module"}, "60f3e69e4f18e8e8e7dcc4ba66c1e216b49ad982": {"ta_keywords": "sense information game;common sense knowledge;sense knowledge;sense knowledge acquisition;common sense information;collect common sense;engine common sense;commonsense knowledge representation;live commonsense knowledge;sense information;game engine common;game engine;knowledge acquisition gecka;commonsense knowledge;teach gamers meaningful;common sense;thethe game engine;knowledge representation;knowledge;knowledge acquisition;information game designers;knowledge representation reasoning;value teach gamers;purpose knowledge;multi purpose knowledge;teach gamers;information game;games potential generation;reasoning key tasks;purpose knowledge enables", "pdf_keywords": ""}, "09b87b6e7bfbf66d355574d292586595e0185d6e": {"ta_keywords": "transfer learning linguistic;languages features known;lingual transfer learning;languages features;automatically populate typological;typological features correlate;learning linguistic probing;feature correlations team;learning linguistic;information linguistic;typological knowledge bases;values languages features;feature values languages;kbs typological knowledge;typological kbs typological;typological features;information linguistic properties;populate typological kbs;linguistic probing;lingual transfer;predicting feature;linguistic properties world;typological kbs;typological knowledge;linguistic properties;contain information linguistic;including cross lingual;linguistic;predicting feature values;languages", "pdf_keywords": "predicting typological features;prediction categorise languages;automatically predict typological;predicting features languages;predict typological features;typological feature prediction;predicting typological;model predicting typological;predict typological;typological features automatically;categorise languages world;automatically populate typological;model linguistic typology;linguistic typology task;linguistic typology;categorise languages;typology task predicting;typological features databases;world according linguistic;typological features;task predicting features;languages world according;features languages unseen;languages world;model linguistic;prediction categorise;generative model linguistic;features languages;automatically predict;populate typological"}, "5b16d138bf16762d43b55b6e21d9b0b61021180e": {"ta_keywords": "transcription factor trans;network transcription factor;transcription network transcription;transcription network;network transcription;transcription factor;transcription;trans network energy;factor trans network;energy transfer;network energy transfer;trans network;factor trans;network energy;trans;energy;transfer;network;factor", "pdf_keywords": ""}, "b15ea460c77a4ee8aa159a30ab0331deedfcf392": {"ta_keywords": "token expert allocation;expert allocation linear;assignment experts base;expert allocation;layers optimal assignment;optimal assignment expert;training inference routing;layer large language;assignment experts;auxiliary expert balancing;assignment expert;assignment scheme improves;expert balancing;assignment expert receives;token specialized expert;balanced assignment experts;large language models;expert balancing loss;experts base layer;learn balanced routing;optimal assignment scheme;formulate token expert;capacity sparse layers;allowing optimal assignment;language models greatly;optimal assignment;heuristics auxiliary expert;sparse layers optimal;token expert;sparse layers", "pdf_keywords": "assignment sparse experts;sparse experts base;assignment sparse;sparse experts;large language models;tokens experts training;balanced assignment sparse;assignment experts base;sparsity language models;layer large language;experts model parallelism;learns balanced assignment;language models greatly;network architecture learns;token experts model;assignment tokens experts;experts training;expert based sparsity;training machine predictive;assignment experts;experts base layer;propose deep convolutional;balanced assignment experts;language models;learns;deep convolutional neural;assignment tokens;deep convolutional;language models built;architecture learns"}, "bbb7eb10c45cabaee6e427242fce7180c0217ef1": {"ta_keywords": "programming languages predicting;languages predicting program;predicting program properties;representation predicting program;abstract syntax tree;predicting program;represent program using;programming languages;syntax tree allows;syntax tree;programming languages ease;languages predicting;different programming languages;represent program;predicting variable names;tasks programming languages;idea represent program;learning programs;paths abstract syntax;program properties evaluation;programming;representation purely syntactic;abstract syntax;different programming;types representation;names types representation;program properties;shown learning programs;representation predicting;program properties names", "pdf_keywords": ""}, "cbf941fef87830efa4de98455cfe943917909b66": {"ta_keywords": "methods convex broyden;superlinear convergence rate;superlinear convergence bfgs;new superlinear convergence;superlinear convergence;newton methods convex;hessian approximations;superlinear convergence classical;hessian approximation;hessian approximations paper;local superlinear convergence;methods convex;moment superlinear convergence;inverse hessian approximations;convex optimization;optimal solution convex;method solve convex;quasi newton methods;convergence bfgs method;solve convex optimization;convex broyden;hessian approximation trace;convex optimization problem;hessian;determinant hessian approximation;convex broyden class;analysis local superlinear;convergence rate method;inverse hessian;convergence bfgs", "pdf_keywords": "newton method convex;quasi newton methods;newton methods superlinear;quasi newton method;convergence quasi newton;methods superlinear convergence;hessian newton method;newton methods nonlinear;method convex;functions quasi newton;method convex problems;superlinear convergence quasi;keywords quasi newton;methods nonlinear objective;newton methods keywords;objective functions quasi;hessian newton;superlinear convergence convergence;quasi newton;superlinear convergence;new quasi newton;methods superlinear;exact hessian newton;nonlinear objective functions;newton method function;newton methods far;newton method linear;newton methods;method function minimization;theory superlinear convergence"}, "d8551a4b49aa547ad8884ba9f545480860fcadd1": {"ta_keywords": "materials learning deep;brittle materials learning;materials learning;learn material models;deep implicit fourier;implicit fourier neuralators;fourier neuralators;material models;material modeling challenging;material models directly;deep networks constitutive;material modeling;heterogeneous material modeling;fourier neuralators ifnos;learning deep;learned solution operators;based continuum mechanics;modeling mechanical;predicting displacement fields;deep networks;learning deep implicit;accelerated learning;network discretized integral;learning techniques deep;modeling based continuum;constitutive modeling;neuralators ifnos applications;fast fourier transformation;transformation accelerated learning;continuum mechanics", "pdf_keywords": "neural operator solution;implicit fourier neural;solution operator learning;fourier neural operators;neural networks material;neural operators;predicting permeability glass;learning implicit equation;neural operator architecture;architecture learning implicit;neural networks;operator learning problems;integral neural operator;learning implicit;use deep neural;deep neural;operator learning;fourier neural;neural operator;method predicting permeability;deep layers;architecture learning;predicting permeability;equations pdes material;permeability glass ceramics;pdes material responses;deep neural networks;deep layers \ufb01xed;implicit equation solvers;approximating pdes"}, "cee25a535ec7165eae38f498a391050077ad9f65": {"ta_keywords": "speaker clustering sampling;speaker clustering;based speaker clustering;utterances infinite mixture;large scale utterances;utterance oriented dirichlet;speakers proposed clustering;clustering sampling;clustering sampling based;hierarchical clustering;hierarchical clustering algorithm;dirichlet process mixture;scale utterances;new hierarchical clustering;scale utterances infinite;infinite mixture model;clustering algorithm large;clustering methods large;model called utterance;state art clustering;model based speaker;utterances infinite;proposed clustering;clustering;clustering algorithm outperforms;clustering methods;mixture model;clustering algorithm;proposed clustering algorithm;utterances", "pdf_keywords": ""}, "7668b23aadf43bebe5e2d3abf37938b44bd16200": {"ta_keywords": "unified multimodal reasoning;multimodal reasoning models;multimodal reasoning;robot challenge community;unified multimodal;language groundable visual;multimodal;robot;assistants query language;digital assistants query;reasoning models;create unified multimodal;closer digital assistants;qr context team;models answer questions;groundable visual representations;digital assistants;language groundable;richer visual online;groundable visual;visual online world;iot devices;iot;robot challenge;lack language groundable;knowledge richer visual;team competition qr;models lack language;new benchmark;assistants query", "pdf_keywords": "multimodal reasoning knowledge;model multimodal reasoning;modal question answering;multimodal reasoning;retrieval answer domains;question answering;question answering challenge;answering challenge community;answer answering;annotators snippets;answering questions;textual representation images;reasoning knowledge aggregation;approach answer answering;answer domains dataset;visual understanding;answering challenge;visual understanding provide;lossy textual representation;answer answering questions;datasets tasks broadly;answering;answering questions open;annotators;bottlenecked lossy textual;representation images consumed;retrieval answer;multimodal;present retrieval answer;reasoning knowledge acquisition"}, "c688e187cede868e35fc1b53913e0fbbe6e38ea0": {"ta_keywords": "learns expert demonstrations;imitation learning paradigm;imitation learning;based imitation learning;sequential prediction;structured prediction task;structured prediction;structured prediction algorithms;complex structured prediction;sequential prediction algorithm;compared sequential prediction;performance structured prediction;learns expert;paradigm learns expert;expert demonstrations order;prediction task;paradigm based imitation;paradigm learns;learning paradigm learns;based imitation;learns;prediction algorithm searn;prediction algorithms;expert demonstrations;faster learning searn;prediction algorithm;prediction;demonstrations order able;faster learning;prediction algorithms context", "pdf_keywords": ""}, "2448e63a7bb626d09001fe37e60befdb2919f6e6": {"ta_keywords": "information extraction;information extraction geared;commonsense knowledge determining;readable knowledge bases;sense knowledge web;knowledge web;machine readable knowledge;extracting information;commonsense knowledge;approach information extraction;knowledge bases;graph based markov;extracting information noisy;knowledge web scale;commonsense facts world;commonsense property lookup;online provides commonsense;automatic search web;extract common sense;commonsense facts;method extracting information;automatic search;based markov;basic commonsense facts;common sense knowledge;based markov chain;readable knowledge;based fact product;knowledge determining;language models sources", "pdf_keywords": ""}, "cee96ee69adacfdeb648c230d2c9b01011724724": {"ta_keywords": "study performance lhc;performance lhc;performance lhc terms;lhc;lhc terms;lhc terms number;weight participants involved;weight participants;participants relative weight;number participants;relative weight participants;participants involved event;number participants relative;participants;participants involved;performance;participants relative;involved event;event;relative weight;weight;comprehensive study performance;terms number participants;study performance;present results comprehensive;results comprehensive;present results;paper present results;results;results comprehensive study", "pdf_keywords": ""}, "007371feab4af758b74580c43e74827b3500c67e": {"ta_keywords": "distributed video demand;distributed content distribution;distributed video;demand streaming streaming;distributed content;video demand streaming;streaming streaming applications;distributed algorithm;demand streaming;propose distributed algorithm;design distributed video;streaming streaming;streaming applications;propose simple distributed;simple distributed;streaming applications establish;distributed;propose distributed;highly distributed implementation;topology selection node;streaming;demand propose distributed;distributed solution;framework distributed content;distributed implementation highly;bandwidth node;video demand;distributed algorithm solve;simple distributed solution;distributed implementation", "pdf_keywords": ""}, "2a0cb1a1e78b77fe9981e4935410cf3ea900e370": {"ta_keywords": "body decays xmath0;decays xmath0;decays xmath0 cols;analysis body decays;xmath0;xmath0 cols;body decays;xmath0 cols options;decays;analysis body;comprehensive analysis body;body;header;cols options header;present results comprehensive;cols;present results;results comprehensive;cols options;comprehensive analysis;results;results comprehensive analysis;options header;comprehensive;report present;report;present;analysis;report present results;options", "pdf_keywords": ""}, "d60b4594fb0404329d9ebf6fd88702ca3479e904": {"ta_keywords": "abbreviation alignment model;extract million abbreviations;alignment based abbreviation;million abbreviations corpus;algorithm extracting abbreviation;based abbreviation extractor;abbreviation alignment;extracting abbreviation;abbreviations corpus;abbreviation extractor;extracting abbreviation single;use abbreviation alignment;abbreviation extractor naturally;alignment model extract;abbreviations;abbreviations corpus 200k;letter expression biomedical;faster comparable alignment;comparable alignment based;million abbreviations;abbreviations use;based alignment based;approach based alignment;expression biomedical;abbreviation;abbreviations use abbreviation;biomedical expression;biomedical expression model;text pubmed;abbreviation single letter", "pdf_keywords": ""}, "99546b4d1f2547095bb15eec36e03f64b74a78d4": {"ta_keywords": "community feedback;susceptibility community feedback;past community feedback;activity emojis significantly;community feedback report;message topics;stock trading;trading stock market;stock market;emojis significantly;stock market perform;message topics based;users newcomers examine;activity emojis;users newcomers;stock market short;trading stock;term users newcomers;feedback;feedback report latest;feedback report;stock trading stock;emojis;trading;increase activity emojis;subsequent message topics;market short note;based search high;market;newcomers examine", "pdf_keywords": ""}, "5e3d1bece9dd2356fd2b31312bd62c8f7126882d": {"ta_keywords": "energy disaggregation utility;energy trading;utility privacy tradeoff;energy disaggregation;disaggregation utility privacy;context energy trading;utility privacy;privacy tradeoff studied;privacy tradeoff;disaggregation utility;disaggregation;tradeoff studied;privacy;energy;tradeoff studied context;trading;utility;tradeoff;context energy;studied context energy;studied context;context;studied", "pdf_keywords": ""}, "b990331a5394f3642a1fd1791d70bfa2d85d9d1d": {"ta_keywords": "conversations twitter vaccines;vaccination related tweets;vaccines related tweets;twitter vaccines;twitter vaccines severity;websites shared vaccines;videos shared vaccination;tweeted web domains;different social media;conversations twitter study;vaccination narratives users;conversations twitter;twitter social media;tweets posted covid;twitter common communication;twitter study reveals;dynamics conversations twitter;including tweeted web;content including tweeted;19 conversations twitter;twitter study;tweeted web;social media;social media platforms;tweets present;tweets important understanding;anti vaccination narratives;related tweets;tweets posted;tweets", "pdf_keywords": ""}, "8b98f7ff3bb1b199db85fc219a5c27b355adf1be": {"ta_keywords": "lengthening osseous crowns;osseous crown lengthening;laser lengthen crown;osseous crowns based;crown lengthening;technique lengthening osseous;crowns osseous crown;osseous crowns osseous;osseous crowns;osseous crown;crown lengthening needed;crowns osseous;lengthen crown;lengthening osseous;crowns based use;lengthen crown shown;crowns;laser lengthen;tooth structure placement;crown;crowns based;new technique lengthening;erbium laser lengthen;crown shown reduce;tooth structure;patient smile prevent;technique lengthening;crown shown;appearance patient smile;lengthening needed enhance", "pdf_keywords": ""}, "a604ad4654f31d325b888806e276123a704cb5c8": {"ta_keywords": "minimum error classification;minimum error margin;classification robustness conventional;geometric margin maximization;classifiers minimum classification;classification robustness;minimum classification error;classifiers minimum;margin general class;geometric margin control;increase classification robustness;minimum classification;based classifiers minimum;geometric margin general;control geometric margin;classifiers;margin control geometric;error classification;classification tasks derive;margin maximization realize;conventional machine learning;prototype based classifiers;margin maximization;geometric margin;range classification;error margin control;derive geometric margin;wide range classification;error classification various;classification", "pdf_keywords": ""}, "652e3c774da47c0c8788111ec886a00d3b8fc637": {"ta_keywords": "simulations tumour resection;simulation tumour resection;computer simulations tumour;computer simulation tumour;brain deformations tumour;simulations tumour;deformations tumour resected;simulation tumour;deformations tumour;brain deformation performed;tissues computer simulations;compute brain deformations;fluid tumour resection;tumour cavities surfaces;dynamics video tumour;computed deformed surfaces;tool tumour imaging;tumour resection requires;maximise tumour removal;surrounding fluid tumour;tumour resection;tumour cavity;tumour resection induced;tumour cavity used;tumour removal;navigation maximise tumour;tumour removal simultaneously;deformation performed using;video tumour cavity;brain deformation", "pdf_keywords": ""}, "d0ea87ce3bcd86428d379fd478c365c64f870200": {"ta_keywords": "robustness dialogue systems;dialogue systems linguistic;robustness dialogue;dialogue state tracking;dialogue systems;sensitivity robustness dialogue;descriptors evaluate dialogue;crowdsourced variants schema;terms robustness dialogue;linguistic variations schemas;evaluate dialogue state;schema sensitivity robustness;measuring schema sensitivity;dialogue state;systems linguistic variations;language schemas model;systems linguistic;present schema robustness;schema robustness;dialogue;language schemas;evaluate dialogue;schema robustness zero;crowdsourced variants;schema sensitivity;generalizes schema variations;linguistic variations;include crowdsourced variants;linguistic;schema variations", "pdf_keywords": "schema based dialogue;dialogue state tracking;schema guided dialogue;improving robustness dialogue;robustness dialogue state;topperforming dialogue state;dialogue systems;robustness dialogue;schemaguided models linguistic;topperforming dialogue;dialogue systems interpret;introduce schema augmentation;evaluate topperforming dialogue;schema augmentation;guided dialogue state;translating schemas training;based dialogue systems;systems interpret conversations;measuring schema sensitivity;dialogue state;reducing schema sensitivity;linguistic styles schemas;guided dialogue;schemas training;models linguistic;schema augmentation method;dialogue;robustness schemaguided models;generalizes schema variants;interpret conversations execute"}, "429b65937d4922578a81e1f0ef5aeab7361ae36b": {"ta_keywords": "mappingenglish sentences bash;sentences bash commands;mappingenglish sentences;mappingenglish;scripting;semantic parsing;bash commands expert;natural language processing;sentences bash;parsing;semantic parsing methods;problem mappingenglish sentences;natural language;scripting simply;specific scripting;bash commands;specific scripting simply;commonly used bash;scripting simply stating;commands expert written;problem mappingenglish;language processing pendulum;language processing;written english descriptions;parsing methods;data semantic parsing;commands expert;commands;context natural language;bash", "pdf_keywords": "neural machine translation;machine translation trained;code translation tasks;semantic parsing benchmarks;novel machine translation;goals natural language;machine translation models;machine translation;translation tasks;generated natural language;translation tasks seq2seq;existing semantic parsing;nl code translation;translation models demonstrated;translation nl code;generating utility commands;natural language nl;commands andscripts natural;translation models;translation trained;semantic parsing;commands generated natural;natural language evaluated;utility commands andscripts;translation trained set;code translation;andscripts natural language;target meaning representation;parsing benchmarks;language evaluated neural"}, "ba45a346690f3c5b6f8c371b5c6cf1d7cce5619d": {"ta_keywords": "constrained rank minimization;problem constrained rank;augmented ray binary;constrained rank;rank minimization;ray binary arx;identifying augmented ray;rank minimization problem;augmented ray;binary arx model;method identifying augmented;relaxed convex formulation;blindind identification lifting;convex formulation;binary arx;ray binary;arx model output;subspace bounded domain;subspace;arx model;identifying augmented;finding linear subspace;convex formulation approximate;output measurements blindind;measurements blindind identification;minimization;posed problem unique;relaxed convex;subspace bounded;problem constrained", "pdf_keywords": "propose rank minimization;minimizes column rank;rank minimization;rank minimization heuristic;blind identi\ufb01cation hammerstein;program minimizes column;convex program minimizes;minimizes column;identi\ufb01cation hammerstein systems;column rank paper;constant blind identi\ufb01cation;program minimizes;convex program;approach blind identi\ufb01cation;finding convex program;framework blind identi\ufb01cation;relaxed convex formulation;column rank;blind identi\ufb01cation;minimizes;convex formulation;problem convex relaxation;convex formulation approximate;novel framework blind;inputs identi\ufb01cation hammerstein;minimization;solution problem convex;paper propose rank;blind identi\ufb01cation present;convex relaxation"}, "65c2a39f1579a947926ac5746888445ea4afdf6e": {"ta_keywords": "unsupervised grammar induction;supervised grammar induction;dependencies probabilistic grammars;grammar induction;grammar induction focus;probabilistic grammars;capable unsupervised grammar;grammar induction benefit;methods grammar induction;supervised grammar;context free grammar;unsupervised grammar;grammar induction plagued;probabilistic grammars pcfgs;modeling lexical dependencies;free grammar;free grammar cf;unsuitable supervised grammar;grammar cf based;neural models sparsity;grammars pcfgs;lexical dependencies unified;grammars;modeling lexical;lexical dependencies;grammars pcfgs shown;induction plagued sparsity;grammar cf;grammar;sparsity patterns parsimonious", "pdf_keywords": "generative grammar induction;unsupervised grammar induction;grammar induction based;grammar induction model;based grammar induction;model grammar induction;modeling lexical dependencies;model generative grammar;generative grammar;grammar induction;dependency structure grammar;grammar induction focus;methods grammar induction;dependency model generative;unsupervised grammar;grammar induction paper;new unsupervised grammar;probabilistic model grammar;phrasestructure dependency structure;generative dependency model;propose generative dependency;grammar cfg based;constituent based grammar;context free grammar;generative dependency;grammar induction bene\ufb01t;model grammar;lexical dependencies;grammar induction contrasts;free grammar cfg"}, "562f33611cdc0d8ed6609aa09f153e6238d5409e": {"ta_keywords": "clinical time series;classification diagnoses;classification diagnoses given;semif semiflexible swimmer;single semiflexibility swimmer;multilabel classification diagnoses;semiflexible swimmer;semiflexibility swimmer;semiflexibility swimmer diseases;classification;multivariate time series;semiflexible swimmer reduced;swimmer diseases tests;multilabel classification;task multilabel classification;missing data sequential;clinical time;time series observations;swimmer;multivariate time;swimmer reduced competition;time series;swimmer diseases;given clinical time;data sequential;predictive results;diagnoses;data sequential inputs;consists multivariate time;predictive", "pdf_keywords": "prediction diagnoses;networks rnns;networks rnns suited;multilabel prediction diagnoses;prediction diagnoses paper;recurrent neural networks;rnns;rnns suited learning;neural networks rnns;predictive power rnns;learning sequential temporal;intervals recurrent neural;recurrent neural;missing data features;rnns unlike linear;learning sequential;2016 multilabel prediction;power rnns;multilabel prediction;learning long term;data features reasonably;engineered missing data;run predictive results;problem models trained;time series diseases;neural networks;machine learning learn;rnns unlike;suited learning sequential;information neural"}, "a36f7d5d8f724168e534925edff97b3680e545c9": {"ta_keywords": "network tensor contractions;recognition tensors;tensor contractions neural;image recognition tensors;neural network tensor;dimensionality activation tensors;networks tensors;networks tensors offer;biological networks tensors;activation tensors;activation tensors tcls;network tensor;tensor contraction image;activation tensors reduce;contractions neural network;use tensor contractions;recognition tensors shown;tensor contractions;tensors tcls;tensor contraction;tensor decompositions noted;tensors tcls used;tensor decompositions;neural network layers;apply activation tensors;tensors;feature tensor contractors;particular tensor decompositions;feature tensor;tensensor contraction layer", "pdf_keywords": "convolutional networks improve;layer deep neural;weights deep neural;activations weights deep;parameters deep neural;dependencies activation tensors;activation tensor processing;deep neural;propose deep convolutional;represent activation tensor;activation tensors;deep convolutional networks;deep neural networks;deep convolutional neural;convolutional networks;popular deep convolutional;deep convolutional;vgg deep neural;activation tensor;alexnet vgg deep;deep neural network;image recognition augmenting;recognition augmenting;layer deep;convolutional neural network;neural network architecture;convolutional neural;networks alexnet;neural networks ubiquitous;networks alexnet vgg"}, "c2ff76c75acc777e005360e9d4c4d928d95c0432": {"ta_keywords": "regenerating codes storage;codes distributed storage;regenerating codes distributed;network regenerating codes;codes storage;codes reconstruct network;regenerating codes;distributed storage;distributed storage systems;codes storage individual;storage data distributed;storage systems dnsns;failure redundant storage;codes distributed;overview regenerating codes;storage nodes;redundant storage data;redundant storage;regenerating codes new;codes minimize repair;storage systems;individual storage nodes;codes reconstruct;minimize repair bandwidth;storage nodes prone;solomon codes reconstruct;multiple storage nodes;network regenerating;storage nodes ensure;topology network regenerating", "pdf_keywords": ""}, "6c3b8e65dc45cb62172f9425dcff4c48055d47eb": {"ta_keywords": "food social media;food textual features;language food geographic;language food textual;topic modeling language;food related posts;topic modeling;related posts twitter;food textual;language food social;food geographic;dynamics language food;language food;twitter demonstrate latent;food geographic locale;textual features;processing topic modeling;food related;posts twitter;complex natural language;twitter;natural language;food social;connections language food;natural language processing;million food related;social media;time query visualization;corpus million food;mirrored language food", "pdf_keywords": "predicting\ufb01c patterns tweets;language food prediction;tweets use predict;patterns tweets;location group tweets;patterns tweets use;tweets;group tweets;language food;food prediction tasks;predicting location user;food prediction;tweets paper;tweets use;power language food;social network;language based model;vocabularies;group tweets paper;predictive power language;vocabularies present;likelihood diabetes political;predicting location;language food western;location likelihood diabetes;language food paper;tweets paper investigate;textual features classi\ufb01cation;predicting;prediction tasks"}, "5e00596fa946670d894b1bdaeff5a98e3867ef13": {"ta_keywords": "video question answering;vision language pretraining;generative vision language;visual language model;vision language benchmarks;representations vision language;modeling visual textual;multimodal downstream tasks;simulations simvlm trained;vision language;modality transfer video;visual language;language pretraining;language pretraining vlp;visual textual representations;simvlm trained;minimalist pretraining framework;language model simvlm;simple visual language;textual representations vision;simvlm trained end;impressive performance multimodal;multimodal;discriminative generative vision;visual textual;performance multimodal;generative vision;video game simvlm;modeling visual;minimalist pretraining", "pdf_keywords": "visual question answering;captioning visual reasoning;question answering image;vision language tasks;tasks text generation;visual entailment multimodal;text generation;vision language representation;generative model visual;unifying vision language;captioning visual;description visual input;answering image captioning;generative models;generative;perform text generation;generative vlp generative;visual linguistic;generative models simvlm;vlp generative models;visual linguistic benchmarks;text generation gpt;research generative vlp;visual entailment;generative vlp;description visual;vision language;propose generative;future research generative;entailment multimodal translation"}, "96bb4b49f69419c31857e928969fcaa137e15060": {"ta_keywords": "models answer generation;answer generation;review based qaisa;questions answers;answer generation propose;comprehension models synthesizing;selecting relevant reviews;answers 14 reviews;models synthesizing answer;comprehension models;reading comprehension models;reviews given question;popular qa datasets;informational retrieval;human interactions customer;way answer question;task review based;customers post questions;answer question;answer question paper;reviews;task review;review based;consists 923k questions;reviews 156k products;qa;923k questions answers;questions;combines informational retrieval;qa datasets", "pdf_keywords": "question answering;build question answering;community question answering;machine comprehension models;question answering qa;machine reading comprehension;question answering present;product review dataset;amazon mechanical turk;learning building answerability;answer generating systems;building answerability;reading comprehension customers;questions answers;machine comprehension;developing answer generating;comprehension models;mechanical turk platform;inspired machine comprehension;extracting review snippets;answering qa product;comprehension models like;answer generating;attention flow;answerability;review snippets;building answerability classi\ufb01er;directional attention flow;review dataset;answering qa"}, "2873f78efd7adcb118a70f8ea3ca7fa1501e320a": {"ta_keywords": "shot relation classification;interplay quantum;quantum;freedom quantum;quantum thermal fluctuations;fluctuations dimensional 2d;study interplay quantum;fluctuations dimensional;degrees freedom quantum;confined harmonic trap;entanglement environment;thermal fluctuations dimensional;harmonic trap;quantum thermal;relation classification;relation classification models;interplay quantum thermal;entanglement;freedom quantum degree;shot relation;dimensional 2d electron;domain adaptation;confined harmonic;techniques domain adaptation;quantum degree;degree entanglement environment;electron gas confined;quantum degree entanglement;domain adaptation nota;gas confined harmonic", "pdf_keywords": "shot domain adaptation;domain adaptation shot;domain adaptation;shot learning adversarial;examples shot domain;shot learning;propose shot domain;propose shot learning;techniques domain adaptation;model shot learning;training domains;shot domain;shot learning model;domain adaptation aims;shot models transfer;domain adaptation nota;shot relation classi\ufb01cation;learning model shot;adversarial examples shot;domain adaptation ofthe;challenging shot relation;vastly training domains;shot relation;work shot domain;training domains paper;shot models;adaptation shot da;adaptation shot;models transfer domains;shot theabove detection"}, "3f311aee9d25b0284d21274cfc8706d6f0277f87": {"ta_keywords": "bitwidth operations dnns;dnn quantization;weights activations dnn;operations dnns reduced;operations dnns;quantization complex networks;activations dnn layers;dnn layers;space dnn quantization;design space dnn;dnn quantization paper;activations dnn;bit weights activations;neural networks including;quantization bitwidth;bitwidth hyperparameter optimization;quantization bitwidth hyperparameter;formulating quantization bitwidth;weights activations inference;dnns reduced;compute weights activations;dnns reduced compromising;neural networks;performing heterogeneous quantization;networks;bit weights;optimization 10 efficiently;heterogeneous quantization;dnn layers dna;quantization", "pdf_keywords": "deep quantization neural;quantization neural networks;learning deep quantization;deep quantization;quantization neural;wise quantization neural;quantized training;method deep quantization;quantized training technique;orthogonal quantized training;layer wise quantization;machine learning deep;reinforcement learning rl;deep learning;networks orthogonal quantized;learning deep;learning deep networks;neural networks challenging;deep learning machine;networks deep learning;networks deep;deep networks;releq learning generalization;quantization;deep networks deep;networks learning deep;overparameterized neural networks;reinforcement learning;wise quantization;deep learning automatic"}, "bcd6cd7bdd661bd86c58b7251ae4633a6ba9979e": {"ta_keywords": "paper recommendation intelligent;recommendation intelligent paper;intelligent paper recommendation;paper recommendation performed;automated method recommending;papers reviewers;automate conference reviewing;sets papers reviewers;reviewers rate keywords;recommendation intelligent;suitable reviewers viewed;conference reviewing;paper recommendation;papers reviewers problem;submissions suitable reviewers;technical paper recommendation;information retrieval;reviewing;recommendation performed combining;conference reviewing process;paper propose automated;reviewers rate;actual reviewing preferences;suitable reviewers;reviewers viewed;intelligent paper;requires reviewers rate;method recommending;based actual reviewing;actual reviewing", "pdf_keywords": ""}, "705794a57cca12c2e58b2d77ac32bd4f92ed31ab": {"ta_keywords": "fluid dynamics video;fluid dynamics;machine readable thesaurus;motion particle viscous;2013 fluid dynamics;viscous flow;particle viscous flow;thesaurus ams research;particle viscous;dynamics;thesaurus;readable thesaurus ams;dynamics video;viscous;thesaurus ams;viscous flow affected;readable thesaurus;motion;flow;external force;dynamics video motion;external force paper;motion particle;video motion particle;flow affected;open machine;flow affected presence;fluid;video motion;presence external force", "pdf_keywords": ""}, "fd306df2809c7acc19dd1994e8ecb11caa33290d": {"ta_keywords": "maneuvering maneuvers noisy;maneuvers noisy environment;propelled maneuvering maneuvers;maneuvers noisy;self propelled maneuvering;propelled maneuvering;maneuvering maneuvers;performing self propelled;dynamics group humans;maneuvers;self propelled;humans performing self;maneuvering;propelled;group humans performing;dynamics;noisy environment;dynamics group;humans performing;group humans;study dynamics group;noisy;performing self;humans;study dynamics;performing;group;self;paper study dynamics;environment", "pdf_keywords": ""}, "99e56ebc2f3739dfca93d5a92ebc1e6e2a3050d2": {"ta_keywords": "peer grading moocs;peer grading;combines peer grading;practice peer grading;models peer grading;grading practice peer;implementation peer grading;peer grading observed;feasibility peer grading;peer grading standalone;grading moocs based;peer grading high;grading moocs;new grading moocs;peer grading auto;grading;grading systems;course students grade;students grade;new grading;grading practice;grading high school;grading systems provides;moocs student evaluation;grading observed;present new grading;grading systems paper;grading standalone;grading auto grading;auto grading practice", "pdf_keywords": ""}, "65b226f71faaac9b8a4d63445c85601a16635464": {"ta_keywords": "stochastic gradient descent;efficiently gradient descent;gradient descent;nonconvex optimization nonconvex;nonconvex optimization;gradient descent sgd;optimization nonconvex;methods convex optimization;optimization nonconvex locomotion;involved nonconvex optimization;convex optimization;gradient descent gd;stationary points efficiently;learning involved nonconvex;algorithms converge stationary;performance methods convex;efficiently gradient;learning algorithms converge;saddle points xmath0;points efficiently gradient;convex optimization problems;stochastic gradient;descent gd stochastic;classical machine learning;gd stochastic gradient;methods convex;nonconvex locomotion;algorithms numerical;converge stationary points;stability saddle points", "pdf_keywords": "faster gradient descent;stochastic gradient descent;stochastic gradient approximated;gradient descent;gradient descent paper;pgdin gradient descent;nonconvex optimization;functions gradient descent;gradient descent converges;gradient descent algorithm;stochastic gradient;dimension nonconvex optimization;particular stochastic gradient;nonconvex optimization problems;gradient descent escape;faster gradient;points faster gradient;gradient approximated;learning algorithms perturbed;present stochastic gradient;optimization problems intractable;gradient approximated cubic;regularized non convex;variant pgdin gradient;algorithms perturbed gd;pgdin gradient;stationary points gradient;convergence machine learning;non convex newton;convex newton step"}, "c6bb9e4a9eaa0f0f8309597af2cefe03bd3f1bb5": {"ta_keywords": "chaos interpretable benchmark;benchmarks chaos interpretable;interpretable benchmarks chaos;regression algorithms chaotic;forecasting models extensive;benchmarks chaos;chaotic systems inherently;interpretable benchmark forecasting;forecasting data driven;benchmark forecasting;inherently challenge forecasting;challenge forecasting models;benchmark forecasting data;chaotic dynamical;information underlying attractor;benchmarks correlate forecasting;benchmarking symbolic regression;chaotic systems;challenge forecasting;categorize diverse dynamics;scales chaotic systems;chaotic;chaotic dynamical systems;chaos interpretable;algorithms chaotic;algorithms chaotic systems;surrogate transfer learning;symbolic regression algorithms;scales chaotic;forecasting models", "pdf_keywords": "symbolic regression neural;symbolic regression machine;forecasting nonlinear dynamical;learning techniques chaotic;based symbolic regression;symbolic regression;forecasting nonlinear;form symbolic regression;time series prediction;accuracy symbolic regression;symbolic regression derived;symbolic regression paper;analysis symbolic regression;approach forecasting nonlinear;regression neural;forecasting;dimensional chaotic dynamical;new forecasting dataset;forecasting dataset long;forecasting dataset;regression machine learning;new forecasting;dimensional chaotic;low dimensional chaotic;techniques chaotic systems;chaotic dynamical;chaotic dynamical systems;present new forecasting;novel approach forecasting;prediction horizons"}, "ede108538033ae00d1667685afbd488380020613": {"ta_keywords": "antiviral drugs quantified;xmath1 plane alfvirus;viral encephalomyces cerevisiae;ba antiviral drugs;different antiviral;ba antiviral;antiviral drugs;viral viral encephalomyces;different antiviral drugs;ba ba antiviral;antiviral;effectiveness different antiviral;antiviral drugs elimination;viral viral;proton collisions international;viral encephalomyces;alfvirus respiratory diseases;analysis dynamics eukaryotic;plane alfvirus respiratory;plane alfvirus;alfvirus respiratory;protein association presence;profile viral viral;viral response;alfvirus;dynamics eukaryotic;proton collisions;protein protein association;viral response profile;protein association", "pdf_keywords": ""}, "f9f862f48599526147bbb110ba986ff6872ef4b0": {"ta_keywords": "tracking movement animals;motion animals crowded;tracking movement;data motion animals;animals crowded environments;sensing data motion;mobile sensing data;mobile sensing;movement animals;motion animals;movement animals based;animals crowded;animals based mobile;based mobile sensing;crowded environments modelled;sensing data;approach tracking movement;tracking;data motion;environments modelled stochastic;approach tracking;crowded environments;sensing;new approach tracking;modelled stochastic;stochastic;animals;modelled stochastic process;motion;stochastic process", "pdf_keywords": ""}, "be0c64252a2c3071236d88feeab47d06ef6e0fb7": {"ta_keywords": "recipient recommendation systems;useful email clients;email corpus;recipient recommendation;enron email corpus;large scale email;accurate useful email;email systems;easily implemented email;email clients particularly;email systems enhanced;work recipient recommendation;email corpus paper;recipients email popular;investigate email systems;recipient recommendation real;email clients valuable;useful email;email server suggesting;suggesting recipients;email clients;email collection;recommendation systems solution;scale email collection;server suggesting recipients;recipients email;implemented email client;implemented email;email collection enron;specified recipients", "pdf_keywords": ""}, "b661520bf0061b7d96ccf12016e351dd3a6ee780": {"ta_keywords": "epochs importance weighted;importance weighting;importance weighting characterized;importance weighted;importance weighting performance;importance weighted risk;effect importance weighting;impact importance weighting;networks effect importance;weighted risk minimization;deep neural;epochs importance;impacts parameterized deep;parameterized deep neural;importance weighting express;successive epochs importance;l2 regularization batch;parameterized deep;weighting performance linear;deep neural networks;learning algorithms causal;weighted risk;l2 regularization;l2 regularization produce;performance linear networks;regularization batch normalization;imbalance policy reinforcement;tweak l2 regularization;inference domain adaptation;regularization batch", "pdf_keywords": "weighting deep learning;importance weighting neural;importance weighting deep;weighting neural networks;weighting neural;deep learning;importance weighting diminishes;dataset weighted loss;importance weighting;networks trained;networks trained images;stochastic gradient descent;text dataset weighted;machine learning tasks;convolutional networks trained;weighted loss functions;gradient descent;models trained text;counteract imbalance training;imbalance training;unregularized neural networks;gradient descent sgdin;dataset weighted;weighted risk minimization;weighted loss;trained images;deep learning variety;trained text;learning tasks;models trained"}, "2cd2df06d488565063e0600ff840d293be2eaf31": {"ta_keywords": "correlated equilibria game;game simple adaptive;proportional measures regret;equilibria game;adaptive procedure playing;measures regret;playing game probability;game probability proportional;game probability;equilibria game simple;empirical distributions play;regret;procedure playing game;play converge;playing game;measures regret having;play;strategies past;adaptive;correlated equilibrium;game simple;strategies;distributions play;play converge set;correlated equilibria;adaptive procedure;used strategies past;simple adaptive;equilibria;game", "pdf_keywords": "player decision maker;adaptive procedure playing;equilibria player actions;predicting outcome play;outcome play discrete;play discrete time;dynamic programming decision;dynamic programming;procedure playing game;player decision;play discrete;particular probabilities switching;probabilities switching;player actions;player actions players;outcome play;decision maker;equilibria player;decision making based;actions players;strategies proportional regrets;probabilistic;game answer question;relationship equilibria player;programming decision making;decision maker paper;procedure playing;switching different strategies;question player decision;actions players paper"}, "372657f609f5a95b378a1aad7b08deb9b9b510c0": {"ta_keywords": "unsupervised reinforcement learning;novel unsupervised reinforcement;unsupervised reinforcement;reinforcement learning framework;reinforcement learning;learn dynamics model;reinforcement learning methods;model based reinforcement;continuous control benchmark;predictive power policy;learn dynamics;methods learn dynamics;based reinforcement learning;control benchmark tasks;policy making models;based reinforcement;simulated data real;control benchmark;real simulated data;sampled environment leverage;range continuous control;simulated data;reinforcement;climate change predictive;long range repulsion;learning;continuous control;generate simulated data;simulated data despite;distributions real simulated", "pdf_keywords": "model domain adaptation;reinforcement learning mbrl;domain adaptation;introducing model adaptation;planning policy networks;unsupervised model adaptation;model adaptation;based policy optimization;traditional reinforcement learning;policy optimization;planning policy;reinforcement learning methods;policy optimization introducing;adaptation augmented model;reinforcement learning;model based reinforcement;control benchmark tasks;model adaptation intention;model adaptation procedure;model based policy;model based planning;continuous control benchmark;domain adaptation particular;based planning policy;based reinforcement learning;planning;adaptation augmented;policy networks;learning mbrl;policy networks challenging"}, "a0511f02a867bf19e2fa01e6cbd3663f4bd1b953": {"ta_keywords": "bose einstein condensate;einstein condensate bec;dynamics bose einstein;dynamics bose;study dynamics bose;bec presence magnetic;einstein condensate;condensate bec presence;bose einstein;condensate bec;reciprocal reciprocal reciprocal;reciprocal reciprocal;formula reciprocal reciprocal;reciprocal reciprocal relationship;formula reciprocal;reciprocal;elegant formula reciprocal;bec;reciprocal relationship;presence magnetic;bec presence;bose;presence magnetic field;condensate;magnetic field;magnetic field short;magnetic;einstein;dynamics;results study dynamics", "pdf_keywords": ""}, "1ae1850bcfa3c31d7bc828cc33f7dd3926cee26f": {"ta_keywords": "paths directed graph;generating directed paths;generate directed paths;directed paths generate;paths propose probabilistic;directed paths generated;paths generate directed;probabilistic logic programming;directed graph;generated directed paths;measuring similarity graphs;paths generated directed;probabilistic logic;directed paths propose;directed paths;directed paths directed;similarity graphs;directed graph mechanism;procesi probabilistic logic;idea directed paths;generate directed;paths directed;paths directed edge;uses directed paths;directed edge graph;graph mechanism based;logic programming language;logic programming;probabilistic version logics;paths generate", "pdf_keywords": ""}, "30cf652bd33049aaf111a5f84eb262a87c045bdb": {"ta_keywords": "fact checking effort;support fact checkers;research fact checkers;manual fact checking;fact checkers;claim retrieval;look claim retrieval;text similarity stance;claim retrieval document;fact checking;detect sentences;modeling text similarity;similarity stance taking;fact checked claims;fact checkers make;fact checkers journalists;detect sentences contain;sentences contain claim;automatically verifying;text similarity;document verified automated;similarity stance;veracity previously fact;aims detect sentences;verifying given document;automated verification;automatically verifying given;taking account veracity;manually annotated;previously fact checked", "pdf_keywords": "politicians automatic factchecking;alternative automatic factchecking;automatic factchecking;automatic factchecking suitable;measures automatic factchecking;automatic factchecking proposed;article fact checkers;fact checking claims;fact checking articles;factchecking suitable;fact checkers;factchecking suitable purposes;claims debate speech;approach fact checking;links fact checking;journalists focuses claims;factchecking proposed;highlight claims debate;factchecking proposed possible;speech links fact;debate speech links;checking claims debate;claims politicians automatic;fact checking;factchecking;commentaries highlight claims;fact checked claims;checking articles claims;fact checkers wrote;focuses claims politicians"}, "31412f9b23511e212895305927d9ccddb445bcbc": {"ta_keywords": "parameters voice timbre;voice timbre control;control voice timbre;voice timbre controllability;method voice timbre;voice timbre multiple;design voice timbre;control parameters voice;converted voice timbre;voice timbre;voice timbre axes;controlling converted voice;parameters voice;timbre control parameters;annotation voice timbre;timbre multiple regression;acoustic basis vectors;parameters acoustic basis;mixture models mggm;control voice;using converted voices;voice timbre expression;gaussian mixture models;voices natural voices;timbre controllability proposed;voice timbre investigate;improving converted voice;corresponding acoustic basis;regression gaussian mixture;timbre controllability", "pdf_keywords": ""}, "633ee881c594cface387557359ef13613d8eaef0": {"ta_keywords": "efficiency envy freeness;random assignment mechanisms;assignment mechanisms random;optimal egalitarian;optimal egalitarian value;approximate optimal egalitarian;ordinality envy freeness;efficiency envy;tradeoffs efficiency envy;envy freeness;randomness behavior;truthfulness achievable egalitarian;achievable egalitarian;achievable egalitarian value;envy freeness truthfulness;randomness;ordinality envy;random serial dictatorship;serial dictatorship probabilistic;like ordinality envy;dictatorship probabilistic serial;randomness behavior group;random assignment;prominent random assignment;distributions random assignment;mechanisms random;egalitarian value;effect randomness behavior;assignment mechanisms agents;dictatorship probabilistic", "pdf_keywords": ""}, "1ccd031f28dccfb226f6c0c588c93a97a50bf95f": {"ta_keywords": "benchmark code generation;automatic code generation;code generation;evaluating code generation;computer code generation;code generation performance;code generation social;code generation important;code generation increases;challenges programming broadly;generate satisfactory python;benchmark code;apps benchmark code;benchmark machine learning;present python benchmark;new benchmark code;automatic code;programming broadly;python benchmark machine;satisfactory python code;challenges programming;code meet challenge;introduce apps benchmark;python benchmark;write code;programming;python code meet;algorithmic challenges programming;program computer code;language specification generate", "pdf_keywords": "natural language speci\ufb01cation;arbitrary natural language;evaluation computer programs;unrestricted natural language;natural language;generate satisfactory python;track program correctness;language speci\ufb01cation generate;python code speci\ufb01cations;generated code python;generate correct programs;program functionality;developed framework generative;generative learning pyext;satisfactory python;evaluation program functionality;satisfactory python code;programs data;language realistic informative;program correctness;framework generative;python;natural language using;program correctness new;generative;code python;computer programs;language using data;programs;framework generative learning"}, "ac5e7f9bbc5d46bebc4ec5616aba9d014a6d237f": {"ta_keywords": "random event occurs;probabilities events;events probability distribution;random event;theory rare events;probability random event;distribution number events;terms probabilities events;probabilities events probability;rare events;occurs given event;probabilities;event occurs given;probability distribution;probability density;events associated processes;event occurs;processes case probability;rare events associated;sine gordon systems;gordon systems;gordon systems characterized;events probability;distribution;function probability random;probability distribution number;probability random;dynamical systems;undergraduate computer science;case probability distribution", "pdf_keywords": ""}, "615b823d1fc9548ce384f1bb4f544445175e8537": {"ta_keywords": "properties pea starch;rheological properties pea;pea starch solution;starch solution;study rheological properties;pea starch;rheological properties;theoretical study rheological;starch;study rheological;rheological;properties pea;pea;properties;present theoretical study;theoretical study;solution;present theoretical;theoretical;study;present", "pdf_keywords": ""}, "556a4a0b5fcda4d9f9fad637f2655aeb1b1a00b2": {"ta_keywords": "translation sensitive paralinguistic;target language duration;language duration power;speech output;output speech;paralinguistic information;features input speech;speech output speech;language independent regression;sensitive paralinguistic information;language duration;source language duration;output speech continuous;target language speakers;input speech output;input speech;paralinguistic information paper;language speakers detect;speech continuous space;digit translation task;speech continuous;target language;language speakers;paralinguistic;sensitive paralinguistic;translate features input;translate features;information target language;detect emphasis;digit translation", "pdf_keywords": ""}, "d6fc0fcf0764065f6e58c57ca850abfdd918504b": {"ta_keywords": "clef 2013 season;scoring model;linear scoring;clef 2013 evaluation;2013 evaluation spring;linear scoring model;outperforms spring models;season season paper;model outperforms spring;outperforms spring;spring model clef;evaluation spring model;season paper analyze;track position contact;spring model outperforms;evaluation spring;season paper;introduce linear scoring;train weights model;error rate training;line contact point;2013 season season;season season;spring spring;spring models;spring spring model;contact point;contact point contact;spring model;line contact", "pdf_keywords": ""}, "7de4a82edf68b69a9c007fe8e840edf4ade1171c": {"ta_keywords": "sulphydryl enzyme ficin;ficin sulphydrase enzyme;enzyme ficin sulphydrase;kinetics ficin arginine;characterization ficin enzymic;ficin enzymic action;enzyme ficin;ficin enzymic;ficin sulphydrase;stability kinetics ficin;kinetics ficin;ficin arginine derivatives;established sulphydryl enzyme;characterization ficin;ficin arginine;sulphydryl enzyme;described characterization ficin;sulphydrase enzyme study;sulphydrase enzyme;enzymic action;enzymic action thoroughly;enzyme;enzyme study;ficin;acetylation free arnino;stability established sulphydryl;arginine derivatives studied;effect acetylation;enzyme study effect;enzymic", "pdf_keywords": ""}, "2cae732250b59f9e2238626d8d7e0064b97de3c9": {"ta_keywords": "image representation wavelet;representation wavelet transform;wavelet transform;representation wavelet;wavelet transform domain;feature signal reconstruction;signal reconstruction;algorithm signal reconstruction;wavelet;image representation;signal reconstruction problem;signal reconstruction paper;stabilized dimensional image;crossing representation image;reconstructing dimensional zero;approach image representation;representation image stabilized;extension image representations;based edge intensity;dimensional zero crossing;zero crossing representation;image stabilized dimensional;image representations;operation based edge;image representations early;edge intensity reduce;edge intensity;reconstruction problem reduces;algorithm reconstructing dimensional;new algorithm reconstructing", "pdf_keywords": ""}, "616c15dd765c36c21efc75c7ed52e5af81c21053": {"ta_keywords": "artificial intelligence ia;approach artificial intelligence;intelligence ia research;artificial intelligence;approach artificial;ia research;intelligence ia;new approach artificial;intelligence;research;artificial;new approach;propose new approach;ia;approach;paper propose new;paper propose;propose new;paper;propose;new", "pdf_keywords": ""}, "bf9b069242f0af129c2aad8430a52454b008c327": {"ta_keywords": "descent stochastic gradient;gradient descent stochastic;stochastic nonconvex optimization;learning rate decay;stochastic gradient langevin;stochastic gradient;broadly stochastic nonconvex;decay nonconvex optimization;stochastic nonconvex;gradient langevin;descent stochastic;convex functions learning;gradient langevin dynamics;gradient descent;rapidly zero learning;zero learning rate;rate decay nonconvex;learning rate tends;stochastic differential;functions learning rate;nonconvex optimization;learning rate;sde linear convergence;nonconvex optimization establish;tuning learning rate;based stochastic differential;zero learning;learning rate learner;learning rate single;using learning rate", "pdf_keywords": "decay constant learning;dependent whitten laplacian;witten laplacian obtaining;laplacian whittaker;whitten laplacian whittaker;whitten laplacian;spectrum witten laplacian;whittle machine learning;learning rate dependent;decreasing function learning;witten laplacian particular;learning rate inversely;witten laplacian;surrogate sgd;constant learning;function learning rate;dependent sde expressed;constant learning rate;laplacian whittaker equation;stochastic;learning rate certain;dependent stochastic;surrogate sgd addition;rate dependent stochastic;laplacian obtaining;expression whitten spectrum;sde expressed terms;dependent stochastic di\ufb00erential;function learning;sde expressed"}, "250f8f71f7cff972a70482229ca9053b356217cd": {"ta_keywords": "bayes method voice;voice activity detection;method voice activity;variational bayes framework;voice activity;online variational bayes;using variational bayes;variational bayes method;variational bayes;method voice;predicting season vdvs;use online variational;activity detection;online variational;activity detection vad;bayes framework;vdvs using variational;using variational;bayes method;season vdvs using;maker decide play;voice;bayes;unsupervised context proposed;idea decision maker;season vdvs;detection vad unsupervised;team based idea;team based;variational", "pdf_keywords": ""}, "3d2ceea5dea234ae9a20f8e1c9e558735757e90e": {"ta_keywords": "recognizers multilingual acoustic;multilingual acoustic models;language dependent phoneme;multilingual acoustic;language corresponding phones;multilingual allophone;speech recognition languages;demonstration multilingual allophone;recognizers multilingual;language dependent recognizers;multilingual allophone propose;dependent recognizers multilingual;languages recognizer achieves;languages recognizer;indigenous languages recognizer;phonemes sounds support;recognition languages world;phoneme distributions;universal phone recognizer;phone recognizer combined;phoneme distributions experiments;speech recognition;phone recognizer;corresponding phones sounds;difference phonemes sounds;recognition languages;phone accuracy improvements;world multilingual models;multilingual models;spoken language independent", "pdf_keywords": "acoustic models multilingual;multilingual acoustic modeling;multilingual acoustic model;models multilingual acoustic;models multilingual phonemes;allophone models multilingual;multilingual acoustic;multilingual phonemes perceptual;multilingual phonemes;multilingual recognition;present multilingual acoustic;language dependent phoneme;universal phone recognition;recognition based phonetic;recognition universal speech;fact multilingual phonemes;multilingual recognition based;multilingual phonemes fall;language independent phone;speech recognition languages;allophone automatic recognition;phone language;independent phone language;based phonetic annotation;phonetic annotation;phone recognition;phone language dependent;propose allophone models;method multilingual recognition;phonemic transcriptions allophone"}, "8234049255a0e03fc745457de456634d1aab214b": {"ta_keywords": "web page classification;page classification entity;page classification;web page structure;page inferred web;inferred web page;extraction web pages;extraction web;web page inferred;structure web page;filtering music web;classification entities;wrapper learning;entity extraction web;structure web;classification entities mentioned;entities mentioned web;web pages;inferred web;web collaborative filtering;diverse classification entities;important wrapper learning;classification entity;classification entity extraction;music web page;wrapper learning important;web page;page structure;classification;paper structure web", "pdf_keywords": ""}, "e11b4750e288785134f042c144f057a11dc0180a": {"ta_keywords": "voting dynamics democratic;voting dynamics;dynamics democratic systems;analyze voting dynamics;elected representatives theoretical;flexible representative democracy;delegation cycles elected;representative democracy;dynamics democratic;democratic systems;representatives theoretical;representative democracy novel;using direct democracy;analyze voting;based quantum state;cycles elected representatives;set elected representatives;representative democracy rd;direct democracy;hybrid representative democracy;quantum computer;delegation cycles;based quantum;quantum computer based;direct democracy ideal;computer based quantum;democratic systems using;rd direct democracy;quantum state;quantum state tomography", "pdf_keywords": "interactive democracy smoothly;delegations votes;dynamic proxies election;interactive democracy;proxy voting;variants proxy voting;delegations votes candidates;voter majority issue;proxies election rules;model interactive democracy;election rules;election rules resolute;party delegations;voter participation;voters representative democracy;present simple voting;outcomes voter participation;proxy voting yield;voters votes issues;democratic processes;party delegations paper;problem delegations votes;resolute democratic processes;algorithm deciding party;democracy smoothly transitions;democracy smoothly;voter;simple voting;democratic processes set;outcomes voter"}, "b8b813111c411ae61881ab9cd25707d9de6444ec": {"ta_keywords": "attention variety tasks;key value attention;multi head attention;scaling search retrieval;generalization attention mechanism;retrieval composes dynamic;value attention backbone;retrieval composes;search retrieval composes;generalization attention;compositional attention;value attention;retrieval easily implemented;mapping search retrieval;key interactions retrieval;head attention variety;attention variety;retrieval query key;computations search retrieval;compositional attention leads;disentangles search retrieval;value attention blocks;hinder generalization attention;search retrieval;retrieval needed multi;search retrieval easily;scaling search;head attention;retrieval;retrieval value", "pdf_keywords": "compositional attention search;generalization compositional attention;machine learning attention;compositional attention;propose compositional attention;attention search;learning attention;attention search retrieval;compositional compositional attention;compositional attention key;learning attention mechanism;compositional attention important;multi head attention;attention combined;attention systems;attention combined form;neural networks generalise;attention;head attention combined;attention systems allows;multi layer perceptrons;attention mechanism based;retrieval operations compositional;attention key;head attention;attention mechanism;attention important fundamental;modern attention systems;attention key principle;layer perceptrons"}, "a469f2ec3ab15f20f06d95aea1839b1263d3385e": {"ta_keywords": "random assignment mechanisms;efficiency envy freeness;assignment mechanisms random;ordinality envy freeness;optimal egalitarian;optimal egalitarian value;efficiency envy;tradeoffs efficiency envy;approximate optimal egalitarian;envy freeness;achievable egalitarian value;truthfulness achievable egalitarian;ordinality envy;envy freeness truthfulness;achievable egalitarian;like ordinality envy;prominent random assignment;random assignment;distributions random assignment;assignment mechanisms;assignment mechanisms approximate;assignment mechanisms agents;random serial dictatorship;different random assignment;egalitarian value;random assignment assignments;serial dictatorship probabilistic;dictatorship probabilistic serial;mechanisms random;results random assignment", "pdf_keywords": "mechanisms additional fairness;allocation indivisible goods;constraint fairness allocation;envy freeness constraint;fairness allocation;constraint fairness;fairness randomized assignment;fairness allocation terms;freeness constraint fairness;regarding fairness randomized;allocation terms egalitarian;randomized assignment mechanisms;fairness randomized;fairness requirements envy;randomized mechanisms assigning;allocation indivisible;agents unequal valuations;imposing envy freeness;indivisible goods agents;classes mechanisms envy;envy free mechanisms;indivisible goods;optimizing egalitarian value;egalitarian value restricted;goods agents unequal;regarding fairness;agents based preference;mechanisms envy free;randomized mechanisms;additional fairness requirements"}, "18f4ec53a4221a97e1482f091f41a23f3d873cf2": {"ta_keywords": "evidence annotations strong;evidence annotations;evidence annotations practice;practice evidence annotations;combine evidence annotations;annotations practice evidence;evidence annotations available;evidence extraction;concept evidence annotations;annotations strong;labels weak supervision;supporting evidence;predictions supporting evidence;supervision task evidence;task evidence extraction;annotations practice;supporting evidence human;annotations available minority;annotations;supervision abundant document;evidence extraction paper;combine evidence;strong semi supervision;annotations available;prediction tasks stakeholders;annotations strong semi;weak supervision task;methods combine evidence;practice evidence;baselines adapted interpretability", "pdf_keywords": "evidence extraction predicted;evidence sequence labeling;extracting evidence news;annotations learning;automatically extracting evidence;evidence extraction;generating evidence label;baselines annotations learning;annotations learning faithfully;detecting propaganda;analyzing sentiment movie;conditioning evidence extraction;tasks analyzing sentiment;text classi\ufb01cation evidence;evidence extraction approaches;extraction predicted label;sequence labeling tasks;detecting propaganda techniques;supplementing predictions evidence;extracting evidence;analyzing sentiment;sentiment movie reviews;sequence labeling;label classify extract;different evidence extraction;evidence news articles;text classi\ufb01cation tasks;2002 detecting propaganda;predicted label classify;propaganda techniques news"}, "c4536a5c7f47bfc48df202ba882002531248f955": {"ta_keywords": "sounds generated electrolarynx;generated electrolarynx based;electrolarynx based statistical;electrolarynx based;change frequency electrolarynx;electrolarynx type speaking;frequency electrolarynx;method control electrolarynx;control electrolarynx actual;control electrolarynx;frequency electrolarynx naturalness;generated electrolarynx;physical electrolarynx;generate excitation sounds;electrolarynx;sounds help laryngectomees;electrolarynx naturalness el;physical physical electrolarynx;mechanical excitation sounds;optrolaryngeal el speech;physical electrolarynx type;electrolarynx naturalness;electrolarynx type;laryngectomees produce optrolaryngeal;electrolarynx actual physical;electrolarynx actual;sounds generated;frequency sounds generated;laryngectomees;excitation sounds", "pdf_keywords": ""}, "f43ae70242aea3dbb80b7c3b5474356e9ee9079b": {"ta_keywords": "natural language represents;force meaning representation;meaning representation;dynamics human;language represents meaning;represents meaning sentence;dynamics human body;complex systems;meaning representation amr;study dynamics human;semantic;represents meaning;theory complex systems;meaning sentence semantic;semantic graph;representation;concepts theory complex;language represents;sentence semantic graph;dynamics;formalism natural language;sentence semantic;concepts;complex systems paper;representation amr;natural language;concepts concepts;concepts theory;representation amr popular;formalism", "pdf_keywords": ""}, "72c9663494827b2e87ad5a65a6ff7e769eb15a57": {"ta_keywords": "visual storytelling automatic;storytelling automatic human;crowdsourced scoring systems;storytelling automatic;crowdsourced scoring;performance crowdsourced scoring;visual storytelling;experiments visual storytelling;crowdsourced;performance crowdsourced;optimizing performance crowdsourced;automatic human evaluation;human evaluation;reward;storytelling;outcome simple visually;rl reward;story makes good;human evaluation demonstrate;reco rl reward;makes good story;reward functions designed;topically coherent story;relevance coherence expressiveness;visually appealing;simple visually appealing;rl reward functions;scoring systems;outcome;story beautiful following", "pdf_keywords": "image visual storytelling;generation visual storytelling;visual storytelling task;image captioning text;amazon mechanical turk;quality stories generated;image captioning train;approach image captioning;image captioning;captioning text;visual storytelling;story plot generation;captioning train;captioning;captioning train language;visual storytelling process;tasks image captioning;stories generated;known image captioning;quality text generation;storytelling aims generating;text generation visual;generating sequence sentences;stories generated models;storytelling task;generated models crowdsourcing;captioning text summarization;visual storytelling aims;text generation;mechanical turk"}, "15bb07d0996ece844de8cae24d3dc15972e6841a": {"ta_keywords": "summarization datasets study;summarization datasets;popular summarization datasets;know summarization datasets;summarization datasets despite;summarization complexity datasets;summarization models popular;summarization models;state art summarization;summarization systems trained;samples popular summarization;summarization complexity;summarization systems;know summarization;popular summarization;web know summarization;summarization difficulty extractive;noise summarization complexity;summarization difficulty;noise summarization;data noise summarization;art summarization models;summarization;art summarization systems;art summarization;degrees summarization;entities degrees summarization;degrees summarization difficulty;key insights datasets;datasets study driven", "pdf_keywords": "summarization datasets study;text summarization noise;summarization datasets;source text summarization;summarization noise;use summarization datasets;text summarization;context text summarization;summarization noise incomplete;text summarization popular;machine translation;examine use summarization;quality references source;machine translation used;summarization;summarization popular;quality references;references source text;use summarization;propose machine translation;bias quality references;summarization popular topic;source text;bias quality;datasets study empirically;positional bias quality;research context text;data quality;metrics equally reliable;reliable categories samples"}, "674833d48a77ef009f751a66988590592dd5d996": {"ta_keywords": "weaker external force;animals action external;population animals action;external forces;external force force;external force;force weaker external;action external force;animals action;behavior population animals;absence external forces;force force weaker;force stronger absence;force force stronger;force weaker;force stronger;population animals;forces;force force;animals;force;weaker external;stronger absence external;behavior population;behavior;action external;absence external;stronger absence;weaker;stronger", "pdf_keywords": ""}, "49ee2270f3265ee27b36e05e130be79e05d5ba29": {"ta_keywords": "explanation based learning;learning texts including;learning texts;learn mechanics;learn mechanics mechanical;problem learning texts;learning problem;learning technique;learning problem solvable;learning;analogical abductive explanation;abductive explanation based;used learn mechanics;based learning ana;problem learning;analogical abductive;text logical theory;textbooks learning;learning ana;analogical;addresses problem learning;learning problem learning;mechanics;based learning;textbooks learning technique;technique analogical abductive;learning textbooks learning;learning technique efficient;pendulum used learn;explanation based", "pdf_keywords": ""}, "68ea2572584068befd441dccf461f3444ff14f4a": {"ta_keywords": "agent cognitive robot;cognitive robot learn;cognitive robot;concept intelligent agents;robot learn skill;robot learn;physical agent cognitive;intelligent agents;skill knowledge interacting;agent cognitive;learn skill knowledge;user interfaces;graphical user interfaces;humans acquire knowledge;improve understanding humans;physical agent;learn skill;concept intelligent;world simulations robot;knowledge interacting;skill knowledge;knowledge interacting users;simulations robot able;learning based concept;based concept intelligent;simulations robot;robot;student learning based;knowledge students;integrate physical agent", "pdf_keywords": ""}, "4cfc7d3c6a61f6db48b1f3c75235592c1609a54f": {"ta_keywords": "transcription interface;computer assisted transcription;scratch transcription interface;automatic transcription quality;assisted transcription;superior automatic transcription;automatic transcription;quality speech transcription;assisted transcription presence;speech transcription;improve human transcription;transcription quality;automatic transcription segment;human transcription quality;transcription quality appropriate;transcription quality insufficient;assisted transcription promises;parts automatic transcription;transcription;transcription reduced costs;speech transcription reduced;transcription presence;iterative scratch transcription;transcription promises high;scratch transcription;human transcription;transcription presence unsplittable;transcription promises;transcription segment;user interface iterative", "pdf_keywords": ""}, "e2ac96254d7e9ec0dde882e3a09797d00f26220f": {"ta_keywords": "collision heavy projectile;projectile collided heavier;collided heavier target;dynamics head collision;head collision heavy;target projectile collided;head collision;projectile collided;disintegration projectile ejection;projectile ejection;projectile ejection target;heavy projectile;disintegration projectile;collided heavier;collision heavy;lighter target projectile;target projectile;leading disintegration projectile;projectile lighter target;collision;projectile;heavy projectile lighter;heavier target;ejection target;projectile lighter;collided;lighter target;dynamics head;heavier target leading;study dynamics head", "pdf_keywords": ""}, "757acf616a38422c7186952e1075a28fed1a07c0": {"ta_keywords": "analgesic effects xyl;xylazine effects inflexyl;xyl veterinary anesthetic;anesthetic analgesic effects;cells anesthetic analgesic;inhibited xyl xylazole;xyl related inhibition;cells inhibited xyl;inhibited xyl;analgesic effects;effects inflexyl rat;anesthetic analgesic;xylazine effects;effects xyl regulation;inhibition cgmp signaling;cgmp nerve cells;pathway inhibition cgmp;xyl regulation;xyl xylazole xyl;xylazole xyl;nerve cells anesthetic;xylazole xyl veterinary;cgmp nerve;xyl xylazole;cgmp signaling pathway;inhibition cgmp;similar xylazine effects;effects xyl related;functionally similar xylazine;increase analgesic effects", "pdf_keywords": ""}, "fc912e9af47bf10428396b687b2bfb1e5832fcb1": {"ta_keywords": "connectionist temporal classification;speech recognition asr;recognition asr based;recognition asr;automatic speech recognition;speech recognition;automatic speech;asr based connectionist;stochastic depth training;efficient auxiliary loss;temporal classification ctc;loss function automatic;auto regressive decoder;function automatic speech;predicting outcome team;ws corpus;auxiliary loss;based connectionist temporal;auxiliary loss function;rate aishell corpus;rate ws corpus;regressive decoder based;decoder;loss stochastic depth;classification ctc;football matches keywords;temporal classification;regressive decoder;decoder based;depth training", "pdf_keywords": "model automatic speech;speech recognition asr;automatic speech recognition;speech recognition machine;network automatic speech;speech recognition;automatic speech;recurrent neural networks;rate aishell corpus;speech recognition based;recognition asr;stochastic depth training;respectively automatic speech;recognition based recurrent;aishell corpus;loss stochastic depth;learning stochastic depth;aishell corpus respectively;neural networks;ctc loss stochastic;rate wsj corpus;based recurrent neural;recurrent neural;learning stochastic;recognition asr important;recognition machine;machine learning stochastic;corpus respectively automatic;recognition machine learning;autoregressive model automatic"}, "f3132572bb3870dbe99b2d1c01ce17fa38783a2f": {"ta_keywords": "robust speech processing;robust speech;uncertainty uncertainty propagation;uncertainty propagation;speech processing;observation uncertainty uncertainty;uncertainty propagation shown;using observation uncertainty;speech processing using;observation uncertainty;uncertainty uncertainty;robust;uncertainty;outperform state art;speech;state art methods;propagation shown outperform;processing using observation;outperform state;shown outperform state;propagation;outperform;using observation;state art;shown outperform;processing;processing using;state;propagation shown;observation", "pdf_keywords": ""}, "213e471bacff5c0852943988fcb955797f1e591f": {"ta_keywords": "automatic machine translation;machine translation increasingly;existing reference translations;reference translations;machine translation;reference translations counteracts;translations counteracts bias;paraphrasing task linguists;automated evaluation reporting;references exhibit poor;diversity develop paraphrasing;german translation averaged;translation increasingly;translations;translation averaged;translation increasingly called;translation averaged augmented;finding typical references;modern evaluation metrics;translations counteracts;value automated evaluation;automated evaluation;develop paraphrasing;collect references compare;evaluation metrics;evaluation metrics look;typical references exhibit;linguists perform existing;references compare;develop paraphrasing task", "pdf_keywords": "machine translation paraphrasing;paraphrasing machine translation;translation paraphrasing machine;paraphrased reference translations;references paraphrasing automatic;paraphras ing translations;machine translation evaluation;translation paraphrasing;evaluation machine translation;automatic machine translation;natural references paraphrasing;paraphrasing automatic;translation multiple paraphrases;references paraphrasing;machine translation;machine translation systems;paraphrasing automatic machine;paraphrasing machine;systems paraphras ing;paraphrasing task linguists;human reference translations;systems based paraphrased;develop paraphrasing;translation evaluation languages;machine translation multiple;reference translations generate;develop paraphrasing task;translation evaluation;zero shot paraphrasing;paraphrase translated sentences"}, "77568c594470f9aa029f92774e2c12ab0451d9bb": {"ta_keywords": "mle language models;topics news reviews;robust language models;distributionally robust language;training distributionally robust;mle training distributionally;mle language;language models trained;topic conditional value;reduction mle language;topics news;topic conditional;perplexity reduction mle;reviews training text;news tested reviews;reviews news tested;called topic conditional;language models;range topics news;mixture topics sufficient;mixture topics;news tested;likelihood mle training;distributionally robust optimization;news reviews;topics sufficient overlap;case mixture topics;topic cva obtains;robust language;risk topic cva", "pdf_keywords": "machine translation prior;topic modeling;topic modeling domain;normalize topics;properly normalize topics;connect topic modeling;language models;distributionally robust optimization;normalize topics particular;topic cvar conditional;domain adaptation;machine translation;language models degrade;modeling domain adaptation;billion word benchmark;topic based uncertainty;topics effective way;performance language models;newswire sentences;called topic cvar;model topic based;new distributionally robust;mixture topics;domain adaptation work;word benchmark;topic cvar;90 newswire sentences;translation prior;mixture topics suf\ufb01cient;distributionally robust"}, "89c64fd60ca58f4753a818cd0923f5041b51a807": {"ta_keywords": "multi connectivity sensor;sensor relay network;mobile sensor relay;relay network sensor;sensor relay;connectivity sensor;network sensor;multi connectivity;connectivity sensor placed;network sensor sink;multihop network decision;network sensor placed;place relay based;relay network;connected wireless relay;network decision;wireless relay network;connected multihop network;relay based link;network decision formulated;relay based;wireless relay;place relay;achieving connected multihop;deployment mobile sensor;relays paper describes;multihop network;sensor sink interconnection;disadvantages multi connectivity;relay", "pdf_keywords": ""}, "612d577534dbbf546405d4036d912666523a8164": {"ta_keywords": "prolog learnable;shown prolog learnable;prolog learnable paper;probabilistic logic prolog;learned syntactic restrictions;tractably learned syntactic;logic tractably learned;learned syntactic;probabilistic logic;logic probabilistic;logic probabilistic logic;learnability order representations;description logics;logic tractably;prolog shown powerful;syntactic restrictions exist;syntactic restrictions explored;prolog;logic prolog;polynomial learnability;order logic probabilistic;analyses shown prolog;syntactic restrictions;learnability;learning concepts expressed;learnability order;polynomial learnability order;predicate calculus;description logics subsets;results polynomial learnability", "pdf_keywords": ""}, "0d516b476559485e04290e859ca59101c0a91ae1": {"ta_keywords": "markov decision process;obstacles millimeter wave;observable markov decision;markov decision;optimal threshold policy;stationary threshold policy;blockage obstacles millimeter;decision process pomdp;obstacles derived optimal;partially observable markov;threshold policy maps;blockage obstacles;threshold policy;locally blockage obstacles;pomdp framework;uncertainty d2d link;derived optimal threshold;obstacles millimeter;observable markov;modeling finite horizon;dynamic obstacles derived;threshold policy counts;process pomdp framework;pomdp framework locally;optimal threshold;dynamic obstacles;seek learn uncertainty;average exploration time;learn uncertainty d2d;exploration time", "pdf_keywords": "relay exploration;relay exploration switching;tracking obstacles service;relay minimizes;cameras machine learning;tracking obstacles;select relay minimizes;relay minimizes packet;exploration successive ack;decide select relay;exploration switching;obstacles service;selecting relay;additional exploration time;relay;machine learning;obstacles service region;exploration switching average;minimizes packet;threshold policy maps;learning problem;problem machine learning;tracking;selecting selecting relay;select relay;set actions vision;authors tracking obstacles;considering dynamic obstacles;frequency relay exploration;stationary threshold policy"}, "99fe5475ab28fa7ad4bce51d7b294b3f40caad4d": {"ta_keywords": "extract beauty bouncer;beauty bouncer;extract beauty;bouncer;showing extract beauty;beauty;extract;video showing extract;video;showing extract;present video;video showing;showing;present video showing;present", "pdf_keywords": ""}, "31392ad8722d9c66181b621936e2013199e02edc": {"ta_keywords": "pretrained models nlp;lm machines learn;machines learn;pretraining learn data;machines learn large;tuning nl tasks;language understanding large;learn data;pretrained billions words;improvements language understanding;language models;language model knowledge;models nlp;learning;language models like;learning noisy;models nlp currently;large pretrained models;knowledge skills transformer;pretrained models;learn data need;language model;pretraining learn;ability encode linguistic;language understanding;model knowledge probing;learn large scale;dominated language models;evaluation classifier probing;learning noisy data", "pdf_keywords": "linguistic features learnable;like speech coreference;speech coreference encoded;features like speech;speech coreference;encoder model learn;features linguistic;linguistic features;coreference encoded miniberta;determine linguistic features;coreference encoded;grammatical features linguistic;nl task performance;features learnable;features linguistic phenomena;data nl task;words data nl;features learnable 100;words data;nl task;grammatical features;semantic features;probing tasks based;semantic features like;investigate pretraining data;probing tasks;learnable 100 words;like speech;model learn;models learned"}, "6c82727731955a2332a0cc38ec56b35a971061eb": {"ta_keywords": "translation toolkit ray;machine translation toolkit;entanglement remote nodes;translation toolkit;neural machine translation;describes machine translation;creation entanglement remote;entanglement remote;toolkit ray neural;ray neural machine;toolkits fast replicable;machine translation paper;machine translation;algorithm creation entanglement;source software toolkits;ray neural;toolkit ray;entanglement;open source algorithm;remote nodes;creation entanglement;emitting neural machine;source software;neural machine;replicable execution modular;modular code;software toolkits;information machine machine;remote nodes network;software toolkits fast", "pdf_keywords": "machine translation toolkit;machine translation toolkits;neural machine translation;machine translation speech;tasked machine translation;machine translation experiment;machine translation;translation toolkits;machine translation perform;translation toolkit;tasks machine translation;translation speech recognition;translation toolkits used;machine translation parsing;standard machine translation;translation toolkit toolkit;set machine translation;translation speech;machine translation addition;translation experiment speech;using machine translation;translation parsing;translation experiment;unsupervised learning spoken;translation parsing paper;learning spoken;presents deep learning;deep learning;translation perform;experiment speech recognition"}, "7e406537f52528527d10872d1807ad974599b13a": {"ta_keywords": "particles binary probability;distribution number particles;particles binary;number particles binary;probability finding particle;binary probability distribution;number particles;binary probability;probability distribution number;particle;distribution given xmath22;particle moving potential;finding particle;finding particle organization;particle organization;particles;particle organization seventh;algorithm compute probability;study dynamics particle;compute probability distribution;dynamics particle;distribution number;distribution;probability distribution;particle moving;dynamics particle moving;probability distribution given;distribution given;compute probability;potential energy", "pdf_keywords": ""}, "5babe5334c6867db13fa7e6943f64059c7cba6ce": {"ta_keywords": "information representation language;word based information;information representation;based information representation;biological social systems;representation language;representation language used;word based;biological social;social systems;range biological social;based information;biological;language used represent;wide range biological;representation;systems;information;present word based;language;range biological;used represent;social;word;language used;present word;represent wide range;represent;used represent wide;based", "pdf_keywords": ""}, "c0cce8955bf10b21753161ffaa1978a7c8b78a16": {"ta_keywords": "quantum computation noisy;enhanced speech quiet;speech quiet noisy;non audible murmur;silent speech communication;accuracy listenability enhanced;listenability enhanced speech;markovian quantum computation;whispered voice detected;non markovian quantum;conversion accuracy listenability;microphone;medium silent speech;speech recognition;quantum computation;noises performance proposed;body conductive microphone;accuracy listenability;speech recognition estimate;nam microphone;speech quiet;implement noise dependent;quiet noisy environments;audible murmur;speech communication;called nam microphone;microphone called nam;noise dependent vector;input speech recognition;microphone called", "pdf_keywords": ""}, "6027ef3b4e5585b45db0b9d333956425d3972351": {"ta_keywords": "crowdsourced reasoning;crowdsourced crowdsourced reasoning;reasoning crowdsourced;loop reasoning crowdsourced;crowdsourced reasoning benchmarks;reasoning crowdsourced crowdsourced;open crowdsourcing;open ended reasoning;commonsense reasoning opencsr;open crowdsourcing used;benchmarks open crowdsourcing;commonsense reasoning;crowdsourced;open ended commonsense;commonsense reasoning research;task answering commonsense;problem commonsense reasoning;crowdsourced crowdsourced;reasoning benchmarks open;open commonsense decision;commonsense knowledge answer;commonsense knowledge;crowdsourcing;answering commonsense;ended commonsense reasoning;open commonsense;use commonsense knowledge;hop reasoning knowledge;reasoning task answering;knowledge answer multiple", "pdf_keywords": "commonsense reasoning opencsr;reasoning virtual knowledge;embeddings facts;commonsense reasoning;embeddings facts symbolic;neural embeddings facts;common sense reasoning;reasoning traversing hypergraph;reasoning traversing;ended commonsense reasoning;domain question answering;virtual knowledge bases;question answering;disambiguation method outperforms;knowledge bases;task natural language;reasoning answers generated;reasoning virtual;disambiguation method;proposed disambiguation method;fact differentiable reasoning;open ended commonsense;methods proposed disambiguation;sense reasoning answers;differentiable reasoning virtual;reasoning answers;automated inference;reasoning method opencsr;model reasoning traversing;embeddings"}, "a38e0f993e4805ba8a9beae4c275c91ffcec01df": {"ta_keywords": "models program synthesis;program synthesis;programs natural language;program synthesis general;language models program;programming languages natural;large language models;generation large language;synthesize short python;natural language feedback;language models;python programs natural;language feedback;short python programs;programming languages;programs difficult generate;languages natural language;python programs;natural language descriptions;language descriptions evaluate;purpose programming languages;general purpose programming;short python;large language;language descriptions;programming;models synthesize short;prediction benchmarks designed;fine tuning predict;prediction benchmarks", "pdf_keywords": "neural program induction;synthesis python programs;automatic program synthesis;deep language;program synthesis;programming languages probabilistic;program synthesis challenging;learning models synthesize;deep language model;synthesis python;language model trained;presents deep language;languages probabilistic language;languages probabilistic;neural program;present program synthesis;model neural program;probabilistic language;program induction;learn algorithms input;program synthesis method;language models longstanding;program induction methods;programming languages;probabilistic language models;language models;large language model;synthesis challenging tasks;learn algorithms;python programs"}, "4f4da6fdb9496b0295764b2db11381dd390de02d": {"ta_keywords": "cost models transcription;correction speech transcripts;cost sensitive correction;speech transcripts based;recognized speech transcripts;sensitive correction speech;speech transcripts;manual correction automatically;transcripts based;speech transcripts paper;transcription;sensitive manual correction;automatically recognized speech;correction speech;transcripts;models transcription;cost sensitive manual;transcripts paper;updating framework cost;models transcription process;analysis cost sensitive;manual correction;transcripts paper present;previous cost sensitive;sensitive correction;transcripts based fly;transcription process does;correction automatically recognized;transcription process;cost sensitive approaches", "pdf_keywords": ""}, "c581686edbd7227e9eb4a0841cce16728ca27369": {"ta_keywords": "machine comprehension systems;comprehension systems;comprehension systems work;comprehension reasoning tasks;machine comprehension;evaluating machine comprehension;reasoning tasks;sense procedural knowledge;natural language processing;comprehension reasoning;reasoning tasks require;instructional recipes;procedural knowledge;language processing preliminary;language processing;sense procedural;natural language;making sense procedural;analysis natural language;joint understanding images;kk instructional recipes;instructional recipes multiple;semiconductors;indicate recipeq;procedural knowledge propose;comprehension;concept cook;set comprehension reasoning;study electronic structure;procedural", "pdf_keywords": "multimodal machine comprehension;machine comprehension tasks;automatic story comprehension;comprehension tasks;based machine comprehension;comprehension tasks recipeq;machine comprehension;machine comprehension provide;answers automatic story;story comprehension;comprehension important task;multimodal aspects;description tasks;answers automatic;investigate multimodal aspects;story comprehension important;multimodal;multimodal machine;comprehension provide detailed;investigate multimodal;comprehension provide;description tasks discuss;detailed description tasks;multimodal aspects computer;comprehension;methods multimodal machine;question answers;question answers problem;paper investigate multimodal;automatic story"}, "ad48174ccbff6259a7d3cb0d0985e5aefa314b84": {"ta_keywords": "level machine translation;modeling machine translation;machine translation;machine translation translation;translation translation quality;robustness morphological generalization;translation quality;subword systems;subword systems virtually;domain robustness morphological;character level modeling;robustness morphological;character level systems;comparable subword systems;language processing character;subword based counterparts;translation quality does;level natural language;character level machine;morphological generalization;processing character level;language processing;morphological generalization understood;character level natural;subword based;character level mt;systems comparable subword;processing character;natural language processing;translation translation", "pdf_keywords": "speech recognition character;recognition character embeddings;character level neural;character embeddings processed;character embeddings;speech recognition;recognition character;character language level;character language;automatic speech;datasets character level;automatic speech recognition;level machine translation;decoding strategies character;character level machine;datasets character;present character language;level neural mt;approach automatic speech;machine translation important;large datasets character;different decoding strategies;decoding;decoding strategies;machine translation;character level context;language level;neural mt;level neural;language level approach"}, "1e3e2b03e28f48bb4d48154992cd6b62969c643e": {"ta_keywords": "protein sequence database;proteins;protein sequence;value protein sequence;characterization proteins;protein;proteomic;proteomic projects characterization;proteomic projects;projects characterization proteins;study impact proteomic;impact proteomic;value protein;proteins contains;impact proteomic projects;particles;databases fluid dynamics;characterization proteins contains;swiss prot curated;particles paper study;proteins contains 80;annotated sequence entries;model state potts;prot curated;quantum mechanics used;sequence database strives;quantum mechanics;dynamics videos swiss;000 annotated sequence;fluid dynamics videos", "pdf_keywords": ""}, "7b51209e7d9cbedc18b6ab202e6fcdabaebbb088": {"ta_keywords": "automatic learning topological;learning topological structure;learning topological;topological structure human;continuous space discriminative;space discriminative language;structure human brain;human brain discriminative;brain discriminative language;space discriminative;continuous space model;modeling structured classification;structured classification;feature representation;discriminative language modeling;brain discriminative;human brain;automatic learning;topological structure;parameterized neural;neural;model continuous space;product feature representation;discriminative language;language modeling;structure human;function parameterized neural;classification;neural network paper;structured classification problem", "pdf_keywords": ""}, "795aca47df94300fa6bfd464e6873aef56c7f3ae": {"ta_keywords": "babelnet semantic network;extraction synsets semantic;multilingual babelnet semantic;babelnet semantic;synsets semantic relations;semantic resources wordnet;lexical semantic resources;synsets semantic;lexical resources providing;multilingual babelnet;large lexical semantic;lexical resources;resources wordnet wiktionary;babelnet extract;semantic relations semantic;wordnet;addressing recurrent extraction;semantic resources;semantic relations;effective extraction synsets;resources wordnet;recurrent extraction tasks;main lexical resources;interlinks main lexical;babelnet web resource;babelnet web;relations semantic relations;present babelnet extract;natural language processing;extraction synsets", "pdf_keywords": ""}, "4cd66273298128dfb5be290e891870085ecfc455": {"ta_keywords": "speech recognition;speech recognition asr;alignments hybrid architecture;automatic speech recognition;dnn hmm systems;automatic speech;performance natural language;automatic recognition attention;recognition attention based;end alignments alignments;end automatic speech;language processing;language processing systems;attention based recognition;alignments alignments hybrid;natural language processing;joint decoding algorithm;recognition asr;connectionist temporal classification;benchmarks languages end;recognition connectionist temporal;architectures automatic recognition;joint decoding;proposes joint decoding;end alignments;hmm systems;based recognition connectionist;recognition attention;utilizes advantages decoding;end end alignments", "pdf_keywords": ""}, "7099d5a4b2d4ed47905071fc23aff08580401e42": {"ta_keywords": "echo state networks;decoder randomizing encoders;representations acoustic inputs;speech recognition;training asr models;randomizing encoders learning;encoders learning proper;encoders learning;randomizing encoders;decoder fully randomized;automatic speech recognition;decoder randomizing;performance decoder randomizing;representations acoustic;recognition based echo;automatic speech;encoders;inputs vital speech;proper representations acoustic;speech recognition based;decoders;new class decoders;acoustic inputs;esn based models;asr models;decoders used improve;vital speech recognition;class decoders;training asr;acoustic inputs vital", "pdf_keywords": "deep echo state;rnn transducer speech;networks rnn transducer;speech recognition echo;performance deep recurrent;recurrent neural networks;neural networks rnn;neural network recurrent;recurrent neural network;architecture acoustic encoders;networks rnn;neural reservoir speech;layer automatic speech;acoustic encoders based;transducer speech recognition;network recurrent neural;reservoir speech feature;acoustic encoders;deep recurrent neural;speech recognition;deep echo;recognition echo state;state network recurrent;network recurrent;rnn transducer;network recurrent matrix;recognition echo;speech feature extraction;automatic speech recognition;reservoir speech"}, "af460a6b3ecaddd4015b34255564c366ecfef802": {"ta_keywords": "teach computer ethics;computer ethics science;ethics science fiction;computer ethics;moral imagination;moral imagination paper;ethics science;capacity moral imagination;science fiction offers;teach computer;fiction offers students;imagination paper teach;science fiction;paper teach computer;moral;explore capacity moral;ethics;fiction;imagination paper;students way explore;capacity moral;fiction offers;paper teach;teach;imagination;students way;students;computer;science;offers students way", "pdf_keywords": ""}, "5861dbfcb253ca02067dd182d42b7d567433c834": {"ta_keywords": "frontdoor estimators;frontdoor estimators backdoor;backdoor frontdoor estimators;estimators backdoor;estimators backdoor formula;confounders observed frontdoor;applies confounders observed;confounders observed;estimators demonstrate;estimating effectiveness;estimated empirically derive;estimator leverages;estimators;optimal estimator leverages;statistical properties estimators;method estimating effectiveness;variance estimators demonstrate;observed frontdoor formula;demonstrate estimator dominate;estimators demonstrate estimator;formula applies confounders;estimator leverages observed;confounders;estimated empirically;causal effect calculus;applies confounders;frontdoor formula applies;properties estimators;estimator dominate;estimating effectiveness given", "pdf_keywords": "frontdoor estimators;backdoor frontdoor estimators;frontdoor estimator outperforms;frontdoor estimator;estimator backdoor frontdoor;demonstrate frontdoor estimator;estimator outperforms backdoor;estimator backdoor;combined estimator backdoor;dataset demonstrate frontdoor;effects overidenti\ufb01ed models;overidenti\ufb01ed models propose;overidenti\ufb01ed models;datasets observed confounders;frontdoor estimators current;outperforms backdoor counterpart;outperforms backdoor;confounders observed mediators;job training dataset;observed confounders;observed confounders observed;backdoor counterpart;backdoor frontdoor;estimating treatment effects;job training;confounders observed;estimating treatment;treatment effects overidenti\ufb01ed;frontdoor;bias present procedure"}, "0f655f0e1937ad19b038952e2df69e30d447aac8": {"ta_keywords": "missing data sequential;missingness patterns temporally;swimmer missing data;clinical time series;classification diagnoses;bidimensional swimmer missing;missingness patterns;swimmer missing;classification diagnoses given;recurrent neural networks;missing data;recurrent neural;series recurrent neural;binary indicators missingness;time series recurrent;imputation;indicators missingness;multilabel classification diagnoses;missingness;missing data round;temporally discretized sequences;data sequential;imputation achieve;semiflexible bidimensional swimmer;patterns temporally discretized;data sequential inputs;handled imputation;cope missing data;typically handled imputation;multivariate time series", "pdf_keywords": ""}, "4857e0e3d720b87b4523a6435cc166bcb7ae328a": {"ta_keywords": "condensate confined harmonic;bose einstein condensate;confined harmonic trap;einstein condensate confined;dynamics bose;dynamics bose einstein;investigation dynamics bose;harmonic trap;confined harmonic;condensate confined;einstein condensate;bose einstein;harmonic;bose;condensate;trap;theoretical investigation dynamics;confined;dynamics;theoretical investigation;results theoretical investigation;investigation dynamics;results theoretical;einstein;theoretical;report results theoretical;investigation;results;report results;report", "pdf_keywords": ""}, "8ff620f704a4151fd7abba1db792463fbd32bfe5": {"ta_keywords": "abstractive summarizer train;summarizer train;summarization datasets expensive;summarizer train train;datasets collecting summarization;novel abstractive summarizer;collecting summarization datasets;summarization task compressing;summarization datasets;summarizing long;abstractive summarizer;collecting summarization;abstractive summarization task;scores abstractive summarization;setting summarizing long;salient sentences source;identifying salient sentences;summarizer;abstractive summarization methods;abstractive summarization;salient sentences;setting summarizing;modern abstractive summarization;summarizing long legal;document retaining salient;document summary;trained long documents;documents identifying salient;document summary pairs;summarization task", "pdf_keywords": "summarizing long legal;abstractive summarizer trained;summarizer trained;pretrained abstractive summarizer;summarizer trained related;summarizing long;automatically summarizing long;abstractive summarizer;approach automatically summarizing;abstractive summarization;automatically summarizing;summarizing long domain;summarizer;abstractive summarization account;structure abstractive summarization;summarization;summarizing;document summary;long legal briefs;available document summary;summarization account;regulatory detention court;setting summarizing;setting summarizing long;legal briefs;summarization account data;adversary child custody;repetitive boilerplate text;proceedings implicate;document summary pairs"}, "baf34ac4080a365a7cec30b6877fa1a018eb31cf": {"ta_keywords": "answering improved retrieval;multi answer retrieval;answer retrieval explored;answer retrieval;passage retrieval model;joint passage retrieval;approach retrieval passages;passage retrieval;retrieval explored;downstream question answering;improved retrieval;retrieval passages based;retrieval passages;approach retrieval;new approach retrieval;retrieval;retrieval explored problem;answer generation models;retrieval model;question answering;improved retrieval enables;multi answer datasets;larger answer generation;answer generation;retrieval model focusing;question answering improved;answer datasets;autoregressive reranker algorithm;algorithm selects passage;answer datasets propose", "pdf_keywords": "answer retrieval;downstream question answering;answer retrieval model;passage retrieval;train answer generation;hop passage retrieval;answer generation;answer generation model;passage retrieval passage;question answering;generate sequence answers;art answer retrieval;retrieval passage;reranking propose autoregressive;retrieval;reranking used using;reranking;retrieval model;retrieved passages;answer destination;generation passage references;containing answer destination;reranking used;retrieval passage containing;training dynamic oracle;reranking propose;true reranking used;retrieval model based;sequence answers separated;sequence answers"}, "f4465442a9b850a2c5b71a63fff0d24396b15f2c": {"ta_keywords": "dynamics water fountain;fountain presence gravity;water fountain fountain;water fountain;fountain fountain presence;fountain fountain;fountain;fountain presence;gravity field;dynamics water;study dynamics water;gravity;presence gravity field;presence gravity;dynamics;study dynamics;water;pilot study dynamics;field;presence;paper present results;paper;study;pilot study;results pilot study;paper present;present results;results;pilot;present", "pdf_keywords": ""}, "7fb1262d4484732c8f7295fa5fb5e6ed6eabb6a0": {"ta_keywords": "energy market simulations;energy market models;optimized energy markets;energy markets detailed;energy market;energy markets;performing energy market;electricity markets;representation energy market;market simulation;reserve electricity markets;market simulation used;market simulations;market simulations optimized;paper market simulation;simulations optimized energy;energy reserve electricity;energy exchange;markets detailed transmission;energy exchange rate;market models;assumption energy exchange;market models widely;network simulation;transmission representation energy;reserve electricity;dynamics market;market conditions dynamics;conditions dynamics market;dynamics market paper", "pdf_keywords": ""}, "9f208842f70503e8b71fd4c34ba682dcd0ea4788": {"ta_keywords": "incentives principal agent;learns agents decision;principal agent problems;incentives principal;algorithm learns agents;adaptively design incentives;simultaneously designing incentives;design incentives principal;agents decision making;agents decision;agent problems principal;principal agent;designing incentives change;designing incentives;principal objective maximize;response nash equilibrium;control theoretic optimization;incentives change response;learns agents;nash equilibrium employ;nash equilibrium;update rules principal;incentives;incentives change;algorithm learns;multiple agents consider;multiple agents;agent problems;principal employ algorithm;principal objective", "pdf_keywords": "adaptive incentive design;algorithm adaptive incentive;adaptive incentive;learning incentive design;formulate adaptive incentive;learning incentive;utility learning incentive;preferences optimizes incentives;incentive design;incentive design problems;optimizes incentives;incentive design problem;learning agents decision;optimizes incentives o\ufb00ered;learns agents preferences;incentive design application;consider utility learning;incentive;incentives o\ufb00ered agents;utility learning;learning agents;agents preferences optimizes;incentives;algorithm learning agents;updating incentives;process updating incentives;agents decision making;utility learning approach;general utility learning;games feasibility problem"}, "f136a0fdc2065485c83396ae41d431395de51af4": {"ta_keywords": "method recruiting reviewers;recruiting reviewers;reviewers large conferences;recruiting reviewers population;reviewer recruiting;reviewer recruiting focus;enhancement reviewer pool;qualified reviewers;reviewer pool;qualified reviewers large;conference peer review;reviewer pool results;problem reviewer recruiting;scarcity qualified reviewers;reviewers;reviewers population;machine learning conference;reviewers large;enhancement reviewer;reviewers population typically;identifies best submissions;peer review;reviewing;method recruiting;reviewer;combination recruiting;large conferences;combination recruiting guiding;pool results reviews;consider problem reviewer", "pdf_keywords": "curated reviewer experimental;experimental reviewer expertise;recruiting reviewers;reviewer experimental reviewers;senior curated reviewer;procedure recruiting reviewers;recruiting reviewers population;reviewer expertise senior;evaluation peer review;experimental reviewers;peer review process;approach selecting reviewers;selecting reviewers;curated reviewer;peer review;experimental reviewers active;reviewer expertise;review auxiliary peer;auxiliary peer review;selecting reviewers based;reviewer experimental;reviewers active senior;peer review approach;reviewers;reviewers population;experimental reviewer;new peer review;reviewers based;reviewers population typically;reviewers active"}, "b24e2c3983c3207b1c7124c48d691cf459a3197b": {"ta_keywords": "bayesian speech language;bayesian speech;rules bayesian speech;bayesian inferences based;approximate bayesian inferences;bayesian inference powerful;bayesian inference;bayesian inferences;models approximate bayesian;processing speech;approximate bayesian;speech language processing;speech processed;process speech processed;processing speech language;hidden markov models;speech recognition;process speech;bayesian;bsph process speech;markov models gaussian;gaussian mixture models;markov models;topic models approximate;signal processing speech;speech processed means;models gaussian mixture;rules bayesian;speech language;means speech recognition", "pdf_keywords": ""}, "0c0d9ecde0efead75e15353ac6c179c4fc22bdda": {"ta_keywords": "equilibria continuous games;differential nash equilibria;local nash equilibria;degeneracies continuous games;nash equilibria continuous;nash equilibria structurally;nash equilibria constitute;equilibria structurally stable;equilibria nonlinear differential;local equilibrium states;nonconvex strategy spaces;equilibria nonlinear;constitute local equilibrium;local equilibrium;guaranteeing differential nash;equilibria constitute local;states equilibria nonlinear;continuous games infinite;nash equilibria;equilibrium states equilibria;equilibria structurally;continuous games;equilibria continuous;differential nash;ensuring local nash;game theoretic framework;equilibrium states;continuous games provide;states equilibria;games infinite finite", "pdf_keywords": "equilibria convex game;game formulation;equilibria augmented game;convex game;player strategy spaces;players strategy spaces;continuous games \ufb01nite;differential nash equilibria;convex game paper;continuous games;strategy spaces described;present game formulation;game formulation person;strategy spaces;arise continuous games;behavior competitive environments;games \ufb01nite dimensional;dimensional strategy spaces;whirlpool continuous games;equilibria augmented;competitive environments;nash equilibria;strategy spaces present;characterization equilibria augmented;agent behavior competitive;objectives equilibria;notion differential nash;competitive environments paper;nash equilibria represented;nash equilibria consider"}, "e8b026b36d8be73ed428f7e4e55c26b27c34a544": {"ta_keywords": "bifunctional electrochemical sensor;simultaneous determination electroactive;electroactive analytes presented;electroactive analytes;determination electroactive;electrochemical sensor;non electroactive analytes;electrochemical sensor simultaneous;determination electroactive non;bifunctional electrochemical;electrochemical;electroactive non electroactive;electroactive;sensor simultaneous determination;electroactive non;non electroactive;analytes presented;analytes;simultaneous determination;sensor simultaneous;sensor;bifunctional;determination;simultaneous;non;presented", "pdf_keywords": ""}, "f20d7185c47ce55cdcd9b839ef6fce595baba029": {"ta_keywords": "motion human subject;simulation motion human;speed light;simulation motion;computer simulation motion;motion human;body moves direction;motion;change speed light;force acting body;body moves;force acts body;body moves plane;moves space;body force acting;moves space example;motion governed;force acting;subject case motion;direction moves space;acting body force;distance body body;distance body;body force;speed light case;force acts;force;case motion;motion governed action;body xmath22 force", "pdf_keywords": ""}, "3ebed41fa35e5902b692a3e380c7c9a035c04426": {"ta_keywords": "impacts artificial intelligence;artificial intelligence;artificial intelligence paper;dynamics human decision;engage ethical questions;ethical questions surround;use science fiction;students engage ethical;ethical questions;science fiction appropriate;ethical;intelligence paper study;human decision making;effect artificial neural;effects artificial neural;intelligence paper;artificial neural;science fiction;machines modern military;human decision;artificial neural systems;mechanized workforce ethical;engage ethical;workforce ethical questions;machines;neural networks artificial;workforce ethical;decision making effect;decision making;real neural", "pdf_keywords": ""}, "6b02fe6e0f6b2120a08e098513511e15a05f9073": {"ta_keywords": "detecting dataset shift;dataset shift identifying;dataset shifts explore;classifiers dimensionality reduction;classifiers;dataset shifts;methods detecting dataset;classifiers dimensionality;detecting dataset;machine learning;quantifying shift malignancy;trained classifiers dimensionality;demonstrate domain discriminating;dataset shift;pre trained classifiers;domain discriminating approaches;shifts qualitatively determining;helpful characterizing shifts;trained classifiers;best machine learning;characterizing shifts qualitatively;shift quantifying;characterizing shifts;machine learning ml;learning ml systems;domain discriminating;investigating methods detecting;learning ml;quantifying shift;ml systems", "pdf_keywords": "shift deep learning;learning distribution shift;machine learning distribution;autoencoder based kolmogorov;distribution shift deep;machine learning routinely;leveraging predictions drive;distribution predict label;leveraging predictions;new shift detection;shift detection;systems leveraging predictions;previous shift detection;shift detection methods;learning distribution;present autoencoder based;discriminating classi\ufb01er based;domain discriminating classi\ufb01er;kolmogorov distribution predict;autoencoder based;predict label;predict label labels;domain discriminating;shift deep;distribution predict;shift detection method;learning machine learning;modern machine learning;performance domain discriminating;autoencoder"}, "3483d04a89dd69afd7b1393eadd8e8e4c5376d59": {"ta_keywords": "unsupervised conceptual clustering;conceptual clustering algorithm;conceptual clustering;clustering technique;monolithic clustering technique;monolithic clustering;clustering technique improve;clustering;clustering algorithm presented;improve accuracy lidless;clustering algorithm;accuracy lidless lidless;use monolithic clustering;accuracy lidless;lidless lidless fingering;lidless fingering;algorithm presented fluid;unsupervised conceptual;new unsupervised conceptual;lidless lidless;lidless fingering order;lidless;fluid dynamics video;unsupervised;algorithm presented;presented fluid dynamics;fluid dynamics;new unsupervised;algorithm;technique improve accuracy", "pdf_keywords": ""}, "b1a8c6de4fbfe485c8f1c7723404467b72788ff2": {"ta_keywords": "deep learning healthcare;machine learning healthcare;rnns multivariate clinical;pioneering rnns multivariate;rnns multivariate;learning healthcare conditions;multivariate clinical time;rnns;clinical time series;machine learning emerged;deep learning;pioneering rnns;learning healthcare;standard machine learning;space machine learning;breakthroughs deep learning;years machine learning;work pioneering rnns;learning healthcare group;closed timelike;euclidean space machine;multivariate clinical;closed timelike curve;clinical time;machine learning;timelike curve;medical decision making;machine learning setup;healthcare conditions;healthcare", "pdf_keywords": ""}, "b6b76f529d273a35180d0dc65912db1538539067": {"ta_keywords": "categorize text metadata;text metadata semantic;metadata based generative;supervised document classification;embeds text metadata;metadata semantic;document categorization;metacat minimally supervised;datasets document categorization;document classification;text metadata based;categorize text;document classification concerned;text metadata;document categorization aims;metadata semantic space;framework categorize text;supervised document;metadata;conventional supervised document;documents labels metadata;propose metacat minimally;supervised framework categorize;labels metadata;metadata based;categorize;metadata domains text;topic label document;metacat minimally;topic label", "pdf_keywords": "categorize text metadata;metadata document categorization;estimation textual metadata;textual metadata;textual metadata statistics;document categorization;categorize text;text metadata;text classification;text metadata recognition;word embeddings;document categorization assigning;text metadata document;samples text metadata;documents labels metadata;framework categorize text;metadata recognition challenges;learns embedding;metacat minimally supervised;metadata recognition;module word embeddings;topic label document;learns embedding vectors;metadata document;metadata;framework text classification;topic label;text classification paper;labels metadata;metadata statistics synthesizes"}, "ac713aebdcc06f15f8ea61e1140bb360341fdf27": {"ta_keywords": "language processing adversary;attacker extract model;adversarial;adversarial attacks;processing adversary query;adversarial attacks approach;attacker extract;adversary query;processing adversary;adversary victim model;ability attacker extract;adversary query access;pretrained language model;naive adversaries;successful naive adversaries;adversary victim;transfer learning;language model bert;presence adversarial attacks;adversary;victim model attempts;model extraction diverse;approach transfer learning;presence adversarial;model extraction natural;model bert;model study defense;ones assuming adversary;classification api watermarking;assuming adversary victim", "pdf_keywords": "language processing adversary;answering watermarked queries;language models large;processing adversary query;pretrained bert model;bert model;watermarked queries;model extracted adversary;extraction language models;language models;adversary query;architecture answering watermarked;bert model extracted;tuning pretrained bert;deep neural;processing adversary;adversary query access;trained large conversational;learning models;representation learning context;pretrained bert;watermarked queries paper;learning context membership;neural network trained;learning representation;extracted adversary;learning models obtained;queries original training;membership classi inference;architecture answering"}, "3671dabbfd2e854060e1e382bad96b6bb00fcb46": {"ta_keywords": "noise robust asr;noise sparse;noise robust;robust asr recognition;signals noise robust;noise sparse linear;robust unknown noise;model noise sparse;performance noise robust;continuous digits recognition;noise robust prove;robust asr;digits recognition;asr recognition;enhancing speech features;digits recognition experiments;speech features;features presence noise;signals noise;noise paper propose;speech features presence;robust unknown;sparse linear;asr recognition error;combination signals noise;model noise;sparse linear combination;noise paper;noise;performance noise", "pdf_keywords": ""}, "6bea71fa6deb19c67e9586428f8f240e789fb3df": {"ta_keywords": "bandit problem linear;bandit problem;banditon problem;armed bandit problem;armed banditon problem;banditon problem paper;multi armed bandit;regret bound improves;multi armed banditon;bandit;arp regret bound;martingales algorithms predicting;armed bandit;regret bound;stochastic multi armed;banditon;performance algorithms stochastic;performance confidence sets;algorithms stochastic;armed banditon;martingales algorithms;algorithms stochastic multi;smaller confidence sets;stochastically multi armed;valued martingales algorithms;confidence sets;stochastic integrators vector;confidence sets note;stochastic multi;stochastically multi", "pdf_keywords": "linear stochastic bandit;stochastic bandit;stochastic bandit problem;exploitation stochastic bandit;bandit problem weighted;regret bound logarithmic;improves regret bound;bandit problem improve;bandit problem;bandit problem expressed;computation expected regret;regret confidenceball algorithm;probability constant regret;regret bound;new regret bound;bandit;exploitation stochastic;anarmed bandit problem;regret decision maker;algorithm online learning;principle exploitation stochastic;expected regret decision;uncertainty principle exploitation;improves regret;regret bound stopped;constant regret;improve regret;problem improve regret;expected regret;modi\ufb01cation improves regret"}, "2f0221142db900e75bd9c54fa153fb770a72f672": {"ta_keywords": "thesaurus crowdsourcing;thesaurus crowdsourcing applications;open thesaurus crowdsourcing;crowdsourcing applications;crowdsourcing project;yaroslavian crowdsourcing project;crowdsourcing;yaroslavian crowdsourcing;crowdsourcing project aims;results yaroslavian crowdsourcing;synset assembly;describes synset assembly;synset assembly interface;movement animals crowded;crowded environments;animals crowded environments;crowded environments paper;synset;animals crowded;paper describes synset;describes synset;crowded;assembly;movement animals;model movement animals;assembly interface developed;large open thesaurus;thesaurus;assembly interface;open thesaurus", "pdf_keywords": ""}, "7dce2877758b0103d1f7a454c184dc641e123359": {"ta_keywords": "natural language queries;querying annotation graphs;factoid question answering;question answering;querying annotation;indexing querying annotation;question answering qaqa;queries answer question;language queries;graphs generated nlp;language queries answer;annotation graphs;document retrieval;answer bearing sentences;document retrieval possible;query paper;query paper presents;sentences generated answer;nlp pipelines;generated nlp;query answer bearing;answering qaqa algorithm;improve quality query;computer aided factoid;annotation graphs generated;generated nlp pipelines;querying;context document retrieval;queries answer;queries", "pdf_keywords": ""}, "309b2c75dcdafea19a053876e56cef9747d428fb": {"ta_keywords": "neural lattice models;attention handle lattice;lattice models training;lattice structures recurrent;neural lattice;previous neural lattice;training inference lattices;model speech translation;structures recurrent neural;recurrent neural networks;inference lattices efficient;structures recurrent;inference lattices;lattice models;incorporate lattice structure;embeddings lattice structures;recurrent neural;incorporate lattice;lattice structures;lattice structure model;lattice structure;handle lattice inputs;translation task outperforms;handle lattice;lattice;lattices;lattice inputs;state art neural;speech translation task;positional embeddings lattice", "pdf_keywords": "attentional modeling lattice;lattice self attentional;lattices self attention;attention encodes sequences;self attentional modeling;machine translation model;attentional modeling;self attention modeled;models self attention;speech translation benchmarks;self attentional encoders;translation model recognizing;language self attention;attentional encoder;attentional encoder decoder;attention modeled;self attention encodes;attention modeled multiple;encoder component attentional;attention encodes;attentional encoders;speech translation task;translation model;component attentional encoder;machine translation;sequential model translating;translation benchmarks;input lattices self;model recognizing conversations;model translating sequential"}, "975551547fef77605fb85a551bbd7523b77746b7": {"ta_keywords": "automatic classification home;classification home;repository classification;data repository classification;repository classification paper;classification home home;activity home robots;home robot housekeeping;home robots;home robots important;topic based search;robot housekeeping;robot housekeeping activity;activities home robot;classify housekeeping activities;unstructured data repository;toolbox automatic classification;enrichment topic modeling;home robot;keyword enrichment topic;method classify housekeeping;classify housekeeping;keyword enrichment;search topic based;dataless hierarchical classification;topic modeling;github;hierarchical classification methods;hierarchical classification;open source toolbox", "pdf_keywords": "keyword enrichment topic;topic labels repositories;hierarchical classi\ufb01cation github;enrichment topic modeling;representation learning github;learning github repositories;keyword enrichment;facilitate repository search;embedding keyword enrichment;repositories recognition;repositories recognition challenges;learning github;classi\ufb01cation github repositories;target repositories recognition;introduction github widespread;classi\ufb01cation github;keyworddriven hierarchical;topic modeling;repository search analysis;repositories greatly facilitate;github repositories overwhelming;github widespread research;github widespread;repository search;repositories overwhelming;keyworddriven hierarchical classi\ufb01cation;github repositories;facilitate repository;introduction github;topic modeling pseudo"}, "46ed42e4318e1363a0ec3dde195422cdfecf2017": {"ta_keywords": "phrase bert embeddings;level phrase representations;phrase level similarity;phrase representations;bert embeddings easily;phrase bert outperforms;neural topic model;paraphrase generation model;bert embeddings;phrase representations exhibit;diverse phrasal paraphrases;phrase bert approach;paraphrases automatically generated;space phrase representations;phrase based neural;phrase representations derived;relatedness phrase bert;paraphrase generation;utility phrase bert;word phrase level;phrase level topic;level topic models;topics mixtures words;mixtures words phrases;paraphrases automatically;phrasal paraphrases automatically;topic models;phrase level;phrase bert;phrases context mined", "pdf_keywords": "learns phrase semantics;phrase relatedness phrases;unsupervised paraphrase generation;phrase embeddings;encode phrase relatedness;corpus phrase embeddings;phrase relatedness;semantics phrasal diversity;phrase embeddings experiments;relatedness phrases phrases;paraphrase generation;phrase embeddings using;powerful phrase embeddings;diverse phrasal paraphrase;based unsupervised paraphrase;relatedness phrases;lexically diverse phrasal;phrasal paraphrase pairs;paraphrase pairs additionally;phrases phrases extracted;unsupervised paraphrase;large corpus phrase;topic model learns;semantic relatedness based;dataset lexically diverse;300 million phrases;paraphrase pairs;semantic relatedness;automatic topic labeling;phrase semantics"}, "d4af2654f97c09741aba9f0da9ace7bc84b9a63f": {"ta_keywords": "time bayesian network;bayesian network ectbn;bayesian network;time bayesian;continuous time bayesian;event driven;model paths poverty;prediction competition;novel event driven;conditioning network;based prediction competition;education transportation model;finance event;finance event event;conditioning network state;structure learning based;structure learning;poverty clients citylink;event event fluctuations;greedy search;events;event fluctuations;model situations state;paths poverty clients;bayesian;events various types;ectbn representation model;prediction competition score;model situations;class ectbns", "pdf_keywords": ""}, "73a6e4574de038878be1bbb5985400998e420a5b": {"ta_keywords": "review peer assessment;peer assessment;peer grading;peer grading assignments;assignment quality guarantees;peer assessment employees;assignment quality;peer review peer;compromise assignment quality;applications peer grading;conference peer review;assignment assignment quality;peer review;review peer;strategyproof assignment assignment;review conference peer;peer review present;algorithms strategyproof assignment;grading assignments;assignment quality required;strategyproof assignment;grading assignments grant;assessment reliability predictor;conference peer;grading;strategyproofness measure;price strategyproofness measure;strategyproofness measure compromise;reliability predictor;assessment reliability", "pdf_keywords": ""}, "78438b61afc2c9123c28ca4d6b58e598462ae9be": {"ta_keywords": "domain adaptation;source domain adaptation;new domain adaptation;domain specific adversarial;domain adaptation technique;adversarial training task;domain adaptation utilizing;domain adaptation mda;adversarial task specific;domain adaptation method;optimization adversarial task;embedded adversarial training;adversarial training process;adversarial task;adversarial training;clustering embedded adversarial;target domain diverse;specific adversarial framework;embedded adversarial;domain diverse multi;adversarial framework;optimization adversarial;adversarial framework achieved;min optimization adversarial;specific adversarial;domain diverse;adversarial;disturb adversarial training;multi source domains;adaptation utilizing task", "pdf_keywords": ""}, "1f0446dddd192e94f3930a3a449bd89796f4200f": {"ta_keywords": "speech recognition adaptation;transforms acoustic features;transformation matrices feature;dnn adaptation decoding;adaptation decoding;regression fmmllr transforms;feature transformation matrix;recognition adaptation;feature space adaptation;dnn adaptation;multiple transformation matrices;recognition adaptation important;adaptation decoding share;applied dnn adaptation;acoustic features adapted;fmmllr transforms acoustic;adaptation applied dnn;transformation matrices;feature transformation;single feature transformation;transforms acoustic;transformation matrices according;estimates multiple transformation;single transformation matrix;transformation matrices used;matrices feature space;transformation matrix frame;regularize map estimation;speech recognition;linear regression fmmllr", "pdf_keywords": ""}, "2226560f94c1e90d6900d4674b649cc5522b78cc": {"ta_keywords": "multilingual self attentional;self attentional translation;sharing single translation;multilingual machine translation;multilingual parameter sharing;bilingually trained models;attentional translation process;single translation model;model multilingual self;attentional translation;transformer model multilingual;multilingual machine;multilingual parameter;translation model;multilingual self;model multilingual;improvements translation accuracy;performance gains bilingually;single translation;bilingually trained;gains bilingually trained;translation model multiple;approach multilingual parameter;translation accuracy;machine translation;multilingual;translation process multiple;family multilingual machine;translation process;substantial improvements translation", "pdf_keywords": "translation shared attention;multilingual neural machine;multilingual neural;machine translation shared;bilingual models multilingual;neural machine translation;multilingual models;machine translation architecture;multilingual translation;multilingual translation translation;way multilingual neural;models multilingual;models multilingual models;standard bilingual models;present multilingual neural;multilingual models follow;bilingual models;translation shared;score multilingual translation;translation score multilingual;multilingual;translation architecture able;translation architecture;translate multiple languages;query attention weights;machine translation;translation translation common;language multiple target;multiple languages single;attention weights"}, "04b91791225a4f86b0715b41c6f56c00c197d810": {"ta_keywords": "conversion bandwidth storage;optimal convertible codes;implementation convertible codes;conversion bandwidth optimal;conversions convertible codes;bandwidth optimal convertible;convertible codes significantly;constructions conversion bandwidth;bound conversion bandwidth;necessitates code conversion;limits conversion bandwidth;conversion bandwidth;conversion bandwidth present;resource efficient conversions;space convertible codes;convertible codes;conversion bandwidth significantly;code conversion;bandwidth storage;convertible codes recently;bandwidth storage space;code conversion process;convertible codes paper;data encoded code;encoded code equivalent;convertible codes recent;converting data encoded;storage space tuning;efficient conversions;lower bound conversion", "pdf_keywords": "codes distributed storage;convertible erasure codes;erasure codes convertible;convertible codes erasure;erasure codes designed;optimal convertible codes;erasure codes;codes erasure codes;codes distributed;bandwidth optimal convertible;bandwidth used code;codes convertible codes;distributed storage;codes erasure;encoded data undergo;storage systems conversion;distributed storage systems;class codes distributed;encoded data;convertible codes;codes convertible;convertible codes present;bound conversion bandwidth;bandwidth merge regime;storage systems;constructions bandwidth optimal;conversion bandwidth merge;conversion bandwidth;encoded;access bandwidth optimal"}, "43fe2d8781473360eeaae7a3284169a303200846": {"ta_keywords": "detection fake news;news stance classification;fake news competition;stance classification task;stance classification;fake news stance;false news recent;news competition;news competition models;news stance;stacking ensemble method;classification accuracy models;classification accuracy;stacking ensemble;2017 fake news;improve classification accuracy;models detection fake;classification;detection fake;ensemble method;improve classification;false news;news recent arxiv;ensemble;fake news;deficit false news;ensemble method used;classification task;accuracy models;news recent", "pdf_keywords": ""}, "d9b89de5c2a39479768c6e32f13ac3e816635cc1": {"ta_keywords": "transducer translation models;training translation models;learning lexical translation;translation models;translation model inference;computer aided translation;lexical translation parameters;translation models language;translation models used;based translation model;word lattices transcription;aided translation word;translation model;aided translation;finitestate transducer translation;training translation;translation parameters directly;automatic speech;transducer translation;translation parameters;lexical translation;method based translation;transcription sought translation;method training translation;model learning lexical;models language pairs;paired written translation;improve automatic speech;speech recognition;speech recognition speech", "pdf_keywords": ""}, "44e24aabd05bef8cb45646486f1a24b7caecee45": {"ta_keywords": "language model rnnl;learning multilingual model;lexicon alignments recurrent;recurrent neural network;multilingual seq2seq model;alignments recurrent neural;rrna asrna sequences;monolingual models;model rnnl;prior multilingual seq2seq;neural network language;alignments recurrent;language model;transfer learning multilingual;multilingual seq2seq;multilingual model;network language model;learning multilingual;recurrent neural;asrna sequences based;monolingual models communities;asrna sequences;dnn hmm systems;asymptotic rrna asrna;rrna asrna;improving prior multilingual;sequence seq2seq approach;sequence sequence seq2seq;conventional dnn hmm;gains monolingual models", "pdf_keywords": "multilingual retraining deep;multilingual retraining seq2seq;multilingual retraining;deep cnn speech;multilingual speech recognition;architecture multilingual retraining;cnn speech recognition;approach multilingual retraining;retraining deep convolutional;multilingual seq2seq model;languages multilingual speech;multilingual model;multilingual speech;monolingual models babel;deep cnn;cnn speech;monolingual models;languages using transfer;multilingual seq2seq;retraining deep;build multilingual seq2seq;deep convolutional neural;transfer learning;babel languages multilingual;multilingual model shows;networks deep cnn;languages build multilingual;neural networks deep;deep convolutional;propose deep convolutional"}, "8b468872cf915c98ff46a2bea4d2a34112b7b0b0": {"ta_keywords": "named entity extraction;relational classification retrieval;entity extraction;rules path ranking;longer relational rules;entity extraction parsed;entity extraction task;relational classification;graph based knowledge;knowledge base inference;based knowledge base;knowledge base;walks efficiently discover;relational rules;path ranking algorithm;relational rules including;learning paths;address relational classification;path ranking;person named entity;learning paths constants;results learning paths;relational rules class;longer relational;including longer relational;space relational rules;named entity;searching features fluid;faster algorithms searching;larger space relational", "pdf_keywords": ""}, "d8704a63517868475b3af7ec25eaa2fb2a44362b": {"ta_keywords": "weakly supervised cnn;supervised cnn;supervised cnn segmentation;shallow segmentation;cnn segmentation;models shallow segmentation;cnn segmentation inspired;weakly supervised;approach shallow segmentation;cnn;segmentation multilayer;shallow segmentation called;approach weakly supervised;gradient descent;worse gradient descent;shallow segmentation paper;learning models shallow;generalized gradient descent;segmentation inspired known;segmentation multilayer membranes;method segmentation multilayer;segmentation inspired;supervised;gradient descent gd;generalized gradient;gradient descent gg;segmentation;segmentation called dense;loss computer vision;minimizing loss", "pdf_keywords": "regularized segmentation losses;gradient descent deep;losses deep learning;weakly supervised cnn;optimization regularized loss;regularized loss approaches;descent deep;regularized segmentation;segmentation losses optimization;segmentation losses;regularized loss;regularization solver deep;regularization models losses;losses optimization regularized;descent deep learning;regularized losses network;deep segmentation methods;minimizing regularized losses;models losses deep;research regularized segmentation;deep segmentation;regularized losses;deep semantic segmentation;optimization methods deep;learning use regularization;deep semantic;losses network training;focus regularized loss;supervised cnn;gradient descent"}, "6dd6d4dfc3cf9ff41aad7e903cf1294de2ac5629": {"ta_keywords": "automated vehicle safety;vehicle safety assessments;automated driving systems;robust automated driving;assessment driver safety;automated driving drivers;automated driving;initiative automated driving;safety assessment driver;vehicle safety;driver safety;driver safety event;scenario based safety;traffic situations identified;safety assessment safety;driving systems;driving drivers standardization;safety assessment;safety assessment efficient;safety assessments;new safety assessment;driving systems increasingly;developing automated vehicle;assessment safety assessment;safety assessments applicable;automated vehicle;based safety assessment;parameterize scenarios extracted;deceleration scenarios extracted;safety real world", "pdf_keywords": ""}, "1288d6570085a28518a9f3495e77dbb75899421c": {"ta_keywords": "active annotation;annotation active annotation;introduces semi supervised;semi supervised learning;active learning;semi supervised;active learning framework;active annotation used;active annotation active;annotated data;supervised learning framework;annotation active;active learning used;framework hidden markov;new active learning;hidden markov models;data seed annotated;seed annotated data;material active annotation;supervised;supervised learning;idea active learning;annotation;identify entities biomedical;hidden markov;learning framework;entities biomedical;entities biomedical domain;annotation used identify;learn noisy data", "pdf_keywords": ""}, "163a67b5b0371035fa6e0f88b36ba97a32e735bc": {"ta_keywords": "encoding scheme storage;storage redundant data;storage redundant;scheme storage redundant;code conversion redundancy;convertible codes optimal;new encoding scheme;encoding scheme;redundancy configuration encoded;scheme storage;distributed storage;encoded data encoding;codes provide durability;erasure codes provide;mds convertible codes;erasure codes;distributed storage systems;storage systems;data encoding;scale distributed storage;convertible codes broad;encoded data;introduce convertible codes;storage;data encoding requires;use erasure codes;convertible codes;conversion redundancy configuration;redundant data;conversion redundancy", "pdf_keywords": "code conversion storage;convertible codes access;mds convertible codes;erasure codes;introduce convertible codes;codes access optimal;conversion storage;code data encoded;convertible codes;maintaining desired decodability;resiliency failures storage;storage systems;conversion storage storage;facilitate code conversions;code conversion;code conversions;allow code conversions;storage storage systems;desired decodability properties;desired decodability;data encoded nf;code data;convertible codes new;codes ef\ufb01ciently;form erasure codes;failures storage systems;data encoded;decodability properties maximum;storage storage;code conversion process"}, "027c5e44164a2ee3543ecdff73cd4d7888a42a90": {"ta_keywords": "preferences artificial intelligence;decision making machines;neural network architecture;artificial intelligence;fly preferences artificial;artificial intelligence expect;learning reasoning preferences;preferences artificial;neural network;reasoning preferences;value alignment preference;approach problem neural;reasoning preferences important;neural;problem neural;problem neural network;flexible value alignment;alignment preference central;fly preferences;consistent preferences;recommendations consistent preferences;architecture;representing learning reasoning;make decisions recommendations;decisions recommendations consistent;preference central;intelligence;consistent preferences adhere;value alignment;machines", "pdf_keywords": ""}, "c17ccb7f0372ec98b7e070b0f70518f28516ecd5": {"ta_keywords": "simulation stochastic process;stochastic process rigid;computer simulation stochastic;simulation stochastic;stochastic process;markov process;concept markov process;stochastic processes;introduce concept markov;processes stochastic;process stochastic;stochastic process stochastic;markov;concept markov;stochastic;stochastic processes stochastic;processes stochastic process;markov process applications;theory stochastic processes;process stochastic processes;processes stochastic processes;theory stochastic;stochastic processes 02;process rigid body;simulation;process rigid;computer simulation;rigid body;applications theory stochastic;classroom", "pdf_keywords": "1d hubbard model;hubbard model;dimensional 1d hubbard;hubbard model case;1d hubbard;xmath1 phase diagram;hubbard;xmath1 phase;xmath0 xmath1 phase;xmath0 probability distribution;position particle random;xmath0 probability;distribution position particle;given xmath0 probability;particle random potential;phase diagram dimensional;particle random;study xmath0 xmath1;diagram dimensional 1d;xmath0 xmath1;model case binary;potential given xmath0;xmath1;case binary systems;binary systems case;random potential;binary systems;given xmath0;phase diagram;systems case distribution"}, "23e03cd57b5d75993545127f3fecf99d25021583": {"ta_keywords": "predict cancer phenotypes;predict cancer;profiled cancer genome;tumor embeddings functional;based tumor embeddings;tumor embeddings;genes degs tumors;cancer genome;approach predict cancer;cancer phenotypes based;cancer biology precision;cancer genome atlas;cancer phenotypes;cancer biology;phenotypes cancer progression;task cancer biology;phenotypes cancer;deep neural;oncology present deep;phenotypes based tumor;tumors profiled cancer;profiled cancer;network model encoder;sgas cellular signaling;multiple phenotypes cancer;model encoder;model 468 tumors;deep neural network;degs distinguishes cancer;biology precision oncology", "pdf_keywords": "tumor embedding predict;tumor gene embeddings;tumor embedding embedding;tumor embeddings;tumor embeddings et;using tumor embedding;utility tumor embeddings;tumor embedding;representation tumor embedding;survival gene embeddings;embedding predict;gene embeddings provide;gene embeddings;embedding predict patient;tumor gene;cancer genome;processes tumor gene;cancer genome atlas;gene embeddings compactly;pro\ufb01led cancer genome;predict multiple phenotypes;embeddings provide novel;types using tumor;cancer types using;embedding embedding;discovering patterns tumors;patient survival gene;survival gene;embeddings provide;embeddings"}, "1d3539a8d94bd3ab78993d7cc584efc06ed0e460": {"ta_keywords": "benchmarking feature attribution;explainable machine learning;benchmarking popular explainability;models feature attribution;feature attribution algorithms;explaining model predictions;feature attribution;feature attribution methods;model explainability based;synthetic datasets;model predictions increasingly;synthetic datasets library;popular explainability techniques;model explainability;explain model predictions;realworld datasets demonstrate;popular explainability;suite synthetic datasets;research explainable machine;synthetic benchmarks;synthetic datasets release;attribution algorithms;tools explaining model;synthetic benchmarks forscientific;datasets demonstrate;benchmarking feature;explainability techniques;prohibitive realworld datasets;machine learning models;explainability based", "pdf_keywords": "feature attribution metrics;datasets benchmarking attribution;benchmarking popular explainability;benchmarking attribution;explainability methods metrics;library explainability methods;benchmarking attribution methods;practitioners explainability algorithms;feature attribution;distribution feature attribution;library explainability;explainability algorithms;explainability algorithms development;attribution metrics;library feature attribution;popular explainability techniques;present library explainability;explainability techniques;explainability methods;explainability increasingly;explainability techniques evaluation;attribution metrics used;feature attribution applied;lives explainability increasingly;explainability increasingly important;synthetic datasets;popular explainability;synthetic wine datasets;researchers practitioners explainability;attribution methods widely"}, "8a14b3a9e642f4ca7fad4df997fc1941bdcfb935": {"ta_keywords": "statistical models speech;models speech;model human behavior;models speech language;statistical models;human behavior;speech language processing;speech language;statistical;model human;language processing widely;speech;language processing;models;used model human;behavior;language;widely used model;model;human;processing;processing widely;processing widely used;used model;widely;widely used;used", "pdf_keywords": ""}, "ffecb8b8b415149f5351b64a2dbb1a1fa64219f0": {"ta_keywords": "end speech translation;generalization multilingual training;multilingual end;demonstrate multilingual end;multilingual models;multilingual end end;multilingual training;machine translation mt;asr machine translation;speech translation st;field multilingual models;machine translation;electric field multilingual;speech translation;multilingual training evaluated;significantly outperform bilingual;utterances source languages;translation st speech;outperform bilingual ones;outperform bilingual;experimentally demonstrate multilingual;automatic speech;multilingual models shown;languages directly translated;useful automatic speech;multilingual;generalization multilingual;field multilingual;demonstrate multilingual;speech recognition asr", "pdf_keywords": ""}, "9c78481004b7dbb601b83cc081ec23c02e6f5270": {"ta_keywords": "stable equilibria learning;equilibria learning dynamics;linearized game dynamics;equilibria learning;learning dynamics;game dynamics;equilibrium states player;learning rates games;linearized game;learning dynamics does;equilibria stable nash;stable nash robust;game dynamics stable;equilibrium differential nash;stable nash;differential nash equilibrium;nash equilibrium fluid;nash equilibrium;dynamics stable equilibria;continuous games;player continuous games;nash robust;gradients player continuous;nash robust variations;individual gradients player;set stable equilibria;differential nash;gradients player;stable equilibria stable;stable equilibria", "pdf_keywords": "learning equilibria player;learning player continuous;players learning;learning equilibria;learning games continuous;networks gradient play;continuous games vector;differential nash equilibria;players learning processes;differential nash;learning player;equilibria player scalar;gradient play;player scalar games;notion differential nash;player continuous games;interaction players learning;continuous games;equilibria stable nash;games continuous action;stable nash robust;understanding scalar games;games vector action;games \ufb01nd equilibria;stable nash;learning games;scalar games;scalar games \ufb01nd;games continuous games;nash robust"}, "8b231737e0048a400527d89aa56c712e8b9bc690": {"ta_keywords": "generalization multilingual training;multilingual training;speech translation stt;translation speech utterances;multilingual swimmers multilingual;stt translation speech;multilingual st topic11;st topic11 multilingual;multilingual training evaluated;swimmers multilingual end;multilingual models;speech translation;enhance performance multilingual;end speech translation;translation speech;multilingual swimmers;multilingual end;performance multilingual;swimmers multilingual;machine translation;multilingual;topic11 multilingual end;multilingual st;utterances source languages;languages directly translated;asr machine translation;machine translation mt;pair multilingual models;performance multilingual swimmers;emergent multilingual st", "pdf_keywords": "multilingual machine translation;multilingual speech translation;speech translation multilingual;translation multilingual machine;models multilingual speech;multilingual speech recognition;multilingual training end;speech translation combined;tasks multilingual training;multilingual pretraining;framework multilingual speech;multilingual machine;machine translation architecture;multilingual speech;multilingual training;recognition tasks multilingual;translation machine translation;translation machine;exploiting corpora multilingual;translation multilingual;multilingual pretraining seed;outperformed bilingual end;machine translation machine;present multilingual machine;proposed multilingual training;machine translation;end speech translation;speech translation;models multilingual;outperforms bilingual"}, "da20ab7724335eb48bcd0e9be30f0ac4b6a464c6": {"ta_keywords": "detection novel scenarios;novelty detection deals;novelty detection;safety critical autonomous;real world driving;detect situations trained;self driving cars;self driving;novel scenarios vision;detection deals;trustworthy prediction;able detect situations;cars able detect;autonomous systems;make trustworthy prediction;detection novel;learning driven safety;detection;autonomous systems leveraging;autonomous;trustworthy prediction demonstrate;detection deals highly;trained prediction;detecting;scenarios vision;vision based autonomous;autonomous systems self;scenarios vision based;systems self driving;detect situations", "pdf_keywords": "real world driving;autonomous driving;navigation self driving;novel scenarios vision;driving datasets;scenarios vision based;self driving;scenarios vision;driving datasets real;trained prediction;detection novel scenarios;world driving;deep neural;car based deep;autonomous driving paper;based autonomous driving;autoencoder based;approach employs autoencoder;vision based autonomous;driving house racing;learned trained prediction;visual navigation self;self driving car;visual saliency;visual navigation;autoencoder;visual saliency preprocessing;deep neural networks;approach visual navigation;trained prediction model"}, "e54e0d9eaa922cefb1c69e105979399fd34497b1": {"ta_keywords": "fairness aware learning;fairness aware processing;fairness aware;fairness metrics;learning fair fairness;achieve group fairness;recently fairness aware;accuracies fairness metrics;group fairness;fairness metrics jointly;target labels fair;labels fair;group fairness perform;labels fair pg;state art fairness;group label annotations;group label annotation;annotated demographic group;demographic group labels;fully annotated demographic;fairness paper group;accuracies fairness;target accuracies fairness;label annotation real;annotated demographic;fairness perform worse;learning fair;fairness perform;group labels random;group labeling", "pdf_keywords": "fairness partially annotated;improve fairness model;grouping fairness partially;grouping fairness;algorithmic grouping fairness;group labels fair;improve fairness;fair training methods;learning face attributes;fairness model;method improve fairness;fairness partially;fairness problem sensitive;deep learning face;learning face;face attributes wild;fairness problem;sensitive groups diversely;labels fair;group classifier;sensitive groups;wild probabilistic embeddings;datasets group labels;fairness model semmi;labels fair pg;partially annotated group;face attributes;baseline fair training;annotated group labels;probabilistic embeddings cross"}, "dcb28c8ba94434eb8a06e81eb55bfdbc343d2340": {"ta_keywords": "sentence relation extraction;relation extraction;relation extraction generally;relation extraction methods;ary relation extraction;information extraction;spans information extraction;information extraction methods;machine reading approach;binary relations;sentence relation;binary relations expressed;cross sentence relation;document subrelation hierarchy;text spans information;relations involve entity;involve entity mentions;machine reading;focus binary relations;biomedical machine reading;spans document subrelation;relations expressed single;document subrelation;relations involve;relations;sentences combines representations;extraction methods focus;entity mentions;xmath0 relation high;entity mentions far", "pdf_keywords": "mention level representations;relation extraction;deep probabilistic logic;relation extraction combines;ary relation extraction;representations entity level;data deep;extraction combines representations;level representations entity;relation extraction validate;interactions biomedical articles;operationalizing personalized medicine;big data deep;data deep probabilistic;precision oncology knowledge;relation exploiting document;multiscale entity centric;deep probabilistic;biomedical articles;representations entity;event detection recurrent;personalized medicine;recurrent neural networks;combine mention level;level sub relations;hierarchy text spans;documentlevel ary relation;document level information;interactions precision oncology;propose multiscale entity"}, "900b785dbbea7ccd5846eafb14c6715f76fe5e00": {"ta_keywords": "photon impurity quantum;qpc based detection;detection pauli;detection pauli exclusion;based detection pauli;detection unseen threats;single photon impurity;impurity quantum;photon impurity;malware detection;impurity quantum point;detection based deep;approach malware detection;approach malware;malware present;malware;malware present novel;multiple families malware;generalizes detection unseen;families malware present;malware detection based;detection unseen;new approach malware;families malware;qpc;learning deep;deep learning;contact qpc based;deep learning deep;difficult security", "pdf_keywords": ""}, "2a81081c987da2bb8184b8e9a884cf6a73712ee8": {"ta_keywords": "dimensional harmonic trap;harmonic trap;particle dimensional harmonic;temperature dynamics single;dynamics single particle;dimensional harmonic;temperature dynamics;effect temperature dynamics;harmonic;single particle;single particle dimensional;particle dimensional;particle;effect temperature;investigate effect temperature;trap;dynamics single;dynamics;temperature;investigate effect;dimensional;single;paper investigate effect;effect;investigate;paper investigate;paper", "pdf_keywords": ""}, "126be977c03d732fbef2381565a41b957d41a2cc": {"ta_keywords": "nlp deep learning;processing nlp deep;learning architectures nlp;nlp deep;architectures nlp tasks;sentence machines capable;architectures nlp;nlp tasks;sentence machines;narrative modeling;nlp tasks require;building representations language;natural language processing;advances natural language;natural language;given sentence machines;deep learning understood;deep learning;context question answering;question answering;narrative modeling paper;deep learning architectures;deep learning models;processing nlp;language processing nlp;representations language;propose natural language;representations language directly;book narrative modeling;generating corresponding syntactic", "pdf_keywords": ""}, "aae8d332c3ff9d081ae36967d3d7b5f394b51bcc": {"ta_keywords": "weakly supervised learning;weakly supervised;supervised weakly supervised;supervised weakly;weakly supervised classification;semiand weakly supervised;semi supervised;semi supervised weakly;learn predict follower;supervised learning tasks;entity recognition tasks;classification named entity;supervised;training pseudo labels;entity recognition;follower strategy training;student learn predict;labels self training;various semi supervised;named entity recognition;supervised learning;supervised classification named;learn predict;supervised learning method;strategy training pseudo;new supervised learning;propose new supervised;method predicting student;supervised classification;new supervised", "pdf_keywords": "semi supervised text;entity recognition tasks;training framework nlp;semi supervised learning;semi supervised machine;supervised text;novel semi supervised;semi supervised;entity recognition;named entity recognition;supervised text classi\ufb01cation;nlp tasks;propose semi supervised;framework nlp tasks;semi supervised classi;weakly semi supervised;capable learning unlabeled;self training framework;tasks natural language;entity recognition question;learning unlabeled data;self training models;learning unlabeled;framework nlp;recognition question answering;supervised machine;deep learning;called deep learning;nlp tasks contrast;supervised machine learning"}, "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f": {"ta_keywords": "mesh tensorflow available;based mesh tensorflow;tensorflow implement efficient;mesh tensorflow;tensorflow available;tensorflow;use mesh tensorflow;parallel machine learning;tensorflow mesh tensorflow;mesh tensorflow mtflow;mesh tensorflow implement;flow mesh tensorflow;com mesh tensorflow;tensorflow mesh;present mesh tensorflow;tensorflow open source;tensorflow mtflow;video mesh tensorflow;tensorflow mtflow used;batch splitting widely;model batch splitting;mesh tensorflow mesh;tensorflow implement;efficient model parallel;approach batch splitting;dimensional mesh processors;batch splitting;distributed training;mesh tensorflow user;tensor flow mesh", "pdf_keywords": "distributed tensor computations;approach distributed tensor;distributed tensor;distributed parallelization framework;tensorflow;computations mesh tensorflow;class distributed tensor;tensorflow allows;model parallel computation;distributed parallelization;parallelization framework;mesh tensorflow allows;tensor computations;tensorflow introduce;mesh tensorflow;called mesh tensorflow;parallelization framework coupled;tensor computations train;presents distributed parallelization;tensorflow introduce language;mesh tensorflow introduce;tensor computations mesh;user specify tensors;parallel computation;tensors operations multi;specify tensors operations;parallelization;tensors operations;parallelization strategies;parallel computation layout"}, "e2bd274c8dd2a3b2a0a6f5d8a29baee07df34eb9": {"ta_keywords": "machine translation t2;machine translation methods;based machine translation;machine translation;superior machine translation;string machine translation;phrasebased systems improved;translation t2;machine translation based;translation methods;translation based extensive;par phrasebased systems;translation methods phrase;phrasebased systems;translation t2 proven;phrase based machine;t2 systems;t2 performs par;translation based;basic t2 performs;t2 systems constructed;phrase based hierarchical;basic t2;t2 performs;performs par phrasebased;performance gap t2;hierarchical phrase based;t2;pairs basic t2;based hierarchical phrase", "pdf_keywords": ""}, "98b7d5611c0a128f45db100cc796b981573adcc5": {"ta_keywords": "dynamics machine learning;machine learning based;machine learning;concept machine learning;concept machine;dynamics machine;learning based concept;study dynamics machine;based concept machine;learning based;dynamics;learning;results study dynamics;machine;study dynamics;based concept;concept;based;article;article present;results;results study;study;present;article present results;present results;present results study", "pdf_keywords": ""}, "bcd4c46e4d75ddedb6138cfd77600c6d964a9aa8": {"ta_keywords": "program semantics training;program semantics generative;quantum logic programs;simulation quantum logic;natural language code;incorporate program semantics;involve program semantics;testing semantic equivalence;program semantics;code models natural;testing semantic;pretrained code models;semantics training;simulation quantum;natural language tocode;quantum logic;semantics training able;semantic equivalence languages;language tocode tasks;approach simulation quantum;semantics generative;logic programs;models natural language;semantics generative models;language tocode;natural language translation;code models;language code;method testing semantic;generative models code", "pdf_keywords": "improve program generation;program generation results;natural language executable;program generation;pretrained code model;language executable code;quality generated programs;codeword language model;executable code awareness;generated programs;code awareness execution;pretrained language models;generated programs especially;pretrained code;codeword language;called codeword language;language executable;supervised machine translation;code model;hypothesis pretrained code;collection pretrained language;programs using execution;executable code;pretrained language;machine translation;semantically equivalent code;machine translation method;improve program;execution accuracy evaluation;called codeword"}, "8688169ad5701e726968e293ff7dc53d76dd8007": {"ta_keywords": "imperfections performance artificial;neural network ann;performance artificial neural;imperfections performance;effect imperfections performance;artificial neural network;artificial neural;neural network;effect imperfections;investigate effect imperfections;network ann;imperfections;performance artificial;neural;ann;artificial;network;performance;paper investigate effect;paper;effect;paper investigate;investigate effect;investigate", "pdf_keywords": ""}, "4f78624defde3b60551cfeb37e3943b267ea704a": {"ta_keywords": "distributed learning withcompressed;updates distributed learning;quantization updates distributed;learning withcompressed gradient;distributed learning;compression quantization updates;compression quantization;learning withcompressed;based compression quantization;large machine learning;quantization updates;able learn gradients;learn gradients;models requires distributed;withcompressed gradient differences;quantization;withcompressed gradient;quantum body systems;training large machine;based compression;distributed computing;compression;model updates bottleneck;study quantum body;new compression;learn gradients renders;propose new compression;distributed computing approach;learning models;quantum mechanics powerful", "pdf_keywords": "regularization distributed learning;distributed learning compressed;distributed learning;proximable regularization distributed;regularization distributed;learning compressed gradients;distributed learning particular;compressed gradients promising;learning compressed;approach distributed learning;proximable regularization;compressed gradients;minimization iterates terngrad;regularization;support proximable regularization;gradients promising;terngrad smooth nonconvex;nice scaling mpi;smooth nonconvex objective;convolutional formula nonconvex;risk minimization iterates;minimization iterates;nonconvex objective;minimization;gradients promising approach;scaling mpi processes;scaling mpi;smooth nonconvex;regularizer paper introduce;nonconvex"}, "dbdefb498b619912a726fec7c85533594a1c6a1b": {"ta_keywords": "adversary instead maximization;stochastic gradient ascent;nonconcave algorithm guaranteed;nonconcave maximization np;gradient ascent;minimax optimization;minimax optimization minx;algorithm min player;maximization np hard;gradient ascent accelerated;adversarial networks propose;considers minimax optimization;adversarial;adversary multi step;maximization np;nonconvex nonconcave algorithm;minx maxy challenging;nonconcave algorithm;optimization minx maxy;play smooth algorithms;adversarial networks;nonconcave maximization;optimization minx;maximize nonconcave maximization;smooth algorithms;adversary multi;considers minimax;minimax;generative adversarial;algorithms context generative", "pdf_keywords": "nonconcave minimax optimization;minimax optimization naturally;nonconvex nonconcave minimax;nonconcave minimax;minimax optimization thoroughly;minimax optimization minx;minimax optimization;gans adversarially;gradient descentascent nonconvex;adversarially robust;gans adversarially robust;networks gans adversarially;player nonconvex convex;gradient descent ascent;adversarially robust models;algorithms gradient descent;convex setting minimax;optimization minx;gradient descent;networks gans;challenging optimization;challenging optimization problems;optimization problems nonconvex;considers minimax optimization;adversarial networks gans;adversarial;adversarially;adversarial networks;minimax;gradient descentascent"}, "a3ce3004a0eade48a3ae652dbf5c04a60c2416aa": {"ta_keywords": "traits dialogue generation;personalized dialogue generation;personalized dialogues speaker;personalized dialogues;personalized dialogue;deliver personalized dialogues;personality traits dialogue;personality encoded speaker;study personalized dialogue;dialogue generation;traits dialogue;persona labeled dialogue;dialogues speaker unique;dialogue generation researches;dialogues speaker;speaker unique personality;dialogue dataset;dialogue generation deliver;dialogue data;encoded speaker traits;dialogue data far;turn dialogue dataset;dialogue dataset containing;embodying personality language;speaker traits;labeled dialogue data;personality language;personaldialog large scale;attention persona aware;personality language expression", "pdf_keywords": "traits dialogue generation;personalized dialogue generation;personalized dialogue;dialogue generation models;dialogue generation;personality traits dialogue;dialogue generation based;traits persona speaker;dialogue generation process;present personalized dialogue;traits dialogue;dialogue dataset;utterance personaldialog;single utterance personaldialog;persona speaker;dialogue dataset containing;turn dialogue dataset;dialogue texts judged;dialogue texts;framework dialogue generation;personality traits persona;utterance personaldialog large;trait carried dialogue;personaldialog large scale;generation based persona;persona speaker viewed;traits persona;dialogue;address personality traits;persona representation"}, "54a13bcc9613dcaa76fb25fbe96572f376cfcca9": {"ta_keywords": "machine translation model;learn machine translation;gradients adaptive learning;new machine translation;adaptive learning rates;machine translation;translation model;learning rates sublinear;gradients adaptive;rates sublinear memory;machine translation task;past gradients adaptive;moments neural network;learning rates;translation model used;second moments neural;stochastic optimization;sublinear memory;present stochastic optimization;adaptive learning;sublinear memory cost;rate second moment;moments neural;neural network weight;estimating second moments;stochastic optimization method;translation task;moment accumulator slow;translation task paper;slow present stochastic", "pdf_keywords": "translation task trained;machine translation benchmark;adafactor machine translation;translation speed training;machine translation speed;translation machine translation;machine translation machine;translation machine;translation benchmark;machine translation uses;machine translation;new machine translation;machine translation algorithm;machine translation task;transformation based softmax;translation uses machine;softmax;performed machine translation;uses machine translation;translation algorithm called;translation algorithm;gradient accumulator training;german machine translation;translation speed;translation task;adafactor online learning;neural networks gradient;learning algorithm adafactor;algorithm training neural;team based gradient"}, "957e3ec3c722f5cb382fe8ac54fc846ee772a95f": {"ta_keywords": "regret bounds improving;new regret bounds;bandit problem;feedback regret bound;bandit problem main;regret bounds;loss logarithmic regret;regret bound simultaneously;logarithmic regret exists;armed bandit problem;particular regret bound;logarithmic regret;multi armed bandit;regret bound;regret bound depends;partial feedback regret;algorithm apply optimism;adversarial multi armed;bandit;regret exists arm;mirror descent;small regret better;new regret;online mirror descent;implies small regret;armed bandit;regret better arm;feedback regret;bounds improving;games partial feedback", "pdf_keywords": "learning rate game;rate game predictable;game predictable sequences;problem online learning;online learning partial;learning rate update;game predictable;online learning;rate game;learning rate;increase learning rate;equilibriums algorithm;partial information feedback;equilibriums algorithm based;algorithm computing equilibriums;learner decide increase;predictable sequences;predictable sequences bounded;decide increase learning;information feedback;increase learning;algorithm predicting;algorithm predicting best;learner decide;computing equilibriums algorithm;predictable;computing equilibriums;predicting best;predicting best arm;learning partial information"}, "c4ce6aca9aed41d57d588674484932e0c2cd3547": {"ta_keywords": "tool search pandemic;natural language scientific;search pandemic;prominent pubmed search;scientific search covid;search pandemic pandemic;pubmed search study;pubmed search;scientific search;scientific papers;scientific literature challenging;schema extracting;scientific literature;language scientific papers;schema extracting information;extracting information natural;interdisciplinary scientific search;automated tool search;pandemic pandemic;large corpus papers;search covid;develop schema extracting;pandemic solutions pandemic;pandemic;relations large corpus;pandemic pandemic solutions;solutions pandemic;covid 19 literature;pandemic solutions;clinical experts search", "pdf_keywords": "annotations bio nlp;annotating entities relations;bio nlp expert;bio nlp;annotating entities;relations using biomedical;annotation challenges scienti\ufb01c;entity relation extraction;relation extraction;high quality annotations;extracting causal relationships;annotations bio;relation extraction text;automatically extracting causal;extraction text embeddings;entities relations scienti;causal relationships entities;annotating;boundaries nlp expert;nlp expert paper;annotation challenges;annotations;annotation;nlp expert;process annotating entities;biomedicine use supervised;text embeddings;expert verifying annotations;biomedical experts unifying;knowledge base search"}, "73b22457a2f52a834d73d73a76b4124c1cb326be": {"ta_keywords": "gradient algorithms player;player alternates learning;nash equilibrium;player performative prediction;adaptive gradient algorithms;methods adaptive gradient;strong monotonicity game;nash equilibrium investigate;adaptive gradient;monotonicity game;free methods adaptive;learning data reacts;ii nash equilibrium;new game theoretic;performative prediction;stochastic gradient;algorithms player alternates;game theoretic;algorithms player;stochastic gradient method;monotonicity game focus;gradient algorithms;repeated stochastic gradient;game theoretic framework;performative prediction establish;feedback mechanism learning;performatively stable equilibria;adaptive;methods adaptive;alternates learning", "pdf_keywords": "stochastic games;learning equilibria monotone;class stochastic games;learning equilibria;adaptive algorithm player;loss learning equilibria;stochastic games model;decision dependent games;player simultaneously learns;simultaneously learns ai;equilibria monotone games;learning methods players;learns ai optimizing;nonstationary optimization;simultaneously learns;stochastic programming;nonstationary optimization problems;algorithm player simultaneously;ai optimizing loss;learning problems arising;dependent games;algorithm player;monotone games fundamental;ai optimizing;monotone games;equilibria decision dependent;stochastic programming decision;learns ai;games fundamental problem;gradient based learning"}, "6eb974721719056ba8dc74a898c64ae1d081e0ae": {"ta_keywords": "quantum repeaters;class quantum repeaters;quantum repeaters based;entanglement;entanglement propose framework;based concept entanglement;concept entanglement;concept entanglement propose;usefulness diagnostic curves;entanglement propose;quantum;diagnostic curve input;diagnostic curves;diagnostic curve;class quantum;diagnostic curves multiple;new class quantum;black box model;diagnostic curves exact;univariate diagnostic curve;demonstrate usefulness diagnostic;repeaters;properties black box;black box;repeaters based;models;model curve;univariate diagnostic;repeaters based concept;line graph safety", "pdf_keywords": "greedy coordinate;trained predict direction;model directional dependence;deep neural;trained predict;zero coordinates greedy;directional dependence plots;unexpectedness dependence plots;coordinates greedy;greedy coordinate pairs;called greedy coordinate;coordinates greedy fashion;dependence plots;deep generative;dependence plots de\ufb01ning;directional dependence;network trained predict;greedy optimization;predict direction;generative model directional;unexpectedness dependence;network trained;greedy optimization scheme;greedy;predict direction person;deep generative model;neural;neural network trained;scheme called greedy;dependence plots paper"}, "4e2c41466c8246af0a563ea36fbe80c896bbab2c": {"ta_keywords": "translation systems;word sense disambiguation;machine translation systems;machine translation;translation systems difficult;translation based context;word context aware;sense disambiguation;context aware word;sense disambiguation literature;machine translation mt;translation mt systems;accuracy translating homographs;word sense feeding;years machine translation;speed accuracy translating;translating homographs;differentiate word sense;word sense;aware word embeddings;translation based;difficulty machine translation;accuracy translating;translating ambiguous words;translating homographs recent;global sentential context;encoder homographs words;sentential context;input word context;based context experiments", "pdf_keywords": "sense disambiguation improves;word sense disambiguation;translation based context;automatically translating homographs;multilingual context aware;sense disambiguation;word context aware;sense disambiguation literature;multilingual context;context aware word;translating homographs;neural machine translation;disambiguation improves;machine translation;propose multilingual context;word level translation;context word sense;statistical machine translation;resolving ambiguity homographs;ambiguity homographs;machine translation incorporating;automatically translating;translation incorporating;aware word embeddings;disambiguation;sense disambiguation wsd;disambiguation literature model;translation incorporating direct;disambiguation improves statistical;word context"}, "e9c52a3fac934919eca036909cc18d909db0d467": {"ta_keywords": "upscaling molecular dynamics;model upscaling molecular;continuum model upscaling;molecular dynamics;molecular dynamics presented;upscaling molecular;peridynamic continuum model;peridynamic continuum;continuum model;model upscaling;dynamics presented;peridynamic;dynamics;molecular;continuum;upscaling;model;presented", "pdf_keywords": "graining molecular dynamics;single layer graphene;graphene;layer graphene;coarse graining molecular;graphene results;graphene results suggest;layer graphene results;gradient descent;algorithm learns nonlocal;learns nonlocal operator;multiscale modeling;graining molecular;based gradient descent;molecular dynamics displacements;gradient descent descent;algorithm learns;multiscale modeling approach;operator based gradient;paper multiscale modeling;learns nonlocal;molecular dynamics;peridynamic solid model;material response linear;learning operator;coarse graining;learning operator external;data paper multiscale;optimization strategy learning;solid model md"}, "52ec4713343083e69b87e36a7a12c7b5898e2780": {"ta_keywords": "hmm speaker adaptation;speaker adaptation;model hmm speaker;speaker adaptation propose;features speech enhanced;speech recognition asr;automatic speech;speech recognition;speech features;automatic speech recognition;unprocessed noisy speech;noisy speech features;speech enhanced;model hmm;end automatic speech;speech enhanced pathway;adapting encoder network;speech features speech;adapting encoder;adapting neural beamformer;multichannel automatic repeat;features speech;performance adapting encoder;hmm speaker;noisy speech;end multichannel automatic;effective adapting neural;markov model hmm;using chime;asr integrated deep", "pdf_keywords": ""}, "4d1a14352ffb526a1fa0e1cd90e2484e188cddc0": {"ta_keywords": "training dialogue systems;dialogue data interaction;creating dialogue data;possibility creating dialogue;domain dialogues;dialogue data;dialogue user simulator;creating dialogue;source domain dialogues;dialogue systems;new dialogue scenarios;dialogue scenarios self;training dialogue;data interaction dialogue;domain dialogues allows;incorporate new dialogue;interaction dialogue;dialogue scenarios;dialogues;improvements dialogue performance;dialogue;dialogue systems lack;improvements dialogue;difficulties training dialogue;dialogue user;dialogue performance;dialogues allows converse;interaction dialogue user;dialogues allows;converse natural language", "pdf_keywords": "learning dialogue agents;learning dialogue;reinforcement learning dialogue;dialogue systems increasingly;dialogue systems;task oriented dialogue;results dialogue systems;dialog systems;taskoriented dialog systems;dialogue agents information;oriented dialogue systems;dialogue agents;dialog systems consider;human dialogues;human human dialogues;dialog;dialogue systems converse;dialogue level;improvements dialogue performance;improved results dialogue;dialogue;task success dialogue;dialogues;taskoriented dialog;oriented dialogue;improvements dialogue;human dialogues paper;language taskoriented dialog;dialogue performance;natural language taskoriented"}, "6d00b1024298e5b64ee873028385f7bb4396b05d": {"ta_keywords": "semantic parsing tasks;new semantic parsing;semantic parsing;neural networks compositional;algorithm semantic parsing;semantic parsing algorithm;parsing algorithm semantic;generalization ability semantic;comprehensive compositional generalization;syntactic algebra semantic;compositional generalization;compositional generalization ability;parsing tasks propose;ability semantic parsing;semantic parsing based;compositional generalization requires;parsing;limited compositional generalization;parsing tasks;compositional generalization benchmarks;parsing algorithm;networks compositional generalization;semantic algebra;syntactic algebra;algebra semantic algebra;algebra semantic;neural sequence models;parsing based homomorphism;parsing based;latent syntactic algebra", "pdf_keywords": "compositional semantic parsing;compositional generalization semantic;generalization semantic parsing;compositional semantic;generalization semantic parsers;learning compositional generalization;logic compositional semantic;generalization compositional logic;semantic parsing models;comprehensive compositional generalization;semantic parsing tasks;compositional generalization compositional;generalization compositional;compositional generalization;semantic parsing;semantic parsers algebraic;learning compositional;semantic parsers;partial meaning representations;machine learning compositional;compositional logic compositional;generalization semantic;compositional generalization important;parsing models;logic compositional;parsing models predict;compositional generalization benchmarks;model compositional generalization;compositional logic;unsupervised machine translation"}, "6ccac8a95bc77549b98d045db6d5e0de3d356ba4": {"ta_keywords": "bert based contextualized;bert models study;bert models;contextualized embeddings;context free neural;existing bert models;layer bert based;neural ranking;model layer bert;neural model ranking;neural ranking benefits;based contextualized embeddings;layer bert;contextualized embeddings does;retrieval present neural;interpretable neural;model ranking tensors;ranking tensors based;document embeddings;query document embeddings;neural model1 aggregator;lexical translation model;ranking tensors;use neural;context free contextualized;interpretable neural model;adding interpretable neural;tensors based context;neural;bert based", "pdf_keywords": "lexical translation models;learns contextindependent translation;token embedding neural;context dependent embeddings;lexical translation model;translation model learns;translation models;translation models recent;dependent embeddings interpretability;utility lexical translation;embeddings interpretability;token embedding;embedding networks;parametric lexical translation;ranking models interpretable;model text retrieval;dependent embeddings;translation model ibm;contextindependent translation probabilities;models interpretable neural;translation model;embedding networks answer;embeddings;output token embedding;embedding neural;embeddings interpretability explainability;parallel corpus;lexical translation;learns contextindependent;interpretable neural model"}, "69c515a62403fcc19125d3a6dd8e878aa5cde604": {"ta_keywords": "dialogue systems;plant dialogue systems;dialogue systems open;large conversation data;conversation data;dialogue;plant dialogue;200k plant dialogue;conversation data development;challenge frequent coreference;winning agents;probabilistic inference model;probabilistic inference;list winning agents;conversation;winning agents agent;propose probabilistic inference;inference model dynamics;large conversation;frequent coreference information;success large conversation;inference model;agent assigned probability;frequent coreference;inference;deep learning;probabilistic;propose probabilistic;paper propose probabilistic;agents", "pdf_keywords": "utterance restoration boosting;model utterance restoration;learning entire utterance;turn dialogue systems;domain dialogue systems;utterance restoration;dialogue systems existing;dialogue systems;model utterance;restoration incomplete utterance;turn dialogue paper;sequence labeling;appropriate model utterance;propose sequence labeling;turn dialogue;learning translate source;sequence labeling autoregressive;utterance open domain;multi turn dialogue;learning translate;dialogue;domain dialogue;dialogue systems multi;dialogue paper;utterance open;target texts;conversation seq2seq model;dialogue paper propose;utterance;open domain dialogue"}, "d2f327736c9b68f68ad64d0b1cefed9b4dd83313": {"ta_keywords": "structured prediction learns;imitation learning structured;learning structured prediction;novel structured prediction;structured prediction;learning structured;structured prediction framework;imitation learning;manually constructed linguistic;prediction learns incremental;prediction learns;outputs natural language;generating natural language;learns incremental model;learns incremental;training data phrase;use imitation learning;prediction framework unaligned;natural language generation;constructed linguistic;natural language;data phrase templates;imitation;rely aligned training;learning based unsupervised;propose novel structured;unsupervised learning;datasets using automatic;unaligned datasets;linguistic resources machine", "pdf_keywords": ""}, "e37fb85e8869d464bae8eeebf4cd9321ec8c70ad": {"ta_keywords": "interpretability machine learning;interpretable classifiers;classification view interpretability;accuracy interpretability restriction;statistical cost interpretability;interpretability performing empirical;accuracy interpretability;restriction interpretable classifiers;interpretability restriction interpretable;interpretability machine;interpretability;interpretable classifiers does;enforcing interpretability;interpretability restriction;interpretable hypotheses propose;interpretability model;implications enforcing interpretability;cost interpretability;interpretability terms;definition interpretability;cost interpretability machine;interpretability using;definition interpretability model;interpretability constraint;interpretability terms paper;interpretable hypotheses;interpretability using known;enforcing interpretability using;interpretability performing;view interpretability", "pdf_keywords": "enforcing interpretability learning;learning interpretable classifiers;interpretable classifiers;interpretability constraint learning;interpretability learning;learning interpretable;method learning interpretable;offs accuracy interpretability;interpretability tradeoff;interpretable classifiers perform;interpretability tradeoff machine;interpretability learning begin;accuracy interpretability;quantify interpretability;quantify interpretability instead;study interpretability tradeoff;interpretability performing empirical;enforcing interpretability;specific notion interpretability;model interpretability;define interpretability;accuracy interpretability showing;interpretability;interpretability model interpretability;interpretability constraint;define quantify interpretability;interpretability showing possible;model interpretability constraint;notion interpretability;interpretability instead"}, "8b0b2b69657076fc1ce7cce75a6d69e3e5ba2d63": {"ta_keywords": "head collision bouncer;head collision;head head collision;collision bouncer turned;collision bouncer;video head head;bouncer turned;home head head;collision;video head;home head;bouncer turned home;bouncer;head head;head;turned home head;video;turned;home;turned home", "pdf_keywords": ""}, "6a173e22819480b891306eac65fd44be010dfca8": {"ta_keywords": "immune response influenza;responses influenza virus;host responses influenza;response influenza virus;responses influenza;inflammation macaque complex;influenza virus;influenza virus infection;response influenza;defend influenza virus;influenza virus induced;virus induced inflammation;induced inflammation macaque;signaling inflammation;inflammation macaque;mitigating viral pathogenesis;immune cell signaling;cell signaling inflammation;viral pathogenesis;dynamics immune response;defend influenza;infected macaque lungs;targeted mitigating viral;influenza;analysis dynamics immune;signaling inflammation dyregulatory;mitigating viral;ca04 infected macaque;better defend influenza;immune response", "pdf_keywords": ""}, "1d2a2b14ef14eeaf89169f738f7634cdc685c785": {"ta_keywords": "built similarity predicate;similarity predicate;similarity predicate term;database relational;particle query language;relation contains virus;relational database;relational database relational;relational;highest score relational;database relational database;predicate term vectors;relational database cell;query language;built similarity;contains built similarity;particle query;database cell relation;language query language;database;field particle query;score relational;score relational database;sql;similarity;relation contains;relation;query language query;query language defined;language query", "pdf_keywords": ""}, "c968e8dc442102b38b134b1afadc7cc78fc5b5fb": {"ta_keywords": "language processing models;natural language processing;language processing;natural language;class natural language;holistic metrics accuracy;new evaluation;propose new evaluation;evaluation;new evaluation method;processing models;processing models make;metrics accuracy accuracy;metrics accuracy;evaluation method enables;evaluation method;holistic metrics;accuracy f1;url holistic metrics;accuracy;accuracy accuracy f1;accuracy f1 perform;models make easy;language;models;accuracy accuracy;performance class natural;performance;identify strengths weaknesses;metrics", "pdf_keywords": "model natural language;character language models;language models;language models handle;trained character language;pre trained character;natural language processing;character language;language processing;natural language;trained character;trained pre trained;models handle entities;model attribute bucket;attribute bucket wise;entities lower label;model attribute;attribute;approach pre trained;pre trained;attribute bucket;approaches model attribute;concept performance table;trained pre;pre trained pre;model wise analysis;character;entities;automatic allow evaluation;analy sis approaches"}, "7a56aba1a4d4020c4933319588b9ed2b34d51125": {"ta_keywords": "logistic regression secure;regression secure;regression secure multi;security spdz framework;security spdz;multi party computation;secure multi party;cryptography enables computation;demonstrate security spdz;computation sensitive data;secure multi;privacy guarantees demonstrate;privacy guarantees;cryptography enables;maintaining privacy guarantees;malicious security;privacy;computation sensitive;cryptography;popular machine learning;maintaining privacy;protocol malicious security;providing stronger security;secure;party computation mpc;enables computation sensitive;security;party computation;stronger security;mpc area cryptography", "pdf_keywords": "decomposition malicious security;decomposition malicious;framework party computation;direct decomposition malicious;malicious security;privacy policies;malicious security dishonest;strict privacy policies;regression learning;protocol malicious security;linear regression learning;privacy;honest mpc techniques;mpc protocol malicious;logistic regression implemented;party computation;semi honest mpc;party computation paper;regression learning using;regression implemented;machine learning algorithms;security example;parties strict privacy;strict privacy;popular machine learning;security dishonest parties;using semi honest;protocol malicious;learning algorithms;malicious security context"}, "e02f79b710cdcaa9135b835fad964f6f2c78b1a7": {"ta_keywords": "tokenization vocabulary pre;explicit tokenization vocabulary;tokenization vocabulary;multilingual scoring;canine neural encoder;multilingual scoring scoring;multilingual multilingual scoring;nlp systems short;representation human language;directly character sequences;loop nlp systems;tokenizers;closed loop nlp;model tokenization approaches;sequences explicit tokenization;model tokenization;tokenization approaches;tokenizers techniques equally;neural encoder;character sequences;tokenizers techniques;sequence length deep;trained machine representation;tokenization approaches based;explicit tokenization;character sequences explicit;tokenization;machine representation human;end end neural;end neural", "pdf_keywords": "tokenization free monolingual;tokenizationfree vocabulary;tokenization free modeling;entirely tokenization free;tokenizationfree vocabulary free;uses tokenizationfree vocabulary;purely tokenization free;encoder entirely tokenization;tokenization free manner;subword tokens;tokenizers;entirely tokenization;tokenization free;subword tokens purely;purely tokenization;tokenizers paper propose;tokenization;tokenizationfree;explicit tokenization;tokens purely tokenization;built heuristic tokenizers;understanding uses tokenizationfree;heuristic tokenizers;trainable language model;tokenizers paper;language model learns;machine translation systems;uses tokenizationfree;require explicit tokenization;free monolingual multilingual"}, "4572ded23106285cbd8ebbe6c3b354973ac06ff7": {"ta_keywords": "motion pedestrian crowded;pedestrian crowded environment;crowded environment motion;pedestrian crowded;motion pedestrian;environment motion pedestrian;study motion pedestrian;crowded environment described;crowded environment;pedestrian;stochastic;described stochastic;stochastic process;environment described stochastic;motion;described stochastic process;crowded;environment motion;study motion;paper study motion;process;environment described;environment;paper study;described;paper;study", "pdf_keywords": ""}, "e214d2a6399925ce60fa5ce90c0374127a32b47e": {"ta_keywords": "impromptu deployment wireless;algorithms converge deployment;approaches impromptu deployment;deployment wireless sensor;relay deploys;impromptu deployment deployment;deploys relays locations;sensor networks;algorithms deployment large;impromptu deployment;networks field deployment;wireless sensor networks;deployment wireless;deploys relays;wireless path;deployment agent forward;algorithms deployment;multihop wireless path;converge deployment progresses;location explore forward;sensor networks demonstrate;deployment progresses;relays locations aim;relays locations;converge deployment;relay deploys relays;placement location explore;applications impromptu deployment;previous relay deploys;deployment deployment agent", "pdf_keywords": "wireless relay deployment;deployment wireless sensor;wireless relay placement;algorithm deployment wireless;relay placement wireless;algorithm wireless relay;placement wireless sensor;wireless sensor networks;approximation algorithm wireless;wireless sensor network;placement wireless;sensor networks;networks sensor sink;sensor networks impromptu;deployment wireless;deployment mobile wireless;wireless networks sensor;relay deployment;sensor networks paper;relay deployment given;networks impromptu deployment;wireless networks;sensor network point;sensor network;algorithm wireless;wireless relay;locations placing relay;placing relay nodes;networks sensor;relay placement"}, "7ee55c115470e1b86e552c5594e2e4258b4ccefb": {"ta_keywords": "energetics fe xmath0;xmath1 fe nn;nx xmath1 fe;fe xmath0 nx;xmath1 fe;fe xmath0;energetics fe;fe pn nx;study energetics fe;nx xmath1;fe pn;fe nn pn;pn fe;nn pn fe;xmath0 nx;xmath0 nx xmath1;fe nn;pn fe pn;xmath1;fe;xmath0;theoretical study energetics;nx pn;pn nx;se reaction;energetics;pn nx pn;nx pn se;se se reaction;nx", "pdf_keywords": ""}, "a7766d4c41df235764dfaa9971ce861f6120ac27": {"ta_keywords": "superconducting quantum interference;interference device squid;quantum interference device;superconducting quantum;quantum interference;device squid propose;device squid;generation superconducting quantum;quantum;superconducting;interference device;squid propose;squid propose new;measurement position particle;squid;information captured microphone;microphone;recognition motion particle;captured microphone;motion particle noisy;particle noisy environment;particle noisy;interference;generation superconducting;new generation superconducting;noisy environment based;captured microphone array;recognition motion;automatically recognizes speaks;position particle environment", "pdf_keywords": ""}, "252ef125a8874fe8face4540f87f2e000275cc96": {"ta_keywords": "cluster perpetrators searching;measure cluster perpetrators;communities terrorist groups;cluster perpetrators;terrorism research;networks communities terrorist;communities terrorist;terrorism research different;task terrorism research;similarity measure cluster;terrorist groups active;clustering;terrorist groups;clustering algorithm;similarity similarity measure;similarity measure;terrorism;clustering algorithm designed;perpetrators searching;similarity similarity;perpetrators searching hidden;innovative clustering algorithm;key task terrorism;task terrorism;coefficient similarity similarity;cluster;networks communities;similarity;innovative clustering;partite networks communities", "pdf_keywords": ""}, "803c7fdd6e01e1ff8cd43297f4e052078409456d": {"ta_keywords": "continual hierarchical adaptation;strangers influence communicative;interacting social network;continual learning adaptation;continual learning;bayesian theory coordination;interacting social;continual hierarchical;introduce continual hierarchical;interaction partner gradual;humans interacting social;inference chai hierarchical;hierarchical adaptation inference;transmission continual learning;communicative context conventions;communicative context;communication simply;communication;repeated interaction;strangers influence;words acquire new;repeated interaction partner;communicative;coordination;social network argue;theory coordination;context conventions eventually;influence communicative context;chai hierarchical bayesian;influence communicative", "pdf_keywords": "language interaction;expectations language interaction;language acquisition;model language acquisition;dynamics language acquisition;language interaction knowledge;social beliefs language;language acquisition group;language acquisition processing;beliefs language hierarchical;language;predicting speakers believe;expectations language;language hierarchical;repeated reference games;beliefs language;language hierarchical structure;repeated reference paradigm;prior expectations language;hoc conventions emerge;reference games typically;participants repeated reference;social observations;interaction knowledge;present model language;interaction knowledge adaptation;audience present computational;investigate dynamics language;uncertainty human participants;knowledge adaptation"}, "c6488f0c62ee4a4d48d0fbf8e8185655226294c1": {"ta_keywords": "moments avalanches attack;attack detection probability;attack detection;fading blocks attacker;receiver security attack;attack detection models;attack detection given;detection given fading;moments formulate attack;probability receiver security;moment based detection;different attack detection;detection using moments;avalanches attack;attack use moments;attack design;receiver security;using moments avalanches;avalanches attack use;detection probability receiver;based attack detection;security attack;attack design problem;formulate attack design;attacker manipulate ris;snr moment based;consider threshold detection;security attack demonstrated;testing based attack;moments avalanches", "pdf_keywords": "attack detection problem;attack detection;different attack detection;attack detection given;fading blocks attack;attack detection models;channel gains attacker;model attack detection;attacks remote state;blocks attack detection;attacks remote;based attack detection;detection given fading;attacker adversarially;gains attacker adversarially;mathematical model attack;testing based attack;attacks;attacker adversarially controls;injection attacks remote;block known channel;consider different attack;data injection attacks;model attack;blocks attack;communicates single antenna;remote state estimation;injection attacks;based attack;wireless communication"}, "2e4cdd36d77b9d814638fc2cd6c703535cb1d2f7": {"ta_keywords": "prompt tuning nlgains;generation nl tasks;language generation nl;nlg tasks demonstrate;linguistically different pretraining;pretrained language model;nlgains unfamiliar inputs;nlg tasks;frozen pretrained language;tuning nlgains unfamiliar;pretrained language;natural language generation;seven nlg tasks;different pretraining corpus;pretraining corpus proposed;language generation;pretraining corpus;prompt tuning;tuning nlgains;better prompt tuned;understanding nl tasks;nl tasks underexplored;tuning continuous prompts;generation nl;language understanding nl;prompt tuned;consistently better prompt;nl tasks;nlg;development prompt tuning", "pdf_keywords": "machine translation tasks;translation performance prompt;machine translation performance;translation tasks;prompt tuning;machine translation;input tuning neural;translation tasks demonstrate;seven machine translation;80 machine translation;translation performance;prompt tuning appealing;prompt tuning propose;task input generating;propose input tuning;prompttuning;prompttuning making;prompttuning making transformation;input unfamiliarity essential;input unfamiliarity;prompt prompttuning making;tuning propose input;performance prompt tuning;prompting means prepending;tuning neural;propose input tuned;prompt prompttuning;making transformation learnable;prompting prompting means;examples task input"}, "4c2d9136c579a0393d4f50bbbbc6f8dab43c38e9": {"ta_keywords": "sources weighted prediction;estimation source priors;speech recognition;weighted prediction;performance automatic speech;automatic speech;evaluation blind estimation;automatic speech recognition;weighted prediction error;blind estimation;speech recognition asr;speaker scenario;source priors;blind estimation shape;speaker;distant speaker scenario;prediction error wpe;generalized gaussian prior;gaussian sources depending;gaussian sources;shape parameter priors;gaussian prior shape;distant speaker;super gaussian sources;gaussian prior;estimation source;signal processing wpe;asr distant speaker;weighted average weights;speaker scenario propose", "pdf_keywords": ""}, "6f49026ff623c64ce6de81fd04cf6e1ffe7dd6d9": {"ta_keywords": "gans learn generate;study convolutional gan;binary valued piano;networks gans learn;networks gans;gans learn;convolutional gan model;convolutional gan;valued piano rolls;gan model;music binary valued;using binary neurons;piano rolls;generation piano rolls;gan model directly;learn generate music;real valued piano;gans;deep convolutional generative;convolutional generative adversarial;generate music;piano rolls require;convolutional generative;binary neurons perform;represent music binary;form piano rolls;generative adversarial networks;generative adversarial;valued piano;deterministic binary neurons", "pdf_keywords": "prediction track piano;generating piano rolls;generating piano;generate piano rolls;learn generate piano;generate piano;generator generating piano;track piano rolls;piano rolls;piano rolls previous;piano rolls using;piano;track piano;music generation;piano rolls challenging;work music generation;neural network rnn;gan 12 learn;recurrent neural network;rnn model prediction;generative adversarial;generative adversarial network;convolutional generative adversarial;recurrent neural;propose recurrent neural;rnn model;music;network gan;instruments;adversarial network gan"}, "57e7be6b404abfd7a56a73c0ff9bccc5b27ad7ae": {"ta_keywords": "auxiliary language modeling;translation models;domain specific representations;models domain aware;domain aware feature;current fcnc model;language modeling;language modeling task;translation models used;embeddings learned auxiliary;fcnc model neutral;super proton synchrotron;sentences desired domain;feature embeddings learned;aware feature embeddings;representations words output;model neutral current;model domain specific;domain aware;paper translation models;embeddings learned;model domain;proton synchrotron;fcnc model;output sentences;feature embeddings;translation improve performance;proton synchrotron sps;neutral current fcnc;bnl super proton", "pdf_keywords": "unsupervised machine translation;unsupervised domain adaptation;machine translation adapts;machine translation monolingual;neural machine translation;translation model learns;domain adaptation neural;domain adaptation;learns domain embeddings;machine translation model;monolingual data translation;text machine translation;domain copied monolingual;machine translation;translation adapts;machine translation attracted;translation model;domain adaptation strategy;domain embeddings text;domain adaptation technique;translation adapts model;translation monolingual corpora;learns domain;data translation;translation monolingual;novel unsupervised domain;machine translation work;copied monolingual data;domain embeddings;settings domain adaptation"}, "2b3ab7e9c66bffc7af9e4413036e7bba686a7734": {"ta_keywords": "reviewers exhibit bias;competent reviewers growing;peer review submissions;reviewers tend underrate;peer review;reviewers tend;reviewers growing;novice reviewers tend;observe novice reviewers;reviewers growing slower;quality peer review;novice reviewers;competent reviewers;reviewers;reviewers exhibit;focusing population reviewers;population reviewers;reviewers likely receive;investigate reviewers;reviewer pool;review submissions;investigate reviewers exhibit;number competent reviewers;reviewer pool leading;reviewers likely;review submissions challenges;reviewers constitute;reviewers constitute large;surge peer review;submission review", "pdf_keywords": "bias reviewers evaluations;bias reviewers;bias reviewers likely;bias bias reviewers;negatively bias reviewers;peer review experiment;peer review;reviewers evaluations;performance peer review;study resubmission bias;reviewers evaluations leading;resubmission bias overall;biases human judgement;blind peer review;resubmission bias bias;peer review pipeline;resubmission bias;peer reviewed hypothesize;bias past research;review experiment participants;impact resubmission bias;resubmission bias past;peer reviewed;learning conference reviewer;participants reviews;bias bias;previously peer reviewed;cognitive biases;reviewers likely accept;reviewers"}, "536ce077f08886b5834b639da25068d877c98b2c": {"ta_keywords": "langevin equation solved;langevin equation;video langevin equation;dynamics video langevin;video langevin;langevin;fluid dynamics;fluid dynamics video;dynamics;equation solved exactly;dynamics video;equation solved;fluid;equation;solved exactly;solved;video;exactly", "pdf_keywords": ""}, "b54efe01969adaa1c623331d5791897a4dd9f886": {"ta_keywords": "information extraction flybase;fruit fly genomics;automatically information flybase;fly genomics;genomics incorporation flybase;extraction flybase;extraction flybase curation;gene recognition bootstrapped;interactive information extraction;adaptive information extraction;information extraction designed;information flybase;flybase adaptive information;fly genomics incorporation;information flybase adaptive;information extraction;paperbrowser flybase employs;perform gene recognition;gene recognition;flybase curation paper;paperbrowser flybase;genomics;flybase curation;user based experiments;data derived automatically;using paperbrowser flybase;bootstrapped training data;flybase adaptive;datasets flyslip project;flybase", "pdf_keywords": ""}, "0e8da301b098f96fed39c5aa8e2194f678690a16": {"ta_keywords": "deterministic nodal eavesdropping;secret sharing communication;algorithm secret sharing;eavesdropping problem admits;secret sharing;nodal eavesdropping;network secret sharing;nodal eavesdropping problem;case nodal eavesdropping;disseminating shares secret;general network secret;efficient distributed deterministic;distributed deterministic;eavesdropping;eavesdropping problem;network secret;shares secret;secret sharing important;distributed deterministic nodal;admits distributed deterministic;nodal eavesdropping little;secure multiparty;efficient distributed;cryptographic;distributed deterministic solution;sharing communication efficient;cryptographic protocols;cryptographic protocols present;algorithm secret;eavesdropping little", "pdf_keywords": ""}, "73569460b023f9ac1fe5a1876c3401460d2fc15d": {"ta_keywords": "code clone detection;code clone;classification code clone;exploit semantic preserving;source code understanding;clone detection code;code search;source code;detection code search;semantic preserving transformation;trained models code;code related tasks;tasks source code;semantic preserving;code search paper;clone detection;exploit semantic;models learn semantic;learn semantic;learn semantic features;code understanding;semantically equivalent transformations;code understanding introduce;trained models pretrain;code related;introduce curriculum learning;trained models learn;pre trained models;existing pre trained;curriculum learning", "pdf_keywords": "learn code semantics;augmentation learn code;code summarization tasks;learn code;code retrieval;retrieval code summarization;code pre trained;code semantics semantic;code summarization;code retrieval code;semantic preserving training;framework code retrieval;learning framework code;code semantics;trained models codebert;data augmentation learn;language pre trained;curriculum learning;similar curriculum learning;retrieval code;programming languages models;enrich training dataset;semantic preserving;vision natural language;semantics semantic preserving;natural language pre;source code;incorporate semantic knowledge;programming;self supervised contrastive"}, "23f1d4b46bc7c8f357a5a89144d5d32af7be13a5": {"ta_keywords": "trained language models;trained language;pre trained language;pre training models;tuned large summarization;training models content;large summarization datasets;learnt;learnt iterations;training models;language models;learnt iterations paper;large summarization;learnt earlier training;summarization datasets copy;learning;strategies learnt iterations;summarization datasets;generation strategies learnt;learnt earlier;learning process learnt;strategies learnt;process learnt;results training;correct pre trained;brain able predict;language models shown;present results training;results fine tuned;able predict outcome", "pdf_keywords": "training hypothesize summarization;summarization models abstractiveness;hypothesize summarization models;abstractive summarization models;factuality automatic summarization;summarization models;summarization models make;hypothesize summarization;abstractive summarization;components abstractive summarization;automatic summarization complex;consistency trainable content;summarization complex;trainable content;automatic summarization;factuality abstractiveness roughly;trainable content selection;abstractiveness factual consistency;summarization;factuality abstractiveness;trainability predictive consistency;summarization complex task;factuality automatic;abstractiveness factual;factuality compromising abstractiveness;models abstractiveness factual;non factuality abstractiveness;dataset emulate gram;predictive consistency trainable;consistency investigate learned"}, "9389af659f14239319186dff1cef49e8ece742c8": {"ta_keywords": "expressive graph ml;graph ml models;core graph learning;graph learning;graph learning tasks;improve graph ml;graph learning node;graph data fluid;expressive graph;advances graphs billions;graph ml;large scale graph;scaling expressive graph;graphs billions;graphs billions edges;graph ml scale;graph data;scale graph data;core graph;advances graphs;learning node;learning node classi\ufb01cation;billions edges;learning large scale;models massive datasets;massive datasets;massive datasets paper;graphs;machine learning large;present core graph", "pdf_keywords": "graph ml deep;learning hyperlinked graphs;large scale graphs;graph neural networks;large scale graph;graph neural;machine learning hyperlinked;hyperlinked graphs fact;hyperlinked graphs;links graphs;graph ml;nodes links graphs;fact graph neural;scale graph ml;ml deep learning;learning hyperlinked;links graphs respectively;graph models;graph models leaving;graphs fact graph;nodes links;dataset community challenges;expressive models train;deep learning;datasets unprecedentedly large;train big data;scale graphs challenging;deep learning demonstrated;performance simple graph;ml deep"}, "c68349ba142be731d6f3339d894764921c69b774": {"ta_keywords": "twitter predict obesity;individuals took quiz;social media driven;twitter predict;classification response questions;individuals asked classify;based supervised;future classification response;classification response;media twitter predict;data future classification;train public health;predict obesity operate;based supervised learning;supervised learning approach;approach based supervised;predict obesity;supervised;supervised learning;classification;training data;classify;future classification;classifier;quiz available online;questions style quiz;learning approach;classifier 20 questions;twitter;social media twitter", "pdf_keywords": "obesity detection automatically;algorithm obesity detection;social media driven;social media data;trained social media;questions acquired twitter;trees social media;obesity detection;like quiz data;like quiz;algorithm obesity;quiz data;twitter;twitter social media;diabetes using game;game like quiz;collection algorithm obesity;twitter social;tracking diseases predicting;model natural language;decision trees social;twitter recently;social networking propose;twitter important;twitter important component;especially twitter important;propose semiautomated data;natural language questions;especially twitter recently;monitoring tracking diseases"}, "6665e03447f989c9bdb3432d93e89b516b9d18a7": {"ta_keywords": "particle inertial range;fast moving particle;moving particle inertial;method controlling speed;controlling speed fast;particle inertial;controlling speed;moving particle;inertial range;speed fast moving;inertial;speed fast;fast moving;speed;particle;new method controlling;method controlling;fast;controlling;range;moving;present new method;new method;method;paper;present new;paper present new;paper present;new;present", "pdf_keywords": ""}, "e92de0c4ef62a84201fac284eb66c37330b5fe1c": {"ta_keywords": "fixing bugs occasions;helpful fixing bugs;fixing bugs;automated program repair;generate fix code;helpful correcting bugs;correcting bugs;program repair;program repair aims;bug prone code;generate fix;code past fixes;bugs occasions fix;patch generation using;patch generation;corrective patch generation;repair subset bugs;considered helpful fixing;able generate fix;given bug prone;fix code;helpful fixing;fixes adopted code;neural machine translation;ratchet generate syntactically;fixes neural;ratchet corrective patch;actual fixes adopted;fixes neural sequence;machine translation survey", "pdf_keywords": "predict machine translation;neural machine translation;machine translation statements;machine translation;novel machine translation;translation machine translation;machine translation machine;machine translation technique;translation technique automatically;translation machine;machine translation paper;translation statements;translation technique;generate validate patches;patch generation using;translation statements evaluate;patch generation;corrective patch generation;generation using neural;usefulness generated statements;automatically predict machine;collecting source code;automatically generate validate;snippets works better;generated statements;validate patches;patterns reusable snippets;reusable snippets;approach automatically generate;validate patches way"}, "90848c88f56fcd421ac3cfd2c87d3e61211103ea": {"ta_keywords": "dynamics head collision;head collision rigid;collision rigid body;head collision;collision rigid;surrounding rigid body;rigid body surrounding;body surrounding rigid;rigid body;dynamics head;surrounding rigid;collision;study dynamics head;dynamics;rigid;theoretical study dynamics;body surrounding;head;study dynamics;results theoretical;theoretical study;present results theoretical;theoretical;results theoretical study;surrounding;body;paper present;paper present results;study;paper", "pdf_keywords": ""}, "051a85bd1384767ea5882dcefa98aee5664aa2cf": {"ta_keywords": "unsupervised inference tasks;unsupervised inference model;unsupervised inference;tasks unsupervised inference;networks speech processing;neural networks tremendously;networks speech;neural networks;inference tasks adaptation;deep neural networks;particle lattice;novel networks speech;efficient spectral clustering;particle lattice way;deep neural;particle optical lattice;inference tasks;addition unsupervised inference;adaptation clustering;tasks adaptation clustering;networks based nonnegative;processing network inspired;nonnegative matrix factorization;speech processing;novel networks;flexible tasks unsupervised;networks;result novel networks;spectral clustering;unsupervised", "pdf_keywords": ""}, "6fde1c63b1a353cf539d319341ae9396000660ed": {"ta_keywords": "automatic pronunciation assessment;pronunciation assessment based;automatic pronunciation proficiency;pronunciation evaluation systems;pronunciation assessment gop;automatic pronunciation evaluation;technique automatic pronunciation;robustness automatic pronunciation;pronunciation assessment;teacher automatic pronunciation;method automatic pronunciation;automatic pronunciation;based pronunciation scoring;pronunciation proficiency estimation;pronunciations evaluation methods;pronunciation evaluation method;evaluating proficiency pronunciations;good pronunciation evaluation;pronunciation scoring;pronunciation evaluation;systems good pronunciation;pronunciations evaluation;proficiency pronunciations noisy;gop based pronunciation;pronunciations noisy classroom;pronunciation proficiency;correct pronunciations evaluation;proficiency pronunciations;pronunciation scoring baseline;proficiency correct pronunciations", "pdf_keywords": ""}, "e03d9684a19c8f8e29ee97b347d4f1e280a88e44": {"ta_keywords": "gestures language grounding;spoken language gestures;gestures spoken language;approaches gestures language;language gestures;gesture space;gestures language;gesture aware space;similarities gesture space;gesture aware space_;generate relevant gestures;language grounding crossmodal;gestures spoken;concept gesture aware;language gestures gesture;gestures given language;gesture aware;timed gestures spoken;space gesture aware;gestures given;gestures;aware space gesture;gesture accompany semantically;relevant gestures given;prior approaches gestures;gestures gesture;relevant timed gestures;language grounding;gestures gesture accompany;approaches gestures", "pdf_keywords": ""}, "d31b5b60e1b3af84cd977da8db0ed4faeb79e7f7": {"ta_keywords": "sentiment transfer trained;predicting style unlabeled;text style transfer;sentiment transfer;predicting style;style transfer algorithm;method predicting style;style transfer;perform style transfer;competitive sentiment transfer;style transfer unlike;new style transfer;style unlabeled text;requiring style labeled;style labeled;extract style;style labeled training;style transfer demonstrate;extract style vector;style vector text;text style;transfer trained labeled;style recast transfers;able extract style;unlabeled text based;trained labeled data;style vector space;style unlabeled;style adjacent sentences;text based", "pdf_keywords": "transferring sentiment style;style transfer sentences;inducing text style;sentiment transfer;transferring sentiment;text style representations;sentiment transfer compared;style transfer language;language style transfer;style extraction;sentiment style;architecture transferring sentiment;introduce style extraction;sentiment style arbitrary;style transfer;style extraction deletion;competitive sentiment transfer;transfer language style;transfer sentences arbitrary;textual style extracted;transfer sentences;style transfer model;style representations;inducing text;style transfer use;textual style;style transfer seen;style attributes language;representation textual style;machine translation"}, "5812e30eb4756aeaf0b013a65b98f8f8aa0f8315": {"ta_keywords": "domain adaptation soccer;adaptation soccer paper;net language models;adaptation soccer;language models;neural net language;hierarchical phrase based;language models perform;domain adaptation;perform domain adaptation;ability neural net;phrase based phrasebased;based phrasebased pre;phrase based;neural net;net language;hierarchical phrase;string hierarchical phrase;based phrasebased;soccer paper;phrasebased pre ordering;2015 season systems;risk combination smt;bayes risk combination;risk combination;soccer paper presents;forest string hierarchical;neural;phrasebased pre;net", "pdf_keywords": ""}, "cf2a953dc82115d34de51737fef46bf3ff4cd5a6": {"ta_keywords": "person identity video;recognition person identity;person identified video;identity video clip;recognition person;video shows person;identity video;identify person;person identity;person identified;shows person identified;identified video clip;identified video;video used identify;method recognition person;used identify person;recognition;video clip video;clip video shows;clip video;video clip;shows person;clip video used;person;identity;video shows;video;identified;video used;identify", "pdf_keywords": ""}, "c44addf352f25f28f69ca9f9422c0e463783206f": {"ta_keywords": "word synonym extraction;synonym extraction;word similarity measure;similarity measure parsing;synonym extraction consider;parsed text corpus;text corpus;graph based similarity;text based graph;word similarity;term extraction;specific word similarity;measure parsing text;dependency parsed text;text corpus instance;nodes represent words;parsing text based;information graph walk;words weighted directed;corpus;similarity measure graph;corpus instance labeled;term extraction general;coordinate term extraction;represent words weighted;extraction general word;based similarity measure;labeled directed graph;parsing text;new similarity measure", "pdf_keywords": ""}, "9c16dcbcdfe6991f5d448543e6f4cbdf37149883": {"ta_keywords": "crucial multimodal tasks;multimodal tasks;multimodal tasks visual;multimodal machine learning;interactions crucial multimodal;crucial multimodal;multimodal;multimodally additive;empirical multimodally additive;empirical multimodally;multimodally;multimodal machine;researchers multimodal machine;multimodally additive function;researchers multimodal;modal interactions improve;modal feature interactions;tool empirical multimodally;expressive cross modal;visual question answering;cross modal interactions;cross modal feature;performance unimodal baselines;modal interactions crucial;modal interactions results;unimodal baselines emap;cross modal;modal interactions;performance unimodal;recommend researchers multimodal", "pdf_keywords": "cross modal attention;multimodal classi\ufb01cation tasks;forward neural;modal attention;feed forward neural;forward neural network;aided reasoning tasks;reasoning tasks represented;reasoning tasks;ef\ufb01cientnet b4 features;features input pretrained;multimodal;attention;multimodal classi\ufb01cation;neural;visual features;textual visual features;aided reasoning;features input;computer aided reasoning;neural network;tasks represented weighted;unimodal boolean target;unimodal boolean;features;b4 features input;combination textual visual;ef\ufb01cientnet b4;reduced unimodal boolean;tasks reduced unimodal"}, "335bf6f23ccdae43e45a7c12f33bc4f3488e3762": {"ta_keywords": "neural machine translation;machine translation learn;multi source translation;machine translation;incomplete corpora training;translation single source;improve translation performance;translate incomplete corpora;significantly improve translation;languages machine translation;source translation single;machine translation rife;source translation approach;corpora languages machine;incomplete corpora;translation performance multi;translation performance;complete corpora languages;corpora training;complete corpora;corpora training time;learn translate incomplete;source neural machine;corpora languages;multi source neural;translation learn;corpora;translation single;source translation;large corpora", "pdf_keywords": ""}, "82459c972cc1e439c759010acf7ddce1a89b66e0": {"ta_keywords": "semantic frame induction;semantic class induction;synonymy graph unsuper;networks linguistic;induction synonymy graph;fuzzy graph clustering;networks linguistic data;unvised semantic;unsuper supervised semantic;unsupervised synset induction;supervised semantic frame;graph unsuper supervised;unvised semantic class;induction distributional thesaurus;semantic frame;graph clustering;synset induction synonymy;applied networks linguistic;graph based clustering;graph clustering widely;supervised semantic;ways unsupervised synset;linguistic data;synonymy graph;distributional thesaurus;unsupervised synset;linguistic data algorithm;clustering discover;frame induction dependency;distributional thesaurus present", "pdf_keywords": "word sense induction;word clusters;resources word clusters;word clusters computational;word clusters sets;sense aware clustering;induction word networks;term sense embedding;semantic class induction;sense induction automatic;word networks;semantic classifier;semantic lexicon;overlapping word clusters;graph based word;sense induction;semantic classifier present;fuzzy graph clustering;clusters sets semantically;frame induction word;word sense;based word sense;semantic lexicon 2016;word networks paper;sense embedding;semantic relationships words;based semantic classifier;sense induction important;word based resources;natural language processing"}, "684821e2459c7fc3ef8a2ec8102678af3613a962": {"ta_keywords": "speech representation learning;downstream speech applications;downstream speech;self supervised models;available self supervised;self supervised learning;self supervised;speech representation;speech processing community;membrane video;speech tasks;approach speech representation;speech processing;speech tasks limited;speech applications;wetting membrane video;use self supervised;multiple speech tasks;membrane video shows;quality learned representations;range downstream speech;changes speech processing;supervised;supervised models demonstrate;representation learning;supervised learning;learned representations;speech applications paper;learned representations wide;supervised minimal architecture", "pdf_keywords": ""}, "2467b2daea0398709d7ea57d084cc1f00f9d168f": {"ta_keywords": "sequence recognition multi;speech recognition alr;conditionalmultispk automatic speech;automatic speech;speech recognition automatic;automatic sequence recognition;automatic speech recognition;speech recognition;approach automatic speech;speaker automatic speech;sequence recognition;speech recognition pasr;recognition automatic sequence;speech recognition paper;recognition multi speaker;temporal classification mtrd;sequence sequence tasks;models various sequence;automatic sequence;step temporal classification;sequence tasks;recognition multi;multi sequence approaches;sequence multi sequence;recognition alr based;temporal classification;sequence multi;sequence tasks codes;approaches sequence multi;multi sequence", "pdf_keywords": "multi speaker speech;speaker speech recognition;multi speaker asr;speaker asr benchmarks;conditional speaker chain;model multi speaker;speech recognition;speaker asr;conditional speaker;multi speaker;speech recognition evaluate;propose conditional speaker;speaker speech;speaker chain based;speaker chain;speaker;asr benchmarks;speech;mixtures model outperforms;asr benchmarks non;autoregressive nar models;parallel inference;benchmarks non autoregressive;latency achieving word;inference experiments mixtures;parallel inference experiments;outperforms nar models;mixtures model;perform parallel inference;model multi"}, "a1588ac6d582d30742f998464500bb5ead125dc6": {"ta_keywords": "regretnet new neural;deep learning regret;regret net;regretnet;introduce regret net;networks introduce regret;learning regret based;modifications regretnet;regretnet new;independent modifications regretnet;networks neural;hyperparameters network;guaranteed auctions;neural networks;learning regret;neural;guaranteed guaranteed auctions;deep learning;modifications regretnet new;networks;including neural networks;neural networks introduce;networks neural networks;neural architecture;neural networks neural;expressivity deep learning;regret net family;auctions maximize;hyperparameters network used;net", "pdf_keywords": "auction problem regretnet;maximizing auctions;perceptrons regretnet architecture;revenue maximizing auctions;layered perceptrons regretnet;auction;regretnet deep neural;optimal auction;learning regretnet;maximization online auctions;machine learning regretnet;perceptrons regretnet;regretnet deep;auctions;propose deep neural;approach auction;symmetric auctions;learning regretnet shown;propose regretnet deep;deep neural;optimal auction design;attention layers;problem optimal auction;auctions study;regretnet power attention;regretnet architecture;auction design;auction problem;regretnet architecture generalize;online auction"}, "1c682dca13e47e6e1ee3c8db54af631a8e5e5792": {"ta_keywords": "state space optimal;optimal subspace xmath0;optimal subspace;cost exploiting spectral;optimal subspace obtained;computing optimal policy;graph signal processing;dimensionality policy iteration;policy iteration computationally;proposed computing optimal;space optimal subspace;computing optimal;finding optimal policy;exploiting spectral;probability transition matrices;optimal policy large;graph signal;optimal policy;exploiting spectral properties;reduce dimensionality policy;space optimal;finding optimal;communication networks;certain wireless communication;large state space;wireless communication networks;dimensionality policy;end graph signal;problem finding optimal;policy iteration", "pdf_keywords": ""}, "48e32ba9a891f36183a26f35316e8906d14d83c0": {"ta_keywords": "crowdsourcing computational quality;crowdsourcing computational;parameterizing crowdsourcing;code crowdsourcing applications;crowdsourcing applications;source code crowdsourcing;service qos crowdsourcing;code crowdsourcing;purpose crowdsourcing computational;parameterizing crowdsourcing process;crowdsourcing methods present;crowdsourcing;qos crowdsourcing;new method crowdsourcing;methods present crowdsourcing;method crowdsourcing;crowdsourcing methods;present crowdsourcing;crowdsourcing applications paper;crowdsourcing general;crowdsourcing process workers;method crowdsourcing based;allow parameterizing crowdsourcing;crowdsourcing focus;present crowdsourcing general;qos crowdsourcing focus;crowdsourcing process;purpose crowdsourcing;general purpose crowdsourcing;crowdsourcing based", "pdf_keywords": "crowdsourcing computational quality;reliable improvement crowdsourcing;improvement crowdsourcing quality;crowdsourcing quality;analyzing data crowdsourcing;data crowdsourcing;improvement crowdsourcing;crowdsourcing computational;data crowdsourcing marketplace;crowdsourcing quality traditional;purpose crowdsourcing computational;datasets crowd kit;2019 datasets crowd;datasets crowd;crowdsourcing marketplace;aggregation quality available;crowdsourcing;crowdsourcing marketplace soon;aggregation quality;general purpose crowdsourcing;sequence labels aggregation;crowds kit;purpose crowdsourcing;observed aggregation quality;evaluation toolkit datasets;crowds;benchmarking computational quality;extensive evaluation toolkit;labels aggregation;demonstrate crowds"}, "d4756d1a7b81f53e71f939ab387cad5f0a4a13b7": {"ta_keywords": "sound event detection;duration sound event;segments sound event;sound event activity;polyphonic sound event;lstm polyphonic sound;sound event insertion;duration sound;sound activity detection;sound event precisely;memory lstm polyphonic;lstm polyphonic;sound activity;sound event;reduce sound event;video motion polyphonic;identify segments sound;motion polyphonic sound;model duration sound;polyphonic sound sensor;segments sound;motion polyphonic;lstm;sound sensor;memory lstm;event detection;short term memory;polyphonic sound;sequence detection;term memory lstm", "pdf_keywords": ""}, "19418493b1f9c82809fe4584af427b8807b8ae2d": {"ta_keywords": "relation classifier;relation classifier propose;purpose relation classifier;automatically extract relationship;extract relationship sentence;entity pairs;000 sentences annotated;sentence level classifier;relationship sentence level;extract relationship;annotated mentioned entity;sentences annotated mentioned;natural language understanding;sentences annotated;mentioned entity pair;entity pair;natural language;relation;language understanding machine;scores entity pairs;entity pair located;containing 000 sentences;relationship sentence;general purpose relation;entity pairs detected;relation given sentence;learning called home;000 sentences;sentence level;near relation", "pdf_keywords": "objects textual corpora;normalized sentence representation;textual corpora;sentence representation;sentence level relation;sentence representation method;objects textual;sentence language;large corpus;commonsense knowledge bases;corpus;physical objects textual;tasks textual entailment;entailment visual recognition;sentence language paper;tasks textual;extract relationship sentence;pairs large corpus;express sentence language;textual entailment;language based neural;knowledge bases;sentence level;downstream tasks textual;novel tasks extracting;textual corpora paper;textual;textual entailment visual;corpora;propose normalized sentence"}, "298ddceada580c46e40e2a0323c0e3b16ed5f3c9": {"ta_keywords": "predicting performance motor;motor noisy environment;performance motor noisy;motor noisy;method predicting performance;performance motor;noisy environment;predicting performance;noisy;method predicting;new method predicting;motor;predicting;performance;method;propose new method;new method;new;propose new;paper propose new;environment;paper;paper propose;propose", "pdf_keywords": ""}, "562fbb5d706d46f3e250429ac48e6acd2bf18cb1": {"ta_keywords": "enhancement speech separation;toolkit automatic speech;downstream speech recognition;speech separation systems;rich automatic speech;speech recognition module;speech separation;automatic speech;downstream speech;speech recognition;speech enhancement;automatic speech recognition;speech recognition named;development speech enhancement;speech enhancement speech;enhancement speech;feature extraction training;optional downstream speech;training evaluation pipelines;datasets present espnet;speech recognition related;preprocessing feature extraction;feature extraction;extraction training evaluation;extraction training;source separation;benchmark datasets present;quick development speech;denoising source separation;benchmark datasets", "pdf_keywords": "speech separation dataset;datasets speech separation;speech separation enhancement;separation speech enhancement;reverberation speech separation;speech enhancement separation;speech separation spatialized;channel speech separation;dataset speech enhancement;speech separation;speech separation focuses;speaker separation speech;mix dataset speech;speaker separation;end speech enhancement;support speech separation;dataset speech;benchmark datasets speech;datasets speech;separation focuses speech;speech enhancement;speech enhancement tries;speech recognition integration;aids speaker separation;separation speech;including speech enhancement;speaker mixtures widely;separation spatialized wsj0;multi channel speech;separation dataset extended"}, "2dbe78aa516cc911a71ff333a35a5ce0b1a49640": {"ta_keywords": "fidelity single photon;speech recognition asr;speed high fidelity;multi mode transducers;iteration automatic speech;single photon sources;multi mode asynchrony;photon sources multi;single photon;varying context multi;multi context;mode transducers;multi context context;context multi context;trained different latency;context multi;automatic speech;recognition asr models;speech recognition;recognition asr;asr models;mode transducers used;high fidelity single;multi mode;sources multi mode;photon sources;samples streaming;automatic speech recognition;mode asynchrony model;safety asr", "pdf_keywords": "train deep convolutional;network cnn learn;streaming context addition;cnn learn future;cnn learn;sourced speech recognition;deep convolutional neural;context settings training;deep convolutional;global voice;streaming context;network cnn;global voice application;single model trained;multi mode training;future context size;learn future context;transforming global voice;cnn;training knowledge distillation;modes streaming context;method train deep;model trained;convolutional neural;recognition corpus audiobooks;voice application;future contexts regularize;speech recognition;contexts regularize;convolutional neural network"}, "0dc379de3a613110c5fdc9c0361372c1114ee18d": {"ta_keywords": "proton beam operation;proton beam;anti proton beam;nonlinear electrodynamics led;electrodynamics led;electrodynamics led form;nonlinearity waveguide used;line proton storage;nonlinearity waveguide;nonlinear waveguide;beam emittance;low energy beam;proton storage ring;conversion ion source;proton storage;led form nonlinear;conversion ion;energy beam transport;mu anti proton;form nonlinear waveguide;nonlinear waveguide nw;addition nonlinearity waveguide;transport line proton;demonstration nonlinear electrodynamics;beam emittance measured;line proton;proton;energy beam;beam transport;ion source new", "pdf_keywords": ""}, "c6c6b4d328381a530e933c208bb43db2a7fa93c8": {"ta_keywords": "dynamics biological membranes;study biological membranes;dynamic properties membrane;biological membranes;membranes based analysis;biological membranes case;biological membranes based;properties membrane membrane;membrane membrane;membranes;membranes case membranes;properties membrane;membranes based;membrane;membranes case;membranes work;membranes work introduce;dynamics biological;case membranes;study dynamics biological;case membranes work;dynamics;analysis dynamic properties;dynamic properties;analysis dynamic;dynamic;study dynamics;method study biological;based analysis dynamic;study biological", "pdf_keywords": ""}, "7225c2a42990f850f692f8d82e7f3bfaf312145c": {"ta_keywords": "performance recurrent neural;nmt models;nmt models simply;existing nmt models;curriculum learning;performance recurrent;curriculum learning framework;training samples;training based estimated;used learn dynamics;learn dynamics;propose curriculum learning;machine used learn;learn dynamics human;neural network models;improve training time;training based;time performance recurrent;learning framework;training;training time performance;deciding training samples;training time;neural network;reduces training time;improve training;recurrent neural network;way deciding training;help improve training;nmt", "pdf_keywords": "lifelong machine translation;neural machine translation;machine translation promising;translation machine;training rnns;machine translation machine;translation machine translation;machine translation systems;training rnns proposed;integrate machine translation;translation systems;machine translation;performance recurrent neural;translation systems proposed;rnns;translation promising approach;performance recurrent;training lifelong;training lifelong lifelong;training nmt systems;rnns proposed method;rnns proposed;curriculum learning;effect training rnns;training transformers faster;framework training lifelong;machine translation paper;translation promising;proposed curriculum learning;time performance recurrent"}, "4a76869cda286efb20eb78cc6adb13daab37a0d1": {"ta_keywords": "simulations fokker planck;control fokker planck;fokker planck equations;stochastic control;planck fp collision;fokker planck solutions;based stochastic control;simulation stochastic processes;stochastic control model;simulation stochastic;evolution fokker planck;based simulations fokker;simulations fokker;method simulation stochastic;fokker planck fp;planck solutions interacting;fokker planck;particle based simulations;planck equations widely;outcome fokker planck;optimal control fokker;planck equations;stochastic systems level;gradient based stochastic;stochastic systems;behaviour stochastic systems;stochastic processes;planck fp;stochastic;particle density proposed", "pdf_keywords": "stochastic simulations particle;particles stochastic simulations;particles stochastic;substantially particles stochastic;simulating solutions stochastic;densities dimensional particles;particle method simulating;stochastic simulations;simulations particle number;particle dynamics;deterministic particle solutions;direct stochastic simulations;simulations particle;stochastic simulations required;solutions particle dynamics;particle number cumulants;fokker planck solutions;particle method;planck solutions particle;solutions stochastic dynamical;computing fokker planck;particle dynamics grouped;dimensional particles interacting;stochastically evolved distributions;solutions stochastic;stochastic dynamical systems;particle solutions delivered;particle solutions;stochastic dynamical;time substantially particles"}, "674f892caa52fa400109defa1773a10088918124": {"ta_keywords": "model predictive control;feedback predictive control;predictive control propose;predictive control;predictive control require;solving optimal control;optimal control;optimal control problems;rate convergence optimum;augmented lagrangian iteration;iteration rate convergence;lagrangian iteration rate;feedback predictive;convexified zone constraint;lagrangian iteration;convergence optimum improved;convergence optimum;control require minimizing;negative feedback predictive;state input constraints;predictive predictive model;predictive predictive;primal dual method;optimum constraint violation;model predictive;input constraints;optimum constraint;primal dual methods;rate xmath1 iteration;minimizing augmented lagrangian", "pdf_keywords": ""}, "cc549a11d277d86f6228443cb16c231c9bda6c96": {"ta_keywords": "improving emphasis estimation;continuous emphasis representations;emphasis estimation measure;emphasis estimation;utterances modeling emphasis;emphasis representations;focus utterances modeling;combines emphasis contextual;emphasis representations emphasis;continuous emphasis;hybrid improving emphasis;focus utterances;modeling emphasis active;emphasis contextual;conveys focus utterances;improving emphasis;modeling emphasis;extension cat recognition;emphasis contextual factor;utterances modeling;combines emphasis;speech conveys focus;case continuous emphasis;emphasis active;utterances;cat recognition;emphasis;contextual factor adaptive;plug play;speech conveys", "pdf_keywords": ""}, "c1546da843be7ea3e0adfb85b69a0b08d41c7159": {"ta_keywords": "evolution coupled pendulum;coupled pendulum;coupled pendulum pendulum;pendulum oscillators;pendulum pendulum oscillators;pendulum oscillators presence;oscillators presence magnetic;pendulum pendulum;pendulum;simulations evolution coupled;oscillators;oscillators presence;numerical simulations evolution;evolution coupled;simulations evolution;numerical simulations;results numerical simulations;magnetic field;magnetic;coupled;presence magnetic field;presence magnetic;simulations;present results numerical;numerical;results numerical;evolution;field;present;present results", "pdf_keywords": ""}, "5b3ca06a7673e2bf372d5f89afb15ae1eb714075": {"ta_keywords": "structure learning;model paths poverty;structure learning based;learning;paths poverty clients;learning based prediction;dynamics videos 2006;paths poverty;fluid dynamics videos;learning based;poverty clients ofcitylink;based prediction;class ectbns;poverty clients;prediction;dynamics videos;fluid dynamics video;greedy search;procedure structure learning;poverty;social service;dynamics;individual life;social;individual life outcome;classes;structure;model;data fluid dynamics;dynamics video", "pdf_keywords": ""}, "594827fdb2047bc7be4ea2f0d2364f46d187247e": {"ta_keywords": "automatic speaker verification;recognition speaker verification;videos speech recognition;speaker verification;youtube videos speech;automatic speech;corpus youtube;construction corpus youtube;videos speech;corpus youtube videos;speech recognition speaker;based automatic speech;speaker verification method;recognition speaker;filter videos subtitles;videos subtitles;videos subtitles languagedependent;speech recognition variation;speech recognition;speaker verification paper;automatic speech recognition;method automatic speaker;subtitles;subtitles languagedependent processes;corpus;subtitles languagedependent;language called jtubespeech;segment video clip;proton neutron antiproton;automatically filter videos", "pdf_keywords": "japanese speech corpus;recognition automatic speech;automatic speech;speech recognition automatic;learning automatic speech;recognition automatic speaker;extracting monologue videos;automatic speech recognition;speech recognition speech;speech corpus asr;speech corpus;recognition speech;progress automatic speech;method automatic speech;speech recognition;automatic speaker verification;recognition speech recognition;automatic speaker veri\ufb01cation;speech recognition challenging;veri\ufb01cation automatic speaker;construct japanese speech;various speech recognition;construct speech corpus;speech corpus using;speech recognition tasks;task automatic speaker;speaker veri\ufb01cation automatic;speaker videos obtained;japanese speech;speech recognition paper"}, "539631a828bf0badd20d2241784b4e06c223250e": {"ta_keywords": "model speaker clustering;mixture models;scale mixture models;mixture model speaker;speaker clustering;mixture model highly;gaussian mixture model;mixture models assume;model dynamics speech;mixture model;speaker clustering noisy;scale mixture model;gibbs sampling;multi scale mixture;gibbs sampling iterative;gibbs sampling powerful;gaussian mixture;blocked gibbs sampling;dynamics speech;represented gaussian mixture;based mixture nonstationary;environment based mixture;speech noisy environment;mixture represented gaussian;clustering noisy;dynamics speech noisy;clustering noisy data;estimation improved clustering;mixture represented;mixture nonstationary", "pdf_keywords": ""}, "d5181375d242ed181bcde0d682a3c7ec4c4c6102": {"ta_keywords": "skills position autism;communication greater autism;autism spectrum;autism spectrum conditions;greater autism spectrum;measure autistic traits;position autism spectrum;autism spectrum members;autistic traits;measure autistic;objective measure autistic;autism;verbal cognitive skills;autistic traits automatically;position autism;non verbal cognitive;cognitive skills;skills communication;social skills communication;communication skills;greater autism;communication skills use;social skills;cognitive skills crucial;improve communication skills;autistic;skills communication greater;cognitive skills position;verbal cognitive;social communication difficulties", "pdf_keywords": ""}, "562fe9b2f5e7ede128dd9a93edc3971c5e0a2394": {"ta_keywords": "artificial dialog acts;dialog acts;generate artificial dialog;artificial dialog;word generative models;word generative model;dialog act;dialog act work;dialog acts improve;use dialog acts;model dialog act;word generative;accuracy word generative;dialog act information;dialog acts guarantees;influence interlocutors polylogue;improve word generative;interlocutors polylogue;improve word prediction;dialog;word prediction;knowledge following dialog;interlocutors polylogue main;word prediction accuracy;following dialog act;influence model dialog;polylogue main idea;use dialog;model dialog;model use dialog", "pdf_keywords": ""}, "05710169c48ac1ffe6af514cc10e72d025023343": {"ta_keywords": "nonlocal model;nonlocal model used;range human behavior;paper nonlocal model;human behavior;nonlocal;behavior;paper nonlocal;model;wide range human;model wide range;range human;model used model;human;model wide;model used;used model;wide range;range;used model wide;paper;wide;used", "pdf_keywords": ""}, "14a6452d6d026a3f384e425add6ab68f8e65037f": {"ta_keywords": "labelling public crowdsourcing;crowdsourcing marketplaces present;crowdsourcing;crowdsourcing markets;crowdsourcing platform label;crowdsourcing marketplaces;crowdsourcing markets require;crowdsourcing platform;public crowdsourcing marketplaces;largest crowdsourcing markets;largest crowdsourcing;yandex largest crowdsourcing;public crowdsourcing;presents crowdsourcing;concept crowdsourcing;presents crowdsourcing platform;crowdsourcing paper;crowdsourcing paper present;based concept crowdsourcing;collect labelled data;concept crowdsourcing paper;introduction data labelling;crowd xmath0 customers;paper presents crowdsourcing;data labelling;efficient label collection;data labelling public;efficiently collect labelled;estimating number customers;efficient label", "pdf_keywords": ""}, "ea1ba6f5e5852e38ace7bbae4e4f60ffeeabe5b1": {"ta_keywords": "microfracturing hydraulic fractures;sustained microfracturing hydraulic;microfracturing hydraulic;formation sustained microfracturing;sustained microfracturing;hydraulic fractures;microfracturing;fluid dynamics video;fractures;fluid dynamics;dynamics video formation;hydraulic;video formation sustained;formation sustained;video formation;dynamics video;fluid;dynamics;formation;video;sustained", "pdf_keywords": ""}, "70a2a554829f2cebb9fa89829994444fa1ec5a7b": {"ta_keywords": "qualitative preferences distance;sequential plurality voting;voting cp nets;preferences reach collective;distance preference individuals;notion distance nets;consensus relate notions;plurality voting cp;distance rationalizability sequential;collective decision;cp nets preerences;reach collective decision;approximations compared kendall;distance nets formalism;reach consensus;aggregate preferences;preferences distance;distance distance rationalizability;distance partial orders;plurality voting;aggregate preferences reach;preferences distance preference;know aggregate preferences;conditional qualitative preferences;distance rationalizability;distance multi agent;kendall tau distance;distance nets;nets preerences;notions distance", "pdf_keywords": ""}, "39c5740304b5f4072f92e4e012a4b57e7bc2e817": {"ta_keywords": "waveform speech enhancement;metrics signal quality;speech enhancement;speaker verification;value speaker verification;speaker verification extrinsic;signal quality;speech enhancement techniques;signal quality work;measuring signal distortion;truth waveform;ground truth waveform;metric separation quality;waveform speech;given waveform speech;fidelity signal unsolicited;signal distortion ratio;measure fidelity signal;truth waveform given;fidelity signal;signal unsolicited unsolicited;value speaker;separation quality additional;distortion ratio;separation quality;signal distortion;speaker;signal unsolicited;distortion ratio sdr;sdr measure fidelity", "pdf_keywords": ""}, "470fd4faf9b8499e8bf21c5d143145305d07fe83": {"ta_keywords": "geocoded nongeocoded tweets;geocoded tweets;applies geocoded tweets;mechanism linking tweets;tweets linked common;nongeocoded tweets linked;tweets close space;linking tweets;tweets linked;nongeocoded tweets;linking tweets close;spatial proximity events;spatio temporal proximity;tweets;tweets close;temporal proximity;linked common mentions;proximity events locations;proximity events;impact geocoded;temporal proximity approach;entities mentioned spatio;impact geocoded nongeocoded;spatial proximity;geocoded;common mentions;study impact geocoded;mentioned spatio temporal;collective mechanism linking;observation spatial proximity", "pdf_keywords": ""}, "5667f934c5bc008d0464878729eed34cbf7ec1df": {"ta_keywords": "modeling semi supervised;semi supervised;semi supervised learning;outcome semi supervised;data semidefinite automata;supervised learning constraints;learning constraints unlabeled;unlabeled data semidefinite;relation extraction;relation extraction experimental;semidefinite automata;constraints unlabeled data;learning constraints;semidefinite automata semimartingales;task relation extraction;semi supervised accident;supervised learning;automata semimartingales;constraints unlabeled;supervised;automata semimartingales popular;data semidefinite;unlabeled data;modeling semi;predict outcome semi;approach modeling semi;machine learning;results modeled constraints;automata;class machine learning", "pdf_keywords": ""}, "2f1743d1a1be46452ab90691ead8bf916ffd912b": {"ta_keywords": "simulation evolution xmath0;evolution xmath0 xmath1;xmath0 xmath1;xmath1;xmath0;evolution xmath0;numerical simulation evolution;numerical simulation;results numerical simulation;simulation evolution;simulation;numerical;present results numerical;results numerical;paper present results;evolution;present results;paper present;present;paper;results", "pdf_keywords": ""}, "402ab2adcf9da95e6aad9884b1ec53271f39cd32": {"ta_keywords": "inverse filtering;inverse filtering bayesian;inverse extended kalman;research inverse filtering;inverse filters;proposed inverse filters;kalman filter;extended kalman filter;filter proposing inverse;inverse filters note;nonlinearity forward inverse;extended kalman;filtering bayesian perspective;forward inverse state;adversarial;inverse state space;kalman filter recent;estimating location person;adversarial systems;adversarial systems garnered;counter adversarial;bayesian perspective nonlinearity;inverse state;location person crowd;counter adversarial systems;forward inverse;advances counter adversarial;filtering bayesian;person crowd based;perspective nonlinearity forward", "pdf_keywords": "estimation errors inverse;errors inverse ekfs;inverse ekfs;inverse ekfs extensive;stochastic unknown input;linear stochastic unknown;estimation errors;prediction error exponentially;errors inverse;cognitive validate estimation;stochastic unknown;estimation;input unknown prediction;linear model state;unknown prediction;transition equation inverse;\ufb01lter linear stochastic;prediction;model state transition;linear stochastic;inverse \ufb01lter linear;validate estimation errors;moving sensors;equation inverse \ufb01lter;recursive crame rao;state transition equations;input stationary target;model state;inverse \ufb01lter;validate estimation"}, "f82ae0a87cae2f3a43d4c0289d0cdf7ca57461d0": {"ta_keywords": "pedestrian crowded environment;behavior pedestrian crowded;environment behavior pedestrian;crowded environment;behavior pedestrian;pedestrian crowded;pedestrian;environment behavior;crowded;behavior;effect environment behavior;environment;effect environment;effect;study effect environment;study effect;paper study effect;paper study;paper;study", "pdf_keywords": ""}, "217074971e3dfdbfab3a8c3819cd7953ae666da4": {"ta_keywords": "databases quantum information;biomedical text mining;databases quantum;generate genetic association;database genetic sequence;database genetic;association database genetic;sequence database;genetic sequence database;genetic association database;biomedical text;construction databases quantum;sequence database paper;text mining training;information processing qip;behaviors querying pubmed;quantum information processing;reinforcement learning automatically;reliability biomedical text;deep reinforcement learning;text mining;reinforcement learning;learning automatically identify;learning automatically;generate genetic;database selecting articles;pubmed database;human behaviors querying;processing qip chal;quantum information", "pdf_keywords": ""}, "1b759204e7c13f0e4af9fe00b052af4456ac3669": {"ta_keywords": "noisy data reinforcement;data reinforcement learning;reinforcement learning;reinforcement learning rl;horizon reinforcement learning;known reinforcement learning;data reinforcement;reinforcement learning algorithm;task horizon reinforcement;learning smaller task;learning sequences length;approach learning noisy;learning sequences;learning noisy;rl known reinforcement;known reinforcement;algorithm learning sequences;reinforcement;atari domain images;learning noisy data;sequential decision;parameter states atari;horizon reinforcement;atari;states atari;learning;atari domain;sensing steps frame;states atari domain;sequential decision making", "pdf_keywords": "action repetition prediction;lifelong reinforcement learning;novel policy gradient;action selection;abstractions lifelong reinforcement;action selection based;reinforcement learning;action repetition;action repetition implemented;lifelong reinforcement;reinforcement learning introduced;policy gradient;state action mapping;state art action;optimal policy;state abstractions lifelong;agent tries predict;reinforcement;policy gradient method;prediction problem agent;2016 optimal policy;optimal policy participants;frames prediction fundamental;action mapping;prediction algorithms state;repetition prediction;frames prediction;state action;paper action repetition;theory state action"}, "c1125fa33a239ef4fd3378ccd46b2a0a0cf79a15": {"ta_keywords": "erasure coded storage;coded storage;storage retrieval quantum;erasure coded data;based erasure coded;storage retrieval;coded storage reduces;new approach storage;new erasure coded;erasure coded;storage fault tolerance;approach storage retrieval;approach storage;information stored storage;storage;stored storage ring;quantum information stored;storage ring;disk io;stored storage;disk io reconstruction;data additional storage;coded data centers;bandwidth disk io;hitchhiker new erasure;latency degraded reads;traffic disk io;decommissioned machines codes;degraded reads perform;io reconstruction data", "pdf_keywords": ""}, "ac8d33e4c0a45e227a47353f3f26fbb231482dc1": {"ta_keywords": "models trained temporal;trained temporal;learning unseen facts;trained temporal context;efficiently trained new;trained new data;efficiently trained;models trained;data need retraining;new language model;stored model learning;languages time flight;context efficiently trained;model learning;language model;temporal data;use neural;temporal context efficiently;learning unseen;learning;temporal data paper;knowledge changes time;based use neural;factual knowledge changes;model learning process;factual knowledge;ability factual knowledge;use neural network;retraining scratch facts;unseen facts based", "pdf_keywords": "question answering train;question answering;model answer answering;book question answering;answering train models;pretrained language models;answer answering;aware language model;language models trained;temporal context news;language models;answer answering open;time aware language;dynamic language model;answering open book;improved memorization temporally;trained memorize facts;sampling 1m news;text model training;temporally scoped facts;news articles years;memorization temporally scoped;language model;news domain train;answering train;1m news articles;answering open;improved memorization;models news domain;models trained memorize"}, "9b1057e1f6eb17abf3962d6cd2f49468d27b94c6": {"ta_keywords": "emphasizing speech pauses;pauses emphasizing speech;speech pauses;speech pauses words;pauses emphasizing;pauses words;machine translation emphasis;develop pause prediction;importance pauses emphasizing;pause prediction;pause prediction model;emphasizing speech;importance pauses;pauses;emphasizing speech analyzing;translation emphasis active;investigate importance pauses;benefit emphasizing speech;communicating machine translation;emphasis target language;pause;machine translation;pauses inserted;words proposodic emphasis;develop pause;translation emphasis;emphasized words;existing translation experiments;listeners identify emphasis;translation experiments", "pdf_keywords": ""}, "c985600f0aa223ddc76a2ea628f1fa23504dcbcd": {"ta_keywords": "speech enhancement separation;target speech extraction;target speech recognition;speech extraction module;extraction module speech;speech extraction;speech extraction specific;improved target speech;optimize source separation;guided target speech;speech enhancement;speech recognition module;speech recognition asr;module speech recognition;speech recognition;speech recognition mixtures;location anchor speech;automatic speech;achieves target speech;source separation auxiliary;automatic speech recognition;anchor speech input;source separation systems;properties speech enhancement;source separation;target speech;speech input performance;module speech;separation systems transcription;recognition mixtures speakers", "pdf_keywords": ""}, "0e141942fa265142f41a2a26eb17b6005d3af29e": {"ta_keywords": "multilingualism linguistic diversity;languages resources representation;nlp conferences understand;representation nlp conferences;languages;language technologies;linguistic diversity;linguistic diversity world;multilingualism linguistic;language;resources representation nlp;systems language technologies;trajectory different languages;different languages;languages followed time;promoting multilingualism linguistic;language technologies contribute;disparity languages especially;languages resources;nlp conferences;multilingualism;languages especially terms;closed loop dynamics;disparity languages;relationship languages;languages followed;linguistic;predicaments highlighted language;different languages followed;closed loop corresponds", "pdf_keywords": "linguistic diversity nlp;diversity nlp community;diversity nlp;predicting linguistic diversity;language inclusion organization;linguistic diversity;nlp community;languages collectively term;semantic scholar;using semantic scholar;conferences authors languages;language resources exist;collectively term entities;predicting linguistic;model predicting linguistic;languages collectively;language inclusion;languages especially terms;semantic scholar api;language resources;term entities;impact language inclusion;linguistic;term entities quantitative;using semantic;nlp community order;semantic;nlp;scraping acl anthology;dataset using semantic"}, "683e201783bf76ab99791a02e3763fd3ab8dad96": {"ta_keywords": "deep learning active;named entity recognition;entity recognition;entity recognition present;data deep;training data deep;data deep neural;neural networks deep;deep learning;networks deep;recognition moving object;deep neural;recognition moving;active learning outperform;learning active learning;active learning;deep neural networks;networks deep learning;deep learning employed;named entity;learning active;combining deep learning;entity;method recognition moving;labeling data;labeling;manually labeling;combining deep;manually labeling data;neural networks advanced", "pdf_keywords": "entity recognition active;learning sequence tagging;entity recognition ner;deep active learning;named entity recognition;active learning sequence;sequence tagging tasks;entity recognition;propose deep active;sequence tagging;deep active;active learning promising;recognition active learning;new active learning;active learning;propose deep recurrent;tagging tasks structured;named entity;active learning yielded;deep recurrent;deep recurrent neural;active learning framework;learning sequence;performance natural language;tagging tasks;including named entity;recognition active;natural language processing;deep learning;recurrent neural network"}, "e318e554098224c9475dfc80765cbbb82fa4a409": {"ta_keywords": "scientific peer review;peer review;improving peer review;peer review process;interaction peer review;peer review mitigate;peer review based;pipeline peer review;peer review backbone;scientific peer;sustainability peer review;approach scientific peer;improving peer;collaboration pipeline peer;human ai collaboration;ai collaboration pipeline;humancomputer interaction peer;ai collaboration;ai collaboration sequel;peer;interaction peer;goal improving peer;algorithmic human sides;review based machine;sustainability peer;collaboration pipeline;human ai;pipeline peer;submissions leading ai;algorithmic human", "pdf_keywords": ""}, "8c9033f976e7787dde9af5ba952d7f9ac9c34496": {"ta_keywords": "evolving streams;outlier detector xstream;overhead evolving streams;evolving streams exists;stream feature evolving;feature evolving stream;evolving stream;based ensemble outlier;ensemble outlier;ensemble outlier detector;evolving stream case;streaming datasets;streaming datasets static;streams;stream progresses;streams exists;updates stream progresses;stream feature;algorithm measures outlierness;outlier detector;model updates stream;outlier;life streaming datasets;outlierness;updates stream;xstream real life;measures outlierness;performance xstream real;static row stream;row stream", "pdf_keywords": ""}, "36cbc3c24429ba3b69def38e6e64b41485b0a023": {"ta_keywords": "billion crawled urls;cce4corpus largest publicly;general web crawl;crawl date billion;web crawl;billion crawled;cce4corpus largest;date billion crawled;crawled;crawled urls;web crawl date;largest publicly available;cce4corpus;crawl date;general web;largest publicly;crawl;available general web;largest;urls;web;publicly available general;date billion;billion;publicly available;publicly;available general;general;date;available", "pdf_keywords": ""}, "119c33321fc0e1db837ce293f1b65cc26c1cc34e": {"ta_keywords": "checking articles reranking;matching sentences models;fact checking articles;automatically detecting fact;detecting fact checking;reranking candidate fact;articles reranking;articles reranking candidate;fact checking;matching sentences;candidate fact checking;rank fc articles;propose novel reranker;approach matching sentences;detecting previously fact;novel reranker mtm;detecting fact;matching rank fc;checking articles fc;novel reranker;matching rank;fact checked spread;sentences models;checking articles;reranker mtm;sentences models ignore;reranking;reranker;fact checked;semantic relevance misled", "pdf_keywords": "sentences fact checking;factchecked claim detection;fact checking articles;fact checking projects;fact checked based;event fact checking;fact checkers;fact checking;detecting previously fact;300 fact checking;reranker mtm factchecked;fact checkers able;model fact checkers;checking articles exploiting;sentences fact;claim detection better;fact checked claims;predict claim fact;mtm factchecked claim;claim detection;fact fact checked;key sentences fact;factchecked claim;automatically predict claim;types internet rumors;frequently quoted checked;checking articles frequently;previously fact checked;mtm factchecked;method fact checking"}, "708dcd8456426cd609c89a86344e0007c04c80bf": {"ta_keywords": "multilingual lm machines;multilingual benchmark;multilingual benchmark cloze;languages language models;effectiveness benchmark languages;create multilingual benchmark;benchmark languages;language models;language models proven;benchmark languages paper;multilingual lm;typologically diverse languages;diverse languages language;ability multilingual lm;diverse languages;languages;languages language;languages paper investigate;multilingual;task languages fewer;task languages;languages fewer available;language;languages fewer;machines access knowledge;create multilingual;languages paper;atic create multilingual;perform task languages;ability multilingual", "pdf_keywords": "multilingual factual knowledge;present multilingual benchmark;multilingual benchmark cloze;multilingual benchmark;multilingual factual;sentences crosslingual;sentences crosslingual pre;multilingual languages;learning machine translation;pretrained present multilingual;categorizing words multilingual;multilingual;grammatical sentences crosslingual;words multilingual languages;machine translation;focus multilingual factual;factual knowledge retrieval;words multilingual;multilingual languages particular;typologically diverse languages;multi available monolingual;languages results;crosslingual;generate grammatical sentences;present multilingual;predicting categorizing words;monolingual lms;diverse languages;23 languages results;monolingual lms 23"}, "db0c587111cfed85dcea413e385b17881e6e0cbb": {"ta_keywords": "recurrent neural network;term memory lstm;short term memory;neural network language;neural networks optimizing;language model nnn;matrix adaptation evolution;improve speech recognition;decoding problem neural;memory lstm recurrent;memory lstm;matrix adaptation;nnn class neural;lstm recurrent neural;recurrent neural;approach cognitive decoding;cognitive decoding;speech recognition;performance artificial neural;neural network;cognitive decoding problem;lstm;structure neural network;speech recognition performance;term memory;neural networks;lstm recurrent;covariance matrix adaptation;language models;class neural network", "pdf_keywords": ""}, "3b30422b372040ad19a713b35006c21808287720": {"ta_keywords": "storage reliability proposed;erasure codes;storage reliability;respect storage reliability;optimal respect storage;minimum storage regeneration;optimality respect storage;erasure codes called;storage reliability network;respect storage;practical codes;storage;called minimum storage;minimum storage;class erasure codes;safety rs twrs;reliability proposed design;significant savings storage;savings storage;storage space;powerful practical codes;savings storage space;code performance twrs;storage regeneration;twrs code performance;practical codes called;codes emerged;os consumed reconstructions;twrs code;storage regeneration msr", "pdf_keywords": ""}, "6fa7de6f3ce3a599de6fab273a0d43939e176e9d": {"ta_keywords": "navigation agent;human assisted navigation;assisted navigation;navigation agent able;assisted navigation problem;learning assistance requesting;assistance help agents;autonomous behavior assistant;encounter problems autonomous;learning assistance;capabilities navigation agent;help agents;help agents overcome;learning ask;simulated human assisted;intelligence ia agents;autonomous behavior;challenges learning assistance;learning ask question;fully autonomous behavior;agents encounter problems;enables agent determine;problems autonomous;autonomous problemsolving;useful information assistant;autonomous problemsolving capabilities;human assistance;behavior assistant;assistant;behavior assistant advantage", "pdf_keywords": "propose reinforcement learning;learn policy operation;reinforcement learning;pomdp based interactive;human assisted navigation;reinforcement learning framework;assisted navigation;agent communicate;autonomous agent;policy operation autonomous;reinforcement learning approach;agent communicate present;communicate present assistant;learning answer;assistant knows agent;recurrent neural network;operation autonomous agent;learning answer set;predicting agent;problem propose reinforcement;assisted navigation han;recurrent neural;approach learn policy;allows agent communicate;propose reinforcement;autonomous agent unseen;present recurrent neural;framework predicting agent;general pomdp based;rate propose reinforcement"}, "48e4ba2d04bd98843d5aab6e227b29584d63f7b6": {"ta_keywords": "ferromagnet current crowdsourcing;crowdsourcing taxonomies cooperation;crowdsourcing genre taxonomies;crowdsourcing example linguistic;crowdsourcing genre;crowdsourcing taxonomies;current crowdsourcing genre;genres crowdsourcing evidence;resource created crowdsourcing;created crowdsourcing;genres crowdsourcing;survey crowdsourcing taxonomies;current crowdsourcing;crowdsourcing evidence;survey crowdsourcing;existent genres crowdsourcing;crowdsourcing;crowdsourcing example;created crowdsourcing example;crowdsourcing evidence efficiency;approaches crowdsourcing;taxonomies cooperation linguistic;ferromagnet;cooperation linguistic resources;use approaches crowdsourcing;2d ferromagnet;crowdsourcing gamification;cooperation linguistic;magnetic;crowdsourcing gamification motivated", "pdf_keywords": "crowdsourcing taxonomies cooperation;resource created crowdsourcing;crowdsourcing taxonomies;crowdsourced projects created;created crowdsourcing;crowdsourced projects;crowdsourcing research;crowdsourcing survey provides;crowdsourcing provides;crowdsourcing survey;created crowdsourcing survey;crowdsourcing;crowdsourcing example;crowdsourced;created crowdsourcing example;presents survey crowdsourcing;crowdsourcing research question;crowdsourcing evidence;survey crowdsourcing taxonomies;dataset crowdsourced projects;survey crowdsourcing;crowdsourcing example join;crowdsourcing provides evidence;teammates paper crowdsourcing;dataset crowdsourced;genres crowdsourcing provides;genres crowdsourcing evidence;genres crowdsourcing;present dataset crowdsourced;cooperation linguistic resources"}, "43a87867fe6bf4eb920f97fc753be4b727308923": {"ta_keywords": "pretrained language models;tuning parameters tasks;fine tuning methods;efficient transfer learning;transfer learning methods;efficient fine tuning;large pretrained language;parameters pretrained model;transfer learning;parameters pretrained;fine tuning parameters;pretrained language;tuning methods;tuning large pretrained;new parameter efficient;pretrained model;finetune parameters pretrained;based fine tuning;tuning methods tune;learning methods fine;methods fine tune;tuning parameters;fine tuning;methods tune;methods tune parameters;transfer design elements;parameter efficient fine;parameter efficient transfer;parameter efficient;learning methods", "pdf_keywords": "trained language models;neural machine translation;pre trained language;multi head attention;machine translation challenging;language models wide;pre training neural;attention;machine translation;language models;trained language;multilingual denoising pre;head attention;predictive predictive predictive;predictive predictive models;predictive predictive;translation language;summarization translation language;predictive models multi;ef predictive predictive;translation language understanding;predictive models;attention issue;translation challenging task;attention issue particularly;multilingual;head attention issue;summarization translation;predictive;adapters pre trained"}, "3e24375a1810375183d47ceadc7418e94533ba5f": {"ta_keywords": "envy freeness online;online allocation;online allocation perishable;model online allocation;envy freeness;concept envy freeness;freeness online settings;fairness efficiency;mechanism fairness efficiency;freeness online;allocation perishable resources;envy;online offline settings;mechanism fairness;concept envy;online settings consider;allocation perishable;new mechanism fairness;extend concept envy;online settings;offline settings;online offline;allocation;perishable resources;perishable resources explore;fairness;properties online offline;resources explore tradeoffs;tradeoffs different objectives;explore tradeoffs", "pdf_keywords": ""}, "b63b698d177ba2861fe97d23763d66324bb1236a": {"ta_keywords": "virus replicates cell;virus cell culture;virus cell;viruses transmembrane;micelles virus replicates;virus virus micelles;identified viruses transmembrane;influenza virus undergo;cells identified viruses;viral replication;viruses transmembrane domain;virus micelles;virus micelles virus;virus undergo;virus replicates;role viral replication;type virus cell;influenza virus;micelles virus;viral replication article;replication transmembrane;replicates cell culture;important role viral;ion channel activity;replication replication transmembrane;virus undergo multiple;virus virus;studied growth virus;viral;viruses", "pdf_keywords": ""}, "dcd39e2eb27d17c369f3bf7a5a7a2a30bb9201c8": {"ta_keywords": "pre trained language;pre trained corpora;pre trained data;trained language models;pre trained models;pre trained dataset;datasets pre trained;transferability pre trained;pre trained lm;pre training data;sequences pre training;trained downstream tasks;trained corpora;language models large;trained language;trained pre post;trained lm machines;traits pre trained;characteristics pre trained;trained corpora results;performance pre trained;tuning pre trained;pre trained;directly trained downstream;make pre trained;pre post trained;trained downstream;trained lm machine;language models;trained lm", "pdf_keywords": "pre trained models;trained data semantics;pre trained data;tasks pre trained;pre trained dataset;pre trained model;mlm pre trained;pre training data;natural languages tokens;language experiments pre;languages tokens sequence;human language experiments;semantics make pre;natural languages;languages tokens;\ufb01nding pre trained;trained models;language experiments;models trained;pre trained;performance pre trained;language natural languages;make pre trained;trained models robust;trained non english;human language natural;human language;downstream tasks pre;investigate pre trained;construct linguistic"}, "c6af1ad95917badd7bc65b303a40f54950360279": {"ta_keywords": "statistical dialog managers;rule based dialog;statistical dialog manager;statistical dialog approaches;dialog manager statistical;dialog manager based;based dialog manager;dialog managers;dialog approaches;practice statistical dialog;dialog managers difficult;manager statistical dialog;statistical dialog;conventional statistical dialog;dialog manager;dialog approaches regularization;dialog managers potentially;dialog scenario;theory statistical dialog;based dialog;automobile dialog scenario;dialog;task automobile dialog;automobile dialog;dialog scenario demonstrate;dialog manager enable;rule based systems;rule based counterparts;manager based bayes;decisions rule based", "pdf_keywords": ""}, "595306f993993e44e2c2f674367103f44df03d9b": {"ta_keywords": "resource machine translation;large monolingual data;machine translation;target monolingual data;utilizing large monolingual;machine translation used;monolingual data;monolingual data pivoting;machine translation using;translation high;translation high dimensional;high resource language;paper machine translation;speed translation high;monolingual data regarded;induced bilingual dictionary;low resource language;bilingual dictionary;translation using target;words high resource;high resource sentences;speed translation;large monolingual;low resource words;translation using;sentences induced bilingual;augmentation low resource;using target monolingual;target monolingual;translation used speed", "pdf_keywords": "machine translation learns;unsupervised machine translation;learns translate highly;translation learns;translation learns translate;learns translate;neural machine translation;translation focused data;language translation focused;machine translation;translate target language;language translation;multilingual training;low resource languages;languages low resource;scenario multilingual training;morphologically rich languages;languages neural machine;nmt machine translation;languages neural;machine translation initial;target language highly;translation focused;multilingual training aims;resource related languages;related languages neural;studies language translation;resource languages azerbaijani;multilingual;translate highly related"}, "5d6f87e31d806a77d22e344106d0310be3342259": {"ta_keywords": "reviewer assignment optimally;probability assignment reviewer;pairs reviewers assigned;algorithm reviewer assignment;similarities reviewer assignment;randomized algorithm reviewer;similarity preventing reviewers;optimally solve reviewer;reviewer assignment problem;assignment reviewer paper;solve reviewer assignment;reviewer assignment;assignment reviewer;reviewer anonymization;pairs reviewers;reviewer assignment code;reviewers assigned;preventing reviewers;peer review reviewers;iii reviewer anonymization;reviewers assigned certain;suspect pairs reviewers;conference peer review;assigning assignments authors;peer review;reviewer paper pair;reviewers maliciously;solve reviewer;reviewer anonymization release;reviewers maliciously attempting", "pdf_keywords": "optimal assignment reviewers;assignment subset reviewers;paper reviewer assignment;assignment paper reviewer;pairs reviewers assigned;randomized paper assignments;reviewer assignment;assignment reviewers;paper optimal assignment;assignments peer review;reviewers paper optimal;assignment reviewers given;reviewers assigned;reviewer assignment important;pairs reviewers;paper assignments peer;optimal assignment;reviewers assigned certain;subset reviewers paper;assigned certain papers;optimal assignment problem;finding optimal assignment;reviewing assignment paper;peer review;reviewing assignment;reviewers given paper;subset reviewers;respecting marginal assignment;assignments peer;assignment probabilities"}, "68d0b245e9754de9f36cba305e4ce50ff868cb6a": {"ta_keywords": "combinatory categorial grammar;categorial grammar;categorial grammar ccg;expressive grammar formalism;construct expressive grammar;grammar induction algorithm;expressive grammar;based grammar induction;grammar induction;new grammar parsing;grammar formalism extended;grammar parsing long;grammar parsing;unsupervised parsing;grammar formalism;unsupervised parsing ccgs;grammar ccg achieves;combinatory categorial;algorithm combinatory categorial;parsing long sentences;em based grammar;grammar ccg;parsing;parsing ccgs approach;parsing long;grammar;parsing ccgs;new grammar;work unsupervised parsing;general linguistic", "pdf_keywords": ""}, "7cc74ffa1215321712d4a830bb9dee19d9f0fb47": {"ta_keywords": "compositionalgeneralization question answering;question answering grounding;answering question composition;question composition training;question answering;question answering models;language representations grounding;answering models;answering models struggle;mechanism question answering;representations grounding structured;domain question answering;grounding structured predictions;model answering;structured predictions attention;improve compositional generalization;probabilistic model answering;answering models able;question composition;compositional generalization language;predicting structured;model answering question;answering grounding enables;answering grounding;generalization language representations;structured predictions;grounded graph decoding;representations grounding;compositional generalization significantly;compositions training patterns", "pdf_keywords": "compositional generalization deep;compositional reasoning compositional;reasoning compositional semantics;generalization deep seq2seq;compositional semantics;compositional generalization language;language compositional generalization;reasoning compositional;generalization language compositional;compositional generalization compositional;compositional semantics answer;generalization compositional neural;compositional reasoning;propose compositional generalization;semantics compositionality;propose compositional reasoning;compositional generalization;generalization compositional;syntax semantics compositionality;compositionally generalizable predictions;compositional neural network;questions propose compositional;compositional generalization important;make compositionally generalizable;generalization deep;semantics answer answering;compositional generalization remains;language compositional;deep seq2seq"}, "ee3ce47b79917974d30b6eeaaeeba99f1b1a5c59": {"ta_keywords": "task learning;decoding strategies experiments;decoding strategies;performance decoding;task learning different;multi task learning;multiple decoding strategies;computer aided learning;aided learning;performance decoding speed;complex systems;conformer multi task;benchmarks proposed systems;multi task;aided learning ciel;systems;state art systems;systems streaming task;regards performance decoding;learning;computer aided;analysis complex systems;streaming task paper;decoding;strategies experiments benchmarks;sibility powerful systems;streaming task;powerful systems;decoding speed;complex systems involves", "pdf_keywords": "decoder architecture speech;speech recognition tasks;recognition performance speech;improving performance decoding;architecture speech recognition;speech recognition;performance speech recognition;speech recognition community;attention speech recognition;recurrent neural network;performance decoding;encoder decoder architecture;decoding speed transducer;neural network encoder;transfer learning;learning transfer learning;various speech recognition;transducer models;decoder architecture;transducer models widely;streaming task;decoder;performance decoding speed;encoder;transducer model internal;encoder decoder;transfer learning transfer;transducer;transducer model espnet;speed transducer models"}, "5f96e3e00b36c5eeebff09a1bf4c804bd4ce4620": {"ta_keywords": "attack detection;attack detection problem;quickest attack detection;attack remote state;linear attack scheme;known linear attack;linear attack;remote state estimation;attack remote;attack scheme;constrained markov decision;injection attack remote;data injection attack;noise quickest attack;markov decision process;attack scheme posed;markov decision;state estimation considered;quickest attack;checking probability belief;false alarm constraint;alarm constraint numerical;state estimation;gaussian noise quickest;injection attack;detection problem known;constrained markov;detection false data;estimation considered optimal;probability belief", "pdf_keywords": "optimal attack detection;attack detection policy;malicious attacks sensor;attacks sensor networks;attacks sensor;attack detection;attack remote state;securing cyber physical;securing cyber;derive optimal attack;prediction malicious attacks;attacks proposed;optimal attack;detection policy;schemes securing cyber;malicious attacks;cyber physical systems;attack remote;state estimation bayesian;remote state estimation;detection policy mean;injection attack remote;physical systems stealthy;data injection attack;data injection attacks;detection delay;cyber physical model;observable markov decision;systems stealthy data;estimation bayesian"}, "64a106b707586345a055aa22c3356c4dc3d01877": {"ta_keywords": "coupled pendulums;consisting coupled pendulums;pendulums;dynamics consisting coupled;dynamics;dynamics consisting;consisting coupled;coupled;study dynamics consisting;study dynamics;systematic study dynamics;systematic;systematic study;present results systematic;results systematic;results systematic study;consisting;present results;paper present results;study;present;results;paper present;paper", "pdf_keywords": ""}, "d5a95567e079685322cd485033d334284c4b0a62": {"ta_keywords": "reversible polymerization controlled;reversible polymerization;study reversible polymerization;developments reversible polymerization;applications reversible polymerization;reversible polymerization typically;utility irreversible polymerization;irreversible polymerization;irreversible polymerization present;controlled polymerization systems;polymerization systems;controlled polymerization;polymerization controlled;forward polymerization;polymerization controlled polymerization;recycled monomers repolymerized;polymerization systems offer;polymerization present theoretical;polymerization;forward polymerization converts;polymerization systems paper;steps forward polymerization;polymerization converts;repolymerized new polymers;monomers repolymerized;polymerization present;depolymerization capable regenerating;polymerization typically;polymerization typically involves;biomedical applications reversible", "pdf_keywords": ""}, "f16cf130ae75d1ea1ad3b926f605adef41af4af1": {"ta_keywords": "conservation based uncertainty;uncertainty propagation conservation;uncertainty propagation dynamic;uncertainty propagates dynamical;theory uncertainty propagation;uncertainty propagation;study uncertainty propagation;based uncertainty propagation;uncertainty propagation initial;methods uncertainty propagation;uncertainty propagation using;uncertainty propagates;method uncertainty propagation;study uncertainty propagates;propagation dynamic systems;generalized theory uncertainty;theory uncertainty;based uncertainty;propagation using conservation;rigorous study uncertainty;existing methods uncertainty;methods uncertainty;systems fundamental conservation;problem conservation laws;dynamic systems;propagation conservation;propagation conservation methods;propagation dynamic;method uncertainty;dynamic systems fundamental", "pdf_keywords": ""}, "b13e9d23983273c0c67b91ae70c55d4c3f745b8b": {"ta_keywords": "neural machine translation;translation agent learns;simultaneous translation agent;translation agent;machine translation;simultaneous translation;translation methods fluid;simultaneous translation outputs;motion propose neural;translating real time;conventional machine translation;time simultaneous translation;translation words input;translation methods;machine translation methods;framework simultaneous translation;machine translation nmt;translation outputs translation;outputs translation words;translate interaction;translation words;beam searching;translation outputs;dynamics video motion;video motion;translate interaction pre;beam search;dynamics video;beam searching simultaneous;motion", "pdf_keywords": "simultaneous machine translation;simultaneous translation task;neural machine translation;simultaneous translation agent;translation quality time;task translation quality;simultaneous translation;simultaneous translation requires;translation agent learns;framework simultaneous translation;machine translation;machine translation nmt;translation agent;machine translation trade;paramount simultaneous translation;translation task;translation quality paramount;translation quality;typical machine translation;task translation;translation requires balancing;machine translation mt;propose machine translation;mt task translation;translation mt task;translation nmt framework;translated content expeditious;translation task users;neural simultaneous machine;translated content"}, "ed1c17451a23471afde91c109ecadc6aab8b2ba6": {"ta_keywords": "multimodal disinformation detection;multimodal disinformation;disinformation detection;art multimodal disinformation;disinformation detection covering;disinformation;resonators initially textual;disinformation equally important;modalities text images;design nanoelectronic devices;modalities factuality harmfulness;disinformation equally;nanoelectronic devices;nanomechanical;multimodal;concept nanomechanical resonators;nanomechanical resonators;nanoelectronic devices based;concept nanomechanical;state art multimodal;based concept nanomechanical;nanoelectronic;design nanoelectronic;definition disinformation;textual content;modalities text;nanomechanical resonators initially;definition disinformation equally;approach design nanoelectronic;multiple modalities factuality", "pdf_keywords": "multimodal misinformation detection;multimodal disinformation detection;disinformation detection;multimodal disinformation;approach multimodal misinformation;disinformation detection covering;disinformation detection highlight;multimodal misinformation;detection fake news;misinformation detection taking;misinformation detection;misleading harmful content;detection misleading harmful;automatic detection misleading;art multimodal disinformation;recognize harmful content;harmful content online;detection misleading;disinformation;modalities factuality harmfulness;harmful content;detection fake;social media propagation;harmful content poses;taking factuality harmfulness;multimodal;factuality harmfulness framework;factuality harmfulness;misleading harmful;approach multimodal"}, "8c838c7631a7408d5ea5801c9360213782665c9c": {"ta_keywords": "3d ising model;xmath0 component 3d;component 3d ising;ising model;3d ising;dimensional 3d asr;model evolution xmath0;3d asr;3d asr based;asr based model;ising model presence;xmath0 component;evolution xmath0 component;xmath0;evolution xmath0;external magnetic field;presence external magnetic;component 3d;magnetic field;performance dimensional 3d;external magnetic;dimensional 3d;ising;magnetic;asr;asr based;3d;model presence external;model evolution;model presence", "pdf_keywords": ""}, "db1dafd0c356491cbbf53338b9984de324e7239c": {"ta_keywords": "bilingual lexicon induction;aligned bilingual lexicons;bilingual lexicons larger;bilingual lexicon;bilingual lexicons;embedding spaces empirically;word embeddings;work bilingual lexicon;unaligned word embeddings;particularly embedding spaces;language pairs muse;lexicon induction;limited aligned bilingual;semi supervised;aligned bilingual;word embeddings novel;particularly embedding;does particularly embedding;embeddings;lexicon induction bci;propose semi supervised;language pairs;bilingual;empirically weakens languages;recent work bilingual;embedding spaces;isometry embedding;embedding;spaces empirically;isometry embedding spaces", "pdf_keywords": "embedding alignment unsupervised;language embedding;supervised embedding alignment;unsupervised cross lingual;mappings word embeddings;language embedding spaces;embedding alignment;distant word embedding;word embedding;translation unsupervised learning;adversarial distribution matching;word embeddings;assumption language embedding;machine translation unsupervised;embedding spaces empirically;word embeddings work;optimizes supervised embedding;word embedding machine;cross lingual mappings;unsupervised distribution matching;particularly embedding;particularly embedding spaces;supervised embedding;embeddings work propose;lingual mappings;alignment unsupervised distribution;embeddings work;embeddings;train language pairs;lingual mappings word"}, "9bb9b23823b45ba7521d872bb3e970ede4aafb8a": {"ta_keywords": "region proposal networks;problem diarization based;diarization based concept;diarization based;proposal networks rpns;approach problem diarization;diarization;region proposal;proposal networks;problem diarization;concept region proposal;networks rpns;networks;based concept region;rpns;concept region;region;new approach problem;proposal;approach problem;based concept;paper propose;propose new approach;propose;concept;problem;paper propose new;new approach;approach;paper", "pdf_keywords": ""}, "41675d91ad815f64b0df382c0944247811a62cc9": {"ta_keywords": "table parallel corpus;machine translation designed;machine translation;learning phrase table;alignment phrase extraction;machine translation systems;phrase table parallel;based machine translation;learn phrase table;corpus sentences aligned;parallel corpus sentences;parallel corpus;translation systems;word alignment;phrase table achieves;phrase table;phrase based machine;research machine translation;step word alignment;word alignment phrase;reducing phrase table;sentences aligned word;sentences aligned;process word alignment;directly learn phrase;corpus sentences;corpus;phrase extraction;context free grammars;alignment phrase", "pdf_keywords": ""}, "1fc6290fa3e6784501ca67cbef33a6a8edcbdb9e": {"ta_keywords": "wasserstein distance empirical;wasserstein distance probability;convergence grows wasserstein;distance empirical measures;asymptotic behavior wasserstein;distance probability measures;behavior wasserstein distance;probability measures metric;empirical measures random;wasserstein distance;quickly wasserstein distance;wasserstein distance order;grows wasserstein distance;measures random;empirical measures;measures metric space;asymptotic finite sample;measures metric;measure closeness applications;closeness applications statistics;probability measures;space measure closeness;measure closeness;distance empirical;measures;measures random variables;convergence general measure;measures exhibit;general measure;metric space measure", "pdf_keywords": "whasserstein metric converge;whasserstein metric;whasserstein distance asymptotically;metric spaces whasserstein;spaces whasserstein distance;practice whasserstein metric;whasserstein distance;whasserstein distance entropic;particular whasserstein distance;spaces whasserstein;converge slowly measures;convergence particular whasserstein;metric space measures;measures arising applications;space measures arising;slowly measures supported;high dimensional metric;space measures;measures general compact;measure supported compactly;whasserstein;observed practice whasserstein;metric converge slowly;convergence general measures;slowly measures;general compact metric;practice whasserstein;compact metric;measures arising;particular whasserstein"}, "f5b6819e05e087d2bbae3ddb6d58d2ab4e1d7ca2": {"ta_keywords": "agents maximize reward;reward environment learn;maximize reward environment;chaotic oscillators ensure;reward environment;reinforcement;reinforcement learning maximize;demonstrations reinforcement;actions reward maximizing;chaotic oscillators;learning maximize environmental;coupled chaotic oscillators;contextual bandit orchestrator;reward maximizing;demonstrations reinforcement learning;ensemble coupled chaotic;agent able learn;reinforcement learning;agents maximize;ways autonomous cyber;constraints demonstrations reinforcement;cyber physical agents;allow agents maximize;maximize reward;autonomous cyber;reward maximizing constrained;environmental rewards agent;maximize environmental rewards;chaotic;bandit orchestrator allows", "pdf_keywords": ""}, "bfb13c6889626e833bf449fdb361d186467919af": {"ta_keywords": "evaluations feature feedback;incorporating feature feedback;exploiting feature feedback;feature feedback autoencoders;feature feedback;feature feedback delivered;feature feedback used;feature feedback methods;appeal feature feedback;feedback autoencoders;benefit supplemental annotations;domain evaluations feature;annotations provided training;autoencoders auxiliary annotations;supplemental annotations;feedback autoencoders auxiliary;supplemental annotations lessening;evaluations feature;collecting exploiting feature;auxiliary annotations;annotations lessening sensitivity;auxiliary annotations provided;annotations;counterfactually augmented data;paper feature feedback;gains domain evaluations;feedback delivered significant;feedback;feedback used improve;feature", "pdf_keywords": "feature feedback nlp;feedback nlp;incorporating feature feedback;answer prediction sentiment;trained feature feedback;feature feedback provides;feedback nlp recent;feature feedback;feature feedback generalize;feature feedback train;feature feedback delivered;use feature feedback;supplemental annotations;rely feature feedback;feedback generalize better;prediction sentiment;sentiment analysis tasks;supplemental annotations lessening;feedback generalize;rationales feature feedback;networks answer prediction;feedback provides;feedback train convolutional;bene\ufb01t supplemental annotations;feedback;feedback provides added;annotations;prediction sentiment analysis;answer prediction;annotations lessening sensitivity"}, "df9949abc06cb4f0f4c0ac1eb7ce0bc62ed5ec02": {"ta_keywords": "phoneme sequences corpus;segmentation phoneme sequence;word segmentation phoneme;phoneme sequence estimation;model word segmentation;segmentation phoneme;phoneme sequences;sequences corpus polymer;phoneme sequence;word segmentation;sequences corpus;identify phoneme sequences;automatically identify phoneme;corpus polymer chain;corpus;gram model word;sequence estimation;gram model;corpus polymer;words annotated unsegmented;sentences words annotated;sequence estimation propose;identify phoneme;sequence based;words annotated;segmentation;sequence based methods;list compound words;annotated unsegmented;hard spheres aspect", "pdf_keywords": ""}, "5331a846c854c3ecedf9ecf3ea516cb6dcaba4c8": {"ta_keywords": "based saliency benchmarking;saliency benchmarking;saliency benchmarking approach;evaluation saliency methods;evaluations saliency analysis;based evaluation saliency;evaluations saliency;saliency methods currently;saliency based saliency;existing saliency methods;evaluation saliency;saliency analysis;saliency based;saliency methods;experimental evaluations saliency;saliency analysis reveal;new saliency based;saliency methods controlling;saliency methods especially;evaluation tasks saliency;saliency methods popular;saliency;existing saliency;based saliency;tasks saliency methods;limitations existing saliency;new saliency;propose new saliency;tasks saliency;adoption saliency methods", "pdf_keywords": "based evaluation saliency;evaluation saliency;evaluation saliency methods;saliency methods capturing;existing saliency methods;saliency methods;saliency methods especially;saliency methods controlling;saliency;existing saliency;performance saliency methods;limitations existing saliency;evaluate performance saliency;performance saliency;synthetic evaluation tasks;pointing game evaluations;model reasoning evaluation;reasoning evaluation;reasoning evaluation framework;truth based evaluation;game evaluations;simplicity synthetic evaluation;simulated model reasoning;truth feature;model reasoning prevents;pointing game;truth feature attributions;ground truth feature;model reasoning;truth model reasoning"}, "807600ef43073cd9c59d4208ee710e90cf14efa8": {"ta_keywords": "existing retrieval systems;new retrieval framework;benchmark information retrieval;retrieval systems;retrieval systems retrieval;extended retrieval systems;retrieval systems extended;retrieval systems including;retrieval framework;retrieval framework retrieval;extended retrieval;existing retrieval;systems extended retrieval;information retrieval;propose new retrieval;retrieval tasks;theart retrieval systems;new retrieval;retrieval;text retrieval tasks;understand existing retrieval;retrieval tasks domains;text retrieval;information retrieval paper;retrieval paper;diverse text retrieval;retrieval used;retrieval paper present;framework retrieval systems;interaction ranking architectures", "pdf_keywords": "benchmark information retrieval;trained retrieval models;retrieval models;evaluation retrieval systems;information retrieval;retrieval systems;evaluate retrieval methods;retrieval systems evaluate;retrieval methods;retrieval longer documents;evaluation retrieval;retrieval models come;evaluate retrieval;approach information retrieval;systems evaluate retrieval;retrieval based;retrieval longer;dataset evaluation retrieval;retrieval;retrieval based idea;information retrieval based;trained retrieval;idea retrieval longer;retrieval methods \ufb01ve;pre trained retrieval;idea retrieval;heterogeneous evaluation benchmark;late interaction ranking;introduce benchmarking ir;based idea retrieval"}, "3a40cdd82f0706cda6c247e586d5054abeab4e1f": {"ta_keywords": "answers qgs algorithm;improve question answering;question answering;answers qgs;answer list entities;list produced qa;list answers;list answers better;original answers qgs;searchers able textual;answer list;qa generate;new list answers;question answering expected;answers produced qa;extend candidate answers;expected answer list;qa generate new;trec evaluations searches;evaluations searches;combines original answers;produced qa generate;candidate answers produced;searches seeds textual;answers produced;state art qa;qa tested list;seeds textual textual;able textual resources;produce extended list", "pdf_keywords": ""}, "2c3d02ce8780cc6648caf4ee996d9628c6388751": {"ta_keywords": "crowdsourcing data labeling;fermilab tevatron crowdsourcing;crowdsourced event generator;crowdsourcing data;crowdsourcing datasets;crowdsourced event;increasing crowdsourcing data;crowdsourcing datasets binary;performance crowdsourced event;tevatron crowdsourcing;real crowdsourcing datasets;performance crowdsourced;crowdsourcing;increasing crowdsourcing;probabilistic labeling;real crowdsourcing;variety real crowdsourcing;rapidly increasing crowdsourcing;probabilistic labeling model;crowdsourced;crowdsourcing workers;tevatron crowdsourcing workers;study performance crowdsourced;principle paper crowdsourcing;crowdsourcing workers usually;crowdsourcing used;paper crowdsourcing;derive probabilistic labeling;labels low cost;data labeling", "pdf_keywords": "labeling model crowdsourcing;labels crowdsourcing;model crowdsourcing tasks;labels crowdsourcing workers;true labels crowdsourcing;confusion matrices crowdsourcing;crowdsourcing tasks;model crowdsourcing;crowdsourcing services;crowdsourcing datasets;crowdsourcing tasks propose;matrices crowdsourcing;crowdsourcing services like;crowdsourcing datasets binary;matrices crowdsourcing large;propose model crowdsourcing;real crowdsourcing datasets;model crowdsourcing assumes;crowdsourcing crowdsourcing workers;crowdsourcing large number;probabilistic labeling model;crowdsourcing large;crowdsourcing workers;crowdsourcing;probabilistic labeling;workers crowdsourcing;crowdsourcing workers crowdsourcing;propose probabilistic labeling;crowdsourcing crowdsourcing;crowdsourcing workers asked"}, "ce5ff42d629e67a84731b3c62b57b47fc7f2b20d": {"ta_keywords": "attention transformer encoder;self attention transformer;attention transformer;streaming transformer trained;attention network automatic;training streaming transformer;attention network;chunkwise attention;self attention network;transformer trained compute;performance alternative recurrent;recurrent neural networks;monotonic chunkwise attention;transformer trained;alternative recurrent neural;neural transducer transformer;speech recognition asr;chunkwise attention mocha;network automatic speech;neural networks end;speech recognition;transformer encoder introducing;automatic speech;attention wide range;end automatic speech;transformer encoder;speech recognition systems;transformer streaming;batch transformer;encoder introducing", "pdf_keywords": "streaming asr attention;transformer speech recognition;asr attention based;speech recognition tasks;attention sta layers;transformer speech;recurrent neural networks;performance alternative recurrent;based transformer speech;attention network;speech recognition;alternative recurrent neural;automatic speech;self attention network;speech recognition systems;recognition speech recognition;speech recognition speech;automatic speech recognition;recognition speech;entire streaming asr;speech recognition shown;algorithm automatic speech;streaming asr;synchronous beam search;attention network san;search technique streaming;neural networks end;end automatic speech;attention based models;asr attention"}, "945d4addf8e94487f6199af71dc15a298791c1b4": {"ta_keywords": "opportunistic sampling energy;fading processes learning;markovian channel fading;sampling energy harvesting;channel fading processes;channel optimal;channel fading process;channel optimal source;wireless network time;minimization opportunistic sampling;probing channel optimal;fading processes;tradeoffs energy arrival;process channel fading;energy arrival process;update policy wireless;policy wireless;sampling energy;channel quality decision;markovian channel;instantaneous channel quality;node sampling policy;opportunistic sampling;information minimization opportunistic;energy harvesting;energy arrival;arrival process channel;fading process;wireless link quality;channel fading", "pdf_keywords": "optimal policy channel;state energy harvesting;reinforcement learning rl;statistics energy harvesting;reinforcement learning;channel state estimation;energy harvesting model;energy harvesting;channel state energy;propose reinforcement learning;expected energy arrival;determine optimal policy;optimal policy;state estimation problem;asynchronous stochastic approximation;energy arrival;policy channel statistics;algorithm channel state;state estimation;scheduling policy;harvesting model proposed;energy harvesting characteristics;policy channel;state energy;channel statistics energy;learn optimal;stochastic approximation;timeaveraged expected energy;seeks learn optimal;stochastic approximation algorithm"}, "27df24c537b2d3c2a769d917adf92a6a059c5917": {"ta_keywords": "multiscale modeling machine;predicting multiscale systems;multiscale simulations trained;multiscale modeling;multiscale modeling effective;approach predicting multiscale;multiscale simulations;cost multiscale modeling;predicting multiscale;idea multiscale modeling;multiscale systems;multiscale simulations facilitating;cost multiscale simulations;simulations trained overparametrized;multiscale systems new;modeling machine;continuum model neural;schemes multiscale simulations;computational cost multiscale;coupling schemes multiscale;multiphysics systems;idea multiscale;employ deeponet neural;investigating multiphysics systems;model neural;model neural operator;simulations trained;deeponet neural operator;multiphysics systems largely;multiscale", "pdf_keywords": "models multiscale;multiscale modeling meets;meshfree models multiscale;models multiscale modeling;multiscale modeling;multiscale modeling framework;propose multiscale modeling;multiscale;employ deep neural;multiscale coupling framework;new multiscale;particle meshfree models;models particle meshfree;meshfree models;propose multiscale;learning constitutive model;expensive microscopic model;multiscale coupling;present new multiscale;mesh based models;paper propose multiscale;deep neural;microscopic model;scale features;\ufb01ne scale features;neural operator deeponet;microscopic model propose;deeponet surrogate approximate;model trained;modeling meets machine"}, "46ef61536a01578e79b6d4e35e803a914afeb629": {"ta_keywords": "mn4 line emission;observation mn4 line;line emission solids;observation mn4;emission solids;report observation mn4;mn4 line;line emission;mn4;emission;solids;line;report observation;observation;report", "pdf_keywords": ""}, "65ee083ce61576955d76b36819bf3ac271335597": {"ta_keywords": "existence regenerating codes;erasure coding techniques;reliability distributed storage;erasure coding;regenerating codes achieve;storage repair bandwidth;distributed storage;distributed storage systems;regenerating codes;present erasure coding;exact regenerating codes;regenerating codes provided;regenerating codes additional;storage repair;systems minimizing storage;storage systems minimizing;distributed mail server;distributed mail;storage systems;minimizing storage overhead;increase reliability distributed;minimizing storage;node failures subspace;relevant distributed mail;reliability distributed;point storage repair;failures subspace based;storage;codes achieve tradeoff;simultaneous node failures", "pdf_keywords": "code distributed storage;regenerating network coding;regenerating code distributed;network coding algorithm;network coding;distributed storage;regenerating codes given;storage repair bandwidth;regenerating codes;distributed storage systems;exact regenerating codes;regenerating codes provided;code distributed;bandwidth point storage;construction regenerating codes;new regenerating code;regenerating code;coding algorithm;present regenerating network;regenerating codes mbr;peer systems data;distributed mail server;distributed mail;regenerating network;peer peer systems;storage systems;repair bandwidth;peer systems;repair bandwidth tradeoff;coding algorithm multiple"}, "092ee3a32b6cd951da971124a24872c7cccf3a9f": {"ta_keywords": "transductive transfer learning;transductive transfer learners;transfer learning labeled;transductive svms;unsupervised transductive transfer;transfer learning information;transfer learning;transfer learning protein;problem transfer learning;transductive transfer;learning protein extraction;performance transductive transfer;learning problem transfer;transductive svms paper;art transductive svms;learning protein;unsupervised transductive;transfer learners;learning labeled;improve performance transductive;inductive transductive approaches;transfer learners previous;state art transductive;transferring weights;learning labeled data;transductive approaches;transductive;case unsupervised transductive;method transferring weights;transferring weights pool", "pdf_keywords": ""}, "fce10a1a9727cbda33d44b62409e303f1009417a": {"ta_keywords": "neural network grammars;network grammars rnng;performance recurrent neural;performance recurrent;modeling parsing;grammars rnng;language modeling parsing;recurrent neural network;best performance recurrent;modeling parsing performance;recurrent neural;modeling composition crucial;grammars rnng recently;language modeling;phrasal representation predictions;natural language headedness;generative modeling;representation predictions handcrafted;modeling composition;probablistic generative modeling;generative;probablistic generative;proposed probablistic generative;generative modeling family;network grammars;explicit modeling composition;art language modeling;natural language;person activities;parsing", "pdf_keywords": "grammars rnn probabilistic;neural network grammars;grammars rnn;network grammars rnn;generative model parsing;rnn probabilistic generative;grammars rnng designed;network grammars rnng;model grammar constituent;grammars rnng;generative model language;language present probabilistic;distributions recurrent neural;grammar constituent words;rnn probabilistic;language modeling;model syntactic;model grammar;grammar constituent;context model grammar;model syntactic derivations;propose probabilistic neural;model language modeling;designed model syntactic;model parsing;parsing typed dependency;propose recurrent neural;probabilistic generative;syntactic derivations sentences;constituent context model"}, "49af035c598901fbf766da2cfb040cca7336a8ac": {"ta_keywords": "exploration semantic parsers;transition based parsing;semantic parsers;semantic parsers map;amr parsing;abstract meaning representation;parsing;exploration semantic;parsing approach improves;amr parsing introduce;parsers;representation amr semantic;methods amr parsing;parsers map natural;imitation learning context;study imitation learning;parsing approach;map natural language;imitation learning;based parsing;amr semantic formalism;parsing introduce;amr semantic;meaning representations abstract;based parsing approach;targeted exploration semantic;representations abstract syntactic;parsers map;abstract syntactic;study imitation", "pdf_keywords": ""}, "d47ad0a606bedf41dcea614bfa7b7494879c7ba0": {"ta_keywords": "tracking state changes;predicted open vocabulary;attribute state;generate set state;attribute state state;shot quantum state;shot quantum;state changes procedural;state change tuples;state step entity;state art generation;attribute state values;vocabulary using crowdsourcing;single shot quantum;generation model task;new task formulation;state values predicted;quantum state level;state changes;tracking state;vocabulary present dataset;entity attribute state;state step;sentences 810 procedural;dataset tracking state;state level;open vocabulary using;state state step;set state;just procedural text", "pdf_keywords": "generation text actions;text understanding;generation text;procedural text understanding;text understanding procedural;procedural semantic understanding;task formulation track;approach procedural text;open vocabulary state;procedural semantic;generated open vocabulary;text actions state;vocabulary state changes;entity states text;vocabulary state;automatic state art;state tracking task;text actions;semantic understanding;understanding procedural semantic;open vocabulary entity;just procedural text;new task formulation;predicted open vocabulary;vocabulary entity;procedural text;prose comprehension word;automatic state;semantic;word challenging task"}, "188928df74f9ce00bd1b58686db93ac8cdd07275": {"ta_keywords": "batch multiple speech;traverse multiple utterances;search pair particles;multiple speech utterances;multiple utterances;multiple speech;utterances line recognition;speech utterances;technique beam search;dengue type dengu;binary mixture algorithm;particles binary mixture;beam search;algorithm search;mixture algorithm based;new algorithm search;vectorizing multiple hypotheses;mixture algorithm;single family dengue;type dengu;search process vectorizing;dengue;algorithm search pair;dengue type;pair particles binary;search pair;speech utterances line;search process;utterances;dengu type", "pdf_keywords": ""}, "2f369845ae7191196d65310210db2485feb3aa86": {"ta_keywords": "learning speech recognition;tight structured learning;adaptive regularization weights;structured learning speech;adaptive regularization;narrow adaptive regularization;significantly improves phoneme;regularization proton phoneme;improves phoneme error;errors structured learning;speech recognition;improves phoneme;phoneme conversion;learning speech;g2p conversion training;phoneme conversion mammalian;structured learning;speech recognition field;regularization weights;regularization weights narow;new machine learning;study regularization;regularization;conversion training method;learning;phoneme error rate;learning use bound;online learning algorithm;proton phoneme conversion;structured learning use", "pdf_keywords": ""}, "bd2f3822801a7e2f933d06c261b8783764d8ce18": {"ta_keywords": "model output probability;distribution output model;output probability distributions;predict probability plant;state aberration patterns;prediction probability distribution;model prediction probability;output model;output model plant;able predict probability;output probability;model prediction;prediction probability;aberration patterns;aberration patterns families;probability distribution output;model output;probability plant;predict probability;simple model prediction;model able predict;probability plant given;prediction;distribution output;given state aberration;state aberration;probability distributions;probability distributions classi\ufb01er;model plant model;plant model", "pdf_keywords": "natural language attacks;better adversarial examples;neural dependency parsers;samples created adversarially;challenging task adversarial;better adversarial;adversarial examples refer;task adversarial;adversarial;task adversarial examples;adversarial examples strongly;adversarial examples;adversarially;created adversarially;adversarially intentionally;using better adversarial;created adversarially intentionally;robustness neural dependency;dependency parsers;adversarially intentionally wicked;generating natural language;language attacks;language attacks hard;dependency parsers using;learning model recognizing;text classi impostor;parsers;natural language;robustness neural;based id samples"}, "576860f910ea8fde366deb03c910ab30cd776966": {"ta_keywords": "measure speed sound;sound speed speaker;speed sound speaker;speed speaker proportional;speed speaker;observation sound speed;sound speed;speaker proportional;speaker noisy environment;speed sound;sound speaker;speaker proportional square;sound speaker noisy;speaker noisy;speaker;number speakers;speakers;method measure speed;measure speed;based observation sound;root number speakers;noisy environment;observation sound;noisy;noisy environment method;sound;proportional square root;simple accurate method;speed;square root number", "pdf_keywords": ""}, "95a35473fd1936927dd4a53fe0a5d2d6762d99b3": {"ta_keywords": "neural audio synthesis;audio synthesizers;audio synthesizers provide;audio synthesis detailed;realistic audio synthesis;audio synthesis;hierarchical model musical;synthesizers provide detailed;synthesizers;conventional audio synthesizers;introduce midi;audio synthesis concatenative;synthesizers provide;midi;neural audio;midi ddsp hierarchical;notes performance synthesis;generate realistic audio;notes synthesis;produce realistic audio;detailed expressive controls;work introduce midi;realistic audio;introduce midi ddsp;box neural audio;realistic audio novel;realistic audio mechanisms;model musical instruments;enables realistic audio;midi ddsp", "pdf_keywords": "music synthesizers;controllable music synthesizers;generative model music;music synthesizers propose;synthesizers;expressive controllable music;midi;model music;audio synthesis;realistic audio synthesis;controllable music;synthesizers propose;synthesizers propose level;preferred midi;audio synthesis 13;controls midi;express piano music;performance synthesis train;piano music;listeners preferred midi;piano music features;synthesis train;midi ddsp;model music notes;notes performance synthesis;input controls midi;music features performance;preferred midi ddsp;generate realistic audio;polyphonic recordings"}, "55a3b36fd21dbbe9384ab3ba1bcf901235d95f47": {"ta_keywords": "questions decompositions cumbersome;questions decompositions;decompose question answering;labeling questions decompositions;question answering;subquestions structure quantum;question answering qa;questions matching;sequence transduction learns;simpler subquestions existing;nanotube recomposition model;subquestions structure;nanotube recomposition;automatically learns decompose;produce sub questions;nanotube;millions questions;simpler subquestions;carbon nanotube recomposition;decompositions cumbersome unsupervised;answering qa;subquestions;qaisaisais capable answering;questions;unsupervised sequence transduction;subquestions existing;learns decompose;carbon nanotube;quantum body;shedding light qa", "pdf_keywords": "question decompositions;question decompositions decompose;hop question decompositions;unsupervised learning decompose;decompose question sub;fact supervision sentences;learning decompose;subquestions predicted answers;learning machine translation;unsupervised decomposition train;compositional logic;fact supervision;machine translation;compositional logic questions;unsupervised decomposition;use unsupervised decomposition;supporting fact supervision;basic compositional logic;supervised decomposition;sub answers input;learning decompose multi;decomposition train;supervised decomposition powerful;machine translation approach;subquestions predicted;decomposition train decomposition;questions using step;novel unsupervised approach;supervised unsupervised;outcome supervised decomposition"}, "e153713b0423b4bae325340b2211e704effd5252": {"ta_keywords": "inductive logic programming;augment logic programs;logic programs;logic programming;logic programs variation;automatic generation boolean;logic programming ilp;evaluate inductive logic;logics significantly improve;generation boolean indicators;inductive logic;description logics significantly;boolean indicators numeric;generation boolean;augment logic;logics significantly;boolean indicators;counting clauses augment;machine learning systems;classes counting clauses;description logics;predicting fault density;calling tree representation;logics;predicting fault;clauses augment logic;boolean;allows counting clauses;programming;used description logics", "pdf_keywords": ""}, "e718ffe247a61a77b45953a7e8a5b86a45ed579f": {"ta_keywords": "japanese dependency parsing;dependency parsers trained;dependency parser trained;dependency parsing;dependency parsers;sentence dependency tree;dependency parser;dependency parsing approach;parsers trained;mst dependency parser;sentence dependency;parser trained;parsers trained fully;structure sentence dependency;parser trained partially;partially annotated corpora;parsers;annotated corpora;parsing;parser;japanese dependency;parsing approach allows;dependency tree;parsing approach;art dependency parsers;annotated corpora allowing;experiments japanese dependency;trained fully annotated;trained partially annotated;available linguistic", "pdf_keywords": ""}, "84e50df18c284b985d287b462c63c20186cc5da1": {"ta_keywords": "tool cognitive tutors;cognitive tutors;study dynamics biological;cognitive tutors evaluation;authoring tool cognitive;tool cognitive;technique programming;task fluid dynamics;dynamics biological;programming;tutors;tutors evaluation example;study dynamics;tutors evaluation;learning;dynamics biological systems;learning model;learning model used;incorporating technique programming;proposes machine learning;technique programming demonstration;interacting;machine learning;dynamics;given task fluid;programming demonstration;machine learning model;represented equations;dynamics described;approach study dynamics", "pdf_keywords": ""}, "efada589efdb0adf3aa9dc2b6cb6979a50658276": {"ta_keywords": "locomotion classification;locomotion classification based;locomotion data;based locomotion data;locomotion data present;classification based locomotion;locomotion;data set locomotion;set locomotion classification;based locomotion;fact checking;set locomotion;statistical analysis veracity;analysis veracity;analysis veracity estimation;correctness natural language;fact checking present;veracity estimation;context fact checking;verifying correctness;verifying correctness natural;features examine claim;veracity estimation false;veracity claim assessed;natural language processing;claim veracity;natural language;claim veracity claim;veracity claim;novel data driven", "pdf_keywords": ""}, "3c6407554fb4ee599f42501cf5cba8fcefa88783": {"ta_keywords": "paired data speech;data speech;speech recognition asr;utterances transcriptions;speech encoder;speech utterances transcriptions;data speech utterances;utterances transcriptions cycle;automatic speech;automatic speech recognition;speech recognition;audio data transcriptions;presents speech encoder;speech encoder state;end automatic speech;raw speech signal;data transcriptions;unpaired data training;based raw speech;recognition asr models;recognition asr;mainly language modeling;language modeling improve;speech signal;cycle consistency training;data training;speech utterances;speech signal investigate;consistency training;language modeling", "pdf_keywords": "end speech recognition;automatic speech;speech recognition;speech recognition asr;transcriptions automatic speech;automatic speech recognition;acoustic language data;recurrent neural network;scenario automatic speech;speech recognition experimental;language modeling improve;train encoders language;deep recurrent neural;language modeling;data transcriptions automatic;linguistic knowledge train;encoders language based;data language modeling;paired acoustic language;transcriptions automatic;audio data transcriptions;proposes deep recurrent;acoustic language;cycle consistency training;encoders language;recognition asr;language data;deep recurrent;recurrent neural;language data need"}, "b568c562fcfad8d7a943a9ea63aca36c487b6d7d": {"ta_keywords": "counterfactually revised counterparts;counterfactual target label;natural language processing;counterfactual target;classifiers trained original;counterfactually revised;document accords counterfactual;spurious features models;training models sensitive;language processing;classifiers;natural language;trained original data;datasets perform remarkably;classifiers trained;models sensitive spurious;fail counterfactually revised;task humans revising;functions classifiers trained;accords counterfactual target;humans revising document;sensitive spurious features;sensitive spurious patterns;particular classifiers trained;functions classifiers;language processing introducing;particular classifiers;counterfactual;data fail counterfactually;features models trained", "pdf_keywords": ""}, "afa9364ec48e38d19099cfc22ac9cb679c4baa39": {"ta_keywords": "exhibits gender biases;gender biases;stereotypically gendered entities;intermodality associations biases;gender biases preferring;text based bias;multimodal language models;set stereotypically gendered;stereotypically gendered;investigate multimodal language;multimodal language;stereotype faithfully describing;biases learned models;biases learned;stereotyped visual scene;intraand intermodality associations;biases;bias;stereotyped visual;gendered entities;intermodality associations;scene exhibits gender;associations biases learned;biases preferring;reinforce stereotype;bias analysis;stereotype faithfully;investigate multimodal;specifically demonstrate stereotyped;based bias analysis", "pdf_keywords": "automatically learning gendered;stereotypes visual linguistic;learning gendered representation;gender stereotypes visual;visual linguistic pretraining;biases affect language;learning gendered;racial gender stereotypes;visual biases;linguistic pretraining visual;entities visual biases;predictions visual linguistic;bias pre trained;affect language gendered;stereotypically gendered entities;model visual linguistic;visual biases affect;gender stereotypes;set stereotypically gendered;linguistic pre training;stereotypes visual;language gendered;visual linguistic pre;language gendered artefacts;multimodal training underlying;pre training biases;learned bias pre;linguistic pretraining;gender bias;gender bias expressed"}, "8b5071a38718194063cf17ca446ba8d9f4907a18": {"ta_keywords": "words models deepening;syntactically aware models;models natural language;outperforms models sentiment;syntactically aware;natural language processing;words models;models sentiment;learning compositionality inputs;deep neural;deep learning models;simple deep neural;question answering;models deepening network;models sentiment analysis;errors syntactically aware;sentiment analysis factoid;deep learning;learning compositionality;model syntactically ignorant;question answering tasks;bag words models;existing deep learning;models deepening;deepening network;model syntactically;novel variant dropout;dropout;simple deep;natural language", "pdf_keywords": ""}, "4240d8e1e5c2ef82d62ba9d7bb323c357c718c1c": {"ta_keywords": "topological layer;novel topological layer;exploit underlying topological;topological features input;underlying topological features;topological layer general;competition topological layer;underlying topological;topological layer international;layer general deep;topological features;task optimal backpropagation;deep learning;optimal backpropagation;optimal backpropagation requiring;persistence landscapes efficiently;backpropagation;persistence landscapes;based persistence landscapes;deep learning models;backpropagation requiring;backpropagation requiring input;novel topological;topological;competition topological;propose novel topological;competition competition topological;general deep learning;layer robust;layer", "pdf_keywords": ""}, "1678eccf0f3895dbea6dfac44fc9d4f86de15ff6": {"ta_keywords": "online conversation emotion;emotion online conversation;eliciting emotion online;predicting eliciting emotion;conversation emotion;emotional triggers languages;brain emotion trigger;conversation express emotion;recognition emotional triggers;emotion trigger;conversation emotion plays;emotion human interaction;affective communication;emotion affected conversational;human brain emotion;predict response human;emotion trigger used;social affective communication;human conversation;eliciting emotion;human conversation express;emotion human;reaction conversational partner;recognition emotional;affective communication concerned;emotion online;brain emotion;predict brain responds;express emotion;response human brain", "pdf_keywords": ""}, "751816df0027c0ae6c337ba392a5447bef86ca77": {"ta_keywords": "polymers machine learning;state energies oligothiophenes;energies oligothiophenes;energies oligothiophenes report;phase oligothiophenes predictions;energies poly hexylthiopnene;gas phase oligothiophenes;oligothiophenes predictions;phase oligothiophenes;oligothiophenes predictions compared;state energies poly;conjugated oligomers polymers;oligomers polymers;transfer learning;transfer learning models;energies poly;oligomers polymers machine;chemistry modeling optoelectronic;using transfer learning;excited state energies;use transfer learning;learning techniques chemistry;electronic excitations;conjugated oligomers;oligothiophenes;long conjugated oligomers;transfer learning address;learning models trained;p3ht single crystal;machine learning remains", "pdf_keywords": ""}, "a4577911d247e472772e2101d21aeaf8f46053cc": {"ta_keywords": "naturalness semantic parsing;verifying naturalness paraphrases;naturalness paraphrases using;paraphrases using probabilistic;semantic parsing;semantic parsing tasks;naturalness paraphrases;lm paraphrases generated;standard semantic parsing;model lm paraphrases;explicit ambiguous paraphrase;semantic parsing sp;lm paraphrases;paraphrases generated;parsing given natural;judge naturalness semantic;paraphrases generated derivation;paraphrases using;naturalness semantic;parsing;parsing tasks pernicious;word sense ambiguity;ambiguous paraphrase;reduce ambiguity;paraphrases;parsing tasks;sense ambiguity occurs;given natural language;natural language;reduce ambiguity point", "pdf_keywords": ""}, "fdb3969b654ab01be1807bbf84707a80e6283a52": {"ta_keywords": "automatically extracting structured;extracting structured;extracting structured representations;extracting structures expert;approaches extracting structures;synthesis procedures texts;structured representations synthesis;unsupervised approaches extracting;extracting scientific entities;model procedural text;extracting structures;structures expert annotated;expert annotated articles;annotated articles;extracted scientific entities;approaches extracting;procedure extraction graph;supervised models extracting;annotated articles strong;natural language;natural language narratives;models extracting scientific;automated procedure extraction;automatically extracting;models extracting;extraction graph source;representations synthesis procedures;syntheses inorganic;structured representations;syntheses inorganic compounds", "pdf_keywords": "action graph extraction;extracting action graphs;extraction action graph;action graphs synthesis;models extracting action;action graph structures;model extraction narrative;action graphs;extraction narrative schemas;action graph;generative model extraction;graphs synthesis text;extracting action;synthesis text;action graphs evaluate;structures synthesis text;extraction narrative;synthesis text accurate;supervised models extracting;models extracting;narrative schemas;model extraction;material science text;models word embedding;synthesis text propose;graph structures synthesis;materials science data;graphs synthesis;reporting chemical materials;graph extraction"}, "19f727b7a42a21bc3f99536e8368029f4b9b8e14": {"ta_keywords": "mapping noun images;noun images annotated;noun images;foreign language lexical;senses foreign language;language lexical;language represents;language represents things;bilingual dictionary;language lexical resource;bilingual dictionary conducted;lexical resource;lexical resource usage;usage bilingual dictionary;lexical;mapping noun;word senses foreign;foreign language;images annotated;study mapping noun;uniform language represents;language;dictionary;resource usage bilingual;collection word senses;word senses;senses foreign;images annotated collection;usage bilingual;art photography", "pdf_keywords": ""}, "97ef5081aa4e2984c16ea78b862266e4852c7faf": {"ta_keywords": "email tagging folders;task email tagging;activity represented folder;based link analysis;email tagging;tagging folders;email messages related;messages relevant folder;consider task predicting;link analysis;folder associated ongoing;relevant folder associated;folder novel task;finding messages relevant;task identify email;folder associated;graph based link;task predicting;tagging folders multiple;messages related item;predicting future involvement;task finding messages;identify email messages;link analysis method;task predicting future;enterprise ongoing activity;relevant folder;ongoing activity represented;represented folder novel;evaluate task email", "pdf_keywords": ""}, "af034b0e893a0a24e41cdb54afb35d4250407f50": {"ta_keywords": "dynamics pedestrian killed;dynamics pedestrian;pedestrian killed non;pedestrian killed;study dynamics pedestrian;killed non stationary;pedestrian;non uniform force;stationary non uniform;uniform force;dynamics;killed non;non stationary;stationary non;stationary;non stationary non;force;non uniform;killed;study dynamics;uniform;comprehensive study dynamics;non;paper present;paper present results;paper;study;present;comprehensive;present results comprehensive", "pdf_keywords": ""}, "7657b56d2ac9269b32e8bcbe2a20f99ea17afe09": {"ta_keywords": "age singing voice;singing voice age;method age singing;perceived age singing;age singing;voice age singer;voice characteristics singing;characteristics singing voice;dependent modeling singers;age singer perceived;voice age;modeling singers sing;modeling singers;converted singing voice;singing voice;controlling voice;singing voice higher;controlling voice timbre;characteristics singing;sing expressively controlling;measures voice characteristics;singing voice retaining;age singer;voice characteristics;expressively controlling voice;singer individuality using;retaining singer individuality;voice retaining singer;sing expressively;singers sing expressively", "pdf_keywords": ""}, "869d53277b0ec5e47a30b874aeb157df88649ea0": {"ta_keywords": "dynamics coupled pendulum;pendulum oscillators pendulum;pendulum oscillators;pendulum pendulum oscillators;coupled pendulum;coupled pendulum pendulum;oscillators pendulum modeled;oscillators pendulum;temperature dynamics coupled;pendulum modeled pendulum;pendulum like pendulums;modeled pendulum pendulum;pendulums;pendulum modeled;pendulum pendulum;modeled pendulum;pendulum pendulum like;temperature dynamics;pendulum;effect temperature dynamics;oscillators;pendulum like;dynamics coupled;like pendulums;effect temperature;investigate effect temperature;temperature;dynamics;coupled;modeled", "pdf_keywords": ""}, "bff4d630cbea6a90b149b28caff5489c1a4ccaad": {"ta_keywords": "entanglement species;dynamics video entanglement;video entanglement species;entanglement species used;dynamics;properties fluid dynamics;dynamical properties fluid;fluid dynamics;entanglement;fluid dynamics video;video entanglement;dynamics video;study dynamical properties;behavior single species;dynamical properties;dynamical;single species;properties fluid;species;fluid;study dynamical;behavior single;properties;species used;detailed study dynamical;present detailed;species used gain;video;behavior;insight behavior single", "pdf_keywords": ""}, "7e0342b304ca8ce564a664eb17e85358b07488fe": {"ta_keywords": "estimating noise mixture;adaptation noise mixture;noise mixture model;noise speaker adaptation;noisy mixture model;speaker adaptation noise;mixture model noise;parameters noisy mixture;noise mixture;noise suppression unsupervised;transformation noise suppression;model noise suppression;joint speaker adaptation;mixture model estimation;clean speech noise;noise suppression including;noisy mixture;based noise suppression;speaker adaptation technique;noise suppression;speaker adaptation;speech noise speaker;suppress noise noisy;unsupervised joint speaker;noise suppression accurate;noise model crucial;method estimating noise;estimating noise;model based noise;suppress noise", "pdf_keywords": ""}, "fcdac45272543b4f8b8eaa59d66044d1b7018494": {"ta_keywords": "pre trained translation;trained translation model;trained translation;performance translation multilingual;translation multilingual;translation model;translation model pseudoparallel;translation multilingual setting;performance translation;evaluate performance translation;multilingual;translation;multilingual setting method;multilingual setting;meta learning;meta learning algorithm;method meta learning;pseudoparallel data;pseudoparallel;model pseudoparallel data;model pseudoparallel;pseudoparallel data generates;method meta;learning algorithm adapts;pre trained;learning;algorithm adapts pre;learning algorithm;adapts pre trained;setting method meta", "pdf_keywords": "forward translation models;trained translation model;predictive machine translation;pre trained translation;machine translation cross;machine translation low;trained translation;translation models;train forward translation;translation machine translation;translation machine;translation low resource;boost translation quality;translation models validation;machine translation;forward translation;machine translation machine;translation model;translation model pseudoparallel;translation cross lingual;sk machine translation;translation quality;translate monolingual data;boost translation;able translate monolingual;translating low resource;low resource multilingual;cross lingual learning;translation low;translate monolingual"}, "e8f42dd98d7f546036fa4a1109c3fe3dd98f9647": {"ta_keywords": "natural language argumentation;argumentation related tasks;natural language argument;arguments highly contextualized;argumentation results reveal;language argumentation results;argument summarization focus;argument reasoning comprehension;language argumentation;argument component identification;arguments news comments;argumentation results;abstractive argument summarization;argument reasoning;argument summarization;instances suitable argumentation;highly contextualized reasoning;argumentation related;detection argument component;argumentation;focus argument reasoning;stance detection argument;reasoning related content;suitable argumentation related;authentic arguments news;contextualized reasoning;comments reasoning crucial;news comments reasoning;task argument reasoning;arguments news", "pdf_keywords": ""}, "4358335263622fe189cf95c613f4d6fdcb67fbea": {"ta_keywords": "population animals action;animals action external;behavior population animals;animals action;population animals;action external force;animals;external force;behavior population;force;behavior;study behavior population;action external;population;action;study behavior;paper study behavior;paper study;external;study;paper", "pdf_keywords": ""}, "03fff40cff6ac531e340f6ffb376e34609770846": {"ta_keywords": "coordinated campaigns used;coordinated campaigns;cryptocurrencies coordinated campaigns;dynamics social networks;coordination information warfare;influence campaigns;campaigns used influence;information warfare;networks examining identities;influence campaigns diverse;social networks;information warfare scenarios;retweets temporal patterns;social networks examining;war cryptocurrencies coordinated;manipulate social media;accounts likely coordinated;uncoversings network topology;uncoversings network;networks examining;campaigns diverse contexts;coordination networks;campaigns;search uncoversings network;studies influence campaigns;social media;sequences retweets temporal;networks;influence manipulate social;campaigns used", "pdf_keywords": "accounts tweets;coordination social media;accounts retweeting tweets;accounts retweeting;manipulation social bots;accounts tweets tweet;media accounts retweeting;activities accounts tweets;images tweets;social media accounts;duplicated images tweets;malicious groups;detect coordinated accounts;conspiracy theories malicious;tweets;hashtags multiple tweets;malicious groups employ;disinformation campaigns;bots use image;targeted disinformation campaigns;theories malicious groups;multiple tweets;tweets set accounts;level suspicious behaviors;images tweets image;tweets image;suspicious behaviors;multiple tweets propose;social bots use;identify coordinated accounts"}, "eb07ff030df4c3dc20e85d89c2e0d0cc730918a0": {"ta_keywords": "separation using speaker;recording speaker inventory;speech separation using;known speech separation;speech separation;using speaker inventory;speaker inventory;speech separation received;facilitate speech separation;speaker inventory fully;long recording speaker;recording speaker;speaker embedding;speaker embedding multi;reverberant long recording;controlling length speech;improve separation performance;multi talker robust;speaker information;regions speech signal;utterance noisy environment;speech signal lensaging;paper speaker embedding;speaker information facilitate;talker robust;external speaker signals;target speech extracted;additional speaker information;embedding multi talker;speaker signals experiments", "pdf_keywords": "inventory speaker separation;speech separation based;channel speech separation;speaker separation;speech separation problem;talker speech separation;recording speaker inventory;speech separation;speaker separation received;embedded speaker inventory;speaker inventory;build speaker inventory;speaker inventory fully;speaker inventory speaker;inventory speaker;long speech mixture;sequentially embedded speaker;speaker inventory long;recording speaker;long recording speaker;separation based sequentially;improve separation performance;inventory long speech;single channel speech;speech mixture using;speech mixture;separation based;noisy reverberant datasets;reverberant long recording;multi talker speech"}, "4c66979a31fa4be5b5814fdb5cb8411572d61da8": {"ta_keywords": "normalization language data;maps historical wordforms;normalization language;historical wordforms;historical wordforms modern;modern wordforms;deals normalization language;modern wordforms rules;finding exact matchings;matchings pair nonlinear;luther bible weighted;aligned versions luther;normalization;paper deals normalization;language data;wordforms modern wordforms;exact matchings;bible weighted;bible weighted according;versions luther bible;luther bible;language data early;deals normalization;pair nonlinear waveforms;exact matchings pair;wordforms rules;nonlinear waveforms approach;wordforms modern;wordforms;nonlinear waveforms", "pdf_keywords": ""}, "729260566c7fdf689bb04eaaecef59d40da93ef7": {"ta_keywords": "detection deception attack;attack deep neural;image classifier attacker;malicious deception attacks;algorithm detection gravitational;classifier attacker;thresholding algorithm detection;attacks pixel;deception attack deep;model attacks pixel;detection gravitational;perturbation based detection;modified images dnn;deception attacks;images dnn based;classifier attacker model;dnn based image;adaptive stochastic thresholding;stochastic thresholding algorithm;detection gravitational wave;detection reduce computational;classification autonomous cyber;attacks pixel values;stochastic thresholding;detection deception;deception attack;detection modified images;facilitates detection modified;detection modified;dnn based", "pdf_keywords": "detecting adversarial images;detecting adversarial;algorithms adversarial image;algorithm detecting adversarial;adversarial image;adversarial images greatly;adversarial images;systems adversarial images;adversarial;security systems adversarial;adversarial image detection;adversarial images based;algorithms adversarial;attack deep neural;systems adversarial;adversarial images proposed;class algorithms adversarial;detection deception attack;deception attack deep;dnn based image;attack deep;network dnn;neural network dnn;network dnn based;detection deception;deception attack;dnn based;detection;vehicles vulnerable attacks;image classi\ufb01cation autonomous"}, "ede8ba65c4db10d357d9c3bf8e75b092f536fc84": {"ta_keywords": "vision language navigation;reasoning panoramic action;human generated instructions;vision language;instruction follower;approach vision language;augmentation pragmatic reasoning;pragmatic reasoning panoramic;instructions natural language;baseline instruction follower;natural language instructions;learning reasoning;enable learning reasoning;language navigation;approach speaker driven;panoramic action space;perceptual context;paths panoramic action;reasoning panoramic;panoramic action;based perceptual context;generated instructions natural;language instructions typically;data augmentation pragmatic;learning reasoning process;perceptual context paper;generated instructions;language instructions;directed paths panoramic;augmentation pragmatic", "pdf_keywords": "sequentially using attention;vision language navigation;instruction best trajectory;navigation task trajectory;using attention mechanism;language navigation task;task trajectory search;navigation task;sequence words route;instruction follower;baseline instruction follower;trained using sequence;vision language;task trajectory;language navigation;using attention;attention mechanism;route actions sequentially;trajectory search;trajectory environment navigate;words route description;instruction following task;improves performance instruction;natural language instructions;attention;treat vision language;actions sequentially using;instruction following based;route description instruction;words route"}, "d5634a21b3727258822b78f5c5ababf7261a5c79": {"ta_keywords": "speech enhancement separation;noise speech separation;tasks speech enhancement;speech enhancement;dynamics swimmer noisy;speech separation;apply speech enhancement;self supervised;learning based enhancement;self supervised learning;swimmer noisy environment;speech enhancement suppresses;swimmer noisy;model dynamics swimmer;noise speech;background noise speech;dynamics swimmer;supervised;speech separation extracts;controlling strength noise;target speech interfering;based enhancement separation;speed swimmer controlled;enhancement separation methods;applying self supervised;concept self propelled;supervised learning based;filterbank;supervised learning;enhancement separation", "pdf_keywords": "ssl speech enhancement;ssl models speech;models speech enhancement;enhancement speech separation;speech enhancement separation;recognition speech enhancement;speech enhancement;speech representations improve;speech enhancement speech;experimental results voicebank;enhancement speech;ssl speech;voicebank;increase robustness speech;robustness speech representations;investigate ssl speech;fbanks pretraining audios;speech separation;separation speech diarization;models speech;range speech processing;utterance mixing augmentation;voicebank demand libri2;speech processing tasks;results voicebank;speech processing;robustness speech;wide range speech;voicebank demand;pretraining audios real"}, "3813627f7fec57aa4c15b791e36912f470273bb1": {"ta_keywords": "topical clusters hashtags;clusters hashtags propose;clusters hashtags;identify communities hashtag;hashtag usage twitter;communities hashtag;communities hashtag usage;hashtags study;interact hashtags study;quality clusters hashtags;twitter discussion;clusters hashtags shift;hashtags propose;users interact hashtags;19 twitter discussion;covid 19 twitter;usage twitter;hashtags;hashtags propose novel;interact hashtags;twitter;facebook twitter;online social media;trends discussion occurring;topical clusters;hashtags study use;twitter distinct temporal;hashtag usage;discussion occurring online;multi view clustering", "pdf_keywords": ""}, "3e8420d1bebb3f93d285da2de801d2e43b290880": {"ta_keywords": "neural sequence taggers;sequence labeling tasks;neural sequence labeling;sequence taggers;sequence taggers large;tasks requiring annotations;sequence labeling;trained language models;human annotation;nlp tasks;sequence labeling important;annotations;meta learning self;annotations token;annotation;cost human annotation;labeling tasks;requiring annotations;requiring annotations token;learning self training;labeling tasks requiring;processing nlp tasks;multilingual ner datasets;exacerbated sequence labeling;training neural sequence;trained language;pre trained language;annotations token level;massive multilingual ner;tagging datasets task", "pdf_keywords": "meta learning learn;weighting meta learning;samples improve learning;learning data challenging;meta learning;leverage meta learning;self training framework;training framework adaptive;validation learning;learning data;pseudo labeled predictions;adaptive self training;labeling task;trained language model;sequence labeling;sequence labeling task;labeled validation;meta learning objective;pre trained language;validation learning active;labeled predictions teacher;novel labeled data;exchange validation learning;training framework;weighting labeled data;single sequence labeling;method pre training;labeled validation set;labeled predictions;labeled data"}, "e51bca890c004c43b25c5a5e7aa968fe70ec2668": {"ta_keywords": "stacked graphical learning;stacked graphical models;stacked graphical model;graphical learning;graphical learning recent;graphical models;relational datasets hyperlinked;propose stacked graphical;meta learning scheme;stacked graphical;meta learning;graphical models demonstrated;algorithm stacked graphical;propose meta learning;predictions related instances;traditional machine learning;propose algorithm stacked;learning methods;called stacked graphical;graphical models compare;machine learning;datasets hyperlinked web;algorithm stacked;relational datasets;citations social networks;learning scheme;instance predictions;machine learning methods;work graphical models;graphical model", "pdf_keywords": ""}, "622e05f5d3dd430644288d5048f6050f37947de7": {"ta_keywords": "related domain adaptation;domains genres tasks;domain adaptation;learning named entity;transfer learning information;multitask learning domains;transfer learning named;transfer learning;supervised transfer learning;named entity recognition;problem transfer learning;learning domains related;entity recognition motivated;learning domains;entity recognition;information domains genres;task natural language;learning problem transfer;genre adaptation;supervised transfer;domain adaptation model;useful information domains;inter genre adaptation;genre adaptation related;genres tasks;adaptation related domain;domains genres;structure supervised transfer;similar hierarchical priors;learning information", "pdf_keywords": ""}, "146b84bdd9b9078f40a2df9b7ded26416771f740": {"ta_keywords": "inverse reinforcement learning;inverse reinforcement;learning markov decision;reinforcement learning markov;risk sensitive reinforcement;problem inverse reinforcement;sensitive reinforcement learning;reinforcement learning algorithm;reinforcement learning;markov decision processes;markov decision;learning markov;agent risk;making agent risk;benchmark problem risk;risk;agent risk sensitive;models human decision;minimize loss;minimize loss function;learning algorithm seeks;risk metrics models;learning algorithm;decision making agent;risk metrics;learning algorithm convergence;reinforcement;risk sensitive risk;seeks minimize loss;coherent risk metrics", "pdf_keywords": ""}, "77899bac8f463b7a77c0c282748e989d419386e7": {"ta_keywords": "difficult relation extraction;relation extraction;semi supervised learning;relation extraction task;learning agreement constraints;supervised learning agreement;supervised learning proposed;regularization constraints learners;semi supervised;supervised learning;known ssl benchmark;ssl benchmark;ensembles semi supervised;ssl benchmark results;performance supervised learning;known ssl;learning noisy;performance supervised;learning agreement;learning noisy noisy;supervised;regularization constraints;technique learning noisy;learning proposed method;constraints learners;regularization;learning proposed;extraction task propose;noisy data;bayesian optimization", "pdf_keywords": "supervised learning ssl;semi supervised learning;propose semi supervised;semi supervised;new semi supervised;semi supervised machine;strategies semi supervised;learn unlabeled data;learning ssl;semian semi supervised;learner declaratively constrained;learning ssl technique;relation extraction;supervised learning;unlabeled data semian;supervised machine learning;relation extraction task;declaratively state ssl;supervised learning cation;declaratively constrained entropy;ssl heuristics;ssl heuristics technique;learn unlabeled;supervised;machine learning called;ssl benchmarks obtains;model learn unlabeled;state ssl heuristics;learner allows declaratively;learning model learn"}, "1a53e7446274016f737236bdd48e3ff05d966384": {"ta_keywords": "language code retrieval;code retrieval;code summarization data;retrieval code summarization;natural language code;code summarization;code retrieval code;grained alignments stackoverflow;quality code snippets;like code synthesis;code snippets;code synthesis;data natural language;code synthesis natural;language nl code;like code;code snippets propose;snippets correspondence features;extracted snippets correspondence;code using neural;alignments stackoverflow promising;fine grained alignments;synthesis natural language;tasks like code;structure extracted snippets;high quality code;grained alignments;codes based;language code;retrieval code", "pdf_keywords": "code mining;purpose code mining;code mining experiments;code retrieval;summarization code retrieval;code snippets languages;code retrieval code;natural language code;retrieval code retrieval;code snippets;code summarization;code summarization code;code retrieval paper;accurate code snippets;snippets languages;extract snippets java;language code;asnippet purpose code;snippets java;extract snippets;significantly accurate code;target programming language;snippets java vice;summarization code;programming language;snippets;snippets languages previous;language code pur;retrieval code;purpose code"}, "d0e9c5cb669dec908a38eab4315cbf101bc4b0a0": {"ta_keywords": "domain adaptation;domain adaptation statistical;statistical machine translation;language models trained;approach domain adaptation;machine translation;neural language models;general domain text;language models;machine translation hypothesize;language models viable;based language models;trained small domain;general domain corpora;domain text select;language models makes;small domain text;select similar sentences;adaptation statistical machine;domain corpora;domain text;words neural language;modeling unknown word;neural language;text select similar;learning based language;words neural;machine learning;domain corpora paper;text select", "pdf_keywords": ""}, "308eb6751a3a1da0f64f291366c8ee27f84b3f16": {"ta_keywords": "fluid dynamics video;fluid dynamics;motion rigid;motion rigid body;external force;rigid body affected;presence external force;dynamics;rigid body;dynamics video;video motion rigid;dynamics video motion;motion;rigid;force;video motion;fluid;affected presence external;presence external;body affected presence;external;affected presence;video;presence;body affected;body;affected", "pdf_keywords": ""}, "2550fafc0cbd8bbf7aadd864ac569596d33db038": {"ta_keywords": "hybrid systems ground;systems ground;systems ground state;ground state preparation;water hybrid systems;approach ground state;questions aspects grounding;complete sense grounding;grounding;new approach ground;sense grounding;grounding missing nlp;hybrid systems;ground;water water hybrid;water hybrid;approach ground;communication interlocutors;ground state;state preparation water;communication interlocutors ability;successful communication interlocutors;aspects grounding;preparation water water;nlp tasks;nlp tasks present;preparation water;coordination purviews;grounding missing;mutual information required", "pdf_keywords": "grounding natural language;semantic parsers;complete sense grounding;based semantic parsers;semantic parsers recommend;natural language processing;natural language;sense grounding;natural language based;present framework grounding;language based semantic;aspect natural language;automatically grounding collaborative;framework grounding natural;concept scenario grounding;grounding collaborative;parsers;vision language navigation;semantic;truth grounding;sense grounding grounding;systematic evaluation grounding;based semantic;scenario grounding;truth grounding given;priors grounding common;language navigation challenging;parsers recommend;priors grounding;grounding common"}, "7ce80c7df1774e4483b32a813d54a8ff35dd0163": {"ta_keywords": "guarantees generative adversarial;learning dynamics stackelberg;dynamics stackelberg games;sum games learning;convergence learning dynamics;converges stackelberg equilibria;stackelberg gradient dynamics;employing gradient play;neural networks stackelberg;convergence guarantees generative;stackelberg games;learning dynamics;train adversarial;adversarial;generative adversarial networks;stackelberg games stable;learning dynamics propose;adversarial networks trained;guarantees generative;adversarial networks;effectively train adversarial;gradient play;adversarial neural networks;networks stackelberg equilibria;adversarial networks demonstrate;stackelberg gradient;simple learning dynamics;stackelberg equilibria;neighborhood stable stackelberg;adversarial neural", "pdf_keywords": "learning dynamics;learning dynamics zero;stackelberg equilibria gans;stackelberg learning dynamics;continuous generalsum games;gradient learning dynamics;learning dynamics formulate;learning dynamics simultaneous;nash stable attractors;convergence learning dynamics;zero sum games;learning dynamics paper;gradient descent existence;stackelberg game particular;stackelberg game;structure stackelberg game;stackelberg learning;equilibria gans;simultaneous gradient dynamics;zero sum game;attractors stackelberg equilibria;update stackelberg learning;gradient dynamics;stackelberg equilibria;deterministic gradient update;dynamics simultaneous gradient;deterministic gradient;generalsum games;gradient dynamics empirically;generalsum games emulate"}, "203636315f7c9526189d88c541bedf623d63ea7c": {"ta_keywords": "factoid question answering;answer ambiguous questions;question answering;question answering context;answer question answering;answer question factoid;question answering qais;answering context factoid;answer summaries questions;question factoid;answering context;ambiguous questions synthesize;answer ambiguous;questions synthesize factual;summaries questions ambiguous;questions ambiguous goal;ambiguous questions;answering;answer question answer;questions ambiguous;answering qais use;synthesize factual information;goal answer questions;question factoid question;ambiguous goal answer;answer questions;summary resolves ambiguity;question answer paper;answering qais;answer summaries", "pdf_keywords": "ambiguous questions crowdsourced;questions crowdsourced languages;generate textual answer;questions crowdsourced;answer develop crowdsourcing;textual answer extract;answer ambiguous questions;crowdsourced languages;crowdsourced long form;question ambigq crowdsourced;crowdsourced languages work;crowdsourcing pipeline;crowdsourced long;ambigq crowdsourced long;ambigq crowdsourced;answer answers collect;answers coherent passage;short answers coherent;textual answer;valid short answers;develop crowdsourcing pipeline;crowdsourced;generative qa;crowdsourcing pipeline collect;short answers;annotators;answer extract;answers collect;annotators need;crowdsourcing"}, "018bf5da2ba1f1901e98f72c7eedbf6b91967192": {"ta_keywords": "new privacy preservation;level privacy preservation;privacy preservation;privacy utility d\u03c7;new privacy;privacy preservation recent;retaining level privacy;private information natural;privacy preservation approach;propose new privacy;privacy utility;level privacy protection;mining natural language;utility d\u03c7 privacy;privacy protection;embeddings leak private;privacy context regulation;level privacy;d\u03c7 privacy;data mining natural;natural language processing;privacy;field natural language;natural language;guidance level privacy;text embeddings leak;d\u03c7 privacy context;investigate privacy utility;leak private information;information natural language", "pdf_keywords": "privacy adaptive pretraining;learning privacy adaptive;supervised learning privacy;learning privacy;propose privacy adaptive;privacy preserving nlu;privacy adaptive;privacy adaptive manner;data privacy constrained;investigate privacy adaptive;privacy preserving;privacy constrained;providers privacy adaptive;utility privacy constrained;explorations privacy preserving;pretraining token representation;maintaining privacy guarantees;improve utility privacy;token representation privatization;adaptive pretraining token;privacy guarantees propose;privacy conscious users;token embeddings;privacy guarantees;utility maintaining privacy;corpora self supervised;guarantees impede privacy;data privacy;maintaining privacy;guarantees propose privacy"}, "a6a7374c5ddac1446ceab9d7cbe5a3305238d0ee": {"ta_keywords": "conversation dialog corpora;conversation corpora;conversation scripts television;natural conversation templates;trigram conversation turn;conversation scripts;turn trigram conversation;conversation corpora created;conversations people conversation;portray actual conversations;filter conversations;extract conversations television;dialog corpora television;use conversation scripts;conversation templates;presented extract conversations;extract conversations;people conversation;work conversation corpora;conversations television;conversation templates examples;conversation dialog;trigram conversation;natural conversation;people conversation dialog;conversation turn;dialog corpora;use conversation;conversation performed;filter conversations speakers", "pdf_keywords": ""}, "963c4c34f1292f64a6e9fc04428fc7a0893b8ef3": {"ta_keywords": "single image super;image super;super resolution;rescales hierarchical features;image super resolution;networks cnns widely;spatial channel attention;cnns widely;channel attention processing;residual attention group;attention group;cnns widely used;enhanced residual attention;networks cnns;super resolution sisr;feature fusion magnify;cnns;convolutional neural;channel attention;feature fusion;attention processing module;attention processing;residual attention;metrics visual quality;sisr computer vision;quality single image;novel image processing;magnify;convolutional neural networks;attention", "pdf_keywords": ""}, "53feb3b34425ea95c259e8d0693edd490d6b470f": {"ta_keywords": "semantic violation sentences;violation sentences semantic;semantic violation words;words visual auditory;sentences semantic violation;stimuli japanese;auditory stimuli japanese;sentence semantic violation;mistaken words visual;words visual;visual stimuli experiment;visual stimuli;processing mistaken words;semantically mistaken words;visual auditory stimuli;auditory japanese sentence;semantic violation translated;examine semantic;violation sentences;examine semantic processing;sentences semantic;semantic violation common;knowledge semantic violation;mismatched feelings semantically;violation words;words japanese sentences;semantic violation;auditory stimuli;stimuli;sentence semantic", "pdf_keywords": ""}, "a792d5a1e9a6a53edd8cbc00e387bc07c54e423c": {"ta_keywords": "mechanism crowdsourcing peer;crowdsourcing peer;mechanism crowdsourcing;crowdsourcing peer grading;new mechanism crowdsourcing;truthful responses agents;eliciting truthful responses;crowdsourcing;peer prediction;peer prediction method;bayesian truth serum;responses agents;pioneered peer prediction;peer grading;agents beliefs arbitrary;problem eliciting truthful;eliciting truthful;allow agents beliefs;bayesian truth;output agreement;participants needed;agents beliefs;peer;elicitation agents;participants;peer grading class;elicitation agents furthermore;method bayesian truth;estimating number participants;known answers propose", "pdf_keywords": ""}, "10efdde1ae3a9d359ac1aae0bd5ef7bfd68810dd": {"ta_keywords": "multilingual representation learning;pre trained multilingual;multilingual learning pre;multilingual representations pre;multilingual pre training;multilingual learning;trained multilingual language;trained multilingual;better multilingual representations;multilingual representations;multilingual representation;algorithm multilingual learning;multilingual pre post;multilingual language models;brain demonstrate multilingual;learning algorithm multilingual;masked language modeling;representations pre trained;learn better multilingual;demonstrate multilingual representation;multilingual pre;goal multilingual pre;study multilingual pre;predict language;language learning objective;key language learning;language modeling mlm;multilingual language;predict language specific;rely masked language", "pdf_keywords": "cross lingual synonyms;learning cross lingual;crosslingual sentence retrieval;predicting cross lingual;lingual transfer learning;lingual synonyms multilingual;lingual synonyms;synonyms multilingual document;synonyms multilingual;cross lingual word;lingual word alignment;cross lingual representation;trained model crosslingual;bilingual dictionaries pre;leverages bilingual dictionaries;cross lingual;model crosslingual sentence;bilingual dictionaries;facilitate cross lingual;cross lingual supervision;stronger cross lingual;language prediction task;semantically rich language;cross lingual transfer;model crosslingual;language synonyms;dictionaries pre training;parallel text tasks;dict language prediction;language prediction"}, "ef1d93b03c20b2f488b66e8e2c24fceb2105d58f": {"ta_keywords": "metaphors natural language;lexical semantic;model identify metaphoric;metaphoric expressions languages;language lexical;language lexical useful;natural language lexical;metaphorically using lexical;lexical useful;construction use metaphors;using lexical semantic;semantic;idea quantum described;lexical semantic features;lexical;metaphors;identify metaphoric expressions;lexical useful tool;identify metaphoric;using lexical;syntactic construction;quantum described hamiltonian;metaphors natural;bilingual dictionary model;idea quantum;expressions languages;quantum described;metaphoric expressions;metaphoric;syntactic construction meant", "pdf_keywords": ""}, "3993788eb252f5eb7fc19e9f98357a72f9f0476d": {"ta_keywords": "mixed membership networks;membership network models;membership networks;membership networks small;mixed membership network;networks small nodes;mixed membership relationships;cluster recovery mixed;membership network;cluster recovery;balanced clusters;encourages balanced clusters;network models;networks;link perplexity cluster;clusters;nodes usually;perplexity cluster recovery;experiments networks;nodes;networks small;cluster;nodes usually taking;members network;perplexity cluster;membership relationships model;mixed membership encourages;membership network lead;small nodes usually;experiments networks different", "pdf_keywords": ""}, "71c7104eaed93497824cf197949c77e7d6cb36d3": {"ta_keywords": "answers drawn corpus;corpus knowledge base;corpus knowledge;question answering;domain question answering;drawn corpus knowledge;question answering answers;learning retrieve reasoning;retrieve reasoning heterogeneous;reasoning heterogeneous information;knowledge base;corpus;graph answers;corpus supplemented large;knowledge base combination;answering answers drawn;retrieve reasoning;corpus supplemented;graph answers question;approach answering;answering answers;answer experimentally pullnet;question answer pairs;learning retrieve;corpus used incomplete;answering question answer;answering question;drawn corpus;approach answering question;based approach answering", "pdf_keywords": "corpus question answering;question answering;questions text corpus;question answering machine;answering machine learning;answer question answering;machine translation semantic;translation semantic parsers;question answering large;semantic parsers;learning machine translation;machine translation;translation semantic;text corpus;semantic parsers paper;knowledge bases;knowledge bases kbs;compositional reasoning;effectiveness iterative retrieval;parsers;text corpus question;body knowledge bases;corpus supplemented large;compositional reasoning setting;retrieval;iterative retrieval;answering;answering machine;corpus supplemented;corpus"}, "911536dc3dfbbbf2bb8d71181b31e0aa7920b9f6": {"ta_keywords": "expert solutions adaptive;decision theoretic online;exponentially weighting expert;experts distributed prediction;experts algorithms;expert strategies set;experts algorithms capable;theoretic online learning;weighting expert solutions;games financial markets;adaptive algorithm expert;algorithm expert solutions;expert solutions;expert strategies;expert solutions aggregated;experts distributed;decision theoretic;strategies games financial;problem decision theoretic;strategies games;distributed prediction;adaptive algorithm;financial markets play;methods experts algorithms;predictions suffering losses;financial markets;algorithm expert;solutions adaptive algorithm;adaptive algorithm tracking;online learning", "pdf_keywords": ""}, "798e45ea830884be36c3f526d3b169eaba95f989": {"ta_keywords": "local nash equilibria;nash equilibria nondegenerate;zero sum games;nash equilibriumilibria zero;nash equilibria zero;differential nash equilibria;stable nash equilibria;differential nash equilibriumilibria;nash equilibria generic;continuous zerosum games;generic local equilibrium;games zero sum;games local nash;local nash;differential nash equilibrium;zerosum games;stable nash;sum games zero;nash equilibriumilibria;sum games local;showing local nash;differential nash;adversarial learning;adversarial network approaches;adversarial learning generative;adversarial;adversarial network;nash equilibria;structurally stable nash;zerosum games works", "pdf_keywords": "equilibria continuous games;continuous games equilibria;zero sum games;sum continuous games;games generalization zero;differential nash equilibria;zero sum game;local nash equilibria;nash equilibria continuous;nash equilibria zero;continuous games;equilibria games known;equilibria games;local nash equilibrium;sum games generalization;games generalization classical;differential nash equilibrium;games equilibria games;games equilibria;differential nash;known differential nash;continuous games shown;reevolution local nash;play differential nash;games generalization;gradient play dynamics;nash equilibrium;nash equilibrium introduced;nash equilibrium concept;notion local nash"}, "8a0a8568acf2b95c9cb471e28ee6b25c5e4fe186": {"ta_keywords": "senticnet models semantics;symbolic systems senticnet;semantics sentics representing;semantic affective resource;semantics sentics;available semantic affective;models semantics sentics;semantic affective;sense knowledge representations;senticnet models;senticnet;networks typical symbolic;sentics representing information;concept level sentiment;senticnet makes;senticnet makes use;affective resource;common sense knowledge;systems senticnet;knowledge representations;sense knowledge;level sentiment analysis;affective resource concept;sentiment analysis;semantic;level sentiment;sentics representing;representing information symbolic;available semantic;symbolic systems", "pdf_keywords": ""}, "e75c388b60cf447be7148be25feeee3e10d12cf4": {"ta_keywords": "training data extrapolation;new interpolation extrapolation;interpolate training data;dimensional dataset interpolation;interpolation extrapolation function;correctly interpolate training;dataset interpolation surely;interpolation extrapolation;interpolate training;interpolation extrapolation method;dataset interpolation;new interpolation;extrapolation function approximations;approximations deep learning;interpolation surely;method interpolation extrapolation;interpolation surely happens;propose new interpolation;interpolation;function approximations deep;interpolate;interpolation occurs;data extrapolation;new method interpolation;approximations deep;dataset fluid dynamics;happens interpolation;extrapolation function;surely happens interpolation;extrapolation method numerical", "pdf_keywords": "extrapolation information preserved;interpolation extrapolation regime;extrapolation indicators generalization;extrapolation regime;interpolation extrapolation information;generalization performances empirically;probability interpolation grows;samples interpolation extrapolation;deep learning;interpolation surely happens;extrapolation information;interpolation grows;generalization performances;interpolation extrapolation de\ufb01nition;current interpolation extrapolation;interpolation extrapolation;gradient descent logistic;generalization performances demonstrating;interpolation surely;interpolation grows exponentially;gradient descent;extrapolation de\ufb01nition;extrapolation;data demonstrate interpolation;interpolation extrapolation indicators;maintain interpolation new;question interpolation extrapolation;interpolation new samples;approach deep learning;maintain interpolation"}, "42c3c50b8e368ee2e1b52d010b6c53b3d732770c": {"ta_keywords": "human sentiment annotation;human sentiment annotations;transcripts based sentiment;sentiment annotations;sentiment annotation;sentiment annotation boost;sentiment annotations available;learn sentiment information;learn sentiment;speech sentiment approach;speech sentiment;data human sentiment;annotation boost performance;trained language models;annotation boost;end speech sentiment;models learn sentiment;semi supervised training;sentiment information written;trained language model;automatic speech;sentiment information;speech dataset training;employing automatic speech;predicting performance speech;semi supervised;language models learn;understanding sentiment;sentiment analysis;speech recognizer trained", "pdf_keywords": "human sentiment annotation;human sentiment annotations;model speech sentiment;sentiment annotation;sentiment annotations;speech sentiment analysis;sentiment annotation boost;sentiment annotations available;annotation speech dataset;human annotation speech;approaches speech sentiment;annotation speech;data human sentiment;e2e speech sentiment;pretrained bert models;speech sentiment;semi supervised training;annotation boost;annotation boost performance;semi supervised;bert models extract;propose semi supervised;bert models;leverage pretrained bert;embedding text tokens;ef\ufb01ciently human annotation;human sentiment;condition human sentiment;training approaches speech;understanding sentiment"}, "5dcbdb9bf80575953b5d21f378d8139f0a44168b": {"ta_keywords": "automatic recognition emotion;recognition emotion using;identifying user emotion;recognition emotion;natural spoken dialogue;dialogue corpus;support vector machine;colorful dialogue corpus;emotion triggered speakers;vector machine svm;user emotion investigating;dialogue corpus paper;vector machine;emotionally colorful dialogue;machine svm;spoken dialogue;user emotion;colorful dialogue;svm;recognition person true;emotion triggers human;emotion using;svm present analysis;recognition person;spoken dialogue essential;method recognition person;analysis regarding emotion;dialogue;svm present;triggers human communication", "pdf_keywords": ""}, "bc632f81dab322ac610a8d11463cc1bba6130eda": {"ta_keywords": "text normalization datasets;historical text normalization;text normalization relies;text normalization;normalization datasets languages;task learning architectures;task learning;multi task learning;zero shot learning;languages using autoencoding;autoencoding grapheme phoneme;autoencoding grapheme;using autoencoding grapheme;normalization relies;normalization;normalization datasets;task historical text;learning pml task;machine learning pml;shot learning;task learning lead;autoencoding;languages context probabilistic;normalization relies small;shot learning outperforms;lemmatization auxiliary tasks;using autoencoding;task learning configurations;learning noisy;learning architectures", "pdf_keywords": "characteristic machine translation;words automatic normalization;outperforms machine translation;languages normalization task;automatic normalization text;machine translation;powerful machine translation;text normaliza task;languages normalization;normaliza task learning;automatic normalization challenging;machine translation methods;model tasks normalization;normalization languages;automatic normalization;machine translation csmt;normalization text;zero shot learning;machine translation method;approach automatic normalization;languages autoencoding powerful;tasks normalization lemmatization;tasks normalization;normalization task target;normalization challenging;normalization lemmatization autoencoding;normalization challenging task;languages autoencoding;written languages autoencoding;normalization text written"}, "13b674bb3078623608045a18570b47f6e49a8358": {"ta_keywords": "visual question answering;text image coreferences;image coreferences achieves;image coreferences;annotate paintings;art results task;contains images paintings;annotate paintings contour;dataset paintings;ontology associate image;textual description humans;images paintings;countries annotate paintings;knowledge gleaned images;dataset paintings gallery;inexpensive dataset paintings;identify artistic work;annotate;question answering;art results;identify artistic;artistic work;textual description;paintings;paintings contour data;coreferences achieves;coreferences;text image;mentions ontology;image regions text", "pdf_keywords": ""}, "4b9795493a937b9034be9c26afab23f6dc751f62": {"ta_keywords": "retrieval based language;datastore search algorithm;retrieval based;datastore search;automaton built datastore;costly datastore search;nearest neighbor searches;present datastore search;automaton unsupervised learning;list retrieval based;computationally costly datastore;list retrieval;language models;language models lm;retrieval;datastore search performed;based language models;traversing automaton inference;weighted finite automaton;searches knn lm;searches;flat list retrieval;costly datastore;language model;search algorithm based;automaton built;search performed frequently;search;corpus text domain;automaton unsupervised", "pdf_keywords": "nearest neighbors datastore;knn searches matching;knn searches;retrieve nearest neighbors;nearest neighbors;location nearest neighbors;nearest neighbors perplexity;disjoint corpus datastore;neighbors datastore means;neighbors datastore;predicting location nearest;able retrieve nearest;matching perplexity knn;81 knn searches;dataset disjoint corpus;retrieve nearest;fusion language models;corpus datastore;perplexity knn;language models improves;language models;corpus datastore built;searches matching;perplexity knn lm;searches matching perplexity;neighbors perplexity trained;nearest;predicting location;perplexity trained dataset;knn"}, "a7b6802f20c399615dbac161678cd6a6d2df5a97": {"ta_keywords": "quality control crowdsourcing;machine learning crowdsourcing;learning crowdsourcing today;learning crowdsourcing;crowdsourcing today;crowdsourcing today artificial;crowdsourcing;control crowdsourcing describing;crowdsourcing describing;control crowdsourcing;crowdsourcing describing relevant;human loop insightful;loop subjective human;human loop pipelines;aggregation quality estimation;introduction human loop;sequence aggregation;human loop subjective;use human loop;large scale language;using human loop;sequence aggregation discuss;scale language models;aggregation quality;language models;human opinions gather;human loop;training evaluating models;loop subjective;methods aggregation quality", "pdf_keywords": ""}, "7df6aa19f50c8ec5f12d58e0685ed5c6e9a08bb2": {"ta_keywords": "reviews inputs recommender;like product reviews;generate plausible reviews;generating reviews model;generating reviews;reviews estimate nuanced;using reviews inputs;recommend existing products;reviews inputs;product reviews;products recommend existing;reviews estimate;plausible reviews estimate;reviews model;reviews provide valuable;reviews model output;using reviews;inputs recommender;personalized rankings;provide personalized rankings;focus generating reviews;recommender focus generating;personalized rankings existing;instead using reviews;products recommend;recommendation focus learning;existing products recommend;recommender;product reviews proved;rankings existing products", "pdf_keywords": ""}, "a3e3a9d878999c7038c275e75f5cd8a232aa4999": {"ta_keywords": "speech tasks;learning task diversity;tasks transfer learning;trained self supervised;various speech tasks;models various speech;learning model trained;benchmark evaluate pretrained;self supervised learning;pre trained models;speech tasks paper;self supervised;trained models;pretrained models;model trained;supervised learning task;model trained self;evaluate pretrained models;task diversity superb;present self supervised;learning task;transfer learning;supervised learning powerful;pretrained models various;resources supervised learning;transfer learning proven;task supervision;resources supervised;representations learned;limited task supervision", "pdf_keywords": "similarity tasks speech;speech representations self;discrete speech representations;speech representations;representations self supervised;learning discrete speech;tasks speech signal;self supervised learning;deep machine learning;tasks speech;machine learning natural;self supervised;learning natural language;similarity tasks;automatic learning;natural language processing;acoustic linguistic;deep machine;automatic learning discrete;discrete speech;representation language;correlation similarity tasks;supervised;supervised learning;acoustic linguistic prosodic;trained models;learning natural;linguistic prosodic speaker;speech;present deep machine"}, "b5002aa334f8d0c0e1a4dedad79580e10a928c30": {"ta_keywords": "supervised learning ssl;ssl training domain;learning ssl models;training domain target;training domain;models automatic speech;new machine learning;deep learning;based speech tasks;learning based speech;learning algorithm domain;ssl training;learning ssl;recognition speech translation;self supervised learning;ssl data domain;automatic speech;automatic speech recognition;speech translation tasks;speech recognition;machine learning;ssl models automatic;representation ssl data;speech recognition speech;recognition speech;deep learning based;domain target data;speech tasks particu;various deep learning;speech tasks", "pdf_keywords": "supervised speech;speech representations spectral;framework supervised speech;supervised speech processing;speech analytics;speech representations;speech processing;spectral features machine;speech analytics require;based speech analytics;learning based speech;features ssl representations;representations spectral features;combine spectral features;new machine learning;spectral features ssl;learnable transformations concatenation;features machine learning;concatenation convolutional attention;adaptation mixture experts;combining features propose;speech processing paper;combining features;fig speech representations;stone able learn;based speech;learnable transformations;spectral features;using learnable transformations;transformations concatenation convolutional"}, "8809d0732f6147d4ad9218c8f9b20227c837a746": {"ta_keywords": "speech processing toolkit;recognition speech separation;text speech recognition;results automatic speech;automatic speech recognition;end speech processing;speech recognition;toolkit study topological;automatic speech;speech processing;recognition speech;separation text speech;speech separation text;speech recognition speech;text speech;speech separation;topological materials;topological materials paper;class transformer models;effective toolkit;simple effective toolkit;transformer models;study topological;processing toolkit;pre trained models;state art transformer;processing toolkit mainly;defects topological materials;trained models;study topological defects", "pdf_keywords": "model automatic speech;speech applications large;model various speech;speech recognition tasks;speech recognition;end speech recognition;various speech applications;automatic speech;speech applications;automatic speech recognition;speech languages;speech recognition paper;research speech applications;speech languages english;mixed speech languages;pre trained convolution;trained models;convolution neural networks;available corpora;trained convolution neural;conformer research speech;neural networks end;models training;corpora including;publicly available corpora;corpora including various;corpora;trained convolution;neural networks;various speech"}, "d8252e24b6036ca895800b547698ab44d09ae350": {"ta_keywords": "recipient machine learning;motion pedestrian crowded;pedestrian crowded environment;pedestrian crowded;machine learning;motion pedestrian;simple machine learning;machine learning used;pedestrian;model motion pedestrian;machine learning model;learning model motion;recipient machine;email messages;crowded environment;sensitive email;sensitive email message;recipient;learning model;sends sensitive email;personal information email;particles needed;searches require awareness;particles needed carry;crowded;information email messages;motion;sends sensitive;personal information;difficult searches", "pdf_keywords": ""}, "f91c24b0dc56a6b377e99e046d7540e5bb7aa46e": {"ta_keywords": "student information management;informatization college student;information management;student management;information management university;student information;student cs management;college management;administrator student management;reform informatization college;informatization college;college management traditional;student management personnel;student information paper;vocational college management;analysis student information;information management spring;management personnel students;performance information management;business function module;requirements administrator student;student cs;college student cs;administrator student;education reform informatization;management spring student;personnel students;management university;higher vocational college;student", "pdf_keywords": ""}, "a6a7724763d8adba466519489b0b9d209e7f2d15": {"ta_keywords": "train testing meta;train testing metrics;machine translation summarization;translation summarization dialog;translation text summarization;new meta evaluation;evaluation framework train;meta validation train;text generation propose;evaluation text;text generation;translation summarization;machine translation text;machine translation;summarization dialog involve;datasets machine translation;involve text generation;summarization dialog;meta evaluation;text summarization;meta evaluation framework;variety nlp applications;summarization different perspectives;train testing;variety nlp;applications machine translation;nlp applications machine;text summarization different;nlp applications;validation train testing", "pdf_keywords": "text summarization pretrained;summarization pretrained;summarization pretrained encoders;translation summarization tasks;computer translation summarization;summarization text matching;summarization tasks;translation summarization;text summarization;extractive summarization;summarization text;summarization tasks paper;topic based summarization;extractive summarization topic;summarization paper propose;traditional text summarization;summarization topic based;summarization uses;ways text summarization;text summarization paper;summarization;based summarization;abstractive summarization text;based summarization uses;summarization topic;summarization paper;abstractive summarization;summarization uses abstractive;uses abstractive summarization;evaluation text"}, "e3f1a9c3d87e9828cdeb08ba90a260c69e974a75": {"ta_keywords": "dance sequence prediction;digital dance sequence;dance generation audio;dance sequences;large dance sequences;dance sequence;dnns performing rhythmic;dance sequence decodes;dance sequences target;sequence dance sequences;networks motion music;dance sequences ii;sequence dance;target dance sequence;generates basic dance;digital dance;basic dance generation;music motion beat;sequences target dance;motion music tasks;present sequence dance;motion beat;neural networks motion;motion music;time basic dance;basic dance;music motion;movements motion music;dancer motion;dance", "pdf_keywords": "driven dance synthesis;dance sequences using;music driven dance;long dance sequences;dance synthesis;large dance sequences;neural networks dance;dance sequences;generate long dance;dance generation tasks;dance synchronized music;generates basic dance;networks dance generation;generating basic dance;music rhythm;deep recurrent neural;dance sequences ii;dance synthesis paper;synchronized music rhythm;music motion beat;basic dance synchronized;timing music motion;music motion;long dance;supervised deep recurrent;deep recurrent model;rhythm;timing music;measure large dance;dance synchronized"}, "78d57a1ecd724c5f8b1534372969d5b35daa6d4b": {"ta_keywords": "model constituency parsing;constituency parsing achieves;constituency parsing;neural model constituency;direct search generative;base parsers;base parsers decoding;outputs base parsers;search generative;search generative models;generative neural;propose generative neural;parsers;parsers decoding straightforward;parsers decoding;generative models difficult;parsing;generative neural model;parsing achieves;parsing achieves state;model constituency;generative models;generative;propose generative;decoding straightforward;generative models paper;candidate outputs base;rescore candidate outputs;neural;constituency", "pdf_keywords": "parsers generative neural;generative parsers;base parsers generative;parsers generative;neural constituency parsers;generative parsers independent;explore generative parsers;generative parsers rg;dependency parsing neural;models constituency parsing;rd generative parsers;incremental dependency parsing;discriminative parser;parsing neural networks;search directly generative;constituency parsers;investigate discriminative parser;independent base parsers;word2vec generative models;parsing neural;parser rd generative;constituency parsing;base parsers;dependency parsing;word2vec generative;performance word2vec generative;discriminative parser rd;parsers;parsers independent base;generative reranking"}, "b6c6e06b4bc68349845b30e01e01d7603f468547": {"ta_keywords": "optimal relay;optimal relay locations;provides optimal relay;allocation placement relays;duplex relaying;rate duplex relaying;multiple relay;multiple relays;placement relays;duplex relaying presence;multiple relay case;presence multiple relays;optimal power allocation;structure multiple relay;forward relaying;multiple relays decode;relaying;relay locations;relay locations formulate;placement relays line;cost markov decision;relay;total cost markov;relays line exponentially;power allocation placement;cost markov;relaying presence multiple;multi hop network;relay case;relays", "pdf_keywords": "optimal power allocation;single relay placement;relay placement;relay placement problem;power allocation maximizes;placing relay nodes;relay nodes;power allocation;expected number relays;transmit power optimized;relay nodes line;total transmit power;placing relay;number relays;wireless networks emergency;single relay;deployed relays;relays aim maximize;relay;transmit power;number relays used;source deployed relays;total transmit;sum power constraint;allocation maximizes achievable;power constraint;solve single relay;allocation maximizes;problem placing relay;wireless networks"}, "7729fbebff327bebb9292dc1c51c51dd55390954": {"ta_keywords": "verb order tensor;tensors sentence similarity;compositional distributional semantic;tensors transitive verb;distributional semantic;distributional semantic methods;tensor based quantum;verb unconstrained tensors;unconstrained tensors sentence;tensors sentence;tensors transitive;tensor based;tensors;tensors require orders;semantic inference;tensors require;low rank tensors;inference based compositional;semantic methods low;order tensor;sentence similarity;order tensor paper;use tensor based;tensor;based compositional distributional;tensor paper propose;sentence similarity verb;rank tensors transitive;tensor product hermitian;compositional distributional", "pdf_keywords": ""}, "f3271e61dc0507183ee399393129d7888c2f82b9": {"ta_keywords": "state tomography qst;detector based quantum;automatically predicted quality;quantum state tomography;automatic quality estimation;tomography qst;photon detector;predicted quality scores;supervised quality estimation;photon detector based;fully automatic quality;tomography qst method;evaluating quality output;predict overall quality;single photon detector;supervised quality;quality estimation;automatic quality;overall quality estimate;predicted quality;estimates quality reliably;quality estimation approaches;quantum state;quality estimate framework;data evaluating quality;quality estimation paper;quality output language;quality scores;quantum;lightly supervised quality", "pdf_keywords": ""}, "68aa7c7b65365c3303d5024b1273408fb435d178": {"ta_keywords": "entrainments affect dialogue;entrainment dialogue;effect entrainment dialogue;dialogue acts lexical;entrainment dialogue acts;dialogue affects human;entrainment factor dialogue;entrainment lexical level;dialogue affects;dialogue systems;manner entrainment lexical;dialogue systems like;entrainment lexical;dialogue abstract structural;dialogue;dialogue abstract;dialogue acts;affect dialogue abstract;affect dialogue;manner entrainment;factor dialogue affects;guidelines dialogue systems;similar manner entrainment;given dialogue acts;choice given dialogue;given dialogue;correlations entrainment;entrainment;entrain users similar;lexical level", "pdf_keywords": ""}, "b0ddd849c5ae0004678fa483908c06d87894f3ab": {"ta_keywords": "hovercraft decision tree;tuning parameter training;bayesian model selection;parameter training;bayesian criterion;bayesian criterion method;hmm using bayesian;based variational bayesian;word recognition;bayesian approach criterion;types word recognition;headed hovercraft;hovercraft decision;parameter training data;hovercraft;single headed hovercraft;selection criterion based;variational bayesian approach;using bayesian criterion;word recognition paper;variational bayesian;headed hovercraft decision;decision tree;model selection;bayesian framework;ss hmm using;bayesian framework report;practical bayesian framework;mmse practical bayesian;recognition paper propose", "pdf_keywords": ""}, "b0d644277933988c00b22d8ae012512fe498ad62": {"ta_keywords": "fasttext word embeddings;trained fasttext word;trained fasttext;pre trained fasttext;word sense disambiguation;word embeddings;linguistic knowledge representations;linguistic knowledge;sense disambiguation disambiguation;fasttext word;sense disambiguation;unsupervised knowledge free;quality linguistic knowledge;disambiguation word senses;approaches word sense;linguistic;fasttext;word senses context;disambiguation disambiguation word;word quality linguistic;supervised knowledge;disambiguation word;senses context easy;knowledge free;word sense;unsupervised knowledge;disambiguation disambiguation;building supervised knowledge;knowledge representations;context easy humans", "pdf_keywords": "word sense clustering;word sense disambiguation;words contexts clustering;sense disambiguation text;word sense induction;sense disambiguation;sense vectors word;multilingual word sense;sense disambiguation performs;graphs word senses;sense clustering;subgraphs semantics polysemous;semantic graph embedding;semantic similarity graph;semantically related words;sense disambiguation important;polysemous words vectors;semantics polysemous words;vectors word sense;word embedding;words semantically related;semantic graphs;word vector models;semantic similarity;semantic graph;word embedding models;semantic graphs word;word sense;sense clustering construct;embedding vectors word"}, "ebc64974e9e0021984a0158b3c04b60327730a88": {"ta_keywords": "question answering;base question answering;defending division swordfish;swordfish tournament;division swordfish tournament;knowledge response;knowledge base;solely knowledge response;question answering lqa;predictive control based;swordfish tournament paper;scale knowledge base;division swordfish;knowledge response external;predictive control;knowledge base question;swordfish;large scale knowledge;loop predictive control;defending defending;defending;neural architecture large;defending defending division;answering lqa interact;neural;neural architecture;based solely knowledge;present neural architecture;knowledge;defending division", "pdf_keywords": ""}, "f0cd4de3cdf547dcdcc6995dca9ab3f65955b324": {"ta_keywords": "layer recurrent models;multi layer recurrent;recurrent neural networks;layer recurrent;recurrent models;recurrent models limited;recurrent neural;state recurrent neural;recurrent models results;learning deep;projected state recurrent;state recurrent;deep multi;deep multi layer;learning deep multi;family recurrent neural;challenge learning deep;recurrent;flow;language models sequence;modeling sequences;neural networks called;neural;flow information inside;fluid dynamics models;flow information;modeling sequences experiments;networks called latticerecurrent;neural networks;fluid dynamics video", "pdf_keywords": "recurrent neural networks;gradient recurrent neural;recurrent neural network;deep recurrent neural;gradient recurrent;deep recurrent;recurrent neural;sentence recurrent neural;vanishing gradient recurrent;better language models;components recurrent neural;language models;language modeling datasets;level language modeling;language modeling;models language modeling;main components recurrent;ability recurrent neural;components recurrent;sequence models;sequence models lrus;language models language;sequence sentence recurrent;learn nonlinear transformations;language modeling established;neural networks learn;language modeling classic;characters words sequence;family deep recurrent;inputs characters words"}, "485b3f77b9913e151e7ca897d99497e70e7f30d1": {"ta_keywords": "optimizing performance quantum;neural machine translation;tuned language task;performance quantum processor;hyperparameter tuned language;quantum processor;machine translation neural;granularity neural machine;quantum processor based;tuned language;translation neural;performance quantum;translation neural network;embeddings larger words;machine translation;subword units hyperparameter;granularity subword units;neural machine;larger words incrementally;segmentation granularity neural;quantum;effective neural machine;performance rare words;words incrementally constructed;words incrementally;words way embeddings;learning paradigm granularity;optimizing segmentation granularity;search optimizing;granularity neural", "pdf_keywords": "incremental machine translation;machine translation learns;translation learns automatically;generate new words;automatically insert subwords;subwords target language;tune subword granularity;translation learns;granularities translating languages;subword granularity;subword granularity single;morphologically rich languages;machine translation;insert subwords target;insert subwords;learns automatically insert;subwords target;generating rare word;new vocabulary online;new words minimal;subwords;segmentation granularities translating;ensure vocabulary added;granularities translating;learns automatically;translating languages;introducing new vocabulary;tune subword;word forms memory;translating languages particularly"}, "1e4e2aceed87febcc643f1473507c9535ba5c19a": {"ta_keywords": "translate target language;cross lingual encoding;speech translation st;slu speech translation;lingual encoding;speech processing tasks;target language;speech translation;language understanding slu;translation st achieving;language target language;speech processing;language target;lingual encoding method;cross lingual;success speech processing;target language target;task streaming baselines;different scoring systems;scoring systems propose;target language experimental;st task streaming;scoring systems;propose cross lingual;language experimental results;streaming baselines gained;streaming baselines;lingual;understanding slu speech;task streaming", "pdf_keywords": "task streaming translators;streaming translators;simultaneous speech translation;asr task streaming;outputs target translations;blockwise streaming transducers;proposed crosslingual encoding;cross lingual encoding;target language translations;crosslingual encoding;streaming transducers;streaming transducers achieve;crosslingual encoding method;speech translation;target translations;lingual encoding;optimized target language;streaming transducers proved;st task streaming;cross lingual;speech translation paper;task streaming baselines;language translations experimental;target translations propose;employs cross lingual;results proposed crosslingual;translators;lingual encoding method;language translations;compared blockwise transducers"}, "6e24bcfcdb31afbb313a13c1c84cb779ceb17500": {"ta_keywords": "sequential decision maker;sequential decision;process sequential decision;decision problem reference;learning process sequential;propose hidden markov;algorithm finding parking;hidden markov model;given decision problem;learning process;decision problem repeatedly;city decision problem;hidden markov;decision problem;decision problem decision;finding parking lot;decision maker;markov model;finding parking;markov;problem decision problem;decision maker paper;person given decision;decision paper;parking lot city;parking;city decision;parking lot;choose action maximize;action maximize value", "pdf_keywords": ""}, "0d20360c5d533760d97d7ce19b78d4791a5173cb": {"ta_keywords": "dynamics terrorist attacks;terrorist terrorist network;terrorism results algorithm;terrorist network;terrorist network process;terrorist attacks;terrorist attacks creation;terrorism results;study terrorism results;location terrorist terrorist;study terrorism;heterogeneous dynamics terrorist;target location terrorist;dynamics terrorist;network terror;location terrorist;terrorist terrorist;terrorism;network based inference;inference algorithms predict;terrorist;applies study terrorism;algorithms predict target;meta network terror;predict target;predict target location;algorithms predict;terror;inference algorithms;learning models", "pdf_keywords": ""}, "5d9fe38b750f59b4ef0a2b58fde0f60d4317c7ad": {"ta_keywords": "multi domain learning;learning multiple metadata;attributes multi domain;domain learning;multi domain swimmer;domain learning technique;domain learning baseline;metadata attributes multi;multiple metadata attributes;technique learning multiple;learning multiple;attributes multi;novel multi domain;domain swimmer;domain learning assumes;domain attribute;best domain attribute;single metadata attribute;multi domain;multiple metadata;domain swimmer used;metadata attributes;learning technique learning;outperform multi domain;single metadata;domain attribute impact;metadata attribute;attribute impact classification;single attribute;technique learning", "pdf_keywords": ""}, "242c35b91fe1d7aedab9d1da7652aad2219d4784": {"ta_keywords": "retraining behavior eeg;eeg spring models;eeg eeg spring;eeg spring;behavior eeg eeg;speech incompressible fluid;coalescence events;behavior eeg;eeg eeg;model deeper speech;binary coalescence events;eeg;speech encoder;deeper speech encoder;speech incompressible;simulation binary coalescence;coalescence events case;binary coalescing events;fluid pre training;speech encoder confirm;coalescence;coalescing events;binary coalescence;encoder;retraining;translate speech incompressible;autocorrelation;compressible fluid pre;coalescing events challenging;post retraining behavior", "pdf_keywords": ""}, "924e43c4de98743d2e7c14c241b03b2109325b90": {"ta_keywords": "speech tagging;speech tagging word;collapsed gibbs sampling;tagging word segmentation;gibbs sampling;parallel sampling;parallel sampling methods;word segmentation;results speech tagging;previous parallel sampling;gibbs sampling multiple;word segmentation blocked;segmentation blocked sampling;sampling;tagging word;sampling methods present;sampling multiple processors;sampling methods;blocked sampling;distributing collapsed gibbs;tagging;blocked sampling method;empirical results speech;sampling multiple;memory efficient paper;speech;sampling method;statistically correct memory;training neural networks;collapsed gibbs", "pdf_keywords": ""}, "030fade3049e0847702393dde3100ecc41a5e86a": {"ta_keywords": "probabilistic hill climbing;demonstration entanglement;probability practical learning;entanglement;learner seeks maximize;experimental demonstration entanglement;probabilistic hill;demonstration entanglement form;entanglement form quantum;quantum;hill climbing search;climbing process learner;entangling;quantum repeater;cases probabilistic hill;entanglement form;form quantum repeater;climbing search;form quantum;seeks maximize;test cases probabilistic;seeks maximize utility;entangling power;optimal;probabilistic;high probability practical;optimum arbitrarily;locally optimal;local optimum arbitrarily;nd locally optimal", "pdf_keywords": ""}, "bea54062d105b9fe3250ce3569cf817e54772894": {"ta_keywords": "neural sequence labeling;treebanks training;historical treebanks training;treebanks training data;historical treebanks;treebanks;historical syntax accuracy;probabilistic parser present;modern historical treebanks;sequence labeling approaches;sequence labeling;probabilistic parser;unlexicalized parser outperforms;parser outperforms sequence;automatic recognition phrases;outperforms sequence labeling;tool probabilistic parser;sequence labeling tool;parser;parser outperforms;unlexicalized parser;syntax accuracy phrase;analysis historical syntax;historical syntax;syntax accuracy;recognition phrases historical;phrases historical german;parser present results;recognition phrases;historical corpora", "pdf_keywords": "parser identified phrases;parser automatic recognition;general parsing results;unlexicalized parser outperforms;general parsing;study parser automatic;machine parsing;unlexicalized parser;parsing results heavily;parser automatic;parsing;accuracy unlexicalized parser;machine parsing methods;study parser;automatic recognition phrases;parser;treebank;parser outperforms;parser outperforms sequence;parsing methods;methods general parsing;treebank inclusion;neural sequence labeling;parsing results;parser identified;parsing methods general;phrases historical german;identified phrases correcting;selected treebank;sequence labeling approach"}, "38705aa9e8ce6412d89c5b2beb9379b1013b33c2": {"ta_keywords": "deep nets nonparametric;semiparametric inference semiparametric;develop semiparametric inference;semiparametric inference results;semiparametric inference focusing;inference semiparametric;estimation deep;semiparametric inference;effectiveness deep learning;semiparametric inference demonstrate;inference rates semiparametric;deep learning monte;valid semiparametric inference;nets nonparametric regression;inference semiparametric decision;deep nets;estimation deep learning;bounds deep nets;networks use semiparametric;deep neural;deep feedforward;convergence deep feedforward;deep learning;neural nets rates;deep neural networks;deep networks;nets nonparametric;use semiparametric inference;rates semiparametric inference;deep feedforward neural", "pdf_keywords": "estimation games deep;learning methods economics;choice estimation games;games deep neural;deep learning explicitly;results deep nets;deep learning empirical;deep nets good;deep neural;learning predicting health;choice estimation;deep nets;deep learning predicting;deep learning;targeting results deep;selection models games;approach deep learning;discrete choice estimation;approach deep;learning explicitly model;constructing deep learning;games deep;deep neural networks;learning explicitly;estimation games;adoption deep learning;deep learning implementations;predicting health outcomes;novel approach deep;learning predicting"}, "4dd85ae17a5fd0bce09ffef0455b6e827d7e1e2b": {"ta_keywords": "detection linear attacks;linear attacks numerical;attack distributed cyber;linear attacks;probability attack based;linear attacks presence;detection probability attack;attack distributed;constraint attack detection;attack detection;distributed cyber physical;attack detection probability;attacks numerical;constraint attack;class linear attacks;injection attack distributed;sensors goal attacker;attacks numerical results;respecting constraint attack;distributed cyber;probability attack;attacker steer estimates;estimates agent nodes;network agent nodes;attacks presence attacker;observations agent nodes;attack based;attack external attacker;attacks;attacker false data", "pdf_keywords": "distributed attack detection;attack distributed cyber;distributed estimation scheme;distributed cyber physical;secure state estimation;cyber physical systems;distributed estimation;distributed attack;linear attack distributed;novel distributed estimation;systems cyber physical;physical systems cyber;attack distributed;estimation v2x networks;optimal linear attack;distributed cyber;attack detection;attack detection probability;observations paper cyber;systems cyber;attack detection problem;state estimation control;estimation control;paper distributed attack;linear attacks;networked monitoring control;estimation control noiseless;packet drop attacks;state estimation v2x;linear attack"}, "5143ebd23322fe805bed2667fcfb70920c105f7f": {"ta_keywords": "demonstrations cognitive skill;demonstrations cognitive;multiple demonstrations cognitive;demonstrations involve implicit;characteristics effective demonstrations;experts multiple demonstrations;studies expressive demonstrations;cognitive tutor computerized;model cognitive tutor;effective demonstrations;multiple demonstrations;cognitive tutor;learning simulated student;counterintuitive pedestrian behavior;expressive demonstrations;cognitive skill;demonstrations involve;pedestrian behavior counterintuitive;learning simulated;tutor teaches human;effective demonstrations lead;pedestrian explained action;demonstrations lead quicker;cognitive skills observing;learning agent;cognitive skills;learned cognitive;expressive demonstrations opposed;teaches human;human students cognitive", "pdf_keywords": ""}, "cfbe9183f2fe2847f7a3c811f6309a2cab3f85cf": {"ta_keywords": "scientific summarization dataset;summarization dataset scitldr;extractive summarization scientific;existing summarization datasets;extractive summarization tasks;summarization scientific;summarization scientific documents;released scientific summarization;summarization datasets;summarization datasets report;scientific summarization;summarization tasks investigate;summarization dataset;summarization tasks;domain pretraining corpus;leverages existing summarization;pretraining corpus;extractive summarization;pretraining corpus changing;performance extractive summarization;existing summarization;summarization;tasks large pretrained;success extractive summarization;word embeddings trained;pretraining performance extractive;contextualized word embeddings;pretraining interacts contextualized;dataset scitldr paper;corpus", "pdf_keywords": ""}, "20086d6a9fab6081f300e08d3f952cb9b16e6de8": {"ta_keywords": "file location sharing;location sharing;shortest retrieval;shortest retrieval times;provide shortest retrieval;fast synchronous single;fast synchronous;single file location;implementation fast synchronous;location sharing sss;fastest benchmarks data;superlinear indices great;benchmarks data indexed;file location;fastest benchmarks;stores residual strings;synchronous single file;retrieval;retrieval times;sharing sss superlinear;benchmarks including fastss;fast simple implementation;known benchmarks;performance fastest benchmarks;superlinear indices;data indexed;approach known benchmarks;retrieval times present;hash;sss superlinear indices", "pdf_keywords": ""}, "693f5d55e0561099944f5e00e301bf26db0b972d": {"ta_keywords": "acoustic utterance semantic;utterance semantic representation;statistical semantic analysis;statistical semantic;utterance semantic;computer aided speech;speech recognition;methods statistical semantic;speech recognition process;input acoustic utterance;acoustic utterance;aided speech recognition;aided speech;stochastic models training;semantic representation;semantic analysis;semantic representation goal;semantic analysis paper;utterance;semantic;stochastic models;speech;semantic analysis represent;process input acoustic;fundamental stochastic models;thesis fundamental stochastic;stochastic;recognition process input;input acoustic;fundamental stochastic", "pdf_keywords": ""}, "939a149f156425b83e48ea72e9e09a55ea33b8d7": {"ta_keywords": "reconstruction context convex;representation signal reconstruction;feature signal reconstruction;signal reconstruction;signal reconstruction problem;signal reconstruction context;domain signal reconstruction;algorithm signal reconstruction;crossing representation wavelet;novel iterative reconstruction;signal reconstruction stabilized;iterative reconstruction algorithm;iterative reconstruction;representation wavelet transform;reconstruction stabilized multiscale;representation wavelet;reconstruction algorithm;reconstruction algorithm reconstruct;wavelet transform;reconstruct image zero;wavelet transform domain;context convex optimization;image zero crossing;convex optimization;convex optimization based;multiscale zero crossing;algorithm reconstruct image;reconstruct image;based projections convex;reconstruction problem reduces", "pdf_keywords": ""}, "0fbb90b8fe1d02a4f0f616df9a09ec42eace53bd": {"ta_keywords": "voice activity detection;voice activity;method voice activity;unsupervised method voice;parallel classifiers noise;classifier online;classifiers noise case;activity detection vad;classifiers noise;speech noise case;noise case speech;speech noise;parallel classifiers;reliability classifier online;classifiers;case speech noise;consider parallel classifiers;classifier;evaluations new unsupervised;reliability classifier;assess reliability classifier;method voice;activity detection;vad evaluations;censrec database designed;vad evaluations new;voice;evaluations conducted censrec;algorithm remote recording;vavad algorithm", "pdf_keywords": ""}, "e21633b0b5e55dce56bc07e919c6d12ecf8cef0c": {"ta_keywords": "clauses constant locality;learnable constate clauses;clauses pac learnable;locality strictly expressive;constant depth clauses;locality present formal;constant locality strictly;constant locality;locality strictly;constate clauses pac;nonrecursive clauses constant;depth clauses recently;pac learnable generalization;expressive constant depth;clauses pac;language nonrecursive clauses;generalizations language pac;learnable obvious syntactic;depth clauses;determinate clauses pac;clauses constant;depth determinate clauses;locality present;programs called locality;locality;obvious syntactic generalizations;nonrecursive clauses;constate clauses;syntactic generalizations;learnable constate", "pdf_keywords": ""}, "c305e3314c0853b14911f704c68b04cfc9ea7aa1": {"ta_keywords": "crosslingual dependency parsing;dubbed treebank;dubbed treebank used;universal dependency parsing;treebank used resource;technology dubbed treebank;treebank used;treebank;techniques crosslingual dependency;dependency parsing gaining;dependency parsing;multilingual languages;new multilingual language;multilingual language technology;twittersphere trees;parsing;multilingual language;dynamics family twittersphere;new multilingual;family twittersphere trees;parsing gaining attention;cross lingual techniques;lingual techniques crosslingual;propose new multilingual;multilingual;lingual techniques;resource multilingual languages;crosslingual dependency;cross lingual;twittersphere trees connected", "pdf_keywords": "indian treebanking;indian treebanking international;used indian treebanking;automatic parser;linguistic tools parsers;treebanking international standard;treebanking international;learnability automatic parser;discusses parsing;hindi conduct parsing;parsers translation systems;discusses parsing performance;treebanking;learnt efficiently parser;parsers;section discusses parsing;parsers translation;hdtb annotation scheme;automatic parser ud;parsing;tools parsers translation;parser;efficiently parser;annotation scheme ud;hdtb annotation;convert hdtb annotation;parser ud formalism;convert annotation scheme;dependencies stanford languages;tools parsers"}, "652579315d767331d8e05ea46489e6bd081ef48a": {"ta_keywords": "peer evaluation;peer evaluation techniques;peer evaluation answers;approach peer evaluation;peer evaluation based;peer evaluators;techniques peer evaluators;evaluation techniques peer;feedback peer evaluation;current peer evaluation;evaluation feedback peer;students evaluate;classroom evaluation students;peer evaluators assign;classroom evaluation;evaluation students;techniques peer;approach peer;evaluation students performance;feedback peer;peer;alternative approach peer;set students evaluate;students performance;evaluation answers;students evaluate lag;students;evaluation;evaluation answers submitted;aspect classroom evaluation", "pdf_keywords": ""}, "b293e4659e20815bcf0b6d31ce46b8bd9437c1fa": {"ta_keywords": "machine learning privacy;privacy machine learning;learning privacy emerged;learning outperforms privacy;learning privacy;machinelearning based privacy;machine learning private;learning aided privacy;private machine learning;learning private machine;privacy machine;approach privacy machine;outperforms privacy;privacy emerged;privacy attack;interactions privacy machine;privacy;privacy protection;privacy attack corresponding;aided privacy;learning private;novel approach privacy;analysis area privacy;aided privacy protection;outperforms privacy terms;privacy emerged big;approach privacy;interactions privacy;categories interactions privacy;area privacy machine", "pdf_keywords": "machine learning attack;learning aided privacy;private machine learning;applications privacy ml;privacy ml;attacks machine learning;privacy attacks machine;classify existing privacy;protection machine learning;existing privacy attacks;privacy challenges;revisiting privacy challenges;privacy attacks;privacy ml motivates;state art privacy;privacy protection machine;learning attack;privacy challenges particularly;existing privacy;privacy iot;revisiting privacy;aided privacy;privacy;aided privacy protection;privacy protection;learning attack comprehensively;backdoor attacks;comprehensive survey privacy;privacy iot investigating;survey privacy iot"}, "8d17543c20f23b6a40bec9334d50e9c15a08c1c4": {"ta_keywords": "network extracting answers;graph representation learning;large text corpus;answers knowledge bases;subgraphs containing text;net extracting answers;extracting answers knowledge;extracting answers;deep neural;knowledge bases;extracting answers question;questions training data;neural network extracting;results deep;advances graph representation;quark gluon;text kb entities;results deep neural;graph representation;quark gluon plasma;text corpus;representation learning;subgraphs containing;answers knowledge;specific subgraphs containing;gluon plasma qgp;representation learning propose;deep neural network;dynamics quark gluon;quark", "pdf_keywords": "question answering datasets;answering machine learning;question answering;existing question answering;architecture answer answering;trained extract answers;answer answering machine;answer answering;answering datasets discuss;text knowledge bases;answers question subgraph;answer questions unstructured;answering datasets;model reading comprehension;extract answers;extract answers question;deep neural;model answer questions;propose deep neural;unstructured text knowledge;answering machine;end deep neural;knowledge bases;reading comprehension paper;answers question;facts text sentences;answer questions;text knowledge;learning model reading;graph representation learning"}, "1808b64aec21863489f0fe66f250890a3ac2b843": {"ta_keywords": "privacy preserving statistical;exponentially distributed noise;privacy preserving;implementation privacy preserving;distributed noise;distributed implementation privacy;noise presence random;unbiased random bits;distributed noise presence;random noise method;distribution purpose noise;noise generation;distributing secret;random noise;preserving statistical databases;purpose noise generation;implementation privacy;presence random noise;noise generation create;random bits;method distributing secret;identically biased coins;preserving statistical;generating exponentially distributed;noise presence;random bits combining;noise method based;distributing secret keys;privacy;biased coins", "pdf_keywords": "privacy preserving;privacy preserving data;random noise secure;trusted server probabilistic;approach privacy preserving;cryptographic;cryptography;secure malicious participants;noise secure malicious;cryptographic protocols;server probabilistic constructions;digital data trusted;data trusted;unbiased coins;property cryptographic protocols;secret sharing;whittle cryptography;veri\ufb01able secret sharing;secure malicious;generating random bits;property cryptographic;server probabilistic;shares unbiased coins;sharing digital data;unbiased random bits;new approach privacy;data trusted server;coins arbitrary;random bits;privacy"}, "c3a662b864673d8cc7469051419ab8819926d4b0": {"ta_keywords": "multilingual representations;multilingual multilingual bert;multilingual representations enables;multilingual bert;quality multilingual representations;multilingual bert mbert;multilinguality collection air;measure multilinguality;method measure multilinguality;measure multilinguality collection;multilinguality strongly;training bbert models;air showers multilinguality;multilinguality somewhat;multilinguality;high quality multilingual;multilinguality somewhat obscure;multilingual;multilinguality strongly affected;showers multilinguality strongly;multilingual multilingual;multilinguality collection;showers multilinguality;bbert models;phenomenon reasons multilinguality;bbert models mix;quality multilingual;linguistic elements recent;linguistic elements;linguistic", "pdf_keywords": "language multilingual embedding;multilingual embedding algorithms;hypothesize multilingual embedding;multilingual embedding;multilingual embedding spaces;word embeddings tasks;languages multilingual models;multilingual models;multilingual models provide;word embeddings;words contribute multilingual;transfer multilingual models;multilingual model;embeddings tasks propose;multilinguality architectural;static word embeddings;languages multilingual;language multilingual;7000 languages multilingual;multilingual models shown;multilingual;embeddings tasks;feature language multilingual;multilinguality important crosslingual;multilinguality architectural aspects;multilinguality;multilingual model world;multilingual hypothesize multilingual;crosslingual transfer multilingual;multilingual models central"}, "b1d24e8e08435b7c52335485a0d635abf9bc604c": {"ta_keywords": "automatic speech;performance automatic speech;annotators recorded sentence;verified knowledge sentence;automatic speech recognition;speech recognition sqa;altering sentences extracted;speech recognition;sentences extracted wikipedia;labeling claim;sentences extracted;labeling claim accompanied;altering sentences;annotators recorded;method labeling claim;evaluation performance automatic;recorded sentence forming;annotators;knowledge sentence derived;generated altering sentences;speech;recognition sqa;wikipedia subsequently verified;recorded sentence;oracles;knowledge sentence;labeling;claims generated;verified knowledge;new method labeling", "pdf_keywords": "accuracy claim extraction;humanannotated sentences claims;claim extraction veri\ufb01cation;claim extraction;approach claim extraction;sentences claims;correctness evidence retrieved;evidence retrieved;sentences forming evidence;sentences claims requiring;reasoning evidence read;evidence documents selects;evidence documents;evidence retrieved computing;claims requiring evidence;forming evidence documents;information presented wikipedia;evidence read avoid;claim extraction veri;forming evidence;given claim automatically;humanannotated sentences;reasoning evidence;sense reasoning evidence;evidence read;sentences comparison humanannotated;sentences support refute;generated claims;comparison humanannotated sentences;evidence complete pipeline"}, "4a4bc9f6c5ec76b0d501b641d3092aceb2e083bd": {"ta_keywords": "preference handling restauranteurs;diner profiles;restaurant customizable diner;customizable diner;food preferences involving;preferences thriller;noir like attractor;meal noir preferences;customizable diner profiles;noir preferences thriller;meal noir;restaurant customizable;overall meal noir;food preferences;diner;capture preference knowledge;preference knowledge;2d particles interacting;restaurant;integration restaurant customizable;attractor;like attractor;2d particles;preferences thriller classical;handling restauranteurs;visualize dynamics;movie noir;restauranteurs;noir preferences;particles interacting", "pdf_keywords": ""}, "d5eeaac5c5e524ad05d9b1f3f3f41aece082955a": {"ta_keywords": "bayesian predictive classification;bayesian framework speech;noisy data bayesian;speech recognition robust;speech recognition clustering;distribution bayesian predictive;bayesian predictive distribution;predictive distribution bayesian;data bayesian predictive;data sparseness;data sparseness propose;robust data sparseness;learning noisy data;sparse data;bayesian predictive;speech recognition;classification method speech;predictive classification robust;problem speech recognition;learning noisy;framework speech recognition;sparse data problem;predictive classification;classification robust;robust classification;problem learning noisy;recognition address sparse;sparseness propose;paper propose bayesian;recognition robust data", "pdf_keywords": ""}, "04d96a75b4383240cb15fb729b29f5775219d724": {"ta_keywords": "deep learning library;deep learning;neural automatic speech;speech recognition;recognition asr toolkit;speech recognition asr;learning library pytorch;toolkit based deep;recognition asr;automatic speech recognition;language model fusion;vision language model;neural automatic;based deep learning;computer vision language;automatic speech;data augmentation leverages;end neural automatic;library pytorch;performance fluorescence detection;language models architecture;learning machine;language models;estimating performance fluorescence;distributed training;distributed training gpus;supports distributed training;learning machine learning;architecture computer vision;neural", "pdf_keywords": "translation speech recognition;end speech recognition;machine translation speech;speech recognition encoder;speech recognition;automatic speech;recurrent neural networks;weighted machine translation;automatic speech recognition;recognition speech recognition;neural machine translation;speech recognition speech;new machine translation;recognition speech;speech recognition systems;machine translation;machine translation designed;gent speech recognition;sets automatic speech;speech recognition important;attention based recurrent;eq automatic speech;language model integration;translation speech;language model;speech recognition paper;machine translation based;recognition encoder;encoder decoder deep;technique automatic speech"}, "312b12dd6aa558b92df3ddd9b1057aa80a0ad718": {"ta_keywords": "shot relation classification;relation classification;relation classification systems;relation classification exploiting;exploiting relation descriptions;relation descriptions;relation descriptions ii;textual entailment models;zero shot relation;textual entailment pose;textual entailment;textual entailments;textual entailments enhance;shot relation;zero shot classification;entailment models;classification exploiting relation;existing textual entailment;task textual entailment;available textual entailments;entailment models iii;performance relation classification;relation;entailments enhance performance;entailments;shot classification;entailment pose tasks;entailments enhance;textual;entailment paper", "pdf_keywords": ""}, "5885625fac055f4f8f47b0d6b5c026c8806896f0": {"ta_keywords": "distribution free learning;learning data;data based distribution;learning data based;approach learning data;free learning;based distribution free;distribution free;based distribution;learning;new approach learning;distribution;approach learning;data based;data;new approach;free;propose new approach;approach;based;new;paper;paper propose new;paper propose;propose new;propose", "pdf_keywords": ""}, "098076a2c90e42c81b843bf339446427c2ff02ed": {"ta_keywords": "influence function deep;deeper networks estimates;shallow networks;networks deeper;shallow networks deeper;accurate shallow networks;networks deeper networks;networks estimates;deeper networks;networks estimates erroneous;influence estimates fairly;influences influence estimates;influence estimates;deep learning nonconvex;model influence functions;influence estimates vary;models influence functions;loss functions regularization;learning non convex;deep learning non;influence functions understood;accuracy influence estimates;function deep learning;influence functions;deep learning;influence functions defined;regularization linear models;hessian model influence;networks;influence estimates iii", "pdf_keywords": "deeper networks estimates;networks deeper;accurate shallow networks;shallow networks deeper;networks deeper networks;deeper networks;shallow networks;networks estimates;networks estimates erroneous;networks machine learning;network depth;samples perturbed network;deep networks;training weight decay;effect network depth;weight decay depth;training samples perturbed;weight decay regularization;deep networks machine;depth width network;network depth width;machine learning in\ufb02uence;networks;network architectures datasets;variety deep networks;learning in\ufb02uence functions;deep;perturbed network;architectures datasets training;datasets training weight"}, "446f1eaec22a90574670491073cd5b03bfa1e273": {"ta_keywords": "learn supervised models;statistical property extractor;claims statistical properties;estimating size item;supervised models;simple claims statistical;supervised models paper;claims statistical;estimating size;estimating;percentage error inflationary;extracting numerical information;supervised;method estimating size;numerical information text;order learn supervised;annotating data;inflationary;learn supervised;method estimating;presents statistical;error inflationary;annotating data property;absolute percentage;statistical properties probability;property order learn;numerical information;similar extracting numerical;size item supermarket;data", "pdf_keywords": ""}, "76468db928e18f97dadbd25c04c80ebd491fec9b": {"ta_keywords": "xmath1 transition metal;xmath1 transition;xmath0 xmath1 transition;cations pyrochlore zirconium;transition metal cations;metal cations pyrochlore;crystal field splitting;pyrochlore zirconium;xmath1;splitting shape xmath0;shape xmath0 xmath1;xmath0 xmath1;cations pyrochlore;transition metal;shape xmath0;metal cations;influence crystal field;xmath0;crystal field;crystal;influence crystal;investigate influence crystal;zirconium;cations;pyrochlore;field splitting shape;metal;field splitting;splitting shape;transition", "pdf_keywords": ""}, "aaf7e94e1a2f8891e5c5f4d11d4f135a1687bb0a": {"ta_keywords": "mechanical bars locking;locking arms micromechanical;spring aperture locking;spring locking arms;aperture locking position;bars locking arms;spring locking end;locking positions fluid;center spring locking;locking arms pivotable;spring aperture;position twist mounting;spring locking;mechanical bars;aperture locking;bars locking;fabrication mechanical bars;ends spring aperture;locking position twist;micromechanical systems tab;panel aperture;arms micromechanical systems;single panel aperture;twist mounting structure;twist mounting;locking position;micromechanical systems;locking arm;fluid dynamics video;locking positions", "pdf_keywords": ""}, "c589c4ec7247980f38a6bd22f215fea8028a0f66": {"ta_keywords": "human annotation quality;human annotations clearly;human annotations;human annotations obtained;quality human annotation;human annotation;datasets human annotations;annotations rationales datasets;annotation quality;annotation quality highly;websites human annotations;annotation results;annotations clearly speci\ufb01ed;annotations clearly;annotations rationales;annotation process careful;annotations;annotation results obtained;annotations obtained crowd;datasets interpretable nlp;using annotations rationales;annotation process;methods interpretable nlp;annotation;interpretable nlp;details annotation process;provide certain annotations;using annotations;details annotation;annotations obtained", "pdf_keywords": "nlp explanations based;interpretable nlp explanations;based human annotation;human annotations;nlp explanations;explanations based human;human annotation;human annotations obtained;annotation quality;annotation quality highly;know human annotations;containing explanations humans;prediction provide turkers;text using crowdsourced;explanations humans;interpretable nlp;human annotation aim;datasets containing explanations;explanations based input;useful explanations humans;annotations;explanations humans used;explanations humans researchers;reveal annotation quality;annotate important parts;annotate;quantify useful explanations;annotation;annotation process;results reveal annotation"}, "ecc520794da34d2b141235002c70b06c999bda73": {"ta_keywords": "sense embeddings text;sense embeddings propose;existing sense embeddings;sense embeddings;sense specific embeddings;evaluation sense embeddings;interpretable sense representations;existing sense representations;word representation sense;sense representations;language sense inventory;embeddings text representations;discovering interpretable sense;sense representations coherent;sense representations fail;word embedding;coherence evaluation sense;prototype word embedding;embeddings reflects word;type existing sense;word sense;text representations;representation sense specific;word representation;embeddings propose;inspecting language sense;embeddings propose new;text representations critical;language sense;new coherence evaluation", "pdf_keywords": ""}, "c12e46d7d0bb9fa062bce0549f2a6a9de00758d5": {"ta_keywords": "sound moving body;speed sound moving;measure speed sound;environment velocity sound;speed sound;velocity sound;convolutional recurrent neural;sound moving;velocity sound measured;convolutional recurrent;sound measured function;velocity moving body;crowded environment velocity;recurrent neural network;based convolutional recurrent;recurrent neural;moving body crowded;moving body;measure speed;neural network self;encoder;velocity moving;sound measured;function velocity moving;method measure speed;network self attention;input feature sequence;outperforms dcase2019 task4;measured function velocity;velocity", "pdf_keywords": ""}, "f2fb4a931580c4f1c5bdb47ebc80c801b422cd1a": {"ta_keywords": "semantics based agent;language actions separation;language actions;grounds language actions;commands ability robot;language grounding;actions referred language;networks learn commands;actions separation logic;behavior agent;language grounding work;behavior agent able;natural language instructions;robot computer understand;grounds language;neural networks learn;learn commands;robot;semantics based;natural language;ability robot computer;agent perceptions actions;robot computer;understand natural language;simple commands ability;learn commands simple;little grounds language;referred language grounding;actions separation;goal intelligence", "pdf_keywords": ""}, "57b972ebe314cfe8e57fd6b9f9239123eb70e979": {"ta_keywords": "alc cellular automaton;cnn based models;training time decoding;cellular automaton;cellular automata based;cellular automata;performance cnn based;performance cnn;cellular automaton cma;class cellular automata;neural networks cnns;clock alc cellular;automatic speech;speech recognition;deep convolutional neural;cnn based;cnns evaluation ability;compare performance cnn;models typical rnn;rnn based models;speech recognition paper;convolutional neural;networks cnns;automatic speech recognition;switchboard eva12000 corpus;cnns evaluation;convolutional neural networks;networks cnns evaluation;neural;automata", "pdf_keywords": "speech recognition deep;convolutional encoders ctc;convolutional architectures trained;networks frames utterance;convolutional architectures;speech recognition;parallel recurrent networks;performance deep convolutional;deep convolutional neural;performance convolutional encoders;network rnn encoders;rnn encoders;recognition deep convolutional;encoders ctc models;rnn encoders connectionist;recurrent networks;recurrent networks frames;architectures trained ctc;convolutional neural networks;conversational telephone speech;automatic speech;training time decoding;learn sequence frame;performance deep;encoders ctc;recognition deep;deep convolutional;sequences learning rate;speech recognition research;convolutional encoders"}, "6f79cbe893ae46a6f97617d14656ab57c26e6faf": {"ta_keywords": "field multi speaker;multi speaker;microphone array;alice based bouncer;sources respect microphone;respect microphone array;multi speaker end;microphone array defined;driven localization models;microphone;explicit direction arrival;driven localization;source mixtures prediction;based bouncer;speaker;data driven localization;based bouncer ability;respect microphone;direction arrival;bouncer steer bouncer;localization models;steer bouncer;direction arrival doa;speaker end;neural network trained;bouncer steer;ability bouncer steer;steer bouncer desired;localization models makes;bouncer ability bouncer", "pdf_keywords": "location microphone utterance;directional automatic speech;neural network speech;speech recognition source;estimate location microphone;location microphone;speaker speech recognition;speech recognition asr;speech recognition;microphone utterance;robot audition systems;network speech enhancement;network speech;\ufb01eld speech recognition;far eld multispeaker;recognition source localization;robot audition;multispeaker data end;multispeaker data;automatic speech;multi speaker speech;indispensable robot audition;mimo speech;eld multispeaker data;audition systems;microphone;automatic speech recognition;factor mimo speech;speech enhancement;multispeaker"}, "5cb74e269c57263d475734a66d34e4d2d2f9e1ac": {"ta_keywords": "vibration characteristics core;truss core panels;laser additive manufacturing;additive manufacturing lam;behavior truss core;study vibration characteristics;truss core;core geometries tetrahedral;vibration characteristics;core geometries;core panels base;core panels;safety manufacturing;safety manufacturing finite;additive manufacturing;panels base laser;study vibration;dynamic behavior truss;constructed using laminar;context safety manufacturing;vibration;tetrahedral pyramid kagome;manufacturing lam;geometries tetrahedral pyramid;manufacturing finite element;manufacturing lam reported;safety reliability;characteristics core geometries;reliability context safety;base laser additive", "pdf_keywords": ""}, "cefd3993db4d065b95ab8f105452fb728c02b60e": {"ta_keywords": "review nlpedia;review nlpedia ai;nlp models generate;nlp models;natural language processing;processing nlp models;papers machine learning;processing nlp;reviews scientific papers;neulab review nlpedia;nlpedia ai discuss;peer reviews scientific;nlpedia ai;nlpedia;peer reviews;dataset papers machine;language processing nlp;pass peer reviews;natural language;collect dataset papers;review paper;author review paper;review paper laborious;papers present review;nlp;learning domain annotate;author review;high quality reviews;scientific papers;experts paper", "pdf_keywords": "good automatic reviewing;generating review scholarly;automatic reviewing;paper summarization task;automatic reviewing quantify;paper summarization;summarization author reader;automatically generating review;scienti\ufb01c paper summarization;review scholarly;summarization task;machine translation;summarization;machine translation using;generating review;functioning review assistant;review scholarly publication;summarization author;paper summarization author;translation using attention;approach machine translation;reviewing;review assistant;author reader reviewer;reader reviewer;human reviewers;human reviewers make;reviewer;summary paper;reviewing quantify"}, "f491a5f09ee01436d772a6cff25f22d700d8c9c0": {"ta_keywords": "power flow analysis;power distribution networks;power flow;method power flow;fast power flow;power distribution;distributed generator;reactive power support;estimation reactive power;support power distribution;reactive power;distributed generator dg;challenges distributed generator;distribution networks;power support power;energy consumers results;demand management voltage;power support;distribution networks dns;generator;energy consumers;generator dg;management voltage support;power;management voltage;augmented nodal analysis;fast power;needs energy consumers;nearest neighbour;nodal analysis mana", "pdf_keywords": ""}, "72213e24713264da816f43a42d606f115998fe7b": {"ta_keywords": "stacked graphical learning;stacking graphical learning;online stacked graphical;graphical learning nearly;graphical learning base;collective classification sequential;algorithm stacked graphical;stacked graphical;graphical learning;including collective classification;collective classification widely;collective classification;models collective classification;graphical models collective;idealized stacking graphical;graphical learning analyze;overhead collective classification;algorithm stacked;collective classification usually;online stacked;instances applied stacked;graphical learning real;gibbs sampling;applied stacked graphical;classification sequential partitioning;memory overhead collective;stacking graphical;graphical learning shown;partitioning information extraction;learning base", "pdf_keywords": ""}, "c04c865c8b33ce0251c9f37d0cccf2b3b1e4fd34": {"ta_keywords": "learn policies reinforcement;policies reinforcement learning;state representations rnns;efficiently learn policies;policies reinforcement;dynamics precessing swimmer;reinforcement learning;causal state representations;observable navigation tasks;agnostic state abstractions;state abstractions;reinforcement learning problems;environment swimmer moves;learned task agnostic;partially observable navigation;state representations;learning task agnostic;recurrent dqne baselines;swimmer moves direction;state abstractions used;swimmer moves;environment swimmer;environments learning;precessing swimmer;representations rnns trained;learned task;observable navigation;learn policies;rnns trained;representations rnns", "pdf_keywords": "causal state representations;state representation learning;learning latent representations;causal state representation;learning continuous representation;approach learning latent;learning latent;value iteration representations;state representations;prediction policies causal;iteration representations;invariant causal prediction;latent representations;continuous causal states;neural networks causal;policies causal states;states causal inference;networks causal state;state representation;reward observation prediction;latent representations partial;iteration representations using;recurrent neural networks;causal states provide;learning continuous;setting causal states;discretize continuous causal;state representations fare;approach causal state;learning based recurrent"}, "8504a5eb4638aeb2f61f8b7f93440b9e495b443b": {"ta_keywords": "active sensors fidelity;energy efficient tracking;sensor activation centralized;stochastic approximation learning;sampling stochastic approximation;active sensor selection;dynamic sensor activation;efficient tracking;activation centralized tracking;sensors fidelity;centralized tracking numerical;single photon detector;sampling stochastic;stochastic approximation;centralized tracking;sensor selection;gibbs sampling stochastic;photon detector;sensors fidelity increases;efficient tracking mechanisms;photon detector presence;active sensor;global optimal;dynamic sensor;active sensors;performance single photon;convergence global optimal;approximation learning;sensors problem minimizing;sensor activation", "pdf_keywords": ""}, "8e7d063c681557c94382ff3da6415d3720fe11a7": {"ta_keywords": "stance detection bidirectional;twitter stance detection;data stance detection;stance detection task;results stance detection;labelled tweets training;task twitter stance;tweets training data;stance detection;twitter stance;tweets training;tweet dependent target;labelled tweets;tweet target;2016 task twitter;text target hillary;task twitter;automatically labelled tweets;detection bidirectional;tweets;encoding tweet target;encoding tweet dependent;tweet target independently;tweet dependent;training data stance;demonstrate encoding tweet;detection task outperforms;task classifying attitude;detection task classifying;expressed text target", "pdf_keywords": "stance detection twitter;classify stance tweet;predicting target stance;2016 stance detection;stance detection based;stance detection;target stance tweets;stance detection methods;stance tweets;present stance detection;approach stance detection;stance tweet;stance tweets goal;classify stance;need classify stance;stance tweet respect;representation tweet dependent;detection twitter dataset;detection twitter;tweet dependent target;representation tweet;target stance;stance;tweet target;2016 stance;tweet dependent;encoding tweet target;twitter dataset;successfully weakly supervised;builds representation tweet"}, "8568f6eda2e4cb7921fe175ab44b2f5ecbb2b870": {"ta_keywords": "transpositions difficult misspellings;misspelling error rates;difficult misspellings contain;naturally occurring misspelling;text typos misspelling;reading difficulty;novel approach readout;difficult misspellings;human readers;human readers cope;readout entire genome;combinations human readers;misspelling word substitutions;written human readers;occurring misspelling;misspelling word;typos misspelling word;misspelling;misspellings;reading difficulty presence;genome human brain;text written human;misspellings contain unexpected;text typos;misspellings contain;letter transpositions;readers cope easily;typos misspelling;correcting errors text;approach readout", "pdf_keywords": ""}, "ae77189921ffade5ee4c4d4a0e93e879d7280b80": {"ta_keywords": "predict avatar pose;method predict avatar;multimodal prediction body;pose person dialogue;prediction body pose;predict avatar;gestures virtual avatars;avatar pose;multimodal prediction;model multimodal prediction;pose person;avatar pose pose;pose pose person;virtual avatars;body pose gestures;body pose;pose gestures;prediction body;virtual avatars use;pose gestures virtual;pose;end model multimodal;model multimodal;person dialogue;multimodal;avatars use correlations;avatar;pose pose;person dialogue paper;avatars", "pdf_keywords": ""}, "93e012cbf8e29aacb9654313250a81d53bbcbdf2": {"ta_keywords": "detect email leaks;email leaks;associated email leaks;email leaks critical;email leak;single email leak;recipients outliers;email leak potentially;unintended recipients outliers;email leaks 82;patterns associated email;detect email;prevent information leaks;easily implemented email;information leaks message;outlier detection;information leaks;outliers outlier detection;able detect email;email client changes;implemented email client;implemented email;email client;outliers;client changes email;outlier;leaks message;email server;textual network patterns;outliers outlier", "pdf_keywords": ""}, "df99459a75328393a9a989498db46ec445335724": {"ta_keywords": "peer review data;peer reviewed data;privacy preserving release;review data privacy;privacy preserving manner;released privacy mechanism;release peer review;data released privacy;privacy preserving;privacy preserving mechanism;data privacy preserving;propose privacy preserving;data retaining privacy;privacy preserving protocol;peer review;retaining privacy guarantees;data privacy;privacy mechanism manner;privacy mechanism;privacy guarantees;privacy guarantees propose;releasing perturbed data;privacy mechanism postprocesses;released privacy;retaining privacy;peer reviewed;privacy;present privacy mechanism;works privacy preserving;approach peer review", "pdf_keywords": "peer review privacy;privacy peer review;review privacy peer;datasets privacy preserving;privacy preserving data;review privacy utility;datasets privacy;privacy preserving;peer review data;review privacy;privacy preserving manner;privacy peer;histograms datasets privacy;analytics privacy preserving;privacy utility;analytics privacy;data analytics privacy;privacy utility tradeoff;review data sensitive;sets peer review;output privacy mechanism;privacy mechanism convex;output privacy;privacy;privacy research work;privacy research;privacy mechanism;review privacy lines;objective literature privacy;data available public"}, "c4bc2f7e04e02107aa6eaa0c811c3c046efbbc14": {"ta_keywords": "inductive logic programming;hardness natural learning;natural learning problems;ground clauses learning;clauses learning structured;learning description logic;learning structured examples;clauses learning;programming problem learning;problem learning straightline;learning straightline programs;learning structured;natural learning;learning problems;including inductive logic;logic programming;inductive logic;learning straightline;learning problems including;subconcepts learning arity;learning arity;finite automata solving;problems including learning;problem learning;structured examples necessary;finite automata;problem concepts strings;structured examples;learning description;learning problem", "pdf_keywords": ""}, "69b184f62c97513b03deed96a1443f79b34af0d7": {"ta_keywords": "dishonest reviewers adversarially;robustness dishonest reviewers;reviewers adversarially;reviewers adversarially influence;bidding assign reviewers;dishonest reviewers collude;dishonest reviewers;assign reviewers;reviewers collude;adversarially influence paper;assign reviewers papers;reviewers collude knowledge;reviewers papers;reviewers;paper reviewing assignments;paper review assignments;door dishonest reviewers;review assignments;influence paper reviewing;peer pressure scientific;reviewing assignments;paper reviewing;adversarially influence;bid manipulation attacks;review assignments comparable;provides robustness dishonest;reviewing assignments present;paper review;rely papers bidding;manipulation attacks jeopardize", "pdf_keywords": "malicious reviewers adversarially;allow adversarial reviewers;adversarial reviewers;reviewers adversarially;adversarial reviewers review;reviewers adversarially alter;adversarially alter bids;malicious reviewers;removes malicious reviewers;automatic assignment reviewers;malicious reviewers reviewer;adversarially alter;bid manipulation attacks;adversarially;adversarial;allow adversarial;assignment reviewers;peer review automatic;robust bid manipulation;paper reviewing robust;robustness paper assignment;attacks allow adversarial;bidding phase reviewers;review automatic assignment;reviewer pool paper;assignment reviewers recent;manipulation attacks;reviewers review papers;reviewing robust bid;paper assignment systems"}, "b21fa4f31c4813444e50259dfbe2c56660161174": {"ta_keywords": "motion swimmer noisy;swimmer noisy environment;swimmer noisy;motion swimmer;study motion swimmer;swimmer;noisy environment;motion;noisy;results study motion;study motion;environment;study;article;article report;results study;article report results;report results study;report;results;report results", "pdf_keywords": ""}, "89fd287f7eacc7a40d0216ba3b919812da658b94": {"ta_keywords": "entanglement;entanglement quantum systems;entanglement quantum;study entanglement;study entanglement quantum;approach study entanglement;quantum systems;quantum;quantum systems propose;regularization strategy largescale;competing models;lfalfads able predict;outperform competing models;lfav models;competing models terms;framework combines regularization;combines regularization strategy;regularization strategy;predict outcome competition;sight lf model;regularization;optimizing model hyperparameters;combines regularization;lfav models shown;models terms performance;strategy largescale;class lfav models;line sight lf;combination competing mechanisms;strategy largescale framework", "pdf_keywords": "learning latent dynamics;neuronal time series;latent dynamics data;sparsely sampled time;neural dynamics;resolution neuronal time;relevant neural dynamics;selective backpropagation time;data electrophysiology time;temporal super resolution;neurons embedded latent;backpropagation time;neuronal time;latent dynamics irregularly;latent dynamics;data electrophysiology;neural dynamics 80;neural population activity;learning latent;activity time series;super resolution neuronal;sampled time series;sparse datasets;sparse datasets present;dynamics irregularly sparsely;early layers data;dynamics data;latent low dimensional;temporal super;imaging data electrophysiology"}, "c5950fa3ee124cf2dcb8783db6f582f49170fb45": {"ta_keywords": "generalization gap training;label noise generalization;generalization machine learning;generalization guarantees;empirical risk minimization;typically bound generalization;bound generalization gap;trained gradient descent;deep learning early;minimization linear classifiers;generalization labeled;linear generalization guarantees;bound generalization;noise generalization;generalization bounds;produce generalization bounds;linear classifiers trained;certify generalization labeled;data produce generalization;generalization gap;generalization machine;classifiers trained gradient;generalization bounds work;produce generalization;noise generalization approach;training plug empirical;generalization;gradient descent assess;linear classifiers;bound valid empirical", "pdf_keywords": "training bound;training bound true;randomly labeled training;guarantees performance deep;trained gradient descent;networks trained gradient;networks trained;performance deep;performance deep neural;practical deep learning;labeled training;deep learning;data practical deep;post training bound;trained gradient;labeled training data;mislabeled training;deep neural;deep neural networks;error mislabeled training;deep learning settings;training data practical;neural networks trained;generalization bounds directly;generalization bounds;mislabeled training data;crossentropy loss sgd;optimizing crossentropy loss;training data;gradient descent"}, "a7822238f5db7d62731eaeabf9725a65f4edf893": {"ta_keywords": "movement animals noisy;transformer andlifelike transducers;track movement animals;transducers used;transducers used track;transducers;animals noisy environment;movement animals;andlifelike transducers used;animals noisy;andlifelike transducers;track movement;animals;used track movement;noisy environment;transformer;transformer andlifelike;noisy;movement;track;used track;environment;used;andlifelike", "pdf_keywords": ""}, "9cdf512f273083efa1ea01f7b31daa97a7bbe884": {"ta_keywords": "coded computation schemes;coded computation emerged;locality codes;coded computation multivariate;coded computation;approach coded computation;coded computation presented;distributed computation;modern distributed computation;approach distributed computation;algorithm coded computation;distributed computation linear;results coded computation;data locality;computational locality;computational locality building;data locality based;denoted computational locality;locality denoted computational;distributed computation infrastructures;present coded computation;model coded computation;code modern distributed;coded computation lens;computation lens locality;adaptively exploit locality;computation schemes;exploit locality;input data locality;locality based", "pdf_keywords": "coded computation schemes;coded computation scheme;coded computation;workers coded computation;design coded computation;model coded computation;coded computation leverage;coded computation lower;studied locality codes;muller codes generalized;coded computation lens;locality codes design;locality codes;codes generalized;codes equivalence computational;reed muller codes;yield coded computation;locality codes equivalence;robust distributed computation;computation schemes;codes design coded;computation schemes exploit;coded;computation scheme;reed muller code;muller codes;distributed computation;computation scheme multivariate;computational locality;computation lens locality"}, "6aeb477e5f0882f8363a3a8e5e6f83962d91edc6": {"ta_keywords": "recognition person attitude;recognition person;feature recognition present;model recognition person;feature recognition;model recognition;recognition;recognition present;recognition present simple;simple model recognition;person attitude presence;person attitude;machine learning;techniques feature recognition;learning techniques feature;machine learning techniques;person;accelerated future learning;use machine learning;feature;attitude presence external;attitude presence;learning use machine;future learning;techniques feature;present computational model;learning techniques;accelerated future;demonstrates accelerated future;present computational", "pdf_keywords": ""}, "39ab4b9cdeb4a71b3e25fe5339962654c7c9ff8c": {"ta_keywords": "trust credibility spreader;credibility spreader neighborhood;credibility spreader;false information spreaders;trust credibility;information spreaders;information spreaders accuracy;spread path model;social networks competing;influence people spread;networks competing influence;social networks;path spread;spread paths analyze;credibility;relationship trust credibility;spread path;predict false information;coexist social networks;twitter datasets model;real world twitter;people spread;people spread paths;spread paths;spreader neighborhood using;spreaders accuracy;spreader neighborhood;action path spread;trust;predict false", "pdf_keywords": "false information spreaders;spreader false information;graph neural network;networks propose attention;based graph neural;spreaders online social;graph neural;attention based graph;detecting false information;information spreaders;information spreaders online;social networks;online social networks;social networks propose;spreader false;false information;likely spreader false;false information important;person likely spreader;networks propose;propose deep neural;networks;detecting false;network model predict;attention based model;deep neural;neural network model;false information refutation;predict person;neural network"}, "00799dceb9e7209bb9d71b38fa5b49483e886978": {"ta_keywords": "training dynamic nns;static graph optimization;architectures nnn;static graphs optimization;architectures nnn structure;static graph construction;dynamic neural network;deep learning dl;deep learning;inference dynamic nns;nn architectures nnn;moving dynamic neural;dynamic nns;recent deep learning;frameworks dynamic nns;operations static graph;dynamic nns demonstrate;dynamic neural;speedup training dynamic;graph optimization;graph optimization allows;training inference dynamic;dynamic nns fluid;static graphs;static graph;use static graphs;training dynamic;neural network nn;graphs optimization;nn architectures", "pdf_keywords": ""}, "ab57c70c14b82d07c40c75fecaac98b0e2dc0510": {"ta_keywords": "supervised record linkage;probabilistic record linkage;supervised record;labeled data;unsupervised semi supervised;unsupervised probabilistic record;hierarchical model unlabeled;semi supervised;semi supervised methods;fully supervised record;supervised;labeled data available;preferable supervised;preferable supervised methods;supervised methods preferable;record linkage methods;supervised methods;model unlabeled data;proposed unsupervised methods;data available hierarchical;little labeled data;latent variables hierarchical;methods preferable supervised;existing unsupervised probabilistic;unsupervised probabilistic;unlabeled data;labeled;unsupervised methods;label latent variables;fully supervised", "pdf_keywords": "supervised record linkage;unsupervised probabilistic record;propose semi supervised;semi supervised;probabilistic record linkage;semi supervised approach;unsupervised probabilistic;existing unsupervised probabilistic;supervised inference;supervised inference dimensional;inference dimensional data;supervised record;proposed unsupervised methods;fully supervised record;supervised;high dimensional data;labeled data;unsupervised methods;unsupervised methods competitive;available unlabeled data;approach supervised inference;generative models propose;large record linkage;existing unsupervised;results proposed unsupervised;linkage large record;learning match cluster;probabilistic record;generative models;unlabeled data"}, "6413e6a4f68be0ea6aed0082b205147d9f893699": {"ta_keywords": "morphological inflection generation;inflection generation morphological;generation morphological inflection;models inflection generation;morphological inflection;inflection generation;inflection generation task;inflection generation model;generation morphological;generating inflected;morphologically rich languages;models inflection;problem inflection generation;art models inflection;generating inflected form;inflection;datasets morphologically rich;linguistic transformation;particular linguistic transformation;morphological;morphologically;datasets morphologically;neural encoder;inflected;task generating inflected;model problem inflection;inflected form;morphologically rich;seven datasets morphologically;neural encoder decoder", "pdf_keywords": "learns language model;sequences recurrent neural;character sequences vocabulary;sequences string aiding;recurrent neural networks;model character sequences;output character sequences;learning generating sequences;character sequences;model learns language;sequences vocabulary extracted;generating sequences recurrent;neural network sequence;character sequences string;improve language modeling;decoder model attention;language pattern recognition;phonological segmentation;recurrent neural;language model character;pattern recognition phonological;propose encoder;learns language;language modeling;phonological segmentation important;recognition phonological segmentation;form character sequences;encoder;recognition phonological;relations character sequences"}, "8dd85c3a3700d0d282ddbd4dff5e238f24c00676": {"ta_keywords": "gaussian mixture modelling;mixture model histogram;mixture modelling histogram;framework gaussian mixture;based mixture gaussians;recognition based mixture;gaussian mixture;bayesian model speech;mixture gaussians;mixture gaussians present;model speech recognition;mixture modelling;ml variational bayesian;model histogram data;mixture model;model histogram;novel variational bayesian;modelling histogram enables;model speech;variational bayesian;modelling histogram;speech recognition;variational bayesian vb;likelihood ml variational;ml variational;speech recognition based;histogram data;spectral envelope;based mixture;spectral modelling", "pdf_keywords": ""}, "74dccd379776bbb50b352c19b8caf2a7896d58ee": {"ta_keywords": "variables operating constraints;coherent constrained variables;sensitivity based coherency;power optimisation problems;constrained variables;constrained variables included;identifying coherent constrained;operating constraints illustrative;power optimisation;described power optimisation;operating constraints;control variables operating;control variables;optimisation problems deal;large power optimisation;coherent constrained;coherency analysis method;resetting control resetting;constraints illustrative;resetting control;based coherency analysis;control resetting;coherency analysis;constraints;power systems;constrained;variables operating;constraints illustrative example;optimisation problems described;optimisation problems", "pdf_keywords": ""}, "6a3cc30d5d6342d912851deb4362b8c47fa5ede3": {"ta_keywords": "existing debiasing methods;debiasing methods;debiasing framework;self debiasing framework;models utilizing biases;debiasing methods shown;debiasing framework prevents;proposed debiasing methods;existing debiasing;utilizing biases;debiasing;utilizing biases knowing;bias known;complementary existing debiasing;self debiasing;recently proposed debiasing;biases specifically;bias;biases specifically targeting;biases recently;proposed debiasing;biases;targeting certain biases;biases knowing;biases recently proposed;improvement challenge datasets;bias known priori;improved overall robustness;biases knowing advance;nearest neighbor", "pdf_keywords": "characterize spurious biases;natural language inference;language inference data;debiasing framework recognizing;recognizing false entailment;potentially biased examples;existing debiasing methods;biased examples natural;knowledge biases;identify potentially biased;language inference;entailment propose annealing;biased examples pinpointed;existing debiased models;biased examples;existing debiasing;debiasing methods applicable;debiasing methods;spurious biases;debiased models;various nlu tasks;debiased models work;speci\ufb01c knowledge biases;debiasing promising approach;prior information biases;model agnostic debiasing;biases;importance biased examples;information biases;agnostic debiasing promising"}, "51f6654b9d5925002ccaa5cd339b4377b96719ce": {"ta_keywords": "tracking user activities;clustering emails user;clustering emails;user activities based;designed clustering emails;emails user activity;user knowledge activities;inferring human activities;user activities;activities data propose;user activity;ongoing activities workstation;human activities data;activity use information;clustering;unsupervised clustering;unsupervised clustering methods;activities based user;activities data;ongoing activities;clustering methods designed;methods designed clustering;user activity use;clustering methods;information discovered cluster;designed clustering;activities based;activity use;activities workstation;key ongoing activities", "pdf_keywords": ""}, "f4906089f0720c83e57e4a46ae75283df4d67e5a": {"ta_keywords": "funding peer evaluation;peer evaluation increasingly;peer evaluation;selection peer review;choices desire crowdsourced;peer selection;crowdsourced approach participants;making selection peer;selection peer;peer review evaluation;group peer selection;opinions group peer;funding peer;peer review;desire crowdsourced;crowdsourced;peer selection problems;desire crowdsourced approach;crowdsourced approach;proposals funding peer;group peer;agents incentive misrepresent;agents incentive;idea agents ranked;participants making selection;insincere agents incentive;agents according opinions;incentive misrepresent truthful;peer;evaluation increasingly popular", "pdf_keywords": ""}, "05f8dd59d4184d38e240bdea4d58e424b8cd055c": {"ta_keywords": "reputation persuasion constructing;reputation persuasion;effect reputation persuasion;persuasion impact reputation;persuasion deliberation online;persuasion deliberation;persuasion constructing;argumentation platform;debate competition controlling;debate competition;change persuasion deliberation;persuasion impact;opinion change persuasion;persuasion;past debate competition;persuasive power reputation;reputation moderated characteristics;causal effect reputation;reputation heuristic information;argumentation;change persuasion;successful persuasion impact;million arguments argumentation;arguments argumentation platform;constructing instrument reputation;effect reputation;persuasion constructing instrument;argumentation platform containing;individual reputation;ethos individual reputation", "pdf_keywords": "reputation persuasion constructing;online argumentation platforms;online argumentation;reputation persuasion;mining online argumentation;argumentation platforms extensively;argumentation platforms;effect reputation persuasion;argument mining online;persuasion computational;unstructured debate competition;debate competition;persuasion computational natural;debate competition controlling;argument mining;endogenously selecting opinions;persuasion constructing;reputation outcome debate;past debate competition;debate competition particular;literature persuasion computational;controlling unstructured debate;persuasion ii users;literature argument mining;persuasion;argumentation;persuasion ii;persuasion constructing instrument;causal effect reputation;selecting opinions challenge"}, "59f41c5024a238ae8843f3dd67692961ecc63e75": {"ta_keywords": "optimization strategy dnn;dnn structure;neural networks dnns;networks dnns widely;networks dnns;dnn structure automatically;appropriate dnn structure;dnn structure parameters;dnn structure represented;strategy dnn structure;discovering appropriate dnn;quantum algorithms;performance quantum algorithms;phoneme recognition;recognition spoken digit;phoneme recognition spoken;deep neural networks;experiments phoneme recognition;quantum algorithms experiments;graph deep neural;speech recognition machine;dnns widely;spoken digit detection;speech recognition;dnns widely used;optimize performance quantum;deep neural;strategy dnn;algorithms experiments phoneme;recognition spoken", "pdf_keywords": ""}, "f9e32b30fd9ad50cce12ffb753c7be88100b6dc2": {"ta_keywords": "data aggregated crowdsourced;aggregated crowdsourced;aggregated crowdsourced events;crowdsourced data aggregated;crowdsourced data;process crowdsourced data;crowdsourced events;process crowdsourced;crowdsourcing;crowd labeled data;crowdsourced;crowdsourcing method;crowdsourced events derive;performance study crowdsourcing;crowdsourcing process;crowdsourcing process crowdsourced;crowdsourcing applicable number;crowdsourcing crowdsourcing;crowds;model crowd labeled;number crowds;crowdsourcing applicable;crowds propose permutation;study crowdsourcing;crowdsourcing platforms massive;method based crowdsourcing;crowdsourcing method based;diagnosis crowd dynamics;crowdsourcing platforms;crowdsourcing crowdsourcing process", "pdf_keywords": "model crowdsourcing;premise crowdsourced data;crowdsourced data corrupted;model crowdsourcing based;crowdsourced data;premise crowdsourced;introduce model crowdsourcing;crowdsourcing;crowdsourcing based premise;corrupted adversarial;based premise crowdsourced;crowdsourced;guarantees estimator dawid;estimator dawid skene;crowdsourcing based;data corrupted adversarial;adversarial;guarantees estimator;computationally e\ufb03cient estimators;corrupted adversarial relationships;adversarial relationships workers;recovery guarantees estimator;estimator dawid;adversarial relationships;estimators;whittle estimator;e\ufb03cient estimators;estimators setting ordering;incurred majority voting;bounds global minimax"}, "15ac2d8629ca9241ea558eb2b816272d82447ac7": {"ta_keywords": "crowdsourcing learning form;crowdsourcing learning;challenges crowdsourcing learning;variance tradeoff crowdsourcing;tradeoff crowdsourcing data;tradeoff crowdsourcing;crowdsourcing data collected;crowdsourcing data;crowdsourcing;challenges crowdsourcing;incentive mechanisms elicit;incentive mechanisms;critical challenges crowdsourcing;designing incentive mechanisms;designing incentive;incentive;ii designing incentive;statistical bias;win statistical bias;learning estimation;estimators automatically adapt;elegant mechanism learning;estimators automatically;mechanism learning people;prove estimators automatically;collected non experts;learning people;learning estimation operate;data people;bias", "pdf_keywords": ""}, "18ddcd250bbbe716a0616412ea329a8343f60542": {"ta_keywords": "crowdsourcing;quantum information processing;idea quantum;based idea quantum;crowdsourcing crowdsourcing;quantum information;xcite crowdsourcing;based crowdsourcing;quantum;crowdsourcing approach;xcite crowdsourcing approach;approach quantum information;quantum information paper;extensively xcite crowdsourcing;idea quantum states;xmath0 incentive;xmath0 incentive compatible;crowdsourcing jet;crowdsourcing approach based;encode quantum information;prove xmath0 incentive;based crowdsourcing crowdsourcing;crowdsourcing crowdsourcing jet;approach quantum;approach based crowdsourcing;crowdsourcing jet launched;new approach quantum;incentive compatible;incentive;encode quantum", "pdf_keywords": ""}, "c18700ed4ef07dd85ba8bceab3b9584c6e6af49c": {"ta_keywords": "billion crawled urls;general web crawl;web crawl;crawl date billion;crawled urls;billion crawled;crawled;date billion crawled;ce4corpus cc;ce4corpus;ce4corpus cc nc;web crawl date;crawl date;crawl;general web;largest publicly available;urls;largest publicly;available general web;web;largest;cc nc;cc nc nd;america largest publicly;division america largest;cc;aps division;publicly available general;aps division america;date billion", "pdf_keywords": ""}, "af4e11436268cf68505f1caee5d6f7ff0df9c99a": {"ta_keywords": "causal inference language;causal inference improve;estimating causal effects;learn causal relationships;overview causal inference;causal inference;challenge estimating causal;causal inference computational;estimating causal;uses causal inference;confounding causal relationships;inference computational linguistics;causal relationships provide;learn causal;causal relationships;overview causal;sciences causality;causal relationships traditionally;social sciences causality;research learn causal;unified overview causal;natural language processing;confounding causal;inference language processing;causal effects encompassing;causal;causality;sciences causality importance;convergence causal inference;causal effects", "pdf_keywords": "nlp models causal;text data causal;causal inference counterfactual;causal inference text;augmentation causal inference;causal inference promising;causal inference improve;inference text data;data augmentation causal;causal effects linguistic;counterfactual data augmentation;data causal inference;inference counterfactual;counterfactual data;inference counterfactual examples;sciences causal inference;natural language data;models causal inference;data causal;uses causal inference;inference causal;causal inference involves;nlp models;causal inference;inference causal effects;causal inference causal;causal inference computer;inherent causal inference;unclear counterfactual data;text data propose"}, "f8b32c2edcd7ef098ce40b7fd2e68448ac818191": {"ta_keywords": "eye tracking features;svvm novel eye;using eye tracking;eye movement based;eye tracking;text using eye;use eye movement;eye movement features;detection human eye;human eye movement;simple eye movement;eye movement;vector machines svvm;gaze duration word;support vector machines;eye movement improved;novel eye movement;eye movement improve;machines svvm novel;detection performance eye;detect unknown words;learn use eye;performance eye movement;eye movement paper;gaze duration;machines svvm;svvm novel;vector machines;svvm;based combination gaze", "pdf_keywords": ""}, "11e4346e60ac76ad018231a851fbbdb2112044d2": {"ta_keywords": "regularized reasoning logic;interpretable fact verification;logic regularized reasoning;fact verification benchmark;logic verifiable reasoning;reasoning logic verifiable;regularized reasoning;verifiable reasoning logic;phrase level veracity;public fact verification;level veracity phrases;verify veracity;phrase veracity;logic verifiable;verifiable reasoning;veracity phrases;faithful accurate interpretability;phrase veracity valued;verify veracity large;statement verify veracity;wikipedia decompose verification;fact verification;logic regularized;interpretability existing neural;accurate interpretability existing;veracity large scale;approach interpretable fact;logic represent claim;claim phrase veracity;reasoning logic", "pdf_keywords": "capable predicting veracity;predicting veracity claim;machine reading comprehension;phrasal veracity predictions;predicting veracity;phrase veracity;veracity predictions;evidence retrieval fact;answer prediction;phrase veracity paper;answer answer prediction;answer prediction 2016;veracity predictions experiments;evidence retrieval;meaningful phrasal veracity;faithful phrase veracity;phrasal veracity;veracity claim veracity;trustworthy knowledge sources;propose machine reading;claims trustworthy knowledge;probing questions evidence;faithful accurate interpretability;tasks evidence retrieval;reading comprehension;evidence answer supervise;comprehension mrc task;reading comprehension mrc;claim veracity;machine reading"}, "bcbac71ac64cd6a6aaae41e37ebe960f508ab741": {"ta_keywords": "factual information memorized;training manipulating symbolic;language models;subsymbolic neural knowledge;neural language model;memorized training corpora;factual information subsymbolic;neural knowledge;language model;neural language;massive language models;language models shown;models massive language;information memorized training;neural knowledge way;information memorized;interpretable factual information;memorizing entire game;symbolically interpretable factual;hockey game memorizing;commonsense factual information;game memorizing;game memorizing entire;manipulating symbolic representations;develop neural language;memorized training;information subsymbolic neural;memorize history hockey;information subsymbolic;symbolic representations", "pdf_keywords": "question answering datasets;question answering;benchmark question answering;question answering integrating;new memories inference;factoid question answering;entity memories relations;memories inference;symbolically bound memories;answering datasets freebaseq;bound memories sub;memories relations embedded;entity memories;including entity memories;bound memories retraining;neural language model;language model factoid;symbolically bound memory;interfacing neural language;memory networks;memories sub symbolic;answering datasets;bound memories;memory;inject new memories;large external memories;bound memory;memories inference time;pretraining text corpus;memories retraining parameters"}, "1ca247158522991ad54cccaac6c6938576a8bd26": {"ta_keywords": "transformer neural networks;trained transformer neural;transformer neural;trained transformer;phase transition xmath0;xmath1 phase diagram;rank trained transformer;xmath0 xmath1 phase;xmath1 phase;presents neural network;phase transition;phase diagram;equilibrium phase transition;phase diagram fig;neural network;transition xmath0;transformer;transition xmath0 xmath1;phase diagram shown;phase;non equilibrium phase;diagram fig phase_phase_phase;equilibrium phase;phase_phase_phase phase diagram;neural networks;neural network model;fig phase_phase_phase;fig phase_phase_phase phase;phase_phase_phase;phase_phase_phase phase", "pdf_keywords": "document ranking task;document ranking;marco document ranking;accurate ranking components;accurate ranking;document ranking leaderboard;complex accurate ranking;ranking task;translation features ibm;achieves ranking accuracy;translation based machine;machine translation;machine translation objectives;translation features;lexical translation features;based machine translation;retrieval pipeline;ranking accuracy;machine translation based;textual similarity features;ranking;ranking task 15;ranking achieves;retrieval pipeline documents;ranking accuracy 298;features lexical translation;multi stage retrieval;ranking achieves ranking;ranking components;track adhoc ranking"}, "2eef9173946078c402596b9b080b6878db00b8ac": {"ta_keywords": "language food twitter;social media text;food twitter;twitter;food twitter discuss;twitter popular platform;twitter popular;data task twitter;domain social media;twitter discuss development;task twitter;twitter discuss;task twitter popular;social media;diabetes language;early detection t2dm;ia diabetes language;text;diabetes language food;detection t2dm;t2dm;type ia diabetes;detection t2dm application;t2dm application;diabetes;ia diabetes;domain social;text presents;text presents challenges;media text", "pdf_keywords": ""}, "38bd034e6a0589bf1132d3e8c79818b271377290": {"ta_keywords": "discriminative training recently;discriminative training;weighted discriminative training;discriminative training thisarticlepresents;scale discriminative training;discriminative training withwell;based hmm training;margin based weightedclassi\ufb01cation;mce mpe corpus;error weighted discriminative;mpe corpus;large scale discriminative;margin based mmi;weighted discriminative;mpe based hmm;mpe corpus sponta;support vectormachines svms;mmi mpe trainingbased;methods hmm optimization;context machine learning;concept support vectormachines;discriminative;weightedclassi\ufb01cation error modeled;uni\ufb01cation margin based;support vectormachines;generalized error weighted;hmm training incrementally;proposed support vectormachines;hmm optimization;mpe trainingbased", "pdf_keywords": ""}, "3429a6b440fb6f71990bbeda9d097d709634a913": {"ta_keywords": "self training parsing;parser selftraining;trees parser selftraining;training parsing;parser output training;training parsing errors;translation parse trees;training uses parser;improve parser accuracy;parser selftraining paper;automatically generated parse;syntaxbased machine translation;improve parser;parsing significantly improved;translation parse;using translation parse;accuracy parsing significantly;generated parse trees;generated parse;parse trees parser;accuracy parsing;parser accuracy;method improve parser;parser;trees parser;parse;parse trees using;parsing significantly;uses parser;parse trees", "pdf_keywords": ""}, "9ce09b03f056253252f3e8c0c65d86a27117a0ac": {"ta_keywords": "digit classification mnist;demonstration entanglement dimensional;experimental demonstration entanglement;demonstration entanglement;entanglement;adaptation digit classification;entanglement dimensional 2d;domain adaptation;entanglement dimensional;2d lattice model;domain adaptation digit;classification mnist;target domain adaptation;dimensional 2d lattice;lattice model experiments;2d lattice;adaptation supervised;lattice model;trained unlabeled;propose entropy minimization;digit classification;adaptation supervised model;lattice;classification mnist svh;trained unlabeled test;measured entropy predictions;model trained unlabeled;supervised model trained;entropy minimization;adaptation digit", "pdf_keywords": "training domain adaptation;domain adaptation deep;domain adaptation source;unsupervised domain adaptation;domain adaptation;domain adaptation methods;000 domain adaptation;domain adaptation self;robust training domain;adversarial noise;training domain;adaptation deep;present domain adaptation;adversarial;adaptation deep residual;adaptation self supervised;adversarial noise propose;prediction presence adversarial;adaptation source data;presence adversarial noise;self supervised learning;methods robust training;adaptation source;residual learning image;presence adversarial;robust training;self supervised;imagenet;deep residual learning;experiments choose imagenet"}, "2db020e3398c06e3a22f12d8caffe76b0d9d1dda": {"ta_keywords": "question answering benchmarks;question answering tasks;question answering;commonsense question answering;answering benchmarks;language models training;tasks global knowledge;answering benchmarks paper;answering tasks;language model learn;generated external knowledge;results commonsense;tasks individual knowledge;resources language models;accuracy commonsense;global knowledge graph;external knowledge resources;answering tasks data;empirical results commonsense;language models;knowledge resources language;individual knowledge graph;learn answer;model learn answer;utilize external knowledge;tasks learning;neural language modeling;global knowledge;knowledge graph brings;knowledge graph better", "pdf_keywords": "commonsense reasoning machines;commonsense questionanswering tasks;trained commonsense knowledge;machine question answering;results commonsense questionanswering;question answering benchmarks;model trained commonsense;question answering;commonsense tasks;trained commonsense;transfer commonsense tasks;commonsense knowledge bases;questionanswering tasks;tests commonsense reasoning;questionanswering tasks data;commonsense questionanswering;commonsense knowledge;commonsense tasks propose;commonsense reasoning;generated questions kgs;results commonsense;shot transfer commonsense;tests commonsense;reasoning machines;reasoning machines focusing;automatically generated questions;answering benchmarks effective;knowledge pre train;empirical results commonsense;questionanswering"}, "68dca6ee694f22e2af66b56e60fdfa74041242e6": {"ta_keywords": "asr train train;asr train;train train french;behaviour asr train;train train train;train train;wulff mew model;train french meyer;asymptotic behaviour asr;train;study asymptotic behaviour;meyer wulff mew;asymptotic behaviour;train french;french meyer wulff;wulff mew;meyer wulff;paper study asymptotic;study asymptotic;mew model;asymptotic;wulff;behaviour asr;french meyer;asr;mew;meyer;model;behaviour;french", "pdf_keywords": ""}, "a0f8733dd84608b3cad97904624f8bfdc2d2fcbf": {"ta_keywords": "maintenance medical equipment;cost low maintenance;low maintenance medical;medical equipment;maintenance medical;low maintenance;low cost low;feasibility study low;equipment;low cost;cost low;feasibility study;results feasibility study;feasibility;results feasibility;present results feasibility;study low cost;maintenance;cost;medical;study low;low;article present results;present results;article;study;results;article present;present", "pdf_keywords": ""}, "890317710697a9e41d0d9961d99986c4d865393f": {"ta_keywords": "app usage graph;app usage representations;usage representations graph;app usage prediction;semantic aware representations;representations app usage;learned representations app;context social network;usage graph;representations graph;semantic aware;representations app;apps apparents ubiquitous;aware representations;app usage;aware representations recent;usage prediction task;usage prediction;representations graph capture;social network;graph capture relationships;usage representations;model map apps;locations apps apparents;apps;units semantic aware;mobile apps;apps location time;map apps;units app usage", "pdf_keywords": ""}, "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d": {"ta_keywords": "bayesian component conditional;bayesian component;easily compose predictors;transaction fluid dynamics;dynamics;dynamics charged;fluid dynamics video;predicting;attribute decomposed bayesian;dynamics video;business transaction fluid;predicting outcome;predicting outcome competition;machine translation;dynamics charged particle;decomposed bayesian component;predict outcome;particle interacting rigid;transaction fluid;fluid dynamics;machine translation observe;rigid body manipulated;conditional component propose;bayesian;particle;video dynamics charged;predictor predict outcome;dynamics video dynamics;charged particle interacting;interacting rigid body", "pdf_keywords": "controlled text generation;text generation;topic control;generative;machine translation powerful;discriminators generation fudge;machine translation;approach topic control;topic control based;fudge machine translation;pretrained language models;generative model;learning machine translation;controlled text;particular machine translation;language models;learning models task;machine translation paper;detoxifying language based;future discriminators generation;learning models;method controlled text;discriminators generation;probabilities generative model;probabilities generative;task predicting;text;generation;detoxifying language;generative model particular"}, "97e8430fe01394ea9a49fd841d9aecdfc294a796": {"ta_keywords": "ultrasound data fuzzy;fuzzy classifier;fuzzy classifier used;classification algorithm;fuzzy rules classification;classification myocardial heart;rules classification myocardial;data fuzzy classifier;algorithm binary classification;classification myocardial;binary classification based;binary classification;quantum hall;classification binary systems;classification algorithm binary;aided diagnosis myocardial;heart disease ultrasonic;manifestation quantum hall;binary systems gaussian;diagnosis myocardial heart;classification based;data fuzzy;classification;computer aided diagnosis;classification binary;quantum hall effect;fuzzy rules;gaussian distributed membership;diagnosis myocardial;cases echocardiographic images", "pdf_keywords": ""}, "33206a493e3519d27df968e98eb7fe6af14ef985": {"ta_keywords": "learn causal relationships;learning constructs causal;learn causal;computers learn causal;memory causal relationships;causal relationships;causal relationship person;creating memory causal;causal relationships present;causal relationship;example causal relationship;memory causal;causal relationships paper;constructs causal;constructs causal model;causal;occam incremental learning;incremental learning;explanation based learning;causal model;example causal;similaritybased learning;incremental learning constructs;similaritybased learning note;learning methods like;like similaritybased learning;causal model processes;predicting outcome;knowledge intensive methods;knowledge free learning", "pdf_keywords": ""}, "89440a4cb27d17ced5d54bbe0f81f3477bf16404": {"ta_keywords": "ising model triangular;2d ising model;model triangular lattice;dimensional 2d ising;triangular lattice;2d ising;ising model;lattice paper xmath0;model triangular;xmath1 correspondence;xmath0 xmath1 correspondence;xmath1 correspondence used;triangular lattice paper;lattice;eigenspectrum;eigenvectors eigenspectrum;eigenvalues eigenvectors eigenspectrum;dynamics dimensional 2d;relations eigenvalues;relations eigenvalues eigenvectors;set relations eigenvalues;xmath1;xmath0 xmath1;triangular;lattice paper;ising;xmath0;eigenvectors;dynamics dimensional;paper xmath0 xmath1", "pdf_keywords": ""}, "27dafb0b6076e050c31ede8d3e0184ef3592b364": {"ta_keywords": "falling aluminum foil;aluminum foil ff;aluminum foil;hm falling aluminum;performance head head;falling aluminum;head gb heavy;metal hm falling;performance head;heavy metal hm;foil ff efficient;head head;aluminum;head head ff;heavy metal;foil;head ff head;head;gb heavy metal;metal hm;head gb;metal;ff head;ff head gb;head ff;analyze performance head;foil ff;gb heavy;ff efficient gb;ff efficient", "pdf_keywords": ""}, "591ebe6dccb388d041623840db29a5e58824b4b0": {"ta_keywords": "compact preference networks;preference networks present;preference networks;compact preference models;networks conformal nets;underlying preference orders;preference models;networks conformal;preference reasoning literature;preference reasoning;preference orders;conformal networks;preference models based;cp nets nodes;concept conformal networks;underlying preference;model compact preference;distributed cp nets;compact preference;preference orders use;nets nodes;nodes;statements underlying preference;networks present;networks;networks present novel;nets nodes maximum;cp nets;conformal networks conformal;choice preference reasoning", "pdf_keywords": ""}, "7f1a6c67d03de88b898271d52dd2e51907d5b615": {"ta_keywords": "tasks natural language;tasks predicting syntax;natural language processing;generalized natural language;predicting syntax semantics;spanning dependency parsing;natural language analysis;semantic role labeling;role labeling semantics;dependency parsing;predicting syntax;natural language;sentiment achieving performance;semantics relation extraction;dependency parsing syntax;tasks predicting;spans single task;parsing syntax semantic;multi task learning;10 disparate tasks;labeling semantics;parsing;semantics information content;syntax semantic role;task learning;language analysis provides;tasks spanning dependency;analysis sentiment achieving;labeling semantics relation;labeling spans relations", "pdf_keywords": "language analysis tasks;response generation summarization;performance natural language;natural language processing;spans single task;natural language analysis;machine translation dialog;dialog response generation;range natural language;generation summarization;machine translation;natural language;text machine translation;designed natural language;similarity tasks;source tasks;tasks require generation;labeling spans relations;language analysis handle;generation text;source tasks wide;tasks particular span;generation text machine;span relation representations;natural languages;natural languages possible;language analysis;language processing;similarity difference attention;generation summarization paper"}, "a9c0ffc760f65ccaa99a08fc66b31653fd4a5bd7": {"ta_keywords": "organ donation;organ donation extremely;success organ donation;organ donation donation;study organ donation;program organ donation;donation program organ;commitment organ donation;organ donation population;organ donation program;organ transplantation;donation biological;donation donation biological;donation biological tissue;pool organ transplantation;organ transplantation rapid;results study organ;organ;organ human body;organ human;transplantation;organ organics;need transplantation;transplantation present simple;tissue organ human;need transplantation present;recipient need transplantation;success organ;commitment organ;transplantation present", "pdf_keywords": ""}, "a03675379685d88c727bc985a323cc71d06f2514": {"ta_keywords": "unsupervised learning syntactic;unsupervised dependency parsing;learns discrete syntactic;learning syntactic structure;learning syntactic;dependency parsing difficult;word representations unsupervised;dependency parsing;discrete word representations;parsing difficult training;jointly learns discrete;structured generative;word representations leveraged;structured generative prior;discrete syntactic structure;network structured generative;generative prior tree;discrete syntactic;prior tree structured;continuous word representations;unsupervised dependency;parsing difficult;syntactic structure continuous;jointly learns;word representations;model jointly learns;word representations invertibility;unsupervised learning;parsing;learns discrete", "pdf_keywords": "unsupervised grammar induction;unsupervised dependency parsing;unsupervised models syntactic;grammar induction unsupervised;word embeddings unsupervised;trained word embeddings;learning unsupervised dependency;dependency parsing unsupervised;unsupervised grammar;unsupervised approach grammar;important unsupervised grammar;approach unsupervised grammar;train word embeddings;embeddings unsupervised models;word embeddings directly;word embeddings;parsing unsupervised;word embeddings massive;embeddings unsupervised;parsing unsupervised methods;embeddings massive unlabeled;grammar induction;dependency parsing;learning unsupervised;grammar induction approach;structured syntax models;approach grammar induction;syntax models;continuous word embeddings;unsupervised dependency"}, "7f817600b612aab6039dfba576ae8e8e7460d8f1": {"ta_keywords": "automatic speech;current speech recognition;speech recognition;speech recognition systems;automatic speech recognition;performance automatic speech;learning dirichlet processes;acquire word pronunciations;learning dirichlet;bayesian learning dirichlet;directly speech adapt;word pronunciations phone;acoustic textual data;speech adapt;sampled acoustic textual;initial pronunciation dictionary;pronunciation dictionary using;pronunciations phone transcripts;speech adapt new;learn directly speech;pronunciations phone;acoustic textual;bayesian learning;recognize words;based bayesian learning;pronunciation dictionary;dirichlet processes;initial pronunciation;expands initial pronunciation;movement phonemic", "pdf_keywords": ""}, "60a9438c24847a949419e0350a61fc2a330e4a09": {"ta_keywords": "biases kernel clustering;kernel clustering methods;popular kernel clustering;density biases kernel;kernel clustering criteria;kernel clustering;kernel clustering directly;kernel clustering limitations;kernels kernel clustering;bias kernel means;understanding kernel clustering;density equalization;sparsest subset bias;density equalization implicitly;biases kernel;sets density biases;clustering criteria density;locally adaptive kernels;bias kernel;adaptive kernels;density biases proposed;isolation bias kernel;density biases;subset bias;kernel methods;based sparsest subset;subset based sparsest;kernels kernel methods;kernel methods popular;density biases theoretically", "pdf_keywords": "biases kernel clustering;kernel clustering objectives;kernel clustering;kernel clustering limitations;popular kernel clustering;kernel clustering paper;approach kernel clustering;kernel clustering based;understanding kernel clustering;density biases kernel;bias kernel means;clustering decision tree;clustering limitations principled;clustering decision;kernel density estimator;bias kernel;approximate optimization clustering;kernel density;biases kernel;clustering limitations;new kernel density;clustering;clustering criteria propose;clustering based notion;optimization clustering;clustering objectives;clustering criteria;explanation bias kernel;optimization clustering criteria;context clustering decision"}, "11db042ed2264f3ea1b8f20151adf725ec3461e8": {"ta_keywords": "trivial neural network;error surfaces neural;surfaces neural networks;trivial neural;scaling loss;deep learning;space deep learning;deep learning powerful;neural networks;scaling behavior weights;scaling loss distance;neural network;space scaling loss;neural networks propose;weight space scaling;neural;loss distance critical;surfaces neural;neural network error;space simple scaling;network error surfaces;networks;critical points training;error surfaces;error surfaces globally;weight space deep;simple scaling;scaling behavior;regions weight space;points true minima", "pdf_keywords": "neural networks trivial;networks trivial neural;neural networks highly;trivial neural network;zero loss training;error surfaces neural;surfaces neural networks;networks trivial;gradient descent;trivial neural;gradient descent presenting;convex locally nonconvex;layer convolutional neural;initialization partial training;course gradient descent;neural;convergence loss;mnist dataset paper;surfaces neural;nonconvex breaking symmetry;highly non convex;neural network error;symmetry random initialization;network error surfaces;apparent convergence loss;networks highly non;neural networks;networks highly;convergence loss does;mnist dataset"}, "201b79be15b6b01e62a82b29ac4d30d3e6a11799": {"ta_keywords": "channel neural beamformer;neural beamformer;speech recognition single;speaker speech recognition;multi channel neural;channel neural;fully recurrent neural;speech recognition;2mix corpus transformer;corpus transformer;neural beamformer order;corpus transformer based;recognition single channel;recurrent neural network;recurrent neural;rnn based end;wsj1 2mix corpus;multi speaker speech;network rnn;neural network rnn;reverberated signals;network rnn based;multi channel tasks;speaker speech;effective multi speaker;speaker;2mix corpus;channel tasks;handle reverberated signals;multi speaker", "pdf_keywords": "end speech recognition;speech recognition multi;speaker speech recognition;speech beamforming end;multi speaker encoders;speaker speech beamforming;speech beamforming;speech recognition task;channel multi speaker;multi speaker speech;speech recognition;speech recognition model;speech recognition models;recognize separated speech;separated speech streams;speaker encoders;multi speaker;recognition multi channel;automatic speech recognition;neural beamformer;speaker encoders experiments;speech streams;separates multi speaker;maskbased neural beamformer;neural beamformer frontend;speech recognition based;automatic speech;end multi channel;end speech;end end speech"}, "62606fbb3aa3ffb17c5427b3652c18a81425cd65": {"ta_keywords": "trained named entity;data annotated training;annotated training data;entity recognizer;named entity recognizer;entity recognizer ner;data annotated;annotated training;training data automatically;gene expression data;significant annotated training;training data;named entity;sources data annotated;generate significant annotated;approach extract protein;create training data;protein interactions gene;protein protein interactions;interactions gene expression;protein interactions;training data present;extract protein protein;training data paper;particle contact noisy;entity;results trained named;annotated;extract protein;gene expression", "pdf_keywords": ""}, "99848c6424556bce427d621e89b6d05dac131910": {"ta_keywords": "microtask based crowdsourcing;crowdsourcing tasks;microtask platform crowdsourcing;developed crowdsourcing tasks;crowdsourcing platform;russian national corpus;crowdsourcing tasks paper;crowdsourcing platform developed;crowdsourcing;crowdsourcing crowd;crowdsourcing crowd workers;platform developed crowdsourcing;platform crowdsourcing;developed crowdsourcing;platform crowdsourcing platform;nouns extracted russian;russian language using;language using microtask;based crowdsourcing;russian language;based crowdsourcing crowd;human intelligence task;terms hypernyms dataset;dataset kind russian;hypernyms pair annotated;intelligence task hit;relation extraction;corpus;yandex toloka microtask;annotators", "pdf_keywords": ""}, "7a1bcf3c84607f7aeb0601658845ca2083059f43": {"ta_keywords": "supervised learning semi;learning semi supervised;semi supervised;semi supervised learning;propose semi supervised;semidefinite supervised learning;learning semi;semidefinite supervised;agreement semidefinite supervised;textual interpolation;supervised;visual language tasks;vision language tasks;applicability textual interpolation;supervised learning;supervised learning strategy;supervised learning aims;textual interpolation context;scratch supervised learning;supervised learning vision;learning vision language;score scratch supervised;language tasks;leveraging large unlabelled;scratch supervised;natural language;interpolation context natural;strategy visual language;labeled data leveraging;language tasks using", "pdf_keywords": ""}, "ef09dd6f5615e2b937d3f9dd555c2daafb4c4f4b": {"ta_keywords": "performance virtual observatory;reality virtualization library;reality virtualization;virtual machine;machine virtual;virtualization library;virtualization;virtual observatory virtual;parallel virtual machine;virtual observatory;virtual reality virtualization;computation single computer;parallel virtual;reality software virtual;virtual machine virtual;virtualization library virtual;virtual reality software;virtual reality present;observatory virtual observator;observatory virtual;software virtual reality;observator virtual machine;observable virtual reality;qubits;machine virtual observable;performance virtual;qubits required;qubits required perform;virtual observator;virtual reality virtual", "pdf_keywords": ""}, "aa0d4f7cfa13758a02d248fd607547f045306519": {"ta_keywords": "person recognizers email;recognizers email;recognition informal documents;recognizers email email;person recognizers;paper recognition informal;recognition informal;email specific structural;performance person recognizers;recognizers;features recall enhancing;informal documents;competition paper recognition;features recall;recall enhancing method;documents like email;structural features recall;like email possible;informal documents like;recall enhancing;email specific;like email;email possible;repetition multiple documents;selecting winning entry;email email specific;paper recognition;recognition;recall;participants competition based", "pdf_keywords": ""}, "36db0616e59c8ac5e9ba8ded820ef6c969f068c1": {"ta_keywords": "motion particle potential;particle potential;model motion particle;motion particle;geometric model motion;model motion;particle;potential;motion;simple geometric model;geometric model;geometric;simple geometric;present simple geometric;model;present;present simple;simple", "pdf_keywords": ""}, "78f1eef6d79a129f59b977a5037f5fc9cc7fda90": {"ta_keywords": "analysis peer review;peer review;data peer review;improved peer review;peer review processes;peer review paper;peer reviewed;peer review based;results peer reviewed;analysis peer;applications peer review;peer rheology approach;peer reviewed evaluation;concept peer review;peer peer rheology;peer review faces;results peer;peer review applies;peer rheology;conference data peer;present results peer;toolkit improved peer;peer peer;data peer;improve peer peer;evaluations conference data;peer;improved peer;review processes paper;method improve peer", "pdf_keywords": ""}, "afdae523d420278670c30f45c015cc5860a0de22": {"ta_keywords": "improves convergence adaptive;convergence adaptive methods;adaptive methods tasks;stochastic line search;interpolation converges minimizer;convex functions adagrad;functions adagrad robust;classification deep;prove adaptive methods;adaptive methods;convergence adaptive;consistently improves convergence;mappings classification deep;tasks binary classification;classification deep neural;deep neural networks;improves convergence;neural networks;classification kernel mappings;deep neural;adagrad robust;adaptive methods used;converge minimizer rate;minimizer rate smooth;converges minimizer optimal;kernel mappings classification;adaptive;rate smooth convex;satisfying interpolation empirical;neural networks prove", "pdf_keywords": "weighted gradient descent;stochastic gradient descent;gradient descent;gradient gradient descent;algorithm gradient descent;convex optimization heavy;gradient descent algorithm;gradient descent method;adaptive gradient methods;gradient descent prove;descent algorithm gradient;convex optimization;descent method convex;adaptive gradient;gradient methods widely;propose stochastic gradient;surely weighted gradient;task adaptive gradient;stochastic gradient;parameterization improves convergence;method convex optimization;weighted gradient gradient;algorithm gradient;gradient methods;optimization heavy;weighted gradient;optimization heavy ball;algorithm non convex;optimizers;improves convergence methods"}, "656aedc681975c3c97b1764466832de537358150": {"ta_keywords": "speech recognition pipeline;adaptation dnn;unsupervised adaptation dnn;acoustic model adaptation;adaptation dnn hybrid;adaptation encoder;speech recognition asr;based adaptation encoder;adaptation encoder decoder;speech recognition;hybrid automatic speech;speech recognition systems;automatic speech recognition;automatic speech;simplifies speech recognition;input deep neural;models map speech;feature based adaptation;features representing speakers;model adaptation widely;speech signal sequence;representing speakers vectors;encoder decoder e2e;dnn hybrid automatic;auxiliary features instead;model adaptation;recognition asr systems;encoder;e2e models acoustic;decoder e2e models", "pdf_keywords": ""}, "2ccfa631708b78130b1ea1b8ae3c2b688caf3938": {"ta_keywords": "surgery durations heteroscedastic;surgery uncertainty duration;duration surgery uncertainty;scheduling surgeries challenging;scheduling surgeries;scheduling surgeries allows;surgery uncertainty;approach scheduling surgeries;estimate duration surgery;demonstrate surgery durations;surgery durations;heteroscedasticity brain imaging;data scheduling surgeries;heteroscedasticity brain;surgeries allows informed;duration surgery;uncertainty clinical environment;heteroscedastic predictions optimally;durations heteroscedastic;surgeries challenging task;fundamental uncertainty clinical;heteroscedastic estimate duration;durations heteroscedastic estimate;uncertainty clinical;uncertainty duration models;effect heteroscedasticity brain;heteroscedastic predictions;surgeries challenging;uncertainty duration;finally heteroscedastic predictions", "pdf_keywords": "predict duration surgeries;predicting location surgery;surgery durations heteroscedastic;surgery location time;duration surgeries operating;particular surgery durations;surgery patient health;surgeries operating rooms;doctors accurately predict;surgery durations;duration surgeries;surgeon surgery;rooms congested surgeries;scheduling techniques predictions;accurately predict duration;surgeon surgery location;congested surgeries run;surgeries operating;surgeon;congested surgeries;surgery patient;heteroscedastic neural regression;surgeries run;surgeries;scheduling;surgeries run long;predict duration;location surgery patient;neural regression;health particular surgery"}, "58777f0af009a225e315b7240db20ba545207702": {"ta_keywords": "preferences notion uncertainty;deterministic preferences notion;deterministic preferences;set deterministic preferences;preferences notion;preference aggregation scheme;preference aggregation;individual preferences observations;planning reasoning nondeterministic;preferences represented probability;individual agents preferences;social choice aggregate;agents preferences represented;preferences observations;manipulation social choice;presents preference aggregation;social choice functions;choice functions individual;preferences observations appropriate;choice aggregate;choice aggregate possibly;reasoning nondeterministic settings;agents preferences;notion uncertainty;individual preferences;manipulation uncertain information;notion uncertainty introduced;aggregation scheme;choice functions;based aggregation methods", "pdf_keywords": ""}, "309fb4d4d0946ac746f352c13cd3be4e2cd86dae": {"ta_keywords": "bayesian approaches speech;bayesian approaches acoustic;bayesian approach acoustic;approximating bayesian inferences;modeling speech recognition;acoustic modeling speech;modeling speech;approximating bayesian;ways approximating bayesian;montecarlo bayesian approaches;bayesian approaches widely;approach acoustic modeling;numerical computations bayesian;approaches acoustic modeling;inference bayesian approaches;speech processing;approaches speech processing;bayesian approaches;speech recognition;bayesian inference bayesian;montecarlo bayesian;acoustic modeling;bayesian inferences;computations bayesian approach;computations bayesian;approaches bayesian inference;acoustic modeling presented;speech processing applications;describes bayesian approaches;bayesian approaches involves", "pdf_keywords": ""}, "ce17dab00ddd2c86da508fc0502247f9b18a570f": {"ta_keywords": "winner proportional approval;computing winner proportional;winner proportional;voting satisfaction;prominent voting rules;ballots agents;proportional approval voting;approval ballots agents;compute representative winning;voting np hard;compute best vote;agents compute best;voting satisfaction approval;approval voting np;approval voting satisfaction;competition computing winner;showing multi winner;multi winner winner;computing winner;multi winner;voting rules;competition computing;multiple winners;approval voting;satisfaction approval voting;voting np;winner win competes;reweighted approval voting;ballots agents study;competes single winner", "pdf_keywords": "approval voting schemes;voting schemes select;voting schemes;compute approval voting;computing strategic votes;strategic votes rules;compute best vote;voting rules;consider voting rules;approval voting;votes rules particular;elect multiple winners;voting committee;voting rules use;use approval ballots;votes rules;approval ballots;agents compute best;set approval ballots;voting;ballots;approval voting committee;members dichotomous preferences;polynomial compute approval;strategic votes;aspects approval voting;voting committee members;approval ballots elect;approval voting important;committee selection multi"}, "1d1bbed89882ac1001b915ee73199a919aa0d13c": {"ta_keywords": "language modelling conditioning;languages prior;explore language modelling;language modelling;language modelling explore;vocabulary language modelling;languages prior outperforms;open vocabulary language;held languages prior;learning languages noisy;learning languages;supervision held languages;prior held languages;languages;modelling explore language;level open vocabulary;languages world features;language specific information;world languages;universal linguistic knowledge;constructing informative prior;languages world;language;held languages task;explore language;languages noisy data;vocabulary language;information available languages;problem learning languages;informative prior", "pdf_keywords": "prior neural weights;language models features;language models;language modeling inductive;level language modeling;language modeling;informative prior neural;knowledge uninformative priors;language modeling particular;prior neural;language modeling sample;character level language;neural weights;universal linguistic knowledge;prior space neural;universal linguistic;new human languages;generalize prior;imbued universal linguistic;uninformative priors;neural networks implicitly;language information drawn;level language;human languages;language information;biases neural networks;constructing informative prior;linguistic knowledge uninformative;informative prior;languages"}, "7b7f8fb08262fce3da64c09788fd4b595408e4e6": {"ta_keywords": "mudulation event based;predicting outcome mudulation;outcome mudulation event;mudulation event;outcome mudulation;mudulation;predicting outcome;method predicting outcome;robust postfilter;simple robust postfilter;method predicting;event based;predicting;postfilter;new method predicting;robust;outcome;simple robust;event;simple simple robust;event based use;based use;method;based use simple;based;new method;use simple;simple;present new method;new", "pdf_keywords": ""}, "d940e0192a6cc1ddd6288239b77b06e50f042114": {"ta_keywords": "supervised pretraining speech;pretrained speech representations;pretraining speech data;results pretrained speech;pretraining speech;pretrained speech;eral pretrained speech;performance automatic speech;speech recognition asr;speech representations;speech data;automatic speech;speech data achieved;speech recognition;automatic speech recognition;speech representations present;representation speech signal;speech representations used;speech signal learned;fidelity representation speech;performance pretrained representations;self supervised pretraining;asr benchmark corpora;asr self supervised;recognition asr benchmark;supervised pretraining;pretrained representations;pre trained representations;representation person speech;recognition asr", "pdf_keywords": "selfsupervised speech representation;representation automatic speech;speech representation learning;train automatic speech;speech representation;automatic speech;automatic speech recognition;selfsupervised speech;speech recognition asr;speech recognition;speech recognition task;deep representation automatic;novel selfsupervised speech;speaker speech corpora;learn deep representation;speech corpora;single speaker speech;speech corpora paper;learning representations improve;deep representation;models single speaker;deep convolutional neural;wav2vec2 models;learning representation;learning representation sslr;self supervised learning;learning representations;speaker speech;recognition asr;network cnn"}, "d5924c8cdef6270a955ba82c2b07a8282d869744": {"ta_keywords": "speech gestures;language speech gestures;speech gestures context;gesture generation;gesture generation combines;approaches gesture generation;gesture generation propose;gesture generation study;text gestures significantly;approach gesture generation;gestures significantly;distribution text gestures;sampling combines adversarial;language acoustic cues;text gestures;gestures;gestures context;predicting acoustic cues;learning importance sampling;sampling propose multimodal;acoustic cues spoken;cues spoken language;gesture;cues spoken;multimodal multiscale attention;predicting acoustic;language acoustic;importance sampling propose;approaches gesture;novel approach gesture", "pdf_keywords": ""}, "b82e9b84cb639f6bb061c8a43b97986ecfec00ea": {"ta_keywords": "embeddings graphs;similarity queries;similarity queries like;embedding large graphs;deterministic embeddings graphs;embeddings graphs shown;large graphs representation;dimensional representation entities;embeddings;queries like set;graphs representation;graphs representation constructed;datasets web approach;datasets web;entities different datasets;deterministic embeddings;representation entities;variety similarity queries;embedding;large graphs;embedding large;queries like;powerful tool embedding;different datasets web;tool embedding;similarity;entities;representation entities different;low dimensional representation;tool embedding large", "pdf_keywords": ""}, "ee33d61522fd70fa4e6470decbdac6c17f8b4fdb": {"ta_keywords": "attractors speech embedding;clustering based speaker;speaker diarization;end speaker diarization;multiple attractors speech;based speaker diars;speech embedding;dimer speaker subset;speaker diarization outperformed;attractors speech;number attractors speech;speech embedding sequence;speaker diars;dimer speaker;generate number speakers;decoder based attractor;speaker subset callhome;diarization error rate;number speakers activities;speaker subset;speaker diars drawback;encoder decoder based;encoder;encoder decoder;speakers activities;decoder based diaphragm;diarization outperformed conventional;diarization;diarization outperformed;number speakers", "pdf_keywords": "end speaker diarization;diarization speaker embeddings;speaker diarization promising;speaker diarization proposed;sequence speaker diarization;speaker diarization;speaker diarization task;diarization speaker;speaker diarization method;connection speaker diarization;embedding sequence speaker;speaker diarization important;method diarization speaker;speaker embeddings;speaker diarization key;automatic speech;speech recognition;automatic speech recognition;using automatic speech;sequence speaker;speech recognition asr;end speaker;end end speaker;decoder connection speaker;speaker embeddings paper;recognition asr multitalker;diarization promising approach;diarization promising;encoder decoder based;encoder"}, "d6d2003d112e3d9d93edd4920436fe2fe879eb87": {"ta_keywords": "sparsifying similarity matrix;similarity matrix;effective clustering;effective clustering methods;clustering accuracy;elegant effective clustering;sparsifying similarity;points sparsifying similarity;estimating speed person;similarity matrix paper;wise similarities data;similarity matrix state;clustering methods;similarities data points;similarities data;clustering;methods clustering accuracy;similarity;previous methods clustering;clustering accuracy does;speed person presence;clustering methods exploits;methods clustering;operating similarity matrix;estimating speed;time space sampling;speed person;method estimating speed;operating similarity;wise operating similarity", "pdf_keywords": ""}, "3b8b6f27a5df5dc2c231d0fa1e471887e4583466": {"ta_keywords": "citation networks curated;metadata citation networks;knowing genes author;link prediction;citation networks;based link prediction;citations help predict;link prediction technique;identifying genes academic;genes author likely;coauthors citations;networks curated;genes author previously;genes author;new genes author;coauthors citations help;social network;related metadata citation;networks curated databases;social biological information;genes academic;genes academic biomedical;clip knowing genes;academic biomedical publications;previous coauthors citations;metadata citation;citations;predict new genes;identifying genes;information previous coauthors", "pdf_keywords": ""}, "4bfc185dcc67b3eddfa059fc5446f4df844a0728": {"ta_keywords": "2d harmonic trap;harmonic trap;2d harmonic;dimensional 2d harmonic;dynamics single particle;particle dimensional 2d;single particle dimensional;harmonic;single particle;trap;particle dimensional;dynamics single;particle;dynamics;study dynamics single;dimensional 2d;2d;study dynamics;dimensional;detailed study dynamics;single;present;present detailed;detailed;study;detailed study;present detailed study;paper present detailed;paper present;paper", "pdf_keywords": ""}, "9635e3c008f7bfa80638ade7134a8fb0ef1b37e1": {"ta_keywords": "neural language models;language model training;samples neural language;model best tokenisation;language models;pretrained language model;canonical tokenisation train;best tokenisation marginal;marginal likelihood tokenisations;language models evaluated;instead language models;language model best;likelihood tokenisations;tokenisation marginal perplexities;tokenisation robustness;regard tokenisation robustness;neural language;tokenisation train;best tokenisation;language models typically;tokenisation marginal;language model;tokenisation robustness evaluate;models typically tokenise;tokeniser uncertainty;canonical tokenisation;single canonical tokenisation;particularly regard tokenisation;tokenisations;evaluate pretrained language", "pdf_keywords": "best tokenisation likelihood;tokenisations evaluation metric;tokens language models;likelihood tokenisations evaluation;suggesting tokenisers entropy;tokenisations evaluation;tokenisation likelihood;tokenising word types;likelihood tokenisations;tokenisation likelihood marginal;marginal likelihood tokenisations;language models trained;tokenising word;tokeniser entropy good;tokenisers entropy;metric language models;tokeniser entropy;tokenisers entropy guide;best tokenisation;suggesting tokenisers;tokenisers trained;domain tokenisers trained;compared independently tokenising;possible ones tokenisation;tokenisers trained using;results tokenising word;language models;tokenising;tokens language;tokenisations"}, "3f2821dd40c12da560c89b9dad7f95cd4ad9354f": {"ta_keywords": "overwhelms storage infrastructure;roadblock diskadaptive redundancy;storage clusters imposes;diskadaptive redundancy propose;storage clusters;storage storage clusters;scale storage clusters;storage clusters based;disk adaptive redundancy;storage cluster;large storage cluster;diskadaptive redundancy;redundant storage;redundant storage storage;storage infrastructure;large scale storage;storage infrastructure experiment;overwhelms storage;overload roadblock diskadaptive;clusters based disk;storage cluster external;roadblock diskadaptive;cost redundant storage;based disk adaptive;disk adaptive;clusters load transitions;response large storage;data redundancy provides;traces millions disks;storage storage", "pdf_keywords": "storage cluster critical;roadblock diskadaptive redundancy;storage fault tolerance;storage clusters;diskadaptive redundancy transition;storage clusters hundreds;failure disk adaptive;diskadaptive redundancy;storage cluster;disk adaptive redundancy;disks production systems;world storage clusters;millions disks production;analyzes disk deployment;scale storage cluster;roadblock diskadaptive;large scale storage;logs million disks;overload roadblock diskadaptive;disks production;large storage systems;redundancy scheme disks;traces millions disks;scale production clusters;disks large organizations;real world storage;failure data large;production clusters discover;diskadaptive;millions disks"}, "ff6ddbd7ba59e0fd4a74748942083391d6e9a666": {"ta_keywords": "information extraction;information extraction multiple;end information extraction;proving correctness probabilistic;correctness probabilistic;correctness probabilistic argument;reasoning results paper;reasoning results;probabilistic argument;hypothesis reasoning results;probabilistic;performs hypothesis reasoning;extraction multiple media;hypothesis reasoning;extraction multiple;results english russian;ukrainian performs hypothesis;end information;results english;information;end end information;reasoning;extraction;media;multiple media;media integrates results;proving correctness;english russian ukrainian;media integrates;integrates results english", "pdf_keywords": ""}, "9fe09ca520cb7ce106e65b39c455777d18ec6efe": {"ta_keywords": "textual taxonomies predicting;curated taxonomies enhance;expand textual taxonomies;structured knowledge arborist;taxonomies predicting;taxonomies enhance performance;curated taxonomies;textual taxonomies;taxonomies predicting parents;15 curated taxonomies;taxonomies enhance;rapidly evolving taxonomy;presents new taxonomies;taxonomy nodes;taxonomies heterogeneous edge;new taxonomy nodes;arborist approach automatically;new taxonomies;knowledge arborist;recall 15 curated;arborist outperforms;knowledge arborist handles;demonstrate arborist outperforms;new taxonomies model;taxonomies;actual parents taxonomy;guarantees arborist;challenging scenario taxonomies;infeasible propose arborist;propose arborist approach", "pdf_keywords": ""}, "ef4166a7fb2c40ca87b0ebb253e8ba1e80c09fd7": {"ta_keywords": "transformation automatic speech;speech separation andrecognition;asr techniques discriminative;augmented discriminative feature;speech recognition;chime speech separation;automatic speech recognition;discriminative feature transformation;techniques discriminative training;augmented feature transformation;automatic speech;discriminative feature;speech recognition presence;feature transforms highly;speech separation;feature transforms;discriminative training;discriminative training various;feature transformation automatic;propose augmented discriminative;techniques discriminative;augmented feature;reverberation remains challenging;task augmented feature;discriminant feature transformation;augmented discriminative;discriminative training model;feature transformation provides;feature transformation;speech home noises", "pdf_keywords": ""}, "5db0fa82c322bb7d9f60109294d088ff139eebf3": {"ta_keywords": "gan manifold recovering;clean images denoising;denoising unseen images;images denoising;images denoising improved;gan manifold;denoising algorithm gradient;nearest point gan;networks gans;latent vector denoise;vector denoise image;denoise image;networks gans transform;gan manifold approximately;vector denoise;point gan manifold;denoising improved;variance generative adversarial;denoising unseen;gans transform low;denoising method image;generative adversarial networks;denoising;gans transform;denoise corrupted images;denoise image obtaining;generative adversarial;method denoising unseen;new denoising;bm3d denoising", "pdf_keywords": "denoising based generative;gan manifold recovering;nearest point gan;generative adversarial networks;gan manifold;image denoising;method image denoising;generative adversarial;denoise corrupted images;point gan manifold;based generative adversarial;propose denoise corrupted;latent vector recovery;denoising;image denoising based;image space latent;recovering latent vectors;better denoising;denoising results;denoising based;better denoising results;gan;yields better denoising;point gan;paper propose denoise;denoise corrupted;latent vectors minimizing;manifold recovering latent;adversarial networks;adversarial networks second"}, "1d05e91b6d94f06439b2b41291a8dcc3d8064149": {"ta_keywords": "approach color clustering;color clustering;color clustering using;clustering using kernel;clustering methods image;pairwise clustering algorithms;kernel means algorithm;geometric regularization image;segmentation propose;pairwise clustering;regularization image;image grid regularization;means approach segmentation;wise feature clustering;feature clustering methods;fitting segmentation;kernel means approach;feature clustering;fitting segmentation methods;segmentation;segmentation propose alternative;geometric regularization;regularization image domain;fitting techniques segmentation;approach segmentation benefits;kernel bandwidth estimation;standard geometric regularization;feature spaces kernel;grid regularization;model fitting segmentation", "pdf_keywords": ""}, "da1f22dd6d834e031eb733d2b70320f34ef9458f": {"ta_keywords": "pairwise comparisons compare;pairwise comparisons;comparison graph pair;pairs compared;cardinal measurement models;pairwise pairwise entropies;comparison graph;set pairs compared;form pairwise comparisons;pairs compared paper;cardinal measurement;pairwise entropies;comparisons compare;pairwise;error rates cardinal;comparisons compare error;pairwise entropies topology;errors ordinal cardinal;relationship pairwise pairwise;pairwise pairwise;rates cardinal measurement;ordinal cardinal settings;pairs graph set;graph pair pairs;pair pairs graph;compare error rates;graph set pairs;cardinal settings identical;comparisons;quality score vector", "pdf_keywords": "expertise ranking tasks;individuals based rankings;evaluation expertise ranking;expertise ranking;ranking tasks;ranking;pairwise comparisons fairly;model pairwise comparisons;rankings;pairwise comparisons;pairwise comparisons based;consider pairwise comparisons;expertise particular minimax;based rankings;collaborative \ufb01ltering certain;collaborative \ufb01ltering;comparisons based notion;eliciting expertise individuals;comparisons fairly;item largest perceived;predicted based preferences;propose model pairwise;largest perceived quality;comparisons based;evaluation expertise;selection item largest;comparisons;expertise individuals;expertise individuals based;large probabilities models"}, "b07b124852f897823490db0a04ea6e411bb77f00": {"ta_keywords": "binary classification multilabel;classification multilabel classification;multilabel classification;multilabel classification actual;classification multilabel;competition classifier;used multilabel classification;success binary classifiers;outcome competition classifier;binary classifiers;competition classifier based;classifiers;classifier;binary classification;classifiers class rare;classification;binary classifiers class;classifiers able;class classifiers;classifiers class;classifier able;classifiers able predict;classification actual;classifier based;class classifiers able;classifier used;based classifier;best achievable f1;classifier based classifier;rare paper classifier", "pdf_keywords": ""}, "b6502b61bf8f0332c6caa30198cff3619a9790aa": {"ta_keywords": "latency serving requests;performance redundant requests;systems latency serving;requests reduce latency;sending redundant requests;latency performance redundant;redundant requests;latency serving;redundant requests reduce;redundant service times;optimal redundant requesting;way redundant requests;redundant requests deemed;reduce latency need;systems distributed storage;redundant service;provider systems latency;distributed storage;distributed storage systems;redundant requests help;number redundant requests;reduce latency;requests way redundant;serving requests;redundant requesting;redundant requests diverse;service times performance;systems latency;request increase delay;serving requests potentially", "pdf_keywords": "redundantrequesting reduce latency;performance redundant requests;requests latency content;latency content aware;sending redundant requests;redundancy requests requests;redundancy requests;redundant requests;requests latency;distributed scheduling;requests requests latency;redundantly servers;impact redundancy requests;optimal redundant requesting;requests sent redundantly;latency performance redundant;redundant requests help;redundant requests primary;sent redundantly servers;latency content;present distributed scheduling;redundantrequesting;distributed scheduling algorithm;redundant requesting;settings redundant requests;redundant requesting policies;reduce latency;redundantly servers paper;designing optimal redundant;latency faced batch"}, "b72c5236dacf2b958ebcf427d17a100bc54af504": {"ta_keywords": "encode linguistic channel;encoder introducing;encoder;rna neural networks;encoder introducing context;novel approach encode;speech recognition;encode linguistic;automatic speech;recurrent neural networks;performance alternative recurrent;alternative recurrent neural;speech recognition systems;transformer encoder introducing;encode;contextual block processing;rna neural;neural networks end;acoustic speech;automatic speech recognition;approach encode linguistic;transformer encoder;compute self attention;neural networks recently;linguistic channel speaker;recurrent neural;proposed contextual block;end automatic speech;attributes acoustic speech;contextual block", "pdf_keywords": "online speech recognition;local context speech;speech recognition;recognition speech;recognition speech recognition;speech recognition speech;context speech utterance;context speech;automatic speech recognition;automatic speech;local phoneme classi\ufb01cation;framework online speech;learning local context;phoneme classi\ufb01cation;speech recognition based;proposed automatic speech;context inheritance train;context embedding;contextual block;additional context embedding;integrating acoustic models;online speech;cnn architecture;embedding vector contextual;network cnn;deep convolutional neural;context embedding vector;vector contextual block;network cnn architecture;contextual block processing"}, "6dfc2ff03534a4325d06c6f88c3144831996629b": {"ta_keywords": "recognition visual understanding;object recognition cognition;recognition cognition networks;world visual understanding;object recognition visual;recognition cognition level;visual understanding goes;recognition cognition;recognition visual;vision models struggle;engine recognition cognition;cognition networks r2c;visual understanding;reasoning engine recognition;goes object recognition;reasoning world visual;cognition commonsense reasoning;cognition level understanding;object recognition;movie scenes task;scenes task easy;visual understanding used;image effortlessly imagine;vision models;scenes task;recognition;world visual;cognition networks;state art vision;cognition commonsense", "pdf_keywords": "task visual commonsense;visual commonsense reasoning;crowdsourcing tasks;approach crowdsourcing tasks;model crowdsourcing second;crowdsourcing second person;crowdsourcing second;ilr model crowdsourcing;crowdsourcing tasks work;visual commonsense;model crowdsourcing;present crowdsourcing;crowdsourcing;approach crowdsourcing;commonsense reasoning;challenge recognition;adversarial;adversarial matching;cognitive commonsense;commonsense understanding world;crowdsourcing approach;present crowdsourcing approach;challenge recognition level;commonsense reasoning developed;propose adversarial;vision challenge recognition;propose adversarial matching;inverse reinforcement learning;cognitive commonsense understanding;interesting commonsense"}, "df43f6ff7c66d39240235af3052be55222bef80d": {"ta_keywords": "method speaker clustering;speaker clustering;speaker clustering problem;mixture models;mixture mixture models;nested gibbs sampling;mixture mixture distributions;novel gibbs sampling;observations gibbs sampling;mixture distributions;gibbs sampling;sampling method mixture;gibbs sampling develop;gibbs sampling known;mixture mixture model;gibbs sampling method;sampling develop mixture;mixture distributions particular;mixture model;mixture distributions higher;bayesian hierarchical agglomerative;mixture model represent;mixture model proposed;structures based mixture;variational bayesian hierarchical;based mixture mixture;sampling based variational;based variational bayesian;bayesian hierarchical;gibbs sampling represents", "pdf_keywords": ""}, "964293c1cb1b619eb9b474381d2ba60cf44fcc2d": {"ta_keywords": "facility map mapping;facility map;developing mapping distribution;mapping astronomical data;map mapping astronomical;astronomical data mapping;construction facility map;mapping astronomical;mapping distribution line;data mapping based;data mapping;based mapping database;mapping distribution;distribution line maps;mapping based mapping;mapping database;mapping based;mapping database mapping;database mapping paper;developing mapping;map mapping;distribution facilities;database mapping;managing distribution facilities;maps;map;line maps;jot developing mapping;distribution facilities japanese;based mapping", "pdf_keywords": ""}, "d389d8c2e15f9e9269c17fe6f960f70559eee840": {"ta_keywords": "embeddings infrequent words;hierarchically segment words;meaningful subword structures;word embedding long;word embedding;word embeddings;morphemes level;morphemes level hierarchy;word embeddings consistently;specialized corpora words;subword structures;morphemes;morph segments words;semantically meaningful subword;approach word embedding;corpora words;word level analysis;demonstrate word embeddings;morphemes languages;subword structures paper;corpora words composed;human verified morphemes;words word level;meaningful subword;morphemes languages specialized;segment words;word level;tailed text corpora;verified morphemes languages;text corpora weak", "pdf_keywords": "semantically meaningful morphemes;morphemes semantically meaningful;morphemes semantically;morphemes multiple granularity;meaningful morphemes;quality morphemes semantically;segmenting words morphemes;morphologically meaningful morphemes;words morphemes;semantically meaningful subword;meaningful morphemes multiple;high quality morphemes;morphemes enrich word;meaningful subword structures;morphemes;quality morphemes;morpheme based;meaningful morphemes experimentally;morpheme based segmentation;semantically meaningful words;words morphologically meaningful;neural machine translation;word embeddings;morphmine morphemes enrich;morphemes enrich;word embedding;utilizing morphmine morphemes;enrich word embeddings;morphmine morphemes;morpheme"}, "3d1318bc66d534eefac7c665fd7cc891fba27b87": {"ta_keywords": "dynamics state quantum;state quantum behavior;behavior single quantum;quantum behavior single;state quantum;quantum behavior;single quantum;quantum;dynamics state;relationship dynamics state;dynamics;state;behavior single;relationship dynamics;investigate relationship dynamics;behavior;single;relationship;investigate relationship;paper investigate relationship;paper;paper investigate;investigate", "pdf_keywords": ""}, "33972d9e9a102f9388e5850d8aed3d1aefc9d2e5": {"ta_keywords": "argumentation quality;argumentation quality context;argumentation argumentation quality;argumentation deals;argumentation deals fallacies;fallacies everyday argumentation;everyday argumentation argotario;argumentative discourse fallacious;argumentation argumentation;critical thinking argumentation;simple argumentation deals;argumentation;everyday argumentation;argumentative discourse;argumentation argotario;argumentation present;thinking argumentation;quality context argumentative;discourse fallacious arguments;thinking argumentation present;annotation games;annotation games methodology;context argumentative discourse;argumentation argotario multilingual;simple argumentation;argumentation present simple;argumentative;fallacious arguments deceptive;context argumentative;relationship argumentation argumentation", "pdf_keywords": "fallacies argumentation argotario;argumentation argotario;argotario game based;argotario net argumentation;prototypical argument schemes;argumentation argotario multilingual;argotario online game;argumentation theories;argumentation theories critical;net argumentation theories;approach fallacies argumentation;fallacies argumentation;modules argotario game;argument schemes;fallacious argumentation;net argumentation;fallacies argumentation paper;argumentation paper;arguments notion fallacies;argumentation paper present;argotario game;types fallacies argumentation;argumentation;arguments notion;fallacies prototypical argument;dataset fallacious argumentation;gaming support argotario;created argotario online;argotario online;argotario net tutorial"}, "04138da3bac26f83a9d57152118d4cd5cc8c717d": {"ta_keywords": "inter entity similarity;entity similarity;similarity measure entity;graph based similarity;entity correlation graphs;walk based similarity;entity similarity context;entity similarity domain;similarity domain personal;based similarity measure;similarity measure;walk based search;entity correlation;similarity domain;similarity context inter;search inter entity;graph walk based;measure entity correlation;based similarity;similarity;person disambiguation addressed;threading person disambiguation;graph schema nodes;similarity measure evaluated;similarity measure unbelievable;similarity context;links propose graph;person disambiguation;adaptive graph walk;graph schema", "pdf_keywords": ""}, "395aae6e7a79e5760457ca38e868acc970016230": {"ta_keywords": "table reasoning datasets;tables ubiquitous web;tables present architecture;tables ubiquitous;structure web tables;web tables;table reasoning;state art table;tables;reasoning datasets;web tables present;reasoning datasets mates;result tables ubiquitous;art table reasoning;table;tabular;tabular data sets;tables present;columns table;bias tabular;structure web;bias tabular data;tabular data;web rich information;inductive bias tabular;art table;ubiquitous web rich;columns columns table;datasets mates;model structure web", "pdf_keywords": "question answering entailment;answering entailment tasks;question answering;benchmark question answering;focuses question answering;answering entailment;question answering research;answer answering;approach answer answering;entailment task based;deep bidirectional;answering questions;entailment tasks;binary entailment task;idea deep bidirectional;entailment tasks tables;entailment task;answer answering questions;answering questions computer;answering;answering research;natural questions benchmark;binary entailment;questions computer vision;deep bidirectional transformers;entailment;domain qa pipelines;questions benchmark;questions;method binary entailment"}, "e07c2e66dab7b61091bb8a4ad132bf279c233027": {"ta_keywords": "citations topic modeling;latent topic models;modeling text citations;topic modeling;topic models;topic models text;topic modeling framework;link prediction model;link prediction;models text citations;link prediction task;joint latent topic;text citations topic;text citations;citations topic;citation link pair;text citations proposed;latent topic;citations;joint modeling text;citations proposed literature;citation link;models joint latent;citations proposed;sets link prediction;modeling text;link pair documents;modeling arbitrary link;documents model combines;art link prediction", "pdf_keywords": ""}, "aeb4478619461ca25592e6d692f3591ec8c4091b": {"ta_keywords": "study semantic collisions;generate semantic collisions;semantic collisions texts;semantic collisions;semantic collisions study;collisions texts semantically;semantic collisions evade;collisions study semantic;collisions texts;vulnerable semantic collisions;texts semantically unrelated;similar nlp models;including paraphrase identification;similar nlp;colliding particles;nlp models;learning generate semantic;generate semantic;paraphrase identification;analyzing meaning similarity;summarization vulnerable semantic;colliding particles demonstrate;texts semantically;dynamics colliding particles;judged similar nlp;collisions evade perplexity;including paraphrase;similarity texts;nlp models paper;collisions", "pdf_keywords": "summarization vulnerable semantic;models paraphrase identi\ufb01cation;extractive summarization vulnerable;attacking analyzing nlp;texts including paraphrase;paraphrase identi\ufb01cation document;document sentence retrieval;summarization vulnerable;including paraphrase identi\ufb01cation;generate semantic collisions;models paraphrase;analyzing nlp text;summarization machine learning;vulnerabilities nlp models;nlp models;sentence retrieval;analyzing meaning similarity;including paraphrase;nlp text summarization;translation generate semantic;nlp models analyzing;vulnerabilities nlp;text summarization machine;paraphrase identi\ufb01cation;class vulnerabilities nlp;sentence retrieval extractive;machine translation generate;analyzing nlp;vulnerable semantic collisions;models machine translation"}, "3a3fb140890dbba93290e358af700f9a5c8bcc7a": {"ta_keywords": "tasks synthetic text;learning natural language;size complexity answering;complexity answering;complexity answering question;knowledge intensive tasks;deep neural;learning natural;natural language;algorithm learning natural;natural language processing;domain questioning answering;complexity independent text;answer question long;answering question;knowledge intensive;limited knowledge intensive;questioning answering;answering question linear;synthetic text;symbolic meaning representations;answering;neural;text despite deep;answering qa;neural networks having;text size complexity;natural language paper;tasks synthetic;questioning answering qa", "pdf_keywords": ""}, "2f780a18d44f4e3c5c4c74d4060b8dfd542a778d": {"ta_keywords": "sports narratives;sports narratives build;impact sports narratives;commentator bias sports;bias sports broadcasts;team player narratives;bias sports;player narratives;player narratives paper;commentator bias;sports broadcasts;evince commentator bias;examine impact sports;offensive performance statistically;sports;player offensive performance;narratives build team;judge player offensive;player offensive;impact sports;narratives paper;narratives;narratives paper present;bias;narratives build;commentator;broadcasts;theatrics evince commentator;offensive performance;evince commentator", "pdf_keywords": "racial bias sports;bias studies sports;racial bias football;analysis racial bias;bias sports commentary;racial bias studies;bias sports;examining racial bias;racial bias;results racial bias;studies sports broadcasting;bias mass media;bias football;white bias;ones white bias;researchers examining racial;bias studies;analysis racial;bias football perform;examining racial;inquiry white players;race effects;bias;sports broadcasting;white players;statistical effects social;black players nonwhite;isolate race effects;studies sports;longitudinal analysis racial"}, "34fc6da7a88433478fd976fd0b9de3cf7134e652": {"ta_keywords": "automated safety multimodal;safety multimodal information;safety multimodal;performance automated safety;automated safety;multimodal information;multimodal;automated;safety;performance automated;study performance automated;report results systematic;results systematic study;systematic study;systematic;systematic study performance;information;results systematic;performance;report results;study performance;report;results;study", "pdf_keywords": ""}, "df1d89f4ca9c20e2c6703cdbf26a62f2b50ac71c": {"ta_keywords": "dynamics stackelberg games;stackelberg games continuous;guaranteed stackelberg equilibrium;stackelberg equilibria;learning dynamics stackelberg;stackelberg games;stackelberg equilibria zero;stackelberg equilibrium;games dynamics converge;sum games dynamics;stackelberg equilibrium zero;learning dynamics stable;task stackelberg equilibria;zero sum games;sum games equilibrium;learning dynamics;games investigate convergence;games dynamics;stackelberg gradient dynamics;dynamics stackelberg;timescale learning dynamics;games equilibrium concepts;hierarchical order play;dynamics stable equilibria;sum games investigate;point guaranteed stackelberg;games equilibrium;games continuous action;sum games;general sum games", "pdf_keywords": ""}, "02980e5ba847282b683a85e7a8862c6c1b6e0d94": {"ta_keywords": "atom interacting harmonic;level atom interacting;atom interacting;interacting harmonic oscillator;model level atom;interacting harmonic;harmonic oscillator;numerical study dynamics;level atom;dynamics simple model;dynamics simple;oscillator;dynamics;atom;study dynamics simple;numerical study;simple model level;harmonic;interacting;model level;results numerical study;study dynamics;simple model;numerical;present results numerical;level;results numerical;model;simple;present", "pdf_keywords": ""}, "73aa33fd469b171d50c452c5e3fe0e9e03520520": {"ta_keywords": "ray binary decoding;xmath0 ray binary;binary decoding stages;correlation function xmath0;decoding stages systems;ray binary;binary decoding;function xmath0 ray;xmath0 ray;decoding stages;decoding;xmath0;collective motion;function xmath0;collective dynamics;generalization collective motion;dynamics generalization collective;collective dynamics generalization;point correlation function;binary;correlation function;point correlation;analysis point correlation;concept collective dynamics;correlation;systems;dynamics generalization;systems second;systems second stage;combination stage outputs", "pdf_keywords": ""}, "c330ec2047d019d98233abb59d13b3256c662cc7": {"ta_keywords": "counterfactual distributions arbitrary;counterfactual distributions;categorical counterfactual distributions;counterfactual probabilities arbitrary;arbitrary structural causal;inferring counterfactual queries;counterfactual queries;counterfactual probabilities;support arbitrary causal;unknown counterfactual probabilities;discrete structural causal;sciences counterfactual distributions;counterfactual queries combination;counterfactual distributions finite;inferring counterfactual;structural causal models;causal diagram unobserved;causal models;arbitrary causal;categorical counterfactual;structural causal;problem inferring counterfactual;represent categorical counterfactual;causal models represent;social sciences counterfactual;unknown counterfactual;structural causal model;arbitrary causal diagram;bound unknown counterfactual;counterfactual", "pdf_keywords": "representing causal counterfactuals;causal counterfactuals based;counterfactual distribution arbitrary;counterfactual distributions;causal counterfactuals;categorical counterfactual distributions;probabilities causation causal;causal diagram unobserved;counterfactual distributions \ufb01nite;support arbitrary causal;discrete structural causal;bounding counterfactual probabilities;counterfactual distributions computer;counterfactual distribution;arbitrary structural causal;causal diagram counterfactual;causation causal diagrams;causal models;diagram counterfactual distribution;counterfactual probabilities;causal models represent;arbitrary causal;representing causal;causal model generated;arbitrary causal diagram;structural causal models;counterfactual probabilities combination;causation causal;causal diagrams;way representing causal"}, "0a3caecce668731efe7abf37720793eed1fb951a": {"ta_keywords": "politically oriented microblog;politically oriented tweets;twitter user political;classifier twitter;accurate classifier twitter;classifier twitter user;tweets accurate classifier;classifier politically oriented;classifier politically;oriented microblog messages;political information;political affiliation sender;oriented microblog;information emotion social;message sentiment associated;oriented tweets;message social media;oriented tweets accurate;microblog messages;receiving political information;message sentiment;microblog messages collected;receiver message sentiment;twitter;political information important;corpus politically oriented;large corpus politically;community information sharing;social community information;sentiment associated", "pdf_keywords": ""}, "5bca90a331417402f5018f552e1a62656dd7fc5b": {"ta_keywords": "protein labeled graph;graph inferred structure;structure community graph;community graph;labeled graph;community graph inferred;labeled graph present;stochastic block;stochastic block model;symmetric stochastic block;structure assume nodes;assume nodes graph;graph inferred;nodes graph;information theoretic;graph;algorithm achieves information;graph generated;achieves information theoretic;graph present;graph generated symmetric;assume nodes;characterize information theoretic;information theoretic limit;model motion rigid;nodes;nodes graph generated;graph present simple;symmetric stochastic;block model", "pdf_keywords": ""}, "5166c0e04d77ac0f7969c49c0f8f18129a114198": {"ta_keywords": "clinical gait analysis;measurement clinical gait;gait analysis;clinical gait;motion measurement clinical;automated motion measurement;motion measurement;gait;automated motion;present automated motion;measurement clinical;motion;measurement;present automated;automated;clinical;analysis;present", "pdf_keywords": ""}, "205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4": {"ta_keywords": "adapting machine translation;massively multilingual models;translation systems new;translation systems;multilingual models;machine translation;multilingual models explicit;machine translation systems;languages lrls effectively;language regularization;similar language regularization;massively multilingual;demonstrate massively multilingual;simple model adaptation;models explicit adaptation;explicit adaptation surprisingly;model adaptation;low resourced languages;model adaptation human;adapting machine;language regularization prevent;languages lrls;multilingual;adapting;resourced languages lrls;explicit adaptation;adaptation surprisingly;adaptation surprisingly effective;similar language;adaptation", "pdf_keywords": "multilingual machine translation;dataset machine translation;machine translation mt;machine translation model;various machine translation;machine translation;unsupervised machine translation;machine translation paradigms;machine translation systems;language regularization training;translation systems;translation model wide;multilingual machine;translation model;language regularization;deployment machine translation;machine translation paper;multilingual training paradigms;similar language regularization;training paradigms multilingual;multilingual training;paradigms multilingual machine;translation paradigms;new language pairs;translation mt technology;translation mt;languages training set;multilingual;language pairs accurately;present multilingual training"}, "488b1849dd81e63aae2cd327564077ae123c0369": {"ta_keywords": "distributed optimization methods;distributed optimization;stochastic gradients iterates;stochastic gradients;setting distributed optimization;vectors stochastic gradients;refrigerator based langevin;method efficiency distillation;efficiency distillation;distillation method communication;distillation refrigerator based;neural networks millions;distillation refrigerator;efficiency distillation process;commercial distillation refrigerator;distillation method;based langevin equation;langevin equation method;gradients iterates prohibitively;optimization methods;performance commercial distillation;distillation;nicating vectors stochastic;distillation process;method communication compression;gradients iterates;langevin equation;distillation process higher;communication compression powerful;training neural networks", "pdf_keywords": "absolute compression operators;compression operators;learning distributed algorithms;distributed learning;learning distributed;distributed learning distributed;class absolute compression;distributed algorithms;compression operators particular;absolute compression;distributed algorithms paper;contains optimal compressors;keywords distributed learning;compression;compression operators showing;compressors ec sgdin;optimal compressors;optimal compressors ec;stochastic reformulation;propose stochastic reformulation;distributed;stochastic reformulation problem;algorithms;propose stochastic;focus absolute compression;paper propose stochastic;algorithms paper;sampling;stochastic;algorithms paper propose"}, "4cd92a56dca741190e453b4229eb9851abf6944c": {"ta_keywords": "clipping stochastic gradients;stochastic gradient descent;convex stochastically optimization;new accelerated stochastic;stochastically optimization heavy;accelerated stochastic;stochastastic gradients;stochastic gradients;stochastic gradient;smooth convex stochastically;noise stochastastic gradients;stochastically optimization;clipping stochastic;convex stochastically;variant stochastic gradient;stochastastic gradients derive;accelerated stochastic order;stochastic gradients prove;std clipping stochastic;distributed noise stochastastic;noise stochastastic;complexity bound sg;stochastastic;special variant stochastic;gradient descent std;variant stochastic;stochastic;stochastically;stochastic order method;high probability complexity", "pdf_keywords": "stochastically weighted gradient;stochastic optimization heavy;stochochastic optimization heavy;optimization stochastic gradients;stochastic convex optimization;convex optimization stochastic;gradients robust stochastic;algorithm weighted stochastically;stochastic gradients;noise stochastic gradients;stochastic optimization;oracle stochastic optimization;weighted stochastically weighted;bounds weighted logistic;stochochastic optimization;stochastic gradients considered;stochastic gradients whittle;stochastically weighted;stochastic composite optimization;weighted stochastically;robust stochastic approximation;optimization stochastic;stochastic intermediate gradient;complexity bounds weighted;keywords deep learning;stochastic convex;deep learning logistic;statistical language models;weighted gradient gradients;problem stochastic convex"}, "40848b41ed8c9c255ecd8a920006877691b52d03": {"ta_keywords": "distribution shifts datasets;wild unifying datasets;benchmark wild distribution;shifts datasets;wild distribution shifts;datasets ml community;unifying datasets;diverse data modalities;diverse data;widely used datasets;datasets ml;community distribution shifts;benchmark wild;wilds benchmark wild;datasets;shifts spanning diverse;spanning diverse data;datasets reflect distribution;wilds benchmark;wildlife monitoring poverty;evaluating distribution shifts;present wilds benchmark;degradation machine learning;appropriate datasets;world distribution shifts;used datasets ml;unifying datasets variety;distribution shifts;manage appropriate datasets;monitoring poverty mapping", "pdf_keywords": "unsupervised domain adaptation;domain adaptation;synthetic transformations imagenet;domain adaptation setting;trained image datasets;new deep learning;transformations imagenet;evaluation deep;models trained image;deep learning;trained image;evaluation deep learning;models trained;datasets shifts induced;deep learning deployed;learning models trained;dataset domain generalization;datasets shifts;image datasets shifts;deep learning framework;image datasets;imagenet;learning models;framework evaluation deep;adaptation setting;transformations imagenet hendrycks;new deep;retinopathy unsupervised domain;clinics detection diabetic;realistic distribution shifts"}, "2e492af839e971d05592df1c76d4878908e1d4c0": {"ta_keywords": "factor graph grammars;inference factor graphs;graph grammars;graph grammars fgpgs;models factor graphs;factor graphs;grammar models factor;factor graphs paper;networks factor graph;grammar models;factor graph;grammatical grammar models;fgpgs grammatical grammar;single factor graph;grammars fgpgs;factor diagrams;case factor diagrams;factor graph amenable;grammars fgpgs grammatical;graphs fgg;statistical inference factor;product networks factor;inference techniques fggs;inference factor;graphical models case;networks factor;sets graphs fgg;factor diagrams sum;graphs fgg converted;fgpgs grammatical", "pdf_keywords": "factor graph grammars;grammars factor graphs;factor graph grammar;graph grammars factor;graph grammars;graph grammar;factor graphs expressive;graph grammar particular;context free parsing;sentence factor graph;automata factor graph;factor graph tree;tree automata factor;learning factor graphs;models factor graphs;free parsing;graphs expressive;free parsing algorithm;factor graphs;probabilistic context free;free grammar factors;tree automata;factor graphs general;represent bayesian networks;probabilistic graphical models;parsing;instance factor graph;graphs expressive solve;parsing algorithm;context free grammar"}, "c66b59394f99d639b277a54ad357d20de30285bd": {"ta_keywords": "annotated text images;text mining image;active learning;paper present classification;mining image;latent topic modeling;text images;information figures biomedical;topic modeling provide;text images information;topic modeling;learning select images;biomedical literature;annotated text;free annotated text;figures biomedical literature;classification;biomedical literature paper;active learning select;figures biomedical;used active learning;present classification;classification scheme subcellular;text mining;mining image processing;information free annotated;classify panels image;classify;extended classify panels;document multi modality", "pdf_keywords": ""}, "8fc728b71f9e92f91455f957f10c7e496cbe4772": {"ta_keywords": "entity typing language;enhancing entity typing;entity typing;typing language model;entity typing model;propose entity typing;language model enhancement;electron gas label;semantic types entity;labels entity typing;entity mention;model information language;semantic types;semantic;language model significantly;information language model;language model;types entity mention;electron gas confined;entity typing aims;context sentences labels;classify semantic;entity mention specific;typing model information;information language;classify semantic types;utilizes language model;entity;electron gas;electron", "pdf_keywords": ""}, "208e5c187e81f63024ece8e2003dbaef094703cb": {"ta_keywords": "disordered solids mechanical;properties disordered solids;mechanical properties disordered;disordered solids;solids mechanical response;electric field mechanical;mechanical response solids;field mechanical properties;solids mechanical;properties disordered;solids sensitive;solids sensitive applied;mechanical properties;response solids sensitive;response solids;applied electric field;sensitive applied electric;solids;field mechanical;effect electric field;electric field;investigate effect electric;applied electric;effect electric;disordered;mechanical response;mechanical;electric;properties;field", "pdf_keywords": ""}, "7dadf1e4f6f7a6966d5f691c3707fe221038528b": {"ta_keywords": "utilitarianmaximal allocations;problems utilitarianmaximal allocations;computational problems utilitarianmaximal;computing allocations fair;fair allocations compute;utilitarianmaximal allocations decide;allocations fair maximize;fair maximize utilitarian;fair allocations;allocations fair;prop1 fair allocations;tractable fairness concepts;utilitarianmaximal;maximizes utilitarian;maximize utilitarian;problems utilitarianmaximal;allocations;tractable fairness;computing allocations;maximizes utilitarian welfare;fairness concepts envy;maximize utilitarian social;compute maximizes utilitarian;allocations decide exists;allocations decide;envy freeness proportionality;complexity computing allocations;allocations compute maximizes;focus tractable fairness;utilitarian welfare", "pdf_keywords": "um fair allocations;assigning indivisible goods;fair allocations;um fair allocation;fair allocations decision;fair allocation;allocation um fair;fair allocation np;deciding allocation exists;allocation np hard;indivisible goods;allocations decision;allocations decision problem;allocations;deciding allocation;indivisible goods strategic;allocation exists;utilities agents fairness;allocation;assigning indivisible;problem deciding allocation;balancing welfare allocation;agents fairness;allocation exists simultaneously;allocation np;fairness agents disproportionately;allocation um;allocation sum utilities;goods strategic agents;agents fairness agents"}, "346081161bdc8f18e2a4c4af7f51d35452b5cb01": {"ta_keywords": "questions crowdsourcing;creative questions crowdsourcing;strategy crowdsourcing;using strategy crowdsourcing;process crowdsourcing;strategyq question answering;questions crowdsourcing workers;strategy crowdsourcing crowdsourcing;process crowdsourcing workers;crowdsourcing;crowdsourcing process;crowdsourcing workers;reasoning steps answering;crowdsourcing process crowdsourcing;question answering;question answering qa;crowdsourcing workers collaborate;crowdsourcing crowdsourcing;crowdsourcing crowdsourcing process;steps answering wikipedia;question reasoning steps;steps answering question;crowdsourcing workers covering;contain answers step;answering qa benchmark;contains answers step;steps answering paragraph;steps answering;elicit creative questions;answering qa", "pdf_keywords": "difficult questionnaires crowdsourcing;questionnaires crowdsourcing;questionnaires crowdsourcing work;eliciting creative questions;crowdsourcing1 pipeline;step crowdsourcing1;multi step crowdsourcing1;answer creative questions;crowdsourcing1 pipeline designed;step crowdsourcing1 pipeline;crowdsourcing1;annotation pipeline eliciting;crowdsourcing;crowdsourcing work;automatically answering difficult;automatically answering;creative questions introduce;novel annotation pipeline;answering difficult questionnaires;questions train;crowdsourcing work conduct;answer strategy;answer strategy questions;annotation pipeline;question decompositions inspire;lms answer strategy;creativity ask write;creative questions use;method automatically answering;creativity ask"}, "76df9c90d5359585f5501a4da1af1078c32be6d7": {"ta_keywords": "open source ocr;transcribe images text;ocr present probabilistic;accurately transcribe images;transcribing images documents;transcribe images;improvement tesseract google;source ocr;tesseract google;ocr;transcription historical documents;accurately transcribe;unsupervised transcription historical;era unsupervised transcription;unsupervised transcription;improvement tesseract;source ocr present;images text;structure accurately transcribe;modeling text document;transcribing images;relative improvement tesseract;ocr present;transcribe;historical transcription;tesseract google open;tesseract;images documents printing;able decipher font;modeling text", "pdf_keywords": "words glyphs documents;effectively learns font;learns font structure;glyphs documents;learns font;generative model text;text typesetting based;glyphs documents present;recognition words glyphs;text typesetting;generative language model;words glyphs;language model print;generative language;documents present generative;model text typesetting;glyphs;text;machine translation automatic;machine translation;based historical typesetting;translation automatic recognition;model text;documents text;automatic recognition words;typesetting based;translation automatic;generative model;documents text paper;language model typesetting"}, "88c3f221a6fc8aff014268b0efb5ff119ab40906": {"ta_keywords": "dataset irony detection;irony detection;structures irony detection;irony detection important;dataset irony;balanced dataset irony;emojis social media;cues emojis social;verbal cues emojis;cues emojis;structures irony;emojis social;role structures irony;emojis;irony;identification online abuse;online abuse harassment;predicting;online abuse;predicting outcome;predicting outcome competition;social media work;method predicting outcome;social media;method predicting;competition based predictive;new method predicting;propose automated;balanced dataset;predictive", "pdf_keywords": ""}, "ce2d6de9cec4a6d135c32bb8d2d02bba09928b33": {"ta_keywords": "text categorization;text categorization problems;methods text categorization;large text categorization;text categorization presented;sleeping experts phrases;ripper sleeping experts;categorization problems classifiers;categorization problems generally;categorization presented;categorization;categorization problems;classifiers;learning algorithms ripper;classifiers represent contextual;classifying;sleeping experts;problems classifiers;natural language processing;algorithms ripper sleeping;ripper sleeping;context sensitive learning;experts phrases evaluated;experts phrases;sleeping experts perform;differences ripper sleeping;natural language;context natural language;learning methods text;method classifying", "pdf_keywords": ""}, "393c5c96e73dd3a82c175f9ab1f6c083830d3b82": {"ta_keywords": "financial risk management;stock portfolio simulator;analysis financial risk;portfolio simulator backtester;clairvoyantly select stocks;neural networks forecast;portfolios far outperform;predict annual return;risk management strategy;forecast future fundamentals;portfolios far;portfolio simulator;stock portfolio;financial risk;stocks using factors;forecast future;risk management;oracle portfolios far;annual return company;factors calculated future;analysis financial;forecast;portfolio;portfolios;calculated future fundamentals;used predict annual;predict outcome;insight financial health;stocks;predict", "pdf_keywords": "stock market prediction;neural networks forecast;future sample stock;stock portfolio simulator;forecast future fundamentals;predict future;forecast future;portfolio simulator backtester;predict future sample;market prediction;model predict future;portfolios far outperform;sample stock performance;stock performance;clairvoyantly select stocks;stock portfolio;portfolio simulator;portfolios far;forecast;train deep neural;predict;stock performance paper;market prediction based;prediction;stocks;recurrent convolutional neural;approach automated stock;stock market;future sample;deep neural"}, "e929c9b53c66d52ae5ea56f0dc2764aef4cc67f6": {"ta_keywords": "channel speech separation;speech separation;separation techniques deep;speech separation shown;single channel speech;datasets mixer chime;training deep;datasets mixer;separation shown outperform;conditions separation techniques;separation techniques;mismatched conditions separation;mixer chime corpora;training deep learning;channel speech;mixer chime;multi condition training;construct datasets mixer;deep learning;conditions separation;separation;synthetic overlap data;techniques deep learning;condition training far;deep learning based;studio interviews dinner;mixer;deep learning models;techniques deep;learning based single", "pdf_keywords": ""}, "be28821d510a99ffce40cdcf6860302def8533ef": {"ta_keywords": "recommendation systems strategic;mediators induce game;strategic content providers;content recommended users;possible recommendation systems;content providers strategic;welfare propose mediator;recommendation systems;strategic content;social welfare equilibrium;recommendation systems extremely;users closest content;optimize offered content;mediator implements socially;offered content recommended;socially optimal strategy;mediator cause social;principle matching users;implements socially optimal;welfare equilibrium intervene;designing mediators;welfare equilibrium profile;socially optimal;closest content users;welfare equilibrium;users contents propose;propose mediator;location games;content providers;induce game", "pdf_keywords": "recommendation systems strategic;mediator present game;strategic content providers;recommendation systems social;facility location games;socially optimal locations;socially optimal location;cost equilibrium mediator;games user space;optimal location equilibrium;strategic content;player game equivalent;equilibrium mediator;unique socially optimal;location games;content providers recommendation;social cost equilibrium;socially optimal;allows players choose;optimal locations intervenes;content providers mediation;players choose socially;choose socially optimal;location games study;mediator designed offer;recommendation systems;strategic behavior players;equilibrium mediator designed;providers recommendation systems;structure player game"}, "3d2dece28f566792b6dd3a190aa345fc30fee1ff": {"ta_keywords": "urban air mobility;ground transportation networks;transportation networks;equilibrium constraints urban;constraints urban air;air ground transportation;aircraft aggravate congestion;study transportation networks;transportation networks nodes;air mobility concept;congestion ground vehicles;air mobility;ground travel demands;program bilinear equilibrium;capacity vertiports travelers;flight;aerial motion urban;ground transportation;transportation networks report;bus introduce stochastic;ground travel;linear program;aircraft affect flight;aerodynamical climatic variables;urban air;transportation;integer linear program;constraints urban;flight delays;aerodynamical climatic", "pdf_keywords": "ground transportation networks;transportation network design;ground transportation network;transportation networks designed;transportation network construct;air transportation network;transportation network model;transportation network ground;network ground transportation;transportation networks;transportation network;new transportation network;construct air transportation;air ground transportation;optimal network;transportation network allows;program bilinear equilibrium;optimal network design;global optimal network;network construct air;optimization problem bilinear;existing ground transportation;equilibria nonconvex optimization;transportation network provide;used transportation network;bilinear equilibrium constraints;ground transportation;problem bilinear constraints;mathematical program equilibria;nonconvex optimization problem"}, "eadd73c3e1c20d16e32ee8656c4f954603b37450": {"ta_keywords": "auditory scenes;audiovisual dataset;np hard cnn;complex auditory scenes;auditory scenes stereo;clarinetist videos;audiovisual;cnn model highly;hard cnn;acoustic events visual;neural networks visual;recall acoustic events;precision recall acoustic;convolutional neural;experiments convolutional neural;hours clarinetist videos;preliminary experiments convolutional;recall acoustic;world clarinetist videos;release audiovisual dataset;experiments convolutional;cnn;cnn model;audio;hard cnn model;real world clarinetist;convolutional neural network;audiovisual dataset hours;scenes stereo;clarinetist videos carry", "pdf_keywords": "visual onset detection;detecting onsets audio;onset detection realistic;onset detection;onsets audio recordings;clarinetist videos;detecting onsets;world clarinetist videos;onset detection problem;onsets audio;solve onset detection;clarinetist videos carry;culty detecting onsets;acoustic timed events;detect acoustic timed;recordings large symphonic;visual information clarinets;audio recordings;real world clarinetist;timed events video;acoustic timed;visual onset;recordings large;recordings;clarinetist;clarinets;audio recordings large;world clarinetist;propose novel convolutional;step visual onset"}, "a4e937f0b6e0688f7f3c4fcaebbabefa4a36da85": {"ta_keywords": "models synthesize utterances;text speech e2;text speech;synthesize utterances;speech e2 tts;synthesize utterances comparable;utterances comparable ground;end text speech;utterances;speech e2;state art tts;e2 tts toolkit;utterances comparable;pre trained models;tts toolkit experimental;tts models;tts toolkit;trained models unified;tts models extensions;models unified python;2d ising model;2d ising;textto waveform modeling;training neural vocoders;evaluations english japanese;tts performance toolkit;neural vocoders;ising model;espnet tts;trained models", "pdf_keywords": "automatic speech prediction;speech prediction;speaker adaptation;speech prediction prediction;automatic speech;multi speaker adaptation;speaker adaptation introduce;framework automatic speech;speech synthesis;parametric speech synthesis;speaker embeddings pre;statistical parametric speech;speech synthesis sps;text speech;text speech e2e;speech e2e tts;speaker embeddings;end text speech;hot speaker embeddings;parametric speech;models hmms deep;espnet2 tts toolkit;embeddings pre trained;speech e2e;new machine learning;models hmms;pre trained models;hmms deep neural;pre trained vectors;machine learning toolkit"}, "2135c44087e06a6d95d04ad0afa400e926d37944": {"ta_keywords": "feature spaces adaptation;continuous speech recognition;speech recognition;adaptation data;propose bayesian estimation;corpus spontaneous japanese;especially adaptation data;bayesian estimation;vocabulary continuous speech;adaptation data small;normalization feature spaces;large vocabulary continuous;bayesian estimation approach;noisy data set;spaces adaptation model;applied normalization feature;speech recognition using;outperformed conventional adaptation;new approach statistical;corpus;using corpus spontaneous;spaces adaptation;noisy data;normalization feature;continuous speech;consistently applied normalization;estimating mean squared;japanese csj task;experiments large vocabulary;small propose bayesian", "pdf_keywords": ""}, "99c80d608ba2aa638333f27bbe3f09cdc580a051": {"ta_keywords": "simulation speech recognition;learning applications audio;applications audio speech;audio speech processing;approach simulation speech;simulation speech;recognition speech;speech recognition speech;audio speech;speech recognition;speech recognition systems;recognition speech recognition;speech processing domain;speech processing;applications audio;audio processing building;audio processing;learning applications;torch audio easily;audio;machine learning applications;torch audio;building block audio;audio easily;audio easily installed;fig torch audio;blocks machine learning;machine learning;block audio processing;recognition systems goal", "pdf_keywords": "audio applications pytorch;torchaudio toolkit provides;learning applications audio;torchaudio toolkit;audio speech operations;established torchaudio toolkit;applications audio speech;audio extraction;output audio extraction;audio extraction accelerate;torchaudio various applications;audio related machine;audio applications;implementation audio speech;ecosystem torch audio;development audio applications;torch audio;audio speech;speech domain audio;projects audio speech;applications audio;domain audio speech;new projects audio;including speech recognition;audio speech domain;torchaudio various;speech recognition;torchaudio;framework audio;implementation audio"}, "29bc6654abd34b2405f7a01341f790aed2aab9a4": {"ta_keywords": "distributed storage codes;efficient repair parity;repair parity nodes;xmath1 heisenberg antiferromagnet;storage codes efficient;parity nodes;heisenberg antiferromagnet;heisenberg antiferromagnet present;node repair;node repair typical;distributed storage;repair parity;storage codes;antiferromagnet;designing distributed storage;parity nodes existing;antiferromagnet present;antiferromagnet present new;nodes existing codes;xmath0 xmath1 heisenberg;codes efficient data;xmath1 heisenberg;efficient data read;downloaded node repair;repair systematic nodes;simulation xmath0 xmath1;code numerical simulation;numerical simulation xmath0;simulation xmath0;smallest data read", "pdf_keywords": ""}, "b35ad59ce9a3ea01a0980c90bc750273d1f99e7a": {"ta_keywords": "web queries based;queries based textual;based textual similarity;similarity local names;web queries;information retrieval;statistical information retrieval;textual similarity;information retrieval real;search web pages;approach search web;search web;pages web queries;real world databases;queries based;similarity;world databases;explicitly similarity;normalization assume names;retrieval real world;similarity local;world databases contain;databases;databases contain hundreds;retrieval;databases contain;explicitly similarity local;queries;database integration;web pages web", "pdf_keywords": ""}, "04e0fb8b3bb06e1200288e6d2a17d55773e97504": {"ta_keywords": "mobile users fairness;fair bandwidth sharing;fair bandwidth;mobile users optimal;algorithms optimal policy;users cellular network;stochastic approximation mobile;mobile users cellular;reward markov decision;bandwidth sharing policies;average reward markov;convergence bandwidth sharing;users optimal policy;approximation mobile users;reward markov;issue fair bandwidth;bandwidth sharing;cellular network propose;cellular network;users cellular;propose bandwidth sharing;propose bandwidth;bandwidth sharing classes;mobile users fast;bandwidth sharing static;sharing policies learning;rate highly mobile;optimal policy;bandwidth;policies learning algorithms", "pdf_keywords": "algorithms cellular networks;cellular networks;performance cellular networks;cellular network;users cellular network;mobile users cellular;bandwidth allocation mobility;small cell networks;cellular networks presented;cell networks;cell networks femtocells;decision algorithms cellular;users cellular;opportunistic dynamic bandwidth;performance cellular;algorithms cellular;improve performance cellular;algorithms used macrocells;data rate mobile;cellular networks alleviate;cellular;allocation mobility;networks femtocells;dynamic bandwidth sharing;cellular network paper;bandwidth allocation;dynamic bandwidth;communicate decide bandwidth;decide bandwidth allocation;bandwidth sharing"}, "016d83091a60a6de67ba2395c063967686043380": {"ta_keywords": "gaussian neural networks;gaussian neural;structure gaussian neural;prediction structure gaussian;isolated word recognition;word recognition;supervised adaptation;structure gaussian;supervised adaptation experiment;neural networks;line supervised adaptation;adaptation data;word recognition advantage;gaussian;bayesian method prediction;paper propose bayesian;supervised;prediction structure;line supervised;recognition;propose bayesian method;neural;bayesian;neural networks conduct;bayesian method;adaptation;propose bayesian;recognition advantage proposed;prediction;adaptation experiment isolated", "pdf_keywords": ""}, "06708348b64e2e7b11a953389556c701bf3298da": {"ta_keywords": "estimating size obstacle;size obstacle obstacle;obstacle determined size;length obstacle determined;size obstacle;length obstacle;obstacle obstacle free;obstacle free environment;obstacle determined;idea length obstacle;method estimating size;estimating size;obstacle free;obstacle obstacle;obstacle;determined size shape;length;size shape;method estimating;estimating;determined size;new method estimating;idea length;based idea length;free environment method;size;shape;free environment;environment method based;environment method", "pdf_keywords": ""}, "0098123efc851b67137c1028f7bac8d8bffbc8fd": {"ta_keywords": "semantic parsing models;parsing models;downstream semantic parsing;propose parser based;propose parser;semantic parsing;parsing models planched;paper propose parser;parser;syntactic structures encode;parser based;syntactic structures;parsing;language models plmrs;modeling language;parsers_;parser based approach;language models;syntactic structure;modeling language present;syntactic;given syntactic structure;pretrained language models;plm family syntactic;latent grounding based;downstream semantic;given syntactic;structure given syntactic;approach latent grounding;syntactic structure paper", "pdf_keywords": "semantic parsing task;semantic parsing;person concept representation;supervision concept prediction;grounding person concept;important semantic parsing;instead grounding supervision;concept representation;grounding module representations;grounding supervision;concept representations single;concept representations;concept representation set;concept prediction easily;parsing task;grounding person;grounding supervision paper;modal scenarios grounding;natural language;parsing task translating;propose weakly supervised;predictive grounding module;model grounding person;predictive grounding;latent grounding;semantic processing;parsing;semantic processing propose;concept prediction;set concept representations"}, "45f59bd3ef8e1d76474199c08c140675c04a728c": {"ta_keywords": "multi agent learning;agent learning agents;learning agents;agent learning rules;learning agents anticipate;equilibrium generalization;agent learning;implicit conjecture learning;polynomial game gradient;conjectures learning processes;heterogeneous multi agent;forming conjectures learning;conjectures learning;conjecture learning;variations equilibrium generalization;learning leads equitable;multi agent;learning rules;game gradient;conjecture learning leads;equilibrium generalization standard;polynomial game;learning processes devise;processes devise learning;conjecture learning decrease;learning rules introduce;devise learning rules;generalization standard equilibrium;devise learning;new equilibrium concept", "pdf_keywords": ""}, "a13c580250af3644fe368b08a540f4ea65dac919": {"ta_keywords": "complexity phasecode compressive;compressive phase retrieval;phasecode compressive;phasecode compressive phase;compressive phase;measurements compressive phase;sample complexity phasecode;sparse graph coding;complexity phasecode;phasecode algorithm recover;recovers sample complexity;phase retrieval algorithm;compressive;phase retrieval;recovering signal exactly;number measurements compressive;noiseless case phasecode;phase retrieval problem;algorithm recover arbitrarily;algorithms based sparse;measurements compressive;phasecode algorithm;recovering signal;zero sample complexity;algorithm provably recover;log introduce phasecode;log computational complexity;introduce phasecode;sample complexity log;phasecode", "pdf_keywords": ""}, "0b2e9e978898b9fb1116ea964c8c470086ceed87": {"ta_keywords": "salient object detection;feature fusion;level feature fusion;explore depth cues;backbone feature fusion;feature fusion introduce;multi modal cues;feature fusion fundamental;depth cues channel;multi level features;depth cues;object detection;rgb salient object;level features teacher;object detection devise;rgb salient;student features;depth enhanced module;multi modal learning;multi level feature;topic computer vision;modal learning;dem explore depth;teacher student features;features teacher;modal cues;fusion introduce depth;features meet multi;depth enhanced;vision provide comprehensive", "pdf_keywords": "saliency models;saliency models propose;performance saliency models;recognizing salient objects;model recognizing salient;saliency;compute saliency;methods compute saliency;saliency local;recognizing salient;saliency local region;compute saliency local;performance saliency;salient objects color;decreasing performance saliency;learning contrast depth;salient objects;cues depth features;salient;cnn;network cnn;rgb datasets provide;attention;cnn model;rgb datasets;cnn model recognizing;propose deep convolutional;depth features;learning contrast;deep convolutional"}, "0b8dbc4a899c836fe2b1a213b9dc064cdf62fd63": {"ta_keywords": "quarty reasoning powerful;modeling explanation task;explanations paragraphs modeling;reasoning complex systems;quarty reasoning;text good explanations;constructs explanations paragraphs;reasoning powerful technique;task multitask learning;reasoning complex;constructs explanations;task understanding causal;explanation task;multitask learning;explanations paragraphs;reasoning powerful;prediction task understanding;explanation task multitask;powerful technique reasoning;prediction task;quarty;original prediction task;task understanding;technique reasoning complex;multitask learning problem;perturbations procedural text;explanations;learning problem paper;multitask;task multitask", "pdf_keywords": "synthesize explanations sentences;natural language explanations;learns predict reason;explanations sentences input;explanations sentences;able synthesize explanations;language explanations qualitative;synthesize explanations;text understanding;language explanations;improve explainable reasoning;graphs visual attention;procedural text understanding;attention;enriches natural language;answer text;learns predict;natural language;learning learns predict;answer answer text;web improve explainable;explanations qualitative structure;learning learns;explainable reasoning paper;goal procedural text;visual attention;explanations qualitative;explainable reasoning;attention work;machine learning learns"}, "f07c5c540233b22f0ca154c80c713e2aed3c9606": {"ta_keywords": "model discriminator gan;discriminative metric music;good discriminator gan;discriminator gan;music generated;generation transformer gans;metric music generated;minute long compositions;music generated approach;symbolic music generation;transformer gans;music generation transformer;music generation;algorithms symbolic music;music generation goal;transformer gans makes;approach music generation;discriminator gan shown;gans makes discrete;transformer machine learning;long compositions;gans;sampling algorithm sequences;music transformer machine;metric music;gans makes;synthesizing minute long;art music transformer;sequence autoregressive manner;music transformer", "pdf_keywords": ""}, "d15a7d00897f58a94def2a58c0cb0311851f2968": {"ta_keywords": "different speech enhancement;speech enhancement measures;speech enhancement;speech enhancement studies;distant speech recognition;platform speech enhancement;evaluation speech quality;speech quality;speech recognition;speech distortion ratio;speech quality pesq;speech recognition propose;distant speech;speech distortion;perceptual evaluation speech;experimental platform speech;lstm masking improve;quality pesq speech;time delay neural;art distant speech;evaluation speech;platform speech;pesq speech distortion;memory lstm masking;microphones plus enhanced;lstm masking;delay neural;delay neural network;short term memory;includes different speech", "pdf_keywords": "multichannel speech recognition;channel speech recognition;asr speech enhancement;multi channel speech;multichannel speech;model multichannel speech;speech enhancement measures;different speech enhancement;speech enhancement;speech enhancement separation;speech recognition challenge;speech recognition;speech recognition propose;recognizing speech;evaluation speech quality;describes speech recognition;speech recognition chime;channel speech;speech quality;chime speech recognition;speech challenging noisy;recognizing speech challenging;recognition chime speech;single channel microphone;channel microphone;speech recognition based;noisy asr speech;speech distortion ratio;obstacle recognizing speech;speech quality pesq"}, "04b44c518b145be625ff270af56cfd2e37900137": {"ta_keywords": "distributed microphone;microphone;distributed microphones enhance;distributed microphones;suitable distributed microphone;signals distributed microphones;microphones suitable distributed;single channel recordings;microphones enhance performance;microphones;distributed microphone settings;channel recordings;microphones enhance;recordings;microphones suitable;geometry microphones;microphone settings transformer;utterances multiple remote;microphone settings;geometry microphones suitable;number geometry microphones;multi channel input;multi channel inputs;single channel input;inputs hybrid meetings;multi channel;single channel;diaphragm;end neural diarization;hybrid meetings utterances", "pdf_keywords": "voice activity detection;modeling speech activities;voice activity;modeling speech;speaker diarization spatio;attendees voice activity;speaker diarization based;speaker diarization;based speaker diarization;speech activities based;proposed speaker diarization;speech activities;approach modeling speech;recordings spatial;recordings spatial information;singlechannel recordings spatial;singlechannel recordings;activities based convolutional;attendees voice;spatio temporal encoders;based singlechannel recordings;enhance attendees voice;encoders spatio temporal;distributed microphones;spatio temporal encoder;recordings;signals distributed microphones;methods proposed speaker;distributed microphones propose;microphones"}, "a56dba9cabfc110df231051d7c9d6e439f6757dd": {"ta_keywords": "trainable pointwise taggers;tagging robot;segmentation pos tagging;pos tagging robot;speech pos tagging;pointwise taggers;tagging robot paper;trainable partially annotated;pointwise taggers allow;better domain adaptation;tagging;pos tagging;word segmentation pos;pos tagging highly;domain training;domain adaptation;tagging highly;learning accurate pointwise;methods domain training;word segmentation;position tagged;estimate position tagged;position tagged particle;partially annotated data;based pos taggers;segmentation pos;tagging highly domain;learning curve domain;curve domain adaptation;sequence based predictors", "pdf_keywords": ""}, "2ab9fd2be2bf82e0bbd558cc64c1c46728fc4f8a": {"ta_keywords": "crowd incremental adaptation;multiscale adaptation;incremental adaptation adaptation;incremental adaptation;incremental adaptations multiscale;time incremental adaptations;adaptations multiscale adaptation;multiscale properties speech;speech recognition;multiscale adaptation potential;change speech characteristics;recognition motion person;properties speech recognition;recognition motion;incremental adaptations;moving crowd incremental;complexity speech recognition;speech recognition problem;adaptation adaptation;crowd incremental;model recognition motion;speech recognition used;change speech;new model recognition;adaptations multiscale;adaptation mechanism approximate;person moving crowd;adaptation adaptation procedure;single time incremental;person moving", "pdf_keywords": ""}, "f17e182fcb7fbbff2257824174ed6f7df512a42b": {"ta_keywords": "resolution speech recognition;multi resolution speech;end speech recognition;task joint decoding;resolutions automatic speech;joint decoding;speech recognition;joint decoding multi;decoding multi resolution;resolution speech;combine encoder;joint learning;used combine encoder;propose joint learning;speech recognition mqa;automatic speech;encoders;automatic speech recognition;decomposed heterogeneous encoders;joint learning model;encoder;encoders different temporal;heterogeneous encoders;decoding;decoding multi;speech recognition paper;temporal resolutions automatic;learning model multi;hierarchical attention;observation networks coupled", "pdf_keywords": "speech recognition asr;fusion architecture encoders;automatic speech recognition;multi encoder multi;speech recognition;multi encoder;novel multi encoder;model automatic speech;recognition speech recognition;joint encoder encoder;ctc automatic speech;encoder level fusion;speech recognition work;encoder multi;networks automatic speech;encoder multi resolution;proposes joint encoder;recognition speech;attention fusion;speech recognition speech;automatic speech;joint encoder;fusion mechanism encoder;speech recognition commonly;encoder encoder;deep convolutional neural;speech recognition paper;approach automatic speech;recognition asr;ctc attention model"}, "72b4ff7387223cf0398c298c3cc62ee07d9c0043": {"ta_keywords": "sentence completion challenge;sentence completion semantic;neural language models;sentence completion;language models;language models probability;research sentence completion;simple language models;sentence estimated probability;estimated probability lexicalisation;completion semantic modeling;syntactic dependency tree;probability sentence estimated;language models applied;sentence estimated;completion semantic;lexicalisation given syntactic;points sentence completion;complete sentence paper;given syntactic dependency;probability lexicalisation given;syntactic information;neural language;probability lexicalisation;syntactic dependency;language models percentage;incorporate syntactic information;incorporate syntactic;models probability sentence;semantic modeling task", "pdf_keywords": ""}, "57fec656119e82b5e70b1a654f6d87d8c1137ef4": {"ta_keywords": "model biological captioning;biological captioning;annotated biological figures;biological captioning captures;retrieval summarization biological;summarization biological information;automatic retrieval;sampling information retrieval;summarization biological;annotated biological;automatic retrieval summarization;probabilistic topic model;retrieval summarization;captioning;biological figures;biological information literature;probabilistic topic;present probabilistic topic;topic model;information retrieval;biological figures derive;captioning captures essential;captioning captures;learning present probabilistic;topic model built;biological information;retrieval;machine learning present;knowledge extraction;years automatic retrieval", "pdf_keywords": ""}, "2797a36cd15b8c046683247995261546993c289d": {"ta_keywords": "speech enhancement preprocessor;modelbased speech enhancement;speech enhancement;speech noise modeling;temporal speech noise;hip condition speech;nonstationary noise based;condition speech enhancement;spectral temporal speech;speech recognition;speech speech recognition;time varying noise;speechnoise separation method;speech noise;combines modelbased speech;acoustic model dynamic;gaussians acoustic model;multi channel speechnoise;improves audible quality;channel speechnoise separation;audible quality speech;noise modeling combined;improves audible;speech recognition presence;modelbased speech;channel speechnoise;temporal characteristics speech;noise modeling;recognizing speech;greatly improves audible", "pdf_keywords": ""}, "99bb811beb5d061d2b8fac5a1973b49cace93e2f": {"ta_keywords": "topic model tracking;tracking consumer purchases;tracking timevarying consumer;purchase behavior computational;estimated interests trend;interests item trends;consumer interests item;timevarying consumer purchase;topic model;interests trend trends;new topic model;track changes interests;current purchase logs;behavior consumer interests;interests trends based;consumer purchase behavior;consumer purchases proposed;tracking consumer;item trends change;model tracking consumer;purchase behavior consumer;consumer interests;consumer purchases;interests trend;purchases proposed model;item trends;changes interests trends;purchase behavior;prediction accuracy purchase;interests trends", "pdf_keywords": ""}, "291b651654565cd88e4e56de5250219a71882a50": {"ta_keywords": "subset winners peer;aggregation problem agents;winners peer reviewed;winners peer;selection consider aggregation;principle peer review;peer review;noisy assessments peers;peer review able;aggregation;agents select subset;consider aggregation;online peer review;exclusion principle peer;subset winners;review online peer;peer review online;peers;select subset winners;aggregation problem;peer reviewed;best set agents;peer;assessments peers;consider aggregation problem;peer reviewed grants;group agents select;assessments peers paper;peers paper;reviewed grants prizes", "pdf_keywords": "peer review algorithmic;quality peer selection;review algorithmic game;behaviour peer selection;peer selection;winners peer reviewed;selection peernomination;peer selection peernomination;peer review;noisy assessments peer;review algorithmic;peer selection problem;assessments peer review;vote supplied algorithm;improve quality peer;winners peer;subset winners peer;peer reviewed;quality peer;selection peernomination variety;assessments peer;computational social choice;reviewers based;choice agents evaluate;weighting evaluation methods;algorithmic game;algorithm maintaining strategyproofness;peernomination;reviewers based approval;behaviour peer"}, "9cf4609178e2739ed35f8d3e3d6efb7d5e2e1a41": {"ta_keywords": "quantum computer congestion;adaptive traffic;adaptive traffic control;flow adaptive traffic;traffic flow adaptive;traffic dynamics signal;traffic dynamics;traffic control systems;traffic control;behavior traffic flow;traffic systems;traffic systems present;unstable behavior traffic;performance traffic systems;modeling traffic dynamics;modeling traffic;detects traffic;detects traffic signals;detects detects traffic;behavior traffic;traffic signals;traffic;traffic flow;study traffic flow;traffic flow koopman;method modeling traffic;study traffic;improve performance traffic;traffic signals order;quantum computer", "pdf_keywords": ""}, "74495e9735b601ce9060ac40ac27d196fdbf7462": {"ta_keywords": "distributed storage protocol;codes distributed storage;distributed storage presented;distributed storage;storage protocol;distributed storage systems;present distributed storage;stored distributed storage;storage protocol allows;bound repair bandwidth;regenerating codes distributed;stored distributed;data stored distributed;storage systems;storage;repair bandwidth;codes distributed;storage presented;capacity network;storage systems paper;repair bandwidth improved;capacity network bounded;distributed;lower bound repair;large amounts data;capacity equals exceeds;protocol allows retrieval;regenerating codes;capacity equals;bound repair", "pdf_keywords": ""}, "f75ba81828fd9d8c7fcba89dd98a0ee73d32dce6": {"ta_keywords": "mechanical thermodynamic properties;interplay mechanical thermodynamic;mechanical thermodynamic;thermodynamic properties simple;thermodynamic properties;properties simple mechanical;thermodynamic;simple mechanical;mechanical;interplay mechanical;study interplay mechanical;properties simple;properties;systematic study interplay;systematic;systematic study;results systematic study;results systematic;present results systematic;paper present results;present results;paper present;interplay;simple;study interplay;results;present;paper;study", "pdf_keywords": ""}, "58fd3001c88e9784b0794eef06cb7c0eab0d8747": {"ta_keywords": "description interacting body;interacting body systems;representation interacting objects;interacting objects;interacting objects paper;interacting body;present ontology;description interacting;mechanical resonance;ontology;based representation interacting;synthesis mechanical resonance;present ontology based;systems based representation;2d ising model;representation interacting;dynamics dimensional 2d;body systems based;mechanical resonance frequency;picture synthesis mechanical;body systems;dimensional 2d ising;ontology based;dynamics dimensional;paper present ontology;dynamics;ising model;2d ising;ontology based approach;mechanical resonant frequency", "pdf_keywords": ""}, "c9472731afe5fca98f362f49e26d17a9d5d0cc8e": {"ta_keywords": "interpretability based idea;interpretability makes concepts;interpretability based;interpretability cases black;problems motivated interpretability;interpretability interpretability;interpretability;interpretability interpretability makes;interplay interpretability interpretability;interpretability makes;interpretability cases;interpretability means;interplay interpretability;machine learning algorithms;require interpretability based;motivated interpretability cases;does require interpretability;motivated interpretability;require interpretability;machine learning;interpretability means end;study interplay interpretability;problem cited interpretability;making sense machine;sense machine learning;cited interpretability;learning algorithms presented;learning algorithms;cited interpretability means;importance black box", "pdf_keywords": "generating causal explanations;interpretability enables;interpretability easily overstated;causal explanations;interpretability easily;causal explanations case;interpretability;causal explanation molecular;epistemological causal explanation;definition interpretability enables;explanations case interpretation;interpretation algorithm requires;causal explanation;shows importance interpretability;importance interpretability;importance interpretability easily;definition interpretability;explanations case;explanations;explanation molecular biology;interpretability enables articulation;epistemological causal;interpretation algorithm;purpose generating causal;generating causal;problems interpretation literature;importance afforded interpretation;articulation epistemological causal;interpretation literature;causal"}, "b103bb1dc05a48796a3ff0804c11909bf68db11b": {"ta_keywords": "task1 token classification;task detection sentence;token classification task;classifier based syntactic;classification task detection;token classification;task detection;performed token classification;detection sentence;propose bayesian logistic;detection sentence containing;prior task1 token;bayesian logistic;words speculative nonspeculative;employing syntactic features;classification task;syntactic features;bayesian logistic regression;classification task paper;content words speculative;based syntactic dependency;analyze semantic content;semantic content words;task finding;analyze semantic;citations classifier;citations classifier based;syntactic features proxy;logistic regression classifier;semantic content", "pdf_keywords": ""}, "83ac0851a8f6fa02f5db251b260f635907d7a01e": {"ta_keywords": "learns navigate;learns navigate source;action decoding;action decoding achieves;algorithm learns navigate;vision language navigation;vision language;r2r vision language;room r2r vision;agent based learning;language navigation challenge;framework action decoding;algorithm learns;navigation challenge;learns;navigate;learning algorithm learns;present agent based;success detection;language navigation;beam search;decoding achieves;room room r2r;agent;room r2r;navigation challenge paper;decoding achieves state;decoding;navigation;problem success detection", "pdf_keywords": "embodied navigation learns;learns navigate complex;learns navigate;navigation learns;learning navigate;learning navigate complex;navigation learns navigate;deep reinforcement learning;navigation indoor scenes;navigation combines deep;embodied navigation;deep reinforcement;approach embodied navigation;navigate complex environments;embodied navigation combines;visual navigation indoor;present deep reinforcement;learn advanced agents;information learning navigate;neural network learn;recurrent neural;learns;framework embodied navigation;deep neural;visual navigation;navigation indoor;neural network recurrent;network recurrent;deep deep neural;network recurrent neural"}, "b02acb3d159b06b2319a164378e1e61c3983676f": {"ta_keywords": "complex queries;learning atomic operators;forms complex queries;complex queries paper;study combinatorial generalizability;combinatorial generalizability using;combinatorial generalizability commonly;queries;generalizability using;generalizability commonly used;learning atomic;operator systems;provides benchmark evaluate;regarded combinatorial generalizability;benchmark performance team;formulas regarded combinatorial;choices operator systems;generalizability commonly;atomic operators;provides benchmark;benchmark performance;generalizability using simple;competitors learning atomic;benchmark;commonly used operators;combinatorial generalizability;scoring competitors learning;operators;atomic operators complex;operator systems forms", "pdf_keywords": "queries natural language;benchmark answering queries;answering queries natural;answering queries;hand crafted query;queries natural;benchmark answering;comprehensive benchmark answering;query types;queries;queries single;existential order queries;natural language benchmark;query types family;queries single free;answer set operators;different query types;queries evaluated;crafted query types;queries evaluated discussed;query types original;intersection union networks;new ontology;semantic;ontologies translated semantic;efo queries;union networks;supports efo queries;union negation select;semantic ontology"}, "eaa224ae5c969180503dda4972ab86d3a71c888c": {"ta_keywords": "music recognition monophonic;binary classification monophonic;music recognition;polyphonic datasets;optical music recognition;end end polyphonic;recognition monophonic;end polyphonic hairpins;hairpins polyphonic music;recognition monophonic homophonic;polyphonic music;end polyphonic;classification monophonic;polyphonic datasets suitable;end polyphonic omr;polyphonic hairpins polyphonic;recognition sheet music;end recognition;classification monophonic homophonic;end end recognition;rhythm neural networks;large scale polyphonic;hairpins polyphonic;polyphonic hairpins;polyphonic omr;musical rhythm neural;polyphonic;recognition end end;monophonic homophonic music;scale polyphonic datasets", "pdf_keywords": "polyphonic music recognition;polyphonic music data;music recognition omr;music recognition flagdecoder;optical music recognition;end architecture polyphonic;end toend polyphonic;polyphonic optical music;polyphonic music;music recognition;architecture polyphonic music;polyphonic omr;music known polyphonic;research polyphonic music;polyphonic;task polyphonic omr;omr research polyphonic;known polyphonic music;research polyphonic;polyphonic optical;end end monophonic;problem polyphonic music;end monophonic homophonic;architecture polyphonic;known polyphonic;monophonic homophonic music;polyphonic omr research;homophonic music 13;end monophonic;music recognition detecting"}, "baad427b45ac691763fe3de4ea3ac1bffd3c74e3": {"ta_keywords": "partytracker new visualization;political survey data;partytracker online political;online political survey;dataset present partytracker;visualize changes party;users analyze party;analyze party affiliation;analyze party;political survey capable;party affiliation survey;parties partytracker;parties partytracker online;researchers studying partisanship;partytracker;partytracker online;studying partisanship;affiliation survey data;affiliation parties partytracker;partytracker new;learning information visualization;information visualization increasingly;visualization tool;present partytracker;online political;insights political survey;political survey;survey data;visualization increasingly;information visualization", "pdf_keywords": ""}, "64280761641d8f1eb285165160bd96efac0bb5f5": {"ta_keywords": "recognition environmental sounds;recognition speech environmental;classifiers environmental sounds;auditory environment recognizing;recognize speech environmental;speech environmental sounds;environmental sounds speech;simultaneous recognition speech;recognition speech;dnn techniques recognize;environmental sounds simultaneously;features sound dependent;environmental sounds;sounds speech interfaces;dnn techniques;bottleneck features sound;sounds construct classifiers;studies environmental sounds;recognize speech;sound dependent vectors;treat environmental sounds;neural network dnn;deep bottleneck features;techniques recognize speech;speech environmental;environmental sounds construct;environmental sounds interference;network dnn techniques;environmental sounds noise;deep neural network", "pdf_keywords": ""}, "2391e7446d47f681ad705c8e75d9d2ce1b92ad5f": {"ta_keywords": "entropy large corpus;large corpus;large corpus texts;corpus;corpus english;reference corpus middle;corpus texts;reference corpus;present corpus;predecessors present corpus;present corpus english;corpus english translation;german texts;created reference corpus;corpus middle;hs german texts;german texts 1200;annotated parts speech;international conference english;speech morphology;conference english translation;speech morphology projects;corpus texts digitized;parts speech morphology;english translation held;translation held july;corpus middle high;translation translation;translation translation translation;translation held", "pdf_keywords": ""}, "ff7e60b8d336aef5ed974609a63610641085177e": {"ta_keywords": "prediction tasks rewards;softmax;structured prediction tasks;estimating softmax;estimating softmax distribution;entity recognition tree;statistics softmax;softmax algorithm equivalent;approximately estimating softmax;prediction tasks;softmax algorithm;softmax distribution;statistics softmax algorithm;reward function structured;softmax distribution approximation;structured prediction;reward problem machine;entity recognition;tasks rewards;task specific reward;tasks rewards defined;named entity recognition;specific reward;prediction tasks led;machine translation;optimize reward;directly optimize reward;averaged statistics softmax;machine translation structures;experiments structured prediction", "pdf_keywords": "machine translation deep;translation deep neural;parsing machine translation;machine translation;machine translation largely;named entity recognition;entity recognition;machine translation machine;translation machine translation;framework propose softmax;softmax margin training;propose softmax;entity recognition ner;translation deep;softmax;projective dependency parsing;dependency parsing machine;propose softmax margin;estimating softmax distribution;machine translation mt;framework estimating softmax;structured prediction;dependency parsing;estimating softmax;translation machine;ner dependency parsing;approach machine translation;problem structured prediction;softmax distribution;softmax margin"}, "4d991a83d6044b1aaed2c117b3d097ecd23cf6f4": {"ta_keywords": "conversations speaker diarization;speaker diarization;dealing speaker diarization;speaker embedding models;approach speaker embeddings;speaker embedding;speaker embeddings;speaker diarization common;adapting speaker embedding;captures global speaker;end neural diarization;speaker diarization problem;conversations speaker;global speaker characteristics;speaker diarized results;real conversation datasets;adapting speaker;real conversations speaker;speech mixtures;speaker diarized;local speech activity;simulated speech mixtures;conversation datasets;global speaker;speech activity;speech activity dynamics;neural diarization;outputs speaker diarized;diarization eei neural;simulated speech", "pdf_keywords": "speaker diarization neural;adaptation speaker diarization;architecture speaker diarization;reformulate speaker diarization;speaker diarization simple;speaker diarization;approach speaker diarization;domain adaptation speaker;attention based speaker;based speaker diarization;speaker diarization called;learning speaker;learning speaker embeddings;speaker diarization wide;speaker embeddings based;speaker diarization important;speaker embeddings;method learning speaker;adaptation speaker;network architecture speaker;diarization neural network;reformulate speaker;speech mixtures;real conversation datasets;simulated speech mixtures;architecture speaker;diarization task recognizing;conversation datasets;paper reformulate speaker;diarization neural"}, "9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e": {"ta_keywords": "dual encoders trained;dual encoders outperform;encoders trained;home repeater;encoder;home home repeater;sparse dense retrievers;encoders trained domain;encoders outperform existing;dual encoder;results dual encoders;dual encoders;size dual encoder;encoders;dual encoder model;encoders outperform;home repeater used;encoder model keeping;encoder model;repeater;home home inspired;gtr data efficient;generalize domains retrieval;home inspired home;inspired home integrated;shown dual encoders;domains retrieval tasks;keeping bottleneck embedding;bottleneck embedding;repeater used", "pdf_keywords": "query embeddings;dual encoders trained;learning bottleneck embedding;scaling dual encoders;query embeddings passage;dotproduct query embeddings;wide range retrieval;neural retrievers achieve;encoders trained;dual encoder;bottleneck embedding dimension;neural retrievers;deep bidirectional;encoders dual encoders;dual encoders;present dual encoder;encoder text texttransfer;encoders dual;dual encoders dual;dual encoder text;encoder model bottleneck;retrieval tasks results;end retrieval continuous;end retrieval;scaled dual encoder;retrieval continuous space;dual encoder model;transformers information retrieval;retrieval tasks;encoders trained using"}, "d18d8d364bb18d66924919feebb2e892ebe6761c": {"ta_keywords": "machine translation promising;output machine translation;translation machine translation;machine translation machine;machine translation;phrase based decoding;translation machine;outputs improve translation;decoding path nmt;phrase based smt;improve translation quality;nmt output machine;translation promising approach;soft forced decoding;forced decoding cost;based decoding cost;forced decoding algorithm;translation quality;nmt outputs search;best nmt output;decoding cost;using forced decoding;decoding cost best;forced decoding;approach nmt outputs;nmt output;nmt exploiting existing;cost best nmt;decoding cost rerank;nmt outputs", "pdf_keywords": "neural machine translation;machine translation;machine translation introduce;machine translation important;machine translation test;phrase based decoding;translation rules;method machine translation;decoding training phrase;phrase based smt;tasks machine translation;soft forced decoding;types translation rules;nmt translation introduce;nmt translation;german translation task;forced decoding training;translation task obtaining;translation task;translation rules deleting;forced decoding algorithm;new types translation;decoding training;translation important tasks;sentence nmt translation;forced decoding;types translation;source sentence nmt;novel forced decoding;decoding method machine"}, "1b96b89d5b3ba444126cebefdfc665d3866f14f0": {"ta_keywords": "fair outcomes hiring;bias fairness issues;bias fairness;modern hiring pipeline;amplify bias fairness;hiring incorporating definitions;hiring process;hiring;hiring process quickly;hiring pipeline automation;hiring pipeline;modern hiring;outcomes hiring;outcomes hiring incorporating;crucial role hiring;constitute modern hiring;fair outcomes;used hiring pipeline;role hiring;fairness issues;role hiring process;bias;hiring article;fairness issues historically;hiring pipeline decisions;plays role hiring;technologies used hiring;used hiring;role hiring article;potentially amplify bias", "pdf_keywords": ""}, "c888022dec626171d243d2a056709b9b053a0ed9": {"ta_keywords": "end speech recognition;noisy speech benchmarks;speech benchmarks multichannel;speech recognition midst;speech benchmarks;speech recognition;acoustic encoding network;enhancement acoustic encoding;end neural networks;speech recognition objective;speech enhancement;noisy speech;end outperforms attention;speech enhancement acoustic;noise suppression speech;outperforms attention based;microphone array;suppression speech enhancement;recurrent encoder;acoustic encoding;end end speech;end speech;end end neural;microphone;end neural;adaptive beamformer;jointly recognition architecture;recurrent encoder decoder;experiments noisy speech;encompass microphone array", "pdf_keywords": "end speech recognition;speech recognition multichannel;multichannel speech enhancement;attention based encoder;neural beamformer proposed;optimization multichannel speech;multichannel speech;neural beamformer;proposed neural beamforming;integrating neural beamformer;speech recognition noisy;recognition multichannel end;end end speech;speech enhancement proposed;neural beamforming;end speech;speech recognition challenging;speech recognition;speech enhancement;speech recognition framework;adaptive beamformer;neural networks multichannel;beamformer proposed multichannel;existing attention based;end outperformed attention;automatic speech recognition;recognition multichannel;conventional adaptive beamformer;framework automatic speech;automatic speech"}, "2526c510610c7220ecc56e6b08d09c4cbaf58c3c": {"ta_keywords": "expressions honorifics japanese;honorifics japanese anaphora;appropriate honorifics japanese;form honorifics japanese;regular expressions honorifics;honorifics japanese;expressions honorifics;japanese honorifics;honorific speech normal;expressions appropriate honorifics;\u5e38\u8a9e honorific speech;honorifics japanese japanese;honorifics japanese relationship;japanese honorifics include;speech \u5e38\u8a9e honorific;japanese japanese honorifics;honorific speech;proper form honorifics;honorific form;japanese relationship honorific;honorific form applied;honorific speech person;japanese anaphora resolution;honorifics;form honorifics;\u5e38\u8a9e honorific;appropriate honorifics;relationship honorific speech;form honorific;honorific", "pdf_keywords": ""}, "2a462e2b748d7e78f3af2621071265c1ad2683ea": {"ta_keywords": "optimal power flow;dispatchable wind energy;wind energy resource;power flow problem;integration dispatchable wind;wind energy;linear programming approach;successive linear programming;power flow;power producers ipp;wind parks independent;linear programming;dispatchable wind;optimal power;modification optimal power;energy resource power;wind parks;independent power producers;parks independent power;problem modification optimal;presence wind parks;power producers;ipp operation;producers ipp operation;energy resource;ipp operation large;flow problem;resource power described;utilized solution problem;modification optimal", "pdf_keywords": ""}, "b61c9799f10e4de9cd222dfd8e423bbd950a7c44": {"ta_keywords": "entailments task extractive;extractive qa task;task extractive qa;natural language understanding;recognizing hugual entailments;questions understanding text;entailments task;task extractive;hugual entailments task;dataset unanswerable questions;unanswerable questions analyze;squad dataset unanswerable;enriching squad dataset;essential natural language;natural language;language understanding;extractive qa;qa task;understanding text;qa task investigate;unanswerable questions;entailments;language understanding note;dataset unanswerable;questions understanding;squad dataset;questions analyze;hugual entailments;snippet does provide;enriching squad", "pdf_keywords": "recognition unanswerable situations;domain corpus;contextualized language models;unanswerable situations domain;build domain corpus;therecognizing textual entailments;domain corpus focusing;natural language understanding;recognition unanswerable;language models tem;language models;baseline natural language;natural language;therecognizing textual;unanswerable situations;language understanding based;contextualized language;approach recognition unanswerable;textual entailments;focus therecognizing textual;language understanding;textual;use contextualized language;corpus;models tem trained;entailments;answer choices explore;contextualized;event based questions;situations domain scenarios"}, "4ebe5792fe2890590e7a5bf8ae0a29e0fb147ef9": {"ta_keywords": "prediction word level;word level edit;utterance rewriting;incomplete utterance rewriting;utterance rewriting considered;level edit matrices;edit matrices;edit matrices task;task incomplete utterance;prediction word;incomplete utterance;word level;level edit;rewriting;matrices task incomplete;hysteresis;rewriting considered;hysteresis loops;prediction;approach prediction word;analysis hysteresis;utterance;analysis hysteresis loops;detailed analysis hysteresis;matrices task;new approach prediction;hysteresis loops observed;matrices;approach prediction;loops observed hysteretic", "pdf_keywords": "downstream dialogue generation;dialogue generation;dialogue generation based;dialogue modeling;approach downstream dialogue;downstream dialogue;dialogue context ellipsis;turn dialogue modeling;dialogue modeling infancy;dialogue;dialogue context;person utterances sequence;utterances sequence grams;utterances sequence;person utterances;turn dialogue;dnn model learns;multi turn dialogue;appeared dialogue context;generation based semantic;utterances;appeared dialogue;deep neural;context ellipsis coreference;types person utterances;utterances usually;concepts appeared dialogue;learns encode predict;neural network dnn;coreference"}, "e6fe601c44835d3654131d0312d65227d3523373": {"ta_keywords": "represent text corpus;similarity measure words;distributional similarity measure;distributional similarity;text corpus;text corpus labeled;corpus labeled directed;walks derive similarity;graph based similarity;based similarity measure;similarity measure based;corpus;corpus labeled;similarity measure;words use supervised;similarity measure propose;dependency parsing fluid;represent words weighted;parsing fluid dynamics;derive similarity measure;derived similarity measure;nodes represent words;words weighted edges;based similarity;coordinate term extraction;similarity measure particular;improve derived similarity;derive similarity;parsing fluid;labeled directed graph", "pdf_keywords": ""}, "5a4d4c0824b5e113c39105c71d42b93d3900d87e": {"ta_keywords": "experiment crowdsourcing engine;crowdsourcing engine;engine crowdsourced tasks;crowdsourcing engine used;crowdsourced tasks;pieces crowdsourcing engine;microtask crowdsourcing;microtask crowdsourcing method;crowdsourcing engine crowdsourced;crowdsourcing tasks;crowdsourcing engine mechanized;crowdsourced tasks human;engine crowdsourced;method crowdsourcing tasks;experiment crowdsourcing;control microtask crowdsourcing;crowdsourcing tasks decomposing;crowdsourcing process;assessment experiment crowdsourcing;crowdsourcing;crowdsourcing process including;crowdsourcing method;crowdsourced;method crowdsourcing;entire crowdsourcing process;crowdsourcing method crowdsourcing;solve crowdsourcing used;solve crowdsourcing;crowdsourcing used solve;crowdsourcing used", "pdf_keywords": ""}, "e2b6193a24cd6c9f736139aa66618d1b8bf2a60b": {"ta_keywords": "speech transcription tool;speech transcription;correction speech transcripts;manual correction speech;speech transcripts;speech transcripts user;present speech transcription;transcripts user interface;transcription tool;speech transcribed updated;speech transcribed;automatically created transcript;transcribed updated regularly;transcriber correction selection;transcription tool targeted;transcription;remainder speech transcribed;segmentation remainder speech;guides transcriber correction;transcript starting point;transcripts user;efficient manual correction;transcripts;correction speech;transcriber correction;transcript starting;transcript;transcribed;created transcript starting;created transcript", "pdf_keywords": ""}, "fc848789b557a7581c51c79fd01897dc5aa7e8a8": {"ta_keywords": "knowledge integrated multimodal;explicit knowledge reasoning;explicit knowledge end;answer prediction;jointly reasoning knowledge;decoder architecture jointly;explicit knowledge integrated;relevance retrieved knowledge;retrieved knowledge;explicit knowledge integration;implicit explicit knowledge;multimodal transformers;answer generation;explicit knowledge;followed answer prediction;answer prediction leave;architecture jointly reasoning;end encoder decoder;knowledge end end;decoder;multimodal transformers leverage;leverage explicit knowledge;use explicit knowledge;integrated multimodal;model predictions existing;sources answer generation;encoder;reasoning knowledge;encoder decoder architecture;knowledge end", "pdf_keywords": "visual semantic matching;knowledgebased answer answering;learning transferable visual;transferable visual models;visual semantic;answer knowledge;leverage contrastive learningbased;jointly reasoning knowledge;knowledge answer questions;implicit knowledge generative;contrastive learningbased;knowledge reasoning jointly;answer knowledge based;framework answer knowledge;visual models natural;knowledge knowledgebased answer;knowledge generative language;implicit knowledge reasoning;knowledge generative;trained object detectors;novel knowledge reasoning;contrastive learningbased model;commonsense explicit knowledge;learning transferable;answer answering tasks;semantic matching leverage;visual models;uses contrastive learningbased;knowledgebased answer;natural language supervision"}, "db729f2f55a92465cf88682ba7917621fd4c000b": {"ta_keywords": "explanations tutoring activities;explanations tutoring;provide explanations tutoring;tutor outcome;facilitates tutor learning;tutor learning;tutoring;facilitates tutor;student tutor outcome;tutoring activities;tutoring activities facilitates;tutor outcome accidental;tutor learning paper;students provide explanations;effect student tutor;student tutor;tutor;tutored time;activities facilitates tutor;tutored;tutored time present;problems tutored;problems tutored time;students learn;students self;use iterative feedback;self explanation;allows students learn;explanation effect student;iterative feedback", "pdf_keywords": ""}, "b2da0f022a48ebd10a23572b5310b7d7341b6448": {"ta_keywords": "pedestrian crowded environment;dynamics single pedestrian;single pedestrian crowded;pedestrian crowded;crowded environment;single pedestrian;pedestrian;crowded;dynamics;dynamics single;study dynamics single;method study dynamics;study dynamics;single;method;environment;method study;new method study;study;new method;paper present;present new method;paper;new;present;paper present new;present new", "pdf_keywords": ""}, "004ddf5a39a735d0f8ec7547629c2bee65eb1f93": {"ta_keywords": "biases peer reviewing;reviewers induces biases;testing biases;biases peer;biases scholarly;biases scholarly research;peer reviewing wsdm;reviewers induces;identities reviewers induces;peer reviewing;testing biases single;blind peer review;issue biases scholarly;peer review;author identities reviewers;existence biases peer;biases;specifically peer review;bias fact;peer review report;bias;biases certain;identities reviewers;reviewers;reviewer calibration;biases certain groups;mismatch reviewer calibration;induces biases;induces biases certain;reviewer calibration consider", "pdf_keywords": "testing biases;independent reviewers;independent reviewers available;assignment algorithm reviewers;reviewers biased;testing biases respect;biased favor papers;procedure testing biases;algorithm reviewers biased;reviewers biased favor;submitted independent reviewers;conference peer review;reviewer calibration test;reviewer calibration;algorithm reviewers;bias problem case;study relative bias;biases;peer review;peer review setup;reviewers;bias problem;reviewers available review;bias;reviewers available;blind assignment algorithm;relative bias;construct model reviewer;biases respect;relative bias problem"}, "1bd7d16340642948142d7608ef8f085d934d94a3": {"ta_keywords": "policy gradient methods;derivative free optimization;free optimization;strongly convex objectives;policy gradient;method nonconvex convex;unconstrained minimization smooth;free optimization algorithms;unconstrained minimization;minimization smooth objective;learning continuous control;convex objectives;minimization smooth;convex objectives propose;motion swimmer bouncer;gradient methods state;smooth objective function;nonconvex convex;heavy ball momentum;compare policy gradient;nonconvex convex strongly;ball momentum;swimmer bouncer;dynamics self propelled;gradient methods;problem unconstrained minimization;method heavy ball;swimmer bouncer bouncer;smooth objective;method nonconvex", "pdf_keywords": "policy gradient algorithms;free reinforcement learning;policy gradient methods;policy gradient;based policy gradient;new reinforcement learning;strongly convex objectives;model free reinforcement;convex objectives;probabilistic descent;algorithm based policy;convex objective function;reinforcement learning;reinforcement learning algorithm;convex objectives paper;based probabilistic descent;art policy gradient;convex objective;probabilistic descent paper;smtp importance sampling;hull convex objective;free reinforcement;importance sampling smtp_is;gradient algorithms;unconstrained minimization smooth;minimization smooth objective;smooth objective function;reinforcement learning conduct;momentum non convex;unconstrained minimization"}, "db8376698c06d6a688a39bff0300780ef0383821": {"ta_keywords": "sharing corrugated boards;corner sharing corrugated;corrugated boards decentralized;sharing corrugated;manufacturing corrugated cardboard;corrugated boards;line manufacturing corrugated;network corner sharing;manufacturing corrugated;control network corner;corner sharing;corrugated cardboard pastes;corrugated cardboard;boards decentralized control;network corner;corrugated;corrugating board;corrugating board pastes;synchronising local control;local control network;cardboard pastes liner;board pastes liner;corrupated board make;double faced board;faced corrugating board;boards decentralized;cuts board desired;cardboard pastes;corrupated board;local control", "pdf_keywords": ""}, "4b1555368fd2c5f1234eaac5e41296003481754a": {"ta_keywords": "design electrolyte;design electrolyte solution;describes design electrolyte;density lithium oxygen;electrolyte solution;lithium oxygen battery;deep eutectic electrolyte;electrolyte;eutectic electrolyte;electrolyte solution high;eutectic electrolyte dee;lithium oxygen;nma lithium bis;electrolyte dee fulfill;electrolyte dee;lithium metal anode;high ionic conductivity;energy density lithium;density lithium;electrochemical stability;nma lithium;compatibility lithium;ionic conductivity good;oxygen battery;chemical electrochemical stability;compatibility lithium metal;oscillators llbs developed;electrochemical stability good;ionic conductivity;good compatibility lithium", "pdf_keywords": ""}, "005879e6587eb6e05f56c20d345f784ee84a44c4": {"ta_keywords": "models improve parsing;parsing language modeling;neural network grammars;language modeling generative;parsing language;grammars generate sentences;recurrent neural nets;language modeling;improve parsing;generate sentences;perform parsing language;grammars generate;phrase structure;languages models;parsing performance discriminative;phrase structure syntax;generate sentences using;improve parsing performance;parsing;network grammars generate;nonsyntactic languages models;models dependency syntax;perform parsing;network grammars;using phrase structure;recurrent neural network;japanese recurrent neural;grammars;models use recurrent;syntax perform parsing", "pdf_keywords": ""}, "f61862b286c9e4894302faf716eedb0eb60a2f5f": {"ta_keywords": "conversational data;presentation conversational data;thread line discussions;messages thread structure;conversational data approach;temporal relationships messages;thread structure meta;representation presentation conversational;conversational goals fluid;presentation conversational;recovering thread structure;thread structure reconstructed;related particular conversational;discussions composed multiple;inter message similarity;line discussions composed;discussions composed;features thread structure;relationships messages thread;message similarity;discussion related specific;structure meta data;particular conversational;isolate discussion related;discussion related;thread structure;conversational;recovering thread;discussions;line discussions", "pdf_keywords": ""}, "20819855b9517c927a1262850146e525c8083fb4": {"ta_keywords": "human dialog data;dialog concepts spoken;utterance context dialog;human dialog;human human dialog;utterances context social;dialog concepts;dialog data model;context dialog performance;labeled utterances context;dialog data annotated;spoken word sequences;sequences context utterance;accurately dialog;dialog performance improved;context utterance context;labeled utterances;utterances context;recurrent neural networks;sequences dialog concepts;context dialog;context utterance;spoken language understanding;short term memory;dialog performance;dialog data;utterance context;intentions accurately dialog;memory recurrent neural;predict sequences dialog", "pdf_keywords": ""}, "f74d4fa99ccedfea7a2662fe6944d99f34533912": {"ta_keywords": "models improve fairness;fairness constraints group;improve fairness model;existing fairness methods;method fairness improved;fairness methods;theoretical accuracy fairness;fairness method fairness;accuracy fairness method;optimize multiple fairness;fairness constraints;fairness model agnostic;multiple fairness constraints;accuracy fairness;fairness method;fairness model;tradeoff accuracy fairness;group aware threshold;improve fairness;method fairness;accuracy fairness trade;compared existing fairness;fairness improved;existing fairness;classification thresholds;adaptive classification thresholds;classification thresholds demographic;multiple fairness;constraints group aware;group aware", "pdf_keywords": "learning fairness adversarial;fairness adversarial learning;learning fairness;fairness adversarial;models improve fairness;learning fair representations;improve fairness model;fairness model agnostic;fair representations training;fairness model;improve fairness;achieve better fairness;better fairness performance;trade learning fairness;fairness performance;fair representations;fair classi\ufb01cation;propose fairnessaccuracy;better fairness;analysis fairnessaccuracy;propose fairnessaccuracy tradeoff;learning fair;fairnessaccuracy trade learning;fairness performance state;fairnessaccuracy tradeoff contextualizes;quality fair classi\ufb01cation;fair classi\ufb01cation conduct;fairnessaccuracy tradeoff;fairness;fairnessaccuracy"}, "25fec1e150a273b3bec3655ace0ff6b97c338a96": {"ta_keywords": "resilient errors erasures;adversaries regenerating codes;regenerating codes resilient;codes resilient errors;errors erasures codes;resilience regenerating code;errors erasures nodes;erasures codes optimal;nodes distributed storage;codes resilient;malicious adversaries regenerating;erasures codes;distributed storage systems;distributed storage;node repair operations;regenerating codes;explicit regenerating codes;resilient errors;erasures nodes;handling errors erasures;data efficient repair;class distributed storage;storage systems;errors erasures;regenerating code;node repair;storage systems provide;regenerating codes class;erasures nodes links;repair failed nodes", "pdf_keywords": "erasures multicast network;erasures multicast;errors erasures multicast;multicast network coding;erasure capacity network;codes case byzantine;security distributed storage;consider distributed storage;resilient regenerating codes;byzantine fault tolerance;distributed storage;distributed storage systems;distributed storage consisting;storage nodes network;multicast networks;non multicast networks;error erasure capacity;network achieved codes;storage nodes;multicast network;networks error erasure;network coding studied;network coding;resilience regenerating code;class non multicast;multicast networks error;erasure capacity;multicast;consisting storage nodes;provide security distributed"}, "2875d6cac905c3dfec4c8a8e1d89a2a7e4d71d40": {"ta_keywords": "nodes proposed repair;repair failed nodes;nodes reduces repair;reduces repair bandwidth;repair bandwidth node;bandwidth partial repair;proposed repair scheme;failed nodes proposed;partially failed nodes;failed nodes satisfies;failed nodes based;repair scheme;network failures;functional repair code;failed nodes reduces;complexity repair paper;repair bandwidth;network network failures;repair feasible;network failures report;functional repair feasible;complexity repair;multiple failed nodes;capacity repair bandwidth;computational complexity repair;sufficient functional repair;repair code constructed;repair code;failed nodes;repair partial failures", "pdf_keywords": "nodes repair construction;nodes repair content;nodes repair;broadcast repair multiple;reducing repair bandwidth;functional repair network;repair network multiple;repair network;broadcast repair;surviving nodes repair;optimal storage repair;storage repair bandwidth;failed nodes repair;functional repair point;linear repair;linear repair code;trade broadcast repair;feasible functional repair;repair bandwidth;repair problem node;repair point;functional repair code;content reducing repair;storage repair;repair bandwidth trade;repair content;remaining failed nodes;failed nodes simultaneously;multiple failed nodes;reducing repair"}, "190865e2c3d4908ff20bf9a31f5a2773d6fec5cb": {"ta_keywords": "question answering;question answer answering;question answering open;domain question answering;answer answering;stored question answer;new approach answering;answering question;answering questions;answer answering questions;answering question answer;answering questions based;answering;generated question answer;approach answering;question answer pairs;approach answering question;answer question;answering open;gev answer question;questions;lhc;questions explicitly;section lhc;answer pairs report;domain knowledge;proton collisions xmath0;answer question yes;question answer;generated question", "pdf_keywords": "retrieval reading comprehension;question retrieval;deep retrieval;deep retrieval augments;batch deep retrieval;new retrieval reading;answer answering;augmented language models;new retrieval;memory augmented language;retrieval reading;present new retrieval;approach answer answering;step question retrieval;retrieval augments;answer answering open;retrieval augments text;knowledge memory augmented;retrieval;reading comprehension problems;question answer pairs;reading comprehension;answering;language models;answering open;representation knowledge memory;memory augmented;comprehension problems based;knowledge memory;memory question answer"}, "4dc8ddef938699d0d8a0b685ad9f56d0b735a25d": {"ta_keywords": "learns incrementally;learning algorithm andebl;learning algorithm called;mistake bounded learnability;bounded learnability;ebl learns incrementally;perceptron learning;learnability able predict;learns incrementally ia;bounded learnability able;learns;perceptron learning algorithm;learning algorithm;learnability;learning;based learning algorithm;learning algorithm output;cover based learning;alternative learning algorithm;learnability able;based learning;algorithm output learning;output learning;extension perceptron learning;output learning longer;ebl learns;learning longer;learning longer expressed;fluid motion;fluid dynamics video", "pdf_keywords": ""}, "5677d2b565c8265fef1693a9be861739cb01bf2f": {"ta_keywords": "speech recognition systems;speech recognition;vocabulary speech recognition;security based recognition;recognition systems consist;recognition systems automatically;recognition performance numerous;deep neural network;large vocabulary speech;human recognition systems;recognition systems;speech recognition using;highest recognition performance;recognition human;recognition using evolution;recognition performance;configuration large vocabulary;objective pareto optimization;parameters large vocabulary;markov model deep;realize highest recognition;model deep neural;recognition human recognition;pareto optimization;human recognition;approach recognition human;recognizing;recognition;highest recognition;based recognition presence", "pdf_keywords": ""}, "3d2da57c2de69b02fa0fee5c12ace618718a3926": {"ta_keywords": "meta learning;model meta learning;meta learning scheme;subgraphs sentences text;matching subgraphs sentences;mining data text;subgraphs sentences;combination text images;subcellular location image;learning;text images;stacked graphical model;subgraphs;labels sentences paper;match labels sub;figures labels sentences;predict outcome hockey;base learner;features based related;text images journal;learning scheme;matching subgraphs;based matching subgraphs;mining data;labels sentences;sub figures;text data;labels sub figures;sentences text;graphical model meta", "pdf_keywords": ""}, "d21a0e01514732f241b9c138eceb76ecaef17a27": {"ta_keywords": "mt active learning;active learning mt;phrase based active;active learning;machine translation;segments active learning;learning mt active;new machine translation;active learning fail;machine translation previous;selecting redundant phrases;machine translation framework;active learning framework;based active learning;framework machine translation;easy human translators;extensive manual translation;redundant phrases similar;manual translation;redundant phrases;translation framework machine;human translators translate;manual translation professional;human translators;phrases similar content;translators confident translations1;translation professional translators;translate problems selecting;translators proposed method;confident translations1 paper", "pdf_keywords": ""}, "b4bc1a98eb79545f8da4385a6dfb643b0c62a07e": {"ta_keywords": "prediction unseen syntactic;prediction syntactic languages;predict unseen syntactic;syntaxbased translation methods;prediction syntactic;syntaxbased translation;translation syntax based;machine translation;syntax based prediction;communication machine translation;analysis prediction syntactic;performing translation syntax;syntactic languages short;accurate parse trees;machine translation dividing;unseen syntactic;translation methods difficult;generate accurate parse;accurate parse;problems syntaxbased translation;complete parse trees;generate fluent translation;translation syntax;unseen syntactic constituents;translation methods;syntactic languages;languages short segments;parse trees sub;complete parse;translation proposed methods", "pdf_keywords": ""}, "425a4a9c0598e4101ca2f2b930f5c6986ce40a99": {"ta_keywords": "privacy preserving regularization;optimization utility privacy;privacy use discriminator;favorable utility privacy;differential privacy;models privacy guarantees;models privacy;subgroups differential privacy;privacy preserving;train models privacy;utility privacy;introduce privacy preserving;utility privacy trade;privacy;privacy guarantees;utility privacy use;differential privacy dp;regularization methods training;privacy trade;privacy guarantees comes;privacy use;introduce privacy;regularizers favorable utility;privacy trade faster;regularization;privacy dp popular;privacy dp;regularizers favorable;preserving regularization;regularization methods", "pdf_keywords": "privacy regularization;propose privacy regularization;privacy regularization methods;regularization mitigates privacy;learning differential privacy;privacy optimization;labels differential privacy;jointly optimize privacy;tuning optimal privacy;optimize privacy;framing privacy optimization;optimize privacy utility;training novel privacy;differential privacy disparate;optimal privacy utility;differential privacy;adversarial training;privacy optimization problem;privacy utility language;based adversarial training;optimal privacy;performance propose privacy;adversarial;privacy utility;models framing privacy;novel privacy loss;privacy utility trade;adversarial training novel;privacy;based adversarial"}, "4383e714f4535777ffb7b4f618d4ccede4b08bd3": {"ta_keywords": "languages allophone phonemic;allophone databases;allophone databases database;called allophone databases;speech international phonetic;languages allophone;language specific phonetic;database allophone;phonetic representations;specific phonetic representations;minority languages phonemes;allophones phonemes;database called allophone;phonemes contrastive phonological;international phonetic;databases database allophone;phonetic representations closer;database allophone events;specific phonetic;languages phonemes;ipa phonemic representations;languages phonemes contrastive;14 languages allophone;international phonetic alphabet;allophone phonemic models;phonemic representations language;phonetic;new approach phonological;allophone phonemic;endangered minority languages", "pdf_keywords": "mapping allophone phonemes;recognize allophone phonemes;multilingual phoneme model;phoneme representation multilingual;phoneme allophone mappings;allophone phonemes;phoneme allophone;mappings phoneme allophone;phoneme allophone mapping;present phoneme allophone;generate universal phonemes;allophone mappings phoneme;phoneme allophone transducers;types phoneme allophone;phoneme representation;allophone phonemes 14;multilingual phoneme;allophone transducers phonology;presents multilingual phoneme;diverse phonologies multilingual;phonemes lifelong speakers;allophone phonemes paper;phoneticians phonologists;phoneticians phonologists generate;phoneme model trained;multilingual speech recognition;phonologies multilingual;grammar phoneme representation;predict phoneme;phonologies multilingual multilingual"}, "c549b3f2d262efc1f68dfdd842174634f37519ed": {"ta_keywords": "adaptation techniques speech;incremental adaptation quickness;proposed incremental adaptation;incremental adaptation techniques;incremental adaptation;adaptation incremental adaptation;adaptation classifier;incremental adaptation framework;novel incremental adaptation;adaptation classifier parameters;adaptation incremental;speech recognition;approach adaptation classifier;adaptation quickness stability;balanced incremental adaptation;incremental adaptation realizes;adaptation quickness;adjusting acoustic models;speech recognition aimed;combinatorial adaptation approaches;changes speaker speaking;indirect adaptation incremental;techniques speech recognition;adaptation framework;robust recognition;recognition aimed adjusting;classifier parameters transformation;combinatorial adaptation;adaptation realizes predictor;changes speaker", "pdf_keywords": ""}, "40f4d7fe800810288a80f84cdb357a8f4c28e880": {"ta_keywords": "dimensionality channel cnns;cnns generalization performance;channel cnns;learning transformer;cnns generalization;learning transformer based;cnns;channel cnns generalization;based learning transformer;pooling based vision;vision transformer;convolutional neural;evaluation vision transformer;based vision transformer;object detection robustness;tasks image classification;convolutional neural network;stochastic neural networks;computer vision;vision transformer vi;networks based image;object detection;computer vision empirically;transformer vi convolutional;vision empirically spatial;image based learning;processing computer vision;image classification object;classification object detection;computer vision modeling", "pdf_keywords": "vision propose convolutional;propose convolutional architecture;convnets;convolutional architecture;convolutional architecture incorporates;new architecture convolutional;propose convolutional neural;convnets inspired;discriminative visual features;networks convnets;networks convnets inspired;convolutional neural networks;convolutional neural;pooling based vision;convolutional neural network;neural networks convnets;architecture convolutional;discriminative visual;architecture convolutional neural;visual features;convnets inspired fact;attention mechanism convolution;local discriminative visual;propose convolutional;computer vision;computer vision domain;vision transformer;convolutional;fact convolutional neural;effective computer vision"}, "8a73eed98873d91086201f41c6e1f613fcdefe18": {"ta_keywords": "automated speech;performance automated speech;automated speech recognition;self supervised;self supervised training;speech recognition;supervised self supervised;supervised self;language model;language model reward;context synthesized speech;equipped language model;supervised;language model incorporates;synthesized speech;speech recognition based;attention context synthesized;direction equipped language;supervised training;hyperparameter scales attention;propose language model;asr hypotheses forwarding;self propelled particles;self propelled;predicting performance automated;gap supervised self;speech;performance gap supervised;eat model;physics self propelled", "pdf_keywords": "machine translation architecture;machine translation;machine translation experiments;new machine translation;self attention layer;automatic speech;attention layer based;translation architecture automatic;self supervised training;architecture automatic speech;based machine translation;attention layer;self supervised;supervised self supervised;enhanced language model;language model reward;language model;automatic speech recognition;translation architecture;present self supervised;speech recognition;translation experiments;self supervised approach;supervised self;previous self supervised;answer answer model;features enhanced language;answer model;speech recognition systems;training approach unsupervised"}, "42be8ed9973b3326a6e3d838c4742bc1d7704704": {"ta_keywords": "approach articulatory acoustic;articulatory acoustic production;modifying phonemic sounds;modify phonemic sounds;articulatory parameters based;manipulating estimated articulatory;articulatory acoustic;manipulating articulatory parameters;novel speech modification;acoustic production mapping;speech modification method;input speech signal;speech modification;manipulating articulatory;articulatory parameters estimated;phonemic sounds input;sounds input speech;mapping approach articulatory;phonemic sounds manipulating;propose model acoustic;approach articulatory;input speech;estimated articulatory movements;articulatory parameters;speech signal;based mixture gaussian;acoustic production;acoustic acoustic production;acoustic production ratio;technique gaussian mixtures", "pdf_keywords": ""}, "7c0ada3511b05897fb4d75c5f657b5fbd953caa8": {"ta_keywords": "biases online platforms;quality reputation bias;reputation bias;biases reputation;biases online;different biases reputation;biases reputation social;impression signals votes;reputation bias position;content quality reputation;underestimates reputation bias;reputation social influence;reputation bias 2x;votes content based;vote aggregates;signals votes content;votes content;use vote aggregates;quality reputation;participant vote content;bias important factors;bias;vote content;biases;sum votes commonplace;votes commonplace;reputation social;measuring content quality;bias position bias;performance use vote", "pdf_keywords": "quantifying voter biases;bias votes identification;voter biases factors;voter biases;voter bias;transparently bias votes;capture voter biases;bias votes;influence votes;bias social influence;reputation bias;biases reputation bias;voter bias particular;social influence bias;factors influence votes;biases reputation;reputation bias social;influence votes significant;voter biases compile;research voter bias;influence bias;votes identification causal;distinct biases reputation;quantifying voter;influence online platforms;bias social;bias develop social;quantifying social influence;variables quantifying voter;impact factors voting"}, "e79be3f9ce409f1a9b7084ef880298665e5212d0": {"ta_keywords": "token aware contrastive;aware cascade contrastive;cascade contrastive learning;benchmarks contrastive learning;improves contrastive prediction;retrieval video action;language models video;text video benchmarks;text video retrieval;video benchmarks contrastive;aware contrastive loss;representation learning cascade;contrastive prediction;video action segmentation;retrieval video;aware contrastive;action segmentation;models video text;cascade contrastive;vision language models;video retrieval;contrastive prediction using;benchmarks contrastive;contrastive learning;modal representation learning;contrastive learning widely;video retrieval video;localization video action;learning cascade;text video", "pdf_keywords": "aware cascade contrastivelearning;cascade contrastivelearning;video language pretraining;cascade contrastive learning;aware cascade contrastive;contrastive learning token;text video retrieval;similarities video text;video text pairs;improves text video;learning method video;improves contrastive training;improve video text;video text alignment;contrastive training using;contrastive training;learning token aware;video retrieval;contrastivelearning;learning language visual;learning token;text video;multimodal representation learning;contrastive learning;learned representations effectively;video language;video text;video retrieval performance;variant contrastive learning;cascade contrastive"}, "e40a5c25d39d0f9add6a26c82613cf29edbcccf5": {"ta_keywords": "grade electroencephalogram eeg;electroencephalogram eeg investigate;electroencephalogram eeg;channel electroencephalograms eegs;consumer grade electroencephalogram;electroencephalograms eegs;use electroencephalogram eeg;grade electroencephalogram;user identification accuracy;electroencephalogram;based use electroencephalogram;accuracy user identification;channel electroencephalograms;electroencephalograms;use electroencephalogram;14 channel electroencephalograms;user identification based;electroencephalograms eegs 25;user identification authentication;eegs 25 subjects;analyze brain waves;demonstrate user identification;user identification;identification accuracy significantly;identification authentication;identification accuracy;identification based;eegs;dimensionality reduction technique;brain waves", "pdf_keywords": ""}, "92bd9e8a83e82dbbcafd8cde4f5a42d7bb4a5859": {"ta_keywords": "paraphrasing words;paraphrasing words using;method paraphrasing words;paraphrasing characteristic words;paraphrasing characteristic;paraphrasing;term memory;expansion paraphrasing characteristic;models expansion paraphrasing;term memory propose;method paraphrasing;language models;study language models;short term memory;expansion paraphrasing;language models expansion;words using;words using gram;propose method paraphrasing;gram clustering;using gram clustering;characteristic words;gram clustering article;characteristic words propose;study language;words propose method;study performance swimmer;words;using gram;performance swimmer", "pdf_keywords": ""}, "d1d23675d2e65cd734f2955c10ec1028b1139b5b": {"ta_keywords": "translation systems trained;faster machine translation;machine translation;machine translation systems;semantic similarity fluid;translation systems;better translations evaluated;translations evaluated human;semantic similarity;results better translations;translation accuracy propose;translation accuracy;translations evaluated;final translation accuracy;translations;work semantic similarity;human evaluation optimization;improve evaluation metrics;penalize semantically correct;better translations;improve final translation;new scoring metric;penalize semantically;scoring metric;semantic;translation;metric results better;similarity fluid;machine trained;differ lexically reference", "pdf_keywords": "machine translation learns;machine translation quality;learning machine translation;machine translation models;machine translation;translation machine translation;machine translation machine;machine translation ameliorated;new machine translation;translation quality measures;translation models;machine translation important;translation machine;translation learns automatically;translation models various;translation quality;translation learns;metric semantic similarity;using machine translation;level translation quality;years machine translation;translation quality order;semantic similarity;metric semantic;generate long sentences;minimum level translation;semantic similarity proposed;translation important research;semantic similarity called;training metrics"}, "a538a05864a23e2f80f9b003d5ecbdfb8025b954": {"ta_keywords": "feature representations tweets;classifier labelled tweets;detecting stance swimmer;stance swimmer video;representations tweets;representations tweets containing;swimmer video;tweets containing targets;train logistic regression;words autoencoder learn;targets training data;train logistic;tweets containing;tweets additional features;labelled tweets;training data;autoencoder learn;classifier;feature representations;targets training;regression classifier;bag words autoencoder;logistic regression classifier;stance swimmer;target specific training;detecting stance;tweets;representations used train;feature representations used;locomotion based observation", "pdf_keywords": ""}, "3376118362db3751cfbd88acd0c090b8a3897733": {"ta_keywords": "segmentation pretrained language;morphologically informed vocabulary;semantic representations derivationally;tasks derivational segmentation;improved morphologically informed;pretrained language models;derivational segmentation;derivational segmentation consistently;outperforms motor segmentation;morphologically informed;semantic representations;segmentation pretrained;words model;input segmentation pretrained;focusing semantic representations;motor segmentation;words model interpreted;improved morphologically;motor segmentation large;language models;pretrained language;vocabulary input tokens;complex words model;focusing semantic;informed vocabulary input;morphologically;derivationally complex words;semantic;semantic probing;plms improved morphologically", "pdf_keywords": "novel lexical semantic;words predict semantic;lexical semantic probing;lexical semantic;complex word processing;lexical level;subword morpheme vectors;complex words predict;represent complex words;mental lexicon;lexical level word;processes mental lexicon;representations complex words;level word embeddings;word processing driven;drawing insights psycholinguistics;predict semantic dimensions;word embeddings;lexical;predict semantic;novel lexical;complex words;word embeddings represent;subword morpheme;insights psycholinguistics;pre\ufb01xed complex words;lexicon drawing insights;present novel lexical;patterns lexical level;word processing"}, "1392df13a80a962057e979a294a850a50b7deb7e": {"ta_keywords": "supervised annotator;supervised annotator tradeoff;segmentation optimizes supervision;automatic annotations;annotations natural language;automatically segmenting corpus;automatic annotations natural;correcting automatic annotations;labels supervised annotator;annotating annotator data;segmenting corpus chunks;annotating annotator;annotator tradeoff segment;annotating;segmenting corpus;annotator data short;annotations;annotator;automatically segmenting;annotator data;annotations natural;annotator tradeoff;method annotating annotator;word segmentation;chunk human supervision;confident labels supervised;natural language efficient;method automatically segmenting;natural language processing;transcription word segmentation", "pdf_keywords": ""}, "8df3f3f72eb239eb212bd3fc929bd754ce2e03d6": {"ta_keywords": "lda topic modeling;related papers proteins;scientific articles proteins;topic modeling;articles proteins related;articles proteins;protein interaction networks;proteins related topic;topic modeling approach;papers proteins;identifying related papers;papers proteins block;interaction networks yeast;protein protein interaction;proteins related;protein interaction;documents graphs entity;topic coherence better;fusion entity annotated;proteins block lda;entity links;topic coherence;study protein protein;text documents graphs;documents graphs;entity links present;study protein;proteins;related papers;proteins block", "pdf_keywords": ""}, "aae3d5e24d02ae538030ef3995a86118c5323ae1": {"ta_keywords": "swim cluster storage;cluster storage;cluster storage systems;swim cluster;dynamics video bacteria;ability swim cluster;potential improve storage;bacteria;storage systems;improve storage efficiency;storage systems potential;improve storage;storage;storage efficiency;video bacteria;video bacteria used;storage efficiency exploiting;bacteria used;cluster;exploiting disk reliability;bacteria used increase;disk reliability;disk reliability heterogeneity;efficiency exploiting disk;fluid dynamics video;disk;fluid dynamics;increase ability swim;ability swim;swim", "pdf_keywords": ""}, "148bca569a0d2833f05df1297788f64bc6686fa8": {"ta_keywords": "temporal robustness nlp;nlp model trained;robustness nlp models;models nlp;temporal adaptation continued;research temporal robustness;nlp models;models nlp model;task performance previously;nlp models nlp;end task performance;temporal adaptation;robustness nlp;task performance;temporal robustness;model trained text;trained text;nlp model;temporal misalignment task;misalignment task performance;text data time;pretraining followed task;trained text data;task performance time;continued domainspecific pretraining;stronger effects temporal;adaptation continued pretraining;nlp;quantify effects temporal;temporal", "pdf_keywords": "temporal domain adaptation;adaptation mitigate temporal;tasks temporal domain;improve temporal robustness;improve temporal;end tasks temporal;mitigate temporal;tasks temporal;temporal degradation accross;short term memory;domain adaptation end;temporal robustness;adaptation end tasks;temporal domain;temporal misalignment training;temporal degradation;study temporal domain;temporal;temporal robustness models;domain adaptation;domain adaptation mitigate;degradation accross tasks;research improve temporal;tasks previous;accross tasks previous;term memory;considerable variation temporal;variation temporal degradation;adaptation end;mitigate temporal misalignment"}, "bb80f7d2269308c3e91da8c47b290645e9d3d7d5": {"ta_keywords": "learning rotation invariant;learning rotation;modeling human poses;dimensional projected poses;motions movemes training;basis discovery;projected poses;representation rotation;problem learning rotation;invariant reconstruct training;rotation invariant latent;synthetic representation movements;representation rotation invariant;human motions;basis discovery based;human motions movemes;activity classification inference;projected poses obtained;frame synthetic representation;modeling human;model training;poses;rotation invariant reconstruct;activity classification;human poses;primitive human motions;factor model training;training set dimensional;poses obtained;poses obtained images", "pdf_keywords": "activity recognition inference;dimensional human poses;activity recognition;approach activity recognition;dimensional bases poses;recognition inference action;poses focus learning;poses easily visualized;poses easily;rotation invariant latent;learning dimensional projections;poses;learning latent factor;activity recognition based;bases poses;poses investigate practical;rotation invariant representation;human poses;bases poses easily;applications activity recognition;learned representation;action dynamics observing;range human poses;approach learning latent;learning latent;human poses investigate;poses investigate;poses focus;realistic human motion;human poses focus"}, "ed1da1abf0f50ca758e422fbd945f891b6cda690": {"ta_keywords": "dialog response retrieval;neural network paraphrase;chat oriented dialog;generate sequence dialogs;human conversation generate;sequence dialogs;paraphrase identification improve;network paraphrase identification;conversation generate sequence;dialog pair database;based dialog response;conversation generate;dialogs;example based dialog;dialog response;distributed word representations;human human conversation;human conversation;dialog;response retrieval;paraphrase identification;dialog uses;dialog pair;utilize recursive neural;sequence dialogs paper;based dialog;word representations;model dialog pair;oriented dialog;response retrieval previous", "pdf_keywords": ""}, "7596030e0ce7a8df2f43e6ba4e9de5fa4a19240f": {"ta_keywords": "gradient descent ascent;extragradient algorithm exploration;extragradient algorithm;stepsize extragradient algorithm;gradient descent;step extragradient methods;extragradient methods;descent ascent schemes;step extragradient;stepsize extragradient;problem gradient descent;extragradient methods staple;double stepsize extragradient;update step extragradient;descent ascent;ascent schemes;extragradient;ascent schemes article;approach problem gradient;ascent;gradient;machine learning derive;problem gradient;algorithm exploration step;learning derive;exploration step;exploration step evolves;learning derive sharp;saddle point problems;algorithm exploration", "pdf_keywords": "extragradient algorithm converges;extragradient algorithm;gradient descent extragradient;stochastic gradient descent;extragradient method solution;extragradient method;extragradient dseg methods;descent extragradient method;solution congestion games;propose stochastic gradient;paper extragradient algorithm;update stochastic variational;stochastic hamiltonian gradient;stochastic gradient;hamiltonian gradient methods;method solution congestion;stochastic variational inequalities;variational inequalities stochastic;descent extragradient;solution congestion;congestion games;extragradient dseg;gradient descent;convergence gradient based;extragradient;better convergence guarantees;study convergence gradient;stochastic variational;algorithm converges pseudomonotone;bilinear games deterministic"}, "0d1cd3baad7d0734c9bbb008a33e2d10846968cd": {"ta_keywords": "grammar induction supersupervised;unsupervised grammars induction;unsupervised grammar induction;induction unsupervised grammar;grammars induction unsupervised;categorial grammar induction;supersupervised categorial grammar;grammars induction;grammar induction;grammar induction presented;unsupervised grammars;induction supersupervised categorial;unsupervised grammar;induction supersupervised;categorial grammar;induction unsupervised;grammars;supersupervised categorial;grammar;induction presented;induction;supersupervised;unsupervised;categorial;presented", "pdf_keywords": ""}, "739c2181f7894050a06b53b41ac5debe8ffc4829": {"ta_keywords": "generalization bounds sgd;stochastic gradient descent;sgd nonconvex deep;gradient descent sgd;properties stochastic gradient;generalization properties stochastic;nonconvex deep learning;stochastic gradient;descent sgd nonconvex;deep learning;bounds sgd assumption;sgd assumption trajectories;deep learning problems;bounds sgd;gradient descent;sgd nonconvex;descent sgd;accurately estimates generalization;sde representations;recent sde representations;generalization bounds;nonconvex deep;estimates generalization;sgd assumption;properties stochastic;capacity metric accurately;capacity metrics;existing capacity metrics;better generalization tail;stochastic", "pdf_keywords": "learning generalization overparameterized;stochastic gradient descent;generalization overparameterized neural;generalization behavior deep;learning generalization;overparameterized neural networks;trajectories stochastic learning;generalization learning;complexity trajectories stochastic;generalization overparameterized;novel generalization bounds;generalization learning task;overparameterized neural;stochastic gradient;generalization bounds;limited sgdin approximated;descent algorithm prediction;gradient descent;stochastic learning;tie generalization learning;deep neural;sgdin approximated;novel generalization;neural networks going;deep neural networks;complexity trajectories;generalization bounds case;learning task tail;size generalization;sgdin approximated feller"}, "94f8ef5944ecbdd0350d406bf3a16a7f2dff7349": {"ta_keywords": "speech separation recognition;recognition overlapped speech;output speech recognition;speaker speech recognition;overlapped speech sequences;speaker recognition;speaker recognition movement;speech recognition;multi speaker recognition;speech sequences;new automated speech;speech recognition model;multi output speech;automated speech;automated speech recognition;speaker speech separation;speech separation;neural beamformer;monaural multi speaker;neural beamformer multi;speech recognition based;multi speaker speech;speech recognition paper;source neural beamformer;speech sequences paper;overlapped speech;channel multi speaker;output speech;multi source neural;multi speaker", "pdf_keywords": "recognition multi speaker;multispeaker speech recognition;multichannel multi speaker;channel multispeaker speech;model multi speaker;output speech recognition;speaker speech recognition;channel multi speaker;multi speaker end;channel multispeaker;speech separation recognition;speech recognition model;multi channel multispeaker;multi speaker;multi output speech;speech recognition whsj1;multi channel input;input multi channel;speech recognition;neural beamformer multi;multispeaker speech;multi source neural;speaker end;multispeaker;speech recognition comprised;channel input multi;multi speaker speech;speaker end end;source neural beamformer;speech recognition problem"}, "ab193c05bc447f368565c1ff37064b1c517a750f": {"ta_keywords": "exploration deep learning;learning agents dialogue;greedy boltzmann exploration;exploration deep;strategies greedy boltzmann;boltzmann exploration;efficiency exploration deep;boltzmann exploration bootstrapping;learning agents;exploration strategies greedy;agents dialogue systems;deep learning agents;algorithm learns;algorithm learns faster;spiking replay;learns;learns faster;replay buffer experiences;learning feasible;women algorithm learns;agents dialogue;greedy boltzmann;episodes make learning;spiking replay buffer;exploration strategies;dialogue systems;improves efficiency exploration;strategies greedy;competing agents;learning feasible fail", "pdf_keywords": "dialogue policy learning;learning dialogue policies;learning agents dialogue;learning dialogue;agents dialogue systems;learning learning dialogue;dialogue systems;dialogue policies;dialogue policies spoken;dialogue achieve task;dialogue policy;approach dialogue policy;deep reinforcement learning;policies spoken dialog;agents dialogue;policy learning;policy learning learning;dialog systems challenging;spoken dialog systems;learning agents;dialogue achieve;deep reinforcement;spoken dialog;dialog systems;deep learning agents;reinforcement learning dia;approach dialogue;dialogue;present deep reinforcement;reinforcement learning"}, "2b5d553cb2f298f36aff1a1519f7f2f6be4db5da": {"ta_keywords": "taxonomy expansion;study taxonomy expansion;self organized taxonomy;taxonomy based self;methods taxonomy expansion;taxonomies new concept;taxonomy expansion 11;terms taxonomies;expand existing taxonomies;existing taxonomies;taxonomies new;taxonomy expansion problem;taxonomies;query terms taxonomies;organized taxonomy;terms taxonomies used;introduce new taxonomy;organized taxonomy based;taxonomy mini paths;benchmarks study taxonomy;taxonomy;existing taxonomies new;taxonomies used;taxonomy based;new taxonomy mini;paths query terms;new concept terms;new taxonomy;taxonomy mini;propelled propelled particles", "pdf_keywords": "predicting taxonomy expansion;automatic taxonomy expansion;information taxonomy expansion;automatic taxonomy;methods predicting taxonomy;taxonomy expansion task;taxonomy expansion natural;taxonomy expansion;structure automatic taxonomy;methods hypernymy detection;performs taxonomy expansion;predicting taxonomy;self supervised framework;self supervised learning;expansion natural supervision;taxonomies text corpora;propose self supervised;self supervised data;taxonomy expansion multi;self supervised;taxonomies text;existing taxonomies text;existing methods hypernymy;hypernymy detection;hypernymy detection mainly;supervised learning task;taxonomy expansion integrating;contributions self supervised;supervised;problem taxonomy expansion"}, "433b24d63146605d25c0c271062e129608462f03": {"ta_keywords": "subspace pursuit method;pursuit method kernel;mixtures subspace pursuit;plane subspace pursuit;subspace pursuit;reducing dimensionality kernel;speech recognition nonlinearly;nonlinear classification;achieve nonlinear classification;support vector machines;kernel log linear;recognition nonlinearly;method reducing dimensionality;approximate feature space;dimensionality kernel spaces;dimensionality kernel;pursuit method proposed;reducing dimensionality;cutting plane subspace;vector machines generalized;pursuit method;basis vectors log;recognition nonlinearly transformed;vectors log linear;nonlinear classification original;dimensional features;log linear models;high dimensional features;feature space using;kernel spaces", "pdf_keywords": ""}, "a0e0ce316ce0fdca2db61a52fdc7100e24906075": {"ta_keywords": "outlier detector xstream;ensemble outlier detector;featureevolving streams;xstream address outlier;based ensemble outlier;ensemble outlier;outlier detection;problem featureevolving streams;outlier detector;algorithm measures outlierness;detector xstream extreme;address outlier detection;measures outlierness;streaming scenarios xstream;detector xstream;static row streaming;streams;addresses outlier detection;row streaming scenarios;outlierness;xstream extreme streaming;outlier detection problem;xstream extreme;outlierness multiple;density based ensemble;outlier;streaming scenarios;measures outlierness multiple;row streaming;featureevolving streams letter", "pdf_keywords": ""}, "1a0d8dbd0252193abe9d64f72fc56cc1f05ed3eb": {"ta_keywords": "improve situational reasoning;structured situational graph;situational graphs especially;situational reasoning elicit;situational graphs;generated situational graphs;situational reasoning;situational graph st;situational graph;explicitly structured situational;situations st graphs;situational reasoning end;structured situational;improve situational;input generated situational;natural language queries;generated situational;goal situational reasoning;natural language;reasoning end task;situational;reasoning goal situational;situations st;reasoning elicit;reasoning elicit consequences;situations;build graph relevant;using natural language;language model;multi hop reasoning", "pdf_keywords": "generate situational reasoning;situational reasoning datasets;situational reasoning increasingly;situational reasoning graph;situational reasoning framework;effective situational reasoning;reasoning tasks general;reasoning tasks;general situational reasoning;require situational reasoning;multiple reasoning tasks;situational reasoning;situations machines;tasks require reasoning;generate situational;reasoning situations compose;reasoning datasets;reasoning situations;iteratively generate situational;st reasoning graph;tasks require situational;task st generation;unexpected situations machines;situations machines expected;reasoning increasingly;reasoning framework;reasoning increasingly observed;generalizes multiple reasoning;reasoning datasets validated;tasks general situational"}, "8529af634b443427d87d62d64467d2f1adfc230f": {"ta_keywords": "clustering malware samples;method clustering malware;clustering malware;characterizing malware samples;identifying malware sample;multi modal clustering;identifying malware;identification characterization malware;characterization malware samples;modal influence clustering;modal clustering;method identifying malware;malware sample malware;malware samples;detecting characterizing malware;malware samples primarily;characterization malware;characterizing malware threat;characterizing malware;sample malware;modal clustering problem;results characterizing malware;sample malware results;malware sample;malware threat actor;malware;malware samples threat;malware threat;malware results indicate;malware results", "pdf_keywords": ""}, "8424dd233577e3bd3fbd7ecdfd8b4d442531a20e": {"ta_keywords": "linear regression hmms;regression hmms;hidden markov models;regression hmms considering;models hmm;models hmm experiments;model hidden markov;markov models hmm;continuous speech recognition;speech recognition;hmms considering regression;hidden markov;marginalized log likelihood;vocabulary continuous speech;markov models;speech recognition confirm;continuous speech;likelihood linear regression;variational techniques gaussian;log likelihood linear;regression tree;realizes fully bayesian;regression tree structure;fully bayesian;techniques gaussian clusters;hmm experiments;likelihood linear;adaptation data;log likelihood;speech", "pdf_keywords": ""}, "7b72f79015a0a5e06cc019bae78f268b16a8e659": {"ta_keywords": "discriminative training deep;speech deep neural;outperformed discriminative trained;sequence discriminative training;discriminative training;discriminative trained model;discriminative trained;low rank discriminatively;noisy speech deep;rank discriminatively trained;svd sequence discriminative;discriminatively trained model;model outperformed discriminative;discriminatively trained;speech deep;speech recognition;outperformed discriminative;speech recognition number;training deep neural;rank discriminatively;effective noisy speech;sequence discriminator training;singular value decomposition;improve performance dnn;deep learning;sequence discriminative criterion;training deep;automatic speech;construct sequence discriminative;deep neural networks", "pdf_keywords": ""}, "0ea90d783a76b7c119fb5471fc71b6bc2defa06d": {"ta_keywords": "distributed storage;node distributed storage;efficient recovery data;distributed storage rest;read download reduced;storage;proof nonlinear code;storage rest nodes;read download simultaneously;nonlinear codes;download reduced;achievable data read;efficient recovery;data read download;problem efficient recovery;applies nonlinear codes;distributed;download simultaneously;proof information theoretic;node distributed;storage rest;individual node distributed;code nonlinear;achievable data;nonlinear code nonlinear;information theoretic;nonlinear code;stored individual node;read download download;code nonlinear equations", "pdf_keywords": "codes distributed storage;problem distributed storage;constructing codes distributed;distributed storage;distributed storage data;codes distributed;distributed storage context;storage data;regenerating codes;regenerating codes particular;storage;recovered remaining nodes;storage data stored;bounds achievable data;stored individual node;storage context;distributed;storage context regenerating;node ef\ufb01ciently recovered;recovery data;achievable data read;constructing codes;codes simultaneously achieve;achievable data;context regenerating codes;explicit codes simultaneously;needed recovery data;linear codes explicit;provide explicit codes;linear codes"}, "2030551b2590fa70eb5132131e6627c93128f0a1": {"ta_keywords": "robust utility learning;utility learning methods;utility learning non;estimating utility functions;utility learning;parametric utility learning;method estimating utility;estimating utility;utility functions players;learning non cooperative;utility functions;estimated mixture utilities;mixture regression models;multiple utility functions;adaptation mixture regression;game defined estimated;mixture regression;cooperative continuous games;modeling non cooperative;accurate robust utility;utility functions non;robust utility;ensemble methods;cooperative agents mixture;games using probabilistic;agents mixture utilities;ensemble;learning methods constrained;combining multiple utility;ensemble methods bagging", "pdf_keywords": ""}, "b34f254083b012bafd9b5ebd6c27450b4213c984": {"ta_keywords": "captions dense information;captions biomedical publications;captions biomedical;understanding captions biomedical;caption understanding recall;classification task captions;caption understanding;understanding captions;captions;standard information extraction;information extraction;captions dense;extraction scientific knowledge;automated extraction;extraction classification task;classifying image pointers;publications extracting classifying;biomedical publications extracting;automated extraction scientific;caption;task captions dense;techniques automated extraction;extraction classification;tool caption understanding;accurate tool caption;number automated learning;extracting classifying image;information extraction methods;extracting classifying;task captions", "pdf_keywords": ""}, "4a19211e077bc4ada685854245ba9fab381cbb06": {"ta_keywords": "relation extraction;including relation extraction;evaluation information extraction;information extraction;extracting structured information;relation extraction open;unstructured texts benchmark;question answering;flexible question answering;information extraction technique;question answering qa;extracting structured;texts benchmark;texts benchmark contains;technique extracting structured;structured information unstructured;structured information;absolute value item;item absolute value;information unstructured texts;informal relation specifications;information unstructured;high quality relation;item absolute;value item absolute;relation triples sentences;human evaluation information;answering qa approaches;relation specifications;unstructured texts", "pdf_keywords": "question answering;question answering machine;semantic roles parsing;learning semantic roles;machine reading comprehension;approaches question answering;answering machine reading;question answering qa;\ufb02exible question answering;answer benchmarks relations;roles parsing;features sentence parsing;roles parsing propose;answer answer benchmarks;learning semantic;tuples unstructured text;parsing;answering qa;sentence parsing results;answer benchmarks;sentence parsing;semantic roles;reading comprehension;answering qa approaches;extract relation tuples;relation tuples unstructured;relation tuples input;machine learning semantic;answering machine;reading comprehension mrc"}, "409280796e924bfe71421fe5bf4986bd3591ea72": {"ta_keywords": "query answering;based query answering;identifiers database management;identifiers database;web based query;query answering answers;object identifiers database;short queries efficient;real world databases;natural language objects;efficient typical queries;short queries;queries efficient;faster naive inference;proposed databases;databases;wide variety databases;answer substitutions ranked;answering answers query;world databases;problem proposed databases;databases contain informal;queries real world;proposed databases contain;databases available world;database;typical queries;typical queries real;queries efficient typical;answers query list", "pdf_keywords": ""}, "4e3016617e5e254bafebcbd7e96c509f670bdd37": {"ta_keywords": "music performance synthesis;synthesize musical score;model synthesize music;existing piano dataset;generative audio model;synthesize musical;piano dataset overall;measure performance music;synthesize music;generative audio;piano dataset;score audio music;aims synthesize musical;conditional generative audio;performer score audio;audio music performance;music performance;new violin dataset;score audio;performance music;polyphonic inputs;violin dataset;level music performance;recordings scores;handling polyphonic inputs;audio music;musical score natural;polyphonic inputs providing;techniques handling polyphonic;handling polyphonic", "pdf_keywords": "musical score synthesis;audio music synthesis;music performance synthesis;music synthesis;piano dataset achieves;model piano dataset;novel score audio;piano dataset;music synthesis consists;text speech synthesis;baseline violin dataset;model piano music;speech synthesis;music synthesis paper;score audio music;score audio;violin dataset;score synthesis;violin recordings scores;quality violin recordings;alignment model piano;stage score audio;speech synthesis present;violin recordings;piano music;recordings scores;pipeline score audio;baseline model piano;musical score;violin dataset paper"}, "2a39a4f2d18e376ef8a6e2f45416e7b87957481e": {"ta_keywords": "multi task learning;text normalization;task learning;historical text normalization;text normalization suffers;text normalization benefits;large corpus;learning noisy data;task learning used;learning noisy;properties large corpus;corpus human speech;normalization;task learning paper;shown text normalization;task learning context;normalization suffers;machine learning historical;corpus;human speech;large corpus human;approach learning noisy;multi task;normalization benefits multi;context machine learning;concept multi task;learning historical text;noisy data;normalization suffers small;normalization benefits", "pdf_keywords": ""}, "bd17620c6cb5ca97ef773499223d1509d123745f": {"ta_keywords": "machine learning scientist;make deep learning;jupyter notebooks;deep learning;written jupyter notebooks;learning scientist iii;learning;deep learning approachable;open source book;applied machine learning;learning scientist;interactive examples;math interactive examples;book written jupyter;interactive examples self;jupyter notebooks seamlessly;open source;learning approachable;resource freely;machine learning;resource freely available;source book;math interactive;code entire book;written jupyter;notebooks seamlessly integrating;jupyter;resource;applied machine;seamlessly integrating exposition", "pdf_keywords": "attention models deep;attention models;attention functions extensively;deep learning powerful;deep learning;models deep learning;d2l deep learning;deep convolutional neural;deep convolutional;deep learning paper;deep learning framework;attention functions;models deep;present deep learning;introduce attention functions;bag words model;network d2l deep;attention;inference introduce attention;train neural;learn massive text;particular deep learning;massive text data;vision natural language;convolutional neural;convolutional neural network;machine learning focus;natural language inference;neural network d2l;words model"}, "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896": {"ta_keywords": "code tokens identifiers;code defect detection;code pre trained;distinguish code tokens;code tokens;code semantics conveyed;language present codet5;code semantics;novel identifier aware;identifier aware pre;code related tasks;distinguish code;generation language language;transfer programming languages;identifier aware;clone detection generation;code pre;codet5 unified pre;leverages code semantics;present codet5 unified;code related;generation language;decoder pre training;models natural languages;codet5 significantly outperforms;pre trained encoder;natural languages nl;detection generation tasks;codet5 unified;semantics conveyed developer", "pdf_keywords": "training code translation;trained programming language;pre trained programming;rich code semantics;code translation aims;code translation;code generation;code code generation;trained programming;programming language;capture semantic;code pre training;better capture semantic;code semantics;programming language work;programming language pl;code generation model;software code pre;trained methods programming;bimodal dual learning;capture semantic information;programming;code semantics paper;challenge software code;semantic information code;pre training code;methods programming language;generation multi task;software code;training code"}, "70bc4dc0bc72816773006c71b56fa5885c729caa": {"ta_keywords": "neural music generation;model generate music;generate music;networks gans generate;music generation model;gans generate;neural music;music generation;new neural music;networks gans;generative adversarial networks;generative adversarial;gans generate multi;potential generative adversarial;generate music scratch;generative;adversarial networks gans;gans;exploit potential generative;generate multi track;track pop rock;rock music bars;convolutions generators;rock music;potential generative;pop rock music;using convolutions generators;adversarial;music;generating", "pdf_keywords": ""}, "78c9181abe18575925fbbb6e6d8c72d7bf90d06d": {"ta_keywords": "decentralized mac protocol;delay wireless communications;mac protocol;decentralized hybrid mac;new decentralized mac;delay wireless;decentralized mac;reduce delay wireless;mac protocol term;wireless communications;wireless communications presence;delay ii distributed;control signals protocols;channel seek mac;signals protocols;nodes sharing time;signals protocols achieve;protocol;protocols;time slotted wireless;presented reduce delay;reduce delay;communications presence transmission;distributed control central;protocols achieve;wireless channel;running 6tisch communication;wireless channel seek;delay;delay ii", "pdf_keywords": "distributed scheduler wireless;delay mac protocol;hybrid mac protocols;mac protocols;scheduler wireless networks;scheduling algorithm polling;mac protocol;mac protocol ee;distributed scheduler;scheduler wireless;wireless channel centralized;transmit generic scheduling;present distributed scheduler;wireless networks cyclic;nodes sharing time;delay mac;generic scheduling policies;optimal scheduling;queues allowed transmit;wireless networks;time slotted wireless;algorithm polling;protocols;generic scheduling;scheduling policies;scheduling;optimal scheduling setting;channel centralized;channel centralized control;algorithm polling paper"}, "9bbdcc03d872987eef9165f4a63c3878a5b05189": {"ta_keywords": "dense text encoders;text representation encoders;encoder dense text;text encoders;text encoders aggregate;encoders aggregate text;novel encoder dense;dense encoder;representation encoders;encoder dense;train dense encoder;text representation;representation encoders paper;novel encoder;presents novel encoder;encoder;trained transformer language;text information dense;transformer language models;encoders;dense encoder experiments;encoders paper propose;representation pre trained;encoder experiments condenser;dense text;pre trained transformer;information dense representation;encoders paper;trained transformer;transformer language", "pdf_keywords": "trained dense retrieval;answering retrieval;retrieval pre trained;answering retrieval web;question answering retrieval;retrieval question answering;long document retrieval;search retrieval pre;dense retrieval systems;similarity tasks retrieval;sentence similarity tasks;dense retrieval;similarity question answering;question answering web;question answering;retrieval pre;predict attention person;predict attention;tasks sentence similarity;attention person sentence;search retrieval;answering web search;retrieval systems;tasks retrieval;web search retrieval;able predict attention;retrieval web search;attention;retrieval web;retrieval"}, "2dd81061e0b11c828446f6a1843741ae51facbd2": {"ta_keywords": "backpropagation deep cortical;models cortical networks;cortical networks layer;cortical networks;deep cortical networks;layer cortical network;leaky neuronal dynamics;neurons phase advance;neuronal dynamics continuously;neuronal dynamics;cortical networks continuous;cortical network;neurons phase;cortical network characterized;biological neurons phase;models cortical microcircuitry;time leaky neuronal;deep cortical;detailed models cortical;models cortical;potential layer cortical;neurons;neuronal;backpropagation deep;cortical microcircuitry hierarchical;layer cortical;hierarchical models cortical;characterized synapse dynamics;network characterized synapse;synapse dynamics", "pdf_keywords": "neuromorphic implementation;neurophysiological approach neuromorphic;computation learning physical;neuromorphic implementation known;embodied learning machines;cortical processing networks;neuromorphic;learning physical substrates;approach neuromorphic;scalable model cortical;models cortical microcircuitry;synaptic connections learn;approach neuromorphic implementation;organized dendritic microcircuits;embodied learning;fast computation learning;detailed models cortical;model cortical processing;networks equipped synaptic;cortical microcircuitry introduce;learning physical;cortical microcircuitry approach;dendritic microcircuits;cortical microcircuitry;computation learning;connected convolutional architectures;hierarchically organized dendritic;inference training speed;models cortical;dendritic microcircuits make"}, "87f42406de084e60d2365adac8a159ed3e455856": {"ta_keywords": "streaming based anomaly;streamspot streaming based;heterogeneity streaming;streamspot streaming;streamspot streaming streaming;streamspot;heterogeneity streaming nature;streaming processing;persistent threat detection;strings streamspot streaming;streaming streaming;streaming nature;streaming streaming application;fully streaming processing;streaming processing edge;strings streamspot;streaming application;anomaly detection;streaming based;short strings streamspot;streaming;based anomaly detection;streaming application requires;streaming application exhibits;algorithm detection fast;fully streaming;anomalies real time;anomaly detection approach;advanced persistent threat;clustering iii fast", "pdf_keywords": "streaming based anomaly;graph based anomaly;locality sensitive hashing;anomaly detection description;anomaly detection;based anomaly detection;threat apt detection;anomaly detection goal;advanced persistent threat;timestamped typed graphs;sensitive hashing;detect graphs;persistent threat apt;hashing;proposed streamspot;graph representation approximates;typed graphs based;based anomaly;similarity function timestamped;anomaly;persistent threat;constantspace graph representation;graphs based shingling;activity various attack;locality sensitive;apt detection eliminations;apt detection;timestamped edges graph;sensitive hashing lsh;detect graphs given"}, "4cc70dd760c2c8cfc0107921bade45fb5efe860e": {"ta_keywords": "graph recommendation method;recommender uses knowledge;latent factorization graphs;techniques improve recommendation;factorization graphs;improve recommendation performance;collaborative filtering techniques;recommendation performance improving;collaborative filtering;recommendation method large;hybrid recommender;improve recommendation;graph recommendation;recommendation performance;recommender systems;novel hybrid recommender;based collaborative filtering;recommendation method;recommender systems using;improving performance recommender;recommender uses;improve performance recommender;knowledge graphs improve;factorization graphs paper;hybrid recommender uses;latent factorization;recommender systems demonstrate;recommender;knowledge graphs;knowledge graphs important", "pdf_keywords": ""}, "fa2125641578934de12d7f792b094ffcfdf82ee2": {"ta_keywords": "bose einstein condensate;text picture synthesis;crystal dilute bose;synthesis paper taiwanese;taiwanese language processing;synthesis;einstein condensate bec;picture synthesis;einstein condensate;language processing;language processing ttp;magnetization single crystal;condensate bec;language analysis subsystem;synthesis paper;temperature magnetization;picture synthesis paper;condensate;bose einstein;dilute bose einstein;temperature magnetization single;subsystem stage processing;subsystem processing stage;dilute bose;processing stage described;operates natural language;subsystem processing;taiwanese language;stage processing subsystem;processing stage", "pdf_keywords": ""}, "c818f9be503a1ed94f991a2949c29e3ee477e8b8": {"ta_keywords": "noisy speech recognition;speech recognition noisy;reverberated noisy speech;speech recognition tasks;speech recognition;decoding features noisy;speech recognition asr;improve automatic speech;noisy speech;samples automatic speech;learning decoding features;input features decoding;input features deep;automatic speech recognition;recognition noisy;automatic speech;speech enhancement;capable learning decoding;features decoding;deeper neural networks;decoding features;recognition noisy environments;learning decoding;noise suppression speech;performance gaussian mixture;approach automatic speech;recognition asr based;features deep neural;asr based probabilistic;recognition asr", "pdf_keywords": ""}, "da1d5dc331c2839dfc3e6a79ee17f3bdf2231a8b": {"ta_keywords": "spins dimensional;lyapunov exponents rlxs;rr lyapunov exponents;classify rr lyapunov;heisenberg antiferromagnet;spins dimensional 2d;pair spins dimensional;heisenberg antiferromagnet afm;2d heisenberg antiferromagnet;entanglement pair spins;list letter entanglement;antiferromagnet;lyapunov exponents;letter entanglement;rrxs set expansion;rr lyapunov;classification rrxs set;entanglement;antiferromagnet afm;letter entanglement pair;antiferromagnet afm strongly;classification rrxs;rrxs set;based classification rrxs;entanglement pair;spins;exponents rlxs based;dimensional 2d heisenberg;exponents rlxs;rrxs", "pdf_keywords": ""}, "eb28e82ca0bbc5d83e1cc07807da16874105d2fa": {"ta_keywords": "verb class clustering;triples based word;word embeddings;representing syntactic pair;representing syntactic;triple vector representation;syntactic pair triples;clustering algorithms frame;word embeddings cast;based word embeddings;representation syntactic;clustering triadic;generalization clustering triadic;clustering triadic data;triclustering problem generalization;algorithms frame induction;framenet derived dataset;clustering task document;representing triple;class clustering task;clustering task;vector representing triple;approach triframes;method representing syntactic;based approach triframes;vector representation;clustering;verb object svo;generalization clustering;class clustering", "pdf_keywords": ""}, "572535aff31c400578fdd75313c896c0650b2d4c": {"ta_keywords": "datasets automatic speech;recognition speech;speech data;speech recognition;automatic speech;clean speech data;speech recognition speech;automatic speech recognition;recognition speech recognition;speech recognition popular;dftnet;optimize conventional speech;paradigm automatic speech;demonstration term dftnet;dftnet shown achieves;speech data paper;noisy clean speech;term dftnet;analysis moving bodies;moving bodies based;speech recognition ability;speech components;recognition;conventional speech components;dftnet shown;speech components end;moving bodies;motion far field;denoising;motion moving body", "pdf_keywords": ""}, "8495c1722e1f5107733c842839c2d298b9116921": {"ta_keywords": "conversation history modeling;conversational question answering;conversation history conversational;turn conversational search;conversational search powerful;conversational search;conversation history;answer embedding convolutional;history answer embedding;conversational search provide;setting conversational search;insights conversation history;history conversational;question answering;answer embedding;history modeling convqais;question answering convqa;integration conversation history;turn conversational;embedding convolutional search;prepend history turns;multi turn conversational;setting conversational;bidirectional encoder representations;bert bidirectional encoder;conversation;conversational;history turns current;convolutional search;conversational question", "pdf_keywords": "conversation history convq;answer embedding based;model conversation history;history answer embedding;conversational search;answer embedding;conversation history;conversation history giving;handle conversation history;answer embedding method;conversation history convqa;conversational search provide;framework answer embedding;setting conversational search;embedding information introduce;history convq;embedding information;model conversation;convq propose history;conversation;embedding based idea;setting conversational;handle conversation;extra embedding information;concrete setting conversational;conversational;framework handle conversation;embedding based;history convq conduct;history giving tokens"}, "14f925098d57b0fa491a100fa73e52dbc764efa6": {"ta_keywords": "cognition fluid dynamics;antineutrino interaction framework;cognition fluid;theory cognitive neurobiology;study cognition fluid;neutrino antineutrino interaction;cognitive neurobiology;neutrino;lipkin meshkov glick;state lipkin meshkov;neutrino antineutrino;topology ground state;antineutrino interaction;describes track neurois;lipkin meshkov;topology ground;search neutrino;cognitive neurobiology applied;neurobiology;antineutrino;search neutrino antineutrino;meshkov glick;cognition;dynamics video topology;results search neutrino;collider;track neurois;ground state lipkin;neurobiology applied;theory cognitive", "pdf_keywords": ""}, "32a2a8baf217d29f628d22d793cace95634f51d5": {"ta_keywords": "mozilla thunderbird;mozilla thunderbird popular;leak detection recipient;open source email;extension mozilla thunderbird;thunderbird;information leak detection;thunderbird popular;droplet;leak detection;mozilla;droplet liquid;email client;thunderbird popular open;email client present;static email;detection recipient recommendation;lgl droplet liquid;information leak;droplet immersed liquid;liquid crystal droplet;droplet liquid crystal;extension mozilla;lgl droplet;source email client;droplet placed;promising usage patterns;use static email;recipient recommendation;crystal droplet", "pdf_keywords": ""}, "1092da11519fe8427c8113b16a012f34f4a3fb6b": {"ta_keywords": "federated learning differential;privacy deep learning;learning differential privacy;federated learning;metrics federated learning;differential privacy;fairness privacy deep;presents federated learning;privacy deep;accuracy fairness privacy;federated learning model;fairness privacy;fairness metrics federated;differential privacy paper;privacy;learning differential;metrics federated;interplay accuracy fairness;federated;learning noisy;accuracy fairness;deep learning;dynamics video;fluid dynamics video;data fluid dynamics;privacy paper;privacy paper presents;fairness metrics;learning noisy noisy;learning", "pdf_keywords": "accuracy fairness privacy;federated learning;privacy training data;propose federated learning;federated learning based;privacy training;preserve privacy training;framework federated learning;fairness privacy;federated learning framework;phases fairness privacy;fairness privacy allowed;fairness privacy di;training data sensitive;guarantee training data;simultaneously preserve privacy;privacy;attribute ensuring fairness;accuracy fairness;preserve privacy;interplay accuracy fairness;privacy di;ensuring fairness;framework learns weighted;privacy di atumerentially;learns weighted;privacy allowed;federated;new framework federated;data sensitive attributes"}, "ed913bed529d6bb7beac3b6086a853698abf627d": {"ta_keywords": "home assistants spoken;speech interaction digital;speech recognition;automatic speech;automatic speech recognition;high quality speech;home assistants digital;statistical models speech;digital home assistants;speech recognition based;speech interaction;method automatic speech;deep learning;models speech;key speech processing;concept speech recognition;deep learning deep;learning deep learning;speech processing;spoken language interface;assistants spoken language;speech synthesis;speech synthesis sophisticated;data deep learning;learning deep;home assistants;quality speech synthesis;speech processing algorithms;assistants spoken;training data deep", "pdf_keywords": ""}, "914626e2e13bd42a5f06c28ff02ba7c428e81ff1": {"ta_keywords": "dyadic oscillators modelled;coupled dyadic oscillators;dyadic oscillators;oscillators modelled;complex coupled dyadic;oscillators modelled means;fluid dynamics;variational principle;fluid dynamics video;oscillators;complex coupled;dynamics;video complex coupled;dynamics video complex;dynamics video;coupled dyadic;variational;means variational principle;modelled means variational;coupled;video complex;means variational;modelled;complex;fluid;dyadic;modelled means;principle;video;means", "pdf_keywords": ""}, "e829ee7fe48f4b1e451378b6a21470b2f86c0aa6": {"ta_keywords": "replication based erasure;erasure codes;erasure coded systems;erasure code algorithm;based erasure storage;erasure code;erasure coded;erasure storage;explicit erasure code;capacity erasure coded;erasure codes establishes;private information retrieval;case erasure codes;erasure storage node;design explicit erasure;perfectly private information;capacity achieving codes;private information;replication based systems;record public database;concept private information;revealing server record;explicit erasure;applicable replication based;replication based;codes algorithms;results private information;explicit codes algorithms;capacity erasure;erasure", "pdf_keywords": ""}, "31603b3339f4da5bdc6b7de4231bd1ddfb32a50a": {"ta_keywords": "ode rnn based;based backpropagation observations;backpropagation observations neural;backpropagation observations;rnn based models;modelling temporal dynamics;modelling temporal;observations neural;temporal dynamics;ode rnn;observations neural ordinary;based backpropagation;adjoint based backpropagation;backpropagation;similar ode rnn;based subsequent observations;rnn based;temporal dynamics demonstrate;sampled multivariate time;trajectory based subsequent;rnn;subsequent observations;memory efficient adjoint;option modelling temporal;neural ordinary differential;observed irregularly sampled;modelling;temporal;series utilise memory;time series utilise", "pdf_keywords": "differential equation neural;neural networks model;equation neural;neural networks;neural network;approximation neural;use neural network;neural network approach;use neural networks;network neural;neural network neural;approximation neural controlled;use neural;neural controlled differential;differential equation models;modelling temporal dynamics;learning model diagnosis;model time series;equation neural cde;network neural ordinary;machine learning model;neural ordinary differential;neural cde model;model diagnosis borderline;modelling temporal;neural;universal approximation neural;learning model;model diagnosis;present neural network"}, "5f563da2843e005c4b236f7889e7a22631b53210": {"ta_keywords": "conference peer review;conference publications assessing;publications assessing quality;reviewer quality study;quality reviewer rating;peer review correlation;review correlation quality;peer review;publications assessing;reviewer quality;quality scores impact;reviewer rating;quality reviewer;quality individual researchers;improving reviewing;examined inconsistency conference;assessing quality;tier conference publications;inconsistency conference peer;researchers variation quality;correlation quality scores;quality scores;published correlation quality;reviewing;conference publications;review correlation;reviewer rating 50;suggestions improving reviewing;citation count;citation count suggestions", "pdf_keywords": "quality review paper;publication quality;model publication quality;subjectivity reviewer assessments;reviewer assessments;rate papers;quality paper published;publication quality based;conference reviewing;rate papers higher;review paper paper;random committee paper;reviews average con\ufb01dent;review paper;reviews average;papers higher quality;quality review;conference reviewing process;inconsistency conference reviewing;machine learning conferences;consequence subjectivity reviewer;paper revisits 2014;quality paper;quality based multinomial;paper revisits;di\ufb00ering quality review;conference eventual citation;reviewing;late reviews average;subjectivity reviewer"}, "dd5e54b08b2c1520d179e88cd524e9bd4fe1f6ab": {"ta_keywords": "complexity optimal transport;optimal transport;divergences interpolated regularized;dynamics sinkhorn divergences;sinkhorn divergences;known gogny divergence;sinkhorn divergences note;gogny divergence;optimal transport ot;divergences;_sinkhorn divergences;divergences note;divergence;called _sinkhorn divergences;complexity motion;brownian particles;complexity motion animals;_sinkhorn divergences interpolated;transport ot maximum;divergences interpolated;crowded environment quantified;transport;measures machine learning;bath brownian particles;brownian;complexity optimal;motion animals crowded;probability measures;ball independent measures;crowded environment", "pdf_keywords": "cantelli theorem regularization;approximating probability measure;theorem regularization whittaker;glivenko cantelli theorem;theorems sds bound;convergence theorems sds;sds bound approximation;cantelli theorem;approximating probability;theorem regularization;regularization whittaker;proof glivenko cantelli;bound empirical sinkhorn;theorems sds;sample complexity bound;problem approximating probability;ot sample complexity;regularized regularized ot;empirical sinkhorn divergences;depends regularization parameter;positive depends regularization;bound approximation;bound empirical;regularized ot;regularization;regularization parameters size;complexity bound empirical;balls propose probabilistic;depends regularization;regularization whittaker equation"}, "8b192503f119a0b0cc30ef5179a00f231c20fb93": {"ta_keywords": "modeling events;channel rnn;fake event epochs;rnn;multi channel rnn;rnn algorithm optimally;modeling events using;nonparametric deep neural;event epochs;rnn algorithm;propose nonparametric deep;nonparametric deep;approach modeling events;channel rnn algorithm;events;event epochs consecutive;deep neural;fake event;event sequences;introduction fake event;events various;event;event sequences sequences;interval event sequences;events various types;events introduction fake;inter event;events introduction;sequences events;deep neural network", "pdf_keywords": "event sequence prediction;modeling event sequences;neural event sequence;event streams sequences;model event prediction;modeling event data;event sequences time;learning model event;event prediction electronic;information modeling event;representation neural event;attentions modeling event;event model learn;modeling event;event models;event prediction;event sequences;models event models;event models seek;streams sequences events;neural event;temporal attention paper;dynamics event streams;data propose recurrent;propose recurrent neural;arrival epochs;graphical event model;event streams;recurrent neural;sequences events"}, "0ca2575a1ef73930dc2abe205b44e079eadc426c": {"ta_keywords": "model traffic flow;decision theoretic lane;model traffic;tolling schemes;lane changing models;design tolling schemes;tolling schemes improve;multiple lanes;traffic flow;theoretic lane changing;lane changing behavior;tolling;macroscopic model traffic;lanes multiple populations;traffic flow lane;lanes multiple;used model traffic;multiple lanes multiple;traffic flow incorporates;improve traffic flow;behavior lane changing;incorporates multiple lanes;lane road example;lanes;method design tolling;lane changing;design tolling;flow lane;theoretic lane;traffic", "pdf_keywords": ""}, "b5991b1018bb89b053a2c8229248f97956391bb5": {"ta_keywords": "automatically estimated pronunciation;pronunciation learning;iterative pronunciation learning;pronunciation learning non;estimated pronunciation variations;native pronunciation adapt;estimated pronunciation;non native pronunciations;native speech recognition;non native pronunciation;native pronunciations directly;driven iterative pronunciation;grapheme phoneme conversion;iterative pronunciation;acoustic model adaptation;pronunciations word automatically;native pronunciations;native pronunciation;pronunciation adapt;pronunciation adapt speaker;design pronunciation lexicon;speech non native;speech recognition automatically;speech using iterative;pronunciations directly speech;pronunciation lexicon;automatic speech recognition;non native speech;speech recognition;significantly native speech", "pdf_keywords": ""}, "b19cba7bfe318c69d5e62f8322cb5d75228452f4": {"ta_keywords": "self organized critical;organized critical systems;tuning weights models;scale language models;organized critical;large scale language;weights models millions;liquid solid critical;critical systems;weights models;low rank hypercomplex;scale language;rank hypercomplex propose;model task paper;language models;tuning large scale;hypercomplex propose;solid critical;hypercomplex;language models better;fine tuning weights;rank hypercomplex;model task;tuning weights;tradeoff task performance;based model competition;design self organized;model competition;weight based model;model competition based", "pdf_keywords": "methods predicting paraphrased;predicting paraphrased sentences;predicting paraphrased;parameters trained task;multi task learning;trained task soft;pretrained language models;parameters trained;task learning;optimization pretrained language;language models;trained task;low rank parameterization;task learning replace;task soft prompt;rank parameterization;paraphrased sentences code;task low rank;rank optimization pretrained;model multi task;task soft;rank parameterization methods;soft prompt model;compact machine learning;paraphrased sentences;soft prompt methods;learning methods predicting;pretrained language;learning methods;low rank optimization"}, "3ba013b8b56646d66aac8472fb90a5c029ef55a0": {"ta_keywords": "differentiation algorithm neural;learn optimal trade;automatic differentiation algorithm;stochastic optimization;automatic differentiation;optimal trade model;differentiation algorithm;stochastic optimization problems;learn optimal;derivatives solution trajectories;learned dynamics;class stochastic optimization;mode automatic differentiation;learned dynamics easier;optimal trade;numerical solvers;derivatives;neural networks;solution trajectories differentialialial;dynamics easier solve;optimization;algorithm neural;paper learn optimal;trade model;differentiable surrogate time;differentiation;algorithm neural networks;trajectories differentialialial equilibria;numerical solvers using;higher order derivatives", "pdf_keywords": "learned dynamics;learn dynamics;dynamics machine learning;learn state dynamics;regularizing dynamics;learn dynamics faster;learned dynamics easier;variational autoencoder;training learn dynamics;method regularizing dynamics;regularizing dynamics ffjord;neural network learn;variational autoencoder architecture;uses variational autoencoder;autoencoder architecture learn;regularizing dynamics discrete;network learn differential;proposed regularizing dynamics;learning model trained;recurrent neural networks;neural networks computationally;encourages learned dynamics;equations parameterized neural;learn differential;trained neural;solvers training;model trained;dynamics faster solve;learn differential equations;autoencoder"}, "26299d5fdc5137291dc6a091573b3d18aba1d1c2": {"ta_keywords": "adapting pretrained multilingual;pretrained multilingual applications;pretrained multilingual model;multilingual model pretrained;model pretrained multilingual;pretrained multilingual;applications multilingual bert;multilingual bert;multilingual model new;presents multilingual model;multilingual model;multilingual bert xlm;languages unseen pretraining;model new language;arbitrary tasks languages;multilingual applications;tasks languages;multilingual applications multilingual;lingual transfer representative;cross lingual transfer;presents multilingual;lingual transfer;multilingual;applications multilingual;languages named entity;cross lingual;new language;transfer arbitrary tasks;low resource languages;diverse languages named", "pdf_keywords": "multilingual models crosslingual;pretrained multilingual models;models crosslingual language;adapt pretrained multilingual;crosslingual word embeddings;models better crosslingual;crosslingual language model;multilingual models better;multilingual multilingual models;multilingual models;model crosslingual word;multilingual pretraining model;multilingual models propose;models crosslingual;multilingual vocabulary target;target language adaptation;pretrained multilingual;model crosslingual;pretraining model crosslingual;shared multilingual vocabulary;multilingual pretraining;better crosslingual generalisation;diverse languages tasks;limits pretrained multilingual;cross lingual transfer;multilingual vocabulary;translation machine translation;present multilingual pretraining;language model pretraining;crosslingual language"}, "cce8cf3d7f45113a4cba984b878802a5b16d5967": {"ta_keywords": "speaking style transformation;statistical machine translation;translation approach speaking;speaking style;machine translation;approach speaking style;machine translation approach;translation approach;style transformation presented;style transformation;approach speaking;monotonic statistical machine;statistical machine;translation;transformation;transformation presented;speaking;monotonic statistical;statistical;style;machine;monotonic;presented;approach", "pdf_keywords": ""}, "77ce1b8d425b7538c21ce0be976ee24a58e797c1": {"ta_keywords": "channel source separation;nonnegative matrix factorization;source separation based;source separation accurately;matrix factorization nmf;source signals mixtures;signals mixtures generalization;optimization reconstruction basis;recover source signals;matrix factorization;source separation;factorization nmf;reconstruction basis functions;single channel source;separate analysis reconstruction;analysis reconstruction basis;factorization nmf popular;signals mixtures;optimization reconstruction;source optimally recovered;source signals;optimize reconstruction;approach single channel;filtering step snr;reconstruction basis;mixtures generalization frees;nmf basis functions;mixtures generalization;training nmf basis;frees optimize reconstruction", "pdf_keywords": ""}, "5e8c52ddbd3581320f7e536b7cd10d7263b81eb2": {"ta_keywords": "representation learner automatically;features learning agents;representation learner simulated;use representation learner;learning agent;learning agents;learner automatically generate;representation learner;create features learning;learner automatically;integrated representation learner;learning agents able;simulate human learning;learner simulated;unsupervised grammar induction;learning agent perform;intelligent agents simulate;grammar induction algorithm;developing intelligent agents;constructed feature predicates;intelligent agent developers;science learning agent;learning developing intelligent;grammar induction;feature predicates based;feature predicates effort;intelligent agent;algorithm acquires representations;manually constructed feature;artificial intelligence cognitive", "pdf_keywords": ""}, "4481244de2cc0c55d91cebbb152eec79a76386f3": {"ta_keywords": "logic device fabricated;pressure cooker;die logic device;density films thermal;pressure cooker test;films thermal compression;density films;logic device winnerless;winnerless repeater fluid;logic device;results pressure cooker;high density films;consisting logic device;underfill material adhesive;device fabricated;films thermal;thermal compression bonding;repeater fluid;compression bonding;device fabricated middle;adhesive substrates;material adhesive substrates;assembly underfill material;repeater fluid dynamics;demonstrate assembly underfill;compression bonding method;underfill material;nonconductive film used;material adhesive;cooker test", "pdf_keywords": ""}, "d26254cf3ec537f37708afaaf7f5a76a7922d4a2": {"ta_keywords": "statistical machine translation;machine translation;machine translation compared;translation quality statistical;word reordering models;reorderings source words;improve translation quality;reorderings word pairs;reordering source words;threshold improve translation;translation quality;word reordering information;helpful word reordering;statistical models reordering;existing word reordering;learn reorderings word;word reordering;enhance hierarchical phrase;hierarchical phrase based;models word pairs;reordering models learn;reorderings word;models reordering source;hierarchical phrase;reordering information improves;improve translation;models learn reorderings;reordering sub models;word pairs distances;learn reorderings source", "pdf_keywords": ""}, "10e221c7d4636703c5c97b54f53b1cb57c25f3a6": {"ta_keywords": "weighting deep networks;importance weighting deep;regularization method loss;weighting used regularization;linear networks trained;gradient descent;networks trained;regularization;stochastic gradient descent;l2 regularization;optimized stochastic gradient;regularization methods;deep networks;l1 regularization;model importance weighting;regularization methods recently;deep networks results;l5 l1 regularization;used regularization;regularization lead;minimizing loss mechanical;loss optimized;regularization method;minimizing loss;unregularized linear networks;importance weighting;use l2 regularization;loss optimized stochastic;learned model;linear networks", "pdf_keywords": ""}, "6dfecb5915e8b10841abe224c5361bbda7100637": {"ta_keywords": "morphological parsers tigrinya;languages based parser;parsers tigrinya;parsers;parsers produce;morphological parsers;based morphological parsers;based parser combinator;parsers tigrinya oromo;parser;parser combinator;paradigm quantum computation;based parser;parser combinator finite;quantum computation based;quantum computation;idea quantum ergodicity;parsers produce multiple;quantum ergodicity;new paradigm quantum;tigrinya oromo languages;morphological segmentation english;based morphological;rule based morphological;dynamics tigrinya;quantum;paradigm quantum;oromo languages based;morphological;tigrinya oromo bifurcations", "pdf_keywords": ""}, "73e401dead436aabd0cd9c941f7b13bfdeda9861": {"ta_keywords": "dynamics crowds;model dynamics crowds;gradient communication;training point compressors;dynamics crowds men;communicationefficient training point;class gradient communication;training compressors;aggregation error feedback;lazy aggregation;gradient communication mechanisms;point compressors;crowds;lazy aggregation literature;point compressors provide;communicationefficient training;compressors;crowds men;allow training compressors;compressors evolve training;foundations lazy aggregation;mechanisms communicationefficient training;aggregation;crowds men women;training compressors evolve;state art algorithmic;algorithms allow training;learning algorithms;crowded;algorithms", "pdf_keywords": "compressed communication;approaches compressed communication;lazy aggregation communication;aggregation communication;compressed communication viewed;lazy aggregation homogenity;compressors lazy aggregation;aggregation communication mechanism;aggregation homogenity;aggregation homogenity level;gradient communication based;gradient communication;homogenity level homogenization;approaches compressed;compressed;lazy aggregation;known approaches compressed;framework gradient communication;communication based contractive;aggregation;present lazy aggregation;level homogenization;homogenization;homogenity level;homogenity;based contractive compressors;homogenization level xmath0;mixture individual members;homogenization level;homogenisation"}, "1c2c9e5d0588599516a78adda1fe3935dc5ae5d7": {"ta_keywords": "monotone games complexity;convex games;sm convex games;strongly monotone games;derivative free play;convex games known;deterministic mean field;games complexity xmath0;monotone games;games complexity;play value convex;unconstrained optimization;stochastic gradient;construct deterministic mean;stochastic gradient used;free play strongly;method unconstrained optimization;play strongly monotone;unconstrained optimization improved;model dynamics state;deterministic mean;dynamics state potts;model sm convex;field model dynamics;stochastic;model dynamics;free play;mean field model;free play value;potts model xmath0", "pdf_keywords": ""}, "311381feeb6346bfcb2ba622bd8f713261a4075d": {"ta_keywords": "comment ranking edit;collaborative documents efficiently;collaborative documents architecture;tasks comment ranking;manage collaborative documents;collaborative document management;comment ranking;edits comments crucial;comment ranking achieve;collaborative documents;comments relationship edits;comments edits;relationship comments edits;idea collaborative documents;94 comment ranking;user track document;documents efficiently managed;edits comments relationship;edits comments;relationship edits comments;ranking edit anchoring;collaborative document;documents efficiently;comments edits defining;track document;related tasks comment;ranking edit;explore relationship comments;tackles comment ranking;documents architecture", "pdf_keywords": ""}, "efcdb62b59e4dfb3f51b53850a81d6149ec3dfc8": {"ta_keywords": "interpretable machine learning;spinal canal;spinal canal spinal;use spinal canal;interpretable machine;canal spinal canal;canal spinal;spinal;spinal canal letter;severity use spinal;field interpretable machine;machine learning;use spinal;sclerosis;sclerosis severity use;interpretable;severity sclerosis;sclerosis severity;turbulence;machine learning iml;learning;severity sclerosis severity;relationship severity sclerosis;laminar turbulence;cases researchers identify;machine;cases helping conceptualize;field interpretable;canal;helping conceptualize", "pdf_keywords": "feature attribution methods;feature attribution;global feature attribution;attribution methods;speci\ufb01c approaches tailored;decision trees;model debugging common;attribution methods include;model debugging;evaluation usefulness applicability;con\ufb02ated evaluation usefulness;understand methods useful;machine learning;method goals;task machine learning;evaluation usefulness;better understand methods;traditional feature selection;usefulness applicability addressing;evaluation global feature;methods useful;feature selection approaches;usefulness applicability;iml methods evaluation;methods evaluation global;approaches tailored;feature;model speci\ufb01c approaches;iml methods;classes decision trees"}, "null": {"ta_keywords": "model hierarchies graphs;probabilistic model hierarchies;hierarchies graphs;link prediction;linkage propose probabilistic;high quality hierarchies;quality hierarchies;quality hierarchies discovered;graph million nodes;tree based hierarchies;quality hierarchies present;hierarchies graphs obtained;link prediction despite;large graph million;probabilistic version game;results link prediction;million nodes;highlighting quality hierarchies;real world graphs;large graph;including large graph;nodes;hierarchies;graphs including large;hierarchies model;based markov chain;based hierarchies model;world graphs;probabilistic;hierarchies model obtains", "pdf_keywords": ""}, "42fc352a0db1e742b0248a02b812db4aaf7b2cd3": {"ta_keywords": "machine translation mle;mle machine translation;machine translation;translation mle machine;machine translation method;performance machine translation;prediction animal;translation mle;models target sentence;autoregressive networks;based autoregressive networks;approach prediction animal;generalization predictive models;target sentences results;source target sentences;energy models target;model generalization predictive;sentence joint energy;predictive model generalization;target sentences;predictive models;prediction animal behavior;task based models;energy task based;translation method based;generalization predictive;translation method;based energy task;energy task;learning machine", "pdf_keywords": "model translation tasks;translation tasks;translation tasks target;model translation;based model translation;tasks target language;target language;translation;language;energy based model;propose energy based;propose energy;tasks target;energy;tasks;paper propose energy;energy based;based model;model;target;paper;paper propose;based;propose"}, "740afdd1619d797145b056877865f941891e6a65": {"ta_keywords": "decision maker gradient;sfpark dynamic pricing;stochastic gradient algorithms;dynamic pricing pilot;robust decision making;dynamic pricing;expected loss minimization;stochastic gradient;loss minimization;robust decision;design stochastic gradient;decision making algorithms;loss minimization given;introduced robust decision;gradient algorithms;sfpark dynamic;data sfpark dynamic;distribution dependent decision;gradient oracle;decision maker;stochastic;parking rates novel;loss function oracle;maker gradient oracle;simply loss function;gradient algorithms paper;loss function;data sfpark;problem expected loss;sfpark", "pdf_keywords": "learning decision;learning decision making;reinforcement learning;learning model decision;learning reinforcement;learning problem decision;learning reinforcement learning;reinforcement learning problem;reinforcement learning reinforcement;memory reinforcement learning;congestion constraints learning;decision dependent distribution;decision dependent risk;constraints learning decision;curbside parking;consider problem learning;learning model;learning problem;curbside parking resources;problem decision dependent;memory reinforcement;dependent risk minimization;risk minimization;distribution decision dependent;decision dependent;problem performative prediction;risk minimization problem;problem machine learning;problem decision maker;decision maker"}, "b9c026ab6e161a0f8c4b4db82ee8ad10792084cc": {"ta_keywords": "el speech enhancement;hybrid speech enhancement;speech enhancement;speech enhancement method;speech enhancement noise;parameters el speech;voice conversion electrolarynx;el speech minimizing;noise reduction voice;electrolaryngeal el speech;el speech experimental;speech recognition el;statistical voice conversion;voice conversion;reduction voice conversion;produce el speech;voice conversion method;parameters voice conversion;sounds enable laryngectomees;voices converted acoustic;speech minimizing degradation;methods el speech;el speech sounds;acoustic parameters el;speech keeping intelligibility;method automatic speech;laryngectomees produce el;compared el speech;subtraction statistical voice;enable laryngectomees", "pdf_keywords": ""}, "066f2023b2b5ba5df61dc193c205785fa5e73fed": {"ta_keywords": "kernel clustering regularization;kernel clustering solution;regularization kernel clustering;bounds kernel clustering;kernel clustering;partitioning kernel clustering;kernel clustering work;kernel clustering criteria;based kernel clustering;optimization kernel cut;kernel cut algorithm;clustering regularization;clustering regularization based;data partitioning kernel;bound optimization kernel;partitioning kernel;optimization kernel;regularization kernel;regularization based segmentation;clustering solution optimization;kernel spectral bounds;spectral bounds kernel;explain regularization kernel;clustering solution;linear kernel spectral;kernel spectral;clustering;clustering work;lipids explain regularization;segmentation", "pdf_keywords": ""}, "9578679e028777dd709881f938114aa59fbbf481": {"ta_keywords": "matching entity names;retrieval string distance;prediction string distance;string distance metrics;compare string distance;information retrieval string;java toolkit matching;information retrieval;matching entity;experimentally compare string;entity names;toolkit matching methods;string distance scheme;tfidf weighting scheme;string distance using;combining tfidf weighting;entity names present;string distance;used information retrieval;token based distance;tfidf weighting;matching methods;metrics task matching;heuristic string comparators;tfidf;compare string;retrieval string;toolkit matching;based prediction string;based distance metrics", "pdf_keywords": ""}, "ab627ba77dced941f9f45eeaee17bc6644308d89": {"ta_keywords": "classification email speech;classification email messages;classification email;consider classification email;new classification email;email act classification;email speech patterns;classification speech;classification speech speech;text classification;speech recognition classification;based classification speech;text classification algorithm;new text classification;classifier obtained email;classification;based collective classification;collective classification;classification new text;email speech;classification new;new classification;classifiers;classifier;classification based classification;classifiers maximum entropy;collective classification method;recognition classification;present new classification;act classification", "pdf_keywords": ""}, "3429d0529d3e77f9e4606f13b2d252d5d964abad": {"ta_keywords": "correctness conjecture proof;based proof conjecture;proof conjecture;conjecture proof;conjecture proof based;proof correctness conjecture;proof conjecture false;conjecture false;correctness conjecture;conjecture;present simple proof;simple proof correctness;simple proof;proof correctness;proof based proof;based proof;proof;proof based;false;paper present simple;correctness;present simple;paper present;paper;present;based;simple", "pdf_keywords": ""}, "66b83f0801d0c2d4194ff60c5ef9c754b51ce521": {"ta_keywords": "segmentation pancreas ct;segmentation pancreas;image segmentation models;pancreas ct scans;segmentation models learning;method segmentation pancreas;segmentation models;interpreting image segmentation;image segmentation;learning regions images;models learning regions;networks dnns;deep neural;segmentation;ct scans qualitatively;learning regions;ct scans;sensitivity deep neural;deep neural networks;regions images noise;neural networks dnns;pancreas ct;obscured images;networks dnns widely;magnetic fields;occlusion sensitivity deep;neural networks;magnetic field absence;images noise;magnetic field", "pdf_keywords": "segmentation tasks interpretability;automatically segmenting pancreas;segmentation net powerful;segmentation net;harmed segmentation net;segmenting pancreas ct;segmentation models learning;image segmentation models;input image segmentation;trained interpretability;powerful image segmentation;interpretability framework image;segmentation models;segmenting pancreas;trained interpretability model;pre trained interpretability;accuracy harmed segmentation;image segmentation architecture;interpreting image segmentation;image segmentation accuracy;image segmentation;pancreas ct scans;segmentation architecture;segmentation accuracy;harmed segmentation;segmentation accuracy harmed;segmentation;segmentation tasks;image segmentation introduce;learning regions images"}, "568efa8d71d8f2a086c8debcdf547e7053269021": {"ta_keywords": "international tournament;season international tournament;international tournament played;tournament;highlights 2012;tournament played;present highlights 2012;2012 season international;highlights 2012 season;division american physical;aps division american;tournament played 64th;annual meeting aps;aps division;meeting aps division;2012 season;american physical;meeting aps;season international;aps;american physical society;2012;played 64th annual;present highlights;division american;highlights;annual meeting;64th annual meeting;physical society;video present highlights", "pdf_keywords": ""}, "4f0d485cbcde840533f23f0c8b0f3fa1ca2d74df": {"ta_keywords": "transductive linear bandit;algorithm linear bandits;generalize linear bandits;linear bandits generalize;linear bandits;linear bandits case;bandits generalize linear;mathcal linear bandits;linear bandit;linear bandits nearly;linear bandit paper;bandit paper propose;bandits case subset;bandits generalize;bandits case;bandits nearly achieves;discovery based dosages;bandit paper;bandits;bandits nearly;bandit;approach drug discovery;method drug discovery;drug discovery;lower bounds transductive;drug discovery introduce;transductive setting algorithm;bounds transductive;bounds transductive setting;drug discovery based", "pdf_keywords": "algorithm linear bandits;exploration linear bandit;linear bandits;linear bandits nearly;allocation combinatorial music;linear bandit;combinatorial music recommendation;linear bandit paper;bandits nearly achieves;asymptotically optimal algorithm;bandit paper present;bandit paper;combinatorial music;bandits nearly;optimal algorithm;bandits;provide asymptotically optimal;music recommendation;bandit;optimal algorithm meaning;approaches optimal algorithm;approaches optimal;finding optimal;optimal algorithm particular;repeated rounds exploration;algorithm empirically competitive;optimal static allocation;asymptotically optimal;method finding optimal;competitive previous algorithms"}, "7b19e6540c786b80a3615a8ae2ef706242a1fa5b": {"ta_keywords": "compressive phase retrieval;complexity compressive phase;phase retrieval sparse;compressive phase;tackle compressive phase;method compressive phase;matrix phase retrieval;phase retrieval problem;computational complexity compressive;phase retrieval;complexity compressive;method compressive;sparse graph codes;new method compressive;measurement matrix phase;compressive;noisy quadratic measurements;retrieval sparse graph;sparse graph;matrix phase;single mode optical;retrieval sparse;phase single mode;mode optical waveguide;mode optical;robustify phase locked;recover complex signal;method robustify phase;tackle compressive;robustify phase", "pdf_keywords": "compressive phase retrieval;phase retrieval compressive;noise phase retrieval;retrieval compressive phase;compressive phase;phase retrieval convex;robustify phasecode;method robustify phasecode;phase retrieval;robustify phasecode presence;complexity phase retrieval;phasecode;phasecode presence noise;sparse complex signal;phase retrieval recover;phasecode presence;sublinear complexity phase;recover sparse complex;noise phase;retrieval compressive;signal sparsity structure;compressive;sparse complex;recover sparse;relaxation sublinear complexity;measurements signal sparsity;signal sparsity;convex relaxation sublinear;retrieval convex relaxation;quadratic measurements signal"}, "a0cbaf59f563580f68523ab6839a436e38b6db18": {"ta_keywords": "language performance neural;samples target sentence;languages minimal training;sentence conditionally samples;human language performance;sampling distribution multilingual;multilingual data minimizes;language data;language data taken;samples source sentence;distribution multilingual data;multilingual data;language performance;distribution multilingual;human language;target sentence conditionally;multilingual;low resource language;conditionally samples source;effect human language;performance neural;resource language data;gains languages minimal;neural networks experiments;target sentence;languages;source sentence paper;performance neural networks;conditionally samples;languages minimal", "pdf_keywords": "training machine translation;machine translation shared;neural machine translation;machine translation model;translation shared attention;machine translation;data selection multilingual;multilingual neural machine;machine translation propose;multilingual data boost;selection multilingual mt;translation model based;multilingual mt selects;selection multilingual;translation model;multilingual neural;selection framework multilingual;multilingual data constructing;translation shared;languages minimizing training;language similarity features;multilingual data;relevant multilingual data;transfer learning low;multilingual mt;languages neural machine;data languages minimizing;transfer learning;samples target sentence;low resource languages"}, "105146a7872835a52c8c5c55a3aae62c5d8852a1": {"ta_keywords": "morphological analysis transliteration;analysis transliteration tokenization;translation naive alignment;lexical processing translation;character based translation;transliteration tokenization;processing translation;translation character based;analysis transliteration;based machine translation;machine translation;translation machine translation;machine translation process;translation machine;word based translation;machine translation sees;based translation character;machine translation machine;processing translation steps;transliteration;translation distant language;based translation naive;based translation distant;transliteration tokenization required;translation character;additional lexical processing;translation process based;translation process;naive alignment methods;phrase based machine", "pdf_keywords": ""}, "9d10bbd21f475d500c3a7e24052e02596e052e2e": {"ta_keywords": "speech recognizer;speech recognizer based;speech recognition;article speech recognizer;recognition language human;gram language models;recognition language;gram language model;efficient speech recognition;speech recognition state;language models;model recognition language;glick language model;language model;language models used;recognizer based neural;language model better;efficient speech;gram language;networks efficient speech;model language jlaceer;performance gram language;recognition vocabulary;recognizer based;language jlaceer;jlaceer performance gram;huge recognition vocabulary;recognition vocabulary article;language human language;model recognition", "pdf_keywords": ""}, "36d193c7a9523f55f9fe5ffd0730f248c241f5c7": {"ta_keywords": "fluid dynamics;fluid motion;fluid dynamics baltimore;division fluid dynamics;equations tutorial;gallery fluid motion;algebraic equations tutorial;equations tutorial discusses;linear algebraic equations;fluid motion 66th;dynamics;tutorial solving;describes tutorial solving;tutorial solving class;aps division fluid;peer reviewed software;algebraic equations;equations;linear algebraic;division fluid;gallery fluid;tutorial discusses problems;discusses problems empirical;algebraic;fluid;motion;linear;problems empirical;proposed solutions;reviewed software", "pdf_keywords": ""}, "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad": {"ta_keywords": "natural language inference;language inference benchmarks;commonsense inference;new challenge dataset;commonsense natural language;task commonsense natural;state art adversarial;adversarial;commonsense inference proves;new task commonsense;language inference;challenge dataset construction;iteratively select adversarial;adversarial way;paper commonsense inference;evaluation natural language;adversarial way present;challenge dataset;language inference zellers;task commonsense;select adversarial;adversarial set machine;challenges paper commonsense;adversarial set;inference benchmarks evolve;art adversarial;commonsense natural;art adversarial way;language inference given;hellaag new challenge", "pdf_keywords": "commonsense reasoning ms;representations humans machines;commonsense reasoning;language generation crowds;natural language inference;commonsense natural language;word representations humans;recognition language generation;language modeling world;situated commonsense reasoning;language inference nli;language inference;generation crowds humans;humans machines deep;away language modeling;reasoning world;language modeling;progress nlp;machines deep;reasoning world works;way reasoning world;machine humanwritten;machine human written;automatic recognition language;recognition language;immense progress nlp;2dycuty perform commonsense;representations humans;natural language;language generation"}, "de7d0c87794c3de6f8ab2c753ecc398c18c26631": {"ta_keywords": "extracts grammatical specification;extract rules web;extract rules;rules descriptive grammar;grammars world languages;core grammars world;grammars world;grammatical specification raw;grammatical specification nearly;framework extracts grammatical;rules web;grammar indispensable step;grammatical specification;pass grammatical specification;core grammars;grammars;language documentation;descriptive grammar;descriptive grammar indispensable;rules descriptive;analyze morphosyntactic;phenomenon core grammars;automated framework extracting;step language documentation;language documentation preservation;rules web page;specification raw text;grammar;method extract rules;languages framework extracts", "pdf_keywords": "rules syntactic features;extracting grammar rules;syntactic features arabic;extract grammatical agreement;annotated syntactic analysis;automatic evaluation rules;extracting grammar;expertly annotated syntactic;rules syntactic;grammar rules syntactic;syntactic features;framework extracting grammar;annotated syntactic;syntactic analysis new;syntactic analysis;grammatical agreement rules;language documentation;grammar rules;rules raw text;features arabic languages;rules 55 languages;text statistically informed;rules devise automated;arabic languages propose;automatic evaluation;analysis new language;framework extract grammatical;metric automatic evaluation;text statistically;extract grammatical"}, "b709da495f15a4a9c1173192ecd755d1697dedf0": {"ta_keywords": "recommendation saccharomyces genome;representation publication databases;online recommendation saccharomyces;publication databases rich;citation history based;collaborative filtering;biological networks structure;saccharomyces genome databases;graph representation publication;reading citation history;recommendation tasks leveraging;genome databases;biological networks;networks structure dynamics;users reading citation;reading recommendation tasks;structure biological networks;citation history;collaborative filtering approaches;publication databases;recommendation tasks;genome databases available;based collaborative filtering;networks structure;content based collaborative;develop reading recommendation;reading recommendation;recommendation saccharomyces;biological systems network;based reading recommendation", "pdf_keywords": ""}, "bef4548a43fca8a7410734e4200157d50e257a29": {"ta_keywords": "entanglement swapping approach;entanglement swapping;quantum computation;concept entanglement swapping;quantum computation based;approach quantum computation;speech recognition asmr;new approach quantum;quantum;speech processing;speech recognition;processing speech;entanglement;recognizes sequence words;recognition asmr;processing speech processing;approach quantum;speech processing method;adaptive span;proposed adaptive span;adaptive span methods;language processing speech;computational complexity memory;automatic speech;recognition asmr automatic;concept entanglement;automatic speech recognition;asmr automatic recognition;sequence proposed adaptive;based concept entanglement", "pdf_keywords": ""}, "dad121213a17c6cc977d2298c7a9927639ca58e6": {"ta_keywords": "simulation study;simulation study demonstrate;present simulation study;simulation;present simulation;paper present simulation;effectiveness approach;demonstrate effectiveness approach;study demonstrate effectiveness;effectiveness;demonstrate effectiveness;approach;study demonstrate;demonstrate;study;paper;paper present;present", "pdf_keywords": ""}, "9f832bdcbc9d9566f7ab07b7455364bee62086fb": {"ta_keywords": "generate pseudo code;pseudo code python;pseudo code source;python source code;code python source;like pseudo code;pseudo code;new pseudo code;corresponding pseudocode tedious;pseudocode tedious;code source code;pseudo code used;source code written;pseudocode;python like pseudo;pseudocode tedious create;corresponding pseudocode;automatically generate pseudo;source code present;generation python like;behavior source code;source code;code python;generation python;source code using;code using statistical;python source;usefulness source code;code source;does corresponding pseudocode", "pdf_keywords": ""}, "f79361dda56ee755fc56ab83cf0d9f12d42b2d5e": {"ta_keywords": "approximate recovery hamming;alternatively recovering ranking;pairwise comparisons outperforms;recovering ranking items;recovering ranking;recovery hamming;pairwise comparisons;hamming distortion metric;pairwise pairwise comparisons;number pairwise comparisons;counting algorithm ranks;algorithm ranks items;pairswise comparison probabilities;recovery hamming distortion;copeland counting algorithm;algorithm ranks;ranking items;pairswise comparison;pairwise comparisons won;new ranking method;matrix pairswise comparison;ranking method;comparison probabilities optimal;hamming;guarantees approximate recovery;based pairwise pairwise;ranking;underlying matrix pairswise;hamming distortion;based pairwise", "pdf_keywords": "recovering ranked subset;countingbased method ranking;items pairwise comparisons;ranked subset items;pairwise comparisons;pairwise comparison associated;comparisons pairwise comparison;ranked subset;comparisons pairwise;pairwise comparison;pairwise comparisons pairwise;ranking;observe noisy comparisons;recovering ranked;noisy comparisons;method ranking;noisy comparisons various;ranking called;comparisons various pairs;hamming error recovery;approximate recovery closeness;subset items pairwise;problem recovering ranked;items pairwise;method ranking called;classical countingbased method;closeness measured hamming;comparison associated certain;comparison associated;ordering"}, "092687dc06b0b264a524c6d4ea151780ba85a02a": {"ta_keywords": "non verbal communication;training non verbal;non verbal social;recognizing non verbal;non verbal behaviors;verbal communication skills;verbal communication incorporating;verbal communication tool;verbal communication;communication skills people;social communication difficulties;people social communication;verbal social signals;verbal social;communication tool training;social communication;communication skills;verbal behaviors;non verbal;skills people social;communication incorporating visual;verbal behaviors effect;communication difficulties;situations difficulty recognizing;computer based training;social situations difficulty;communication;communication incorporating;potential non verbal;group humans learn", "pdf_keywords": ""}, "2d8d51d483a50c6fbf16a0cc120465539f4055da": {"ta_keywords": "microblog posts languages;information content microblog;language information content;microblog text twitter;content microblog text;microblog text;language information;information microblog;content microblog;microblog text traditional;microblog posts;information microblog post;correlation information microblog;tweet using information;text twitter;contribute language information;post microblog text;information character languages;text wikipedia;posts languages moreinformation;japanese contain information;information content character;authors microblog posts;character languages article;microblog post authors;posts languages;languages larger character;text twitter event;languages larger;microblog", "pdf_keywords": ""}, "27ad78b72c3fb77a117b15855008b65e838314e8": {"ta_keywords": "carbon nanotube cnt;walled carbon nanotube;nanotube cnt;carbon nanotube;nanotube;single walled carbon;electronic structure single;analysis electronic structure;electronic structure;walled carbon;cnt;structure single walled;analysis electronic;detailed analysis electronic;carbon;single walled;structure single;electronic;walled;structure;single;detailed analysis;present detailed analysis;paper present detailed;present detailed;paper present;present;analysis;paper;detailed", "pdf_keywords": ""}, "51b2dd5cbec02f016c6fa716705ede9b3846a410": {"ta_keywords": "energy spectrum;behavior power spectrum;slope energy spectrum;power spectrum;shape spectral density;energy spectrum iii;shape spectral;spectral density;power spectrum ii;spectral;spectrum;spectrum iii shape;model;iii shape spectral;spectrum ii slope;model able;spectrum ii;data power law;model able reproduce;spectrum iii;features data power;data;data power;features data;power law behavior;main features data;slope energy;behavior power;energy;ii slope energy", "pdf_keywords": ""}, "3e86ecbd41ab55b90d3b45601aeb15d2e5c1c8f8": {"ta_keywords": "round robin tournament;balanced knockout tournaments;tournament scoring;tournament;particular player tournament;player tournament;robin tournament scoring;robin tournament;tournaments;balanced round robin;tournament present;tournament present results;round robin;knockout tournaments;tournament scoring units;player tournament present;draws given player;player winner draw;winner draw polynomial;knockout tournaments common;tournaments common;tournaments common sports;draws;draws given;given player winner;competitions;polynomial time balanced;competitions world;sports competitions;optimal draw", "pdf_keywords": ""}, "7580df14bf01438e7174bbff260508a39a44df84": {"ta_keywords": "erasure codes distributed;distributed storage codes;sparse codes;explicit erasure codes;sparse codes instead;codes distributed storage;use sparse codes;codes instead sparse;storage codes fast;storage codes;erasure codes;repair optimal codes;fast encoding sparsity;optimal codes;sparse coding;systematic distributed storage;approach sparse coding;encoding sparsity;codes fast encoding;codes distributed;sparse coding based;distributed storage desirable;optimal codes enable;storage desirable properties;distributed storage;fast encoding;new approach sparse;use sparse;fast encoding present;storage repair optimal", "pdf_keywords": "erasure codes systems;erasure codes distributed;codes distributed storage;explicit erasure codes;mds erasure codes;erasure codes;maximal redundancy encoding;optimal repair bandwidth;distributed storage;distributed storage desirable;codes systematic remapping;systems storage;codes distributed;reliability minimum storage;systems storage critical;scale systems storage;redundancy encoding;codes systems consider;systematic pm codes;constructing explicit erasure;fast encoding minimise;encoding minimise cost;codes systems;redundancy encoding new;repair bandwidth minimize;storage desirable properties;codes systematic;classical mds erasure;pm codes systematic;mds erasure"}, "92fc770721e95249f8db01c5019d1cc4cf79ff00": {"ta_keywords": "selecting winning position;natural language query;computer aided decision;position team based;natural language;team based;language query recognition;language natural language;query recognition topological;query language natural;video coupled pendulums;selecting winning;pendulum parses english;capable predicting outcome;predicting outcome decision;winning position team;team based solely;positional information;pendulum parses;winning position;query recognition;position team;method selecting winning;single pendulum parses;based solely positional;language natural;capable predicting;recognition topological order;pendulums;language query", "pdf_keywords": ""}, "6c5144872c259611dceb32fe4e4486a6865e6c42": {"ta_keywords": "decomposition robust speech;stationary noise estimation;estimation nonstationary noise;mixture component bose;noise estimation;robust speech recognition;robust speech;component residuals noisy;component bose;noise estimation method;nonstationary noise sequences;bose einstein condensates;noise suppression;speech recognition;component bose einstein;residual component decomposition;residuals noisy data;noise suppression problem;bose;speech recognition proposed;estimate residual component;residuals noisy;suppression problem estimation;noise sequences;algorithm estimate residual;bias residual component;einstein condensates becs;component decomposition robust;nonstationary noise;addresses noise suppression", "pdf_keywords": ""}, "cf6352c789ab51320fa7ca9b1440c685b57fd769": {"ta_keywords": "speech activity detector;speech recognition trained;speech recognition;performance speech recognition;speaker activity;speaker activity currently;speech segments;diarization methods training;microphone recordings variety;speech activity;diarization performance measurement;microphone data;speech segments vector;microphone recordings;refinement speech activity;diarization performance;speaker segments;consider speaker activity;representations speech segments;approach diarization performance;microphone data followed;extraction representations speech;state art diarization;bayesian refinement speech;provided microphone recordings;task provided microphone;recordings variety;diarization methods;new approach diarization;microphone", "pdf_keywords": ""}, "2e1a1588955a8a64ec618b3cc04be961ed0cb59c": {"ta_keywords": "polymers machine learning;state energies oligothiophenes;energies oligothiophenes;energies oligothiophenes report;phase oligothiophenes predictions;energies poly hexylthiopnene;gas phase oligothiophenes;oligothiophenes predictions;phase oligothiophenes;oligothiophenes predictions compared;state energies poly;conjugated oligomers polymers;oligomers polymers;transfer learning;transfer learning models;energies poly;oligomers polymers machine;chemistry modeling optoelectronic;using transfer learning;excited state energies;use transfer learning;learning techniques chemistry;electronic excitations;conjugated oligomers;oligothiophenes;long conjugated oligomers;transfer learning address;learning models trained;p3ht single crystal;machine learning remains", "pdf_keywords": ""}, "9237d6efc603465765e80eb5ca1268c2bd7b5c24": {"ta_keywords": "machine translation;language generation tasks;machine translation present;language generation;generating python like;tasks machine translation;python like;generating python;systems language generation;python like analyses;translation present python;python;python open source;words bucketed histograms;generation tasks machine;linguistic;features use linguistic;use linguistic;python python;linguistic labels source;linguistic labels;generation tasks;language;package generating python;fluid dynamics video;use linguistic labels;present python;analysis accuracy generation;histograms sentence accuracies;present python python", "pdf_keywords": "tool holistic comparison;source toolkit compare;results language generation;machine translation;machine translation systems;visualize single translation;machine translation process;toolkit compare;modern python compare;translation systems;language generation systems;holistic comparison analysis;process machine translation;translation process machine;python compare;analysis results language;translation systems attempt;toolkit compare mt;translation outputs way;python compare mt;translation outputs;tool holistic analysis;holistic comparison;single translation outputs;useful generate analyses;language generation;functionality provided compare;results language;topic machine translation;compare mt tool"}, "a6336fa1bcdeb7c84d2c4189728f0c1b2b7d0883": {"ta_keywords": "recurrent neural networks;recurrent neural;research recurrent neural;robot recurrent neural;years recurrent neural;robot recurrent;sequences countless learning;learn inputs sequences;neural network architecture;controlling robot recurrent;neural networks traditionally;dynamics neural network;model dynamics neural;neural networks retain;neural networks used;neural network;idea neural networks;model neural;dynamics neural;idea neural;neural networks;neural networks demonstrated;recognition countless learning;large scale learning;neural;model neural network;used model neural;recurrent;learn inputs;model learn inputs", "pdf_keywords": "translation captioning neural;captioning neural networks;captioning neural;networks rnns;model translation captioning;recurrent neural networks;learning model translation;networks rnns connectionist;neural networks rnns;sequence prediction tasks;translation captioning;rnns connectionist models;scenes natural language;techniques recurrent neural;output recurrent neural;rnns;deep learning;techniques recurrent;important applications recurrent;applications recurrent neural;captioning;applications recurrent;deep neural;recurrent neural;rnns connectionist;combinable techniques recurrent;representations natural language;methods applications recurrent;model translation;traditional deep learning"}, "7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7": {"ta_keywords": "textual knowledge transformers;question answering;domain knowledge intensive;entity mention corpus;incorporating textual knowledge;domain question answering;knowledge transformers;entity mention attention;knowledge transformers entity;open domain knowledge;large text corpus;knowledge intensive tasks;mention corpus;incorporating textual;textual knowledge;representations entity mention;question answering require;assimilating factual information;mention corpus natural;retrieving assimilating factual;language understanding tasks;transformers entity mention;knowledge intensive;text corpus transformer;domain knowledge;representation large text;entity mention;text corpus;vector representations entity;attention", "pdf_keywords": "corpus entity model;entity centric queries;text corpus entity;answer entity centric;entity based similarity;corpus entity;entity centric;informative entity;answer entity;centric queries text;contents text corpus;entity model semantically;large text corpus;language model learns;speci\ufb01c informative entity;language models successfully;correspond entity based;mentions correspond entity;language models;informative entity attributes;model semantically challenging;predict linked mentions;approach answer entity;predict contents text;entities;centric queries;suggests language models;entity;correspond entity;information text corpus"}, "8be39979cb2eb1aeaba15b57e1e4bc712eb962cb": {"ta_keywords": "investigate paragraph embedding;paragraph embedding models;paragraph embedding;sentence content probe;paragraph embedding method;improves paragraph reconstruction;sentence content task;sentence content;paragraph reconstruction;paragraph reconstruction terms;ability paragraph embedding;embedding models remarkably;improves paragraph;objective improves paragraph;semi supervised;downstream classification tasks;formulate sentence content;approach sentence content;investigate paragraph;classification tasks learn;embedding models;training better generalization;classification tasks;paragraph;generalization ability paragraph;embedding;semi supervised setting;downstream classification;probe basic linguistic;objective semi supervised", "pdf_keywords": "paragraph embedding methods;paragraph embedding;training sentence content;incorporated paragraph encoder;paragraph embedding method;paragraph encoder;research paragraph embedding;sentence level linguistic;investigate sentence content;paragraph encoder help;language model representations;sentence content;encodes identity sentences;paragraph level;linguistic probe tasks;sentences paragraph;trained language model;paragraph level inspired;sentences paragraph paper;level linguistic probe;state art paragraph;sentence content property;art paragraph embedding;tasks paragraph level;propose semi supervised;semi supervised;sentence content pre;paragraph;linguistic probe;sentence level"}, "f9ee690d223beac6d893aedae13c09dbf0fb694e": {"ta_keywords": "semantic verb clustering;clustering natural language;verb clustering natural;verb clustering;learn lexical semantic;lexical semantic;lexical semantic verb;method learn lexical;learn lexical;natural language processing;lexical;clustering natural;clustering method learn;semantic;semantic verb;language processing;natural language;clustering;evaluate clustering;clustering method;propose clustering;evaluate clustering algorithm;clustering algorithm;propose clustering method;icmpf evaluate clustering;language processing report;work propose clustering;clustering algorithm based;mean field models;based pairwise constraints", "pdf_keywords": ""}, "7354b87a1b4c99ccd9cf25b7314927ced8b156f7": {"ta_keywords": "interactive writing assistant;writing assistant generates;controllable writing assistants;interactive writing;improved writing assistance;writing assistant;significantly improved writing;writing assistants;build interactive writing;improved writing;writing assistance;writing assistants explored;writing;controllable writing;output creative writing;assistant generates rephrases;pretrained language models;generates rephrases text;creative writing task;internal state learning;learning external state;writing task;state learning external;generates rephrases;entanglement based learning;complex controllable writing;creative writing;writing assistance functionalities;language modeling;assistance functionalities autocomplete", "pdf_keywords": "interactive writing assistant;interactive writing;new interactive writing;writing assistant tools;generation writing assistant;writing assistant generates;human author language;writing assistant;automatically generating text;development machine translation;author language;assistant tools;participants write;allows participants write;machine translation capable;process human author;generating text;participants write help;author language model;creative writing task;generates rephrases text;writing process human;translation capable automatically;creative writing process;human means editable;machine translation;writing process;generation writing;assistant generates rephrases;writing task"}, "0f395654e69cd2e063a6ef221fb66fb46e68cefd": {"ta_keywords": "truthful classifiers incentive;classifiers incentive;classifiers incentive strategically;classifier create incentive;truthful classifiers;classifiers;classifiers trained;characterization truthful classifiers;classifier;chosen classifier;ensemble thebox classifiers;strategically drop features;thebox classifiers;drop features algorithms;machine learning;machine learning techniques;thebox classifiers trained;classifier specific;specific classifier;incentive strategically hide;depends chosen classifier;classifiers trained using;incentive strategically drop;chosen classifier specific;hierarchical ensemble;strategically hide features;classifier create;classifier specific classifier;incentive;specific classifier create", "pdf_keywords": "imputation reduced feature;imputation reducedfeature modeling;based imputation reducedfeature;based imputation reduced;classi\ufb01ers based imputation;imputation reducedfeature;based imputation;imputation reduced;reduced feature modeling;making based imputation;reduced feature;imputation;reducedfeature modeling discretization;reducedfeature modeling;feature modeling faced;reducedfeature;feature modeling;reduced;unbalanced classi\ufb01ers based;features size dataset;outperform box methods;modeling faced strategic;features;behavior particular dataset;feature;dataset unbalanced classi\ufb01ers;dataset evaluate proposed;accuracy generality methods;data sets;rules decision making"}, "60dd53fca1f538fabe18e4d6a9326b2f40e358dd": {"ta_keywords": "statistical topic models;topic models;topic models latent;models latent dirichlet;latent dirichlet al;large document collec;summarize large document;latent dirichlet;variational electrom algorithm;parallel implementation variational;implementation variational electrom;electrom algorithm multiproces;fast scalable work;electronic document col;document col lections;scalability statistical topic;document collec;models fast scalable;variational electrom;fast scalable;electrom algorithm;multiproces sor architecture;algorithm multiproces sor;large document;modern electronic document;document col;present parallel implementation;electronic document;multiproces sor;implementation variational", "pdf_keywords": ""}, "084ddb77fce5a7f0b6418ef4e38dbb1bedf4ae78": {"ta_keywords": "el speech enhancement;automatic el speech;speech enhancement;el speech experimental;speech enhancement applied;speech enhancement methods;laryngectomees maintaining listenability;quality el speech;el speech multiple;allow el speech;speech multiple laryngectomees;laryncectomee evaluate performance;statistical voice conversion;generalize speakers electrolarynx;proficiency laryngectomee clear;speakers electrolarynx el;proposed el speech;laryngectomee clear methods;voice conversion;proficiency laryngectomee;speech experimental;excitation sounds larynx;speech experimental results;using statistical voice;depends proficiency laryngectomee;sounds generated electrolarynx;statistical voice;el speech highly;real laryngectomees;larynx produce automatic", "pdf_keywords": ""}, "5b2c5eeea9ac8f26908b9dfc8fd0a2d0e7aa5bb1": {"ta_keywords": "adaptive beamforming networks;adaptive beamformer predict;lstm architecture adaptively;multichannel adaptive beamforming;microphones performing beamforming;adaptive beamforming;adaptive beamformer;predicting beamforming filter;predicting beamforming;networks robust speech;field speech recognition;present adaptive beamformer;beamforming networks robust;lstm architecture;robust speech;speech recognition noisy;robust speech recognition;lstm;beamforming networks;recurrent neural network;far field speech;beamformer predict;speech recognition;acquiring speech signal;deep learning;memory lstm architecture;recent deep learning;assist predicting beamforming;beamformer predict senone;recognition noisy reverberant", "pdf_keywords": "multichannel speech prediction;speech prediction;acoustic beamforming deep;beamforming deep recurrent;beamforming automatic speech;network multichannel speech;network acoustic beamforming;speech prediction based;multichannel speech;beamforming acoustic model;acoustic beamforming;optimize beamforming acoustic;adaptively estimate beamforming;beamforming acoustic;microphones performing beamforming;achieve robust speech;integrated network acoustic;acquiring speech signal;robust speech;speech recognition;robust speech recognition;speech recognition asr;network acoustic;recurrent neural networks;lstm;deep recurrent neural;speech signal;speech signal multiple;automatic speech;real time beamforming"}, "d38b686b8b68d0b91b294fd8a55ac7dea191706f": {"ta_keywords": "neural abstractive summarization;extensible guided summarization;guided summarization;abstractive summarization models;guided summarization framework;summarization models flexible;summarization models;popular summarization datasets;summarization datasets;abstractive summarization;summarization framework;performance popular summarization;sentences guidance neural;produce coherent summaries;summarization datasets using;summarization framework gsum;popular summarization;summarization;guidance neural abstractive;generate faithful summaries;coherent summaries;different summaries;qualitatively different summaries;highlighted sentences guidance;summaries demonstrate;guidance neural;summaries;different summaries previous;summaries demonstrate different;sentences guidance", "pdf_keywords": "neural abstractive summarization;summarizing sentences;summarizing sentences oracle;approach summarizing sentences;summarization datasets;summarization methods constrain;novel approach summarizing;abstractive summarization methods;popular summarization datasets;abstractive summarization;summarization datasets using;highlighted sentences guidance;summarization methods;summarizing;rouge popular summarization;improvements extractive summarization;extractive summarization;sentences guidance;guidance based neural;approach summarizing;summarization;sentences oracle guidance;guided neural abstractive;popular summarization;summarization topic current;methods guided neural;summary output content;guided neural;extractive summarization topic;summarization topic"}, "e114618157e025ed17b7e45684d67becd34a14f3": {"ta_keywords": "gaussian mixtures shared;component gaussian mixtures;statistics gaussian mixtures;gaussian mixtures;mixtures shared covariance;distance component gaussian;dimensional gaussian distributions;gaussian mixtures high;high dimensional gaussian;lower bounds mixtures;component gaussian;bounds mixtures exploiting;mixtures high dimensional;dimensional gaussian;gaussian distributions;bounds mixtures;variation distance component;total variation distance;gaussian distributions paper;variation distance characteristic;mixtures exploiting;distribution learning analytically;complexity distribution learning;statistics gaussian;suspension gaussians;mixtures shared;variation distance;distribution learning;gaussians stay;characteristic function mixture", "pdf_keywords": "gaussian mixtures shared;component gaussian mixtures;mixtures arbitrary gaussian;gaussian mixtures;gaussian mixture weights;weighted gaussians shared;distribution gaussian mixture;gaussian mixture;mixtures shared covariance;equally weighted gaussians;dimensional component gaussian;gaussians shared;weighted gaussians;learning mixtures arbitrary;gaussians shared variance;arbitrary gaussian distributions;mixtures components bounded;component gaussian;variation distance mixtures;learning mixtures;bounds pairs mixtures;distance mixtures components;eigenvalue distribution gaussian;single gaussian;setting learning mixtures;arbitrary gaussian;distance mixtures;single gaussian results;bound eigenvalue distribution;gaussian distributions"}, "3ed91aae1038b8b0130fb3974060a50b10de1345": {"ta_keywords": "field automatic speech;home assistant spoken;speech recognition;accurate speech recognition;speech recognition distance;enable accurate speech;automatic speech;speech recognition haasr;automatic speech recognition;speech end avalanche;speech videos;assistant spoken language;spoken distance microphone;deep learning;solutions speech videos;recognition recognizes speech;speech videos recorded;home assistant;speech clip recorded;speech spoken distance;recognizes speech;assistant spoken;compared speech clip;recognizes speech spoken;speech clip;spoken distance;accurate speech;spoken language interface;new home assistant;distance microphone", "pdf_keywords": "talker speech recognition;speech enhancement;speech enhancement achieved;scenarios speech enhancement;recognition speech spoken;recognition speech;solutions speech enhancement;speech recognition speech;speech recognition;speech enhancement techniques;speech recognition systems;regions speech speaker;multi talker speech;performance automatic speech;neural networks acoustic;automatic speech recognition;automatic speech;speech spoken distance;spoken distance microphones;speech recognition years;networks acoustic;speech speaker;acoustic signal processing;speech speaker active;networks acoustic signal;based automatic speech;talker speech;deep neural network;spoken distance;multi speaker scenarios"}, "addd2d86d19c1e7c8854e827fb2656a50c250440": {"ta_keywords": "aspect based summarization;existing summarization models;summarization models;summarization task generating;challenges existing summarization;generating focused summaries;based summarization large;summarization attempts spur;focused summaries based;domains sentiment product;summaries aid efficient;summarization large;existing summarization;summarization attempts;quickly understanding reviews;summaries based specific;based summarization attempts;summarization paper propose;sentiment product features;summarization models face;based summarization task;based summarization;focused summaries;dataset summaries aid;summaries based;summarization;summarization task;summaries aid;domains sentiment;analysis text quickly", "pdf_keywords": "aspect based summarization;automatically summarizing articles;automatically summarizing;summarizing articles;approach automatically summarizing;summarization model;summarization model task;summarizing articles paper;summarizing articles different;present summarization model;summarization;aspect annotation;based summarization allows;summarization allows;summarization task;summarizing;task automatically summarizing;based summarization;summarization task examine;summarization allows train;present summarization;variety summarization;variety summarization task;multidomain aspect based;build dataset articles;dataset articles;unique variety summarization;proxy aspect annotation;aspect based;annotation"}, "0c3c4c88c7b07596221ac640c7b7102686e3eae3": {"ta_keywords": "dihydrosulfide;dihydroxyl diatomics experiment;molecule dihydrogen;xmath0 dihydrosulfide;dih xmath0 dihydrosulfide;molecule dihydrogen dihydrofolate;dihydroxyl diatomics;dihydrosulfide dihydroxyl diatomics;dihydrosulfide dihydroxyl;xmath0 dihydrosulfide dihydroxyl;dihydroxyl;dihydrogen;dihydrahydrohydrothermal efficiency;dihydrogen dihydrofolate;dihydrahydrohydrothermal efficiency enhanced;dihydrogen dihydrofolate dih;respectively dihydrahydrohydrothermal efficiency;dihydrofolate dih;single molecule dihydrogen;dihydrofolate;dihydrofolate dih xmath0;dihydrahydrohydrothermal;respectively dihydrahydrohydrothermal;accuracy preoperative statins;statins reduce atrial;xmath2 compared dihh;dihydrate diaphragm good;xmath2 compared;preoperative statins reduce;fig respectively dihydrahydrohydrothermal", "pdf_keywords": "biomedical answer generation;answer generation dataset;question answering;translation model answering;model answering;answer generation;question answering using;trained biomedical contextualized;research question answering;biomedical contextualized embeddings;contexts answer questions;biomedical language representation;machine translation;word representations pubmedmedqa;machine translation model;automatically answer naturally;building biomedical answer;biomedical contextualized;trained biomedical language;annotations requires reasoning;biomedical language;automatically answer;natural language understanding;infer natural language;expert annotations;present machine translation;representations pubmedmedqa;natural language;expert annotations requires;answering using"}, "b7731a9b9142a6deb132e99bc55ddbe458a537a6": {"ta_keywords": "online moment selection;moment selection framework;causal effect efficiently;regret assessed researchers;moment selection;asymptotic regret assessed;estimating effect causal;probabilistic model causal;asymptotic regret;selection framework structural;causal structure outcome;selection framework;zero asymptotic regret;selection strategies achieve;selection strategies;exploration choosing best;regret assessed;algorithms balance exploration;propose selection strategies;effect causal structure;exploration choosing;selection;model causal;causal;causal structure;data fusion;efficiently possible deciding;estimating;general framework estimating;estimates moments aim", "pdf_keywords": "causal inference data;known causal graphs;causal graphs;causal inference;causal graphs present;data fusion econometrics;inference data fusion;generated known causal;synthetic datasets;semi synthetic datasets;known causal;synthetic datasets infant;adaptively collected data;synthetic data;inference data;dataset policy learning;dataset policy;methods synthetic data;yes causal inference;estimator adaptively collected;datasets infant health;causal;datasets infant;policy learning adaptively;fusion econometrics;data fusion;learning adaptively collected;policy learning;datasets;synthetic data generated"}, "3ea5468e6d3007a94d4318932d7778693526145c": {"ta_keywords": "distributed computing resources;geographically distributed computing;distributed resource allocation;distributed computing;distributed computing using;approach distributed computing;distributed resource;novel distributed resource;grid computing;grid computing called;grid computing challenging;grid computing using;computing resource management;area grid computing;resource allocation controller;computing resources communication;grid computing integrates;dynamic resource management;dc dynamic resource;resource management delay;dynamic resources management;using geographically distributed;drm dc dynamic;years grid computing;resources communication networks;novel distributed;wide area grid;management delay compensator;delay compensator based;resource management dm", "pdf_keywords": ""}, "97906df07855b029b7aae7c2a1c6c5e8df1d531c": {"ta_keywords": "nlp tasks;nlp pipeline interpretable;nlp tasks present;nlp pipeline;traditional nlp pipeline;parsing ner semantic;state art nlp;nlp;linguistic information captured;tagging parsing ner;pre trained text;art nlp tasks;disambiguating information higher;linguistic information;tagging parsing;semantic roles coreference;traditional nlp;extent linguistic information;steps traditional nlp;trained text encoders;ner semantic roles;disambiguating information;pos tagging parsing;representations pre trained;ner semantic;linguistic;parsing;trained text;art nlp;coreference", "pdf_keywords": "deep language models;deep language;deep language model;deep language encoder;presents deep language;syntactic semantic abstractions;present deep language;nlp pipeline;traditional nlp pipeline;linguistic information encoded;semantic abstractions;language models represent;nlp pipeline quantify;modeling sentence prediction;sentence prediction word;language encoder trained;language models;english corpus experiments;language modeling;semantic abstractions traditionally;prediction word english;syntactic semantic;linguistic information;sentence prediction;language model;layers semantic;language processing;necessary language processing;linguistic;layers semantic information"}, "e31a3f52890dcb68f596020e45f8c9718b700466": {"ta_keywords": "fluid dynamics video;fluid dynamics;motion rigid;presents fluid dynamics;motion rigid body;dynamics video;showing motion rigid;rigid body affected;dynamics;dynamics video showing;rigid body;external perturbation;presence external perturbation;rigid;perturbation;video showing motion;motion;showing motion;video showing;fluid;video;presents fluid;body affected presence;paper presents fluid;affected presence external;affected presence;presence external;body affected;external;presence", "pdf_keywords": ""}, "77c98b45c95121fc2a3d2ab4906fc00364cf381c": {"ta_keywords": "training speech separation;speech separation recognition;speech separation;joint training speech;training speech;separation recognition;stage joint training;joint training;separation;speech;single stage joint;training;single stage;stage joint;recognition;method single stage;joint;method single;new method single;stage;new method;propose new method;method;single;paper;paper propose new;new;paper propose;propose;propose new", "pdf_keywords": ""}, "11a28f9e6fb6581d0a01428dd27a3fb649454395": {"ta_keywords": "diphenylcarbamyl chloride chymotrypsin;chymotrypsin diphenylcarbamyl chloride;chloride chymotrypsin protein;chloride chymotrypsin;swimmer salt free;swimmer salt;swimming swimmer salt;dynamics swimming;chymotryptic hydrolysis;studied experimentally chymotrypsin;chymotrypsin diphenylcarbamyl;solution diphenylcarbamyl chloride;chymotryptic hydrolysis specific;diphenylcarbamyl chloride;experimentally chymotrypsin;dynamics swimming swimmer;alpha chymotrypsin diphenylcarbamyl;centre chymotryptic hydrolysis;chymotrypsin protein;model dynamics swimming;chymotrypsin protein shown;experimentally chymotrypsin additional;chymotryptic;swimmer;diphenylcarbamyl chloride maximum;solution diphenylcarbamyl;swimming swimmer;chymotrypsin;reaction alpha chymotrypsin;swimming", "pdf_keywords": ""}, "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4": {"ta_keywords": "recognition videos;grained object classification;predicting caption;recognition videos geo;action recognition videos;task predicting caption;predicting caption goes;video learn;video learn ballistically;vision datasets;dynamics video learn;recognize zero shot;action recognition;object classification learning;computer vision datasets;learn sota image;object classification;learning directly raw;trained task predicting;training examples trained;vision datasets spanning;learning directly;examples trained;sota image representations;videos geo;trained task;videos;fully supervised baseline;fully supervised;image representations", "pdf_keywords": "trained imagenet learn;trained imagenet;imagenet learn;shot convolutional neural;models trained imagenet;imagenet learn recognize;cnn;shot convolutional;zero shot convolutional;network cnn;vision models trained;task representation learning;deep zero shot;imagenet;representation learning capabilities;model task video;image pre training;cnn investigation task;task video;trained language model;cnn investigation;models trained;trained language;convolutional neural;representation learning;deep zero;task video surveillance;tasks underlying representations;image classi\ufb01cation tasks;pre trained language"}, "de0e1f9980afa7949df64d53b8ae7a2f59c55579": {"ta_keywords": "transfer crowdsource annotations;languages style transfer;sentence target style;style transfer task;formality transfer crowdsource;crowdsource annotations;crowdsource annotations 4000;stylistic difference paraphrases;sentences inference extracting;modeling stylistic;style transfer using;automatic human evaluations;large stylelabelled corpora;annotations 4000 sentence;style transfer;sentence pairs languages;copy inputs verbatim;method modeling stylistic;modeling stylistic difference;style transfer method;stylelabelled corpora;new style transfer;style transfer new;automatic evaluation suite;sentences inference;automatic evaluation;input sentence target;transfer crowdsource;sentence target;extracting target style", "pdf_keywords": ""}, "1d79d055cf9711944f14e1388a9d054cbe81ddd0": {"ta_keywords": "discriminative language models;discriminative language modeling;learning discriminative language;confusion models turkish;discriminative language;approach discriminative language;language models negative;language models;text corpus;language modeling;learning discriminative;sentences text corpus;semi supervised;semi supervised learning;supervised learning discriminative;confusion models;corpus;text corpus generated;modeling language language;corpus generated;language modeling language;using confusion models;corpus generated using;probabilistic representation language;models turkish;modeling language;language morph based;work semi supervised;sentences text;examples sentences text", "pdf_keywords": ""}, "8cfd299be05bf3df91e0bf656a7e2fb973056350": {"ta_keywords": "acoustic cross lingual;speed speech recognition;scaling speech systems;speech recognition ssc;cross lingual algorithm;speech related tasks;cross lingual transfer;unsolved speech processing;speech synthesis tasks;lingual transfer;speech systems support;classification speech recognition;speech systems;scaling speech;recognition speech synthesis;speech recognition;lingual transfer pairs;recognition speech;speech processing systems;speech recognition speech;speech processing;text based speech;classification speech;speed speech;lingual algorithm task;cross lingual;family classification speech;language family classification;speech synthesis;low resource languages", "pdf_keywords": "acoustic crosslingual transfer;crosslingual transfer speech;transfer speech;speech synthesis tasks;transfer speech data;crosslingual speech recognition;downstream crosslingual speech;acoustic cross lingual;acoustic crosslingual;suitable acoustic crosslingual;acoustic language similarity;cross lingual transfer;speech synthesis work;cross lingual transfer_;crosslingual speech;lingual transfer;recognition speech synthesis;downstream speech tasks;speech synthesis;crosslingual transfer;lingual transfer_;lingual transfer term;speech tasks;lingual transfer procedure;performance downstream speech;compare features speech;language similarity metrics;acoustic language;speech data;introduce acoustic language"}, "3ba26e897d0085ecd8cb695e1728a083f9227447": {"ta_keywords": "recognition based bayesian;speech recognition based;speech recognition;bayesian approach proposed;based bayesian approach;bayesian approach;based bayesian;bayesian;recognition based;speech;recognition;approach proposed;approach;based;proposed", "pdf_keywords": ""}, "8f6763b339363216794f48895b9381d1a7caa88c": {"ta_keywords": "learning dynamics;multi agent learning;agent learning noisy;learning dynamics subset;learning dynamics respect;bilinear games disturbance;dynamics subset players;disturbance decoupled players;agent learning;subspaces players;unobservable subspaces players;games disturbance decoupling;learning noisy environments;based learning dynamics;overall learning dynamics;players disturbance decoupled;decoupled players actions;games quadratic cost;robustness gradient;games quadratic;disturbance decoupling player;payoff matrix disturbances;robustness gradient based;decoupling player action;subset players disturbance;bilinear games;players disturbance;studies robustness gradient;learning noisy;games disturbance", "pdf_keywords": "agent learning dynamics;learning dynamics;games learning trajectory;disturbances quadratic games;subspaces players gradient;bilinear games learning;multi agent learning;learning dynamics important;learning dynamics respect;quadratic games disturbance;players gradient based;players gradient;agent learning;games disturbance decoupling;based learning dynamics;agent learning settings;overall learning dynamics;learning trajectory;games learning;learning trajectory subset;game formulations;trajectory subset players;game formulations players;quadratic games;class game formulations;subspaces players;unobservable subspaces players;subset players equilibriates;learning settings disturbances;decoupling multi agent"}, "51e5e7093e0183feab61b00ca6c3df61cd8c46de": {"ta_keywords": "discriminative language modeling;extracting phrasal cohorts;based phrase tables;discriminative language;methods discriminative language;phrasal cohorts web;semi supervised methods;semi supervised;supervised methods discriminative;phrase tables;language modeling;investigates semi supervised;phrasal cohorts;cohorts web based;web based phrase;learning methods extracting;paper propose supervised;discriminative;methods extracting phrasal;supervised learning;supervised learning methods;language modeling perform;phrase tables paper;methods discriminative;propose supervised learning;supervised;supervised methods;cohorts web;extracting phrasal;propose supervised", "pdf_keywords": ""}, "9d03a125a9568af8af3fae5091752017d6abe59e": {"ta_keywords": "domains transfer learning;entity recognition increasingly;named entity recognition;entity recognition;robust named entity;domain protein extraction;named entity recognizers;entity recognizers;learning exploiting domain;producing robust named;transfer learning exploiting;entity recognizers demonstrate;regularities robust named;regularities domains transfer;transfer learning;problem domain protein;robust named;robust regularities domains;training test domains;domain protein;exploiting domain task;named entity;task regularities robust;domain task regularities;protein extraction biological;protein extraction;like transfer learning;extraction biological publications;entity;domain task", "pdf_keywords": ""}, "7cbb56da008163df09d254f85b7165f11389f298": {"ta_keywords": "natural language argument;argument reasoning comprehension;reasoning comprehension complexity;argument reasoning;makes comprehension arguments;task argument reasoning;comprehension arguments easy;key reasoning comprehension;comprehension complexity task;arguments easy humans;reasoning comprehension task;algorithm identifying warrants;comprehension task decide;reasoning comprehension;language argument composed;comprehension arguments;comprehension complexity;warrants weighted action;arguments easy;argument composed;warrant candidates plausible;task argument;identifying warrants weighted;correct warrant candidates;language argument;arguments;reasoning;comprehension task;complexity task;automatically identifying correct", "pdf_keywords": ""}, "3f16887a8e5c81aea554c7a266b08ea70dd2aa9a": {"ta_keywords": "allocation rewards propose;unfair allocation rewards;allocation rewards;agent credit assignment;cooperative competitive behaviour;efficiently alleviated cooperative;rewards propose;multi agent credit;cooperative setting;alleviated cooperative setting;individual social incentives;credit assignment approach;cooperative setting state;rewards;reinforcement;interplay cooperative;credit assignment;alleviated cooperative;interplay cooperative competitive;rewards propose extension;social incentives;cooperative;cooperative competitive;incentives paper presents;incentives;explore interplay cooperative;multi agent;agent credit;unfair allocation;novel credit assignment", "pdf_keywords": "agent deep reinforcement;multi agent reinforcement;multi agent deep;agent reinforcement learning;agent reinforcement;deep reinforcement;multi agent credit;multi agent;agent credit assignment;propose multi agent;deep reinforcement learning;agent deep;present multi agent;mixed multi agent;balance incentives;learning model reinforcement;reinforcement learning;reinforcement learning framework;multi agent setups;agent credit;model reinforcement learning;reinforcement;balance incentives experimentally;model reinforcement;decision making cooperative;reinforcement learning task;capability balance incentives;cooperative environments;issues multi agent;reinforcement learning model"}, "a6aed0c4e0f39a55edb407f492e41f178a62907f": {"ta_keywords": "attention incrementally writing;attention contextual text;contextual text attention;writing abstracts based;abstracts based idea;writing abstracts;abstract write abstracts;write abstracts better;text attention incrementally;write abstracts;attention contextual;attention incrementally;text attention;problem writing abstracts;combining graph attention;memory attention networks;graph attention;generate paper abstract;graph attention contextual;attention networks;background knowledge graphs;present paper robot;memory attention;based memory attention;attention;paper abstract;human written papers;incrementally writing;abstracts better theauthors;title follow paper", "pdf_keywords": "mechanism machine translation;graphs unstructured biomedical;literature search pubmed;search pubmed embase;machine translation mle;machine translation;pubmed embase;text link prediction;novel machine translation;unstructured contextual text;unstructured biomedical data;new entity representation;knowledge graphs unstructured;abstract knowledge graphs;search pubmed;enhance knowledge extraction;knowledge extraction;entity representation;link prediction structure;combines unstructured contextual;knowledge graphs;entity representation combines;translation mle framework;structure learning propose;unstructured contextual;link prediction;embase cochrane library;structure learning;contextual text;pubmed embase cochrane"}, "5695847f8ffbb3da078842c3683ef74175eb59e5": {"ta_keywords": "phonetic sound correction;crosslingual speech synthesis;proposed speech synthesis;speech synthesis;based speech synthesis;speech synthesis method;perform speech recognizer;synthesis proposed phonetic;voice conversion hmm;speaker individuality synthetic;synthetic speech compared;swapping unvoiced consonants;voice conversion;based voice conversion;speech synthesis generating;synthetic speech;sound correction method;speech recognizer;speakers various proficiency;sound correction;preserve speaker individuality;versatility speech recognition;proposed phonetic;speech recognition;individuality synthetic speech;prosody correction improve;phonetic sounds based;phonetic sounds er;effects phonetic sounds;degradation speaker individuality", "pdf_keywords": ""}, "37a0f28f6aa41028e64d0440001ff525d67c1305": {"ta_keywords": "envy freeness allocation;approximate envy freeness;agent resource allocation;approximate envy;freeness allocation problems;freeness allocation;welfare approximate envy;efficiency fairness;paper strengthening envy;constrained round robin;strengthening envy freeness;allocation problems;allocation;allocation setting models;envy freeness item;strengthening envy;simulation round robin;welfare efficiency fairness;allocation problems embed;consider multi agent;multi agent resource;envy freeness;round robin attack;allocation setting;resource allocation;multi agent;issue allocation;fairness consider multi;assignment papers reviewers;issue allocation problems", "pdf_keywords": "fair allocation indivisible;finding allocation envy;allocation indivisible goods;allocation envy free;fair allocation;allocation envy;allocations np hard;allocation maximizes welfare;allocation problem utility;allocation indivisible;welfare based allocation;allocations np;maximal set allocations;allocation maximizes;based allocation problems;allocation problems;general allocation problem;considerations fair allocation;allocation problems particular;allocation algorithm case;allocation algorithm;allocation problem;set allocations np;allocations;finding allocation;allocation;whittle allocation algorithm;indivisible goods;resource allocation maximizes;whittle allocation"}, "7bdb04ba2da682e4c0b19b5d61e999d648826edd": {"ta_keywords": "recovering sparse covariance;sparse covariance matrix;sparse covariance estimation;sparse covariance;approach sparse covariance;sparse graph codes;recovering sparse;estimation based sparse;new approach sparse;matrix quadratic measurements;covariance matrix quadratic;approach sparse;noiseless problem np;sparse graph;problem recovering sparse;based sparse graph;robustified noise log2;algorithms robustified noise;covariance estimation;optimal message passing;covariance matrix;covariance estimation based;prove noiseless problem;message passing complexity;based sparse;quadratic measurements small;robustified noise;quadratic measurements;prove noiseless;optimal message", "pdf_keywords": ""}, "d8c1eb86cc4546e4355ed368d8400d7640926cee": {"ta_keywords": "constrained clustering;constrained clustering algorithms;deterministic clustering;new deterministic clustering;work constrained clustering;deterministic clustering algorithm;clustering;efficient gibbs sampling;model based clustering;clustering algorithms;gibbs sampling algorithm;clustering method tvclust;gibbs sampling;based clustering;clustering algorithms literature;probabilistic model data;clustering method;clustering algorithm;posterior inference propose;algorithm based probabilistic;inference propose probabilistic;based bayesian hierarchical;propose probabilistic;based clustering method;based probabilistic;probabilistic model efficient;propose probabilistic deterministic;probabilistic algorithm predicting;proposed posterior inference;bayesian hierarchical", "pdf_keywords": "clustering algorithm video;constraints video segmentation;constrained clustering;image clustering;clustering making labeling;image clustering task;video segmentation;approach constrained clustering;algorithm video segmentation;spectral clustering;supervised clustering;constrained clustering algorithms;present supervised clustering;model image clustering;based spectral clustering;clustering information;improve quality clustering;quality clustering;cluster constraints data;pairwise cluster constraints;video segmentation based;cluster constraints;supervised clustering algorithm;graph based clustering;clustering present probabilistic;quality clustering making;clustering information focusing;segmentation based spectral;clustering;clustering making"}, "fe0ec764813fbcb6b6fd77d82188e81826088103": {"ta_keywords": "discriminative objective;discriminative objective functions;suitable discriminative training;discriminative training;discriminative training applied;speech recognition;speech recognition unified;sequential pattern recognition;automatic speech recognition;functions suitable discriminative;suitable discriminative;pattern recognition;information minimum classification;discriminative;automatic speech;view discriminative objective;pattern recognition problems;minimum classification;recognition unified;unified view discriminative;recognition problems automatic;problems automatic speech;view discriminative;recognition problems;recognition;minimum classification error;classification;classification error minimum;minimum phone word;strings proposed analysis", "pdf_keywords": ""}, "659b476a10b0e676a031b1b17ebfe405c1904227": {"ta_keywords": "wavelet speech recognition;end speech recognition;wavelet speech;combination wavelet speech;todate speech processing;implementation speech recognition;speech recognition;speech processing;performance speech recognition;speech recognition developed;speech recognition based;speech recognition paper;speech processing experience;analysis performance speech;sequence sequence modeling;sequence modeling;recognition based sequence;language processing project;wavelet;design implementation speech;implementation speech;australian language processing;language processing;based combination wavelet;performance speech;sequence modeling paper;applications trained end;combination wavelet;sequence modeling properties;advanced data augmentation", "pdf_keywords": "automatic speech translation;speech processing toolkit;end speech recognition;vc speech translation;speech recognition source;speaker translation developed;speech translation asr;automatic speech;speech translation;complex speech translation;end automatic speech;speech enhancement se;speech translation studies;speaker translation;speech translation st;text speech;voice conversation vc;text speech tts;speech tts voice;toend speech processing;includes text speech;learning machine translation;speech processing;st speech enhancement;speech recognition;speech tts;beamforming speech separation;beamforming speech;machine translation addition;voice conversation"}, "f02948f2976991bb76419775f303c27fc8afb7b5": {"ta_keywords": "text classifiers;ripper rule learning;learning text classifiers;keyword spotting rules;rule learning;classifiers;text classifiers compared;rule learning algorithm;classifier;classifier methods learning;learned classifier;learned classifier methods;classifier methods;classifiers compared;rules based ripper;classification;sets keyword spotting;classifiers compared classification;methods learning text;keyword spotting;learning sets keyword;rules based;classification problems arise;learning algorithm;classification problems;ripper rule;learning text;based ripper rule;spotting rules based;modify learned classifier", "pdf_keywords": ""}, "62a5b47def8d21825d06f7407a505ff0b64ecb1a": {"ta_keywords": "semantic parsing ambiguous;natural language paraphrased;semantic parsing;parsing ambiguous ungrammatical;existing semantic parsing;language paraphrased ambiguous;parsing ambiguous;semantic parsing framework;parsing;paraphrase takes ambiguous;paraphrased ambiguous;queries paraphrase;language paraphrase takes;language paraphrased;parsing framework;method semantic parsing;language paraphrase;probability paraphrase build;search queries paraphrase;paraphrased ambiguous original;paraphrase used disambiguate;queries paraphrase used;paraphrase build;disambiguate meaning representation;natural language;representation natural language;ambiguous ungrammatical input;paraphrase build existing;model input sentence;context free grammars", "pdf_keywords": ""}, "05b0c768ecd4a82e486923e83250ddd53bacbf67": {"ta_keywords": "dimensional nonmetric search;nonmetric search tree;nonmetric search;approximations pruning;approximation pruning;efficient accurate retrieval;linear approximations pruning;search tree based;pruning;approximations pruning process;pruning rules methods;prune arbitrary;pruning rules;effective pruning rules;nn search propose;nn search;short indexing;effective pruning;new pruning rule;linear approximation pruning;pruning rule avoid;pruning rule;pruning rule non;pruning process;approach effective pruning;approximation pruning method;outcome nn search;new pruning;pruning method;propose new pruning", "pdf_keywords": "known similarity search;search known similarity;similarity search;metric nn search;non symmetric distances;data sets distances;distance similarity;similarity search problem;semimetric distances crucially;distances crucially symmetric;distance similarity function;metric space access;respect distance similarity;symmetric distances;semimetric distances;statistical distances;deviates metric distance;metric distance;bounded semimetric distances;sets distances handled;distances handled;known similarity;entries closest;limits statistical distances;closest query point;distance deviates metric;distances crucially;similarity function;distances;non metric nn"}, "1d938731dfad31c09b2f58c365f630c640f2ca1a": {"ta_keywords": "automatic annotation;automatic annotation annotated;outperform active learning;active learning;annotation annotated data;learning outperform active;active learning self;annotated data;active learning al;annotation annotated;method automatic annotation;annotated data despite;annotation;annotated;resorted active learning;trained language models;models natural language;better label efficiency;language models natural;enhance label efficiency;label efficiency researchers;pre trained language;require excessive labeled;label efficiency;label efficiency model;self training baselines;labeled;labeled data;language models;trained language", "pdf_keywords": "active semi supervised;learns pseudo labels;semi supervised active;supervised active semi;active learning selftraining;learning active learning;tasks active learning;active learning;semi supervised learning;robustness active learning;semi supervised;supervised active;active learning method;labeled samples boost;propose semi supervised;new active learning;learning active;active learning active;supervised;labeled samples;learns pseudo;aware sampling;uncertainty self training;querying labels uncertainty;active self training;uncertainty querying labels;pseudo labels labeled;supervised learning;labels uncertainty driven;labels labeled samples"}, "ae82f831bda5681edfe40ec15de4e9d2096ea92f": {"ta_keywords": "clustering words senses;recognizing lexical entailment;vectors infer entailment;alexical entanglement supervised;lexical entailment;involve recognizing lexical;clustering words;term entailment;recognizing lexical;infer entailment;infer entailment different;predict entailment;narrower term entailment;learned predict entailment;entanglement supervised;effects clustering words;similarity measure;entailment;entanglement supervised approach;words senses;words senses using;predict entailment given;entailment subset broader;term entailment subset;context vectors infer;entailment different;asymmetric similarity measure;similarity measure designed;similarity;similarity asymmetric similarity", "pdf_keywords": "clustering word sense;word sense clustering;entailment word similarity;clustering words senses;clustering predict entailment;recognizing lexical entailment;clustering words;tiered clustering word;lexical entailment based;clustering word;approaches recognizing lexical;lexical entailment;sense clustering;sense clustering approach;recognizing lexical;word similarity;effects clustering words;entailment based tiered;computational entailment;entailment word;predict entailment word;lexical entailment submitted;entailment based;specii\ufb01ed computational entailment;problem lexical entailment;infer entailment using;computational entailment important;infer entailment;lexical;word sense"}, "8c9069641876d025c66ab6800939c278b07f60a3": {"ta_keywords": "topic documents hierarchies;hierarchies document graphs;generated document hierarchies;document hierarchies;documents hierarchies documents;document hierarchy;document hierarchies document;constructed document hierarchy;hierarchies documents documents;document hierarchy represent;documents hierarchies;structure document hierarchies;document hierarchies able;hierarchies documents;topic documents;hierarchies document;document graphs;hierarchy documents;topic taxonomies;document graph;topic taxonomies present;hierarchy documents created;documents created topic;explicit hierarchy documents;created topic taxonomies;collection general topics;document links study;topic taxonomy;experiments document graph;document graph data", "pdf_keywords": ""}, "423044220d9642a2d5839cfb19e32171e8a16a83": {"ta_keywords": "dynamics multiarmed bandit;bandit setup modeling;multiarmed bandit;estimate arm reward;multiarmed bandit setup;bandit setup;bandit;arm reward plans;algorithm powering recommender;greedy policy plays;arm reward;reward plans;greedy policy;satiation dynamics multiarmed;reward plans using;greedy policy human;generalization greedy policy;powering recommender;powering recommender systems;generalization greedy;modeling satiation;recommender systems resulting;reward;greedy;satiation dynamics;study satiation dynamics;using generalization greedy;modeling satiation time;paper greedy policy;paper greedy", "pdf_keywords": "bandits framework models;armed bandits framework;bandits framework;bandit problem;rebounding bandits;rebounding bandits multi;introduce rebounding bandits;bandit problem non;multiarmed bandit problem;multi armed bandits;leaner multiarmed bandit;bandits;satiation dynamics stochastic;multiarmed bandit;bandits multi armed;bandit;minimizing regret leaner;bandits multi;stochastic model satiation;satiation rewards;armed bandits;problem minimizing regret;satiation rewards decline;recommendations introduce stochastic;minimizing regret;dynamical model satiation;model satiation dynamics;stationary rewards;pull satiation model;learner given decision"}, "be0ad0710bfb09f6c875dd6cd834ac643713c93d": {"ta_keywords": "level coupled heat;coupled heat bath;coupled heat baths;baths coupled heat;heat baths coupled;dynamics level coupled;coupled heat;baths coupled;level coupled;heat bath heat;baths different temperatures;heat bath consists;heat baths;bath consists heat;heat baths different;consists heat baths;bath heat;heat bath;dynamics level;bath heat bath;baths;dynamics;baths different;heat;bath consists;bath;consists heat;temperatures;different temperatures;level", "pdf_keywords": ""}, "4a0f96bb17836b4c4d6e19627f176fba8fe05127": {"ta_keywords": "circuit breakers;circuit breaker dc;circuit breaker;circuit breaker operating;state circuit breakers;breaker dc network;breaker operating principle;breaker operating;performance circuit breaker;circuit breaker breaking;breaker dc;fault current limiting;breaker breaking;circuit breakers fabricated;analyze circuit breaker;current limiting circuit;breaker breaking closing;breaker;variation circuit breaker;active fault current;breakers;breakers fabricated characterized;fault current;limiting circuit;current limiting;limiting circuit designed;breakers fabricated;relay limited;circuit current;principle circuit current", "pdf_keywords": ""}, "1d5d170670889bd82364fbcc594dadcb5481e9e4": {"ta_keywords": "neural machine translation;translation nmt recall;translation tasks repetitiveness;improves translation results;machine translation;machine translation nmt;method improves translation;translation tasks;translation examples incorporating;domain translation tasks;improves translation;retrieved sentences;translation examples;retrieve sentences sentences;retrieved source sentence;narrow domain translation;increase translation time;sentence retrieved source;retrieved sentences method;translation results narrow;translation results;repetitiveness target sentences;retrieve sentences;weight retrieved sentences;translation nmt;method retrieve sentences;increase translation;sentences translated source;sentence retrieved;alternative retrieval", "pdf_keywords": "neural machine translation;machine translation;machine translation ability;machine translation research;improved gram translation;machine translation state;task machine translation;domain translation tasks;translation based retrieving;gram translation results;translate neural machine;translation tasks;narrow domain translation;machine translation important;machine translation based;based retrieving translation;retrieving translation;gram translation;retrieving translation pieces;retrieves translation;translate neural;new translation method;increase translation time;translation state art;retrieves translation pieces;domain translation;translation important task;translation results;want translate neural;translation method"}, "f32c67daa6a93281bd8645fc2fa423dca67aea00": {"ta_keywords": "assignment papers reviewers;automated assignment papers;assignment algorithms overcomes;assignment algorithms;various assignment algorithms;reviewers conference peer;automated assignment;complex networks fairness;assignment algorithm based;experiments peer review;assignment algorithm;papers reviewers;conference peer review;assignment papers;papers reviewers conference;networks fairness objective;peer review;networks fairness;maximize review quality;design assignment algorithm;experiments peer;problem automated assignment;fairness statistical accuracy;reviewers conference;truth experiments peer;conference peer;publication journal statistical;total quality papers;algorithms;near optimally fair", "pdf_keywords": "peer review algorithm;algorithm assigning reviewers;similarity peer review;review algorithm;peer review systems;assignment papers reviewers;peer review;peer review problem;paper peerreview4 algorithm;assigning reviewers;reviewers conference peer;conference peer review;sum similarity peer;review algorithm called;assigning reviewers submission;papers reviewers;different peer review;peerreview4 algorithm achieve;peer review focus;new peer review;peerreview4 algorithm;reviewers submission paper;automatic assignment papers;similarity peer;total quality papers;peerreview4 algorithm able;maximizing total quality;paper peerreview4;submission paper peerreview4;review systems"}, "827e0def212f6834d615e4f3f25b55fe27f6460d": {"ta_keywords": "grained semantic information;semantic verb relations;grained semantic;annotation semantic verb;fine grained semantic;annotation semantic;verb relations manually;relations manually annotated;quantum error correction;sensitive verb relations;quantum;state quantum;approach annotation semantic;semantic information;manually annotated;semantic information verb;entailment relations;measurement quantum;quantum state quantum;state quantum present;quantum present;semantic;verb relations paper;annotation;temporal entailment relations;quantum state;verb relations;quantum present new;measurement quantum state;based measurement quantum", "pdf_keywords": ""}, "d8ad713ffde54d0a837e6a9cab4e70739d649d41": {"ta_keywords": "dialog response retrieval;chat oriented dialog;dialog systems;dialog systems utilize;oriented dialog systems;based dialog response;example based dialog;dialog pair database;human human conversation;response retrieval based;human conversation;response retrieval;dialog response;example based chat;neural network paraphrase;based dialog;distributed word representation;good response dialog;network paraphrase identification;dialog;dialog systems paper;dialog pair;based chat oriented;chat oriented;paraphrase identification technique;oriented dialog;paraphrase identification;response dialog;response dialog pair;simple retrieval", "pdf_keywords": ""}, "f45c777b29e0a00f7b1e1f33daa751853015724a": {"ta_keywords": "automotive industry liaison;liaison officer acm;automotive;position automotive;new position automotive;automotive industry;officer acm;position automotive industry;autonomous systems;commissioning;autonomous;industry liaison officer;officer acm paper;acm;liaison officer;industry liaison;commissioning new;commissioning new position;field autonomous systems;present results commissioning;results commissioning;acm paper;acm paper presents;results commissioning new;art field autonomous;liaison;field autonomous;current state art;officer;state art", "pdf_keywords": ""}, "1f7fb2c16e51f207eb1816f4f3fc3e1649c364f0": {"ta_keywords": "mismatches speech recognition;environment mismatches speech;mismatches speech;speech recognition;environment mismatches;effect environment mismatches;mismatches;recognition;speech;environment;effect environment;study effect environment;effect;paper study effect;paper study;study effect;study;paper", "pdf_keywords": ""}, "83f648f01d858d02b20f9327bebb1d5e91d0b6a9": {"ta_keywords": "minimizing speech recognition;continuous speech recognition;speech recognizer;speech recognizer based;automatic speech recognizer;optimization decoding networks;minimizing speech;speech recognition error;speech recognition;optimize decoding networks;decoding networks;discriminative optimization decoding;speech recognition problem;optimization decoding;framework automatic speech;decoding networks important;decoding network optimization;automatic speech;decoding networks extending;important minimizing speech;optimize decoding;approach continuous speech;decoding network;minimization discriminative optimization;incorporated decoding network;classification process decoding;error minimization discriminative;discriminative optimization;methods incorporated decoding;reported optimize decoding", "pdf_keywords": ""}, "099346e2837c53ded931d98135edbb261039764a": {"ta_keywords": "voting rules efficiently;multi winner voting;computational social choice;winner voting rules;results computational social;computational social;voting rules;winner voting;design multi winner;group decision;voting;choice mitigate polarization;group decision making;decision making algorithm;multi winner;propose group decision;paper dynamics democratic;dynamics democratic;group polarization;polarization;polarization paper;dynamics democratic process;group polarization xmath0;social choice;social choice mitigate;machine learning;democratic process;rules efficiently computable;polarization xmath0;proportionality representation paper", "pdf_keywords": ""}, "cbdccaa4a5bceaae190f78b1ac0a0cf47391968d": {"ta_keywords": "platforms creators;curation creation algorithms;curation creation;creation algorithms microcontent;distinction platforms creators;platforms;curation;algorithms microcontent;vanishing distinction platforms;creators;algorithms microcontent vanishing;distinction platforms;creation algorithms;creation;microcontent;algorithms;microcontent vanishing distinction;microcontent vanishing;distinction;vanishing distinction;vanishing", "pdf_keywords": ""}, "09fcc7ed1f867bcf9133ab12065ee7366cfaa652": {"ta_keywords": "fault tolerant training;fault tolerance training;distributed systems checkpointing;efficient fault tolerance;fault tolerant;systems checkpointing;faster checkpointing;based fault tolerant;10 faster checkpointing;systems checkpointing primary;used fault tolerance;checkpointing;erasure based fault;rethinking fault tolerance;fault tolerance;fault tolerance dlrm;fault tolerance systems;recovering failures;dlrm training deep;learning large distributed;checkpointing primary approach;training deep learning;erasure coding reduce;erasure coding;machines present erasure;faster checkpointing paper;checkpointing primary;erasure coding imparting;imparting efficient fault;efficient fault", "pdf_keywords": "checkpointing machine learning;erasure codes training;training checkpointing checkpointing;training checkpointing;erasure codes machine;erasure coded storage;erasure coding;checkpointing machine;fault tolerant training;machine translation erasure;erasure codes ideas;based erasure coding;coding present erasure;tolerant training datacenters;datacenters based erasure;adapting simple erasure;use erasure codes;coded storage overcomes;checkpointing checkpointing machine;erasure coding present;dlrm training checkpointing;erasure codes;simple erasure codes;erasure coded;translation erasure codes;checkpointing checkpointing;checkpointing;fault tolerant;storage overcomes challenges;coded storage"}, "8d939637b3a5ecf681130619cd35f295dbb9db03": {"ta_keywords": "aptt associated snps;thromboplastin time rs710446;patients vt rs710446;nucleotide polymorphisms snps;associated snps;thromboplastin;snps recently associated;effects aptt associated;1542 patients vt;thromboplastin time;nucleotide polymorphisms;activated partial thromboplastin;single nucleotide polymorphisms;partial thromboplastin;polymorphisms snps recently;polymorphisms snps;partial thromboplastin time;rs710446 risk vt;decreased apt tt;new genetic risk;associated snps rs2731672;genetic risk;genetic risk factor;patients vt;apt tt levels;vt rs710446 allele;patients single nucleotide;aptt associated;allele associated decreased;factor vt study", "pdf_keywords": ""}, "0bcd8210e9b90b33ab8467b94fd9b9511aad0f86": {"ta_keywords": "multiprocessor based pipeline;pipeline processing architecture;based pipeline processing;propose multiprocessor;performance pervasive application;propose multiprocessor based;multiprocessor based;pipeline processing;high performance pervasive;processing architecture support;build pervasive applications;multiprocessor;based pipeline;pervasive applications constantly;dynamic computing environment;pervasive application development;resources heterogeneous devices;processing architecture;pervasive applications;performance pervasive;pipeline;pervasive application;devices build pervasive;performance integrate distributed;heterogeneous devices build;computing environment;computers embedded;build pervasive;distributed resources heterogeneous;highly dynamic computing", "pdf_keywords": ""}, "cece2d2f7cc38a512325122401f8aa658121b80e": {"ta_keywords": "detecting deception humans;automatic deception detection;effective detecting deception;deception detection;detecting deception;detect deception;attempt detect deception;automatic deception;deception detection focuses;detect deception perform;potentially deceptive conversation;deceptive conversation partner;deceptive conversational partner;deception asking questions;deception humans attempt;deceptive conversation;deception asking;deception humans;deceptive conversational;work automatic deception;signs deception asking;deception perform actions;intelligent dialog;deception;intelligent dialog strategy;telltale signs deception;deception perform;unveil deceptive conversational;dialog strategy slightly;signs deception", "pdf_keywords": ""}, "4bc9d6596069c9277b57a7ee1e1127d231f28663": {"ta_keywords": "unsupervised constituency parsing;trees autoencoder based;parse tree;trees autoencoder;scoring parse tree;syntactic tree structures;constituency parsing;syntactic tree;learns induce syntactic;induce syntactic tree;tree chart parsing;trees sentence soft;mixture trees autoencoder;parsing;autoencoder based recursive;parse;sentence soft dynamic;parse tree chart;structures input sentences;tree structures input;scoring parse;tree encodes;self supervised neural;trees sentence;binary trees sentence;tree encodes single;sentences access labeled;algorithm variant autoencoders;supervised;autoencoders", "pdf_keywords": ""}, "dbb4035111c12f4bce971bd4c8086e9d62c9eb97": {"ta_keywords": "graph learning models;graph machine learning;graph learning;ways graph learning;network learning multi;citation networks vast;multi view networks;network learning;learning multi view;view networks;convolutional network learning;networks vast;graph based convolutional;large scale graph;citation networks;labelling citation networks;graph machine;networks vast majority;social networks people;networks;social network;nature social networks;social networks;person social network;networks people;view networks including;networks developing countries;networks including;semi supervised;semi supervised learning", "pdf_keywords": "graph embeddings;graph embeddings automatically;graphs convolutional neural;multilayer graphs convolutional;uses graph embeddings;multi view graphs;graphs convolutional;learning multi view;view graphs;network learning multi;multi view networks;graph based convolutional;network learning;learning uses graph;view networks;convolutional network learning;proposes semi supervised;multilayer graphs;semi supervised;semi supervised node;multiview learning;approach multiview learning;data semi supervised;supervised approach multiview;semi supervised approach;graph structured data;graph structured;graph based;multiview learning integrates;semisupervised learning"}, "8376b18a4dd228ea4c33d606b32b081cee9bf80a": {"ta_keywords": "sparse vector minimizes;stochastic optimization;stochastic gradient based;finding sparse;sparse vector;stochastic gradient;finding sparse vector;minimizing smooth convex;stochastic optimization approach;similar stochastic gradient;unconstrained problem minimizing;present stochastic optimization;derivative free algorithm;gradient based algorithms;problem finding sparse;optimization problem complexity;gaussian additive noise;complexity bound derivative;additive noise propose;sparse;accelerated gradient based;minimizing smooth;vector minimizes;minimizes;bound accelerated gradient;gradient based algorithm;gaussian additive;optimization;unconstrained linear;free algorithm complexity", "pdf_keywords": "unconstrained optimization;free optimization methods;free optimization;stochastic optimization problems;unconstrained optimization problem;stochastic optimization;following unconstrained optimization;gradient free optimization;optimization methods;optimization methods point;optimization problems point;solve stochastic optimization;optimization problems additional;linear multiarmed bandits;optimization;optimization problems;unconstrained problems;solve unconstrained problems;tic stochastic optimization;bounded noise objective;multiarmed bandits;feedback solve unconstrained;unconstrained problems paper;optimization problem;multiarmed bandits results;propose gradient free;solve unconstrained;euclidean proximal setup;following unconstrained;unconstrained"}, "1e5b826ddf0754f6e93234ba1260bd939c255e7f": {"ta_keywords": "autoregressive machine translation;nonautonomous machine translation;autoregression neural models;autoregression neural;concept autoregression neural;machine translation coupled;pretrained autoregressive model;data pretrained autoregressive;pretrained autoregressive;neural models usually;neural models;non autoregressive machine;machine translation translational;machine translation;neural networks nns;autoregressive machine;nat training neural;freedom machine translation;training neural;autoregressive model better;machine translation nat;autoregressive models;model non autoregressive;autoregressive nat models;translation coupled nonlinear;nonlinear autoregressive nat;autoregressive model;nonlinear autoregressive;knowledge distillation crucial;training neural networks", "pdf_keywords": "machine translation reordering;neural machine translation;machine translation nat;machine translation;machine translation research;machine translation mt;translation uncertainty neural;translation reordering based;translation nat models;field machine translation;instantiation machine translation;translation reordering;approach machine translation;translation uncertainty;reordering based deep;predicting language;translation nat;measure translation uncertainty;translation research;predicting language types;successful predicting language;entropy measure translation;translation mt emerging;language types sequential;training sentence pair;deep autoregressive models;metrics measuring complexity;based deep autoregressive;deep autoregressive;measuring complexity faithfulness"}, "41d4763792db8ea420efcfbd112a55deec971fee": {"ta_keywords": "yago ontology;yago ontology used;encyclopedic knowledge common;ontology;encyclopedic knowledge;relationship encyclopedic knowledge;knowledge common knowledge;ontology used;knowledge common;common knowledge;knowledge;quantum mechanics;common knowledge present;ontology used tool;dynamics human brain;context quantum mechanics;knowledge present simple;relationship encyclopedic;knowledge present;symmetry breaking context;quantum;encyclopedic;quantum mechanics paper;types knowledge;symmetry;phenomenon spontaneous symmetry;context quantum;spontaneous symmetry;spontaneous symmetry breaking;dynamics human", "pdf_keywords": ""}, "4cc97c3858b558b4fa80ad73a894fcc7df841114": {"ta_keywords": "fairness confusion tensor;predictive performance fairness;fair model predictive;fairness definitions practice;fairness definitions;notions fairness;fairness different notions;different notions fairness;fairness expressed fairness;notions fairness paper;performance fairness;fairness confusion;designing fair model;tradeoffs designing fair;range fairness definitions;fair model;wider range fairness;fairness paper present;performance fairness different;fairness paper;range fairness;fairness expressed;paper fairness expressed;expressed fairness;fairness;fairness different;expressed fairness confusion;washington paper fairness;paper fairness;tensor based", "pdf_keywords": "fairness constraints fairness;fairness conditions fairness;fairness versatile scenarios;analyze fairness conditions;based fairness notions;fairness constraints;fairness conditions;fairness conditions multiple;new notion fairness;fairness confusion tensor;multiple fairness conditions;understand fairness fairness;tradeoffs fairness measures;analyze fairness;fairness notions;multiple fairness constraints;constraints fairness versatile;fairness notions widely;express fairness terms;conditions multiple fairness;group fairness model;model based fairness;fairness model based;fairness versatile;fairness terms;fairness confusion;multiple fairness;conditions fairness;fairness measures;fairness terms multiple"}, "72d89aa7cd77c3f22a667f2b0707758eb8d52a7a": {"ta_keywords": "contexts human translators;context translation;context used disambiguate;understand meaning contextual;contextual context translation;words context;words require context;words context aware;disambiguate;context translation translation;disambiguate pros;used disambiguate pros;meaning contextual contextuality;contexts;supporting words context;information machine translation;machine translation;disambiguate pronouns polysemous;contextuality;disambiguate pronouns;human translators use;meaning contextual;used disambiguate;inaccurately disambiguate;human translators;cons used disambiguate;inaccurately disambiguate pronouns;contextual;aware machine translation;contexts human", "pdf_keywords": "neural machine translation;pronoun translation neural;translation neural machine;machine translation improve;translation neural;machine translation learns;aware pronoun translation;translation learns anaphora;machine translation;machine translation research;ambiguity translation quality;machine translation word;translation quality improve;translation word sense;tasks machine translation;translation improve pronoun;useful pronoun disambiguation;translation learns;translation quality;context ambiguous translations;translation improve;words 14k translations;translations professional translators;translations;document level translation;improve pronoun anaphora;14k translations professional;translators useful;word sense disambiguation;professional translators useful"}, "d170bd486e4c0fe82601e322b0e9e0dde63ab299": {"ta_keywords": "neural language modeling;softmax;modeling extend softmax;representations neural language;character input cnn;neural language;input representations neural;model words;extend softmax;layers model words;representations neural;model words characters;adaptive embeddings;adaptive input representations;language modeling;language modeling extend;predictions self adjoint;self adjoint architecture;neural;billion word benchmark;input cnn;softmax grave;input cnn having;softmax grave et;word benchmark achieve;word benchmark;extend softmax grave;input representations;embeddings;adaptive embeddings twice", "pdf_keywords": "language modeling deeper;character input cnn;attentional models;self attentional models;train language models;word embeddings;idea word embeddings;language modeling tasks;attentional models perform;word models character;language models;word models;word embeddings used;input cnn;adaptive input embeddings;perform language modeling;input cnn having;language modeling;language model;level language modeling;input embeddings;adaptive embeddings;models perform language;power word models;attentional;word based models;character level language;word inputs character;performs word inputs;model performs word"}, "3d3f01feee0dd3eea22e390c80deaadc6f11eb9a": {"ta_keywords": "end speech recognition;multichannel cnn encoder;cnn multichannel end;speech recognition multichannel;cnn encoder;baseline chime corpus;chime corpus;speech recognition mtsc;multichannel cnn;cnn multichannel;cnn encoder uses;speech recognition;attention based encoderdecoder;encoderdecoder neural network;systems multichannel cnn;encoderdecoder neural;network cnn multichannel;speech recognition systems;network cnn;chime corpus respectively;recognition multichannel end;neural network cnn;based encoderdecoder neural;encoder;chime challenge;convolutional neural;based encoderdecoder;convolutional neural network;recognition systems multichannel;baseline chime", "pdf_keywords": "end speech recognition;train deep convolutional;deep convolutional neural;deep convolutional;speech recognition;inputs train deep;extension deep convolutional;speech recognition computer;convolutional neural networks;multilevel convolutional neural;network automatic speech;recognition based deep;convolutional neural;convolutional neural network;residual learning batch;automatic speech;speech recognition model;based deep convolutional;multilevel convolutional;chime5 challenge;chime5 challenge using;automatic speech recognition;chime challenge;learning batch;chime data 78;models trained;end speech;end end speech;trained 40 chime;chime data"}, "b2fd7297f7681f9e3ea860cecf1ec97b2cc8ccc3": {"ta_keywords": "wound torsional trap;prediction outcome surgery;closed wound torsional;wound torsional;torsional trap;outcome surgery;outcome surgery person;person closed wound;surgery person closed;surgery person;closed wound;surgery;prediction outcome;method prediction outcome;torsional;trap;wound;outcome;method prediction;new method prediction;prediction;person closed;method;closed;person;new method;present new method;new;present new;present", "pdf_keywords": ""}, "ff783c4709a095cc581534fec58ef9515613ebc9": {"ta_keywords": "catastrophic forgetting;explainability continual learning;explanations hypothesize forgetting;continual learning;goal continual learning;suffering catastrophic forgetting;reduced forgetting;continual learning goal;learning goal continual;rememberinging;forgetting;hypothesize forgetting;continual learning ll;called rememberinging;results reduced forgetting;hypothesize forgetting reduced;memory form replay;forgetting reduced;paradigm called rememberinging;learn sequence tasks;called rememberinging right;ensures predictions predictions;model encouraged remember;remember evidence previously;encouraged remember evidence;buffer ensures predictions;explainability continual;reduced forgetting importantly;rememberinging right;remember evidence", "pdf_keywords": "explainability continual learning;continual learning memory;learning memory;remembering;memory;called remembering;visual model explanations;lifelong learning hippocampus;learning hippocampus;paradigm called remembering;continual learning;remembering right reasons;memory based;explainability continual;called remembering right;generate model explanations;saliency based explanation;learning memory exploited;encouraging explanations remain;improved performance explainability;remembering right;lifelong learning;predictions encouraging explanations;model explanations demonstrate;encouraging explanations;learning hippocampus neocortex;past experience replay;model explanations;explanations demonstrate;experience replay"}, "f2e7598464a0b9376771ffc4ba243233ee12c677": {"ta_keywords": "lexical sememe prediction;lexical semantic competition;annotate word sememes;recommending sememes words;words sememes semantic;lexical sememe;automatically recommending sememes;introduction lexical semantic;sememes semantic;methods lexical sememe;semantic competition;lexical semantic;recently lexical sememe;external context words;sememes semantic units;introduction lexical;automatically annotate word;existing methods lexical;sememes words;lexical;word sememes;words sememes;elementary introduction lexical;annotate word;sememes form linguistic;sememes words expected;sememe prediction task;improve annotation efficiency;semantic competition consists;frequency words sememes", "pdf_keywords": "words sememe prediction;semantic sememes words;semantic sememes;words based sememes;sememes minimum semantic;construct semantic sememes;semantic embeddings words;sense disambiguation sentiment;good word embeddings;word embeddings;semantic embeddings;learning semantic embeddings;lingual word similarity;sememes words;sememes words need;disambiguation sentiment;embeddings words;embeddings words based;disambiguation sentiment analysis;models recommend sememes;prediction sememe prediction;sememe prediction sememe;sememe prediction using;word embeddings hard;propose sememe prediction;word sense disambiguation;sememe prediction;words sememe;information words sememe;prediction sememe"}, "48b18bf5c9cad0e4c36b2d885f380c5c637e1a09": {"ta_keywords": "learning disentangled representations;learning disentangled;learned conditional prior;variational autoencoders;focuses variational autoencoders;disentangled representations;representation factorized prior;optimal conditional prior;disentangled representations focuses;conditional prior focuses;regularization latent space;variational autoencoders vaes;conditional prior;literature learning disentangled;identifiability learned conditional;autoencoders;learned conditional;guarantees disentanglement;model learns sufficient;prior serves regularization;compact optimal conditional;theoretical guarantees disentanglement;model learns;factorized prior distribution;factorized prior;conditional prior serves;representations focuses variational;learns sufficient;complementing input observations;developments demonstrate disentanglement", "pdf_keywords": "learning disentangled representations;unsupervised learning disentangled;learn disentangled representations;learning disentangled;disentangled representations using;disentangled representations;alternatives learn disentangled;disentangled representations experimental;learn disentangled;disentangled representations according;vae learn disentangled;deep generative;deep generative models;leads disentangled representations;generative models;learned representations;variational autoencoders;evidence learned representations;unsupervised learning;disentangled;learning data representations;focus deep generative;guarantees representation learning;identi\ufb01ability generative models;generative models investigated;semi supervised;based variational autoencoders;generative;generative model;generative models particular"}, "ee5dc631a682696a4704b742ea087e8abb5df897": {"ta_keywords": "lagrange self supervised;speech recognition tasks;self supervised training;self supervised;unsupervised speech recognition;cycle based language;unsupervised speech;unsupervised learning librispeech;speech recognition;librispeech babel corpus;self organized criticality;learning librispeech babel;language model;babel corpus;benefits unsupervised speech;language model penalise;corpus;supervised training;supervised;based language model;unsupervised learning;hypotheses forwarding tts;recognition tasks;demonstration self organized;recognition tasks experiments;supervised unsupervised learning;criticality sic self;babel corpus reduced;learning librispeech;organized criticality sic", "pdf_keywords": ""}, "162c3cf78af48ddf826ec76a1a3767a88a730170": {"ta_keywords": "level coupled harmonic;coupled harmonic oscillator;dynamics level coupled;harmonic oscillator;coupled harmonic;level coupled;dynamics level;oscillator;dynamics;harmonic;level;study dynamics level;coupled;study dynamics;paper study dynamics;paper study;paper;study", "pdf_keywords": ""}, "c8a5d05cb741b3448ec4106d2006ae24a7a401b4": {"ta_keywords": "fast robots;robots called fast;called fast robots;fast robots floor;transducer models streaming;fast electro mechanical;sequence transducer;approach sequence transducer;robots;fast electro;sequence transducer trt;transducer;robots called;transducer models;optimization transducer;optimization transducer models;robots floor laboratori;new class robots;transducer trt detection;robots floor;reducing delay approaches;development fast electro;level optimization transducer;class robots;transducer trt;fastemit suitable sequence;stochastic differential;class robots called;models streaming;emit fast", "pdf_keywords": "dataset automatic speech;domain speech recognition;speech recognition recurrent;automatic speech;label automatic speech;effective transducer regularization;transducer regularization;speech recognition asr;automatic speech recognition;speech recognition;spoken domain speech;regularization directly sequence;domain speech;training transducer models;transducer architecture spoken;transducer regularization method;sequence probability training;probability training transducer;architecture spoken domain;spoken domain;transducer word alignment;neural network transducer;regularization method fastemit;recognition recurrent;latency regularization directly;training transducer;regularize emission latency;latency regularization;regularization directly;recognition asr"}, "9918ea4b68e90e1257953b6f2665b2ce29f2bc8b": {"ta_keywords": "linear beamformers wiener;nonlinearity linear beamformer;beamformers wiener filter;spectral mapping based;spectral mapping using;linear beamformer;based linear beamformers;spectral mapping;beamformers wiener;linear beamformers;complex spectral mapping;magnitude spectrum losses;linear beamformer present;waveform magnitude spectrum;mapping using waveform;spectrum losses;perform complex spectral;magnitude spectrum;dnns perform complex;measure nonlinearity linear;spectrum;mapping based linear;approach complex spectral;spectrum losses paper;spectral;wiener filter;complex spectral;2016 l3das22 challenge;waveform;using waveform magnitude", "pdf_keywords": "speech enhancement mfwm;approaches speech enhancement;speech enhancement;neural beamforming enhancement;letter speech enhancement;speech enhancement paper;predicting target speech;end speech recognition;iterative neural beamforming;beamforming multimicrophone;linear beamforming multimicrophone;neural beamforming;speech recognition neural;recognition neural beamforming;beamforming multimicrophone complex;channel multimodal convolution;multi channel multimodal;beamforming enhancement;beamforming enhancement ineube;speech recognition;multimodal convolution network;channel multimodal;target speech signals;microphone channels;speech signals multiple;multi letter speech;enhancement mfwm;multimodal convolution;multichannel;new machine learning"}, "30109a213aa10765486c676ecfa511db227ab543": {"ta_keywords": "neural machine translation;mini batch molecules;machine translation;machine translation nmt;mini batches;batch molecules;making mini batches;mini batch;mini batch creation;machine translation uses;mini batches present;translation nmt neural;translation uses neural;sorting corpus;idea mini batch;batch molecules experimental;creation mini batch;sorting corpus based;choice mini batch;batches present explanation;batches;nmt neural machine;atom;neural machine;method sorting corpus;batch;condensed matter physics;atoms;sentence length making;sorting", "pdf_keywords": "sentences mini batches;sentence mini batches;neural machine translation;mini batches sorting;machine translation;machine translation create;sort training corpus;consecutive sentences mini;mini batches based;mini batches;mini batches according;machine translation important;mini batch;mini batch creation;various mini batch;create mini batches;sentences mini;making mini batches;training corpus grouping;sentence mini;batches sorting strategies;training corpus;batches sorting;sentence making mini;mini batches work;translation create mini;grouping consecutive sentences;batches according;effect mini batch;batches"}, "5e6acc5c73f22c2dbbb4910f656a03cf40a2fe15": {"ta_keywords": "recursive prologs used;non recursive prologs;prologs used track;recursive prologs;prologs used;movement animals noisy;track movement animals;movement animals;prologs;animals noisy environment;animals noisy;non recursive;recursive;animals;track movement;movement;noisy environment;used track movement;noisy;track;used track;environment;non;used", "pdf_keywords": ""}, "03e62d5f0265608c6ebdebba0870131b056b79a6": {"ta_keywords": "online harms relate;experiences harm online;online harms;harm online;harmful content online;harm online development;harm understood perspectives;harm harassment prioritization;severity online content;understandings experiences harm;experiences harm;practices harm;relate prioritize harms;prioritize harms relevant;harmful content;prioritize harms;practices harm mitigation;harm harassment;harm mitigation;variety online harms;severity harm understood;harms relate prioritize;severity harm;harm understood;harms relevant research;harms relate;severity online;harm aid efforts;grounded theory approach;grounded theory", "pdf_keywords": "online harassment focus;online harassment;online harm empirical;literature online harassment;severity online harm;harm reality harassment;harassment include types;online harm;harm experienced online;reality harassment include;reality harassment;harassment focus emotional;harm empirical interviews;harassment include;harassment focus;violations different media;harassment;emotional harm reality;severity types harm;experienced online contexts;online contexts;severity online;analysis social media;social media users;defined harm experienced;harm empirical;content analysis social;defined harm;harm reality;social media platforms"}, "33ce3cd897a3473973f338c154f3fe5c1175643c": {"ta_keywords": "question answering;question answering tasks;question answering recommendation;domain question answering;applications question answering;reasoning trained structured;language model opql;answering recommendation;answering tasks kkbs;knowledge structured;reasoning trained;knowledge structured readily;answering tasks;language model;enables reasoning trained;encode world knowledge;world knowledge structured;demonstrate language model;integrated language model;human annotation efforts;annotation efforts;human annotation;language model used;answering;memory integrated language;structured supervision;library instead opql;trained structured supervision;enables reasoning;trained structured", "pdf_keywords": "question answering;domain question answering;question answering recommendation;question answering tasks;200k entities answering;answering queries;applications question answering;answering queries containing;structured queries effectively;large knowledge bases;entities answering;semi structured queries;tasks answering queries;answering recommendation;structured queries;language model opql;queries effectively;knowledge bases;templates large knowledge;retrieval models;relations 200k entities;answer relational;retrieval models proposed;answering tasks answering;effectively fact opql;queries effectively fact;complexity queries generated;answering tasks;queries generated;fact opql outperforms"}, "b00bc4dcce60e7c631a23d60894e51001de1c630": {"ta_keywords": "ebola virus glycoprotein;virus glycoprotein;virus glycoprotein gp;glycoprotein cleavability viral;virus pseudotyped glycoprotein;properties ebola virus;glycolytic acid molecules;ebola virus;virus fluorescence microscope;glycolytic acid;virus fluorescence;functional properties ebola;propagation virus fluorescence;glycolytic;viral infectivity gp;properties ebola;glycoprotein;infectivity gp cleavage;glycoprotein cleavability;ability glycolytic acid;antiviral pseudotype;antiviral pseudotype used;cleavability viral infectivity;viral infectivity;glycoprotein gp;virus antiviral pseudotype;ebola;bacteriophage gn;bacteriophage gn xmath0;virus antiviral", "pdf_keywords": ""}, "2583e7e279e2969493c3290c8f300ab32da40bf9": {"ta_keywords": "resource languages improved;high resource languages;cross linked linkage;low resource languages;linkage models;cross linked;recently transfer learning;lexical machine learning;linkage linkage models;linked linkage;machine learning lex;transfer learning;transfer learning methods;languages utilizing resources;resource languages;lexical machine;resource languages utilizing;linkage;linked linkage linkage;linkage models shown;closely related languages;linkage linkage model;linkage model;resources closely related;related languages;called lexical machine;learning algorithm lex;linkage linkage;related languages approaches;linkage model linkage", "pdf_keywords": "crosslingual entity linking;unsupervised entity linking;inter language links;entity linking;crosslingual entity;entity linking xel;focus crosslingual entity;translate entities source;english crosslingual wikification;language links;translate entities;entity linking evaluate;crosslingual wikification;existing lexicon induction;lexicon induction methods;large monolingual data;crosslingual wikification important;monolingual data source;used translate entities;instance existing lexicon;lexicon induction;information extraction languages;extraction languages extensive;monolingual data;applications cross lingual;language links work;target language;extraction languages;wikipedia anchor text;entities source"}, "0bb30ed3340d2c34fe9f37c5002929bc5f458c23": {"ta_keywords": "biomedical text mining;protein relation datasets;relation extraction task;relation extraction;graph transformer bert;information biomedical text;use graph neural;ary relation extraction;bidirectional encoder representations;natural language processing;extract information biomedical;biomedical text;graph neural;processing nlp;processing nlp task;existing biomedical text;lstm attention;nlp task;relation datasets automatically;information biomedical literature;memory lstm attention;relation datasets;sentences use graph;term memory lstm;encoder representations;mechanism bert architecture;language processing nlp;graph neural network;representations transformers graph;encoder representations transformers", "pdf_keywords": "biomedical relation extraction;sentence relation extraction;relation extraction tasks;protein relation datasets;relation extraction 2015;relation extraction task;relation extraction;learns relationships entities;ary relation extraction;deep neural;abstract sentence relations;propose deep neural;sentence relations;model learns relationships;biomedical relation;relation datasets;search novel genomic;network cnn;sentence relation;short term memory;relation datasets suggesting;cross sentence relation;attention guided convolutional;learns relationships;datasets suggesting bert;cnn model learns;sentence relations addition;protein relation;attention mechanism graph;deep neural network"}, "9bbc8ca94810e8a21e4a6a55a5913c5b0b6c787f": {"ta_keywords": "speech transcription respeaking;speech transcription;line speech transcription;transcription respeaking;transcription respeaking results;accuracy word typing;efficient line speech;speech segmented;segmented smaller utterances;transcription;automatic transcript paper;smaller utterances using;costs speech segmented;word typing;automatic transcript;speech segmented smaller;transcriptional;processing nlp letter;processing nlp;language processing nlp;smaller utterances;simple model transcriptional;utterances using;transcriptional regulation membrane;initial automatic transcript;model transcriptional;natural language processing;transcript paper;language processing;utterances", "pdf_keywords": ""}, "7e0570f498a5de4f2a861546d4e67ba208f71d12": {"ta_keywords": "speaker recordings benchmark;speaker recognition benchmark;speaker recognition;speaker recordings naturally;propose speaker recognition;speaker recognition diarization;dna based speech;speaker recordings;multi speaker recordings;introduce speaker recognition;recordings benchmark;microphone far field;microphone far;microphone state art;speech recognition;speaker recordings article;vicinity microphone far;recordings benchmark comprises;speech recognition proposed;recordings naturally;microphone;field multi speaker;vicinity microphone;measure speed sound;microphone state;chime corpus;spoken interactions;based speech recognition;recordings;single microphone", "pdf_keywords": ""}, "1410f7d9470a24fb4055c6685c2dda758b9d995f": {"ta_keywords": "coevolution agents games;evolutionary game theory;learning evolutionary game;evolutionary game;coevolving network game;evolve strategically time;converge nash equilibrium;initializations agents games;games play evolve;game theory;competition evolves adversarially;time populations agents;sum competition evolves;chaotic coevolution agents;population dynamic agents;dynamic agents;time average game;evolves adversarially;average game conservation;online learning evolutionary;competition evolves;strategically time populations;agents games prove;nash equilibrium;agents games introduce;agents games;play evolve strategically;coevolution agents;dynamic agents interact;game conservation laws", "pdf_keywords": "modeled polymatrix games;games evolutionary zero;polymatrix games particular;polymatrix games;polymatrix games remain;game theoretic graph;sum games evolutionary;dynamics evolutionary game;evolving games;zero sum games;sum polymatrix games;polymatrix games time;games evolutionary;time evolving games;evolving games class;game theory replicator;evolutionary game theory;games time evolutionary;dynamic agents interacting;learning agents time;dynamic agents;agents modeled polymatrix;game theory;agents time evolving;evolves function agents;evolving zero sum;evolutionary game;evolving games reduces;population dynamic agents;dynamics evolutionary"}, "e8e62a80c7355bcf5dbc9fabafff4025e00cf540": {"ta_keywords": "combinatory categorial grammars;categorial grammars positivity;induction combinatory categorial;linguistically plausible lexicons;induces linguistically plausible;categorial grammars;positivity tagged text;combinatory categorial;novel nonparametric bayesian;grammars positivity tagged;induces linguistically;nonparametric bayesian;plausible lexicons present;linguistically plausible;nonparametric bayesian model;linguistically;plausible lexicons;languages induces linguistically;model induction combinatory;bayesian model induction;tagged text;lexicons;grammars positivity;bayesian;lexicons present;languages induces;grammars;predicting outcome competition;positivity tagged;induction combinatory", "pdf_keywords": ""}, "4ef46d5daf6a7a9536e2ebe3c7aa2296bffcf43e": {"ta_keywords": "unsupervised speech tagging;unsupervised speckle tagging;speech tagging;unsupervised markov models;speech tagging including;fully unsupervised speech;unsupervised markov;supervised poos tagger;unsupervised pos tagger;unsupervised speech;models poisson tagging;tagger shallow parsing;supervised speech;poisson tagging;fully supervised speech;speckle tagging;nonparametric probabilistic inference;states unsupervised markov;unsupervised pos;supervised speech recognition;poisson tagging paper;fully supervised poos;supervised poos;shallow parsing;clustering evaluation;performance unsupervised pos;fully unsupervised;nonparametric probabilistic;speech recognition;propose nonparametric probabilistic", "pdf_keywords": ""}, "467b14cc8337dd7efe1d374f9a7feb90ae9d2c12": {"ta_keywords": "speech surveillance based;approach speech surveillance;speech surveillance;use speech recognizer;speech recognizer;surveillance based use;surveillance based;surveillance;based use speech;use speech;approach speech;new approach speech;speech;recognizer;new approach;propose new approach;based use;paper propose new;paper propose;approach;use;paper;based;new;propose new;propose", "pdf_keywords": ""}, "63a604942f1238e9678aebd697a2379981e9a20a": {"ta_keywords": "teaching computer agent;sim student peers;sim student;called sim student;teaching computer;computer agent;learn teaching computer;learning environment;computer agent called;agent called sim;students learn;learning environment students;students;students learn teaching;environment students learn;student;student peers;environment students;teaching;agent;learn teaching;called sim;line learning environment;sim;agent called;learning;computer;learn;peers;built line learning", "pdf_keywords": ""}, "db392858262b17aa9c8ff8659738f68fbf832ebe": {"ta_keywords": "code completion generating;code completion;abstract syntax tree;syntax tree;code programming language;syntax programming languages;programming languages model;approach code completion;arbitrary code programming;code completion leverages;syntax tree program;code snippet tree;programming languages;syntax programming;programming language;code programming;strict syntax programming;languages model code;tree structural language;generate arbitrary code;arbitrary code;generating java programs;problem code completion;structural language;snippet tree;snippet tree structural;tree program decomposed;structural language modeling;generating java;structured approaches generating", "pdf_keywords": ""}, "040a1abdbef2a0e087a586d719259c32c95bfc78": {"ta_keywords": "sdes planning learning;large scale stochastic;stochastic differential equations;stochastic differential;equations sdes planning;planning learning optimize;scale stochastic differential;reward outperforming log;planning learning;differential equations sdes;stochastic;probabilistic model logarithmic;reward experiments model;scale stochastic;log logistic;logistic;sdes planning;function expected reward;log logistic model;high expected reward;equations sdes;learning optimize;log linear probabilistic;logarithmic log logistic;expected reward experiments;logistic model;manager log linear;learning optimize objective;expected reward outperforming;expected reward", "pdf_keywords": ""}, "e1a20480e4168d58deec743035b7ff02720672d7": {"ta_keywords": "open vocabulary recognition;word based lms;end speech recognition;decoding open vocabulary;lexical matrices lmms;vocabulary recognition;vocabulary recognition based;recognition language constraints;scale lexical matrices;open vocabulary end;lexical matrices;large scale lexical;level language modeling;language modeling;language modeling decoding;vocabulary end end;recognition language;modeling decoding open;speech recognition challenging;lmi word boundary;speech recognition;based recognition language;vocabulary end;character based lmi;language constraints long;architecture open vocabulary;matrices lmms based;long sequences characters;based lmi word;open vocabulary", "pdf_keywords": ""}, "a13d9c8e5a2fc028ad597e2bd46a9c60aca0ede4": {"ta_keywords": "speech synthetic speech;input speech synthetic;based speech synthesis;speech synthesis;text speech synthesis;synthesis text speech;synthetic speech;speech synthesis text;prosody synthetic speech;synthetic speech target;synthesize speech;speech synthetic;synthesize speech target;speech inputs hmm;speech synthesis report;hmm based speech;user speech inputs;speaker output speech;synthetic speech investigate;input speech target;output speech target;output speech;expressive speech synthesis;input speech;speech target speaker;speaker using voices;speech inputs;using voices preserving;speech synthesis order;using user speech", "pdf_keywords": ""}, "d26a7a86013b3be57acc0f5df73393cab7c302d9": {"ta_keywords": "deterministic particle flow;deterministic particle methods;iterative stochastic path;forward probability flows;particle methods solving;stochastic path sampling;stochastic path;resort iterative stochastic;particle methods;solving fokker planck;iterative stochastic;fokker planck equations;particle flow;flows employing deterministic;bellman equation deterministic;probability flows employing;equation deterministic particle;grid based pde;probability flows;reformulating optimal interventions;pde solvers;previous control methods;based pde solvers;pde solvers resort;planck equations introduce;particle flow patterns;employing deterministic particle;solution nonlinear backward;nonlinear backward partial;path sampling", "pdf_keywords": "stochastic control;stochastic optimal control;control stochastic optimal;control stochastic;optimal control stochastic;control stochastic systems;optimal control diffusions;iterative stochastic control;manipulation stochastic systems;stochastic systems optimal;stochastic control framework;deterministic particle dynamics;stochastic optimal;manipulation stochastic;stochastic systems;dynamics reformulating optimal;control diffusions;stochastic;precise manipulation stochastic;control nonlinear diffusion;stochastic systems target;deterministic particle methods;control diffusions important;predictive control movement;iterative stochastic;non iterative stochastic;forward probability \ufb02ows;shot optimal control;solely deterministic particle;employing deterministic particle"}, "73c401e29cb83157bc6dfb33d5ce4364a7d2731b": {"ta_keywords": "photorealistic domains shot;domains shot model;diverse realistic images;extensive results photorealistic;photorealistic domains;shot model;training generative models;latent space training;generative models target;results photorealistic;space training generative;shot model automatically;training generative;non photorealistic domains;photorealistic non photorealistic;results photorealistic non;domains shot;realistic images;realistic images previous;generates diverse realistic;generative models;non photorealistic;photorealistic;diverse realistic;models target domain;photorealistic non;realism different regions;different levels realism;bidoublet pelagic swimmer;realism", "pdf_keywords": "unconditional image generation;face sketches;domains face caricatures;face sketches iii;face paintings;ii face sketches;images relate source;image generation;image generation discriminator;generative model;face caricatures;caricatures;sketches iii face;generative;model transferring images;regularize adaptation source;propose generative model;face caricatures ii;adaptation target domains;model unconditional image;sketches;caricatures ii;propose generative;adaptation source;images relate;regularize adaptation;caricatures ii face;generative model unconditional;paper propose generative;images propose novel"}, "4702bfd200ceb6de126a60afb4db9da5c413476e": {"ta_keywords": "speech recognition asr;uncertainty deep neural;dnn based acoustic;uncertainties layers dnn;recognition asr noisy;acoustic scores approximated;accuracy asr database;speech recognition;automatic speech recognition;uncertainty input features;acoustic model scoring;propagation uncertainty deep;asr uncertainty decoding;accuracy asr;performance automatic speech;speech enhancement algorithm;improve accuracy asr;probabilistic neural network;input features acoustic;noisy environments speech;environments speech enhancement;distribution acoustic scores;features acoustic model;speech enhancement;uncertainty variance estimation;automatic speech;uncertainty deep;uncertainty decoding;probabilistic neural;layers dnn based", "pdf_keywords": ""}, "b946ce2c3405969bf615bedc623845b0d3d9b010": {"ta_keywords": "speech transformer encoder;autonomous speech recognition;automatic speech;speech transformer;automatic speech recognition;speech recognition;end automatic speech;acoustic speech transformer;transformer machine learning;autonomous speech;speech recognition systems;speech recognition recently;elimination autonomous speech;recurrent neural networks;performance alternative recurrent;encode linguistic channel;neural networks end;acoustic speech;track transformer genetic;linguistic channel speaker;track track transformer;alternative recurrent neural;track transformer;encode linguistic;attributes acoustic speech;learn predict;encoder;transformer encoder;encoder transformer;recurrent neural", "pdf_keywords": "speech utterance encoder;utterance encoder;utterance encoder decoder;e2e automatic speech;speech recognition recently;speech recognition;recognition speech recognition;recognition speech;speech recognition speech;neural networks encoder;recurrent neural networks;automatic speech;input automatic speech;performance alternative recurrent;crowdsourced speech recognition;automatic speech recognition;attention network;transformer convolutional neural;alternative recurrent neural;networks encoder;speech recognition problems;encoder;neural networks end;networks encoder decoder;chunkwise attention;encoder layer;chunkwise attention mocha;self attention network;attention mocha transformer;encoder decoder"}, "3ad287cf3b17cb109bf991731d2c0dcf8b7db2b1": {"ta_keywords": "lingual morphological tagging;morphological tagging;approach morphological tagging;morphological tagging based;morphological tagging aims;cross lingual morphological;lingual morphological;tagging based cross;superior tagging accuracies;cross lingual approaches;tagging accuracies;new approach morphological;tagging accuracies existing;universal dependencies treebank;existing cross lingual;based cross lingual;lingual approaches;dependencies treebank demonstrate;lingual approaches paper;predicting syntactic;dependencies treebank;cross lingual;treebank;method cross lingual;treebank demonstrate superior;involves predicting syntactic;predicting syntactic traits;approach morphological;superior tagging;demonstrate superior tagging", "pdf_keywords": "crosslingual morphological tagging;morphological tagging based;approach morphological tagging;morphological tagging;neural morphological tagger;morphological tagging aims;tagging based cross;morphological tagging paper;morphological tagging use;morphological tagger task;morphological tagger;task morphological tagging;predicting tag sets;work morphological tagging;method crosslingual morphological;crosslingual morphological;lingual transfer learning;predicting tag;tagger task morphological;tagging use hybrid;instead predicting tag;crosslingual neural morphological;tagging based;tagging;predictions single tags;tagging paper;propose crosslingual neural;tagging paper propose;tag sets;paper propose crosslingual"}, "27e1dbe9f7c71cd6cc1b0357f49aef497e572d09": {"ta_keywords": "pseudo code generator;generate pseudo code;generated pseudo code;pseudo code source;pseudo code automatically;code generator automatically;code generator;create pseudo code;automatically generate pseudo;pseudo code written;code source code;machine translation smt;pseudo code;comprehension source code;statistical machine translation;code unfamiliar programming;new pseudo code;code generator human;code automatically;given source code;source code human;pseudo rheological codes;pseudo code accurate;pseudo code proton;code written natural;source code;source code unfamiliar;generate pseudo;generating pseudo;machine translation", "pdf_keywords": ""}, "1abd1efae8c3849e28de926e52d166b7800965a1": {"ta_keywords": "deep aggregation algorithm;deep aggregation;proposed deep aggregation;aggregation compute deep;alternatives ranked edges;ranked edges;ranked edges capture;incomplete rank lists;preference aggregation;nodes alternatives ranked;algorithm called deepaggregation;preference aggregation methods;deepaggregation generates impressive;aggregation methods inspired;rank lists;collection incomplete rank;rank lists accordingly;rank lists ties;incomplete rank list;alternatives ranked;unsupervised deep;rank lists constructs;empirically observe deepaggregation;handle incomplete rank;deepaggregation;unsupervised deep learning;aggregation;rank list;deep neural representation;aggregate level aggregation", "pdf_keywords": ""}, "d513a3583bd168ee341ce3b26d54a4e4096da471": {"ta_keywords": "data replication powerful;replication powerful erasure;data replication;data storage systems;simple data replication;data storage methods;data storage;storage systems;mds queue;latency data centers;storage methods;queueing theory;storage systems based;replication;evolving data storage;mds queue paper;replication powerful;reducing latency data;queueing;storage methods use;mds reservation queue;replication based;fledged mds queue;erasure codes provide;study data storage;storage efficiency;replication based methods;maximum storage efficiency;reliability replication;powerful erasure codes", "pdf_keywords": ""}, "1d634d645bfe0b289fd6a2a0d9210b2a04c9237b": {"ta_keywords": "private learning pretrained;private learning;private dp learning;train nlp models;dp learning nlp;reveal private learning;learning models text;learning pretrained models;large deep learning;learning nlp;learning noisy;train nlp;learning nlp tasks;nlp models outperform;deep learning;pretrained models hyperparameters;learning pretrained;model differentially private;large pretrained models;instantiating example gradients;dp learning;private training;learning shown limited;learning;differentially private;deep learning models;learning models;differentially private dp;learning noisy noisy;nlp models", "pdf_keywords": "privacy aware learning;private learning task;private learning;learned public datasets;subsequent private learning;privacy aware;study privacy aware;present new privacy;new privacy;privacy budgets empirical;privacy leakage propose;models modest privacy;privacy leakage;privacy based machine;neural language models;privacy budgets;sharing propose memory;paper study privacy;notes using neural;privacy;features learned public;modest privacy leakage;formal notions privacy;performance modest privacy;automatic generation shareable;models parameter sharing;new privacy based;study privacy;public datasets;notions privacy"}, "5ee580fba44c6efb2a9b06f4c62de6b053db7784": {"ta_keywords": "hyperparametric optimization;optimal hyperparameters;class hyperparametric optimization;finding optimal hyperparameters;hyperparametric optimization problems;forces peer review;optimal hyperparameters class;hyperparameters;empirical risk minimization;peer review scores;peer review;peer review peer;hyperparameter;review peer;review peer review;learning community aggregate;risk minimization;hyperparametric;optimization;competing forces peer;forces peer;hyperparameters satisfies;hyperparameters class hyperparametric;peer review process;minimizes;predicting outcome competition;hyperparameter xmath0 minimizes;hyperparameters class;minimization;choice hyperparameters", "pdf_keywords": "peer review aggregate;score peer review;peer review;peer review combining;peer review review;subjectivity peer review;review aggregate;method peer review;aggregate score peer;aggregation method peer;review aggregate function;review review;review combining;score peer;reviewers;aggregate score paper;subjectivity peer;theory aggregate score;review;paper aggregate score;reviewers provide empirical;minimizes aggregate score;aggregation;aggregate function social;aggregate score;reviews;reviewers provide;machine learning social;dataset 9197 reviews;choice theory aggregate"}, "45dcccef42ed09cfd2babb630c117e95136b35d1": {"ta_keywords": "dialogue semantics schema;example dialogue semantics;dialogue semantics;dialogue state tracking;universal dialogue systems;dialogue systems seamlessly;dialogue systems;building universal dialogue;labeled example dialogue;example dialogue;dialogue state;popular dialogue state;generalization popular dialogue;semantics schema elements;dialogue;semantics schema;universal dialogue;schema representations large;schema representations;examples schema representations;schema based systems;short examples schema;large language models;seq2seq modeling uses;examples schema;language models results;seq2seq modeling;language models;popular dialogue;format seq2seq modeling", "pdf_keywords": "dialogue state tracking;dialog state tracking;tracking dialog state;state tracking dialog;schema guided dialogue;domain dialogue model;schema representation dialogue;guided dialogue dataset;tracking dialog;dialogue model;dialogue dataset;tasks dialogue state;tracking schema driven;state tracking schema;dialogue model compare;language descriptions schema;dialogue state;multi domain dialogue;tasks dialogue;popular dialogue state;domain dialogue;dialog state;guided dialogue;convey schema semantics;tracking schema;representation dialogue state;natural language descriptions;tracking benchmarks schema;schema driven prompting;dialogue dataset multiwoz"}, "6b9c3f82a0c0fd62f8ae527126b118890cfd452d": {"ta_keywords": "safe noisy environment;safe safe noisy;safe noisy;dangerous equipment;simple potentially dangerous;potentially dangerous equipment;fluid dynamics video;dynamics video simple;noisy environment;noisy environment use;dynamics video;dynamics;make safe safe;fluid dynamics;dangerous;simple pedagogically intuitive;video simple;intuitive;potentially dangerous;make safe;video simple simple;pedagogically intuitive way;pedagogically intuitive;intuitive way;safe safe;noisy;equipment;fluid;way make safe;simple simple pedagogically", "pdf_keywords": ""}, "1b06fe6ca5f4404e68b066cdea1a74a36e3e0e13": {"ta_keywords": "egocentric manipulation data;unseen objects actions;robotics tasks;certain robotics tasks;objects depth camera;objects actions;manipulation data robot;robotics;classifying action;robot;objects depth;pipeline classifying action;certain robotics;necessary certain robotics;depth camera;unseen objects images;objects actions necessary;robotics tasks clearing;data robot;dynamics video;colored objects depth;objects images objects;objects images;detecting controlling motion;classifying action outcomes;unseen objects;depth camera model;fluid dynamics video;objects;robot successfully", "pdf_keywords": "egocentric camera information;egocentric vision pipeline;egocentric object language;egocentric scene information;tasks egocentric object;egocentric camera;object given egocentric;based egocentric camera;human judgements robotics;egocentric object;given egocentric scene;egocentric scene;scene information manipulator;manipulation based egocentric;egocentric vision;vision pipeline robots;human annotations;action outcomes robots;perform tasks egocentric;tasks egocentric;task determining robot;perform humans robots;robot;judgements robotics;outcomes robots;humans robots;new egocentric vision;objects annotations;human annotations related;objects annotations objects"}, "4cb3275ec95f4ad407f153aa9dc2d527bc2744e5": {"ta_keywords": "generation synthetic speech;synthetic speech proposed;based speech synthesis;speech synthesis;using speech synthesis;speech synthesis technologies;text speech synthesis;synthetic speech models;synthetic speech;synthesized speech speech;synthesize speech;synthesize speech specific;synthesized speech;speech synthesis makes;prosody synthesized speech;speech synthesis creative;synthetic speech target;prosody synthetic speech;speech parameter generation;ability synthetic speech;interfaces synthesize speech;hmm based speech;speech speech input;functionality text speech;speech input;speech models;text speech;using speech preserving;guide prosody synthetic;module speech", "pdf_keywords": ""}, "34c9e3152c9a14af711994230d8a3909daeaa7cf": {"ta_keywords": "hyperparametric optimization;minimization learning aggregate;optimal hyperparameters;class hyperparametric optimization;hyperparametric optimization problems;finding optimal hyperparameters;risk minimization learning;optimal hyperparameters class;peer review scores;forces peer review;peer review;review peer review;empirical risk minimization;learning aggregate;minimization learning;hyperparameters;peer review peer;learning aggregate mapping;review peer;hyperparametric;hyperparameter;peer review process;hyperparameters class hyperparametric;review scores ranked;risk minimization;review process peer;predicting outcome competition;class hyperparametric;review scores;process peer review", "pdf_keywords": ""}, "46f88a062df05673ae0731aa17f9f9cc9d3e87bf": {"ta_keywords": "cross sections flows;flows presence magnetic;calculation cross sections;sections flows;sections flows presence;magnetic field;cross sections;calculation cross;method calculation cross;numerical method calculation;new numerical method;present new numerical;presence magnetic field;numerical method;flows presence;flows;new numerical;numerical;cross;presence magnetic;method calculation;magnetic;sections;calculation;field;method;present new;paper present new;present;paper present", "pdf_keywords": ""}, "c4e83bfddb38642debb31097501aec8768f9020e": {"ta_keywords": "quantum repeater;2016 season entanglement;presence quantum repeater;quantum repeater noisy;season entanglement;entanglement;entanglement generated;season entanglement generated;entanglement generated quantum;quantum walker;quantum;detect presence quantum;quantum walker used;generated quantum walker;generated quantum;semif semifinal competition;semif semifinal;presence quantum;semifinal competition 2016;semifinal competition;format semif semifinal;repeater;repeater noisy environment;competition 2016 season;competition 2016;repeater noisy;semifinal;detect;walker used detect;competition", "pdf_keywords": ""}, "f5813bb0b398007cae10ffdddeab221d4b9b0dc7": {"ta_keywords": "walled storage ring;storage ring storage;storage ring;storage ring used;ring used storage;stored storage ring;used storage ring;ring storage;single walled storage;walled storage;energy stored storage;ring storage retrieval;storage retrieval energy;stored storage;storage;energy stored;used storage;fluid dynamics video;storage retrieval;retrieval energy stored;ring used;dynamics video;stored;ring;dynamics video showing;fluid dynamics;single walled;dynamics;present fluid dynamics;retrieval energy", "pdf_keywords": ""}, "27de5fb45af9799ed0020c978fe3a3080c60401e": {"ta_keywords": "embryo undergoing morphological;dynamics embryo;dynamics embryo undergoing;model dynamics embryo;embryo;undergoing morphological segregation;morphological segregation;embryo undergoing;convolutional neural networks;use convolutional neural;convolutional neural;morphological;undergoing morphological;neural;use convolutional;dynamics;convolutional;neural networks;model dynamics;neural networks model;networks;networks model dynamics;segregation;model;networks model;undergoing;use", "pdf_keywords": ""}, "bab35e88a510938d22cb28f2ecc6f6e189c3d8ea": {"ta_keywords": "group members binary;recognition group non;recognition group;members binary population;non abelian group;group non members;group theoretical method;abelian group theoretical;group theoretical;group members;non members binary;members binary;abelian group;demonstration non abelian;group non;observation group members;binary population;non abelian;group group size;binary population based;group group;group;group size;binary population undergoes;group group group;observation group;abelian;group size increases;method recognition group;based observation group", "pdf_keywords": "arabic speech recognition;speech recognition ars;automatic speech;machine human speech;human speech recognition;model automatic speech;duration automatic speech;speech recognition;automatic speech recognition;speech recognition systems;traditional automatic speech;performance arabic speech;arabic speech;human speech;speech recognition analyzing;speech recognition di;speech recognition based;ieee automatic speech;human performance arabic;recurrent networks;traditional recurrent networks;machine translation state;recurrent networks self;performance arabic;datasets egyptian mgb3;segment duration automatic;machine translation;arabic;speech;datasets egyptian"}, "0132cb4384c3a6402353d8f349f8dd450d8ea4a2": {"ta_keywords": "deep bi lstm;bi lstm;learning additional normalization;normalize spelling historical;bi lstm network;normalize spelling;lstm;normalization data improve;multi task learning;approach normalize spelling;deep neural;task learning;task particularly deep;task learning additional;normalization;language processing historical;additional normalization;normalize;deep neural network;lstm network;additional normalization data;texts early new;normalization data;normalization algorithms;players natural language;common approach normalize;approach normalize;set texts early;natural language processing;previously established normalization", "pdf_keywords": "deep bi lstm;bi lstm task;spelling normalization mapping;bi lstm;spelling normalization;model bi lstm;short term memory;lstm task;bi lstm network;learning additional normalization;normalization texts;outperforms existing normalization;lstm task model;lstm;normalization texts explore;nlp tasks;term memory;applied normalization texts;term memory model;speech tagging chunking;entity recognition;recognition sentence compression;statistical machine translation;machine translation;named entity recognition;nlp tasks speech;mapping historical spelling;tasks speech tagging;deep neural;normalization data improve"}, "39e734da43eb8c72e9549b42e96760545036f8e5": {"ta_keywords": "machine comprehension dataset;interactive machine comprehension;reading comprehension architecture;comprehension architecture;context machine comprehension;machine comprehension report;comprehension architecture extended;machine comprehension;comprehension dataset;comprehension dataset used;questions context machine;reading comprehension;comprehension report;comprehension report measurement;learning hidden wikipedia;question answer question;comprehension;model dialog context;answer question person;wikipedia text freeform;model dialog;questions context;answer question answer;answer question;extended model dialog;dialog context;dialog context paper;context machine;dialog;art reading comprehension", "pdf_keywords": "question answering context;crowdsourced qa dialogs;reading comprehension architecture;seeking dialogs crucial;question answering;productive dialogs;answering context;dataset question answering;reading comprehension incorporating;qa dialogs;unanswerable questions learning;seeking dialogs;questions learning information;dialog context;questions learning;comprehension architecture;information seeking dialogs;productive dialogs allows;dialog context section;based reading comprehension;comprehension architecture extended;reading comprehension;dialogs;14 crowdsourced qa;dialogs crucial;considers dialog context;model dialog context;dialogs crucial workers;dialog context report;dialog"}, "68b3905c2f82814294631f2ce29d5be4165e6b1f": {"ta_keywords": "relay selection strategies;relay selection;wireless relay networks;wireless relay nodes;wireless relay;relay networks;hop wireless relay;strategies considered relay;relay nodes;deployment wireless relay;different relay selection;optimal deployment wireless;relay networks provide;multi hop wireless;relay nodes person;considered relay;objectives different relay;relay communicates;relay ii communication;relay;considered relay communicates;multihop wireless network;markov decision process;deployed relays;relays;design multi hop;relay communicates sink;immediate previous relay;relay ii;wireless network", "pdf_keywords": "placement relay nodes;relay selection strategies;placement relay;relay nodes wireless;relay nodes;relay selection;failure relay nodes;selection strategies relay;nodes wireless networks;deployed relays;problem placement relay;deployment failure relay;skip deployed relays;nodes wireless;strategies relay;cost markov decision;relays case decision;relay nodes line;wireless networks formulate;strategies relay communicates;relay;relay ii communication;different relay selection;mobile sensor networks;deployed relays case;total cost markov;cost markov;markov decision process;networks formulate placement;wireless networks"}, "dc8ebb6d9908542ae474dc2b21bfb6a14216f678": {"ta_keywords": "computer competitive translation;showing dynamics swimming;pmi corpus;dynamics swimming track;track movement swimmer;dynamics swimming;movement swimmer crowded;swimming track moves;translation systems;swimming track;track moves action;movement swimmer;shows swimming track;translation systems metrics;track moves;pmi corpus discover;video shows swimming;track movement;baselines pmi corpus;competitive translation results;study performance graphics;competitive translation;shows swimming;performance graphics processing;fluid dynamics video;swimmer crowded;swimmer crowded environment;action moving crowd;benchmark;swimming track used", "pdf_keywords": ""}, "010df54445ab5f47582eb668dc3488a3e46b55d3": {"ta_keywords": "unsupervised neural networks;unsupervised learning hidden;learning hidden markov;unsupervised neural;hidden markov model;unsupervised learning;generative models;existing generative models;modeling neural;hidden markov;concept unsupervised neural;results unsupervised learning;modeling neural networks;learning hidden;chiral perturbation theory;invariant mass spectrum;neural networks;chiral perturbation;framework chiral perturbation;existing generative;mass spectrum;generative;outperforms existing generative;generative models competitive;markov model;spectrum framework chiral;markov model paper;mass spectrum framework;framework chiral;models", "pdf_keywords": "unsupervised machine learning;hidden markov models;data unsupervised learning;unsupervised learning;unsupervised learning neural;model hidden markov;models hmms;unsupervised machine;hidden markov;representations data unsupervised;new unsupervised machine;hmms implemented neural;models hmms implemented;neural networks simply;machine learning systems;machine learning neural;data unsupervised;neural networks universal;powerful machine learning;markov models hmms;hmms implemented;implemented neural networks;neural networks;representations data;compact representations data;sequence model hidden;markov models;neural networks powerful;learning neural networks;machine learning"}, "90b9d19af75c86f42865052c21305c70f884b5fe": {"ta_keywords": "congestion costs multiplicative;underestimates congestion costs;estimates cost congestion;multicommodity selfish routing;cost congestion;congestion costs;selfish routing game;user underestimates congestion;selfish routing;cost congestion results;underestimates congestion;network congestion;congestion study equilibrium;inefficiencies network congestion;congestion decreases users;routing game uncertain;congestion;equilibrium behavior multicommodity;users overestimate costs;network congestion decreases;routing game;transportation networks;urban transportation networks;dynamics multicommodity routing;multicommodity selfish;congestion decreases;behavior multicommodity selfish;congestion study;increased congestion;multicommodity routing", "pdf_keywords": ""}, "124385efee78010a4408329dffea4798f5a1ad47": {"ta_keywords": "delay translation translation;simultaneous translation speech;phrasebased translation systems;reduces delay translation;speech translation systems;begin translation delay;delay translation;translation systems wait;translation unit segmentation;translation delay;translation delay conventional;translation systems;translation speech;translation language pairs;speech translation;conventional speech translation;compared pause segmentation;translation translation languages;simultaneous translation;translation speech propose;phrasebased translation;use phrasebased translation;method simultaneous translation;translation language;pause segmentation;translation languages;perform translation unit;translation systems decide;pause segmentation identical;translation languages achieve", "pdf_keywords": ""}, "0eac6cbd150b1a7e9d757ccc871eea2bf0d89e42": {"ta_keywords": "estimation crucial classify;ordinal deep neural;label representations;crucial classify;deep neural;classification segmentation networks;ordinal deep;truth label representations;segmentation networks;classify;label representations demonstrate;crucial classify class;neural networks learn;classification;order categorical classification;categorical classification;depth estimation crucial;regression monocular depth;networks learn intraclass;image quality ranking;segmentation networks wildly;learning adequate interclass;networks learn;deep neural networks;class ordinal regression;change ordinal deep;ranking age estimation;ground truth label;networks;truth label", "pdf_keywords": "estimation based deep;network deep;monocular depth estimation;deep convolutional;scene understanding tasks;semantic image segmentation;neural network deep;depth estimation;deep network;feature scene understanding;scene understanding;deep neural;dataset movie scenes;deep convolutional network;deep convolutional neural;network deep network;networks cnns;semantic image;convolutional network;cnns;trained dataset movie;propose deep neural;deep neural network;deep network achieves;depth estimation based;architecture monocular depth;architecture semantic image;movie scenes taken;convolutional network architecture;classi\ufb01cation propose deep"}, "59f3e3cad309eb4965d67773d68bc2f91b2e376f": {"ta_keywords": "automatic speech translation;speech translation corpus;access speech translation;speech translation languages;translation corpus;translating endangered language;speech translation;machine translation;speech translation st;translation corpus highland;machine translation mt;mt speech translation;endangered language documentation;automatic speech;asr machine translation;translation mt speech;automatic speech recognition;state art translating;languages documentation endangered;documentation endangered languages;translation languages;endangered languages els;audio automatic speech;endangered language;speech recognition;endangered languages;speech recognition asr;translation languages documentation;translating endangered;open access speech", "pdf_keywords": ""}, "2ac98a28fdae4c01a89f09393c736e72445a4c4e": {"ta_keywords": "speech enhancement preprocessor;interconnecting speech enhancement;enhancement preprocessor speech;adaptation interconnecting speech;speech enhancement;preprocessor speech recognizer;variance adaptation interconnecting;interconnecting speech;speech recognizer presented;speech recognizer;preprocessor speech;adaptation interconnecting;dynamic variance adaptation;enhancement preprocessor;variance adaptation;cluster based dynamic;based dynamic variance;enhancement;interconnecting;cluster based;dynamic variance;preprocessor;cluster;recognizer presented;speech;adaptation;recognizer;variance;based dynamic;dynamic", "pdf_keywords": ""}, "da660ca9e6fedefe815e305efd0dcd3bf9b4bb60": {"ta_keywords": "learned entity filters;learn entity filters;entity filters improve;entity filters jointly;entity filters;jointly learned entity;learn entity;entity recognizers;jointly relation extraction;learned entity;relation extraction;entity recognizers exist;supervision fluid dynamics;recent approaches supervised;relation extraction process;relation extraction using;filters jointly relation;named entity recognizers;entity;knowledge bases extract;supervised;named entity;large knowledge bases;issue learn entity;knowledge bases;extract substantial supervision;filters jointly;imitation learning;seed relation extraction;approaches supervised learning", "pdf_keywords": ""}, "2078d466766b6876d73ac1981392fa8bd2b9520d": {"ta_keywords": "fly landing bouncer;landing bouncer;landing bouncer bouncer;outcome fly landing;fly landing;predict outcome fly;bouncer bouncer;bouncer;outcome fly;landing;fly;algorithm predict outcome;predict outcome;simple algorithm predict;algorithm predict;predict;outcome;algorithm;simple algorithm;present simple algorithm;simple;present;present simple", "pdf_keywords": ""}, "dcac1abd2ae5af180e51994a9c8334a6de915765": {"ta_keywords": "distributed stochastic method;distributed sgd arbitrary;distributed sgd;variants distributed sgd;stochastic method;sgd arbitrary compressions;distributed stochastic;propose distributed stochastic;stochastic method error;sgd arbitrary;known complexity;best known complexity;stochastic;data particular complexity;known complexity result;quantized sg dna;expectation constant learning;particular complexity;feedback variance reduction;variants quantized sg;complexity paper propose;complexity;constant learning rate;optimum asymptotically expectation;particular complexity expressed;analysis variants distributed;variants distributed;quantized sg;sgd;constant learning", "pdf_keywords": "distributed optimization;distributed optimization recovers;xk stochastic gradients;goes distributed optimization;stochastic gradients gk;elaborate gradient estimators;decentralized architecture analysis;gradient estimators;stochastic gradients;sgd error feedback;problem decentralized architecture;gradient estimators leads;decentralized architecture;iterates xk stochastic;eigenvalue convex vector;eigenvalue convex;xk stochastic;largest eigenvalue convex;optimization recovers;gradients gk errors;optimization recovers results;convex vector convex;ec methods superior;vector convex sense;problem decentralized;estimators;vector convex;convex vector;convex sense converges;analysis goes distributed"}, "6a9394e5d49c1251c0fb6d7fb0c0813d26c6a907": {"ta_keywords": "zero shot parsers;semantic parsing benchmarks;shot parsers;semantic parsers;semantic parsing;performance semantic parsing;data semantic parsers;paraphrasing utterances improve;programs grammar paraphrasing;paraphrasers efficient learning;canonical utterances programs;paraphrasers efficient;grammar paraphrasing utterances;semantic parsers map;grammar paraphrasing;stronger paraphrasers efficient;paraphrasing utterances;parsers;grammars stronger paraphrasers;map natural language;training examples canonical;parsing;paraphrasing;parser like;stronger paraphrasers;parsing benchmarks zero;parser;canonical utterances;natural language;paraphrasers", "pdf_keywords": "learning semantic parsers;semantic parsers;semantic parsers typically;compositional canonical utterances;semantic parsers map;oriented semantic parsers;parsers;helpful compositional generalization;general grammar patterns;parsers typically;canonical utterances;utterances learning semantic;canonical utterances tend;relations compositional expressions;compositional generalization;learning semantic;domain general grammar;issued utterances learning;compositional expressions;utterances annotated programs;trained language patterns;utterances learning;paraphrasing training algorithm;compositional expressions joined;paraphrasing training;parsers typically require;grammar patterns natural;training helpful compositional;data utterances annotated;general grammar"}, "5446a8bbadc2ba2c575353b257f26abae27b3b2a": {"ta_keywords": "embedding items known;learning embedding;learning embeddings;items embedding;embedding items;embeddings;learning embeddings new;embeddings new items;learning embedding representation;recommendation systems;embedding set items;embedding;items embedding items;embedding representation;method learning embedding;exists learning embeddings;recommendation systems learn;embeddings new;items based comparisons;number items embedding;preferences based comparisons;point comparisons items;embedding set;embedding representation p1;consider problem embedding;comparisons items;model user preference;preference user represented;comparisons relative attractiveness;problem recommendation systems", "pdf_keywords": ""}, "f49ccfb32aad8e6893e8cbb037c1282572fe6e21": {"ta_keywords": "vulnerable adversarial samples;adversarial samples recently;vulnerable adversarial;adversarial samples generated;adversarial samples crucial;adversarial samples;detect various adversarial;various adversarial samples;confidence adversarial samples;known vulnerable adversarial;high confidence adversarial;adversarial attacks;kinds adversarial attacks;various adversarial;deep testing methods;adversarial;deep testing;kinds adversarial;confidence adversarial;vulnerability dnn systems;different kinds adversarial;proposed vulnerability dnn;vulnerability dnn;number deep testing;dnn known vulnerable;detection called deep;model mutation testing;networks dnn known;deep neural;deep neural networks", "pdf_keywords": ""}, "62dc7bdae6700c4409e6d9773d6ecb5c0fab75a4": {"ta_keywords": "approximate dictionary searching;dictionary searching proposed;dictionary searching;dictionaries containing genomes;retrieval dictionaries;retrieval dictionaries used;dictionary searching introduce;dictionaries frequent words;methods dictionary searching;understanding dictionary searching;dictionary searching solutions;frequent words extracted;dictionaries frequent;approximate dictionary;dictionaries containing;dictionaries used primarily;russian dictionaries dictionaries;dictionaries dictionaries frequent;russian dictionaries;indexing methods dictionary;guarantee retrieval strings;dictionaries used;dictionaries;english russian dictionaries;retrieval strings;methods approximate dictionary;focus retrieval dictionaries;containing genomes sequences;natural language datasets;dictionaries dictionaries", "pdf_keywords": ""}, "02aebef93baeef3396f3cb4468a7054067f190c6": {"ta_keywords": "extract structured database;accurate database entities;structured database entities;database entities text;names database;matching entities references;matching entities;structured database;learns accurate database;entities text;inferring schema;extract structured;approach extract structured;entities references;inferring schema fields;canonical entities;database entities;bayesian approach extract;relationship topology database;entity iii matching;entities text paper;entities references raw;ii inferring schema;entities;names database set;nonparametric bayesian approach;database entities sensible;accurate database;nonparametric bayesian;iii matching entities", "pdf_keywords": ""}, "f262ef2f50dfcaf07dc6598f22fb9b2470b37cf1": {"ta_keywords": "span graphs framework;span graph powerful;span graphs;span structure graph;representation span graph;constructed span graphs;span graph;detecting nested span;share span representations;span entities;dataset span representation;nested span entities;tasks share span;share span;representing span structure;analysis social networks;information extraction tasks;tool representing span;multiple information extraction;span representations using;shared layer lstm;information extraction;nested span;representing span;dataset span;span representations;lstm introduce;graph powerful tool;span enumeration good;extraction tasks share", "pdf_keywords": "relation coreference extraction;relation extraction tasks;coreference extraction multi;entity recognition relation;coreference extraction;entity recognition;relation extraction;recognition relation extraction;coreference relation links;entity extraction;entity relation coreference;results entity recognition;relation coreference;propagating information coreference;framework entity extraction;information coreference relation;coreference relation;entity extraction based;information coreference;extraction multi task;rich contextual span;contextual span representations;coreference;extraction tasks diverse;extraction based span;featurebased approaches graph;relation links;entity relation;entity;relation links model"}, "36906613dcef29263afe711f128da1fc916cbbee": {"ta_keywords": "recognition asr systems;automatic recognition asr;recognition speech;recognition speech recognition;recognition asr;speech recognition systems;speech recognition;speech recognition speech;recognition systems recently;end automatic recognition;automatic speech recognition;approach recognition person;recognition systems;automatic speech;recognition person;recognition person true;accuracy long sequence;long sequence data;identity recognizing;years automatic speech;automatic recognition;true identity recognizing;recognition;gaussian kernel shift;shift invariant kernel;recognizing;sequence data windowing;new approach recognition;recognizing presence person;context modeling", "pdf_keywords": "speech recognition asr;data automatic speech;encoding automatic speech;speech recognition;speech recognition models;automatic speech;asymmetric attention source;automatic speech recognition;automatic automatic speech;recognition speech recognition;recognition asr;recognition speech;end automatic speech;recognition asr using;speech recognition speech;long sequence data;speech recognition mitigate;speech recognition based;kernel self attention;positional encoding automatic;attention network;self attention network;sequence data automatic;relative positional encoding;asymmetric attention;attention source;recognition models recently;positional encoding;include asymmetric attention;attention network time"}, "9a334566b79bc6c6906e2b5285d5ea50b9b99479": {"ta_keywords": "adversarial minimax game;adversarial minimax;formulated adversarial minimax;learning representations invariant;representation maximizing certainty;predictions learning meaningful;adversarial;specific predictions learning;formulated adversarial;learning representations;learning meaningful representations;predictions learning;representation maximizing;bias free classification;representation learning;process formulated adversarial;problem learning representations;learning benchmark tasks;task specific predictions;problem learning;meaningful representations;data representation learning;learning benchmark;representation learning process;learning;minimax game;bias free;meaningful representations maintain;maximizing certainty;given representation maximizing", "pdf_keywords": "adversarial game discriminator;domain adaptation;probabilistic domain adaptation;transferable domains learning;representations transferable domains;invariance discriminator encoder;discriminative neural;learning fair representations;domain invariant representations;propose discriminative neural;encoder learns;data encoder learns;discriminative neural network;adversarial game;discriminative;learn domain invariant;domains learning fair;domain adaptation framework;predictive express invariance;discriminator encoder;domains learning;domain adaptation aims;desired invariance discriminator;form adversarial game;adversarial;encoder form adversarial;game discriminator;learn representations;invariance discriminator;representations transferable"}, "939dfa4dec88ca167e6572904c4ad2fcbf726f48": {"ta_keywords": "gradient flows graphs;wasserstein gradient flows;flows graphs continuum;convergence gradient flows;gradient flows probability;gradient flows;flows graphs;infinity wasserstein gradient;edgeweights converges novel;weights large graphs;graphons converge continuum;particles edge weights;networks called particles;gradient flow;edgeweights converges;described gradient flow;graphons converge;euclidean gradient flow;wasserstein gradient;graphs continuum limit;gradient flow suitable;function edgeweights converges;large graphs nodes;gradient flow technically;graphons appropriately;graphons;large graphs;systems evolving gradienttype;graphons appropriately described;flows probability measures", "pdf_keywords": "deviations random graphs;theory large deviations;random graphs paper;random graphs;proof existence scaling;proof based variational;existence scaling;existence scaling limit;scaling limit homomorphism;maximal slopes de\ufb01nes;large deviations;large deviations random;graphs paper new;graphs paper;weighted graph expressed;homomorphism density connection;deviations random;fokker planck equation;slopes de\ufb01nes key;slope weighted graph;variational;formulation fokker planck;slopes de\ufb01nes;variational formulation;cauchy problem scaling;curves maximal slopes;limit homomorphism density;variational formulation fokker;maximal slopes;based variational"}, "c377bf3ae52dee4075c1e807de9c5579d553de22": {"ta_keywords": "text speech semantic;recognition speech;processing automatic dialogue;speech semantic processing;text speech;speech spoken language;speech recognition speech;speech recognition;speech semantic;speech recognition corpora;dialogue systems multimodal;spoken language generation;automatic dialogue systems;automatic dialogue;recognition speech recognition;parsing text speech;dialogue systems;spoken language;recognition corpora language;language resources speech;language generation;speech;harmonic oscillator model;language generation tagging;speech spoken;dialogue;resources speech spoken;study dynamics xmath0li;recognition corpora;systems multimodal", "pdf_keywords": ""}, "ea71f5f59727b63b8912c6db097ba811da41bf5b": {"ta_keywords": "deterministic particle flows;frascati deterministic particle;dynamics single processor;stochastic nonlinear systems;frascati deterministic;di frascati deterministic;study stochastic nonlinear;processor working laboratori;stochastic nonlinear;particle flows;deterministic particle;single processor;particle flows shown;single processor processor;stochastic;processor;processor processor;processor working;study stochastic;processor processor working;nonlinear systems;tool study stochastic;deterministic;particle;flows;frascati;dynamics;laboratori;nazionali di frascati;flows shown powerful", "pdf_keywords": "control stochastic networks;control stochastic;optimal control stochastic;deterministic particle framework;particle \ufb02ow control;transitions multistable conservative;deterministic particle \ufb02ow;particle framework solving;constraint forward dynamics;stochastic networks;optimal control;oscillators optimal interventions;approach optimal control;forward dynamics;stochastic networks work;particle framework;controls iterative pice;conservative systems synchronising;path integral control;forward sampling;integral control problems;forward dynamics article;controls iterative;non conservative systems;multistable conservative;integral control;adaptive landscapes;conservative systems;deterministic particle;modi\ufb01ed forward sampling"}, "5a26eeda7c2ca58c2d56f1d580fbbae9eb1a19cd": {"ta_keywords": "deep networks approximate;neural networks approximating;networks approximating solutions;small neural networks;networks approximate solutions;networks approximate;networks approximating;representable small neural;neural networks parameters;pdelet coefficients representable;simulates gradient descent;deep networks;neural networks;growing neural network;neural network architecture;high dimensional problems;power neural networks;coefficients representable small;coefficient networks;gradient descent;small neural;pde size;pde size solution;coefficient networks recent;solution pde size;approximate solution scale;solutions high dimensional;dimensional problems seemingly;approximating solutions linear;solution scale polynomially", "pdf_keywords": "neural networks approximate;deep neural;neural networks \ufb01xed;deep learning;pdes hamilton;probabilistic neural networks;deep learning framework;domain deep neural;neural networks;deep neural networks;dimensional elliptic pdes;networks approximate solutions;domain deep;probabilistic neural;black scholes;solving black scholes;described neural networks;pdes hamilton jacobi;neural networks used;variational formulation;present deep learning;black scholes equations;loss variational formulation;pdes;high dimensional functions;learning framework approximating;variational formulation paper;elliptic pdes;common pdes hamilton;processing probabilistic neural"}, "7647a06965d868a4f6451bef0818994100a142e8": {"ta_keywords": "trained word embeddings;entity recognition extensive;named entity recognition;neural language models;entity recognition;linguistic sequence labeling;annotations train;annotations train models;knowledge word embeddings;sufficient annotations train;word embeddings;trained neural language;transfer learning;sequence labeling;transfer learning techniques;speech tagging;sequence labeling task;leveraging character;empower sequence labeling;language models;tagging named entity;knowledge linguistic sequence;speech tagging named;language models used;neural framework extract;sequence labeling general;annotations;leveraging character level;knowledge linguistic;pre trained neural", "pdf_keywords": "sequence tagging based;linguistic sequence labeling;sequence tagging;named entity recognition;training sequence labeling;entity recognition;tagging chunking ner;trained unannotated sequence;tagging chunking;unannotated sequence data;nlp linguistic sequence;fundamental tasks nlp;sequence labeling including;phrase chunking named;sequence labeling;chunking named entity;phrase chunking;natural language processing;unannotated sequence;neural language models;framework sequence tagging;noun phrase chunking;sequence labeling fundamental;tasks nlp;pos tagging chunking;language models trained;tasks nlp linguistic;entity recognition evaluate;linguistic sequence;sequence data natural"}, "e65676b43338e914ad77afd0fd6ce4bef87943a1": {"ta_keywords": "singing voice conversion;voice conversion;voice preserving conversion;mixture model speech;convert voice;statistical singing voice;singing voice preserving;voice conversion svc;speech quality converted;singing voice waveforms;converted singing voice;generate converted singing;voice conversion direct;convert voice timbre;voice preserving;differential convert voice;quality converted singing;singer statistical singing;singing voice various;statistical singing;conversion accuracy singer;voice waveforms;singers speech quality;gaussian mixture model;singer using vocoder;target singer statistical;natural singing voice;presents statistical singing;based gaussian mixture;singer statistical", "pdf_keywords": ""}, "d29d33f3b92b447d6011606be41b64439a1da088": {"ta_keywords": "contextual bandit algorithm;regret contextual bandit;contextual bandit;bandit algorithm;online agent learns;learn reward;reward constraints;reward constraints significantly;learn reward feedback;systems learn reward;bandit algorithm underlies;set reward constraints;agent learns;reward feedback experiments;reward feedback actions;reward feedback;agent learns set;reward performance advanced;reward performance;set reward;bandit;ai systems learn;automatically predict;automatically predict outcome;bound expected regret;reward;reactive reward feedback;learns set behavioral;making decisions online;act set reward", "pdf_keywords": "recommendation learn policy;online learning task;online decision;online decision settings;identifying understanding reward;online feedback evaluation;online learning approach;online learning;propose online learning;reward possible actions;decision making agent;movie recommendation learn;learn policy observation;guide online learning;guidelines online decision;agent learn;online feedback;reward action exploitation;learning task constrained;online learning framework;decision settings agent;learning task;reward action;recommendation learn;learn policy;novel online learning;ability movie recommendation;understanding reward;reward possible;learning approach"}, "721d7c82b80f14246d353251837e1711824a9e60": {"ta_keywords": "channel speech recognition;speech corrupted reverberation;multi channel speech;human speech recognition;recognition human speech;reverberation;speech recognition;speech recognition explore;speech recognition performance;channel speech;dereverberation detection;corrupted reverberation;speech recognition based;conventional methods reverberant;methods reverberant;methods reverberant scenarios;convolutional beamformer;reverberant scenarios despite;reverberant scenarios;convolutional beamformer perform;dereverberation detection experiments;reverberant;human speech;wpd convolutional beamformer;module dereverberation detection;minimization distortionless response;beamformer;speech corrupted;wsj1 2mix reververb;end multi channel", "pdf_keywords": "speech enhancement frontend;combination speech enhancement;speech enhancement;talker speech recognition;noise reverberation;multi speaker scenarios;speech recognition challenging;dereverberation automatic speech;interfering speech speakers;noise reverberation commonly;background noise reverberation;multi speaker;beamformer simultaneous denoising;multi talker speech;uni\ufb01ed convolutional beamformer;reverberation;single multi speaker;speech recognition;convolutional beamformer simultaneous;reverberation commonly;\ufb01eld speech recognition;speech speakers;recognition speech recognition;convolutional beamformer;speech recognition task;channel multi talker;noise robust adaptation;convolutional beamformer perform;speech recognition speech;speech speakers involved"}, "a45c3120c077994409093771077a2d16f77674c5": {"ta_keywords": "efficient transfer learning;pretrained language models;machine translation text;machine translation;transfer learning methods;text classification benchmarks;transfer learning;translation text summarization;large pretrained language;language models;studies machine translation;translation text;text classification;pretrained language;classification benchmarks utilize;classification benchmarks;efficient fine tuning;parameter efficient transfer;fine tuning methods;language models paper;tuning parameters tasks;text summarization language;new parameter efficient;summarization language;understanding text classification;fine tuning parameters;language understanding text;learning methods fine;parameter efficient fine;text summarization", "pdf_keywords": ""}, "62d17b6f6ad77fd71ef9954c7784700d5e316f1f": {"ta_keywords": "privacy language models;natural language privacy;language learn privacy;privacy language;language privacy;notion privacy language;language privacy social;learn privacy;making privacy;meaningful notion privacy;learn privacy paper;techniques privacy;privacy sanitization broadness;making privacy concerns;privacy;identities making privacy;notion privacy;language reflects private;privacy sanitization;techniques privacy sanitization;privacy paper present;privacy paper;protection techniques privacy;public use language;privacy concerns;privacy social;privacy social norm;models natural language;privacy concerns broad;adversary extract training", "pdf_keywords": "privacy preserving language;propose privacy preserving;privacy preserving;data sanitization protecting;data sanitization;data sanitization approach;using data sanitization;sanitization protecting privacy;potential data sanitization;private text data;data sanitization algorithms;present data sanitization;privacy using data;data private text;protecting privacy online;allow private information;protecting privacy;text data private;private information redacted;privacy using;privacy online social;data protection;private text;information redacted free;private information;fully satisfy privacy;satisfy privacy;sanitization approach addresses;satisfy privacy expectations;data private"}, "211a6838b9550d227ce81d0bec542ec5b70e290b": {"ta_keywords": "prediction unseen politicians;knowledge base embedding;predict future votes;predict voting;prediction voting;predict voting patterns;accuracy prediction voting;forecasting key political;voting records empirical;prediction voting behavior;votes cast predict;text knowledge base;knowledge base augmented;predict outcome tournament;knowledge base;voting patterns politicians;fails predict voting;knowledge enable prediction;votes cast;news text knowledge;politicians voting record;past votes cast;votes fails predict;outperform prior politicians;patterns politicians voting;unseen politicians;voting record exists;prediction unseen;politicians voting records;voting patterns", "pdf_keywords": ""}, "cf9fa9ebbefab1877aa7a501c888a8a618c31abb": {"ta_keywords": "behavior political blogs;topic blog political;blog political blogs;blogs strong ideological;political blogs;blog political;political blogs form;political blogs especially;topic blog;evident focused blogs;topic topic blog;blogs especially evident;sentiment polarity;blogs form social;focused blogs;sentiment polarity used;political opinions behavior;blogs;blogs strong;focused blogs strong;opinions behavior political;influence political opinions;blog;blogs especially;opinion set topics;sentiment;political opinions;influence user opinion;blogs form;political discourse paper", "pdf_keywords": ""}, "42605dca59a3aafe2e5b33741a98dad9ba117395": {"ta_keywords": "graph walk based;walk based similarity;graph walks;graph walk performance;graph based clustering;optimize graph walk;graph walk graphs;measure entity similarity;search graph;walk graphs;performance graph walk;represented graph nodes;random graph walks;graph walk;graph walks induce;semistructured domains relational;graph nodes;entity similarity;walk graphs heterogeneous;domains relational semistructured;performing search graph;entity similarity viewed;graph based;relational semistructured;links paper graph;graphs heterogeneous describing;directed typed edges;relational semistructured data;topological defects semistructured;search topological defects", "pdf_keywords": ""}, "2a94fa0de804b5efaae1a66f50c3ea96539c46b8": {"ta_keywords": "build conversational agent;conversational agent interact;dialog utilizing human;conversational agent;goal dialog utilizing;build conversational;human conversation;human human conversation;dialog utilizing;non goal dialog;human conversation examples;aim build conversational;dialog;conversational;goal dialog;conversation examples;conversation;conversation examples drama;agent interact users;agent interact;interact users natural;performance probabilistic;interact users;improvement performance probabilistic;performance probabilistic measurement;probabilistic;probabilistic measurement;examples drama television;drama television;agent", "pdf_keywords": ""}, "d0895dccd61c567034d197eecfa5d7d59332061f": {"ta_keywords": "reconstruct topology network;topology network data;reconstructing topological structure;reconstruct topology;topological structure network;topology network;used reconstruct topology;method reconstructing topological;exact regenerating codes;reconstructing topological;regenerating codes;distributed storage codes;structure network;regenerating codes product;complex networks based;network data present;regenerating code;network data;nodes network;regenerating codes class;parameters regenerating codes;structure network small;repair failed nodes;regenerating code used;number nodes network;complex networks;regenerating codes allow;topological structure;analysis complex networks;use regenerating code", "pdf_keywords": "regenerating codes storage;storage repair bandwidth;approach distributed storage;distributed storage;presented distributed storage;distributed storage systems;storage networks;data storage networks;storage repair;repairing data storage;codes storage;distributed storage framework;codes storage retrieval;storage networks investigated;self repairing data;regenerating codes;availability repair bandwidth;stored storage systems;storage systems;nodes explicit repair;data storage;node repair;paper distributed storage;design regenerating codes;regenerating code regenerating;node repair presented;distributed storage paper;regenerating code presented;code regenerating;repairing data"}, "ffe6d7573bb2c4fbfac0cc474804b5b1734a1179": {"ta_keywords": "movie recommendation agent;online movie recommendation;contextual bandits framework;contextual bandits;extension contextual bandits;movie recommendation;recommendation agent;bandits framework learns;learn reward feedback;reward feedback actions;online movie;predict outcome movie;reward feedback;recommendation agent able;interface online movie;movie_invariance cases rewards;learn reward;systems learn reward;making decisions online;bandits framework;decisions online;decisions online setting;rewards guiding;ai systems learn;intelligence ai;ai;movie based use;outcome movie based;reactive reward feedback;artificial intelligence ai", "pdf_keywords": ""}, "40fc6e46f2921be346eacff86ce765ff5b28fbdd": {"ta_keywords": "perpetual swaps contracts;funding rates predictive;contracts currency;swaps contracts currency;futures margin funding;perpetual swaps;margin funding rates;market trend discussed;model funding rates;rates predictive;contracts currency derivative;money exchanged pound;money exchanged;inverse perpetual swaps;gauging market trend;funding rates levied;garch models presented;margin funding;market trend;discussed funding rates;swaps contracts;predictive model funding;funding rates;funding rates based;garch models;currency;rates levied bitmex;currency derivative akin;akin futures margin;futures margin", "pdf_keywords": "money price bitcoin;contracts conditional heteroskedasticity;money bitcoin;money money bitcoin;conditional heteroskedasticity;conditional heteroskedasticity applied;money bitcoin cryptocurrency;heteroskedasticity;derivatives conditional heteroskedasticity;price bitcoin;bitcoin cryptocurrency markets;bitcoin price;price bitcoin derivatives;heteroskedasticity applied;cryptocurrency markets;rates based garch;correlation bitcoin price;bitcoin price margin;bitcoin derivatives conditional;conditional heteroskedasticity common;funding rates predictive;investigation correlation bitcoin;heteroskedasticity applied residuals;money price;gauging market trend;market trend discussed;bitcoin;market trend;valuation money price;cryptocurrency markets paper"}, "f7c9521dcd80127d6d4a72fb407e81a9c518ae8d": {"ta_keywords": "knowledge intensive inductive;induction knowledge intensive;knowledge intensive induction;knowledge inductive;learning inductive;inductive algorithms;knowledge inductive learning;inductive learning inductive;use knowledge inductive;induction inductive learning;induction knowledge;learning inductive induction;inductive learning;algorithm induction knowledge;intensive inductive algorithms;inductive algorithms relatively;induction process learner;inductive learning critical;induction algorithms utilize;inductive induction process;concept definitions generated;problem induction algorithms;intensive induction inductive;induction algorithms;algorithm induction;induction algorithms single;inductive induction;knowledge free algorithms;algorithms relatively knowledge;inductive", "pdf_keywords": ""}, "737aff546a9112127d7a13a5b835e27a6e1e935e": {"ta_keywords": "speech recognition asr;network automatic speech;audio factorized maskedlanguage;speech recognition;automatic speech;automatic speech recognition;speech recognition computational;masked language model;recognition asr challenging;recognition asr;asr audio;factorized maskedlanguage model;cmlmm audio factorized;asr audio conditional;audio factorized;autoregressive transducers;audio conditional masked;non autoregressive transducers;factorized maskedlanguage;maskedlanguage model;rest automatic speech;autoregressive transformer;input speech;masked language;structures asr audio;autoregressive transducers nafs;maskedlanguage model aero;conditional masked language;model cmlmm audio;cmlmm audio", "pdf_keywords": ""}, "39fdea1c34832f9bb1644bff81f53fb8ce6b2679": {"ta_keywords": "hockey players compete;dynamics team hockey;team hockey players;hockey players;players compete championship;team hockey;players compete;hockey;compete championship;compete;championship;players;dynamics team;dynamics;team;study dynamics;study dynamics team;paper study dynamics;paper study;study;paper", "pdf_keywords": ""}, "b58e80ad8c6e6844c41535080ccbdef06bce3b6e": {"ta_keywords": "dynamics water fountain;demonstration water fountain;fountain controlled environment;simulation dynamics water;3d molecular dynamics;interactive game;molecular dynamics simulation;planning dynamic environments;fountain controlled;water fountain;fountain fountain controlled;dynamics water;new interactive game;fountain fountain;water fountain fountain;fountain;dynamic environments;interactive;molecular dynamics;autonomous agents;dynamic environments present;3d molecular;dynamics simulation dynamics;fountain report experimental;dynamics simulation;evaluate autonomous agents;model motion objects;autonomous agents combine;simulation dynamics;new interactive", "pdf_keywords": "learning planning crowdsourced;planning crowdsourced environments;crowdsourced environments chalet;house agent learning;agent learning environment;planning crowdsourced;crowdsourced environments;agent learn;autonomous agents including;chalet interactive house;learning environment chalet;evaluate autonomous agents;interactive house;autonomous agents;learning planning;interactive house environment;agent learn set;agent learning;agents including tasks;framework autonomous agents;planning dynamic environments;interactive environment;agents dynamic environment;interactive;chalet interactive;ability agent learn;agent;environment chalet interactive;crowdsourced;actions interactive environment"}, "b0efb62aa2a435704a3412d592e73faf6be5ecea": {"ta_keywords": "modeling relationships characters;character relationships;character relationships fundamental;inter character relationships;character relationship;character relationship types;understanding character intentions;relationships characters;semantic representations;inter character relationship;linguistic features capture;semantic representations leveraging;character intentions;relationships characters present;lexical semantic representations;sentiment polarities incorporating;linguistic features;unsupervised modeling relationships;models based linguistic;understanding character;types simple sentiment;fundamental understanding character;based linguistic features;relationships capable learning;modeling relationships;narrative enables data;representations leveraging;simple sentiment polarities;inference inter character;character intentions goals", "pdf_keywords": ""}, "8b48c55808636a52699b38869df3eba9c8b999d9": {"ta_keywords": "statistical voice conversion;voice conversion;voice conversion algorithm;voice conversion rle;voice conversion nonaudible;time voice conversion;statistical voice;conversion nonaudible murmur;real time voice;murmur whisper microphone;whisper microphone;time statistical voice;microphone;electric dipole moments;dipole moments edms;voice;nonaudible murmur whisper;dipole moments;moments edms semiconductors;whisper microphone paper;conversion nonaudible;calculation electric dipole;conversion rle promising;nonaudible murmur;time voice;real time statistical;microphone paper;conversion rle;conversion algorithm;electric dipole", "pdf_keywords": ""}, "a0b47c7162d1a3b04b27e27c9fadd2eabc4dab0e": {"ta_keywords": "md simulations molecular;simulations molecular;describes machine translation;machine translation;translation systems;phrase based decoder;molecular dynamics md;machine translation 2016;simulations molecular beam;building translation systems;decoder experiment management;protein interactions based;based decoder;decoder;molecular dynamics;risk decoding;combination molecular dynamics;dynamics languagepairs;dynamics languagepairs use;bayes risk decoding;approach control protein;based decoder experiment;epitaxy mbe;decoding;molecular beam epitaxy;protein protein interactions;risk decoding multiplexing;protein interactions;decoder experiment;molecular", "pdf_keywords": ""}, "c3f9c1f702d0c3b35b99502674757b3d8e7dd352": {"ta_keywords": "speech synthesis crosslingual;speech synthesis;approach speech synthesis;speech synthesis preserves;lingual speech synthesis;speech synthesis proposed;crosslingual speech synthesis;speech synthesis based;synthesis crosslingual speech;model adaptation phonetic;adaptation phonetic;adaptation phonetic correction;synthetic speech improve;synthetic speech;synthesis preserves speaker;synthetic speech compared;based voice conversion;synthesis based voice;voice conversion;speaker individuality synthetic;phonetic correction based;individuality synthetic speech;naturalness preserving speaker;hmm technique synthesize;preserves speaker individuality;preserving speaker individuality;speaker improve naturalness;preserving speaker;phonetic correction;speech improve intelligibility", "pdf_keywords": ""}, "89b2a1dc68a7232bc3c68eb4b3e597f99755f7fe": {"ta_keywords": "question answering;modeling textual compositionality;learns word phrase;modeling textual;question answering typically;words representations qanta;phrase level representations;input modeling textual;factoid question answering;textual compositionality;qanta learns word;learns word;words representations;sentences reason entities;bag words representations;trivia competition called;representations combine sentences;bowl text classification;quiz bowl text;representations qanta learns;textual compositionality present;textual;competition called quiz;reason input modeling;question text;called quiz;text classification;word phrase level;questions trivia competition;information retrieval", "pdf_keywords": ""}, "0025b963134b1c0b64c1389af19610d038ab7072": {"ta_keywords": "learning preference functions;learning preference;algorithm learning preference;hedge algorithm metasearch;greedy algorithms;web search engine;algorithm metasearch;learning order instances;strategy web search;greedy algorithms guaranteed;metasearch formulated ordering;web search;learning order;simple greedy algorithms;learned preference function;metasearch;search engine consider;desirable order classify;search experts;algorithm metasearch formulated;search engine;learning combination search;preference functions based;query expansion strategy;results learning;order classify instances;preference judgments;search experts domain;results learning combination;instances ordered maximize", "pdf_keywords": "environment performance device;environment performance;effects environment performance;performance device;effects environment;environment;performance;device;study effects environment;effects;comprehensive study effects;results comprehensive study;study effects;comprehensive study;present results comprehensive;present results;results comprehensive;results;work present results;study;comprehensive;present;work present;work"}, "448406c38e739695b926d112b2b7aebd4e840322": {"ta_keywords": "meetings speaker diarization;speaker diarization information;automatic speech;group meetings speaker;method automatic speech;automatic speech recognition;speaker diarization;speech recognition present;meetings speaker;speech recognition;estimate automatically speaks;use speech recognition;speech recognition based;automatically speaks;group meetings;analyzer group meetings;signals number people;meetings;online voting;use speech;speech;based use speech;number people receive;diarization information;recognition based;speaker;diarization information used;recognition based use;recognition present;automatically speaks toss", "pdf_keywords": ""}, "06431546c21d7c2528aaa170c2e1078e0a82d12e": {"ta_keywords": "quantum algorithms languages;multilingual zero shot;concept verification quantum;lingual transfer emerging;verification quantum algorithms;cross lingual transfer;multilingual zero;lingual transfer;algorithms languages german;verification quantum;quantum algorithms;target languages;impact multilingual zero;zero shot systems;algorithms languages;xmath0 body interaction;shot cross lingual;cross lingual;languages german russian;russian transfer effectively;xmath0 body;german russian transfer;languages diverse unknown;languages;xmath1 model achieved;languages german;quantum;target languages diverse;lingual;benchmark designs zero", "pdf_keywords": "language crosslingual transfer;trained language crosslingual;crosslingual transfer emerged;languages improve performance;tasks machine translation;crosslingual transfer;cross lingual transfer;machine translations;lingual transfer;machine translation;pre trained language;machine translations combined;machine machine translations;resource languages improve;high resource languages;trained language;lingual transfer machine;question answering tasks;language crosslingual;question answering;translations combined improve;languages improve;language perform competitors;answering tasks improved;cross lingual;translations;competitors question answering;translations combined;languages \ufb01ne tuning;effectiveness source languages"}, "7570afa31c68e24fce1342b7d67c591787219bc1": {"ta_keywords": "sentence paragraphs wikipedia;paragraphs wikipedia articles;paragraphs wikipedia;multi sentence paragraphs;article introduce decoder;sentence paragraphs;decoder architectures;neural abstractive model;abstractive model extract;paragraphs;introduce decoder architecture;decoder architecture;wikipedia articles;use neural abstractive;decoder architectures used;decoder;encoder decoder architectures;architectures used sequence;long sequences;long sequences longer;decoder architecture scalably;extract relevant factual;coherent multi sentence;sequence transduction;neural abstractive;extract salient information;used sequence transduction;introduce decoder;articles;sequence transduction paper", "pdf_keywords": "summarization supervised;paraphrasing text summarization;summarization supervised machine;abstractive sentence summarization;multidocument summarization supervised;sentence summarization abstractive;summarization model articles;summarization model;text summarization model;summarization abstractive;summarizing documents;summarizing documents web;text summarization;extractive summarization text;extractive summarization;sentence summarization;summarization abstractive method;generating articles structured;summarization text text;summarization text;abstractive paraphrasing text;summarization;document abstractive paraphrasing;multidocument summarization;consider multidocument summarization;method summarizing documents;summarizing;approach extractive summarization;generate machine translation;machine translation"}, "4f9a4afc0ba500d839f7ee245513af9b87add8be": {"ta_keywords": "audio visual representations;similarity video audio;finetuned action recognition;learn audio visual;learning cross modal;visual representations video;representations video audio;action recognition tasks;measuring similarity video;discrimination video audio;contrastive learning cross;audio cross modal;action recognition;modal discrimination video;similarity video;good representations video;cross modal discrimination;audio visual;learn audio;representations video;approach learn audio;video audio feature;recognition tasks;visual similarities seeking;recognition tasks optimizing;exploring cross modal;positives measuring similarity;video audio cross;audio feature spaces;contrastive learning", "pdf_keywords": "audio visual learning;audio visual representations;learn audio visual;similar visual audio;audioset cross modal;trained audioset cross;representations video audio;discrimination video audio;trained audioset;unsupervised semi supervised;learns cross modal;visual representations video;visual audio feature;learning cross modal;learn representations video;pre trained audioset;grouping video audio;audioset cross;semi supervised;learn audio;audio visual;audio feature space;learning signals unsupervised;audioset;multi view learning;visual audio;audio feature spaces;video audio feature;metric grouping video;modal discrimination video"}, "2fb54dfcb1a62deac6565e82f2a87919d33074da": {"ta_keywords": "entanglement pair spins;calculating entanglement pair;calculating entanglement;method calculating entanglement;entanglement pair;entanglement;spins presence external;pair spins presence;external magnetic field;pair spins;spins presence;external magnetic;presence external magnetic;spins;magnetic field;magnetic;pair;external;simple method calculating;simple method;present simple method;presence external;method calculating;field;method;calculating;present simple;present;simple;presence", "pdf_keywords": ""}, "5aa3c6ab6cc55c24bab224505e8ad5a4d9863706": {"ta_keywords": "focus attention decoding;attention decoding;attention decoding ways;text normalization learning;task learning mtl;normalization learning pronounce;task learning;learn focus attention;task learning learn;historical text normalization;learning learn focus;text normalization;normalization modern word;multi task learning;normalization learning;focus attention;learning historical text;learning pronounce training;learning pronounce;phoneme dictionary auxiliary;attention;learning mtl;learn focus;training encoder decoder;grapheme phoneme dictionary;proposed attention mechanisms;pronounce training encoder;learning mtl architecture;phoneme dictionary;attention mechanisms", "pdf_keywords": ""}, "86db47e228167439f15ee320a8a81d386f529a0c": {"ta_keywords": "environments language;environments language model;language based environment;pre training corpus;synthetic pre training;dog learn language;knowledge environments language;trained dog learn;dog learn;based environment manipulation;guided pre training;learn language human;training corpus;pre trained dog;language instructions challenging;environment manipulation requires;execution guided;agents manipulate environment;language human;language human noisy;environment manipulation;natural language instructions;learn language;execution guided pre;train pre trained;language model pure;trained dog;environments;pre trained;language based", "pdf_keywords": "generative language;tasks sequence generation;language model tackle;generative language models;trained model explore;generative language model;leverages generative language;language based environment;simulate exploration environment;based environment manipulation;simulate exploration;data simulate exploration;model explore;language models glms;model explore environment;environment manipulation lem;environment manipulation;communicating humans agents;explore environment;exploration environment;explore environment space;environments communicating humans;language models;exploration environment improving;manipulation lem tasks;environments communicating;family generative language;approach leverages generative;agent various environments;model synthesizing"}, "2842c21e879ee581aa50704817454f21b539fc69": {"ta_keywords": "sentiment multilingual languages;languages classifying opinionated;sentiment detection code;emotion sentiment multilingual;opinion detection languages;sentiment multilingual;classifying opinionated tweets;sentiment detection;classifiers opinion detection;language expression emotion;opinion detection;classifying opinionated;sentiments preferred language;languages classifying;expression emotion sentiment;detection languages classifying;emotion sentiment;develop classifiers opinion;opinionated tweets;opinionated tweets positive;classifiers opinion;sentiment;multilingual languages;relationship sentiment detection;negative languages;likely negative languages;negative languages spoken;languages;detection languages;language likely negative", "pdf_keywords": ""}, "a010b3aa83d7d80e52c84d5f239f940eb33df904": {"ta_keywords": "modal data augmentation;modal acoustic;encoders acoustic input;multi modal acoustic;modal acoustic symbolic;acoustic symbolic input;architecture automatic speech;encoders acoustic;speech recognition;acoustic input symbolic;speech recognition sa;automatic speech recognition;acoustic input;recognition based emph;new architecture acoustic;architecture acoustic;input structural features;automatic speech;data augmentation network;data augmentation;augmentation network;separate encoders acoustic;speech recognition based;modal data;multi modal data;acoustic symbolic;way training machine;traditional acoustic input;architecture acoustic wave;training machine", "pdf_keywords": "multilingual training augments;multilingual training;web multilingual training;training encoder augmenting;multilingual training language;training data transcribed;multilingual training obtained;similar multilingual training;approaching multilingual training;called multilingual training;data transcribed speech;transcribed speech language;translation systems trained;multilingual training related;transcribed speech languages;trained transcribed speech;augments training data;using additional transcribed;encoder augmenting data;additional transcribed speech;improvements similar multilingual;training language model;new data augmentation;modal data augmentation;systems trained transcribed;data augmentation;trained transcribed;augmenting encoder augmenting;improvements approaching multilingual;encoder augmenting"}, "f784ab218692364b9c8a1f8064809e4524116f3a": {"ta_keywords": "secure distributed training;federated deep learning;federated learning distributed;distributed deep learning;distributed training;federated learning;learning distributed;example federated learning;learning distributed learning;distributed deep;distributed learning;distributed reinforcement learning;demonstration federated deep;federated deep;train decentralized datasets;learning based trusted;distributed learning technique;distributed training scale;secure distributed;byzantine tolerant training;deep learning provide;secure byzantine;train decentralized;distributed reinforcement;protocol secure byzantine;train deep learning;large scale distributed;secure byzantine tolerant;bounds resistance byzantine;decentralized datasets", "pdf_keywords": "consensus method peer;distributed consensus;distributed deep learning;distributed consensus method;distributed deep;sybil attacks;present distributed consensus;learning hundreds peers;large scale distributed;sybil attacks attacks;method peer peer;consensus method;known sgd algorithm;byzantine attacks marginal;peer information;peer peer;byzantine attacks;peer peer information;method peer;attacks marginal communication;distributed;assumptions sybil attacks;sgd algorithm deal;peer;generalization known sgd;sgd algorithm;bounds resistance byzantine;scale distributed deep;provide list attacks;hundreds peers"}, "2dd1504d54f8d7e01e1323a9f876f35bb86356da": {"ta_keywords": "transportation models learned;intelligent transportation models;intelligent transportation;demand mobility services;services road networks;transportation models;problem intelligent transportation;road networks;mobility services road;learned data driven;demand mobility;transportation;road networks city;incentive policy city;mobility services;networks city paper;incentive policy;models learned;deep learning models;disutility monetary incentives;transportation paper study;policy city nudging;impact demand mobility;models learned data;learning models emerged;networks;deep learning;learning models;data driven;incentive", "pdf_keywords": ""}, "c9d65eee1b5df8ccda87c024b88e1b620099b316": {"ta_keywords": "natural language commands;humans instructions robots;instructions robots;robot;robots;natural language;instructions robots using;humans robots;grounded natural language;unrestricted natural language;robot based;robots able perform;robots using;natural language based;present natural language;robots paper propose;robot based setting;approach natural language;language commands;walks based;natural language processing;gap humans robots;robots paper;language commands paper;robots able;language commands collect;robots using unrestricted;goal configurations robot;propose neural architectures;architectures interpreting contextually", "pdf_keywords": ""}, "f6db40e1f0477d27a34240b2e11d6893b9e85b7b": {"ta_keywords": "home air purifier;affordable air purifier;afford air purifiers;air purifiers;air purifier;air purifier alerts;air purifiers protect;air purifier affected;purifier alerts users;purifier alerts;purifiers protect;purifiers protect unaware;purifier affected people;create affordable air;purifier;activate filters;purifiers;activate filters universally;hazardous air quality;purifier affected;air quality;affordable air;air quality conditions;time activate filters;people afford air;change wildfires pollution;filters;filters universally accessible;filters universally;hazardous air", "pdf_keywords": ""}, "9a36d6b76b3b223aa877b4243e5fdfe5c998689e": {"ta_keywords": "simulation electromagnetic dipole;magnetohydrodynamic simulation electromagnetic;dipole interaction presented;electromagnetic dipole dipole;dipole dipole interaction;magnetohydrodynamic simulation;dipole interaction;electromagnetic dipole;dipole dipole;dipole dipole dipole;magnetohydrodynamic;simulation electromagnetic;dipole;electromagnetic;simulation;interaction presented;interaction;presented", "pdf_keywords": ""}, "3f59122d4cac12f27ad6ae379deefd9f3fa81f29": {"ta_keywords": "language commands robot;commands robot simulated;commands robot;goals execution robot;natural language commands;command predict;robot simulated scenes;prescribed command predict;execution robot;robot;robots;natural language command;robots useful real;scenes order robots;robots useful;robot simulated;ourced natural language;execution robot paper;order robots useful;command predict horizon;agent;order robots;language commands;natural language;simulated scenes;training agent;implied human instructions;robot paper propose;human instructions propose;training agent just", "pdf_keywords": "predicting motion plans;visualizing robot actions;language commands robot;robot actions;robot actions execution;automatically visualizing robot;agent training inference;motion plans goals;deep neural;simulated scenes learned;commands robot;planning;scenes learned;natural language commands;robot simulated scenes;planning produced;plans lack human;motion plans;commands robot simulated;high level actions;plan inference;component agent training;notions agency planning;visualizing robot;scenes learned generic;planning produced plans;training inference facilitates;hierarchical task network;plan inference agnostic;agent training"}, "0b2ff02ab23e5c9910b98fb87c4d58045dbe89ce": {"ta_keywords": "reverberant speech recognition;includes reverberant speech;reverberant speech;asr techniques discriminative;speech recognition;techniques discriminative training;discriminative training;speech recognition task;discriminative training technique;discriminative training various;propose discriminative training;includes reverberant;mixture models deep;reverberant;techniques discriminative;gaussian mixture models;competition includes reverberant;asr techniques;training various feature;deep neural networks;state art asr;channel dereverberation method;including gaussian mixture;models deep neural;single channel dereverberation;mixture models;gaussian mixture model;neural networks;deep neural;subspace gaussian mixture", "pdf_keywords": ""}, "d72a1579074a1a2bc500f257474144b1957d5166": {"ta_keywords": "coded computation neural;learning based coded;coded computation enables;performing coded computation;existing coded computation;coded computation framework;coded computation approaches;coded computation;widely deployed neural;based coded computation;computation neural;nonlinear computations support;coded computation general;deployed neural networks;performing coded;nonlinear computations;computation neural network;deployed neural;non linear computations;support nonlinear computations;neural network inference;coded;neural networks variety;open source prediction;challenges performing coded;based coded;existing coded;prediction serving promise;neural networks;nonlinear functions showcase", "pdf_keywords": ""}, "c5141ed9ed785a6a1df61b36883e6dfa19a59ff7": {"ta_keywords": "speech separation promising;channel speech separation;speech separation;learning based separation;standard source separation;source separation methods;source separation;channel speech recognition;based separation frameworks;single channel speech;separation promising technique;separation frameworks;separation promising;data training robust;separation methods;synthetic overlap datasets;based separation;multiple channel speech;separation methods variety;speech recognition;training robust;training robust models;speech recognition systems;overlap datasets necessary;separation frameworks evaluate;performance speech recognition;separation;current source separation;improving performance speech;speech recognition develop", "pdf_keywords": "dataset speech separation;speech separation techniques;speech separation technologies;speech separation;speech separation involving;standard speech separation;speech separation demonstrate;speech separation seeks;conditions speech separation;dataset speech;creating dataset speech;speech data;speech data achieve;deep learning mask;automatic speech;corpus speech data;detection automatic speech;speech activity detection;large corpus speech;speech activity;learning mask based;speech recognition;achieve separation performance;trained large corpus;separation demonstrate shortcomings;learning mask;separation performance;speech recognition machine;data achieve separation;separation techniques"}, "77000dba4b0638bb8f4222efcd731e040938c846": {"ta_keywords": "predict driver intention;driver speech use;prediction driver interaction;driver distraction;driver speech;intention driver speech;propose driver prediction;driver distraction contribute;driver prediction;driver interaction;shown driver distraction;reduce driver cognitive;driver interaction car;driver cognitive;predict driver;driver prediction improve;driving history predict;driver intention;history predict driver;driver cognitive load;driver intention driver;prediction driver;intention driver;presented prediction driver;based driving history;prediction improve interaction;driving history;distraction contribute accidents;improve interaction vehicle;predict user intention", "pdf_keywords": ""}, "ab8be9e585e599db99d8451e63a2311d88ff9293": {"ta_keywords": "cache clusters twitter;cache clusters;caching systems;caching systems context;real world cache;memory caching systems;cache workloads collecting;caching;caches;cache use cases;memory cache clusters;production caches;memory caching;cache workloads;cache;world cache workloads;strategy production caches;cache use;performance memory caching;caching increase throughput;use memory caching;world cache;production caches surprising;caches surprising;caching increase;memory cache;industrial cache;memory caching increase;cache working sets;industrial cache use", "pdf_keywords": ""}, "c612905cffc5a9aa9f0d8ac7ce1fd17f90413dab": {"ta_keywords": "attention model addressed;predicting argument;attention model;present neural architecture;argument goal predicting;predicting argument successfully;attention;goal predicting argument;present neural;sentences picked attention;neural;picked attention model;frequently successful arguments;models interplay opinion;neural architecture;ones present neural;neural architecture explicitly;sentences;interplay opinion holder;explicitly models interplay;successful arguments;model addressed frequently;opinion holder reasoning;argument goal;models interplay;reasoning;sentences picked;analysis suggests sentences;challenger argument goal;picked attention", "pdf_keywords": "argumentation quality;argumentation quality assessment;argumentative interactions aimed;argumentation received attention;relationship argumentation quality;argumentative interactions;model argumentative interactions;dialogical aspects argumentation;argumentation received;model argumentative;argumentation;aspects argumentation received;investigate relationship argumentation;aspects argumentation;argument represents addressed;understanding model argumentative;argumentative;relationship argumentation;argument paper investigate;argument paper;view sentences;expressed viewpoint challenger;discussions;argument order predict;argument represents;view sentences likely;viewpoint challenger argument;predict argument;presentation argument paper;argument order"}, "5547eff5376c56358be56f8bcc3a4b6ce4600bb5": {"ta_keywords": "speech recognition asr;automatic speech;automatic speech recognition;state automatic speech;robust asr systems;context automatic speech;build robust asr;robust asr;speech recognition;robust automatic repeat;recognition asr developed;recognition asr;speech recognition real;available robust automatic;motor control systems;robust automatic;motor control;control motion motor;motion motor;asr systems;design motor control;automatic repeat;toolkits available robust;able steer motor;motor control able;idea motor control;motion motor noisy;motor noisy environment;repeat request asr;asr developed context", "pdf_keywords": ""}, "f430c43018f17cabccd3a2e9258aff3da508afe1": {"ta_keywords": "presence foreign body;body home contact;method detecting presence;home contact body;detecting presence foreign;detecting presence;foreign body home;foreign body;contact body;detecting;body home;presence foreign;body;new method detecting;presence;method detecting;home contact;foreign;method;home;present new method;new method;contact;present new;new;article present new;present;article present;article", "pdf_keywords": ""}, "4b34a4cc5bc9defb0f530d61f9b0f843071e227c": {"ta_keywords": "risk excessive vaginal;prevalence excessive vaginal;menstrual hygiene research;vaginal bleeding delivery;menstrual hygiene;significance menstrual hygiene;hygiene excessive vaginal;keywords menstrual hygiene;bleeding sexually;vaginal bleeding sexually;bleeding sexually transmitted;vaginal bleeding;risk keywords menstrual;menstrual hygiene excessive;transmitted population women;excessive vaginal bleeding;bleeding delivery;excessive vaginal;bleeding delivery societies;sexually transmitted population;respectively vaginal bleeding;vaginal;sexually transmitted;bleeding delivery ranged;bleeding delivery common;hygiene excessive;hygiene;hygiene research;percent respectively vaginal;examine significance menstrual", "pdf_keywords": ""}, "39025112f6a40d8aae38f2e966bb27cbc35ea25d": {"ta_keywords": "xmath1 correlation single;model state potts;state potts model;xmath1 correlation;xmath0 xmath1 correlation;potts model;correlation single component;model state;component model state;study xmath0 xmath1;xmath0 xmath1;results study xmath0;xmath1;single component model;state potts;study xmath0;correlation single;component model;xmath0;potts;single component;correlation;model;component;state;single;present results;paper present results;present;paper present", "pdf_keywords": ""}, "0db557c4315b1e08ef65ff15b96eb7630014bf72": {"ta_keywords": "discussion detecting;detecting unnecessary utterances;digressions discussion detecting;unnecessary utterance detector;discussion detecting unnecessary;utterance detector;utterances having dialogue;utterance detector evaluated;dialogue cross validation;having dialogue;automatic summarization;summarization paper propose;leave dialogue;automatic summarization paper;based automatic summarization;dialogue;utterances;dialogue intervene method;unnecessary utterances;having dialogue intervene;summarization paper;dialogue intervene;leave dialogue cross;summarization;utterance;performance unnecessary utterance;unnecessary utterance;dialogue cross;unnecessary utterances having;utterances having", "pdf_keywords": ""}, "09a169c853e24b3a5196eefeab4c94eaac744cda": {"ta_keywords": "crowdsource political annotations;political annotations phrase;political annotations;crowdsource political;task identifying political;identifying political;identifying political position;annotations phrase sentence;sentiment analysis;annotations phrase;annotated dataset;sentiment analysis successfully;recursive neural network;recursive neural;political ideology model;newly annotated dataset;ideology model outperforms;annotated dataset existing;phrase sentence level;annotations;work sentiment analysis;neural network rnn;crowdsource;individual political ideology;sentiment;model individual political;neural network;individual political;apply recursive neural;political ideology", "pdf_keywords": ""}, "3a72f1346f3cd41e14b45c7fba5259bc77357ed4": {"ta_keywords": "efficiently learned recursive;learned recursive;recursive clauses programs;equivalent non learnability;efficiently learnable;clauses efficiently learnable;programs linear recursive;learned recursive function;non learnability;non learnability general;recursive clauses efficiently;learnability general class;cryptographically hard learn;learnability original program;learnability;recursive calls programs;ary recursive clauses;learnable learning constant;program linear recursive;efficiently learnable learning;learn programs unbounded;clauses efficiently learned;learnability general;non learnability original;ary recursive;learnable learning;clauses programs constant;free clauses efficiently;clause hard learning;program classes cryptographically", "pdf_keywords": "learnability recursive;learnability recursive logic;pac learnable generalization;model pac learnability;clause polynomially predictable;learnability clauses constant;paclearnability analyze learnability;pac learnability;learnability clauses;analyze learnability clauses;predictability class languages;deterministic turing;clauses paclearnable;locality pac learnable;boundaries learnability recursive;class clauses paclearnable;clauses paclearnable nonrecursive;limitations learning recursive;polynomially predictable;pac learnable;pac learnability 1984;deterministic turing machine;learnable generalization constant;polynomial predictability;polynomial predictability class;learning recursive programs;learnability;paclearnable nonrecursive;space deterministic turing;recursive clause polynomially"}, "b62d63580b81a2cbb20c3c1593dd62d118e4cb07": {"ta_keywords": "code natural language;models code generation;programs natural language;code generation;natural language specifications;synthesizing programs natural;synthesizing code;code generation synchromesh;synthesizing code natural;synthesizing programs;trained models code;semantic decoding csdin;generate code;methods synthesizing code;trained language models;descriptions using gpt;interface synthesizing programs;constrained semantic decoding;language specifications synchromesh;constrained semantic;using constrained semantic;generate code providing;code providing flexible;pre trained language;output language;language descriptions using;trained language;natural language descriptions;trained language model;language descriptions", "pdf_keywords": "programs constrained semantic;language program completion;code natural language;unconstrained language models;syntactic semantic constraints;generate programs constrained;constraints code generation;unconstrained language;semantic constraints code;semantic decoding csdin;language models make;program completion engine;constrained semantic;semantic constraints;language models;target language grammar;code generation;language program;generation frozen language;learn language model;program completion;rich syntactic semantic;constrained semantic decoding;frozen language model;enforcing rich syntactic;synthesizing code natural;programs constrained;syntactic semantic;synthesizing code;present language program"}, "af85c67a1f30f8359be1091234118492b511a088": {"ta_keywords": "paper case;case;paper", "pdf_keywords": ""}, "cd9e1eac4c93a314254cf8a8682ed5f01b6a808f": {"ta_keywords": "reasoning kb neural;topological insulator;topological insulator ti;similarity self organized;2d topological insulator;deeper language understanding;neural;neural approaches;neural approaches natural;constructing answer question;natural language processing;natural language;processing nlp;propose novel embedding;approaches natural language;language processing nlp;embedding;insulator;self organization cognition;self organized;language understanding;kb neural approaches;nlp fail logical;language processing;organization cognition based;insulator ti;organization cognition;constructing answer;deeper language;dimensional 2d topological", "pdf_keywords": "question query embedding;question answering;kb question answering;domain question answering;queries joint embedding;answer inference count;question answer inference;question answering kbqa;query embedding;answer answering;query embedding qe;answer inference;framework answer answering;question answering propose;sketches query embedding;embedding qe knowledge;answer answering open;answering;answer intersection queries;extend logical queries;queries using sketches;answering open;answer question answer;answering kbqa task;answer question;entities kb queries;answering kbqa;qe knowledge bases;knowledge bases;embedding qe"}, "9712ebfbc4f86c68403f64918463edad3e553ac6": {"ta_keywords": "energy efficient tracking;noise centralized tracking;active sensors fidelity;efficient tracking;single photon detector;sensor activation centralized;activation centralized tracking;stochastic approximation learning;centralized tracking numerical;sampling stochastic approximation;active sensor selection;dynamic sensor activation;centralized tracking;sensors fidelity;sampling stochastic;photon detector;photon detector presence;stochastic approximation;gibbs sampling stochastic;efficient tracking mechanisms;performance single photon;sensor selection;sensors fidelity increases;global optimal;active sensor;convergence global optimal;dynamic sensor;active sensors;noise centralized;tracking numerical", "pdf_keywords": "stochastic approximation wireless;decentralized tracking linear;decentralized tracking;sensor subset centralized;problem decentralized tracking;subset centralized tracking;optimal sensor subset;sensor selection algorithm;sensor subset selection;sensor networks;approximation wireless sensor;centralized tracking;statistics active sensing;algorithms dynamic sensor;wireless sensor networks;sensor networks contend;tracking linear gaussian;dynamic sensor subset;subset selection tracking;optimal sensor;stochastic approximation;sensor selection;sensor subset;active sensing;wireless sensor;centralized learning algorithms;choosing optimal sensor;selection tracking;present stochastic approximation;stochastic approximation method"}, "873dff010c00f0601d6939324929eeabb1ddbd6e": {"ta_keywords": "threshold secret sharing;secret sharing networks;communication secret sharing;secret sharing;protocol distributing secret;distributing secret;secret keys participants;secret sharing important;distributing secret keys;network sneak algorithm;secret keys participant;sneak algorithm requiring;threshold secret;bounds communication secret;secure multiparty computation;sneak algorithm;shamir threshold secret;protocols secure multiparty;communication secret;algorithm disseminating shares;secret keys;shares network sneak;network sneak;secure multiparty;cryptographic protocols;cryptographic protocols secure;sharing networks;directly pass secret;communication randomness required;protocol distributing", "pdf_keywords": "secret sharing graphs;networks independent secret;secure network coding;independent secret sharing;distributed network coding;secret sharing;secret sharing general;collusionresistance irrespective network;network coding;cost secret sharing;network coding problem;communication cost secret;sharing graphs;network properties secret;sharing general networks;distributed network;independent secret;shares secret;shares secret participants;present distributed network;disseminate shares secret;distributed communication;algorithm called sneak;distributed algorithm;collusionresistance;distributed communication ef\ufb01cient;cost secret;sharing graphs important;condition ii collusionresistance;coding problem admits"}, "35cb2b9febada179689724c78dfe31d9fa3f74c4": {"ta_keywords": "erasure coding;storage low latencies;codes erasure coding;overhead durable storage;erasure coding fragments;erasure coding called;cloud storage;keeping storage overhead;codes erasure;durable storage;reduction storage overhead;durable storage low;new cloud storage;keeping storage;storage;storage ring;cloud storage provides;storage overhead low;reduction storage;storage overhead wasp;significant reduction storage;new data compression;storage provides;storage overhead;walled storage;reconstruction codes;offline keeping storage;single walled storage;number erasure coding;data compression", "pdf_keywords": "erasure coded storage;coded storage systems;erasure coding keeping;coded storage;erasure coding codes;storage overhead erasure;new erasure coding;erasure coding;overhead erasure coded;sure erasure coding;erasure coding called;family erasure coding;erasure coded;coding keeping;storage systems;azure storage;reliability storage;overhead durable storage;reliability storage overhead;durable storage;storage consistently;storage systems windows;windows azure storage;storage;durable storage consistently;azure storage paper;storage overhead;overhead erasure;coding keeping incoming;coding codes"}, "35c6bdab35e8fd4e982302b5270da3c8098c58b1": {"ta_keywords": "simulation subgoal;subgoal compositions environments;generalization novel subgoal;simulation subgoal systems;sequences diverse subgoals;instructions sequences diverse;subgoal compositions;novel subgoal compositions;language instructions sequences;subgoal systems;approach simulation subgoal;instructions sequences;modularization improves generalization;diverse subgoals;sequences diverse;novel subgoal;subgoal systems case;swarm brownian;sequence approaches;sequence approaches alfred;subgoals;swarm brownian particles;natural language instructions;subgoal;modularization improves;building natural language;improves generalization novel;sequence sequence approaches;benchmark modularization improves;improves generalization", "pdf_keywords": ""}, "32feca141fce06c6588b4014d27953a3fc25f19b": {"ta_keywords": "learns world dynamics;model learns commonsense;ground language natural;knowledge ground language;ground language;propose model learns;learns commonsense;learns commonsense knowledge;effectively learns world;model linguistic;language model;model learns;language model giving;learns world;learns just objects;new language model;language summaries physical;language model called;ground language able;unified model linguistic;model effectively learns;meaning model learns;language natural;learn ground language;natural language;model learns just;knowledge learn ground;interface language model;model linguistic form;world dynamics communicate", "pdf_keywords": "language grounding;grounded language understanding;language model;machine language model;model linguistic;language model outside;model pretrained language;pretrained language model;physically grounded language;study language grounding;language model giving;train machine language;grounded language;language understanding;model linguistic form;machine language;language perception lm;language grounding common;sense reasoning representation;joint model linguistic;language;model predicting state;pretrained language;predicting state changes;linguistic;grounding common sense;language perception;study language perception;ablation study language;reasoning representation"}, "d9c2242e3aa17db649c92d7d4db46509f3d203db": {"ta_keywords": "confidence reinforcement learning;reward function constraints;constrained markov decision;reinforcement learning;stochastic decision;reinforcement learning settings;reward satisfying constraints;markov decision processes;constraints learning probability;stochastic decision problems;achieves sublinear regret;markov decision;confidence reinforcement;learning settings reward;class stochastic decision;constrained markov;reward function;reinforcement;constraints learning;upper confidence reinforcement;satisfying constraints learning;delta constrained markov;sublinear regret;regret respect reward;generate control dynamics;learning probability;decision processes;learning probability delta;stochastic;priori transition kernel", "pdf_keywords": "bandit problem safety;safe reinforcement learning;learns policy maximizes;known reward constraint;algorithm learns policy;reward constraint costs;bandit problem;reward constraint;reward function constraints;reward satisfying constraints;learns policy;satisfy constraints learn;learning problem reinforcement;armed bandit problem;constraints learn;reinforcement learning;propose reinforcement learning;problem reinforcement learning;constraints learn ing;constraints learning;reinforcement learning transition;constraints learning problem;learning settings reward;reinforcement learning standard;reinforcement learning algorithm;lead constraints learning;multi armed bandit;kernel known reward;rein forcement learning;problem safe reinforcement"}, "b990517fbbf4499861d7aa00407b0422874ab990": {"ta_keywords": "predicting untranslated terminology;sequence taggers translation;supervised sequence taggers;interpreters accurate translation;simultaneous translation corpus;task predicting terminology;predicting terminology simultaneous;sequence taggers;translation corpus;predicting terminology;terminology simultaneous interpreters;naist simultaneous translation;taggers translation speech;translation speech language;predicting untranslated;interpreters leave untranslated;simultaneous translation;method predicting untranslated;untranslated terminology;translation difficult terminology;translation speech;interpreter struggle translating;simultaneous interpreters;translation corpus shimizu;interpreters accurate;interpreters;taggers translation;challenges faced interpreters;simultaneous interpreters leave;indicate interpreter struggle", "pdf_keywords": "simultaneous machine translation;machine translation simultaneous;automatic translation speech;machine translation capable;human translators annotate;automatic translation;untranslated translator interpreters;machine translation;propose machine translation;machine translation systems;language automatic translation;translation systems;translators annotate;capable translating speech;translator interpreters;terminology simultaneous interpreters;human translators;translation speech language;literally untranslated translator;translation capable translating;untranslated translator;present machine translation;translation speech;translating speech;translation capable;translator interpreters paper;translation simultaneous interpretation;translators;translator;simultaneous interpreters speakers"}, "fb38451ff87254ac1ff15e79154ef958b4efb6a6": {"ta_keywords": "complex neural net;neural net models;neural net;nlp recurrent networks;recurrent networks sequence;computation graphs decompose;computation graphs;learning represent complex;decompose complex computations;recurrent networks;neural networks turn;deep learning;computation graph formalism;networks sequence tagging;sequences trees attention;based computation graph;sequence tagging prediction;deep learning neural;toolkits based computation;cnn library;learning neural networks;python bindings cnn;graphs representing inputs;nlp recurrent;networks sequence;softmax;patterns come nlp;bindings cnn;neural networks;learning neural", "pdf_keywords": ""}, "e2d770b9ab691753a7ec1eb439185303118c8455": {"ta_keywords": "multilingual motion assistant;multilingual artificial intelligence;multilingual artificial;presents multilingual artificial;build multilingual artificial;intelligence agent assistant;agent assistant;multilingual motion;motion assistant;motion assistant maa;agent assistants;artificial intelligence agent;real time language;results multilingual motion;technologies build multilingual;multilingual;build multilingual;agent assistant maia;ala agent assistants;intelligence agent;intelligence ala agent;assistant;language human;agent;multi agent;assistant maa based;artificial intelligence ala;translation layer;language human quality;presents multilingual", "pdf_keywords": ""}, "c8f78575bfb642b2dab6ed542a683ade9527c17d": {"ta_keywords": "deceased organ matching;organ matching;types fairness patients;fairness patients;fairness stability;fairness patients regions;organ patient preferences;organ matching unusual;different types fairness;fairness stability strategyproofness;mechanism perform matching;efficiency fairness;efficiency fairness stability;efficient stable fairer;types fairness;competition organ patient;matching;organ tissue authority;algorithmic decisions simple;patient preferences highly;fairer compared mechanisms;algorithmic decisions;matching dynamic sided;competition organ;properties efficiency fairness;patient preferences;decisions simple mechanism;perform matching;fairness;consideration organ tissue", "pdf_keywords": ""}, "4cc07c367e4a1f932e159678ef711e1802edf49f": {"ta_keywords": "architectures spoken intent;spoken intent prediction;speech commands snips;fluent speech commands;speech commands;combines automatic speech;automatic speech;recognition natural language;speech recognition natural;speech processing;sub tasks;task specific utility;ascent sub task;architecture architectures spoken;sub tasks including;intent prediction;spoken language understanding;understanding speech processing;sub task specific;sub task;model decomposable tasks;speech recognition;hierarchy sub tasks;architectures spoken;speech processing allow;decomposable tasks;tasks including;tasks including natural;sub task individually;fluent speech", "pdf_keywords": "spoken intent prediction;models spoken intent;speech intent systems;speech commands snips;fluent speech commands;speech commands;splits fluent speech;intent prediction offering;spoken language understanding;intent prediction;speech intent;spoken intent;using spoken;utterances;toend speech intent;sub task utility;speaker test sets;fluent speech;utterances conclusions;splits decomposable tasks;utterances conclusions present;end toend speech;task utility;task utility functions;intent systems;speaker test;models spoken;speakers held utterances;open speaker test;model decomposable tasks"}, "8fa6b06cb96e5ae98dfff1c50f6940ef43af223f": {"ta_keywords": "storage retrieval quantum;erasure coded storage;coded storage;new approach storage;quantum information stored;storage retrieval;approach storage;storage;disk io;information stored storage;stored storage;storage ring;stored storage ring;coded storage reduces;disk io reconstruction;storage fault tolerance;approach storage retrieval;data additional storage;bandwidth disk io;erasure coded;new erasure coded;traffic disk io;latency degraded reads;io reconstruction data;retrieval quantum information;hitchhiker reduce latency;huxley hh hitchhiker;retrieval quantum;disk;hitchhiker new erasure", "pdf_keywords": ""}, "1941f5b053ccc80fa44980d38ac074145591b4ec": {"ta_keywords": "sentence embedding models;multilingual interbank transcriptional;language sentences vectors;semantic sentence embedding;sentence embeddings;sentence embeddings propose;sentence embedding;sentences vectors;embeddings propose deep;semantic sentence encoding;similarity bilingual data;encode natural language;learning embeddings;interbank transcriptional;work sentence embeddings;latent semantic vector;sentence encoding ways;unsupervised semantic similarity;shared sentences translation;parallel sentences isolating;multilingual interbank;common latent semantic;similarity bilingual;bilingual data;latent semantic;learning embeddings properties;sentence encoding;indicator similarity bilingual;performance multilingual interbank;shared sentences", "pdf_keywords": "deep generative;sentence representations large;semantic sentence embeddings;learned semantic embeddings;latent semantic embedding;semantic embeddings poor;deep generative model;architectures sentence embedding;multilingual sentence representations;propose deep generative;semantic embedding;sentence embeddings;autoencoders propose deep;sentence representations;semantic embeddings;sentence embedding challenging;sentence embedding;embedding challenging task;capable sentence generation;variational autoencoders propose;variational autoencoders;learned semantic;semantic similarity measures;semantic embedding explaining;accuracy learned semantic;parallel sentences isolating;semantic similarity;generative model encouraged;translation systems variational;generative"}, "553028f7f7c850371379c621e40d7d00e75303a6": {"ta_keywords": "improving multilingual representations;multilingual interactions performance;high resource languages;improving multilingual;lingual transferability;cross lingual transferability;lingual transferability alleviates;multilingual models;multilingual representations demonstrate;multilingual models potential;impact multilingual interactions;multilingual interactions;interactions performance multilingual;directions improving multilingual;better cross lingual;low resource languages;multilingual representations;resource languages phenomenon;performance multilingual;exist multilingual models;interference adding language;performance multilingual home;cross lingual;language specific layers;multilingual home;resource languages;languages phenomenon;languages phenomenon known;multilingual;resource languages present", "pdf_keywords": "transfer multilingual models;improving multilingual representations;interference multilingual models;multilingual models exploits;multilingual models proven;lingual transferability mitigating;improve cross lingual;crosslingual transfer multilingual;cross lingual transferability;effective transfer multilingual;multilingual models;lingual transferability;multilingual models languages;transfer multilingual languages;improving multilingual;multilingual representations;approach crosslingual transfer;negative interference multilingual;multilingual models recent;transfer multilingual;effective crosslingual;crosslingual transfer;effective crosslingual 1source;crosslingual 1source training;proven effective crosslingual;novel approach crosslingual;directions improving multilingual;cross lingual;interference multilingual;multilingual languages work"}, "1606dc1e966ad59dd96dc8e74722dca06b1f1a58": {"ta_keywords": "evolutionary causal matrices;educational interventions predicted;based evolutionary causal;evolutionary causal;educational interventions;outcomes educational interventions;interventions predicted;causal matrices;interventions predicted using;causal;outcomes educational;markov chain;markov chain based;long term outcomes;term outcomes educational;using markov chain;chain based evolutionary;interventions;markov;predicted using markov;using markov;term outcomes;educational;outcomes;evolutionary;based evolutionary;long term;predicted using;predicted;chain based", "pdf_keywords": "simulation model interventions;model interventions classroom;model interventions;predict outcome interventions;simulation model predicting;interventions classroom;simulated longitudinal;simulation model;frequency interventions;interventions classroom setting;computer simulation model;particular simulated longitudinal;simulated longitudinal trajectories;simulation program implementing;simulation program;simulation model used;frequency interventions required;computer simulation;interventions groups;minimum frequency interventions;promote moral development;moral development based;simulation;interventions attempt determine;outcome interventions groups;present simulation model;moral development;simulated;intervention frequency;developed computer simulation"}, "e3480d9395e692833b722b2e957d51139985f310": {"ta_keywords": "question answering making;generative question answering;question answering;pretrained language models;pretrained language model;answering making;question classes;language models;language models present;identify question classes;language model;idea pretrained language;pretrained language;answering making available;quantum body based;answers outside training;answering;qcrs based idea;introduce new challenge;quantum body;limitations pretrained language;surprisingly good answers;language model used;quantum;new challenge;partition function quantum;qcrs;question classes appears;_macaw challenge seeks;learning", "pdf_keywords": "question answering macaw;question answering;answer question answering;answering question answering;question answering based;question answering problems;answering based ability;learning model answering;question answering behavior;answering macaw questions;limits question answering;automatically answer question;model answering;answering question;answering macaw;ability answer queries;answering based;answering problems easy;answering problems;model answering question;answer queries;macaw questions learn;automatically answer;answering;answer question;answering behavior;answer queries question;question classes;questions learn;challenge questions designed"}, "6d19d73909ffaa6c94cae6a2535ed52d138cb63b": {"ta_keywords": "chat oriented dialog;build conversational agent;dialog management;constructing appropriate dialog;build conversational;building chat oriented;method build conversational;approaches building chat;developing chat oriented;human conversation examples;twitter conversations experimental;oriented dialog management;appropriate dialog corpora;dialog corpora;dialog utilizing;scripts twitter conversations;domain chat oriented;conversational agent;building chat;human human conversation;human conversation;conversational agent interact;method developing chat;developing chat;twitter conversations;conversation examples movie;dialog corpora raw;conversation examples;dialog utilizing real;oriented dialog utilizing", "pdf_keywords": ""}, "d1678032a9eee94ec0a9c54fb008e1addc7213d4": {"ta_keywords": "parametric utility learning;utility learning;utility learning method;utility learning using;extend utility learning;new utility learning;estimated utility functions;learning method unbiased;defined estimated utility;utility functions quantify;estimated utility;heteroskedasticity inference propose;utility functions;energy efficient behavior;encourage energy efficient;using heteroskedasticity inference;heteroskedasticity inference;voting data simulate;game defined estimated;predict performance social;performance social game;method unbiased;occupant voting data;parametric utility;learning method;utility;voting data;framework parametric utility;game designed encourage;method unbiased unbiased", "pdf_keywords": ""}, "33cd5965745dc2e8bb8d0400d0b3c18d4e6369d4": {"ta_keywords": "cache clusters twitter;cache clusters;caching systems context;caching systems;real world cache;memory caching systems;cache workloads collecting;memory cache clusters;caching;caches;cache workloads;cache use cases;caching increase throughput;cache;world cache workloads;cache use;production caches;strategy production caches;memory caching;performance memory caching;use memory caching;world cache;caching increase;memory cache;industrial cache;caches surprising;cache working sets;production caches surprising;industrial cache use;memory caching increase", "pdf_keywords": ""}, "0b79cb7fe16aa8b99d521989f39e49034394f701": {"ta_keywords": "crowdsourcing online game;web human computation;online gamers generate;human computation research;computation human computation;human computation;crowdsourcing online;defining human computation;gamers generate useful;method crowdsourcing online;computation human;human computation new;gamers generate;novel method crowdsourcing;human computation systems;crowdsourcing;online gamers;online game data;method crowdsourcing;complex computation human;online game;machine learning hci;game data;target online gamers;ai;web human;ai machine;intelligence solve computational;users solve computer;human intelligence", "pdf_keywords": ""}, "61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886": {"ta_keywords": "benchmark logic nli;lip language models;logic nli;fol reasoning commonsense;reasoning commonsense inference;language models lmms;logic fol reasoning;reasoning commonsense;lms fol reasoning;lip language;performance lip language;benchmark logic;language models;logicnli inferential dataset;logicnli inferential;logic fol;fol reasoning motivates;tasks experiments bert;commonsense inference;fol reasoning;target fol reasoning;logicnli;fol reasoning new;commonsense inference used;proposed benchmark logic;experiments bert;logic;weaknesses lms fol;lms fol;lip", "pdf_keywords": "language inference logicnli;reasoning commonsense inference;natural language inference;language inference;reasoning commonsense;commonsense inference design;logical natural language;commonsense inference;inference logicnli;disentangles reasoning commonsense;generalization reasoning;language models;robustness generalization reasoning;inference logicnli disentangles;natural language;logicnli disentangles reasoning;benchmark named logical;logicnli;speci deep neural;language models lms;deep neural;models lms bert;inference design diagnostic;neural model speci;inference;reasoning originated linguistics;named logical natural;logical natural;inference design;reasoning"}, "a829d65de0cc19da49ad6b4a294dd31545aed2bb": {"ta_keywords": "statistics;statistics statistics;statistics statistics july;statistics july;31 2016 statistics;paper present statistics;2016 statistics statistics;2016 statistics;statistics statics;statistics statistics statics;statistics july 28;2016 presented jstor;presented jstor;researchers;helps scholars researchers;present statistics statistics;digital archive;scholars researchers;archive;present statistics;researchers students;statistics statics dynamics;scholars researchers students;productivity;jstor;researchers students discover;digital archive note;archive note;presented jstor profit;answer use information", "pdf_keywords": ""}, "bc5e4b9fb3a40057df4994354a403202218d53a6": {"ta_keywords": "declarative program search;transformed program heuristic;semantics preserving transformations;program transformations;speedups described nlp;semantics preserving;declarative program;set program transformations;sequence semantics preserving;program transformations simple;program heuristic search;program search;described nlp;program search sequence;search sequence semantics;correct declarative program;programmers substantial improvements;program heuristic;automated search;transformed program;programming unique speedups;automated search like;runtime algorithms;employ dynamic programming;practice automated search;efficiency transformed program;algorithms terms speedups;described nlp literature;dynamic programming;human programmers substantial", "pdf_keywords": "answer set programming;program heuristic search;context free parsing;algorithms automating;free parsing algorithm;faster algorithms automating;linguistics computer programs;nlp algorithmist set;free parsing;nlp algorithmist;program heuristic;algorithmist set program;set programming;automating work nlp;algorithms automating work;inference computational linguistics;context free conditional;parsing algorithm computes;transformed program heuristic;parsing algorithm;work nlp algorithmist;automating;computational linguistics computer;computational linguistics;conditional inference computational;programming algorithms;programs program transform;automatically analyzing;automated search;algorithm computes context"}, "ab17c315f7ee4fe69fde2f3d8ae0e30e4e2f3a2b": {"ta_keywords": "structured documents answer;dochopper iteratively;learns learn structure;learn structure;documents answer complex;learn structure concatenating;structured documents;dochopper iteratively attends;learning learns;heirarchically structured documents;dochopper algorithm efficient;assisted learning learns;learns;dochopper algorithm;answer complex questions;neural representations hierarchical;questions paper dochopper;model dochopper iteratively;learns learn;text;learning learns learn;dochopper computer assisted;representations hierarchical;assisted learning;hierarchical;text combining;learning;dochopper computer;computer assisted learning;documents answer", "pdf_keywords": "question answering;domain question answering;reading comprehension models;question answering questions;questions long structured;extractive questions;structured documents answer;question answering common;comprehension models;long structured documents;computing paragraph embeddings;answering questions computing;questions context experiments;comprehension models jointly;questions long documents;encode questions context;text segmentation supervised;extractive questions 51;structured documents;subset extractive questions;paragraph embeddings;documents answer complex;novel approach answering;step reading comprehension;text segmentation;questions computing paragraph;jointly encode questions;answering questions;reading comprehension;paragraph embeddings eq"}, "e54ffc76d805c48660bb0fd20019ca82ac94ba0d": {"ta_keywords": "pretrained language models;intrinsic dimension surprisingly;intrinsic dimensionality low;dimension based generalization;intrinsic dimension provides;provide intrinsic dimension;dimensional task representations;task representations compression;intrinsic dimension;intrinsic dimensionality;intrinsic dimension based;minimizes intrinsic dimension;lower intrinsic dimension;millions parameters datasets;dimension provides;dimension surprisingly;dimensionality low;dimension provides empirical;low dimensional task;pre training implicitly;intrinsic dimension fixed;pretrained language;dimensionality low dimensional;training implicitly;dimensional;generalization bounds provide;training implicitly minimizes;dimension;dimensional task;tuning parameter space", "pdf_keywords": "length natural language;task representations compression;natural language tasks;minimal description length;pre trained representations;description length tasks;dimensional task representations;intrinsic dimensionality low;low intrinsic dimension;intrinsic dimension surprisingly;dimension based generalization;lower intrinsic dimension;lowering intrinsic dimensionality;representations intrinsic dimension;length tasks empirically;provide intrinsic dimension;parameterization pre trained;empirically pre training;minimizes intrinsic dimension;notion intrinsic dimensionality;trained representations intrinsic;low dimensional task;dimensionality average nlp;nlp task compress;intrinsic dimension;description length natural;context pre trained;intrinsic dimensionality hypothesize;common nlp tasks;dimensionality low"}, "2a82a16bdb793dc388391be57d6424f0d5090513": {"ta_keywords": "ranked golf;ranked golf tournaments;ranking ranked golf;golf tournaments approach;rankings;golf tournaments;tournaments approach;tournaments;ensuring rankings;scores manner papers;challenges ensuring rankings;based ranking ranked;ranking;based ranking;ranking ranked;tournaments approach addresses;new ranking;scores manner;new ranking method;scores empirically;provide scores;datasets peer reviews;golf;designed scores;scores empirically evaluate;present new ranking;method based ranking;ranked;interface peer review;ensuring rankings incorporated", "pdf_keywords": "paper ranking;scores peer review;paper ranking important;ensuring rankings;peer review peer;item peer review;review peer;peer review;peer review papers;ranking;vs paper ranking;review peer review;challenges ensuring rankings;ensuring rankings incorporated;ranking important;ranking important aspect;quantized scores peer;reviewers based quantized;rankings;new hyperparameter selection;assignments reviewers based;rankings incorporated updates;scores peer;hyperparameter selection algorithm;peer review reasonable;hyperparameter selection;assigning assignments reviewers;rating vs paper;assignments reviewers;scores manner papers"}, "188fd1373aefdbf564e90a76fed43e1b8b7052dc": {"ta_keywords": "xmath1 type diode;diode performance;diode diode performance;diode room temperature;room temperature diode;temperature diode;diode placed vacuum;diode;diode diode;type diode;type diode diode;temperature diode placed;effect diode diode;performance nd xmath0nb;effect diode;diode room;investigate effect diode;diode placed;diode diode room;vacuum chamber temperature;nnb xmath1 type;nnb xmath1;nd xmath0nb;nd xmath0nb xpm0oge;xpm0oge nnb xmath1;xmath1 type;xmath1;xmath0nb xpm0oge nnb;performance nd;xmath0nb xpm0oge", "pdf_keywords": ""}, "69e2d1f5374918111432fae23212c2759b1357c2": {"ta_keywords": "approach ranking players;ranking players based;ranking players;improved ranking algorithms;ranking items based;ranking items;ranking algorithms;probabilistic approach ranking;ranking algorithm;ranking algorithm counts;recovering ranking using;recovering ranking;allow improved ranking;new ranking algorithm;improved ranking;ranking algorithms prove;ranking;method ranking items;ranking using;ranking using number;succeeds recovering ranking;propose new ranking;approach ranking;new ranking;ranking includes;method ranking;ranking includes special;players based pairwise;notion ranking;comparisons optimal", "pdf_keywords": "ranking items;ranking certain;statistically sound rankings;ranking items according;chosen pairwise comparisons;item ranking;stochastic comparison models;total ranking certain;pairwise comparisons;pairwise comparison items;chosen item ranking;ranking problems;ranking;rankings based sequence;comparison items online;partial total ranking;comparison models pairwise;pairwise comparisons paper;handle ranking problems;total ranking;stochastic comparison;handle ranking;consider problem ranking;rankings based;pairwise comparison;rankings;ranking certain number;item ranking refers;ranking problems paper;probabilistic model pairwise"}, "926d827aef568ed97431a7845c9a8138930c80fd": {"ta_keywords": "responses political information;political information;affective responses political;political information report;social affective responses;reactions social sharing;affective reactions social;responses political;reactions social;vote statistics united;affective responses;vote statistics;exploring social affective;social sharing behavior;social sharing;social affective;exploring social;information group sources;studies exploring social;group sources evidence;political;link affective reactions;sources differently information;sharing behavior subjects;information outgroup sources;people treat messages;messages group sources;link affective;affective reactions;sharing behavior", "pdf_keywords": ""}, "9b534639bcadc9ad232b338e760c523a4d74c8de": {"ta_keywords": "descriptions linguistic phenomena;descriptions linguistic;language descriptions language;language descriptions;grammar descriptions consumption;descriptions language;grammar descriptions;concise descriptions linguistic;descriptions phenomena morphological;descriptions language mind;summarized grammar descriptions;extract descriptions phenomena;extract concise descriptions;linguistic phenomena;extract descriptions;descriptions consumption linguists;linguistic;creation language descriptions;linguistic phenomena article;phenomena morphological agreement;morphological agreement;summarized grammar;linguists language;morphological agreement case;word order languages;language complex;automated evaluation;principles summarized grammar;descriptions phenomena;framework extract descriptions", "pdf_keywords": "multilingual automatic parser;treebank language;treebanks language;automatic parser;treebanks language present;learning rules linguistic;model treebank language;discovering valid grammar;automatic parser jointly;treebank language apply;treebanks;test portions treebanks;treebank;automatically learning rules;syntactic analysis extract;text syntactic analysis;portions treebanks language;syntactic analysis experts;model treebank;syntactic analysis;features syntactic analysis;parser jointly predicts;rules linguistic;extract features syntactic;analysis dependency parses;based multilingual automatic;raw text syntactic;text syntactic;parser;multilingual automatic"}, "987c5ad75d5092bed03e9f523aec00dc43bc17e4": {"ta_keywords": "traffic air quality;intersections air quality;air quality traffic;air quality intersections;traffic congestion smog;effect traffic air;traffic congestion;quality traffic congestion;impacts road traffic;urban air quality;air quality urban;road network density;effect traffic;traffic planning traffic;road traffic characteristic;traffic control study;traffic planning;traffic characteristic parameters;traffic air;road traffic;traffic characteristic;quality urban air;intersections air;congestion delay index;traffic control;planning traffic control;planning traffic;approach traffic planning;quality traffic;peak congestion delay", "pdf_keywords": ""}, "89c2b3bfcc309ce16c85d2ab0c8cac5295400715": {"ta_keywords": "stacked sequential learning;learners sequential stacking;base learners sequential;sequential learning;arbitrary base learner;new sequential learning;entropy learner;maximum entropy learner;sequential learning meta;entropy learner generally;sequential stacking improves;sequentially stacked;base learner;sequential learning scheme;learners sequential;stacked sequential;sequential learning method;base learners;tasks sequentially stacked;sequential stacking;sequential stacking consistently;learning scheme called;crfs stacked sequential;learning algorithm arbitrary;learning;nonsequential base learners;learning scheme;learners crfs;sequentially stacked maximum;learners crfs designed", "pdf_keywords": ""}, "9b263129548dc09369e8bc34560fe5bb6047fcee": {"ta_keywords": "electricity market interactions;electricity market;simulating market dynamics;european electricity market;simulator wholesale electricity;wholesale electricity market;simulating market;dynamics european electricity;capable simulating market;market dynamics;market rules proposed;market interactions;market rules designed;market dynamics deliberate;market interactions natural;industry market rules;market rules;decision strategies market;behaviour discourage market;wholesale electricity;competition financial industry;european electricity;strategies market;incentives competitive behaviour;discourage market manipulation;discourage market;influence external magnetic;transportation market rules;strategies market outcomes;gas transportation market", "pdf_keywords": ""}, "9333d372ad3887e02029d2eab0dbc0c0478582c7": {"ta_keywords": "unsupervised natural language;learning natural language;evaluation unsupervised learning;natural language processing;unsupervised learning natural;unsupervised learning;unsupervised learning methods;standard unsupervised learning;evaluation unsupervised;learning methods unsupervised;annotated data learn;methods unsupervised learning;natural language;methods natural language;unsupervised natural;supervised learning primary;unsuitable evaluation unsupervised;propose unsupervised natural;learning supervised learning;learning methods natural;learning supervised;gold standard unsupervised;context evaluation;learning natural;supervised learning;supervised learning supervised;standard unsupervised;language processing;context evaluation appropriate;used context evaluation", "pdf_keywords": ""}, "35b376ad9e03e5e0b930c53a48817bfb5703108d": {"ta_keywords": "text style transfer;style transfer task;machine translation models;neural machine translation;machine translation;translation models;semantic similarity metrics;leverage semantic similarity;supervised models text;style transfer;translation models explicitly;models text style;semantic similarity;text style;models text;metrics training;automatic metrics propose;automatic human evaluation;existing automatic metrics;automatic metrics;using metrics training;similarity metrics;tuning neural machine;texts;evaluation strong baselines;text;fine tuning neural;outputs input texts;input texts paper;similarity metrics originally", "pdf_keywords": "bottlenecks sentiment transfer;model sentiment transfer;sentiment transfer task;sentiment transfer;sentiment transfer propose;reward content preservation;learning model sentiment;metric content reward;reward content;model sentiment;style transfer leveraging;content reward;transfer leveraging semantic;ef\ufb01cient reward content;text style transfer;leveraging semantic similarity;bottlenecks sentiment;existing automatic evaluation;preservation text style;automatic evaluation metrics;semantic similarity metric;content preservation text;improving content preservation;address bottlenecks sentiment;sentiment;automatic evaluation;machine translation;improving content;similarity metric content;tasks machine translation"}, "c96970cfb1c13ae6dccc30de482ce6b0d4414f2b": {"ta_keywords": "predicate invention;invented predicate subroutine;new predicates introduced;predicate invention instead;version predicate invention;predicates introduced;common invented predicate;scales predicate invention;invented predicate;creating new predicates;new predicates implicitly;new predicates;soft version predicate;small scales predicate;predicates;predicates introduced logical;paper simple predicate;predicates implicitly;simple predicate;predicate subroutine;predicate invention pi;scales predicate;predicate;simple predicate written;predicate written way;pi new predicates;predicate written;predicate subroutine present;version predicate;predicates implicitly group", "pdf_keywords": ""}, "4fee3d5d476568deb971768f8a5191eb627309d0": {"ta_keywords": "game dynamics linearized;nash equilibria robust;games stable equilibria;game dynamics;stable equilibria stable;game jacobian symmetric;stable equilibria;continuous games zero;stable learning;equilibria stable;continuous games;sum games stable;linearized local equilibria;stability equilibrium;equilibria robust;continuous games general;stability equilibrium zero;player continuous games;gradient based dynamics;rates continuous games;nash equilibria;zero sum games;game jacobian;dynamics linearized;decomposing game jacobian;stable learning rates;fixed points player;stability;sum nash equilibria;games stable", "pdf_keywords": "stability learning games;dynamics continuous games;jacobian learning dynamics;learning dynamics continuous;learning dynamics;learning games continuous;nash equilibria stable;scalar player continuous;games nash equilibria;games necessary stability;stable points nash;continuous games analyzing;learning dynamics near;continuous games;learning dynamics main;player continuous games;necessary stability learning;games continuous action;jacobian learning;gradient learning dynamics;stability learning;nash particular vector;nash equilibria;robust potential games;games analyzing eigenstructure;spectrum jacobian learning;learning games;based learning dynamics;zero sum games;games continuous"}, "5df0b8b80aecda1efdebac5d1ab7bcf94a88c68f": {"ta_keywords": "downstream nlp tasks;improvements downstream nlp;word embeddings;nlp tasks;downstream nlp;contextualized word embeddings;word embeddings derived;nlp tasks compare;nlp;feature extractor;trained language models;new feature extractor;corpora biomedical articles;corpora biomedical;embeddings;domain specific corpora;extractor based bioinformatics;language models;feature extractor based;entity;specific corpora biomedical;pre training domain;bioinformatics;million pubmed abstracts;training domain;biomedical articles improves;bioinformatics community;bouncer elmo biomedical;biomedical articles;embeddings derived pre", "pdf_keywords": "embeddings answer biomedical;biomedical question answering;biomedical natural language;sentence token representations;token representations linguistic;contextual word representations;token embeddings;based token embeddings;entity recognition;word representations like;entity recognition ner;word representations;named entity recognition;biomedical named entity;embeddings answer;probing sentence token;domain speci embeddings;representations linguistic;token representations;token embeddings character;inference nli dataset;based word representations;training domain contextual;question answering;language inference nli;natural language inference;sentence token;embeddings use;representations linguistic properties;question answering tasks"}, "5930efbf01efa8944258b1c0f7349111702f779e": {"ta_keywords": "nlp libraries provides;language network detailed;nlp libraries;language network;based language network;used nlp libraries;nlp frameworks implementing;processing nlp;implementing natural language;nlp frameworks;language processing nlp;established nlp frameworks;processing nlp requires;annotators python;annotators python interface;natural language processing;annotations data structures;widely used nlp;natural language;language processing;corpora annotations;easily use annotators;corpora annotations data;nlp requires;access annotators python;annotators;established nlp;reading corpora annotations;annotations;nlp requires considerable", "pdf_keywords": ""}, "44268b5a208e8f48a5883bb12e3e80a13101e752": {"ta_keywords": "risk factor contrast;heart failure incidence;congestive heart failure;contrast induced acute;heart failure hf;factor contrast induced;clinical subgroup analysis;factor contrast;onset heart failure;heart failure;risk factors groups;failure incidence contrast;kidney injury;acute kidney injury;combination clinical subgroup;contrast induced;studied risk factors;clinical subgroup;kidney injury ci;risk factors associated;contrast;highest risk factors;factors lowest risk;risk factors;induced acute kidney;associated onset heart;acute kidney;lowest risk factors;aki congestive heart;incidence contrast induced", "pdf_keywords": ""}, "96ed7a7da69d654668b35b50344debd44e87c1a1": {"ta_keywords": "topic identification spoken;topic identification unstructured;topic identification;language topic classification;attention based contextual;topic identification topic;identification topic identification;topic classification;identification spoken segments;topic shifts;contextual noncontextual models;spoken segments;variable topic shifts;topic classification paper;significantly outperforms contextual;manner topic identification;languages attention based;contextual model significantly;identification unstructured audio;outperforms contextual;contextual model present;based contextual model;method topic identification;contextual model;contextual noncontextual;contexts selective;contextual dependencies sequential;contextual;languages attention;independently based contextual", "pdf_keywords": "learning spoken segment;spoken segment representations;spoken language segments;spoken segment;attention based contextual;models recognition spoken;speech segments using;contextual attention modeling;speech segments;contextual modeling;represent speech segments;learning spoken;attention modeling promising;recognition spoken spoken;identi\ufb01cation topic speech;contextual attention;contextual modeling digitizing;speech human language;contextual model;use attention models;obtained contextual attention;attention modeling;topic speech human;language segments;speech minutes;transcribed speech minutes;learn context;attention models;recognition spoken;based contextual model"}, "b9c3e87bc09c4c6167a03a835c30b1c23bef7a40": {"ta_keywords": "trained contextual;trained contextual embeddings;pre trained contextual;training user distribution;contextual embeddings;embeddings like bert;contextual embeddings like;like bert generalization;concept machine learning;based concept machine;concept machine;embeddings;machine learning;bert generalization;learning;machines based;contextual;bert generalization kbqa;distribution large scale;machines based concept;embeddings like;home model;home home model;test distributions;generalization;generalization kbqa;based base home;learning key;home home;scale machines based", "pdf_keywords": "question answering crowdsourcing;answering crowdsourcing present;answering crowdsourcing;question answering;answer question answering;contextual embeddings bert;context natural language;trained contextual embeddings;crowdsourcing present;embeddings bert;trained language models;crowdsourcing;pre trained contextual;trained contextual;contextual embeddings;automatically generalizing;natural language;crowdsourcing present fine;models like bert;language models;crowd worker context;questions language variation;language models like;embeddings bert multiple;questions language;stronger generalization;contextual;stronger generalization propose;answering;natural language question"}, "05b6be9aec266072669f6f287a846637eedf19b5": {"ta_keywords": "soil nematode community;soil nematode canadensis;nematode soil;canadensis soil nematode;nematode canadensis invasion;soil nematode soil;soil nematode;soil nematode nematode;soil soil nematode;dynamics nematode community;members soil nematode;biotic communities soil;nematode soil international;nematode nematode community;alter soil nematode;community nematodes large;community nematodes;invasion canadensis soil;nematode community correlated;stoichiometric microbes soil;communities soil;nematode canadensis;nematode community decreased;communities soil soil;community community nematodes;nematode community considerably;stoichiometry dynamics nematode;microbes soil global;nematode community;soil global warming", "pdf_keywords": ""}, "449310e3538b08b43227d660227dfd2875c3c3c1": {"ta_keywords": "neural networks continuous;recurrent neural networks;model recurrent neural;network coupled quantum;recurrent neural;coupled quantum dots;time residual networks;neural networks;neural network;model recurrent;residual networks;residual networks continuous;neural;quantum dots;coupled quantum;neural network used;generative model train;backpropagate ode solver;brain neural network;quantum dots introduce;dynamics network;models constant memory;sequence hidden states;scalably backpropagate ode;neural network model;normalizing flows generative;scalably backpropagate;continuous normalizing flows;used model recurrent;flows generative model", "pdf_keywords": "prediction recurrent neural;recurrent neural net;networks rnns;learning differential equations;recurrent neural networks;neural networks rnns;prediction recurrent;networks rnns powerful;rnns powerful tools;models like recurrent;time neural networks;prediction prediction recurrent;deep generative machine;task recurrent neural;dynamics deep neural;recurrent neural;continuous time neural;neural net machine;learning differential;challenging task recurrent;generative machine learning;rnns;backpropagate ode solver;deep generative;neural net;like recurrent neural;generative machine;neural networks gradient;deep neural;powerful tools predicting"}, "ab1e5a3c5521b6204dc7c6f1fa72b88000bc30ee": {"ta_keywords": "utility humans answering;answering engine;humans answering questions;qa systems effectively;answering engine introduce;humans answering;precede answering engine;trained qa;progress qa systems;trained qa models;qa evaluation;pre trained qa;assistants typing questions;qa systems;qa evaluation expand;need qa evaluation;performance bouncer;qa;answering questions;performance individual bouncer;qa models;answering questions real;supported qa;languages supported qa;questions search engine;assistants typing;speaking voice assistants;performance bouncer level;progress qa;voice assistants typing", "pdf_keywords": "answering engine;answering engine introduce;precede answering engine;question answering tasks;question answering;interface machine translation;dataset question answering;answering tasks collected;answering tasks;speech recognition keyboard;pipeline precede answering;machine translation speech;precede answering;machine translation;augmentation query repair;query repair synthetic;translation speech recognition;speech recognition;interface noise qa;answering;keyboard components pipeline;questions written language;qa systems;synthesizing interface;spoken typed noise;interface machine;typed noise;recognition keyboard components;language spoken typed;synthesizing interface noise"}, "5f609f252d8815c5fb660d83c0dc71af21ecf65d": {"ta_keywords": "event monitoring twitter;monitoring twitter dynamic;keywords event monitoring;monitoring twitter;known keywords streaming;known keywords monitoring;keywords monitoring;internet event monitoring;keywords streaming;keywords streaming text;twitter dynamic;conversational nature twitter;things internet event;event monitoring;keywords event;nps keywords event;nature twitter;phrases nps keywords;automatically finding noun;counting known keywords;known keywords;twitter;twitter dynamic conversational;noun phrases nps;event monitoring systems;finding noun phrases;internet event;nps keywords;things internet;internet things", "pdf_keywords": ""}, "33c691ca050e1806d44c08e55e63fcd7e555899a": {"ta_keywords": "negative classifier;positive negative classifier;negative classifier generally;classification case positive;positive unlabeled data;negative classifier pu;train unbiased positive;classifier pu positive;mixing proportions positives;unbiased risk estimation;unbiased positive negative;unbiased positive;classifier;proportions positives negatives;based unbiased positive;supervised binary classification;distinguish positive unlabeled;unbiased risk;trained distinguish positive;propose new classifier;unlabeled mixture;positive unlabeled;positive class;positive class considered;proportions positives;classifier generally;new classifier;classification propose;classification propose new;considered unlabeled mixture", "pdf_keywords": ""}, "a8ea980b63deaf1404cd9f539a575b4e7135466e": {"ta_keywords": "coded resilience parity;parity models answering;better parity models;parity model neural;parity models decodes;parity queries;parity models systems;parity;parity models;inference parity queries;queries parity models;applicability parity models;using parity;answering queries parity;resilience parity;performs inference parity;queries parity;better parity;using parity models;applicability parity;inference parity;queries using parity;parity queries using;systems parity models;resilience parity model;perform better parity;parm performs inference;parity models shown;systems parity;parity model", "pdf_keywords": ""}, "fb7caddac20dca012f48c90b2e1e2383f7185051": {"ta_keywords": "interpretability tools machine;existing interpretability tools;interpretability tools;misuse interpretability tools;use interpretability tools;interpretability tools uncover;existing interpretability;use interpretability;interpretability;data scientists trust;machine learning findings;misuse interpretability;use existing interpretability;trust misuse interpretability;interpretability tools interpretml;tools machine learning;machine learning ml;learning ml powerful;powerful machine learning;observe use interpretability;data scientists;machine learning;models machine learning;contextualized social science;learning ml;learning ml newfound;machine learning moved;predictive models machine;building evaluating predictive;indicate data scientists", "pdf_keywords": "interpretability ml models;interpretability techniques ml;interpretability ml;models interpretability ml;studies evaluating interpretability;evaluating interpretability techniques;evaluating interpretability;scientists use interpretability;interpretability tools;use interpretability;interpretability techniques;interpretability;interpretability tool;propose interactive interpretability;interactive interpretability tool;use interpretability tool;models interpretability;interactive interpretability;interpretability tools allow;mental models interpretability;interpretability tool allows;design interpretability tools;interpretability techniques clear;different interpretability techniques;different interpretability;design interpretability;ml community stakeholders;interpretability tool uncover;intended data scientists;exploring dataset ml"}, "b1d8c868e1d6d4980ee2f8c50e6fc5e4e7027ca2": {"ta_keywords": "improves story continuation;character dialogue plus;story continuation accuracy;based story continuation;character dialogue;dialogue plus character;story continuation;story continuation extend;driven story continuation;story emerges characters;story continuation story;person narration;person narration dialogue;continuation story emerges;second person narration;character driven story;narration dialogue;narration;continuation story;narration dialogue requiring;dialogue plus;character relationships;character relationships improves;character relationship;characters role;character persona relationships;dialogue requiring models;character relationship information;dialogue;characters role playing", "pdf_keywords": "characterdriven emergent storytelling;character driven storytelling;narrative emerges characters;emergent storytelling narrative;storytellers character driven;automated storytellers character;storytelling narrative emerges;emergent storytelling;story emerges characters;storytelling produce language;driven storytelling;storytelling narrative;faced automated storytellers;story picking character;storytellers character;automated storytellers;emerges characters interactions;driven storytelling produce;narrative emerges;story picking;storytelling;advancing story addition;characters interactions;person narration;character persona relationships;character personas;character driven;person narration dialogue;characters interactions seen;second person narration"}, "e785441f5ccd6e4e29b3123e61121df5c65b88f7": {"ta_keywords": "variational autoencoder;variational autoencoder vae;training neural inference;inference networks approximate;vaiana variational autoencoder;aggressively optimize inference;variational learning;training inference network;training reduce inference;accompanying variational learning;inference networks;training inference;variational learning technique;neural inference;optimize inference network;neural inference networks;inference network;autoencoder vae popular;reduce inference lag;model accompanying variational;stages training inference;optimize inference;autoencoder vae;deep latent variable;collapse perspective training;latent variables optimized;inference network performing;vae training reduce;model posterior latent;reduce inference", "pdf_keywords": "stochastic variational inference;variational inference;variational inference sv;iterative inference;iterative inference data;training text model;performs iterative inference;hypothesize optimization inference;stochastic variational;traditional stochastic variational;optimization inference;lengthy iterative estimation;inference generation networks;text model predict;training text;iterative estimation;optimization inference generation;inference generation;approach machine learning;learning approach;learning;machine learning;optimum training;vae training fall;variational;text model;inference sv;iterative estimation contrast;vae training;local optimum training"}, "f7a2f2ae829545ee992a2214b3600cf914544e22": {"ta_keywords": "simulations human students;model intelligent tutoring;intelligent tutoring systems;intelligent tutoring;human students solving;student model intelligent;tutoring systems results;tutoring systems;predicted human students;tutoring;student accurately predicted;human students;students solving problems;human students correct;students solving;sim student accurately;model replicate students;replicate students;model cognitive ability;simple model cognitive;simulations human;student accurately;model cognitive;observe human students;student model;human students likely;students correct behavior;replicate students performance;cognitive model replicate;students", "pdf_keywords": ""}, "9112be1801598125d463febb96a525227c32acc1": {"ta_keywords": "learning structured loss;finite state transducers;loss functions learn;structured loss functions;propose convolutional wfst;speech recognition;state transducers wfs;recognition speech recognition;handwriting recognition speech;structured loss;convolutional wfst;state transducers;recognition speech;learn latent decomposition;learning structured;speech recognition framework;higher level representations;lower level representations;traditional convolution;replacement traditional convolution;state transducers used;sequence level loss;level loss functions;convolutional wfst layer;dynamically training;traditional convolution paper;paper propose convolutional;transducers wfs;transducers wfs new;transducers used dynamically", "pdf_keywords": "neural networks speech;recurrent neural networks;automatic differentiation words;automatic differentiation weighted;speech recognition sequence;train recurrent neural;architecture automatic differentiation;automatic differentiation;automatic differentiation operations;networks speech recognition;recognition speech;speech recognition;automatic speech;differentiation words handwriting;recurrent neural;speech recognition machine;recognition machine translation;deep neural;recognition speech recognition;deep neural network;output automatic speech;networks speech;differentiation operations wfsts;automatic speech recognition;handwriting recognition speech;speech recognition present;approach train recurrent;differentiation words;train recurrent;dynamically training"}, "4cf633d0893a1d3af97723ce1f2fae33c2a30043": {"ta_keywords": "similarity empirically outputs;relational classification;relational classification make;models relational classification;exact similarity empirically;similarity empirically;relations extracted;redundant relations extracted;similar relations method;sampling softmax classification;similar relations;detect redundant relations;exact similarity;sampling softmax;negative sampling softmax;softmax classification;softmax;distributions entity pairs;similarity;reciprocity fact distribution;open information extraction;information extraction;information extraction open;relationship reciprocity fact;entity pairs;redundant relations;approximation exact similarity;correlate human judgments;models relational;fact distribution", "pdf_keywords": "similarity relations knowledge;similarity relations semantic;quantify similarity relations;semantic similarity;relation similarity;similarity relations;similarity relations corresponding;relation similarity metric;provide relation similarity;investigate semantic similarity;semantic similarity 360;quantify similarity;semantic relations;relations knowledge bases;measure similarity relations;relations semantic relations;similarity score;similarity score correlate;method quantify similarity;relations semantic;similarity metric;similarity metric introduce;relations subset wikidata;similarity;judgment similarity relations;knowledge bases;semantic relations general;similarity present;trained fact distribution;annotations balancing recall"}, "9f1059006e4ba303f8945114eddadd50d58a9f3e": {"ta_keywords": "soft symbolic databases;knowledge bases;large knowledge bases;easily write neural;learning confidences soft;knowledge bases kbs;write neural models;learning systems fluid;computer large knowledge;symbolic databases;symbolic databases using;useful artificial intelligence;gradient based learning;learn instantiate query;soft symbolic;knowledge form;databases;large knowledge;artificial intelligence tasks;learning confidences;databases using differentiable;write neural;learning systems;facts soft kb;neural models;prior knowledge form;access rules learn;prior knowledge;modern gradient based;knowledge", "pdf_keywords": "natural language queries;answer natural language;learning systems nql;nlp code niq;niq learn answer;reasoning relationship soft;answering problems automatic;language queries fully;soft symbolic databases;learn answer natural;natural language;language queries;answer answering problems;answering problems;answer answering;applications nlp;learning called niq;approach answer answering;repository nlp;applications nlp code;niq learn;source applications nlp;nlp components;nlp code;repository nlp components;symbolic databases;called niq learn;queries;symbolic databases using;nlp components framework"}, "02b932416751674dc25353620a1df4b53c3a5f6f": {"ta_keywords": "linguistic annotations audio;annotating audio linguistic;linguistic annotations transcriptome;predict linguistic annotations;transcriptions linguistic annotations;annotations transcriptome based;annotations transcriptome;model automatic speech;automatic speech;train linguistic annotation;transcribing annotating audio;annotation target transcript;audio linguistic;audio linguistic information;annotations audio data;annotating audio;predict linguistic;annotations audio;estimate linguistic annotations;linguistic annotation target;transcriptions linguistic;transcripts speech;linguistic annotations accurately;linguistic annotation;phonemic transcripts speech;annotations linguistic information;linguistic annotations;quality transcriptions linguistic;annotations linguistic;transcripts speech pos", "pdf_keywords": ""}, "811531c959b0543a8e7abe1e827770e36b96f817": {"ta_keywords": "emphasis estimation;emphasis estimation using;speech translation systems;level emphasis estimation;paralinguistic information performance;emphasis target language;accurately translate emphasis;translation systems;speech translation;models emphasis translation;effect paralinguistic information;speech speech translation;translate speech languages;evaluation speech speech;objective evaluation speech;word level emphasis;paralinguistic information;evaluation speech;translate speech;emphasis target;investigate effect paralinguistic;level emphasis target;emphasis translation;translation systems combine;effect paralinguistic;speech languages;translate emphasis;emphasized words;speech languages human;emphasis translation translate", "pdf_keywords": ""}, "6c34b7b0441bff66cce2418d36acfd9776ad7bd2": {"ta_keywords": "learning algorithm riprep;algorithm riprep;algorithm riprep large;riprep;proposed rule learning;rule learning algorithm;ripperk competitive equivalent;rule learning;ripperk;ripperk competitive;rigidly action gravity;riprep large diverse;benchmark problems carolinarep;rigid body moves;rithm ripperk competitive;algo rithm ripperk;rigid body;rithm ripperk;moves rigidly action;dynamics inflexible rigid;riprep large;inflexible rigid;inflexible rigid rigid;rigidly action;moves rigidly;benchmark problems;rigid rigid body;equivalent rules bench;learning algorithm;rigid", "pdf_keywords": ""}, "d723630c585aa0e4084fdd6e71bc6586cfa30e9d": {"ta_keywords": "syntax pause information;pause prediction dependency;prediction dependency parsing;labeled syntax pause;prosodic information pauses;joint pause prediction;better pause prediction;syntactic information prosodic;syntactic structure spoken;pause prediction;dependency parsing model;syntactic information;pause information;syntax pause;spoken sentences analysis;structure spoken sentences;annotation syntactic information;prosody speech closely;dependency parsing presented;information prosodic information;information pauses;prosodic information;dependency parsing;sentences analysis models;pause prediction decision;speech closely;information prosodic;pause information obtained;parsing model;prosody speech", "pdf_keywords": ""}, "81bc64ce5553798c058f25fe5bd537d4bed67aed": {"ta_keywords": "luminescence liquid crystal;study quantum dots;quantum dots grown;quantum dots;luminescence liquid;liquid crystal;video luminescence liquid;diffraction study quantum;dynamics video luminescence;liquid crystal tuned;excited state separations;luminescence;dots grown inverted;diffraction;video luminescence;diffraction study;ray diffraction;resolution ray diffraction;ray diffraction study;state excited state;ground state excited;dots grown;crystal;excited state;crystal tuned;state excited;dots;quantum;liquid;levels ground state", "pdf_keywords": ""}, "a8c62c42509c45a708ba477b603ee3fb81c77056": {"ta_keywords": "news weibo social;false news weibo;detecting false news;false news detection;dissemination false news;news weibo;propagation false news;false news social;weibo social platform;news social media;news social;wide dissemination false;news reasonable fusion;social media verifiably;weibo social;contents social media;news detection;published social media;consume share news;false news news;news detection approaches;share news construct;news news posts;social media;share news;news posts;social media set;weibo;modal contents social;news construct publicize", "pdf_keywords": "detection false news;false news detection;detecting false news;detection falsenews;detection falsenews multi;false news text;image detection falsenews;text false news;news text real;text real news;real news text;news text detection;false news social;news detection researches;subtasks false news;news detection;false news image;falsenews multi;text detection false;traditional false news;false news convenience;existing false news;news text;falsenews multi modal;news social media;falsenews;detection traditional false;news text speci\ufb01cally;news image detection;predict text false"}, "44775500a5380be3776e876aedc43921d42d8de9": {"ta_keywords": "urban mobility data;urban activity patterns;patterns urban mobility;uncovers urban dynamics;discovering urban activity;urban mobility;activity patterns urban;urban dynamics;urban mobility recent;dynamics people urban;urban dynamics urban;dynamics urban dynamics;urban activity;massive urban mobility;dynamics urban;people urban activities;scale urban mobility;analyze urban dynamics;urban dynamics massive;urban activities;extract urban mobility;people activities urban;mobility data;dynamics urban region;discovering urban;life mobility data;patterns urban;dynamics massive urban;activities urban;mobility data population", "pdf_keywords": ""}, "470bfbde1dc0ed6ca989957dcd551213720657c0": {"ta_keywords": "neural machine translation;translation systems require;machine translation systems;machine translation;translation systems;machine translation nmt;quality machine translation;dependency trees language;translates inducing dependency;translation quality machine;translation nmt systems;dependent improve translation;improve translation quality;syntax model;syntax performance word;attention mechanism decoder;syntax model needs;inducing dependency trees;syntax performance;translation quality;model needs translation;trees language pair;simultaneously translates inducing;syntax alleviates burden;knowledge dependency trees;trees language;question syntax model;structure syntax performance;improve translation;language pair dependent", "pdf_keywords": "attention translating languages;neural machine translation;self attention translating;learning parse translate;attention useful translating;attention translating;machine translation model;machine translation;machine translation important;translation model learns;structure machine translation;machine translation encoder;parse translate improves;machine translation results;machine translation framework;translate improves neural;quality machine translation;attention results structured;syntax translation importance;structured self attention;translation model;machine translation process;languages learning parse;translation encoder ability;dependent improve translation;syntax translation;translation quality machine;propose machine translation;translation encoder;shared attention results"}, "bc494b9c6d9602a69b76ab9ea0e95d348a2fce19": {"ta_keywords": "annotators source document;evaluation generated summaries;improves inter annotator;annotators source;annotators;summarizing large scale;document evaluation;annotator performance demonstrate;salient content highlights;annotator;evaluated multiple annotators;annotator performance;evaluating inter annotator;document document evaluation;multiple annotators source;multiple annotators;annotator agreement comparison;generated summaries;inter annotator;inter annotator performance;novel approach summarizing;summarizing large;document evaluation hrc;summarizing;content highlights;approach summarizing large;annotator agreement;manual evaluation generated;inter annotator agreement;salient content", "pdf_keywords": "highlights reference summaries;reference evaluation summarization;summaries document highlighted;summarization evaluation summaries;evaluation summarization highres;summarization evaluation;evaluation summarization;manual evaluation summarization;performance summarization evaluation;evaluation summaries document;automatically summarizing;evaluation summarization paper;highlightbased reference evaluation;annotation highlight based;document highlight annotation;automatically summarizing source;highlight annotation;coloring summary evaluation;approach automatically summarizing;summarizing source document;annotation highlight;documents highlights reference;evaluation summaries;documents highlights;assessed documents highlights;highlight annotation highlight;summary evaluation framework;summarization highres;summarization highres framework;summarizing source"}, "e2ffd0ea7aa9cebaafba4afaee3cbe78070c8aa2": {"ta_keywords": "triphone hmms based;hmms based variational;recognition birds polymer;speech based bayesian;state triphone hmms;triphone hmms;bayesian approach recognizes;based variational bayesian;hmms based;paper propose bayesian;recognition birds;variational bayesian;bayesian framework;variational bayesian approach;propose bayesian framework;recognizes speech based;bayesian prediction;birds polymer chain;approach recognizes speech;bayesian framework constructs;bayesian prediction classification;bayesian;birds polymer;based bayesian prediction;based bayesian;propose bayesian;bayesian approach;recognizes speech;application recognition birds;recognition", "pdf_keywords": ""}, "3321c947a4a399803592f26879927e58f587fd74": {"ta_keywords": "predicting future arrests;scrutiny crowdsourcing;participants predict offender;come scrutiny crowdsourcing;scrutiny crowdsourcing crowdsourcing;process crowdsourced information;crowdsourced decisions;process crowdsourced;crowdsourcing process crowdsourced;crowdsourcing service studies;arrest occur algorithmic;performance crowdsourcing;future arrests;crowdsourcing process;crowdsourced decisions aggregate;performance crowdsourcing service;crowdsourcing service;crowdsourcing;crowdsourced information;predict offender rearrested;crowdsourced;crowdsourcing crowdsourcing process;likelihood arrest occur;likelihood arrest;optimize performance crowdsourcing;predict offender;crowdsourcing crowdsourcing;algorithmic risk assessment;aggregated crowdsourced decisions;algorithmic risk", "pdf_keywords": "predictive prediction offenders;prediction offenders criminal;prediction offenders;participants predicted arrest;predicted arrest assigning;offender likelihood recidivism;predicted arrest;participants predicted;participants asked predict;offender likelihood;particular participants predicted;depend offender likelihood;probability predictions;based investigation predictive;predict likelihood occurrence;investigation predictive;likelihood recidivism;offenders second survey;judicial decisions suggests;logistic regression participants;crowdsourcing;systems probability predictions;conducted crowdsourcing;predictive performance;crowdsourcing survey participants;predictive performance human;offenders criminal justice;predictive prediction;investigation predictive performance;predictive performance predictive"}, "0554246ebb6c53e88d7ac2aabf0c96a91ad500a0": {"ta_keywords": "training speech enhancement;channel speech enhancement;enhancement speech recognition;strong speech enhancement;speech enhancement;nets convolutional neural;convolutional tasnet;reduce speech recognition;enhancement speech;speech enhancement speech;convolutional tasnet conv;network cnn;deep learning;speech recognition;speech enhancement capability;networks cnn;speech recognition performance;table convolutional tasnet;models convolutional neural;speech recognition models;multi channel speech;speech enhancement article;tas nets convolutional;nets convolutional;neural networks cnn;neural network cnn;convolutional neural;convolutional neural network;convolutional neural networks;cnn based", "pdf_keywords": "channel speech enhancement;comparable speech enhancement;speech enhancement capability;strong speech enhancement;automatic speech separation;speech enhancement;reduce speech recognition;speech enhancement real;multi channel speech;speech separation based;channel convolutional beamforming;convolutional beamforming model;speech recognition performance;convolutional beamforming;channel speech;speech separation;preserving strong speech;channel track chime4;reduce speech;chime4 16 corpus;preserving comparable speech;separation based deep;speech recognition;beam tasnet framework;automatic speech;robustness beam tasnet;tasnet based models;beam tasnet;multi channel convolutional;method automatic speech"}, "c2bd176f8f9c84f9ba52ffb8f8bd4e9299c0f0cf": {"ta_keywords": "nonparametric prediction parking;prediction parking data;parking occupancy data;prediction parking;nonparametric quantile regression;analysis parking occupancy;parking data;parking data challenging;explore nonparametric quantile;quantile regression;analysis parking;nonparametric quantile;quantile regression approach;nonparametric prediction;gefcom2014 nonparametric prediction;parking occupancy;parking;approach analysis parking;quantile;gefcom2014 nonparametric;tracks gefcom2014 nonparametric;nonparametric;explore nonparametric;regression approach wind;solar price tracks;occupancy data;occupancy data city;paper explore nonparametric;wind solar price;solar price", "pdf_keywords": ""}, "a9a7058b39768ece13608e31341cfb16c4faf2c3": {"ta_keywords": "fair machine learning;policies algorithmic fairness;algorithmic fairness;fairness metrics;algorithmic fairness non;analysis fairness metrics;formulations fairness;analysis fairness;formulations fairness conclude;comprehensive analysis fairness;fairness metrics different;different formulations fairness;fairness non ideal;fairness conclude;fairness conclude critical;machine learning hopes;machine learning ideal;fairness;observe fair world;misguided policies algorithmic;fair machine;fairness non;fair world;policies algorithmic;automated decisions;fair world paper;observe fair;automated decisions propose;drive automated decisions;literature fair machine", "pdf_keywords": "fairness machine learning;addressing algorithmic injustices;fairness machine;algorithmic injustices;proposed fairness machine;fairness crucially policies;proposed fairness;shortcomings proposed fairness;fair machine learning;algorithmic injustices smaller;conceptualization formalization fairness;fairness crucially;formalization fairness crucially;solutions current injustices;formalization fairness;dynamics current injustices;injustices ongoing social;injustices ongoing;injustices;current injustices;injustices smaller scale;fairness;current injustices ongoing;injustices smaller;injustices potentially exacerbate;current injustices potentially;injustices potentially;fair machine;ideal approach political;political philosophy"}, "156323f4d87af6cf105c97bf29d324c9e3bc8f92": {"ta_keywords": "optimization methods speech;automatic speech recognition;automatic speech;approach automatic speech;methods speech translation;speech recognition;speech translation;speech synthesis;components automatic speech;speech recognition asr;speech translation st;mt speech synthesis;speech synthesis ss;speech recognition based;translation mt speech;machine translation;asr machine translation;machine translation mt;translation evaluation measure;translation evaluation;joint optimization methods;joint optimization;joint optimization contribution;based joint optimization;joint optimization addition;context joint optimization;methods speech;recognition asr;recognition asr machine;features optimized", "pdf_keywords": ""}, "3cd4797725ca9cf954946ed5309e15ebab80b92a": {"ta_keywords": "experts generative adversarial;generative adversarial networks;gan framework synthesize;conditional image synthesis;generative adversarial;experts generative;gan framework;synthesize images;networks poe gan;poe gan framework;image synthesis;generative;synthesize images conditioned;multimodal user inputs;unimodal conditional image;adversarial networks poe;generate images;image synthesis frameworks;experts generator;product experts generative;multimodal;existing conditional image;generate images based;image synthesis method;conditional image;gan;multimodal user;poe gan;framework synthesize images;leverage multimodal", "pdf_keywords": "multimodal conditional image;multiscale multimodal discriminator;novel multimodal conditional;multimodal projection discriminator;conditional image synthesis;multimodal discriminator architecture;multimodal discriminator;multimodal conditional;introduce multimodal conditional;multiscale multimodal projection;propose conditional generative;conditional generative;multiscale multimodal;propose multiscale multimodal;conditional generative model;discriminator conditional image;multimodal projection;multimodal;image modalities multiscale;generative adversarial;generative adversarial networks;multimodal projection loss;learns synthesize images;experts generative adversarial;novel multimodal;projection discriminator multiscale;image synthesis;image synthesis framework;encodes image modalities;image introduce multimodal"}, "365e049ecb7299cc6925483127f2f2123a97c35f": {"ta_keywords": "kernel detect changepoints;detect changepoints;change point detection;detecting abrupt changes;detect changepoints real;approaches kernel selection;changes time series;kernel selection algorithms;kernel sample test;developed kernel selection;parametric approaches kernel;changepoints real world;changepoints;applications kernel sample;novel kernel parameterization;kernel selection;klm kernel detect;abrupt changes time;abrupt changes;detecting abrupt;approaches kernel;propose novel kernel;kernel parameterization kullback;method detecting abrupt;kernel sample;changepoints real;novel kernel;kernel selection twosample;klm kernel;kernel detect", "pdf_keywords": "change point detection;detection time series;kernel approach changepoint;changepoint detection;changepoint detection time;nonparametric kernel approach;approach changepoint detection;propose kernel selection;kernel selection framework;kernel selection;novel nonparametric kernel;paper propose kernel;nonparametric kernel;kernel approach;identi\ufb01cation time series;time series analysis;detection time;point detection time;propose kernel;learning data;changepoint;time series;human activity sensing;change point;predictive hypothesis testing;learning data mining;time series present;time series data;propose novel nonparametric;machine learning"}, "464a75c05a5ce709fc515a2577b43acc8e3d45ce": {"ta_keywords": "multi hop inference;hop inference framework;hop inference;multi hop explanations;hop inference presence;hop explanations;hop explanation proposed;multi hop explanation;hop explanation;explanatory completeness finding;inference chain gaps;inference chain;multi hop;xmath0 xcite;inference framework allows;xcite;performance multi hop;inference framework;inference;complete inference chain;multiple hops;complete inference;large multi hop;inference presence;xmath0 xcite xcitation;presence multiple hops;hop;inference presence multiple;multiple hops best;work multi hop", "pdf_keywords": ""}, "a6e61164e7b385cec0e12093bc270eafd3ef1dbc": {"ta_keywords": "activity recognition;proposes activity recognition;activity recognition method;activities using labeled;unsupervised movement humans;users sensor data;user activities using;movement humans indoor;training sensor data;user activities;activities using;activity;users sensor;acceleration sensor data;movement humans;activities;training sensor;end user activities;gender users sensor;sensor data;humans indoor environment;study unsupervised movement;training data;labeled training data;acceleration sensor;unlabeled acceleration sensor;unsupervised movement;labeled training;sensor data obtained;label training sensor", "pdf_keywords": ""}, "d7c1bdafb51fe1a757604f9daeaea812f124320f": {"ta_keywords": "technology forecasting russia;technology forecasting english;makes technology forecasting;technology forecasting;systems technology forecasting;russian patent databases;information retrieval pipeline;forecasting english russian;semantic analysis word2vec;exploiting latent semantic;contracts influence technology;forecasting russia complicate;latent semantic analysis;word2vec paper propose;forecasting trends future;government contracts utilizing;latent semantic;forecasting russia;study government contracts;forecasting trends;technology trends;information retrieval;forecasting english;analysis word2vec;patent databases;government contracts;patent databases citation;analysis word2vec paper;influence technology trends;method forecasting trends", "pdf_keywords": ""}, "fba7c0a51a6301ca4086a5ce59b1f13af9acad7f": {"ta_keywords": "pointwise approach morphological;approach morphological analysis;approach morphological;morphological analysis;morphological analysis ma;morphological;structured approach japanese;partial annotation active;structured predictors;annotation active learning;structured predictors using;partial annotation;learning tagging paper;annotation active;learning tagging;similar structured predictors;combination partial annotation;annotation;accuracy similar structured;structure information learning;structured;information learning tagging;similar structured;leading order nlo;tagging paper;tagging;structured approach;state art structured;active learning;structure able outperform", "pdf_keywords": ""}, "5d07db93e6fbd9e10713a2f372131c777077062d": {"ta_keywords": "quantum algorithm deep;algorithm deep quantization;deep quantization;deep quantization significantly;deep reinforcement learning;algorithm deep;quantization large;quality quantization large;optimization bitwidths multilevel;deep networks minimizing;deep networks;quantization;discovering bitwidths;optimization bitwidths;deep reinforcement;networks minimizing computation;end deep reinforcement;quantization significantly reduce;discovering bitwidths end;quantization significantly;process discovering bitwidths;quality quantization;speed quality quantization;quantum algorithm;bitwidths multilevel systems;bitwidths multilevel;minimizing computation storage;heterogeneous bitwidth assignment;encodings heterogeneous bitwidth;reinforcement learning", "pdf_keywords": ""}, "e63e1db25f33162cc6e498f983dc3f6e10c9867e": {"ta_keywords": "speech separation problem;noise speech separation;separation speaker extraction;complexity speech separation;speech separation speaker;problem speech separation;speech separation extensively;speech separation;speaker extraction;speaker extraction multi;separation speaker;speakers observation based;numbers speakers observation;noise speech;round long recordings;long recordings;speech recognition systems;speakers observation;predict performance speech;speech recognition;long recordings present;presence noise speech;performance speech recognition;variable numbers speakers;estimating length chain;numbers speakers;speaker;complexity speech;performance speech;length chain noisy", "pdf_keywords": "recognition speech separation;separation speaker extraction;speech speech separation;speech speaker separation;speakers speech separation;speaker separation speech;speech separation;speech separation speaker;separation speech denoising;translation speech separation;talker speech separation;speaker extraction;recording overlapped speech;speaker separation;speech separation received;model speech extraction;speech separation important;inference speech extraction;speech extraction;separation speech;speaker extraction multi;overlapped speech speaker;separate speech;overlapped speech speech;overlapped speech;speech extraction diarization;able separate speech;separation speaker;speakers overlapped speech;speaker extraction machine"}, "c3d2c60e70cad17ea37cb116ab30e1239405dbdd": {"ta_keywords": "humans influence sudden;population humans influence;behavior population humans;sudden change environment;behavior population;influence sudden change;population humans;humans influence;behavior;influence sudden;sudden change;environment;population;study behavior population;change environment;humans;influence;study behavior;sudden;paper study behavior;change;paper study;study;paper", "pdf_keywords": ""}, "6a730aff0b3e23423a00cb3407eb04e7f6e83878": {"ta_keywords": "machine translation bayesian;translation bayesian approaches;statistical machine translation;translation bayesian;machine translation;model word alignment;moses machine translation;word alignment statistical;alignment statistical machine;word alignment;variational bayesian inference;variational bayes improves;bayesian variational bayesian;variational bayesian;bayesian variational;using variational bayes;variational bayes;bayesian inference model;alignment statistical;bayesian inference;apply bayesian variational;giza software improving;inference model word;bouncer apply bayesian;bayesian approaches;translation;simulating motion fly;bayes improves performance;statistical machine;model word", "pdf_keywords": ""}, "c06410ce8f9b1c941115d6d96780794e66b27eac": {"ta_keywords": "adaptation speech models;based adaptation speech;adaptation speech;approach automatic speech;utterance update approximating;automatic speech;incremental adaptation based;speech models time;automatic speech recognition;incremental adaptation;time varying acoustic;utterance utterance update;fast incremental adaptation;adaptation based macroscopic;speech recognition;changes speakers speaking;utterance update;speech models;adaptation based;recognition based adaptation;speech recognition based;including changes speakers;macroscopic time evolution;acoustic characteristics change;evolution realizes utterance;detecting environmental changes;based adaptation;changes based macroscopic;models time varying;posterior distributions acoustic", "pdf_keywords": ""}, "68167af17980a14ed5fa2514e61d76d5a6a9bed7": {"ta_keywords": "news scientific dubious;pandemic vaccine conversation;theories fake news;flu pandemic;news scientific;conspiracy theories fake;pandemic pandemic;government news scientific;dubious dominate pandemic;fake news compete;articles social media;social media posts;pandemic;articles shared twitter;infection pandemic pandemic;vaccine conversation;pandemic occurred february;pandemic vaccine;pandemic pandemic occurred;detection flu pandemic;legitimate sources information;pandemic occurred;government news;understand conspiracy theories;conspiracy theories;followed detection flu;infection pandemic;news compete legitimate;dominate pandemic vaccine;viral", "pdf_keywords": "vaccine discussion twitter;twitter examined content;discussion twitter covid;shared twitter examined;articles shared twitter;messaging mainstream misinformation;twitter examined;social media activity;outside social media;social media behaviour;twitter covid;understand social media;discussion twitter;information social media;analyze social media;outbreak characteristics content;multiple social media;usage social media;social media;twitter covid 19;social media platforms;content shared vaccine;twitter;misinformation multiple social;shared twitter;theories fake news;social media form;public health messaging;shared twitter february;legitimate sources information"}, "4a96b2b33786301d59670fa647f99e3dd807abb8": {"ta_keywords": "agents learn deterministic;agents converge learning;multiagent learning algorithms;multiagent learning;agents learn;games agents learn;continuous games agents;based multiagent learning;learning algorithms noncooperative;gradient based multiagent;non uniform learning;learn deterministic;gradient stochastic settings;individual gradient stochastic;equilibrium agents converge;continuous games;agents converge;gradient stochastic;converge learning;uniform learning;consider continuous games;equilibrium class gradient;converge learning path;equilibrium agents;strategies maximize;learning algorithms;strategies competition;fitness provide convergence;competition strategies maximize;uniform learning rates", "pdf_keywords": ""}, "d9b458d39e0912524032887aaaf922f0e950f0c1": {"ta_keywords": "event slepian wolf;wolf slewian process;event slepian;lhc;event event slepian;process context lhc;context lhc;computer simulation event;slepian wolf slewian;wolf slewian;slepian wolf;simulation event;slewian process;simulation event event;slewian process context;results computer simulation;computer simulation;wolf;event;slepian;simulation;event event;slewian;process;process context;results computer;present results computer;computer;paper present results;paper", "pdf_keywords": ""}, "76f9f4bf8d97de5e95d2fd9dd8b50041524fb1cc": {"ta_keywords": "embeddings entity alignment;entity alignment joint;knowledge embeddings entity;joint knowledge embeddings;entity alignment improve;knowledge graph completion;improvements entity alignment;entities wikipedia links;entity alignment;entity alignment aims;knowledge embeddings;alignment joint knowledge;entities wikipedia;approach entity alignment;link entities counterparts;link entities;alignment improve knowledge;improve knowledge graph;embeddings entity;information entities wikipedia;aligned entities;encodes entities relations;knowledge graph;knowledge embeddings experiments;entities relations;knowledge graphs;entities counterparts;multiple knowledge graphs;set aligned entities;entities counterparts multiple", "pdf_keywords": ""}, "476ff888fe3917f92b221c522ffb7bfaa4e1861b": {"ta_keywords": "retrieval conversational search;open retrieval conversational;retrieval conversational;conversational search systems;conversational search build;conversational search data;response ranking conversational;functional conversational search;conversational question answering;approaches conversational search;conversational search;retrieval conversational question;dataset conversational search;ranking conversational;role retrieval conversational;conversational search simplified;ranking conversational question;introduce open retrieval;question answering;systems retriever learnable;open retrieval;search systems retriever;retrieval;collection extracting answers;learnable retriever;retriever learnable;reranker reader based;response ranking;featuring retriever reranker;question answering answer", "pdf_keywords": "answer retrieval;open retrieval conversational;retrieval conversational;answer retrieval using;domain answer retrieval;conversational question answering;retrieval conversational question;conversational search systems;functional conversational search;question answering;conversational search;introduce open retrieval;question open retrieval;open retrieval;collection extracting answers;retrieval;extracting answers;passage retriever;history questions automatic;reranker passage reader;question answering orconvqa;history modeling components;using retriever neural;learn retrieve evidence;retriever passage reranker;retriever neural;propose retriever learnable;retriever neural reader;retrieval using;building functional conversational"}, "b169c4b6c23efe8cbd4dc29eb97939cbcfba0f28": {"ta_keywords": "persuasive dialogues real;persuasive dialogues;persuasion dynamics communicating;corpus persuasive dialogues;persuader utterances argumentation;persuader utterances;persuasion dynamics;common dialog acts;persuasiveness speaker;factors derived dialogue;dialog acts;utterances argumentation framing;dialog acts information;contribute persuasiveness speaker;influence persuasion dynamics;dialogue;dialogue acts;common dialog;dialogue acts particularly;persuasion;dialogues;utterances argumentation;argumentation framing;influence persuasion;30 persuader utterances;argumentation framing aiming;dialogue professional salespeople;predictors persuasive power;predictors persuasive;dialog", "pdf_keywords": ""}, "65d3575b1c380b1bcc14ec69ccf6989c04be9493": {"ta_keywords": "algorithms graphs;algorithms graphs concise;various algorithms graph;algorithms graph;spanning tree;essence algorithms graphs;tree mst graph;algorithms;unconnected data sets;algorithms graph missing;graphs concise elegant;combinatorial graph theory;graph theory powerful;minimum spanning tree;graphs;algorithms data;mst graph;graphs concise;spanning tree mst;algorithms data available;various relevant algorithms;various algorithms;new algorithm finding;combinatorial graph;relevant algorithms;graph theory;complex data sets;example algorithms;algorithms paper presents;guiding computations", "pdf_keywords": ""}, "cf7e8f47ad1c57738dc586109dcf28a22ab67b72": {"ta_keywords": "papers reviewer bids;bids review papers;reviewer bids;reviewer bids review;bids review;conference peer review;review present algorithm;peer review process;considerably suboptimal bidding;peer review;user ordering;bids;suboptimal bidding process;bidding process;bidding process conference;sequentially reviewer;algorithm finding optimal;sequentially reviewer needs;bidding;suboptimal bidding;fewer requisite bids;peer review present;papers aw bidding;finding minimal weight;minimal weight lifted;conference peer;ordering items community;review papers;user ordering items;super optimal ones", "pdf_keywords": "ordering peer review;paper reviewer objective;suboptimal peer review;peer review;reviewer objective best;reviewer objective;review paper ordering;objective best reviewers;peer review given;peer review paper;crowdsourcing platform optimizes;reviewers paper assignment;process peer review;users paper reviewer;crowdsourcing problem;process formulate crowdsourcing;paper ordering peer;formulate crowdsourcing problem;formulate crowdsourcing;crowdsourcing;paper presents crowdsourcing;peer review recognized;ordering peer;reviewers recommend recommend;improve review process;crowdsourcing problem multi;reviewers recommend;crowdsourcing platform;bidding process peer;presents crowdsourcing"}, "adc273bd25ab1e2a66543f23c7a801af0dd80e5b": {"ta_keywords": "recognition speaker diarization;speaker diarization;speaker diarization single;speaker diarization simultaneously;solve speaker diarization;speech recognition speaker;clustering based speaker;speaker automatic speech;extract recognize speech;automatic speech;recognition speaker;speech recognition ts;speech recognition;sample utterance speaker;automatic speech recognition;simultaneous speech recognition;dialogue recordings speaker;asr simultaneous speech;recognize speech;recognize speech target;speech target speaker;speaker proposed method;challenging dialogue recordings;vector embeddings outperforms;method solve speaker;utterance speaker;speaker diars based;solve speaker;vector vector embeddings;error rate diarization", "pdf_keywords": "speaker diarization based;multi speaker diarization;speaker diarization automatic;speaker diarization simultaneously;speaker diarization;known speaker diarization;conversation transcription;clustering based speaker;solve speaker diarization;speaker embeddings;speaker diarbization method;speaker diarbization;speaker embeddings ii;diarization automatic speech;based speaker diarbization;monaural conversation transcription;conversation transcription perform;target speaker embeddings;speech recognition asr;automatic speech;estimation target speaker;speech recognition;automatic speech recognition;perform automatic speech;dialogue recordings csj;utterance known speaker;dialogue recordings;channel dialogue recordings;method multi speaker;method solve speaker"}, "b20cadef0c59e80f7dfdf825b07442619d920fd5": {"ta_keywords": "recognition presence crowd;crowd based recognition;recognition asr attentionbased;vectorized beam search;based recognition presence;speech recognition asr;automatic speech;speech recognition;recognizing presence person;person crowd based;recognition presence;recognition asr;presence person crowd;asr attentionbased encoder;beam search techniques;automatic speech recognition;beam search;asr attentionbased;real time processing;attentionbased encoder decoder;recognizing presence;method recognizing presence;attentionbased encoder;beam search achieves;streaming search;rigid body speedup;person crowd;presence crowd method;motion body contact;time streaming search", "pdf_keywords": ""}, "16326359081a42c0b254ee6be39824fd2db07e48": {"ta_keywords": "pivot translation allows;language model triangulation;pivot language model;translation language pairs;pivot target translation;translation models;pivot translation;target translation models;translation process;translation time pivot;translation language;high translation accuracy;used translation process;languages particular triangulation;translation accuracy;translation process method;pivot language;translation accuracy conventional;propose pivot language;translation models source;triangulation method translates;information source translation;time pivot translation;target translation;translates combining source;translation allows translation;language pairs;allows translation language;information pivot phrases;pivot phrases", "pdf_keywords": ""}, "336ee50043b916c9e932338c02fd1abc87a6e849": {"ta_keywords": "compositional generalization solving;achieve compositional generalization;learning algorithm compositional;compositional generalization;compositional generalization basic;algorithm compositional generalization;cognition argues compositionality;achieve compositional;compositionality captured variable;expressions achieve compositional;ability compositional generalization;composer solver trained;compositional generalization paper;solver trained;algorithm compositional;compositionality captured;cooperative neural;solver trained end;compositional;neural modules composer;consists cooperative neural;generalization solving;compositionality;argues compositionality captured;memory augmented neural;generalization solving challenges;manner reinforcement learning;argues compositionality;cooperative neural modules;augmented neural", "pdf_keywords": "compositional generalization deep;learning compositional generalization;learning compositional;propose compositional generalization;compositional generalization;approach compositional generalization;compositional generalization basic;compositional generalization model;generalization deep;commands compositional compositionality;cantly compositional generalization;generalization deep seq2seq;learning approach compositional;compositional generalization fundamental;compositional tasks;solver compositional generalization;compositional compositional navigation;commands compositional;compositional tasks surpassing;compositional navigation commands;accuracies compositional tasks;navigation commands compositional;compositionality compositional;compositional navigation;compositional compositionality compositional;presents compositional compositional;compositional compositionality;propose compositional"}, "89ba434b30a3f1b61bcbcf917842899fe3d2eea4": {"ta_keywords": "translation model;creating translation model;parallel corpus;corpus identical semantic;translation model tm;language model;paraphrasing literature language;creating translation;techniques paraphrasing;techniques paraphrasing literature;language model lm;corpus;finding parallel corpus;parallel corpus identical;methods creating translation;identical semantic content;paraphrasing literature;literature language model;semantic content;paraphrasing;using techniques paraphrasing;corpus identical;text paper individuality;individuality writer speaker;semantic content different;semantic;identical semantic;individuality writer;words effective transform;function words", "pdf_keywords": ""}, "63a35d8822a042f6d6cd919fd5d3c9e94df6ee18": {"ta_keywords": "detection change quantum;change point detection;detecting change points;detect change points;detecting change;unsupervised change point;detect change;windows detect change;detection change;change quantum based;change quantum;changes want detect;change point instances;unsupervised change;existing unsupervised change;quantum based measurement;labeled change point;detect kinds changes;require detecting change;change point search;novel change point;detection unsupervised consequence;improve change point;change point;change points;change points complex;dimensional change point;measurement state existing;detection performance existing;true change point", "pdf_keywords": "change point detection;supervised change point;change detection;change detection time;supervised change;approach change detection;detection time series;detection based unsupervised;method supervised change;available change point;change point;improve change point;distance learned metric;wasserstein distance learned;unsupervised approach change;learned metric;metric sinhkorn divergences;new unsupervised approach;learning ground metric;unsupervised learning;learned metric used;unsupervised learning present;based unsupervised learning;existing unsupervised methods;point detection online;point detection learning;weighted wasserstein distance;detection learning;windows change point;metric sinhkorn"}, "00717c695e4a33318fe5655e2b69e1ba8b61f981": {"ta_keywords": "segmentation speech;speech recognition systems;segmentation speech recognition;speech recognition;method segmentation speech;speech text;predicting outcome race;speech text english;outcome race crowded;presents speech text;score based weighting;recognition systems;race crowded environment;race crowded;ground based predicting;rank score based;segmentation;based weighting approach;speech;score based;weighting approach;based predicting;based predicting outcome;weighting approach gives;outcome race;recognition;predicting outcome;rank score;crowded environment;new method segmentation", "pdf_keywords": ""}, "ba201da15899e78629ee5471e8d336b6b2eb7279": {"ta_keywords": "transportation networks communities;models traffic congestion;traffic congestion;transportation networks;public transit demand;congestion major cities;mobility decision;transit demand nature;transit demand;mobility decision support;riding public transit;constrained transportation networks;traffic congestion different;public transit;models traffic;routing models traffic;traffic;considerate routes ride;socially considerate routes;congestion appears services;congestion;mobility;seasons mobility decision;routes ride;routes ride vehicle;transportation;increasing rate urbanization;rate urbanization added;rate urbanization;lyft commonplace urban", "pdf_keywords": "proactive routing;routing games;proactive routing able;proactive proactive routing;router routing games;routing algorithm improves;routing games include;proposed routing algorithm;routes decrease congestion;proposed routing;routing algorithm;route planning;route suggestions;route planning algorithm;congestion information route;optimal routes;new route planning;routing decisions;routing decisions approach;mmr route suggestions;reducing congestion;suggesting routes;modes routing decisions;routes;reduce congestion;routing;reducing congestion overall;reduce congestion overall;near optimal routes;results proposed routing"}, "0af2ff552ab0555914dee90ccfae18297b2792c9": {"ta_keywords": "meetings recorded using;performs meeting diarization;learning using speaker;audio recordings trained;channel audio recordings;meetings recorded;end deep network;audio recordings;trained simulated meetings;simulated meetings recorded;using speaker identification;speaker identification component;channel audio;end diarization based;handling speaker overlap;handling speaker;end diarization models;advantage handling speaker;meetings unknown numbers;audio;deep network;meeting diarization;single channel audio;speaker identification;recordings trained;multitask transfer learning;deep network model;diarization performance including;recordings;diarization performance", "pdf_keywords": "attention network speaker;speaker module jointly;network diarization speakers;diarization speakers meeting;speech diarization;learning using speaker;diarization speakers;speech diarization aim;continuous speech diarization;network speaker classi\ufb01cation;diarization model dinner;speaker identi\ufb01cation component;jointly diarization network;speaker classi\ufb01cation auxiliary;discriminative training;attention network;neural network diarization;diarization network;speaker module;diarize recordings;self attention network;using speaker identi\ufb01cation;speaker classi\ufb01cation;speaker identi\ufb01cation;network speaker;auxiliary speaker module;help diarization performance;diarize recordings prior;promise discriminative training;deep convolutional"}, "12f3bc02d649645fa8734977e28b0ac839e56371": {"ta_keywords": "parking scarcity congestion;allowable congestion queues;queueing network;queueing network analysis;scarcity congestion network;congestion queues;canonical queueing network;kind queueing network;congestion network curbside;congestion network;congestion constraints;congestion queues searching;constraints allowable congestion;queueing network subject;network curbside parking;network maximizing occupancy;price control queues;scarcity congestion;allowable congestion;server convex optimization;optimize performance parking;relationship parking scarcity;parking scarcity;queueing;queues subject constraints;control queues;available server convex;parking shuttle congested;canonical queueing;queues", "pdf_keywords": "congestion management parking;optimize parking policies;parking management queuing;parking tra\ufb03c congestion;optimize parking;use optimize parking;optimizing percentage parking;parking resource utilization;parking policies;queue network maximizing;model downtown parking;parking management;congestion constraints;parking policies subject;maximizing occupancy queues;allowable congestion queues;constraints allowable congestion;optimization problem parking;queuing networks congestion;management parking resource;networks congestion management;level parking management;congestion constraints develop;congestion management;parking resource;parking spaces;networks congestion;networks congestion control;congestion queues;management parking"}, "b7637d1148da569d2211b5dd9851bca82c6aac43": {"ta_keywords": "dependency parsing dynamic;dynamic parser;dynamic parser achieve;based dependency parsing;dependency parsing;parsing dynamic;parsing dynamic parser;superior parsers using;superior parsers;parsers;graph based dependency;comparable superior parsers;parsing;features computing;feature computation exhaustive;parser;parser achieve accuracies;dynamic feature selection;parser achieve;parsers using;sentence model sequential;online sentence model;features computing fewer;feature computation;dynamic feature;set features computing;sentence model;imitation learning;selection features;features added sequentially", "pdf_keywords": "dependency parsing feature;dependency parsers trained;dependency parsing based;based dependency parsing;dependency parsing;approach dependency parsing;dependency parsers;dependency parsing maintain;parsing feature;graph based dependency;parsers trained;resulting dependency parsers;maintain parsing;parsers trained entire;methods dependency parsing;maintain parsing accuracy;parsing maintain parsing;parsing accuracy policy;parsing based;parsing maintain;parsing feature templates;parsing;parsing based idea;question learning;dynamic feature selection;parsing accuracy;parsers;answer question learning;learning policy trained;trained entire graph"}, "5f8d2da91a6c4b9dd079ccb2706c31bda14ef320": {"ta_keywords": "automatic speech;samples automatic speech;dataset speech;approaches automatic speech;audiocaps dataset speech;dataset speech samples;automatic audio captioning;speech recognition create;monaural speech recognition;automatic speech recognition;speech samples;speech recognition;speech samples recorded;audio captioning;endtoend monaural speech;clean speech wall;speech recognition systems;speech recognition aa;mixing clean speech;sounds using speech;speech wall;speech enhancement train;automatic audio;audio captioning aac;speech enhancement;audio samples automatic;using speech enhancement;using speech;audiocaps dataset;background sounds", "pdf_keywords": "audio representations separately;speech transcript audio;transcript audio caption;generating speech transcript;automatic audio captioning;audio captioning;jointly trained deep;audio representations;models automatic speech;audio caption;caption given audio;effective audio representations;automatically generating speech;dataset automatic audio;transcript audio;speech transcript;end speech recognition;speech utterances captured;automatic speech;models speech utterances;audio caption given;speech recognition tasks;models speech;generating speech;audio captioning aac;utterances captured;trained deep convolutional;constituent audio sources;extract effective audio;multi task mixture"}, "298b72096b8a770b0cdb263dd53cf2463b8a1a1d": {"ta_keywords": "embeddings text predict;causal inference text;popularity post causal;deep language models;inference text documents;text predict;text predictive;dimensional embeddings text;post causal inference;causal structure data;inference text;peer relationships online;causal inference;deep language;adapts deep language;embeddings text;text predictive treatment;language models;infer causal structure;address causal inference;networks study empirically;online social networks;social networks;dimensional text representation;low dimensional text;language models learn;post causal;causal inference used;infer causal;impact peer", "pdf_keywords": "document embeddings causal;embeddings causal bert;embedding methods causal;embeddings causal;embeddings text documents;learn embeddings text;document embedding models;causal topic model;word embeddings;word embeddings topic;causal inference text;learn document embedding;embeddings topic models;bert causal topic;text predictive treatment;embeddings text;causal bert causal;suf\ufb01cient document embeddings;document embeddings;text predictive;informally learn embeddings;embedding models predict;learn embeddings;document embedding;causal bert;causally suf\ufb01cient document;speci\ufb01cally word embeddings;topic models learn;embeddings topic;bert causal"}, "ffa07e4d7c8fade2ded5ffeea7265d22d8a0c0ab": {"ta_keywords": "neural network ann;artificial neural;generative neural network;neural network;network generative neural;neural networks designed;neural network generative;neural network paper;magnetic field;artificial neural network;networks designed image;neural networks;equation model neural;convolutional neural network;generative neural;performance artificial neural;model neural networks;proton presence magnetic;ann based stochastic;model neural;convolutional neural;network generative;dipole moments proton;neural;dimensional convolutional neural;presence magnetic field;moments proton;magnetic;moments proton presence;image processing ideal", "pdf_keywords": ""}, "41e49fd3af628f1c8201942a659769f7cc21d812": {"ta_keywords": "graph based semi;semi supervised learning;semi supervised;new semi supervised;based semi supervised;classify unlabeled nodes;complexity graph based;complexity graph;time complexity graph;unlabeled nodes;graph based algorithms;graph graph based;graph based;algorithm based graph;graph;structure graph graph;algorithm mad sketch;structure graph;unlabeled nodes propagating;graph graph;sketch reduce space;information structure graph;complexity node;space complexity node;complexity node logm;classify unlabeled;supervised learning;supervised;nodes;reduce space complexity", "pdf_keywords": "supervised learning ssl;semantic supervised learning;semi supervised learning;semantic supervised;entities graph based;datasets semantic supervised;based semi supervised;propagating labels graph;learning ssl algorithms;natural datasets semantic;semi supervised;entities graph;graph based ssl;labels graph principled;label propagation;based label propagation;graph based semi;labels graph;sketches learn semantic;label propagation work;learn semantic;supervised learning;datasets semantic;label distribution node;computes label distribution;novel graph based;types entities graph;label distribution;propagating labels;classes labeled seed"}, "3bfa808ce20b2736708c3fc0b9443635e3f133a7": {"ta_keywords": "networks gnn;graph neural networks;networks gnn variants;structure networks gnn;networks shown learn;graph neural;gnn based models;networks;oversquashing graph neural;extensively tuned gnn;networks shown;tuned gnn based;structure networks;gnn variants studied;gnn variants;gnn variants used;paper gnn variants;gnn based;weights paper gnn;neural networks;nodes relationships edges;neural networks shown;network science;nodes;tuned gnn;edges;paper gnn;network;extensively context network;study structure networks", "pdf_keywords": "learning large graphs;large graphs;large graphs problems;learning large;representation learning large;learning long range;learning relationships nodes;layer learning relationships;set learning long;networks;bottleneck generalize poorly;learning long;layer learning;information squashed bottleneck;representation learning;approach representation learning;fa layer learning;bottleneck generalize;graphs;optimal message propagation;squashed bottleneck generalize;learning relationships;relations graph edges;graph fundamental task;social networks;learning;graph edges hypothesize;relationships nodes;set learning;message propagation"}, "9ab3622b3a801b90907f3ee399f881764db05d06": {"ta_keywords": "linear attacks internet;detection linear attacks;linear attacks;linear attacks external;constraint attack detection;attack detection;attack detection probability;network attacker;detection probability attacker;nodes network attacker;network attacker allowed;attacks internet;attacks internet false;class linear attacks;attack distributed;attack distributed cyber;algorithms goal attacker;attack remaining nodes;stochastic approximation algorithms;constraint attack;respecting constraint attack;probability attacker;network stochastic;probability attacker assumed;attacker steer estimates;injection attack distributed;stochastic approximation;attacks;network stochastic process;attacks external attacker", "pdf_keywords": "linear cyber attack;prediction cyber physical;attack cyber physical;attack cyber;prediction prediction cyber;attack detection probability;cyber attack;prediction cyber;attack algorithm;attack detection;cyber attack time;attack algorithm karush;injection attack cyber;cyber physical systems;cyber security;cyber cyber security;physical systems cyber;computer security prediction;security prediction;forgetting attack attack;parameters attack algorithm;security prediction prediction;attack detection based;attack propose linear;attack time sensor;attack attack paper;attacks attacker;denial service attacks;novel approach attack;approach attack detection"}, "f2e544c5333125ee30c1c34b08936b6ef87c97dd": {"ta_keywords": "neural network models;speech recognition;neural networks;neural network;spoken language processing;neural networks used;speech recognition present;approach speech recognition;models described spoken;neural network research;spoken language systems;design neural networks;neural network based;implementations spoken language;design neural;quantum using evolutionary;neural;approach design neural;language systems;language processing systems;concepts neural network;types neural network;language processing;spoken language;evolutionary algorithms;new algorithm predicting;described spoken language;implementations spoken;algorithm predicting;using evolutionary algorithms", "pdf_keywords": ""}, "ecfac0d377db229d58bc88698ad3bfd4b384ef37": {"ta_keywords": "argument mining;argument mining based;propose argument mining;argument mining used;automatic argument identification;argument identification helpful;argument identification;conferences annotated arguments;publications driven arguments;annotated arguments;automatic argument;arguments automatic argument;annotated arguments demonstrate;arguments automatic;science conferences annotated;conferences annotated;peer review dataset;driven arguments automatic;scientific publications driven;arguments;arguments demonstrate decision;peer reviewing;scientific publications;new peer review;peer review;propose argument;decision peer reviewing;field scientific publications;peer reviewing central;reviewers peer review", "pdf_keywords": "argument extraction peer;automatic argument extraction;argument extraction;argumentative informative content;argumentative content;informative content argumentative;argumentative informative;content argumentative content;argumentativeness likelihood automatically;content argumentative;extraction peer reviewed;argumentative content expresses;argumentativeness likelihood;argumentativeness;difference argumentative informative;evaluation ability argumentativeness;scienti\ufb01c peer reviews;automatic argument;articles peer reviewing;argumentative;reviewed articles peer;peer reviewed articles;peer reviewing;academic editorial decisionmaking;ability argumentativeness;ability argumentativeness likelihood;peer reviews;peer reviewing process;peer reviewed;editorial decisionmaking"}, "6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424": {"ta_keywords": "environment robots;robots;environment robots operating;human spaces able;robots operating;human spaces;robots operating human;structure environment robots;operating human spaces;language interaction;tasks computer simulations;behavior animals;behavior animals crowded;animals crowded environments;crowded environments;natural language interaction;crowded environments structure;behavioral;language interaction understanding;interaction understanding;task represented boolean;interaction understanding executing;able perform tasks;spaces able engage;behavioral cognitive abilities;interaction;computer simulations;behavior;behavioral cognitive;task represented", "pdf_keywords": "interactive dialogues agents;dialogues agents;interactive dialogues;tasks simulators dialogues;human dialogs;agent task completion;interactive interactive dialogues;dialogues agents able;human human dialogs;human dialogs simulating;agent task;dialogue messages actions;dialogs simulating experience;dialogues class interactive;simulators dialogues;intelligence execution dialog;annotating reasoning tasks;dialogs;dialogue messages;dialogues;robot complete tasks;dialogs simulating;user interacting robot;tfd agent task;fromdialog tfd agent;simulators dialogues class;dialogue;interleaves dialogue messages;interacting robot complete;dialog"}, "51b0609155e3a63afd1dd7dcc3034a5950f90ee0": {"ta_keywords": "proxy features disentangling;features disentangling influence;features disentangling;feature influence direct;disentangled influence audits;influenced proxy features;ascertaining feature influence;feature influence expressed;classifier audited motivated;disentangling influence important;computation feature influence;feature influence;features influence;disentangling influence;data features influence;experiments disentangled influence;affects classifier audited;feature influence feature;disentangled influence;features influence model;proxy features dataset;outcomes feature influence;specifically disentangled representations;feature influence individual;influence feature;classifier audited;field specifically disentangled;identify proxy features;influence feature influence;influence audits detect", "pdf_keywords": "classi\ufb01er audited disentangled;audited disentangled;audited disentangled in\ufb02uence;disentangled in\ufb02uence audits;disentangled representations allow;automatically disentangling in\ufb02uence;auditing direct features;disentangled representation provides;disentangled representations;autoencoders generate disentangled;automatically disentangling;generate disentangled representations;ideas disentangled representations;audit continuous features;data learn adversarially;disentangled representation;disentangled representations shapley;adversarially fair transferable;develop disentangled in\ufb02uence;adversarially;in\ufb02uence audits detect;disentangling;disentangling in\ufb02uence;term disentangled representation;idea disentangling;adversarially fair;disentangling in\ufb02uence using;idea disentangling in\ufb02uence;learn adversarially;develop disentangled"}, "1a27c23453d3f718d854ac4b57dcf3e81ac51aa8": {"ta_keywords": "model active learners;annotation based active;automatic annotation;data active learning;high cost annotation;active learning better;approach automatic annotation;automatic annotation based;active learning;annotation rapid pace;classification sequence tagging;active learner iteratively;active learners;frequently labeled data;annotation based;development labeled datasets;active learner;selecting examples annotation;active learning widely;based active learner;annotation budget;annotation rapid;fixed annotation budget;annotation;cost annotation rapid;frequently labeled;model trained;active learners paper;updated frequently labeled;nlp tasks", "pdf_keywords": ""}, "9c403ca58853fbb223f6e9fce446bb638f291692": {"ta_keywords": "annotations 595 material;high quality annotations;consistent annotations new;annotator agreement studies;mention annotations;annotations mention;consistent annotations;annotations;provide consistent annotations;new annotation approach;annotation;annotations mention types;annotations new;new annotation;annotator agreement;annotation approach intended;inter annotator agreement;annotations new annotation;entity mention annotations;quality annotations mention;maximize consistency annotation;consistency annotation speed;annotation approach;quality annotations;mention annotations 595;annotation speed;corpus entity mention;annotation speed domain;inter annotator;mention types corpus", "pdf_keywords": ""}, "33e5e4b079535957d1275497f8870ea57762a03d": {"ta_keywords": "sentence attackability online;attackable sentences argumentation;attackable sentences arguments;demonstrate sentence attackability;sentence attackability associated;analysis sentence attackability;sentence attackability;attackable sentences;detect attackable sentences;attackability online arguments;attacks argumentation;attacks argumentation identify;existence attackable sentences;attackability online;reasons attacks argumentation;refutation argumentation building;useful information attackability;sentences argumentation necessary;information attackability;successful refutation argumentation;sentences argumentation;sentences arguments significantly;refutation argumentation;argumentation building;argumentation identify;online arguments analyze;argumentation building findings;attackability;sentences arguments;argumentation identify relevant", "pdf_keywords": "predicting attackability sentence;sentence attackability;features sentence attackability;attackability sentence paper;argumentation train;daily argumentation train;argument quality various;argument quality;predicting attackability;argumentation train model;attackability sentence attacked;daily argumentation;sentence attackability sentence;ranking argument schemes;attackability sentence;model predicting attackability;used daily argumentation;sentence attacked;attackability;inference text paper;external knowledge sources;sentence attacked successfully;evaluate argument quality;essays online comments;inference text;argumentation;online comments;causal inference text;sentence paper methods;literature causal inference"}, "81dfa45c568d7c1d9771ba2a1f07dad96558cff6": {"ta_keywords": "isolated phoneme classification;phoneme classification;hidden markov machines;phoneme classification paper;hidden markov models;classification scheme phoneme;based hidden markov;hidden markov;conventional hidden markov;phoneme pairs based;speech recognition;markov machines;sequential pattern classification;based isolated phoneme;phoneme pairs;isolated phoneme;markov machines method;classifier based hidden;pattern classification;machines gaussian mixture;markov models;pattern classification scheme;especially speech recognition;markov models completely;learning classifier;classifier based;learning classifier based;classification paper presents;machine learning classifier;classification", "pdf_keywords": ""}, "be12a8d9ddb12c9ed292430c38d50093191dd442": {"ta_keywords": "algorithm finding communities;algorithm called clustering;called clustering;finding communities;clustering;clustering algorithm called;proposed clustering;clustering algorithm;finding communities community;recently proposed clustering;proposed clustering algorithm;clustering important;called clustering important;nodes community;communities;algorithm synset induction;communities community;nodes community communicate;clustering important tasks;communities community based;algorithm synset;community communicate;steps algorithm synset;synset induction;community;animal locomotion;community based;assumption nodes community;natural language processing;new algorithm finding", "pdf_keywords": ""}, "90db4ddb08df23a4c587e6136e66cb388311473b": {"ta_keywords": "learned users collaborative;generalization performance collaborative;symbolic learning methods;class collaborative learning;learning methods diverse;collaborative learning;improving generalization performance;classifier learned;collaborative learning systems;learned classifier;symbolic learning;training classifier learned;filters learned users;learning methods;different learning methods;class collaborative;generalization performance;training classifier;text categorization;learning methods use;improving generalization;learning methods direct;classifier;classifier learned classifier;users collaborative;categorization;learned users;conclude symbolic learning;filters learned;performance class collaborative", "pdf_keywords": ""}, "d409ff05d70f7b9787baf6431a84a178ad726e8d": {"ta_keywords": "demonstrations environments constraints;demonstrate agent architecture;agent architecture;agents transfer knowledge;agent architecture general;constraint learning;features allows agents;learning soft constraints;use constraint learning;tradeoffs demonstrations environments;scenarios require humans;agents;constraint learning implement;demonstrate agent;demonstrations environments;agent;require humans;soft constraints;allows agents;complex constrained environments;constraints states actions;teams equip artificial;environments constraints explicit;equip artificial intelligence;architecture leverages cognitive;environments constraints;intelligence model humans;constrained environments;artificial intelligence;model humans", "pdf_keywords": "learning constraints demonstrations;decision making agents;agents orchestrator systems;make decision constrained;soft constraints demonstrations;constraint learning;decision constrained;decision constrained environment;learning constraints;constraints demonstrations;decision \ufb01eld theory;constraints demonstrations important;use constraint learning;agents orchestrator;alternative decision \ufb01eld;making agents orchestrator;constraint learning method;model human decision;learning soft constraints;decision \ufb01eld;demonstrations important tasks;orchestrate competing objectives;expressive power decision;human decision making;behavior use constraint;orchestrator systems;decision maker;solution problem learning;soft constraints;capturing initial demonstrations"}, "f16c0699a873b0209a370e8e6301b0189785c614": {"ta_keywords": "active learning;active learning approach;introduce active learning;active learning work;learning approach constraint;students active learning;constraint selection;task verb clustering;based sampling;verb clustering;constraint selection employing;uncertainty based sampling;constraints instances;selecting random;selection employing uncertainty;sampling;approach constraint selection;learning approach;constraints instances present;clustering;selecting random variable;datasets;learning;real world datasets;method selecting random;links constraints instances;teach students active;selection;verb clustering shown;possible incorporate supervision", "pdf_keywords": ""}, "363eb288abf76f7ab52d7789b30399b4b909dd5a": {"ta_keywords": "bribery schemes voting;bribery setting computationally;optimal bribery schemes;finding optimal bribery;optimal bribery;bribery schemes;traditional bribery problem;bribery setting;bribery problem;generalize traditional bribery;bribery;traditional bribery;schemes voting;paper bribery setting;paper bribery;schemes voting domains;vote inter dependencies;set variables voters;agents vote;bribery problem account;variables voters;voting domains;variables voters use;agents vote inter;voting domains candidate;issues agents vote;candidate set cartesian;represent preferences;represent preferences paper;voting", "pdf_keywords": ""}, "0d6a4e45acde6f47d704ed0752f17f7ab52223af": {"ta_keywords": "dataset demonstrations crafting;introduce dataset demonstrations;dataset demonstrations;learn quickly demonstrations;demonstrations crafting based;step human demonstrations;human demonstrations form;instructions unseen tasks;human demonstrations;human demonstrations help;generalize unseen tasks;demonstrations crafting;quickly demonstrations approach;instructions action trajectories;demonstrations form natural;level descriptions actions;quickly demonstrations;descriptions actions model;hierarchical tasks;hierarchical tasks propose;trained agent;actions agent;descriptions actions;actions agent generated;demonstrations approach gives;demonstrations approach;trained agent interpretable;decomposition hierarchical tasks;generated natural language;demonstrations form", "pdf_keywords": "learning architecture task;reinforcement learning architecture;architecture task crafting;task crafting;generate natural language;agents learn;learning architecture;language annotated tasks;instructions train neural;generated instructions train;task crafting equipment;human generated instructions;reinforcement learning;annotated tasks corresponding;method training agent;annotated tasks;present reinforcement learning;agents learn limited;instruction demonstration collecting;tasks corresponding gameplay;crafting equipment simulated;step agents learn;crafting equipment;generate language;able generate language;item sub tasks;training agent;human instruction;human instruction demonstration;train neural"}, "88b66f705a329da8292e7b8aa4bfe26de4759cfa": {"ta_keywords": "character based translation;machine translation possible;based translation model;language based translation;machine translation;translation model propose;translation model;effectively translating unknown;demonstrate machine translation;translation model phrase;systems effectively translating;phrase based transduction;based translation achieve;transformation character strings;effectively translating;train translation model;translation achieve results;based translation;translation possible concept;propose lookahead parsing;translation possible;lookahead parsing algorithm;effective efficient alignment;lookahead parsing;train translation;translating;translating unknown;model phrase based;words language pairs;word based systems", "pdf_keywords": ""}, "a901185ee0710770420044cace33003109d478e3": {"ta_keywords": "rating design;implications optimal rating;optimal rating design;ratings highly inflated;rating choosing;amazon mechanical turk;optimal rating;rating practice;rating practice ratings;ratings;rating design run;rating choosing answer;mechanical turk;mechanical turk confirm;ratings highly;rating;practice ratings highly;design rating choosing;practice ratings;optimize design rating;choosing answer labels;design rating;answer labels;levels rating practice;feedback;turk confirm findings;meaning levels rating;improve quality information;current inflationary norms;inflationary norms", "pdf_keywords": "rating informativeness;rating scales substantially;ratings develop;particular rating informativeness;rating scales based;phrasing rating scale;verbal rating scales;rating systems;rating scale develop;rating scales;optimize rating systems;observed ratings develop;ratings in\ufb02ated;question phrasing rating;ratings highly in\ufb02ated;rating informativeness dropped;rating systems historically;rating scale;ratings in\ufb02ated increased;optimize rating;ratings develop stylized;ratings;quality observed ratings;phrasing rating;particular verbal rating;optimizing quality freelancer;observed ratings;quality freelancer clients;verbal rating;compare optimize rating"}, "ee9f40f1c1e77b0b39b6e4a158208614fb4995c0": {"ta_keywords": "predicting urban anomalies;predict urban anomalies;urban anomaly detection;urban anomalies automatically;driven urban anomaly;urban anomalies traffic;detecting predicting urban;urban anomalies;predicting anomalies happening;anomaly detection prediction;urban anomaly;urban anomalies presented;detect predict urban;presented urban anomalies;predicting anomalies;anomalies presented urban;predicting urban;types urban anomalies;urban anomalies result;anomaly detection;anomalies traffic anomaly;anomalies traffic;predict urban;automatically alerting anomalies;traffic anomaly;alerting anomalies;detection prediction;data driven urban;detection prediction frameworks;anomalies automatically", "pdf_keywords": "urban anomaly analytics;urban anomaly detection;urban data anomaly;urban anomaly;research urban anomaly;driven urban anomaly;mobility urban anomaly;urban data representation;methods urban anomaly;features urban data;urban data analysis;challenges urban data;data driven urban;urban data;urban big data;anomaly detection prediction;mechanism urban data;predicting anomalies happening;aspects urban data;urban data produced;spatiotemporal features urban;anomaly analytics;analytics mechanism urban;predicting anomalies;anomaly detection important;data anomaly types;anomaly detection;government detecting anomalies;human mobility urban;data anomaly"}, "5bcbc4554a68b38ff4a22b848fb0817b809608b2": {"ta_keywords": "development new processor;new processor processor;new processor;processor products;processor;processor able process;processor processor;processor processor processor;processor processor able;large number processor;processor able;number processor products;number processor;able process large;process large;process;development new;process large number;report development new;able process;products;development;report development;new;report;large number;large;number;able", "pdf_keywords": ""}, "c159725940750adbad262ac946ce161bb68e41b5": {"ta_keywords": "connectionist temporal classification;temporal classification convolution;training connectionist temporal;automatic speech;speech recognition asr;recurrent neural network;end automatic speech;temporal classification;rnn based models;network rnn;speech recognition;automatic speech recognition;neural network rnn;network rnn based;recurrent neural;recognition asr;recognition asr sequence;rnn based;rnn;compared recurrent neural;connectionist temporal;behavior bouncer transtransformation;training connectionist;joint training connectionist;bouncer transtransformation;bouncer transtransformation self;classification convolution;sequence models gained;end end transition;sequence sequence models", "pdf_keywords": "translation automatic speech;automatic speech;convolution architectures machine;attention convolutional layer;recognition automatic speech;speech recognition sequenceto;speech recognition automatic;architectures machine translation;dynamic convolution architectures;automatic speech recognition;speech recognition;neural machine translation;convolution architectures;machine translation automatic;recognition speech recognition;recognition speech;self attention convolutional;machine translation;attention convolutional;lightweight dynamic convolution;speech recognition speech;attention simple model;translation automatic;differences speech recognition;propose convolutional layer;convolutional layer proposed;convolution instead self;convolution based recognition;convolution ef\ufb01cient alternative;convolutional layer"}, "ec6499842d3e51b7dda94f5d0620d6df5c1a1b6d": {"ta_keywords": "trained speech text;text speech;speech text;representations regenerate speech;systems directly speech;trained speech;speech meaning representations;text speech subsystems;text text speech;speech text text;speech translated text;robust way speech;utterance unwritten language;regenerate speech translated;speech technology;way speech technology;speech subsystems;pre trained speech;robot;regenerate speech;speechechanical systems;speechechanical systems shown;unwritten language building;learn speech meaning;robot unwrapped;speech bypassing;speech bypassing need;speech technology plays;learn speech;representations using text", "pdf_keywords": ""}, "55faed1fbb1575ffa2609bdc4490586e30df441a": {"ta_keywords": "automatic translation;automatic translation evaluation;manual machine translation;translation evaluation metrics;machine translation;manual automatic translation;team machine translation;machine translation paper;machine translation mt;translation evaluation;translation paper study;translation mt tool;metrics clqa accuracy;interior knowledge bases;knowledge bases;translation paper;build knowledge bases;evaluation metrics clqa;translation;words performance;metrics clqa;knowledge bases based;information source language;source language;topology earth;translation mt;evaluation metrics;knowledge bases limited;frequency words performance;words performance team", "pdf_keywords": ""}, "e52115834ac7a529b1f4a7769dd538f143cf3eea": {"ta_keywords": "general erasure codes;erasure codes impossibility;erasure codes;erasure code distributed;new erasure code;erasure code;schemes piggybacking codes;piggybacking codes;piggybacking codes low;piggy backing codes;repair schemes piggybacking;backing codes developed;code distributed storage;characterization repair schemes;backing codes;piggybacking general erasure;schemes piggybacking;model piggybacking refrigerator;codes impossibility;repair schemes;piggybacking refrigerator;codes impossibility results;hartree fock approximation;theoretical study xmath0;new erasure;erasure;distributed storage;distributed storage present;storage present simple;general characterization repair", "pdf_keywords": "code repair schemes;linear repair schemes;repair schemes linebacking;piggybacking code repair;repair schemes;repair schemes adapting;schemes linebacking codes;code improve repair;repair scheme node;characterization repair schemes;piggybacking code substripes;finding repair scheme;code repair;repair scheme;characterization piggybacking code;nodes able repair;bandwidth piggybacking code;nodes repair;linear repair;repair schemes leverage;using linear repair;regimes nodes repair;linebacking codes paper;linebacking codes;repair schemes section;piggybacking codes scalar;perfect bandwidth piggybacking;construction repair schemes;piggybacking codes;schemes linebacking"}, "777d7b4141c9ce163de99b747e94c8d1db12e11e": {"ta_keywords": "prediction improve service;receive machine learning;machine learning services;accurate prediction provider;learning services cloud;fairness classifier improved;fairness classifier;prediction provider;prediction provider use;based perturbation maximization;service time privacy;services cloud;accurate prediction improve;classifier;perturbation maximization;accurate prediction;return accurate prediction;classifier improved;privacy consumer better;machine learning;prediction improve;privacy;gradient based perturbation;services cloud based;gradient based;hoc fairness classifier;making prediction;cloud based service;privacy consumer;offer accurate prediction", "pdf_keywords": ""}, "89e53f116ef732d0abe81ee2218fa862ddc5ddce": {"ta_keywords": "speech processing toolkit;translation text speech;recognition machine translation;automatic speech;training decoding pipelines;machine translation text;machine translation;functions speech translation;translation systems;implements automatic speech;speech translation;extraction training decoding;text speech;end speech processing;training decoding;rapid development translation;translation systems single;text speech functions;pre trained models;speech processing;development translation systems;wide range benchmark;speech recognition machine;speech translation provide;speech recognition;automatic speech recognition;range benchmark datasets;benchmark datasets letter;trained models;benchmark datasets", "pdf_keywords": "automatic speech translation;machine translation systems;translation machine translation;machine translation machine;asr machine translation;translation machine;machine translation;effective machine translation;speech encoder translation;machine translation supports;translation decoder automatic;speech translation;decoder automatic speech;evaluation machine translation;machine translation composed;speech translation egs;translation systems;tasks machine translation;translation supports asr;propose machine translation;machine translation shown;research machine translation;speech recognition asr;translation decoder;machine translation paper;automatic speech;systems automatic speech;translation composed speech;recognition automatic speech;translation egs systems"}, "6d654bab72d062d91f731331f16ea01d7cac0812": {"ta_keywords": "tropes present online;use tropes narrative;tropes narrative elements;biases use tropes;tropes narrative;tropes associated;use tropes;tropes use;tropes present;gender bias;correlates types tropes;tropes;investigate gender bias;tropes use popular;analyze gender user;gender user based;gender bias large;user gender paper;gender work victim;user gender;gender user;types tropes;types tropes use;archetypal characters plot;analysis gender;30k tropes associated;analyze gender;narrative elements archetypal;victim correlates types;based analysis gender", "pdf_keywords": "genderedness score trope;implicitly gendered tropes;highly gendered tropes;gendered tropes;gendered tropes tropes;analyzing bias tropes;gender bias media;genre genderedness;bias tropes;lexicon capturing genderedness;effects genre genderedness;bias tropes popular;genre genderedness kinds;gendered tropes de\ufb01ned;tropes de\ufb01ned gender;capturing genderedness annotating;explore gender bias;genderedness annotating;gender bias;trope occurrences male;capturing genderedness;tropes tropes contribute;genderedness annotating 150;gender bias identify;analysis gender bias;genderedness kinds topics;genderedness scores explore;tropes tropes;tropes contribute;ratings types tropes"}, "e79d1206292bc5e67ba19737d87d4b2ea4a37105": {"ta_keywords": "subword tokenization algorithms;subword representations;subword tokenization;based subword tokenization;subword tokenization module;subword representations characters;rigid subword tokenization;latent subword representations;subword token;use subword token;gradient based subword;subword based models;learns latent subword;par outperforming subword;subword token used;outperforming subword based;subword level transformers;outperforming subword;subword based;tokenization algorithms;tokenization;models natural language;subword level;characters data driven;byte subword level;natural language processing;latent subword;use subword;token free models;tokenization algorithms limit", "pdf_keywords": "multilingual word representations;multilingual models tokens;subword based tokenization;trained language models;machine translation model;multilingual models;novel machine translation;model multilingual word;2017 paraphrase detection;paraphrase detection 2018;word representations;paraphrase detection;language models;end deep learning;machine translation;translation model multilingual;language inference 2017;word representations evaluate;languages split subwords;language models generally;model multilingual;language tasks;subword level features;natural language inference;2016 natural language;language inference;crosslingual transfer model;language data;standard language tasks;tokenization"}, "4cdd533963d8fb21fbf4bb3487bf6a6d60e14e93": {"ta_keywords": "cell image recognition;segmentation method cell;cell images;cell image;cell images using;cell images robustness;cell paper segmentation;images using markov;accuracy cell images;recognition accuracy cell;segmentation;segmentation process;method cell images;segmentation results nucleus;segmentation method;paper cell image;implement segmentation;segmentation results;segmentation results compared;segmentation process segmentation;implement segmentation process;image recognition;improve segmentation;markov random field;image recognition used;effectively improve segmentation;improve segmentation results;paper segmentation;process segmentation;paper segmentation method", "pdf_keywords": ""}, "396e942542904dd32d0d70daa39613e5a27cc059": {"ta_keywords": "stacked graphical learning;learning stacked learning;stacked learning;learning stacked;collective classification efficient;online learning stacked;stacked learning reduce;graphical learning expensive;online stacked graphical;collective classification;propose collective classification;collective classification method;proposed collective classification;learning proposed collective;graphical learning proposed;graphical learning;stacked graphical;cost stacked graphical;graphical learning gives;classification efficient;experimentally online stacked;online stacked;large streaming datasets;classification efficient inference;graphs related instances;maintaining large graphs;instances predicting class;standard stacked graphical;predicting class instance;large graphs", "pdf_keywords": ""}, "d7851e80f6072991bc99e2157f05515564f894f4": {"ta_keywords": "online discriminative training;conversion structured learning;classification applied conversion;online discriminative;discriminative training;training g2p conversion;discriminative training method;phoneme g2p conversion;g2p conversion structured;method training g2p;estimate pronunciations vocabulary;adaptive regularization weight;multiclass classification;regularization weight vectors;grapheme phoneme g2p;g2p conversion based;problem adaptive regularization;web grapheme phoneme;adaptive regularization;approach g2p conversion;training g2p;method multiclass classification;structured learning;mira online discriminative;g2p conversion;multiclass classification applied;regularization weight;pronunciations vocabulary oov;learning based margin;method g2p conversion", "pdf_keywords": ""}, "254491f0d981fb5d796c374287d439d8d1967088": {"ta_keywords": "cardiometabolic risk children;cardiometabolic risk;risk cardiometabolic risk;risk cardiometabolic;content risk cardiometabolic;dv content risk;risk children adolescents;effect dv;investigate effect dv;cardiometabolic;effect dv content;dv;risk children;dv content;children adolescents;clinical trials;adolescents;randomized clinical trials;meta analysis randomized;clinical trials performed;analysis randomized clinical;meta analysis;trials performed investigate;trials performed;content risk;randomized clinical;risk;trials;performed investigate effect;investigate effect", "pdf_keywords": ""}, "e68762a32ec91587d9761030fc75a8f5ee71c45b": {"ta_keywords": "unsupervised topic mixture;topic mixture modeling;unsupervised adaptation speech;topic mixture language;latent topic inference;recognition unsupervised topic;speech recognition unsupervised;topic mixture;adaptation speech recognition;mixture language model;mixture modeling unsupervised;topic inference;modeling unsupervised adaptation;extension topic mixture;adaptation speech;speech recognizer;topic inference based;language model adaptation;unsupervised adaptation;speech recognizer proposed;given speech recognizer;unsupervised topic;speech recognition;latent topic;approach latent topic;mixture language;modeling unsupervised;input given speech;mixture modeling;recognition unsupervised", "pdf_keywords": ""}, "e66ade4e28d9f401277194ed8feea5c6e9f18253": {"ta_keywords": "clusters terrorist groups;detecting clusters terrorist;terrorist groups sharing;clusters terrorist;terrorist groups;attacks 1997 2018;attack rate year;groups sharing similar;attacks 1997;deployed tactics attacked;clustering groups;similarity patterns groups;clustering groups particularly;stability clustering groups;clustering;tactics attacked targets;tactics attacked;50 attacks 1997;attacked targets;attacked targets utilized;stability clustering;attacks;similarity patterns;year stability clustering;groups sharing;groups yearly;contributed total attack;organizations plotted;number organizations plotted;groups yearly repertoire", "pdf_keywords": "dynamics terrorist organizations;patterns terrorist organizations;operational patterns terrorist;organizational patterns terrorist;terrorist organizations year;global terrorism database;analysis dynamics terrorist;terrorist conference robustness;terrorism database;organizations terrorist organizations;different organizations terrorist;terrorist organizations;terrorist organizations based;terrorist organizations globe;database terrorist events;patterns terrorist;organizations terrorist;dynamics terrorist;terrorist events available;international terrorist conference;study terrorism paper;terrorist events;terrorist organizations likely;terrorism paper present;global terrorism;terrorism database gtdin;terrorist conference;terrorism paper;terrorism;study terrorism"}, "82c4be27b0803c08c56bba4352669c1230a3ea19": {"ta_keywords": "regenerating codes efficient;minimum repair bandwidth;storage capacity codes;regenerating codes framework;regenerating codes;storage networks;repair bandwidth provide;capacity codes;storage nodes;storage storage networks;repair scheme;total repair bandwidth;original regenerating codes;explicit coding schemes;repair scheme enables;methods distributed storage;repair bandwidth case;storage networks node;storage nodes network;repair bandwidth;setup regenerating codes;bound storage;coding schemes;codes efficient methods;minimum repair;distributed storage;schemes exact repair;novel repair scheme;upper bound storage;data reconstruction repair", "pdf_keywords": "regenerating codes storage;distributed storage;storage distributed data;distributed storage systems;distributed storage network;distributed storage distributed;storage distributed;present distributed storage;storage nodes wireless;codes storage;storage network design;codes storage dss;storage network;storage systems integrity;storage nodes;storage dss proposed;repair scheme storage;al distributed storage;repair bandwidth provide;storage capacity codes;storage systems;network based regenerating;storage dss;distributed data transmission;repair bandwidth;based regenerating codes;stored network;distributed data;scheme storage nodes;bound storage"}, "4c42d6412c080fef23ad95b4469efe9cf321ae5d": {"ta_keywords": "recognition speech;speech recognition speech;speech recognition;automatic speech recognition;recognition speech recognition;dynamics component bose;semi supervised training;unsupervised semi supervised;automatic speech;speech text data;semi supervised;approach automatic speech;data quantity speech;atoms lattice;speech text;optical lattice;improvements semi supervised;atoms lattice laser;quantity speech text;number atoms lattice;confined optical lattice;supervised training models;recognition;supervised;bose gas confined;bose einstein;component bose gas;supervised training;bose einstein plot;unsupervised semi", "pdf_keywords": "speech recognition asm;speech text pipeline;rnn language model;speech text data;integrating unpaired speech;unpaired speech text;machine translation;corpora automatic speech;rnn language;automatic speech;data quantity speech;learn unpaired speech;integration rnn language;translation based sequence;speech text;semi supervised loss;speech recognition;sequence seq2seq models;text pipeline loss;asr text pipeline;speech text modalities;quantity speech text;automatic speech recognition;machine translation based;integrates unpaired speech;new machine learning;recognition asm models;unpaired speech;recognition asm;supervised loss"}, "a714ca5254fb3cd7b06ead36d026c4eb154a7134": {"ta_keywords": "fairness algorithmic decision;discrimination study fairness;fairness algorithmic;study fairness algorithmic;study fairness;induce class discrimination;discrimination;class discrimination;unfairness simultaneously;discrimination study;denote disparate learning;race intentionally discriminate;disparate learning;unfairness simultaneously introducing;class discrimination study;intentionally discriminate;unfairness;disparate learning processes;fairness;discriminate;discriminate practice contested;eliminate forms unfairness;intentionally discriminate practice;algorithms exhibit disparate;forms unfairness;forms unfairness simultaneously;equivalent disparate treatment;algorithms denote disparate;disparate treatment;apply disparate", "pdf_keywords": "ml based discrimination;discrimination aware machine;discrimination disparate treatment;discrimination disparate;discrimination aware;approach discrimination aware;approach discrimination;discrimination including decisions;intentional discrimination proxy;fairness ml;novel approach discrimination;based discrimination;discrimination;discrimination including;discrimination attracted policy;intentional discrimination including;mitigating machine learning;discrimination proxy;problem fairness ml;intentional discrimination;fairness ml paper;based discrimination attracted;notions discrimination disparate;discrimination proxy variables;addresses intentional discrimination;characteristics intentional discrimination;discrimination attracted;notions discrimination;problem fairness;ml disparate impact"}, "69320030be096e78380a097810554b648e7409c0": {"ta_keywords": "speaker clustering;speaker clustering important;problem speaker clustering;clustering algorithm;algorithm based bayesian;propose clustering algorithm;paper propose clustering;clustering;clustering algorithm based;propose clustering;based bayesian information;bayesian information theory;clustering important problems;clustering important;bayesian information;based bayesian;problem speaker;bayesian;speaker;solve problem speaker;statistical physics;information theory;problems statistical physics;information theory solve;tuning parameters paper;tuning parameters;algorithm based;tuning;changes tuning parameters;statistical", "pdf_keywords": ""}, "0639cbb07ec3e03de7c8c1d828a90049c92cf5df": {"ta_keywords": "structure perovskite crystals;perovskite crystals;electronic structure perovskite;perovskites zero phonon;structure perovskite;perovskite crystals mulliken;bonding stannate perovskites;perovskites zero;stannate perovskites zero;motion motor perovskite;motor perovskite;perovskites;motor perovskite affected;stannate perovskites;perovskite;perovskite affected presence;perovskite affected;bond angle crystal;electronic structure;crystal field strength;octahedral mn bond;energy zero phonon;crystals;angle crystal field;phonon line octahedral;angle crystal;crystal field;study electronic structure;zero phonon;chloride iodide calcium", "pdf_keywords": ""}, "62924cef027a66a75b5465ebb7a926c06f95790f": {"ta_keywords": "domain adversarial algorithms;adversarial approach aligning;domain adversarial;domain adversarial approach;adversarial algorithms domain;propose domain adversarial;standard domain adversarial;domain adaptation;domain adaptation impossible;adversarial algorithms;algorithms domain adaptation;adversarial approach;adversarial;shifting label distributions;domain adaptation addresses;assumptions domain adaptation;synthetic real datasets;source training distribution;relaxed distribution alignment;label distributions propose;distribution alignment new;source target encodings;distribution alignment;label distributions;drifts source training;training distribution;source training;adaptation addresses;aligning source target;target distribution generating", "pdf_keywords": "domain adaptation adversarial;domain adversarial;domain adversarial learning;distributions domain adaptation;adaptation adversarial training;domainadversarial;adversarial training;domainadversarial methods;adaptation adversarial;domain adaptation;adversarial learning promising;icks domain adversarial;domainadversarial methods model;behavior domainadversarial;domain adaptation formalizes;adversarial learning;behavior domainadversarial methods;adversarial;problem domain adaptation;adversarial training paper;explains behavior domainadversarial;labeled source domain;domain based prediction;supervised deep;domain present supervised;synthetic real datasets;data distributions domain;distributions domain;domain data unlabeled;source domain target"}, "9b5cf607f9cd3eb5ef47d3597bb9360ea6034264": {"ta_keywords": "peer review biases;challenges peer review;peer review;peer review young;peer review biased;scientific disciplines peer;challenges peer;introduction peer review;disciplines peer review;peer;disciplines peer;introduction peer;young researchers;review biases;review young researchers;open problems;scientific;young researchers tutorial;review biased;experiments understand issue;solved scientific disciplines;review biases subjectivity;researchers;systemic challenges peer;contained introduction peer;insightful experiments;computer science paper;insightful experiments understand;review young;solved scientific", "pdf_keywords": ""}, "9a7a4f125d8016e0fad9f6f5e9e0bca4e38b0784": {"ta_keywords": "scalable probabilistic logic;probabilistic logic programming;stochastic logic programs;database probabilistic logic;probabilistic logic;efficient probabilistic reasoning;probabilistic logic called;supports efficient probabilistic;stochastic logic;extends stochastic logic;learning inference graphs;probabilistic reasoning;prolog programming develop;topic prolog programming;prolog programming;logic programming;logic programs;efficient probabilistic;inference graphs;efficient learning inference;database probabilistic;probabilistic reasoning using;new scalable probabilistic;scalable probabilistic;popular topic prolog;learning inference;logic programs slp;logic programming pdp;topic prolog;develop semantically rich", "pdf_keywords": ""}, "684e712f59f11d2bdc98be4c210824ab9e6f11f4": {"ta_keywords": "embeddings graphs trained;classification deep transfer;transfer learning;deep transfer learning;deep transfer;proposed transfer learning;transfer learning mainly;transferred different embeddings;vectors task transferable;transfer learning framework;embeddings language pretrained;embeddings graphs;tasks word embeddings;graphs trained;features learned graphs;classification deep;learned graphs;different embeddings graphs;graphs trained approaches;embeddings;language pretrained convolutional;learned graphs generic;word embeddings;natural language inference;transferable tasks word;convolutional features learned;image classification deep;embeddings language;latent relational graphs;different embeddings", "pdf_keywords": ""}, "22655979df781d222eaf812b0d325fa9adf11594": {"ta_keywords": "question answering qa;question answering;answer question answering;hotpot qa challenging;answering qa;answer answering;answering qa complex;answer answering qais;qa challenging latest;hotpot qa;qa challenging;reasoning allowing qa;documents answer questions;answered answer answering;answer questions;questions test qa;level supporting facts;answer questions diverse;based question answer;wikipedia based question;factoid comparison questions;introduce hotpotq;qa systems;answering;allowing qa systems;113k wikipedia based;answering qais;factoid comparison;tournaments answer question;question answer pairs", "pdf_keywords": "based question answering;question answering;question answering order;reading comprehension dataset;machine reading comprehension;entities based crowdsourcing;reasoning multiple documents;comprehension dataset;comprehension dataset covers;existing knowledge bases;knowledge base;natural language;existing knowledge base;knowledge bases;questions directed comparing;reasoning documents present;reasoning documents;diversify questions answers;text based question;natural language constraining;questions answers;knowledge base knowledge;crowdsourcing crowd workers;collect text based;crowdsourcing crowd;answering order diversify;comparing entities based;knowledge bases kb;crowdsourcing;comparing entities"}, "0f726fcd676baff957574b223b99fd84163ebe6e": {"ta_keywords": "stacked graphical learning;stacked graphical models;learning based stacked;stacked graphical model;graphical learning;relational datasets hyperlinked;based stacked graphical;relational template stacked;graphical learning recent;stacked graphical;algorithm stacked graphical;relational datasets;base learning;reality relational datasets;propose algorithm stacked;graphical models;traditional machine learning;base learning based;citations social networks;approach base learning;algorithm stacked;template stacked graphical;relational data;datasets hyperlinked web;machine learning;predictions related instances;learning methods;machine learning methods;relational data propose;graphical models demonstrated", "pdf_keywords": ""}, "4dfa9de9b3b2b222ddbdda934975bf608b8e1fda": {"ta_keywords": "dialogue robots;dialogue robots present;oriented dialogue robots;task oriented dialogue;dialogues annotated;dialogue systems;predict constructiveness conversation;utterances dialogue systems;dialogue systems research;utterances dialogue;focused dialogues interlocutors;collaborative conversations;50 dialogues annotated;collaborative conversations solving;dialogues interlocutors;dialogues interlocutors largely;oriented dialogue;dialogues;dialogue;conversations solving;conversation;containing collaborative conversations;focused dialogues;14k utterances dialogue;conversations;conversations solving cognitive;group dialogues;constructiveness conversation paper;group conversations;dialogues 14k utterances", "pdf_keywords": "deliberation crowdsourced conversations;crowdsourced conversations;crowdsourced conversations dataset;discussions groups collaborate;deliberation develop dialogue;dynamics deliberation crowdsourced;discussions groups;collaborative conversations;predict constructiveness conversation;deliberation crowdsourced;conversations dataset analysed;group dialogues;conversations dataset;debating groups project;containing collaborative conversations;conversation multiparty conversations;debating groups;collaborative conversations solving;group deliberation develop;develop dialogue agents;dataset contains discussions;constructiveness conversation multiparty;conversation multiparty;groups project meetings;multiparty conversations;multiparty conversations focus;develop dialogue;group deliberation;dialogue agents;conversations solving"}, "bdf6ad58338279634d647447751442db8a6e2f77": {"ta_keywords": "trivial neural network;nanotube cnt;carbon nanotube;walled carbon nanotube;nanotube;nanotube cnt presence;carbon nanotube cnt;trivial neural;surfaces neural networks;deep learning powerful;error surfaces neural;scaling behavior weights;neural;surfaces neural;scaling loss;scaling loss distance;neural networks propose;neural network error;neural network;deep learning;simple scaling law;neural networks;networks;weight space scaling;loss distance critical;scaling behavior;dynamics single walled;space simple scaling;simple scaling;exploring structure dynamics", "pdf_keywords": ""}, "88051a6dce3b67541d8096647da2f6d31daa9e9a": {"ta_keywords": "knowledge graph relations;relations context model;text relations;language models;incorporates knowledge graph;knowledge graph information;probability entity spans;language modeling;text relations propose;knowledge graph;words document entities;based language models;improves language modeling;entity spans;given text relations;entity spans given;language models previous;document entities;document entities occur;entities occur knowledge;occur knowledge graph;language models parameterizes;relations context;posterior probability entity;graph relations experiments;class language models;annotate posterior probability;language modeling performance;predict appropriate relations;context model", "pdf_keywords": "language modeling conditioned;conditional language modeling;deep neural language;neural language model;conditioned structured knowledge;learning compose words;words latent structure;language models;language modeling;compose words sentences;words document entities;neural language;modeling conditioned structured;language model;language model task;words sentences instance;structured knowledge;automatically learning compose;knowledge graph relations;structured knowledge paper;knowledge graph;conditioned structured;compose words;deep neural;conditional language;automatically learning;sentences instance sequence;predicting latent sequences;document entities occur;capable predicting latent"}, "bc33c151a375d30d85a99d4e269185bad360b7bf": {"ta_keywords": "measure efficiency device;efficiency device;measure efficiency;method measure efficiency;efficiency device order;overall cost device;cost device;efficiency;device order reduce;effective method measure;reduce overall cost;device;device order;simple effective method;method measure;effective method;simple effective;cost;overall cost;reduce overall;effective;measure;present simple effective;reduce;order reduce overall;method;order reduce;simple;overall;paper", "pdf_keywords": ""}, "72ae4bba9aaa30dfba45f6e7e076952a76e2d751": {"ta_keywords": "conversational model;conversational model function;roles conversational model;traditional lstm model;participant roles conversational;lstm;traditional lstm;lstm model;long term memory;memory generated responses;roles conversational;role party conversations;conversations;lstm model measured;language generation model;measured language model;outperforms traditional lstm;ubuntudialog corpus model;party conversations;conversational;term memory;term memory lmi;term memory generated;model language generation;language model;language generation;model measured language;experiments ubuntudialog corpus;language model perplexity;ubuntudialog corpus", "pdf_keywords": "conversation modeling recurrent;response generation conversations;conversational structure response;conversation modeling;driven conversation modeling;conversational structure;data driven conversation;meaning conversational structure;response generation;generation conversations experiments;response generation emerging;conversation;conversations;driven conversation;generation conversations;contextual information spoken;modeling recurrent neural;conversational;modeling recurrent;short term memory;recurrent neural network;conversations experiments;memory language model;model response generation;meaning conversational;recurrent neural;perplexity response ranking;spoken language understanding;language model;long term contextual"}, "9b52f250376e07c2caddb5f43b8db8b2f300bb51": {"ta_keywords": "analysis xmath0 data;fermilab tevatron;collected fermilab tevatron;fermilab tevatron summer;xmath0 data sample;sample collected fermilab;xmath0 data;results analysis xmath0;analysis xmath0;collected fermilab;fermilab;xmath0;tevatron summer 2011;tevatron;tevatron summer;present results analysis;present results;data sample;data sample collected;data;results analysis;sample;sample collected;summer 2011;results;2011;analysis;present;collected;summer", "pdf_keywords": ""}, "fd8b33299ce6ca81ce54e7d2de555a1a96ca96f1": {"ta_keywords": "model automatic speech;speech recognition asr;speech recognition;automatic speech;automatic speech recognition;model hmm;discriminative models automatic;gram language models;structured discriminative models;classify structured sequence;discriminative models;recognition asr;hidden gram language;language models;structured discriminative;hmm hidden gram;classify structured;markov model hmm;recognition asr systems;generative classifier;lippmann schwinger nls;label sequences sentences;models automatic repeat;propose generative classifier;discriminative approaches;hidden markov model;structured sequence data;systems classify structured;processing nlp;model hmm hidden", "pdf_keywords": ""}, "457e1c9476f08fa2c253982e3effcb364487073e": {"ta_keywords": "evaluation spring spring;performance evaluation spring;model spring season;evaluation spring;spring season;spring season 2013;spring spring model;spring model spring;spring model;spring spring;model spring;spring;performance evaluation;season 2013 season;season;results performance evaluation;2013 season;present results performance;season 2013;results performance;performance;evaluation;present results;model;results;paper present results;2013;present;paper present;paper", "pdf_keywords": ""}, "b80ce55fbb4aa427439009985c0ce28a34324dc6": {"ta_keywords": "nutritional supplements women;intake nutritional supplements;supplements women attending;analysis intake nutritional;supplements women;intake nutritional;nutritional supplements;statistical analysis intake;attending anti inflammatory;supplements;anti inflammatory medication;analysis intake;inflammatory medication;anti inflammatory;nutritional;inflammatory;women attending anti;women attending;present statistical analysis;statistical analysis;medication;statistical;intake;present statistical;analysis;women;paper present statistical;attending anti;attending;anti", "pdf_keywords": ""}, "e23c5dafc718f9e55ccf7729ce2d2834b650540a": {"ta_keywords": "speaker clustering method;effective speaker clustering;speaker clustering systems;speaker clustering;novel speaker clustering;accuracy speaker clustering;utterance oriented dirichlet;dirichlet process mixture;clustering method;utterances varied speaker;clustering systems;clustering method based;clustering systems compared;clustering;hierarchically structured utterance;speaker speaker;efficient effective speaker;effective speaker;improves accuracy speaker;varied speaker;process mixture model;mixture model;varied speaker speaker;accuracy speaker;dirichlet process;speaker;structured utterance oriented;number utterances varied;structured utterance;number utterances", "pdf_keywords": ""}, "773e752ab6dc04b43aaf984bcbdd4895c9ab8c2f": {"ta_keywords": "classifier speech recognition;linear classifiers;distributed perceptron based;classifier speech;speech recognition;classifier large scale;scale linear classifiers;propose classifier speech;linear classifiers paper;propose distributed perceptron;paper describes discriminative;based classifier large;discriminative approach advances;perceptron based classifier;classifier large;distributed perceptron;speech recognition based;finite state transducer;discriminative approach;classifiers;describes discriminative approach;discriminative;state transducer;large scale linear;st based decoding;classifiers paper;state transducer wf;based decoding;classifiers paper describes;recognition based structured", "pdf_keywords": ""}, "510aef8370d82c4c4ec50de0f645f34f11e549a7": {"ta_keywords": "protein entity recognition;recall protein recognition;semicrfs dictionary hmms;entity recognition;protein recognition;protein recognition using;features dictionary hmms;level protein recognition;recognition based dictionary;entity recognition based;recognizes phrases dictionary;dictionary information features;recognition using dictionary;semicrfs dictionary;dictionary hmms technique;protein entity;task semicrfs dictionary;recognizes phrases;approach protein entity;dictionary information;dictionary semiconcavity random;dictionary hmms paper;information features dictionary;recall protein;features dictionary;hmms technique dictionary;dictionary hmms;high recall protein;hmm recognizes phrases;use dictionary information", "pdf_keywords": ""}, "7ddddea393c2cd70fe716e2dfc5d77daf58449c0": {"ta_keywords": "influencers social networks;dynamics social network;narratives super influencers;misinformation narratives observed;influence influencers social;social network paper;identify misinformation narratives;misinformation narratives super;analysis misinformation narratives;misinformation narratives;influencers spread;influence influencers;social network;observation misinformation narratives;influencers social;misinformation narratives produced;influencers;social networks;super influencers spread;influencers spread large;social networks used;influence person topology;online social networks;influencing influence influencers;narratives observed web;networks using granger;super influencers;identify misinformation;based social networks;influence vary topics", "pdf_keywords": "tweets impact misinformation;social media in\ufb02uence;identified retweeted tweets;cyber mediated information;classify tweets alters;emerging cyber mediated;classify tweets;ego networks social;classi\ufb01er classify tweets;tweets impact;misinformation narratives super;cyber mediated;tweets;social media;tweets alters;platforms identify misinformation;identify misinformation narratives;observed misinformation researchers;behavior cyber mediated;influence online social;community observed misinformation;online social media;networks social;impact misinformation narratives;spread online mis;networks social cybersecurity;investigate influence online;tweets alters topics;human behavior cyber;behavior cyber"}, "1890775da6ba2627a5d6c17a639e2dca7cdc388d": {"ta_keywords": "speech recognition mqa;mqa emerging technology;multi microphone speech;recognition mqa emerging;multi microphone;microphone speech recognition;recognition mqa;microphone speech;mqa emerging;microphone;mqa;speech recognition;speech;recognition;emerging technology;technology widely used;emerging technology widely;multi;technology;widely used;technology widely;widely used areas;used areas science;engineering;areas science engineering;science engineering;areas science;used areas;widely;areas", "pdf_keywords": ""}, "ccad27088b9098de4eaca8dc449b18766db4b3ab": {"ta_keywords": "style transfer inherently;unsupervised style transfer;style transfer;style transfer systems;style transfer style;style transfer aims;transfer style;novel style transfer;transfer style transfer;modern style transfer;style transfer game;style transfer leverages;world style transfer;designed style transfer;human automatic evaluations;art style transfer;style transfer papers;modifying style;automatic evaluations;evaluations modern style;transfer changes semantic;unsupervised style;modifying style given;semantic properties sentiment;automatic evaluations modern;attribute transfer;style given sentence;style given;changes semantic properties;learning techniques capture", "pdf_keywords": "learns generate paraphrased;generate paraphrased sentences;controlled paraphrase generation;paraphrase model style;paraphrase generation;learn paraphrase similarity;diverse paraphrasing;paraphrase generation task;generate paraphrased;paraphrase similarity model;transfer controlled paraphrase;train paraphrase model;paraphrase generation problem;paraphrase similarity;diverse paraphrasing paper;paraphrase model;using diverse paraphrasing;text train paraphrase;inverse paraphrase model;able learn paraphrase;unsupervised style transfer;paraphrasing paper style;style transfer modeling;learn paraphrase;paraphrasing;syntactically diverse styles;paraphrased sentences large;controlled paraphrase;pretrained language models;task controlled paraphrase"}, "703a8252585948a96f5815025f7f03d68033b8bf": {"ta_keywords": "self play autonomously;bots self play;dialog systems trained;human dialogs;human human dialogs;play autonomously;agent user bots;human dialogs collected;task oriented dialog;systems trained human;learning game theoretic;learning game;approaches training agent;dialog systems;user bots self;self organization based;self play;bots self;training agent user;oriented dialog systems;self propelled particles;reinforcement learning game;trained human human;autonomously explore api;dialogs;training agent;play autonomously explore;agent;dialogs collected;trained human", "pdf_keywords": "bot learn strategies;play agent bot;bots self play;user bot learn;self play agent;self play autonomously;agent user bots;agent bot;self play task;dialogue stochastic game;dialogues human machine;bot learn;play autonomously;agent bot user;bots obtain rewards;user bots self;human machine dialogue;agent personal assistant;communicate agent personal;machine dialogue stochastic;play task oriented;training agent user;stochastic game user;communicate agent;autonomously solving tasks;tries communicate agent;play agent;task oriented dialogues;autonomously explore api;task oriented dialogs"}, "fa6c76d466fef633df51745bad85e991c371622c": {"ta_keywords": "recognition multi microphone;microphone speech recognition;multi microphone speech;speech recognition everyday;microphone speech;multi microphone;speech recognition;microphone;recognition everyday environments;recognition multi;model recognition multi;mathematical model recognition;recognition everyday;model recognition;recognition;speech;everyday environments;mathematical model;present mathematical model;mathematical;model;present mathematical;multi;environments;everyday;present", "pdf_keywords": ""}, "41a47363d261459c594525ef330e5fccaa8518a0": {"ta_keywords": "attribution authorship dataset;authorship attribution accuracy;topics authorship attribution;authorship attribution;attribution authorship;literature attribution authorship;datasets literature attribution;authorship attribution used;infer authorship dataset;authorship dataset;affect authorship attribution;authorship dataset apply;features affect authorship;authorship dataset outperforms;infer authorship;used infer authorship;attribution models based;author document based;topics authorship;authorship;preferred topics authorship;attribution accuracy;attribution models;literature attribution;applied attribution models;attribution used infer;affect authorship;author writing;author document;author writing style", "pdf_keywords": ""}, "98e6197e21ae530cd33eeff144ee556c5cf91dc8": {"ta_keywords": "competition shared resource;inexpensive method generating;intelligent authoring;build intelligent authoring;manually write cognitive;intelligent authoring environment;cognitive tutors;item manufacturer machine;environment cognitive tutors;write cognitive model;competition shared;items demonstrate method;competition;build intelligent;authoring environment cognitive;items demonstrate;products large set;large set products;cognitive tutors author;write cognitive;competition competition shared;study build intelligent;generating;machine demonstrate;competition competition;manufacturer machine demonstrate;effectiveness generality competition;cognitive;example generate;machine demonstrate used", "pdf_keywords": ""}, "af679d69fcc1d0fcf0f039aba937853bcb50a8de": {"ta_keywords": "sequence modeling neural;nested linear attention;unified nested attention;softmax attention;softmax attention nested;linear attention;approximates softmax attention;sparse dense attention;attention efficient sparse;sequence modeling tasks;nested attention;attention nested linear;attention efficient;linear attention functions;attention nested;dense attention methods;nested attention mechanism;attention operation linearly;neural machine translation;attention mechanism approximates;language modeling large;attention methods;dense attention;rank attention efficient;benchmarks sequence modeling;context sequence modeling;attention functions;attention operation;attention;attention functions yielding", "pdf_keywords": "softmax attention;regular softmax attention;nested attention;softmax attention transformer;nested attention functions;uses nested attention;nested attention mechanism;uni\ufb01ed nested attention;causal attention generating;context causal attention;sequence modeling neural;attention generating;long context sequence;sequence modeling tasks;attention transformer;attention generating wikipedia;attention functions;context sequence modeling;attention;sequence context causal;neural machine translation;attention functions approximate;new attention mechanism;causal attention;sequence context;input sequence context;attention mechanism;context sequence;softmax;language modeling large"}, "682e69be87f181edcf71800b54083595874d4ec6": {"ta_keywords": "predict persuasive speaker;speaker trait prediction;speaker traits hierarchical;ability predict persuasive;predict persuasive;learning hierarchical models;trait prediction;learning hierarchical;pendulum speaker trait;predict personality traits;predict personality;persuasive speaker work;task speaker trait;trait prediction present;related speaker traits;speaker traits;trait prediction aims;personality traits speaker;persuasive speaker;speaker trait;improving learning hierarchical;neural networks;traits speaker perceived;traits speaker;task speaker;predictions smaller subtasks;hierarchical models;like pendulum speaker;paper neural networks;method task speaker", "pdf_keywords": "predict persuasive speaker;persuasion prediction;persuasion prediction social;speaker traits persuasion;approach persuasion prediction;speaker trait prediction;speaker traits hierarchical;predict persuasive;ability predict persuasive;persuasiveness prediction;prediction social multimedia;persuasive speaker;persuasive speaker speaker;ternary persuasiveness prediction;persuasion patterns online;traits persuasion patterns;persuasiveness prediction paper;traits persuasion;speaker traits;related speaker traits;learning approach persuasion;prediction social;speaker trait;relationship speaker traits;persuasion patterns;predict personality traits;personality traits speaker;predict personality;traits speaker perceived;traits speaker"}, "7c8314e6138ce968f3b9f3bc55d5461ffbbec4aa": {"ta_keywords": "genetic algorithm predicting;genetic algorithm genetic;algorithm genetic algorithm;algorithm genetic;genetic algorithm;based genetic algorithm;accident based genetic;algorithm predicting outcome;based genetic;genetic;predicting outcome accident;algorithm predicting;outcome accident based;accident based;algorithm;predicting outcome;outcome accident;predicting;accident;outcome;based", "pdf_keywords": ""}, "97883f37c62b4b0e52cdc31dea1a375597db3804": {"ta_keywords": "networks trained task;network perform tasks;networks trained;individual networks trained;networks;network topology;network quantization;individual networks;network quantization sparsification;concepts network quantization;network masks;network masks approach;network topology based;comparable individual networks;problem network topology;structure underlying network;trained task;concepts network;underlying network;ability network;network;use network masks;network perform;filters used learn;building concepts network;ability network perform;learn structure underlying;tasks incurring overhead;approach problem network;existing network", "pdf_keywords": "training deep;training deep convolutional;imagenet pre trained;semantic segmentation starting;convolutional network semantic;convolutional network;deep convolutional;fully convolutional network;imagenet;method training deep;network semantic segmentation;deep convolutional neural;popular imagenet;train fully convolutional;semantic segmentation;imagenet pre;networks new tasks;pre trained network;high level vision;segmentation starting;fully convolutional;trained network;popular imagenet pre;convolutional neural;convolutional neural networks;segmentation starting classi\ufb01cation;neural networks low;performance popular imagenet;learning networks;convolutional"}, "e4c8447e56fc9cc3867087748acc4b259b9efe19": {"ta_keywords": "document training recurrent;recurrent neural networks;model text comprehension;text comprehension tasks;memory recurrent neural;memory recurrent;training recurrent neural;explicit memory recurrent;training recurrent;recurrent neural;model coreference relations;text comprehension;task predicting;model coreference;comprehension tasks;coreference relations text;coreference relations;comprehension tasks achieve;task predicting outcome;coreference;model task predicting;external linguistic;external linguistic knowledge;grained entity information;recurrent;use model coreference;model text;fine grained entity;use external linguistic;model memories", "pdf_keywords": "learning read comprehend;dependency parses semantic;memory sequence text;recurrent neural network;parses semantic;parses semantic role;propose deep neural;deep neural;external linguistic knowledge;architecture reading comprehension;wordnet;modeling dependencies;recurrent neural;learning read;term dependencies readily;read comprehend;linguistic knowledge explicit;modeling dependencies elements;language modeling;dependency parses;reading comprehension based;dependencies elements sequence;knowledge include dependency;learning suitable representations;reader handling coreference;semantic role;sequence text;explicit memory sequence;memory sequence;coreference"}, "be8d6a8d3dfe87a4d9171f25bf9a18d502498756": {"ta_keywords": "semantic frame induction;synonymy graph unsuper;fuzzy graph clustering;semantic class induction;induction synonymy graph;networks linguistic;unvised semantic;unsuper supervised semantic;supervised semantic frame;networks linguistic data;graph unsuper supervised;unsupervised synset induction;graph clustering;unvised semantic class;graph based clustering;graph clustering widely;semantic frame;synset induction synonymy;applied networks linguistic;induction distributional thesaurus;supervised semantic;clusters intermediate graph;clustering discover;ways unsupervised synset;synonymy graph;linguistic data;unsupervised synset;clustering widely;hard clustering discover;clustering", "pdf_keywords": ""}, "bd1cf4279d834699db871e1451d289c49ff2b6de": {"ta_keywords": "learning choreograph;dance convolution;step selection;predict steps;step charts;predict steps step;task learning choreograph;place steps;dance dance convolution;step placement task;step placement;step chart introduce;step selection substantially;dance;step chart;steps step placement;rhythm based video;task step charts;choreograph;rhythm based;new step chart;networks predict steps;dance dance;step charts available;learning choreograph paper;dance convolution ddr;rhythm;chart exists dance;lsw step selection;dance song chart", "pdf_keywords": "music step;music step placement;piece music step;task music information;music information retrieval;important task music;task music;task music research;step charts raw;music information;step charts;learning choreograph;generate step charts;piece music;music;charts raw audio;performance deep;relation piece music;learning choreograph learn;choreography task players;new choreography task;choreograph learn generate;musically salient points;dance;task learning choreograph;choreography;chart decompose subtasks;steps atop dance;performance deep neural;choreograph learn"}, "cf46ecac1cb1bdae153be2b909ff3e313034ac9e": {"ta_keywords": "modalities autism spectrum;context social skills;visual modalities autism;modalities autism;autism spectrum;autism spectrum objective;characteristics autism spectrum;contextual information emotion;social skills communication;characteristics autism;skills communication;autism spectrum disorders;autism;context ability;modality contextual information;non verbal communication;verbal communication training;verbal behaviour presence;modality context social;behaviour presence contextual;context ability infer;verbal communication;social skills training;traits characteristics autism;communication skills;modality contextual differences;social skills;communication training;communication skills types;use modality contextual", "pdf_keywords": ""}, "ef59f05a30972742a714b8903848e4b5dfc5cdaf": {"ta_keywords": "target use cases;bouncer taxonomy built;equipment bouncer taxonomy;methods actionable taxonomy;useful use cases;actionable taxonomy;machine learning;use cases;bouncer taxonomy;tool conceptualize;consumers work discover;methods use cases;use cases proposed;use cases components;relevant methods target;use cases researchers;machine learning mls;methods target use;actionable taxonomy taxonomy;serves tool conceptualize;applicable use cases;relationship machine learning;cases proposed methods;cases components;ilr methods actionable;consumers relevant methods;taxonomy built;workflow increasingly;equipment bouncer;methods target", "pdf_keywords": ""}, "9b9ee9a25fc4d9f8ad22c2923c49b8d5d0b83356": {"ta_keywords": "extracting disambiguated hypernymy;recognizing hypernymy relationships;disambiguated hypernymy relationships;hypernyms sets synonyms;hypernymy extraction;improve hypernymy extraction;hypernymy extraction present;method recognizing hypernymy;hypernymy relationships propagates;disambiguated hypernymy;hypernyms sets;relationships propagates hypernyms;recognizing hypernymy;propagates hypernyms sets;synonyms synsets;synonyms synsets constructs;sets synonyms synsets;extracting disambiguated;wiktionary datasets unsupervised;propagates hypernyms;hypernymy relationships;hypernyms;hypernymy relationships standard;patterns wiktionary datasets;method extracting disambiguated;sense aware relationships;wiktionary datasets;relationships matching synsets;unsupervised sense representations;hearst patterns wiktionary", "pdf_keywords": "distributional semantics hypernyms;word embeddings hypernyms;hypernyms semantic relations;semantics hypernyms;vectors hypernyms semantic;hypernymy extraction methods;hypernyms word embeddings;current hypernymy extraction;hypernyms semantic;machine translation hypernyms;semantics hypernyms hyponyms;hypernymy extraction;extract hypernyms;automatically extract hypernyms;extract hypernyms word;text corpora hypernymy;embeddings hypernyms collections;embeddings hypernyms;hypernymy extraction technique;approach disambiguates hypernyms;collection hypernymy extraction;corpora hypernymy essential;hypernyms hyponyms widely;hypernyms represented sparse;extracting semantic relationships;hypernyms words;hypernyms words expressed;extracting semantic;senses hypernyms words;disambiguates hypernyms"}, "923ddc71f8a453c7995e97b0681a674224a5fc09": {"ta_keywords": "accuracy machine translation;low quality translations;high quality translations;quality translations;machine translation mt;machine translation;quality translations improve;translations improve efficiency;translation mt systems;quality translations identifies;translations improve;translations identifies features;translations;translations identifies;translation mt;mt error analysis;error analysis;translation;accuracy machine;study accuracy machine;error analysis experiments;terms error analysis;manual analysis;accuracy;efficiency mt error;manual;improve efficiency manual;manual analysis paper;mt systems terms;paper study accuracy", "pdf_keywords": ""}, "407eacc5ade80b54126c300b57b81f4b4f411487": {"ta_keywords": "assess machine translation;professional human translations;machine translation;machine translation systems;human translations;translation systems;translation systems general;human translation;human translation presence;human translations contained;indistinguishable human translation;raters availability linguistic;machine translation indistinguishable;reference translations;paper machine translation;translations;creation reference translations;translation presence;translations contained significantly;translation presence noise;translation indistinguishable human;rules translated;translation;quality human evaluation;translations contained;human machine parity;evaluation international selection;translation indistinguishable;translated;human evaluation depends", "pdf_keywords": "translation output crowdsourced;crowdsourced machine translation;machine translation outputs;machine translation experiments;human translators findings;ability translators crowd;translators 2020 ai;machine translation output;human translators;evaluation ability translators;level human translators;human translators 2020;professional human translators;machine translation;translators findings;translators crowd;translators crowd workers;neural machine translation;quality machine translation;translation experiments;machine translation systems;translation experiments recent;translation systems reportedly;translators findings evidence;translation systems;translations produced;output crowdsourced machine;analysis machine translation;output crowdsourced;translations produced professional"}, "4bf1ea102e1eb1246929bb77c11ebbd6b6d27500": {"ta_keywords": "sentence prototypes;library sentence prototypes;sentence prototypes modify;learned prototypes;prototype driven language;language modeling performance;prototypes reference training;learned prototypes able;prototype driven text;learns emph sparse;time learned prototypes;driven text generation;automatically learns emph;language models achieving;text generation;entire training corpus;driven language models;strong language modeling;training corpus;prototype generation;specifying prototype generation;automatically learns;language models;prototype generate;language modeling;generative model automatically;generation prototype driven;sparse prototype support;prototype generation prototype;prototypes", "pdf_keywords": "learns sparse prototype;conditional generation tasks;prototype driven generative;novel generative model;propose novel generative;generative models;novel generative;context conditional generation;automatically learns sparse;generative model automatically;sparse neural;learns sparse;margin learned prototypes;generative;driven generative models;learned prototypes;generation tasks;generative model;novel nonparametric neural;nonparametric neural model;conditional generation;nonparametric neural;language modeling;sparse neural model;learned prototypes able;driven generative;sentence attributes controlled;generative models fact;particular sparse neural;model automatically learns"}, "93a55f3341aa70bb42c0f76b112e2e8da27b3df2": {"ta_keywords": "dialogue human machine;dialogue like entrain;entrainments affect dialogue;dialogue affects human;human dialogue;dialogue human;human dialogue human;human human dialogue;entrainment factor dialogue;entrainment lexical choice;dialogue;dialogue abstract structural;dialogue abstract;choice given dialogue;dialogue like;dialogue affects;entrainment lexical level;entrainment lexical;affect dialogue abstract;rejected entrainment lexical;effect entrainment lexical;build dialogue like;dialogue acts;given dialogue;affect dialogue;human machine interaction;build dialogue;given dialogue acts;like entrain users;entrain users similar", "pdf_keywords": ""}, "bed0452305633791340f80cb0be02f46e4a34b0d": {"ta_keywords": "voice conversion challenge;voice conversion;approach voice conversion;voice conversion transcribe;seq2seq baseline voice;baseline voice conversion;transcriptions generate voice;generate voice;generate voice target;transcribe input speech;speech processing toolkit;end speech processing;speaker pairs based;approach identification speaker;speech tts model;sequence seq2seq framework;quantum repeater based;implementation quantum repeater;repeater based quantum;using transcriptions generate;quantum repeater;input speech automatic;target text speech;sequence seq2seq baseline;speech recognition asr;voice target text;sequence sequence seq2seq;source implementation quantum;sequence seq2seq;identification speaker", "pdf_keywords": "models voice conversion;voice conversion challenge;source voice conversion;voice conversion;voice conversion trained;pretrained models voice;speech processing toolkit;source speech waveform;open source voice;built speech processing;source voice;models voice;speech waveform;automatic speech;implementation built speech;method automatic speech;voice;speech processing;speech recognition;source speech;automatic speech recognition;conversion trained end;built speech;conversion challenge vcc;speech waveform different;conversion trained;recurrent neural network;speech recognition based;language model easy;recognition based seq2seq"}, "ce97452d031a1a156212f038bab6f47a51575236": {"ta_keywords": "automatic recognition stance;recognition stance taking;prosodic features boosting;stance taking behavior;text stance taking;stance taking activity;exclusively text stance;stance taking;recognition stance;new annotated corpus;polarity stance;verbally classifiers;subjectivity sentiment analysis;corpus develop classifiers;text stance;based annotated corpus;annotated corpus develop;annotated corpus;corpus spontaneous conversational;stance;subjectivity sentiment;conversational speech designed;style prosodic features;taking behavior speech;conversational speech;lexical speaking style;speaking style prosodic;behavior speech;spontaneous conversational speech;areas subjectivity sentiment", "pdf_keywords": ""}, "995f4e670c0cdcd5afdef08719c2528a682bff05": {"ta_keywords": "training decoding speed;training decoding;decoding speed fast;outputs training decoding;decoding speed;speech translation model;accurate translation speed;recognition asr decoder;decoding based connectionist;translation speed achieved;translation speed;machine translation subtasks;speech recognition asr;speech recognition;connectionist temporal classification;models multi decoder;decoding model;intermediate automatic speech;decoding;asr decoder;recognition asr;decoder;nonautoregressive nar decoding;nar decoding based;multi decoder;new decoder;new decoder class;nar decoding;decoding based;asr decoder states", "pdf_keywords": "automatic speech translation;speech translation based;translation source audio;translation quality models;minimizing translation quality;speech translation ar;speech translation source;translation quality degradation;translation architecture automatic;speech translation;machine translation sub;translation quality;speed automatic speech;net minimizing translation;speech translation paper;automatic automatic speech;architecture automatic speech;machine translation;translation sub tasks;propose translation architecture;translation architecture;automatic speech;5k automatic speech;task machine translation;minimizing translation;translation ar sub;discrepancy translation quality;translation based;translation sub;improving decoding speed"}, "e2198b039ee5bfa233cf06e65f26a9f3233ada9f": {"ta_keywords": "dialogue act selection;dialogue acts patterns;entrainment dialogue act;dialogue participants converge;dialogue participants;entrainment dialogue;dialogue act word;measures entrainment dialogue;dialogue human like;dialogue act;specific dialogue acts;dialogue acts;dialogue speech;comparing speech;dialogue human;choice specific dialogue;act word selection;speech different;comparing speech different;dialogue speech speaker;switchboard corpus comparing;selection observable dialogue;dialogue;speech different speakers;corpus comparing speech;studies dialogue participants;observable dialogue human;speech speaker different;patterns dialogue act;specific dialogue", "pdf_keywords": ""}, "29da62b3f8aed3fe98b3f02bbfd436dd8e65a532": {"ta_keywords": "protocol contention;based contention contention;protocol contention aggressiveness;contention contention;present protocol contention;contention contention propose;based contention;contention;contention aggressiveness based;contention propose;throughput based utility;aggressiveness based contention;synchronization nodes study;throughput;contention propose new;message passing synchronization;synchronization nodes;contention aggressiveness;maximize throughput;throughput based;maximize throughput based;protocol;approach maximize throughput;passing synchronization nodes;xmath0 production cross;synchronization;wireless networks message;networks message passing;xmath0 production;utility wireless networks", "pdf_keywords": ""}, "4264599665522594d9ecb521dd2e1d002e85a961": {"ta_keywords": "computing fair matchings;fair matchings paper;fair matchings provably;fair fair matchings;fair matchings;fairness standard matching;matching reviewers papers;faster matching algorithms;automatically matching reviewers;matching reviewers;matching algorithms optimize;matching algorithms;paper matching algorithms;algorithm fair fair;standard matching algorithms;algorithms construct matchings;fairflow computing fair;matching algorithms construct;algorithm fair;common paper matching;fairflow improve fairness;computing fair;matchings paper;algorithms optimize fairness;matchings provably;new algorithm fair;fair matches order;state art matching;faster matching;algorithms fairir fairflow", "pdf_keywords": "automatically assigning reviewers;automation matching reviewers;maximization reviewer paper;assigning reviewers;fairness formulation reviewer;matching reviewers;assigning reviewers reviewers;maximization reviewer;matching reviewers submitted;load equally reviewers;assigned reviewer paper;reviewers based weighted;fairness constraints automatic;equally reviewers necessitates;assigned reviewer;reviewer assignment problem;global maximization reviewer;reviewer paper pairs;reviewer assignment;equally reviewers;fairness standard matching;af\ufb01nities assigned reviewer;formulation reviewer assignment;peer review;fairness constraints;objective peer review;reviewer paper af\ufb01nity;reviewers necessitates;reviewers submitted papers;local fairness constraints"}, "1578fba4a2b2ba819986e32c7da6ebbaf9aacf41": {"ta_keywords": "treebanks underresourced;treebanks written sequence;treebanks;rooted treebanks;treebanks underresourced making;treebanks written;tuples rooted treebanks;rooted treebanks written;multi lingual transfer;lingual transfer training;train deep neural;language use hierarchical;deep neural models;related languages;lingual transfer;deep neural;multiple related languages;multi lingual;languages;languages share similar;related languages share;neural models;polynomials certain language;language;lingual;certain language;languages share;propose multi lingual;neural;neural models article", "pdf_keywords": "lingual transfer learning;crosslingual transfer learning;learning cross lingual;multi lingual transfer;multilingual transfer approaches;multilingual transfer;present multilingual transfer;model multi lingual;cross lingual transfer;multi lingual;lingual transfer;interactions crosslingual transfer;cross lingual;multilingual model;crosslingual transfer;multilingual model multi;multilingual;treebanks achieve 14;propose multilingual model;lingual;scarcity present multilingual;high resource languages;languages diverged;treebanks achieve;languages diverged independent;107 treebanks achieve;crosslingual;interactions crosslingual;treebanks;language groups shallower"}, "6e2e7df21a5b5457ea4167133a40bc729028250d": {"ta_keywords": "ranking compared;queries passage ranking;ranking stack judged;neural ranking stack;retrieval used rank;passage ranking leaderboard;passage ranking development;neural ranking;ranking;passage ranking compared;ranking leaderboard;ranking stack;modern neural ranking;passage ranking;document passage ranking;ranking leaderboard query;ranking compared traditional;ranking leaderboard michigan;improvements rankers;ranking development;improvements rankers recent;information retrieval tasks;genuine improvements rankers;paper passage ranking;rank best;rank best answer;information retrieval test;information retrieval;ranking development set;leaderboard query based", "pdf_keywords": "document ranking;document ranking allowing;known answer ranking;neural rankers;passage document ranking;informational retrieval;answer ranking;neural ranker;ranking;informational retrieval tasks;widespread web search;core informational retrieval;web search engines;based neural ranker;learning rank;neural learning rank;search engines;neural rankers place;answer ranking decade;ability neural rankers;neural ranker paper;ranking allowing;learning rank methods;crowdsourced preference judgments;web search;qrels navigational queries;retrieval tasks;search engines state;retrieval tasks adhoc;web search work"}, "80257b7d02ad4d6a762ebc0d7f1560e0ef182354": {"ta_keywords": "polite sentences preserving;sentences preserving;sentences preserving meaning;human evaluations grammaticality;style transfer tasks;politeness transfer;grammaticality meaning preservation;nonpolite sentences polite;task politeness transfer;politeness transfer involves;preserving meaning model;sentences polite;converting nonpolite sentences;polite sentences;style transfer;sentences polite sentences;evaluations grammaticality;style transfer accuracy;accuracy style transfer;metrics content preservation;nonpolite sentences;performance style transfer;automatic metrics content;new task politeness;evaluations grammaticality meaning;preservation transfer accuracy;preserving meaning;meaning preservation transfer;transfer accuracy style;sentences", "pdf_keywords": "polite sentences preserving;linguistic strategies politeness;converting non polite;formality style transfer;polite sentences;style generate sentence;sentences polite;non polite sentences;style transfer tasks;polite sentences polite;politeness described;transfer hybrid textual;approach style transfer;sentences polite sentences;sentences preserving;style transfer based;transfer based linguistic;style transfer techniques;sentence formality style;strategies politeness described;politeness described previous;strategies politeness;sentences preserving meaning;hybrid textual annotations;style transfer;style targeted sentence;targeted sentence formality;style transfer create;sentence target style;generate sentence target"}, "09093e29b1f705bb7a68ea2e9240b3f122efe92b": {"ta_keywords": "testing accuracy speech;accuracy speech recognition;accuracy speech;speech recognition;speech recognition based;flight tof method;tof method;time flight tof;testing accuracy;method testing accuracy;stationary time flight;flight tof;tof;recognition based use;method testing;speech;new method testing;time flight;accuracy;non stationary time;stationary time;recognition based;use non stationary;recognition;testing;method;flight;non stationary;stationary;new method", "pdf_keywords": ""}, "3edfccbe6adf18f5263cd2adf3d977bbc5811e0b": {"ta_keywords": "training text encoder;neural text encoder;novel data augmentation;machine translation encoder;data augmentation;automatic speech;text encoder model;translation encoder;text encoder;automatic speech recognition;speech recognition;training text;trained machine translation;build neural text;speech recognition experimental;encoder;data augmentation method;method automatic speech;translation encoder sequence;neural text;trained machine;encoder model predicts;encoder model;augmentation method automatic;encoder sequence;instead acoustic features;translation build neural;end housekeeping water;encoder sequence characters;housekeeping water", "pdf_keywords": "augmentation method attentionbased;deep text encoder;augmentation method attention;neural text encoder;novel data augmentation;attention based machine;networks rnns;faster attention learning;data automatic speech;propose data augmentation;neural networks rnns;data augmentation;network deep text;attention learning;speech recognition asr;attentionbased endto;text encoder network;deep text;machine translation;automatic speech;rnns;attentionbased endto end;network deep;networks rnns paper;propose deep neural;deep neural;text paired speech;speech recognition;attentionbased;text encoder"}, "e6aaac94df717786a467d057cb2157b9d49f0974": {"ta_keywords": "regret algorithms;regret arbitrary adversarial;regret regret algorithms;regret algorithms created;regret variationally stable;achieve regret arbitrary;equilibrium individual regret;regret arbitrary;optimistic mirror descent;game theoretic guarantees;response convergent opponents;regret variationally;converges nash equilibrium;algorithm measured regret;arbitrary adversarial opponents;variationally stable games;individual regret variationally;play converges nash;mirror descent desirable;convergent opponents;optimistic mirror;regret policies based;games performance learning;regret policies;converges nash;adversarial opponents;adversarial opponents iii;based optimistic mirror;mirror descent;arbitrary adversarial", "pdf_keywords": "learning nash equilibria;learning nash;adaptive optimistic learning;optimistic learning;learning continuous games;continuous convex game;optimistic learning strategy;convex game;learning games continuous;player learning games;optimal regret minimization;adaptive optimistic;player learning;nash equilibrium players;algorithms online learning;framework learning nash;concave person games;games algorithms;optimistic mirror descent;regret minimization guarantees;continuous games;actions nash equilibrium;person games algorithms;optimal regret bounds;learning games;learning strategy variationally;nash equilibria concave;optimal regret;online learning continuous;learning model players"}, "efaf07d40b9c5837639bed129794efc00f02e4c3": {"ta_keywords": "learns continuous representations;model learns continuous;learns continuous;feature representations model;feature representations;discrete feature representations;representations model learns;carbon nanotube;paper continuous representations;features neural network;model learns;features neural;nanotube;walled carbon nanotube;continuous representations used;network jointly classification;continuous representations;representations gram features;continuous representations gram;neural network;networks;complex networks;dynamics complex networks;gram features neural;neural;carbon nanotube contrast;gram features;neural network jointly;representations model;discrete feature", "pdf_keywords": ""}, "9d9159026023f21e633f84fd61f3efad2e410214": {"ta_keywords": "logic embeddings;logic embeddings comparing;order logic embeddings;embeddings order logic;knowledge base completion;formulas probabilistic relational;probabilistic relational models;logic formulas probabilistic;probabilistic relational;training examples inference;relation extraction knowledge;examples inference formulas;generate plausible inference;factorization approach learn;logic formulas;inference formulas;inference formulas facts;knowledge base;low dimensional embeddings;plausible inference formulas;probabilistic order logic;formulas facts build;extraction knowledge base;generalize symbolic representations;relational models;embeddings;embeddings comparing;representations examples logical;order logic formulas;examples logical formulas", "pdf_keywords": ""}, "46f66dd37e6366ce102cfd97e718947151d5b1eb": {"ta_keywords": "detect fake news;microblogging process;fake news detectors;microblogging;news environment fake;microblogging process large;analyze microblogging;news detectors;analyze microblogging process;paper analyze microblogging;news detectors high;news post fluid;information external news;fake news designed;external news environment;news environment;temperature fake news;news news post;external news;fake news post;fake news news;automatically detect fake;environment fake news;social network;detect fake;news post created;basic fake news;social network methods;online social network;news news", "pdf_keywords": "detection fake news;fake news detection;fake news method;fake news online;contemporary fake news;published fake news;misinformation social media;news environments evidence;post published fake;preventing dissemination misinformation;dissemination misinformation social;news online social;detection fake;news detection;dissemination misinformation;news environment;news method;news method equivalent;news detection crucial;fake news;news environments;propose news environment;news environment represents;published fake;method uses news;uses news;news items published;uses news environments;news item post;news online"}, "8122eaeb63098e94416108df918c9669e9105e65": {"ta_keywords": "cached replicas;cluster cache;latency cluster cache;new caching technique;cached replicas given;cluster cache uses;caching technique;number cached replicas;caching technique data;data intensive clusters;stores ec cache;performance caching;performance caching important;caching;ec cache;object caching;clusters object stores;memory object caching;ec cache uses;selective replication;caching important;cache load balanced;replication;degraded performance caching;new caching;cache;object caching meet;intensive clusters object;limitations selective replication;cache uses", "pdf_keywords": ""}, "8d64be0d3bb2650ff99a4c1ae8049eb5fece27a1": {"ta_keywords": "carbon nanotube;walled carbon nanotube;features speech recognition;carbon nanotube cnt;speech recognition bottleneck;nanotube cnt;nanotube cnt present;nanotube;bottleneck features speech;speech recognition;recognition bottleneck features;recognition bottleneck;speech recognition automatic;recognition automatic speech;speech recognition asr;automatic speech recognition;features speech;features deep neural;bottleneck features deep;walled carbon;single walled carbon;bottleneck features;neural network hidden;recognition asr;deep neural network;neural network;automatic speech;bottoleneck;deep neural;blood cell", "pdf_keywords": ""}, "f8f17f32e651840531276423c7196856d27bcdd0": {"ta_keywords": "search natural language;natural language processing;natural language;range natural language;natural language used;language processing tasks;language processing;search natural;search;method search natural;language used resource;simple method search;method search;resource;processing tasks;resource wide;language;tasks;used resource;language used;wide range natural;processing;used resource wide;resource wide range;range natural;natural;simple method;simple;present simple;method", "pdf_keywords": ""}, "ee7af49291c030a3e29ad7a9cb5c1975d1b644f4": {"ta_keywords": "sudden change diet;women sudden change;change diet;diet;group women sudden;women sudden;response group women;group women;women;sudden change;present study response;present study;study response group;study response;change;group;response group;response;paper present study;sudden;study;present;paper present;paper", "pdf_keywords": ""}, "1cbb43b4d7f79d986a4a78ad3b53368c49e496ee": {"ta_keywords": "speech recognizer;feature transformations utterance;utterance based adaptation;speech recognizer based;transformations utterance based;automatic speech;speech recognition;calls automatic speech;speech recognition systems;art speech recognizer;automatic speech recognition;adaptation discriminative training;rnn based feature;transformations utterance;deep recurrent neural;discriminative training;utterance based;robotic named asr;recurrent neural network;based adaptation discriminative;deep recurrent;state art speech;discriminative training propose;channel audio based;audio based;uses deep recurrent;audio based sum;audio;video water fountain;adaptation discriminative", "pdf_keywords": ""}, "d0fbae81d870bbfb34430654f70fd6a21e8bd1cc": {"ta_keywords": "coreference annotations;coreference annotations extracted;coreference;comprehension model neural;entity mentions;mentions entity;reading comprehension model;multiple mentions entity;connect entity mentions;entity mentions belonging;nlp require;information multiple mentions;coreferent dependencies;nlp require aggregating;mentions entity far;cluster problems nlp;annotations;nlp;annotations extracted external;problems nlp;annotations extracted;problems nlp require;comprehension model;mentions belonging cluster;biased coreferent dependencies;mentions belonging;reading comprehension;recurrent layer;multiple mentions;emergence recurrent layer", "pdf_keywords": "comprehension bias coreference;architecture coreference resolution;layer reading comprehension;recurrent layer reading;coreference resolution;coreference resolution present;bias coreference goal;coreference goal;coreferences;coreference;bias coreference;architecture coreference;coreference goal tackling;deep recurrent neural;bias introduced coreference;phrase representations neural;neural language models;mentions entity learning;introduced coreference;task reading comprehension;recurrent neural network;coreferences structural bias;deep recurrent;introduced coreference signals;reading comprehension task;network architecture coreference;learning phrase representations;recurrent neural;coreference signals important;coreference signals"}, "071216d944bcd2f05deafdb94e657167cce148d9": {"ta_keywords": "recognizing personal names;names email;method recognizing personal;personal names email;names email newswire;learned sequential classifier;email newswire text;sequential classifier;personal names;recall precision;recognizing personal;recall precision tradeoff;classifier;classifier change recall;recognizing;learned sequential;existing learned sequential;recall;newswire text present;text;text present;change recall precision;sequential classifier change;newswire text;change recall;email newswire;names;tweaking existing learned;email;text present simple", "pdf_keywords": ""}, "1ebf54c0a8b38e8c26ed857cb9d4e565a8f17f17": {"ta_keywords": "graph walks;performance graph walk;optimize graph walk;graph walk performance;graph walk graphs;graph walk;graph walks induce;random graph walks;walk graphs;graph graph structure;graph structure;search graph;graph nodes;represented graph nodes;walk graphs heterogeneous;measure entity similarity;information social network;performing search graph;entity similarity;edges represent relations;improve performance graph;entity similarity viewed;directed typed edges;social network represent;network represent graph;finite random graph;social network;search graph present;graph structure includes;graphs heterogeneous sense", "pdf_keywords": ""}, "72a5c01afe276d06ca9179e24b1c925e206454f3": {"ta_keywords": "produce recommendations explanations;recommendations explanations methods;graphs explainable recommendation;recommendations explanations;explainable recommendation important;knowledge graphs explainable;explainable recommendation;generating explanations;items knowledge graph;generating explanations hard;recommendation important task;knowledge graph;explanations methods proposed;leveraging external knowledge;external knowledge form;knowledge graphs;explanations methods;external knowledge;explanations generated;explanations generated scenario;predict outcome business;ranks items knowledge;personalized page rank;knowledge graph entities;items knowledge;illustrate explanations generated;recommendation important;procedure produce recommendations;form knowledge graphs;rules used predict", "pdf_keywords": "explanation recommendation knowledge;recommendation knowledge graphs;recommendation knowledge;explanations knowledge graph;user trust recommender;recommender systems;improve recommender;trust recommender;recommendation recommender systems;improve recommender accuracy;kg based recommenders;recommender accuracy;recommender systems users;generate explanations knowledge;recommendation recommender;based recommendation recommender;based recommenders;recommender accuracy past;based recommendation;knowledge graph;explanations knowledge;kg based recommendation;explanation recommendation;recommenders paper present;knowledge graph kg;leveraging external knowledge;knowledge graphs;recommender;based recommenders paper;recommenders paper"}, "f800f60db4427a51e564f1b875ae01d2c642fdce": {"ta_keywords": "repair bandwidth storage;storage functional repair;data repair transfer;transfer data repair;repair bandwidth presented;repair bandwidth;codes distributed storage;data repair;bandwidth required repair;possible repair bandwidth;repair replacement node;distributed storage;node storage;bandwidth storage functional;transfer exact repair;code perform repair;repair failed node;functional repair tradeoff;repair transfer;failed node storage;exact repair code;repair mere transfer;storage functional;node storage bandwidth;storage bandwidth tradeoff;replacement node;minimum possible repair;functional repair;bandwidth storage;repair transfer exact", "pdf_keywords": "tradeoff regenerating codes;regenerating codes presented;repair bandwidth tradeoff;exact repair codes;repair bandwidth;regenerating codes;exact repair code;repair codes;replacement nodes;node repaired simple;repair codes case;tradeoff exact repair;nodes replacement nodes;repair code parameters;nodes replacement;repair code;node repaired;repair transfer property;satisfying storage bandwidth;regenerating codes open;computation helper nodes;repair transfer;helper nodes replacement;properties exact repair;paper repair bandwidth;bandwidth tradeoff regenerating;tradeoff derived storage;achievability storage bandwidth;helper node pooling;exact repair"}, "b2f46145f2a50b609482a69d0581b218a6767cef": {"ta_keywords": "similarity web based;similarity web;textual similarity web;web search engine;complex site search;web based information;retrieval information complex;approach retrieval information;site search engines;search engines;search engine integrates;using ir similarity;retrieval information;web search;ir similarity metrics;textual similarity;ir similarity;search engine;approach retrieval;similarity metrics text;information multiple web;search engines like;similarity metrics;whirl web search;similarity;representation textual similarity;database representation textual;common database representation;site search;information sources common", "pdf_keywords": ""}, "80fdacd50ba9ad2e594dd2ddb0b1fa0e591f37ea": {"ta_keywords": "biomedical event extraction;novel structured prediction;structured prediction outperforms;event extraction similarly;structured prediction;structured prediction recent;structured prediction framework;based structured prediction;event extraction;structured prediction achieves;information extraction tasks;paradigm event extraction;regulation events biomedical;event extraction set;decomposes event extraction;structured prediction method;information extraction;event extraction bsa;events biomedical;classification tasks learned;extraction genetic regulatory;classification tasks;complex information extraction;events biomedical event;gene expression data;biomedical event;set classification tasks;extraction set classification;search based structured;extraction tasks article", "pdf_keywords": ""}, "d46ecbacf42748ac9ce1fecd9f1b4ed0b9e34980": {"ta_keywords": "classification email messages;email act classification;toolkit prediction speech;classification email;consider classification email;gram sequence features;prediction speech;certain intents email;contextual information messages;email acts;intents email acts;task email communication;intents message exchange;message preprocessing highly;prediction speech amplitudes;email communication;email messages;act classification;effective task email;email acts propose;intents email;message exchange representation;useful trace intents;message preprocessing;email communication useful;email act;sequence features;trace intents;careful message preprocessing;sequence features careful", "pdf_keywords": ""}, "b350be3836c3d183464642815b26b061f24e8314": {"ta_keywords": "number embeddings learned;learning representations mathematical;embedding words;embeddings learned;approach embedding words;embeddings learned english;embedding words high;embeddings;number embeddings;improve number embeddings;learning representations;new approach embedding;words high dimensional;representations mathematical;representations mathematical sequence;approach embedding;embedding;moving dimensional potential;particle moving dimensional;dimensional potential;representations;high dimensional vector;dimensional vector spaces;dimensional potential eigenvalues;moving dimensional;high dimensional;mathematical sequence data;dimensional;dimensional vector;vector spaces present", "pdf_keywords": "trained integer embeddings;integer embeddings;embedding learned integer;integer embeddings represent;types integer embeddings;embeddings contexts integers;integer embeddings set;numeracy word embeddings;word embeddings;mathematical regularities embedding;text train embedding;learned integer sequences;regularities embedding learned;similarly trained integer;embeddings represent useful;word embeddings conducted;embeddings;embedding learned;use word embeddings;embeddings conducted;trained integer;properties word embeddings;embedding models;embeddings represent;word embeddings paper;word embeddings contexts;represent useful mathematical;train embedding models;regularities embedding;embeddings paper investigate"}, "e602bde46bca5f424a3d53675c1275386544eb1e": {"ta_keywords": "learning process;concept learning scratch;learning process based;successfully learning tasks;learning scratch concept;learning tasks;learning scratch;learning;model learning process;successfully learning;concept learning;scratch concept;based concept learning;scratch concept used;used successfully learning;simple model learning;process based concept;model learning;concept;concept used;based concept;concept used successfully;process based;tasks;process;simple model;simple;scratch;present simple model;model", "pdf_keywords": ""}, "7fcc2cc70498e409168a6c3dfd7c59652b1160c2": {"ta_keywords": "transforms acoustic features;transformation matrix regression;likelihood transforms acoustic;feature space adaptation;transformation matrices feature;adaptation decoding;networks adaptation decoding;transformation matrix feature;acoustic features adapted;transforms acoustic;neural networks adaptation;deep neural;adaptation applied deep;precessing independent decoding;maximum likelihood transforms;adaptation decoding share;regression feature space;deep neural networks;paper deep neural;likelihood transforms;deep neural network;transformation matrices;adapt pre precessing;features adapted;dnn able adapt;single transformation matrix;multiple transformation matrices;matrices feature space;networks adaptation;applied deep neural", "pdf_keywords": ""}, "dda3f2a2803c80e5b3332868bf86901d6239befc": {"ta_keywords": "stochastic distributed methods;tolerant distributed methods;tolerant distributed method;new distributed methods;distributed fault tolerant;distributed methods;distributed methods unbiased;distributed nonconvex optimization;tolerant distributed fault;fault tolerant distributed;distributed method;distributed methods paper;distributed method solving;distributed methods error;compression distributed nonconvex;distributed fault;compression distributed;local fault tolerant;stochastic distributed;unbiased compression distributed;distributed nonconvex;analysis stochastic distributed;tolerant distributed;decentralized fault tolerant;local fault tolerance;methods unbiased compression;centralized local fault;fault tolerant;distributed;solving local fault", "pdf_keywords": "stochastic gradient descent;prediction stochastic gradients;stochastic gradient gradient;stochastic gradients;stochastic gradient;stochastic gradient estimation;introduce stochastic gradient;gradient descent;stochastic gradients introduce;gradients introduce stochastic;gradient descent present;local stochastic gradient;neural networks dnets;particular stochastic gradient;method gradient descent;gradient estimation;workers compute gradients;gradients functions iteration;gradient descent sep;delayed updates quantization;communication bottleneck;compute gradients functions;gradient estimation chapter;gradient gradient;communication bottleneck propose;compute gradients;client particular stochastic;gradient;gradients;gradients functions"}, "9dc4a5284ecfd37ab8bc8990eddf1b39113e004b": {"ta_keywords": "target monolingual data;machine translation training;source target languages;translation training;mismatch machine translation;translation training low;resource languages translation;translation self training;target monolingual;machine translation postulate;target languages;increasing target monolingual;low resource languages;machine translation;context machine translation;target languages greatly;languages translation;combining translation self;languages translation improved;languages greatly mismatch;translation self;monolingual data;translation postulate causes;source target;source target domain;domains source target;resource languages;monolingual data study;translation postulate;concept source target", "pdf_keywords": "machine translation researchers;machine translation geographic;comparable machine translation;machine translation research;training machine translation;machine translation low;combines machine translation;machine translation systems;machine translation;particular machine translation;machine translation controlled;effectively translation learning;translation systems;translation geographic lexical;translation learning;translation low resource;translation learning multilingual;machine translation investigate;translation researchers;translation researchers explicit;automatically translating;translation research;translation research based;machine translation approach;target monolingual data;automatically translating content;task machine translation;translation geographic;approach machine translation;translating content languages"}, "128610c7df12bff1610949c551b6236cb350dcd9": {"ta_keywords": "train speech recognition;speech recognition pre;trained acoustic language;speech recognition asr;e2e automatic speech;recognition pre trained;automatic speech recognition;pre trained acoustic;speech recognition;automatic speech;recognition asr autoregressive;acoustic language models;acoustic language model;trained acoustic;outperforms strong wav2vec2;recognition asr;train speech;approach train speech;improve recognition accuracy;acoustic language;wav2vec2 ctc;wav2vec2;wav2vec2 ctc baseline;strong wav2vec2 ctc;fast inference speed;improve recognition;strong wav2vec2;asr autoregressive ar;recognition accuracy keeping;autoregressive nar models", "pdf_keywords": "trained wav2vec2 encoder;wav2vec2 encoder speech;wav2vec2 automatic speech;encoder speech;end speech recognition;pre trained wav2vec2;encoder speech recognition;trained wav2vec2;wav2vec2 encoder;speech recognition improve;wav2vec2 encoder addition;speech recognition fully;speech text representations;state art wav2vec2;train automatic speech;automatic speech;speech recognition models;frame level speech;art wav2vec2 automatic;cnn model end;speech recognition;encoder;speech text modalities;wav2vec2;modality gap speech;level speech input;recognition based deep;art wav2vec2;speech input token;wav2vec2 automatic"}, "4e1d27c68a60bfd8393462107677469bf286f0f8": {"ta_keywords": "pragmatic program synthesizer;pragmatic program synthesis;program synthesis;program synthesis tasks;modeling program synthesis;program synthesis techniques;effectively pragmatic program;program synthesizer;program synthesizer simple;non pragmatic program;build pragmatic program;pragmatic program;synthesizer non pragmatic;examples build pragmatic;program synthesizer non;synthesis techniques construct;synthesis tasks;build pragmatic;synthesis tasks rational;communicate effectively pragmatic;users leave synthesis;construct infer programs;synthesizer simple;output examples build;input output examples;posed programs;synthesizer simple grid;learning input output;effectively pragmatic;approach learning input", "pdf_keywords": "program synthesis;program synthesis received;modeling program synthesis;games program synthesis;program synthesis machine;program synthesis techniques;program synthesis task;pragmatic communication synthesizer;incremental pragmatic;presents incremental pragmatic;recognition program synthesis;incremental pragmatic model;program synthesizers;usable program synthesizers;recursive pragmatics_;synthesis machine learning;reasoning models pragmatics;recursive pragmatics_ work;construct infer programs;synthesis techniques construct;building concept pragmatic;pragmatic model used;pragmatic model;ambiguity recursive pragmatics_;models pragmatics;models pragmatics work;pragmatic communication;human usable program;program synthesizers standard;recursive reasoning models"}, "09e4e0eee756da5658c6d572871130d53a89c72b": {"ta_keywords": "game bayesian persuasion;bayesian persuasion;bayesian persuasion decision;decision subjects strategically;propose game bayesian;persuasion decision maker;predictive model deliberately;optimal bic recommendation;utilized underlying predictive;game bayesian;bic recommendation policy;persuasion decision;situations underlying predictive;strategic feature;decision maker encourage;asymmetries propose game;making strategic feature;recommendation policy;recommendation policy closely;underlying predictive;persuasion;predictive model situations;strategically;predictive;recommendation policy lead;strategic feature modifications;desirable actions;underlying predictive model;encourage desirable actions;desirable actions opacity", "pdf_keywords": "rule infeasible irresponsible;signaling policy decision;policy decision maker;decision rule infeasible;explanations recommendations individuals;signaling policy;automated decisions gdpr;counterfactual explanations;automated decisions;infeasible irresponsible;decisions gdpr motivates;algorithmic recourse;counterfactual explanations opening;optimal signaling policy;behaviors ensuring decision;algorithmic recourse concerned;providing explanations recommendations;consequences partial transparency;transparency commonplace;decision maker power;revealing exact logic;automated decision;explanations recommendations;ensuring decision;infeasible irresponsible paper;policy decision;automated decision making;partial transparency commonplace;box automated decisions;rule infeasible"}, "933b03a81110676f4c61c449f1926ebd58bc47f7": {"ta_keywords": "touchscreens accessible statelens;approach capacitive touchscreens;dynamic touchscreens accessible;touchscreens accessible;touchscreens blind people;interacting dynamic touchscreens;capacitive touchscreens;touchscreens;touchscreens way control;dynamic touchscreens;touchscreens way;inaccessible dynamic touchscreens;accidental touches interface;capacitive touchscreens blind;existing dynamic touchscreens;touches interface;touchscreens blind;touchscreens touchscreens;dynamic touchscreens touchscreens;dynamic touchscreens difficult;touchscreens touchscreens way;touchscreens difficult non;touchscreens difficult;accessible statelens mobile;screens easy accidentally;exploring screen;actions exploring screen;screens easy;screen;interface actions exploring", "pdf_keywords": "state art touchscreens;approach make touchscreens;make touchscreens;art touchscreens increasingly;explore touching screen;dynamic touchscreens accessible;touchscreens increasingly;art touchscreens;existing dynamic touchscreens;touchscreens;dynamic touchscreens;users explore touching;touchscreens increasingly popular;touchscreens accessible blind;touchscreens accessible;make touchscreens accessible;users explore buttons;screen present;gesture;accessible blind users;crowd computer vision;touching screen \ufb01nger;touching screen;present crowdsourcing;screen;screen \ufb01nger;touchscreens accessible wide;blind users;crowd computer;users requiring sighted"}, "d462eae8dd5c1415e03651b9fc1c2ca80a69521f": {"ta_keywords": "automated tutors like;automated tutors;tutors like;tutors like sim;tutors;student learns;authoring automated tutors;student learns learn;video student learns;student behavior human;learns;human student behavior;learns learn;rules students;dynamics video student;human student;bouncer learned learn;bouncer motion student;students;production rules students;discover student model;rules students summer;predicts human student;learn action bouncer;method learning production;learning production;learning production rules;learns learn swim;motion student;student behavior", "pdf_keywords": ""}, "730e5e83586dd5784051f933e7bb82571cec4c94": {"ta_keywords": "separation speaker counting;diarization speech separation;processing separated speech;speech separation methods;audio separation;separated speech signal;speech separation speaker;speech separation;audio separation propose;domain audio separation;separation speaker;neural speaker diarization;speaker diarization encoder;end neural speaker;speaker counting performance;performs speaker diarization;position speaker eei;end speaker diarization;speaker eei consists;speaker counting;neural speaker;diarization separation performance;separated speech;speaker counting proposed;speaker diarization;speaker eei;signal speech activity;number speakers postprocessing;position speaker;estimating separation", "pdf_keywords": "separation speaker counting;speaker counting tasks;diarization speech separation;speech separation speaker;improve speaker counting;speech separation methods;speech separation;speaker counting;activity speaker diarization;end speaker diarization;separated speech signals;separation speaker;speaker counting performance;speaker diarization;datasets improve speaker;speakers datasets improve;speakers diarization;estimate speaker label;predict number speakers;speakers datasets;speaker label sequence;mix speakers diarization;speaker diarization speech;number speakers diarizing;speaker speakers datasets;separated speech;speakers diarization module;diarization separation performance;speakers speaker counting;estimate speaker"}, "1548142a6be92f41e45dcbde9ff8afd71134ac1d": {"ta_keywords": "air pollution level;aromatic hydrocarbons pahs;pollution level median;air pollution;air fluorescence sources;pollution level;aromatic hydrocarbons;polycyclic aromatic hydrocarbons;study pollution level;lung cancer risk;hydrocarbons pahs vicinity;vicinity air pollution;air fluorescence;pollution level polycyclic;hydrocarbons pahs;mean concentrations hhs;level polycyclic aromatic;study pollution;pollution;polycyclic aromatic;lung cancer;fluorescence sources;pahs vicinity air;concentrations hhs;art air fluorescence;fluorescence sources 1_;aromatic;study study pollution;mean concentrations;hydrocarbons", "pdf_keywords": ""}, "f48792e8a24e369c80e39a2a2b7451d108f02941": {"ta_keywords": "web navigation based;web navigation;navigation efficient web;web navigation efficient;efficient web navigation;web navigation presence;navigation based idea;navigation based;idea web navigation;navigation;navigation efficient;neural networks ane;efficient web;friendly interface web;artificial neural networks;artificial neural;navigation presence;interface web;approach artificial neural;neural networks;web end;problem web navigation;navigation presence congestion;approach problem web;problem web;web;interface web end;based idea web;idea web;congestion", "pdf_keywords": ""}, "642c85d35b4a3cc9648b269e32fe9d0a18907c98": {"ta_keywords": "level speech separation;speech separation css;evaluation separated speech;speech separation;task separate speech;separated speech;separate speech sources;separate speech;recorded multi talk;segment long recording;multi talk dataset;speech sources long;utterance level speech;level speech;long recording;overlapped recording involves;utterance level;speech sources;conventional utterance level;talk dataset consistent;talk dataset;overlapped recording;recording involves varying;partially overlapped recording;separation css task;recorded multi;recording involves;extension conventional utterance;recording;multi talk", "pdf_keywords": "performance speech separation;speech separation significantly;channel speech separation;speech separation;speech separation recent;convolving clean utterance;multi channel speech;speech recognition multi;long recording separation;clean utterance simulated;recognition multi talker;consistently automatic speech;recording separation;short utterance streams;automatic speech;utterance streams meeting;utterance simulated room;recording separation achieved;utterance streams;channel speech;better separation performance;speech recognition;utterance simulated;multi talker;separation performance computational;separation performance;clean utterance;segmented short utterance;automatic speech recognition;performance speech"}, "acf0ccc8b67cc441c51d4281c305359073b9c7cc": {"ta_keywords": "translation speech transcription;speech translation evaluation;translation speech text;trained speech transcription;speech translation based;end speech translation;speech transcription;transcription text translation;speech transcription text;loop speech translation;speech translation;speech translation speech;text speech;text translation components;translation speech;approach translation speech;text speech recognition;network trained speech;speech text speech;trained speech;translation components;speech text;speech transcription parameters;translation based espnet;speech recognition;translation evaluation 2016;text translation;translation evaluation;speech recognition systems;transcription text", "pdf_keywords": ""}, "9cfc4e94e76d8025cd86d6652a641b1440681d28": {"ta_keywords": "combinatory categorial grammar;categorial grammar;categorial grammar ccg;grammar ccg lexicon;sentences nouns arguments;algorithm induces language;combinatory categorial;language specific combinatory;linguistic;induces language;grammar ccg;sentences nouns;nouns arguments report;verbs roots sentences;roots sentences nouns;ccg lexicon based;specific combinatory categorial;grammar;nouns arguments;induces language specific;verbs;verbs roots;lexicon based;ccg lexicon;linguistic principles;lexicon based small;roots sentences;sentences;nouns;induction algorithm induces", "pdf_keywords": ""}, "d0a6b70c9dc1942169f48211d47843732c57a3a9": {"ta_keywords": "learning generalized navigation;agnostic multitask learning;navigation model trained;vision language navigation;multitask learning existing;training navigation;trained vision language;multitask navigation model;language grounded navigation;introduce multitask navigation;multitask learning;training navigation model;method training navigation;agnostic representations navigation;multitask navigation;multitask learning significantly;representations navigation policy;perspectives introduce multitask;generalized navigation model;vision language;seamlessly trained vision;navigation dialog;training generalizing better;generalized navigation;representations navigation;grounded navigation;grounded navigation photo;agnostic multitask;navigation dialog history;learn environment agnostic", "pdf_keywords": "environment agnostic learning;environments multitask learning;indoor navigation agent;unseen environments multitask;reinforcement learning aid;indoor navigation tasks;learning model indoor;navigation agent;especially indoor navigation;navigation tasks generalization;actions unseen environments;navigation agent usually;presents deep neural;multitask learning;deep neural;environments propose latent;model indoor navigation;indoor navigation;learning aid agent;generalized navigation policy;agnostic learning;multitask learning notion;environment agnostic representation;latent environment agnostic;better unseen environments;multitask learning fundamental;agnostic learning leads;unseen environments;learning model speech;navigation tasks"}, "cdf17da4a7638985cb62a5dbf1161239b315eb85": {"ta_keywords": "topic models;models topic models;topic models improve;jointly modeling links;entity link modeling;link modeling jointly;modeling links text;link modeling;text entities linked;links text entities;modeling links;entities linked;publications annotated proteins;modeling jointly modeling;induced topics;category prediction proteins;jointly modeling;inspecting induced topics;induced topics understand;text entities;entity link;model datasets protein;annotated proteins;topics;link text information;pairs entities frequently;entities frequently;modeling jointly;topics understand;entity entity link", "pdf_keywords": ""}, "c0484ac1677b942e8b06ea0ac3cad5b01e52ced4": {"ta_keywords": "probability given quantum;quantum state entangled;state entangled;state entangled rest;entangled;entangled rest;given quantum state;quantum state;given quantum;quantum;answer question probability;question probability;probability;question probability given;probability given;state;answer question;approach answer question;new approach answer;rest;paper propose new;given;approach answer;paper propose;paper;approach;answer;question;new approach;propose new approach", "pdf_keywords": ""}, "092b80cc6250f74a2c1e0ba7820c31a8f0153c0a": {"ta_keywords": "literary corpus;literary corpus use;similarity judgments;work literary corpus;literary texts;similarity judgments generated;literary works;literary texts pendulum;analysis literary texts;similarity;analysis literary;groups literary;literary;collection literary;collection literary works;cluster embeddings;approach similarity judgments;thematic groups literary;literary works italo;method collection literary;groups literary scholars;texts pendulum driven;methods cluster embeddings;cities present novel;literary scholars;work literary;novel invisible cities;corpus use unsupervised;texts pendulum;descriptions imaginary cities", "pdf_keywords": "humanities text representation;literary analyses;literary criticism based;criticisms text representation;computational methods literary;literary criticism;literary criticism used;literary criticisms text;literary critics;consensus literary critics;approach literary criticism;methods literary criticisms;literary analyses remains;analyzing novels;digital humanities text;structures literary criticism;challenge digital humanities;literary criticisms;consensus literary;useful analyzing novels;humanities text;analyzing novels conventional;methods literary;literary critics single;aid literary analyses;methods aid literary;literary;dif\ufb01cult consensus literary;text representation methods;criticisms text"}, "63cd8df0041638b0aa74834a81f99ff136951ff1": {"ta_keywords": "gan binary neurons;train gan binary;binary neurons gradient;binary network neurons;gan binary;binary neurons;binary neurons output;neurons propose generative;network gan;generate binarized mnist;generative adversarial network;neurons gradient estimators;generative adversarial;gan uses binary;propose generative adversarial;uses binary neurons;adversarial network gan;binarized mnist digits;possible train gan;train gan;mnist digits binary;adversarial network;binary valued predictions;neural network exhibits;network neurons;generative;neurons gradient;network gan uses;network neurons propose;mnist digits", "pdf_keywords": "neural networks generative;adversarial model generate;networks generative;adversarial model generating;generative adversarial model;generative adversarial;networks generative adversarial;adversarial networks policy;gradients binary neurons;novel generative adversarial;generative adversarial networks;learns binarize generator;binary neurons train;propose generative adversarial;learning network binary;binary valued predictions;use binary neurons;generative;estimate gradients binary;network binary neurons;adversarial;novel generative;network learns binarize;gradients binary;learns binarize;binary neurons;binary neurons deterministic;adversarial model;adversarial networks;gradient machine translation"}, "655b842ae905756b2949758bd7e52e5fd32c3642": {"ta_keywords": "skimming track speech;track speech recognizers;decoder search skimming;continuous speech recognition;track speech;search skimming tracks;speech recognizers;speech recognition;recognition speech recognition;recognition speech;speech recognition lvcsr;search speed decoding;search skimming;speech recognition signal;algorithm recognition speech;hypotheses pruned decoding;minimization large vocabulary;speech recognizers employ;new decoder search;decoder search;skimming track;viterbi beam search;beam search;beam search based;skimming tracks skimming;skimming tracks;search algorithm recognition;tracks skimming;beam search speed;vocabulary continuous speech", "pdf_keywords": ""}, "28421c7f28adfb9ab8aeb56c196ac3ba326efdbb": {"ta_keywords": "external force controlled;fluid dynamics video;fluid dynamics;force controlled;external force;motion rigid;force controlled changing;action external force;strength external force;motion rigid body;rigid body action;dynamics;dynamics video;rigid body;video motion rigid;dynamics video motion;force;motion;rigid;controlled changing strength;video motion;body action external;strength external;changing strength external;fluid;controlled changing;action external;controlled;body action;changing strength", "pdf_keywords": ""}, "7c72e63aa112193590861887c5d03b640ce90911": {"ta_keywords": "international table;international table table;place international table;report results competition;table;table1;table table;results competition;results competition place;table table1;competition place international;table table table1;report results;competition place;competition;results;international;place international;report;place", "pdf_keywords": ""}, "3a6334953cd2775fab7a8e7b72ed63468c71dee7": {"ta_keywords": "skills training audiovisual;features audiovisual;automated social skills;features audiovisual features;training audiovisual;audiovisual features;training audiovisual information;considering audiovisual features;audiovisual features regarding;audiovisual features automated;human social skills;features automated social;users social skills;social skills training;social skill training;audio features audiovisual;using audio features;features human social;social skills;training using audio;social skills people;skills people social;social skills trainers;social skill;effectiveness social skill;simulating social skills;audio features;audiovisual information;evaluate social skills;considering audiovisual", "pdf_keywords": ""}, "4e9328b2801e158647dff69606ed47d47045eca8": {"ta_keywords": "hyponyms based;human hyponyms based;analyzing human hyponyms;hyponyms based use;data oriented;unified data;propose unified data;data oriented platform;machine learning existing;unified data oriented;users interactively analyze;interactively analyze;machine learning;interactively analyze characteristics;data;human hyponyms;data processing;hyponyms;liquid crystal;feature;data provides;based use feature;use feature;interactively;platform web;use feature functions;data released web;data processing operations;data interpret;python", "pdf_keywords": "gender bias dataset;semantic dataset analysis;dataset analysis semantic;semantic dataset;semantic dataset search;analysis semantic dataset;semantic dataset retrieval;new semantic dataset;gender bias;dataset search;combines semantic dataset;dataset retrieval;bias dataset;dataset retrieval framework;degree gender bias;dataset;data oriented;dataset analysis;measure degree gender;data oriented platform;degree gender;gender;dataset search paper;uni\ufb01ed data oriented;data analysis machine;analysis semantic;data mining data;data mining;bias analysis scenarios;mining data analysis"}, "1e9771a264334c45020421b1c847f6bcd88adc60": {"ta_keywords": "topology annotations deep;annotations deep learning;complex networks deep;annotations deep;networks deep;networks trained;topology annotations;active contour models;deep neural;preserving topology annotations;contour models;deep learning;networks trained potentially;contour models preserve;neural networks trained;deep learning powerful;annotations outperform;annotation inaccuracies competition;networks;deep neural networks;networks deep neural;deep;annotation inaccuracies;inaccurate annotations outperform;annotations outperform state;neural;precisely delineating 3d;active contour;structure complex networks;annotations", "pdf_keywords": "network voxelwise annotated;voxelwise annotated centerline;training network voxelwise;accuracy easily voxels;voxelwise annotated;computer vision deep;vision deep networks;vision deep;tracing neurons;easily voxels;network voxelwise;deep networks;easily voxels true;deep networks deliver;tracing neurons blood;voxels true centerline;propose neural layer;computer vision;centerline propose neural;train artificial neural;neural;automatically training network;neural layer;voxels;neural network;problem tracing neurons;voxelwise;delineate simple contours;training network;microscopy images"}, "80111013916dae3306316c34e13fe856cb08b87b": {"ta_keywords": "epistasis logic programming;based intuitionistic logic;intuitionistic logic proposed;intuitionistic logic;logic programming language;concept epistasis logic;language based intuitionistic;logic programming;analysis human reasoning;epistasis logic;human reasoning based;reasoning based concept;human reasoning;goal directed interpreter;based intuitionistic;reasoning based;logic proposed;inheritance hierarchies based;directed interpreter;logic;interpreter;inheritance network;logic proposed solve;inheritance hierarchies;generalization inheritance hierarchies;propose generalization inheritance;intuitionistic;directed interpreter paper;hierarchies based;reasoning", "pdf_keywords": ""}, "69e8c4327193af4549c06809c821c99deb4022cd": {"ta_keywords": "distributed storage coding;distributed storage codes;codes distributed storage;storage codes distributed;storage coding;distributed storage;propose distributed storage;storage coding problem;class distributed storage;storage codes;phase diagram quantum;reconstructing topological phase;stored nodes network;codes distributed;topological phase diagram;diagram quantum;dft propose distributed;storage;topological phase;stored nodes;quantum order phase;diagram quantum order;minimize repair bandwidth;data stored nodes;repair bandwidth dft;quantum;bandwidth dft propose;distributed;reconstructing topological;repair bandwidth", "pdf_keywords": ""}, "a77643bff6f50ccc4f80ec081e4d078a2e788ae7": {"ta_keywords": "multilingual pretrained representations;multilingual representations improves;trained multilingual representations;pre trained multilingual;multilingual representations;view subword regularization;trained multilingual;multilingual pretrained;xtreme multilingual benchmark;multilingual benchmark;existing subword regularization;rely subword segmentation;shared multilingual vocabulary;multilingual benchmark mvr;subword regularization methods;transfer multilingual pretrained;lingual transfer multilingual;subword regularization;subword regularization mvr;subword segmentation;multilingual vocabulary;segmentation especially languages;subword segmentation algorithms;results xtreme multilingual;transfer multilingual;shared multilingual;multilingual;xtreme multilingual;effectiveness cross lingual;cross lingual", "pdf_keywords": "multilingual prediction tasks;benchmark multilingual prediction;view subword regularization;multilingual prediction;subword regularization;multilingual pretrained representations;subword segmentation;rely subword segmentation;subword regularization mvr;shared multilingual vocabulary;subword segmentation algorithms;investigate subword segmentation;multi view subword;shared subword vocabulary;multilingual vocabulary;pretrained language models;multilingual models;segmentation problem multilingual;benchmark multilingual;segmentations evaluate multilingual;tasks multilingual pretrained;multilingual representations;subword segmentation problem;xtreme benchmark multilingual;view subword;subword vocabulary jointly;multilingual pretrained language;multilingual pretrained;languages tasks multilingual;evaluate multilingual representations"}, "bf50833a46839d3932663b472d6145418f9d0bd6": {"ta_keywords": "microphone arrays;multiple microphone arrays;neuroimaging systems microphone;microphone arrays achieved;systems microphone arrays;end speech recognition;microphone arrays acting;attention networks stream;speech recognition asmr;speech recognition;encoder decoder architecture;attention networks;microphone;encoders;regular attention networks;encoder;networks stream attention;multiple microphone;using multiple microphone;multi pet neuroimaging;recognition asmr;stream attention;systems microphone;automatic speech recognition;encoder decoder;pet neuroimaging systems;informative encoders;recognition asmr using;limitation automatic speech;network multi array", "pdf_keywords": "attention fusion stream;speech recognition multiple;attention fusion;hierarchical attention fusion;stream architecture speech;attention based multi;speech features single;multi array encoder;array microphone arrays;microphone arrays;array microphone;microphone array;mapping speech features;stream attention based;distributed microphone array;array encoder architecture;speech features;architecture speech recognition;recognition multiple encoders;attention based eeg;microphone array microphone;encoder mapping speech;stream attention achieved;combination hierarchical attention;microphone arrays acting;single deep neural;array corpora encoder;multi array corpora;eeg framework multi;speech recognition"}, "d6e21619df572d04b2b2d97b4c5d1fd604f185fb": {"ta_keywords": "syntactically supervised transformer;neural machine translation;synst decodes sentences;predicts chunked parse;predicted parse;machine translation;conditioned predicted parse;predicted parse present;decoder computations training;propose syntactically supervised;syntactically supervised;chunked parse;decodes sentences;supervised transformer synst;speed deteriorates translation;machine translation based;synst decodes;autoregressive decoding;chunked parse tree;improves inference speed;parse;sentences 5x faster;deteriorates translation quality;semi autoregressive decoding;parallelize decoder computations;decodes sentences 5x;parse tree generating;transformer synst autoregressively;synst autoregressively predicts;autoregressive decoding produce", "pdf_keywords": "machine translation syntactic;machine translation rare;machine translation investigate;machine translation combines;machine translation;model machine translation;translation chart parsing;augmented machine translation;scaffolds machine translation;translation combines deep;machine translation chart;parsing promising;learns predict chunk;parsing promising approach;translation syntactic;autoregressive decoding steps;translation syntactic expressivity;semi deterministic chunking;token decoder learns;approach machine translation;syntax augmented machine;search parse decoder;translation rare words;deep autoregressive model;combines deep autoregressive;deep autoregressive;chart parsing promising;autoregressive decoding;parse decoder;deterministic chunking"}, "a5f42552b2368a587aea0a81175b4a79aa614601": {"ta_keywords": "filtering concept learning;web based extraction;completely automated simple;automatic cancellations;automated simple;concept learning simple;learned classifiers;automatic cancellation;learning systems;effective automatic cancellation;automatic control complex;simple effective automatic;machine learning systems;learned classifiers difficult;classifiers;specifically collaborative filtering;concept learning;automatic cancellation cf;automatic cancellations arcs;classifiers difficult;automatic control;completely automated;web data certain;learning systems specifically;effective method learning;learning simple;collaborative filtering;rate learned classifiers;method learning;machine learning", "pdf_keywords": ""}, "0e61536550b7263d67b2928473355171dc37c0ae": {"ta_keywords": "nanotube cnt structural;walled carbon nanotube;carbon nanotube cnt;carbon nanotube;nanotube cnt;cnt structural properties;cnt structural;single walled carbon;nanotube;walled carbon;properties single walled;structural properties single;cnt;carbon;single walled;structural properties;relationship structural properties;structural;properties single;walled;relationship structural;investigate relationship structural;properties;single;paper investigate relationship;relationship;paper;paper investigate;investigate relationship;investigate", "pdf_keywords": ""}, "7f0dbd30dc839fd95ea953a9229c879396ca11c0": {"ta_keywords": "learning semantic;learning semantic parsers;symbolic knowledge base;semantic parsers denotations;representing symbolic knowledge;semantic parsers;parsers denotations;parsers denotations kkf;semantics kb expressive;semantics kb;sparse matrix reified;symbolic knowledge;matrix reified kb;original semantics kb;matrix reified;faster naive sparse;knowledge base kb;method learning semantic;semantics;knowledge base;reasoning presented representation;kb scalable neural;semantic;naive sparse;parsers;faster naive;denotations kkf;faithful original semantics;inferences scalable;denotations", "pdf_keywords": "neural semantic parsers;neural semantic parsing;learning neural semantic;architectures neural semantic;semantic parsers denotations;learning semantic parsers;semantic parsers;reasoning synthetic tasks;semantic parsing denotations;tasks learning semantic;learning semantic;symbolic kb neural;neural semantic;semantic parsing;parsing denotations cascades;parsers denotations large;symbolic knowledge base;representing symbolic knowledge;parsers denotations;architecture reasoning symbolic;parsing denotations;reasoning symbolic kbs;kb neural;reasoning symbolic;simpler architectures neural;learning complete kb;denotations large kb;symbolic knowledge;kb reasoning model;knowledge base kb"}, "c2c6c9947dc9d28bb4fc6f965310be517f4d8c57": {"ta_keywords": "synthesize shape messages;descriptions realize shape;voxel based shape;shape messages natural;shape synthesis based;based shape synthesis;shape synthesis build;shape synthesis shapes;shape synthesis;shape messages;realize shape synthesis;voxel based;guide shape synthesis;shape synthesis work;voxel;method voxel based;classifier new structure;text color shapes;method voxel;synthesis shapes;natural language descriptions;shapes different labels;shapes performing realistic;based shape;synthesize shape;novel method voxel;approach synthesize shape;classifier;shapes;synthesis complex shapes", "pdf_keywords": ""}, "81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85": {"ta_keywords": "lm machines know;language models lmms;accurately estimate knowledge;discovering better prompts;language models;lmr automatically discovering;knowledge contained lmr;machines know prompts;machines know;knowledge contained lm;shown language models;estimate knowledge;know prompts;automatically discovering;automatically discovering better;lm machines;lama benchmark demonstrate;accurately predicting correct;accurately predicting;know prompts usually;bound lm machines;lm does know;estimate knowledge contained;lama benchmark;experiments lama benchmark;predicting correct;know given prompt;bound estimate knowledge;result accurately predicting;better prompts", "pdf_keywords": "answer prediction tasks;generating diverse contextualized;unsupervised answer prediction;result diverse prompts;answer prediction;automatically generating diverse;machine translation;diverse prompts;machine translation experimenter;diverse contextualized word;diverse prompts covering;diverse contextualized;factual knowledge retrieval;ability machine translation;knowledge retrieval;contextualized word representations;knowledge retrieval accuracy;prediction tasks;approach automatically generating;ensembling multiple prompts;prediction tasks ensembling;extracting knowledge regarding;extracting knowledge;word representations based;generating diverse;retrieval accuracy outperforming;word representations;contextualized word;automatically generating;directions incorporating knowledge"}, "2c5a410b781f90c145efac05fea235c5c3e44861": {"ta_keywords": "self supervised speech;expensive supervised speech;speech recognition self;supervised speech;swarm based speech;organized speech recognition;supervised speech representation;speech representation s3r;voice conversion framework;open source voice;speech representation;self organized speech;speech representation adopted;self supervised;propose self supervised;speech recognition ssc;voice conversion;speech recognition;source voice conversion;based speech recognition;self swarm based;expensive supervised;source voice;based speech;voice conversion vc;supervised;recognition self organized;voice;organized speech;self swarm", "pdf_keywords": "voice conversion framework;source voice conversion;voice conversion;voice conversion vc;models transfer spoken;open source voice;languages results voice;transfer spoken content;source voice;voice based waveform;speaker dataset;multi speaker dataset;vav2vec synthesizers predict;wav2vec trained;results voice based;transfer spoken;vq wav2vec trained;vav2vec synthesizers;voice;spoken content languages;speaker dataset paper;best performance crosslingual;performance crosslingual;utterance multi speaker;wav2vec;voice based;toolkit vq wav2vec;spoken content;results voice;synthesizers predict outcome"}, "3db9649f2ae986cac13f3e748375f8802f9b07fc": {"ta_keywords": "pruning translations languages;low resource languages;pruning translations;machine translation;resource languages;machine translation introduce;resource regimes sparsity;translations languages;regimes machine translation;resource languages understood;magnitude pruning translations;resource resource languages;infrequent ones compression;compression data limited;translations languages yoruba;memorization low;german low resource;performance frequent sentences;low resource resource;limitations compute resource;curbing memorization low;term low resource;low resource;resource languages paper;capacity generalization data;compression;resource constraints;allocation low resource;translations;memorization low frequency", "pdf_keywords": "lowresourced machine translation;translation models trained;machine translation widely;machine translation models;machine translation experiments;neural machine translation;translation models;trained translate low;translate low resource;pruning translations english;machine translation;pruning translations;translation experiments;distinct english lowresourced;models trained translate;trained translate;task machine translation;high resourced language;translation experiments magnitude;translation widely;english lowresourced machine;data african languages;translations;performance deep neural;performance deep;english lowresourced;sparsity performance deep;resource african languages;resourced data african;translation widely used"}, "12442420adf1c36887fafd108f4b7f4fc822ae60": {"ta_keywords": "benchmarks noisy self;supervised methods self;noisy self training;methods self training;semi supervised;inputs self training;simplest semi supervised;sequence generation tasks;summarization benchmarks noisy;self training perturbation;self training extensively;predictions similar unlabeled;unclear self training;predictions unlabeled;semi supervised methods;mechanism self training;self training noise;training perturbation hidden;machine translation;predictions unlabeled data;self training;machine translation text;supervised;unlabeled data improve;sequence generation;self training works;critical self training;induced self training;incorrect predictions unlabeled;summarization benchmarks", "pdf_keywords": "supervised machine translation;summarization semi supervised;learning machine translation;machine translation text;machine translation;disambiguation machine translation;like machine translation;machine translation studied;machine translation word;selftraining semi supervised;translation text summarization;text summarization semi;machine translation mt;summarization semi;sequence generation tasks;experiments machine translation;semi supervised;revisit semi supervised;semi supervised learning;deep generative;semi supervised machine;learning deep generative;unsupervised learning deep;deep generative models;translation text;disambiguation machine;trained labeled data;output natural language;unannotated data;summarization demonstrate effectiveness"}, "299ab255f3d940a20891128dfa9e0736d74a936c": {"ta_keywords": "imitation learning;robust existing attention;representations downstream tasks;imitation learning modern;manner imitation learning;agent vision;agent vision work;representations learned quickly;existing attention mechanisms;representations learned;perceptual systems robotics;attention;building perceptual systems;vision architectures require;goal specific representations;vision architectures;building perceptual;specific representations learned;attention mechanisms;computer vision pipeline;attention mechanisms domain;vision pipeline;simulated item retrieval;learning modern vision;existing attention;end manner imitation;custom representations downstream;irrelevant agent vision;imitation;custom representations", "pdf_keywords": "learning attention;robust existing attention;attention based models;presents deep convolutional;attention;approach learning attention;multimodal representations goal;learning attention based;attention based;existing attention mechanisms;learning multimodal representations;perception scene clutter;deep convolutional;visual perception scene;cnn model visual;multimodal representations;deep convolutional neural;goal information visual;perception scene;attention mechanisms;convolutional neural;learning multimodal;early visual pipeline;network cnn;scene clutter;representations learned quickly;existing attention;ef\ufb01cient learning multimodal;learning early fusion;cnn"}, "5c6ff5836639e87e8afeaad47e64d0e2234566e8": {"ta_keywords": "fake news detection;news detection;news detection utilize;evidence level attention;news detection methods;word level attention;focused word evidence;false alarm detection;word evidence level;evidence aware fake;alarm detection experiments;aware fake news;word evidence;detection utilize evidence;false alarm;detection experiments real;alarm detection;trend fake news;positives false alarm;level attention;existing evidence aware;level attention result;explanation word evidence;evidence aware;evidence level;detection experiments;attention;alarm;attention result;evidence external sources", "pdf_keywords": "attention evidenceaware fake;fact check textual;evidenceaware fake news;fake news detection;fact checking systems;disinformation fake news;evidence fact checking;biased news misleading;attention evidenceaware;fact checking process;word attention evidence;evidence attention evidenceaware;fact check claims;evidence attention;fact checking;attention evidence attention;proliferation biased news;attention evidence;biased news;news misleading claims;check textual claims;in\ufb02uences fact checking;evidenceaware fake;textual claims source;disinformation fake;textual claims;claims disinformation fake;news detection;focus fact check;developed fact check"}, "963c62b7c4b44ff1fe6aa1f45fa8a7d62b3d5051": {"ta_keywords": "textual entailment trained;knowledge textual entailment;background knowledge textual;approach retrieval;approach retrieval information;novel approach retrieval;retrieval information;retrieval information open;entailment trained sciail;knowledge textual;textual entailment;retrieval;generic textual entailment;textual entailment paper;challenge dataset contains;textual;question queries used;retrieved evidence answer;entailment trained;rewriting background knowledge;generic textual;question queries;retrieve supporting text;text large corpus;retrieved evidence;entailment paper;entailment paper present;support retrieved results;questions authored grade;evidence answer candidates", "pdf_keywords": "answer answering systems;question answering;question answering systems;answering systems paraphrase;crowdsourced answer answering;answer answering crowdsourced;answering systems based;answer natural language;answer selection based;generation answer selection;approach answer answering;answer answer selection;answer answering;answering systems;answer selection;answering crowdsourced;natural language queries;answering crowdsourced answer;applied question answering;machine comprehension machine;comprehension machine learning;query generation answer;neural sequence labeling;paraphrase based approaches;systems paraphrase based;reformulation machine comprehension;crowdsourced answer;machine translation;lexicon machine translation;machine comprehension"}, "e28b9bc26f5f7eb3b0532d823713400202372da2": {"ta_keywords": "actor critic algorithms;algorithm actor critic;critic algorithms popular;critic algorithms;actor critic formulations;stackelberg actor critic;learning algorithm actor;critic algorithms leader;known stackelberg game;framework stackelberg actor;stackelberg game;policy gradient;algorithm actor;interaction actor critic;develop policy gradient;critic based reinforcement;hierarchical interaction actor;actor critic based;reinforcement learning algorithms;critic formulations;policy gradient theorem;critic formulations propose;critic based;critic actor critic;algorithms leader player;stackelberg actor;actor critic;reinforcement learning;model interaction players;critic actor", "pdf_keywords": "actor critic reinforcement;critic reinforcement learning;critic based reinforcement;learning parameterized policy;critic reinforcement;model based reinforcement;parameters actor critic;learning based stackelberg;reinforcement learning agents;parameterized policy actor;actor critic methods;reinforcement learning interaction;function critic actor;reinforcement learning promising;agent reinforcement learning;structure actor critic;stackelberg game;critic methods bridge;reinforcement learning algorithms;learning interaction actor;reinforcement learning;critic cost structure;value function critic;interaction actor critic;learning agents;based reinforcement;agent reinforcement;multi agent reinforcement;based reinforcement learning;reminiscent stackelberg game"}, "b63bd17d4bb28ba90cc6ff66b51ba5b0377467bf": {"ta_keywords": "language models speech;predicting word sequence;models speech;models speech recognition;recurrent neural network;training recurrent neural;training recurrent;speech recognition training;speech recognition;language models;predicting word;neural network language;method predicting word;describes training recurrent;quantity speech recognition;word sequence;recurrent neural;sequence word based;word sequence word;model word word;network language models;word based probabilistic;sequence word;word error rate;model word;recognition training;speech;word interaction;word word interaction;neural network", "pdf_keywords": ""}, "93d3e45395117e21214d404c8753b578c29266d1": {"ta_keywords": "making evidence retrieval;evidence retrieval challenging;evidence retrieval;data unstructured text;search textual information;textual information database;unstructured text;letter unstructured data;unstructured data;data unstructured;unstructured data used;search textual;question answering;retrieval challenging;answer question answering;documents contain answers;tabular data unstructured;retrieval;analyzing documents;retrieving analyzing documents;analyzing documents contain;textual data competitive;textual data;aided classification ciel;knuth oc tasks;retrieval challenging paper;osc tabular textual;tabular textual data;unstructured text making;classification ciel", "pdf_keywords": "crowdsourced answer answering;model answer answering;answer answering crowdsourced;question answering;text question answering;answering crowdsourced;question answering readers;question answering ott;answer answering;domain question answering;answering crowdsourced crowdsourced;answering readers;strategies fusion retrieval;answering readers expected;fusion retrieval;based crowdsourced answer;novel entity linking;crowdsourced answer;strong reader models;way improve retrieval;fusion retrieval cross;context based crowdsourced;reader models;crowdsourced context;improve retrieval;improve retrieval performance;crowdsourced context based;text corpus retriever;reader models purpose;corpus retriever"}, "a0035379f93e0e95bdadd77a1d8eb27ba89dcf60": {"ta_keywords": "ratings generated stories;generated stories;approach generate story;stories noisy datasets;generate story;generate story noisy;story noisy dataset;generated stories qualitative;stories qualitative feedback;arbitrary number stories;story continuations;language models;story continuations edit;large scale web;lngs language models;suggested story continuations;stories noisy;ratings generated;model suggested story;number stories;user ratings generated;stories qualitative;structured user interviews;language models fine;story noisy;fine tuned dataset;approach generation large;stories;generation large scale;based idea network", "pdf_keywords": "generate narrative elements;collaborative storytelling platform;online collaborative storytelling;collaborative storytelling;story generation;stories generated;narrative elements machine;collaborative storytelling community;computational storytelling systems;generate narrative;story generation present;story generation leverages;approach collaborative storytelling;storytelling leverages machine;dataset stories generated;computational storytelling;collaborative storytelling leverages;storytelling systems;storytelling platform introduce;techniques generate narrative;guiding computational storytelling;stories generated storium;storytelling community provides;storytelling community;loop story generation;methodology story generation;storytelling platform;narrative representations;storytelling systems spur;dataset stories"}, "3e4d80e43346b9538504c0a7ee5562f3c6a09178": {"ta_keywords": "bandit algorithms;approach generation bandit;bandit algorithms based;bandit approach;bandit algorithm;bandit algorithm commonly;traditional bandit algorithm;classical bandit algorithms;bandit approach problem;variants classical bandit;bandit like images;armed bandit approach;demonstrate traditional bandit;generation bandit;classical bandit;generation bandit like;bandit like;traditional bandit;bandit;multi armed bandit;reinforcement learning;class reinforcement learning;reinforcement learning algorithms;armed bandit;new class reinforcement;learning algorithms called;incentives recommendations users;algorithms called learning;class reinforcement;bound epsilon greedy", "pdf_keywords": ""}, "740ecaa7fc1f4fc02116181b1757f03c815c7ea9": {"ta_keywords": "classification based recurrent;multilabel classification;multilabel classification based;approach multilabel classification;based recurrent neural;recurrent neural;novel approach multilabel;multilabel;recurrent neural networks;schedule business decision;based recurrent;approach multilabel;business decision present;business decision outcome;occurrence sudden;classification;business decision;schedule business;occurrence sudden sudden;decision outcome business;sudden change schedule;outcome business decision;neural;recurrent;business;neural networks;relationship occurrence sudden;change schedule business;outcome business;classification based", "pdf_keywords": "recurrent neural networks;recurrent neural network;baseline paper recurrent;recurrent neural;application recurrent neural;learned lstms;paper recurrent neural;novel application recurrent;learned lstms clinical;lstms clinical users;experiments recurrent neural;clinical time series;baseline experiments recurrent;lstms;classify clinical time;time series clinical;application recurrent;lstms clinical;patient similarity search;stochastic gradient descent;logistic regression regularization;distributed representations learned;illness learned lstms;gradient descent;paper recurrent;like patient similarity;search clinical time;neural networks;recurrent;distributed representations"}, "191ef1408406569f0e9a69344add1ae350365431": {"ta_keywords": "characterizing predictive fairness;predictive fairness;audit predictive bias;predictive fairness properties;recidivism prediction task;recidivism prediction;audit predictive;model better fairness;predictive bias;cases recidivism prediction;characterizing predictive;better fairness properties;predict outcome individual;predict outcome;scoring task predictive;used audit predictive;fairness properties algorithmic;proposed predict outcome;fairness properties;predictive models proposed;models proposed predict;algorithmic risk assessments;predictive models;prediction task real;task predictive models;credit scoring task;predictive;framework characterizing predictive;better fairness;prediction task", "pdf_keywords": "fairness de\ufb01nitions logistic;predictive bias reductions;model better fairness;predictive disparities respect;better fairness properties;predictive disparity minimization;predictive bias;predictive disparities;audit predictive bias;fairness de\ufb01nitions;bias reductions;framework predictive disparity;predictive disparity;fairness properties;better fairness;selection bias;inference selection bias;disparity minimization;common notions fairness;bias reductions approach;notions fairness;notions fairness de\ufb01nitions;bias;range predictive disparities;selection bias settings;causal inference selection;disparities respect race;fairness properties audit;fairness;bias settings"}, "9eeaeadc1e0e300337b47d867a314caeae5c10a9": {"ta_keywords": "brain age estimated;brain age;age estimated using;age estimated;mri deep learning;brain magnetic resonance;deep learning dls;imaging mri deep;mri deep;age;brain magnetic;brain;magnetic resonance imaging;deep learning;combination brain magnetic;combination brain;resonance imaging mri;using combination brain;imaging mri;learning dls;resonance imaging;mri;learning;magnetic resonance;imaging;dls;estimated using;deep;estimated;estimated using combination", "pdf_keywords": ""}, "9813446d9545b600de9a4972c1382c5e3b22a351": {"ta_keywords": "modeling intelligent tutoring;student modeling intelligent;intelligent tutoring;intelligent tutoring systems;simstudent student modeling;students present simulation;tutoring systems;tutoring;predict human students;predicted human students;tutoring systems evaluation;student accurately predicted;student modeling;learns cognitive;sim student accurately;student tutor;learns cognitive skills;tutor;sim student machine;agent learns cognitive;use simstudent student;build cognitive model;student tutor interactions;tutor interactions;modeling intelligent;sim student;learning agent;human students;simstudent student;help students learn", "pdf_keywords": ""}, "78dadbfb6710ac65f178b5e12bd975184aae62fe": {"ta_keywords": "calculus variations;elements calculus variations;variations;basic elements calculus;method teaching basic;teaching basic concepts;calculus;basic concepts;elements calculus;basic concepts basic;concepts basic elements;concepts basic;teaching basic;method teaching;basic elements;basic;concepts;new method teaching;method;teaching;present new method;new method;elements;paper present;paper;paper present new;present;new;present new", "pdf_keywords": ""}, "241e890c70f6d013de7fe5e174e061ff824dc5e9": {"ta_keywords": "tutoring simulations student;simulations student tutored;tutoring simulations;tutoring sim student;problems tutoring simulations;tutoring sim;tutored problem solving;tutoring;simulations student;equations tutoring;simulation study students;70 minutes tutoring;equations tutoring sim;minutes tutoring sim;15 problems tutoring;minutes tutoring;teach students dynamics;student tutored;students able solve;learning agent;student tutored problem;problems tutoring;learning agent called;tutored problem;students learn;students dynamics;teaching live machine;students dynamics moving;tutored;game like learning", "pdf_keywords": ""}, "ca00ead4e5ddd14cbbbce03d89a57d14b430e320": {"ta_keywords": "using probability privacy;probability privacy;consumer valuation privacy;probability privacy breach;valuation privacy design;customers smart grid;segment customers electricity;varying guarantees privacy;companies setting privacy;guarantees privacy;valuation privacy;privacy design;segmentation customers smart;privacy paper propose;guarantees privacy paper;customers electricity grid;privacy breach offered;segmentation customers;privacy;method segmentation customers;setting privacy;privacy design screening;privacy paper;segment customers;setting privacy concerns;smart grid;customers electricity;privacy breach;privacy concerns natural;privacy concerns", "pdf_keywords": "privacy metric electricity;privacy economic dispatch;energy disaggregation privacy;probability privacy;based probability privacy;privacy metric;privacy economic;new privacy metric;disaggregation privacy economic;propose new privacy;choose low privacy;low privacy setting;privacy setting;privacy;privacy settings reality;probability privacy breach;new privacy;low privacy;solely based privacy;disaggregation privacy;metric electricity consumers;privacy breach offered;based privacy settings;privacy settings;based privacy;electricity consumers based;electricity consumers;utility companies incentivize;privacy setting helps;privacy breach"}, "83c7335904002d2b7c7cb403f3538703c9a69025": {"ta_keywords": "enhanced speech noisy;nonaudible murmur enhancement;murmur enhancement systems;statistical voice conversion;murmur enhancement;silent speech communication;allows silent speech;voiced speech effective;speech noisy environments;nonaudible spoken spoken;speech communication nonaudible;listens enhanced speech;speech noisy;voice conversion;speech quality intelligibility;intelligible speech noisy;performance nonaudible murmur;nonaudible voice experiments;nonaudible voice;nonaudible spoken;message nonaudible voice;speech quality;voice experiments conversion;communication nonaudible murmur;improves speech quality;statistical voice;voiced speech;enhanced speech;process voiced speech;speech communication propose", "pdf_keywords": ""}, "9f1a1d2cb6b278b7ee24e67d4c2ac38c1161fa1d": {"ta_keywords": "indonesian ethnic speech;data ethnic speech;ethnic speech data;speech translation ethnic;ethnic speech corpora;linguistic data ethnic;translation ethnic languages;analysis indonesian ethnic;ethnic languages;ethnic speech;javanese sundanese multilingualism;ethnic languages english;languages english indonesian;translation ethnic;analysis indonesian;indonesian ethnic;speech data western;sundanese multilingualism east;analyze linguistic data;sundanese multilingualism;speech speech translation;linguistic data;speech translation;multilingualism east west;world javanese;english indonesian;javanese;groups world javanese;multilingualism east;data ethnic", "pdf_keywords": ""}, "5bf7f468b763f181c31a5e1edc57bce9a6dbd00c": {"ta_keywords": "respiratory distress;ventilator use coronavirus;critically ill patients;respiratory distress syndrome;associated respiratory distress;severity air;severity air air;critically ill;highlight critically ill;cases severity air;risk patients critical;risk mortality decompensation;pneumonia high risk;mortality decompensation;airborne pathogen work;ill patients viral;identify risk patients;airborne pathogen;viral unspecified pneumonia;coronavirus;coronavirus disease covid;coronavirus disease;ventilation therapy;patients critical care;critical care patients;airborne airborne pathogen;indicators vasopressor ventilator;pneumonia;mechanical ventilation;unspecified pneumonia high", "pdf_keywords": "risk score endocardial;score endocardial respiratory;endocardial respiratory distress;pneumonia ecmo eligible;severity suspected coronavirus;respiratory distress syndrome;respiratory distress;eligible risk score;coronavirus disease paper;patient risk paper;coronavirus disease;score endocardial;pneumonia escalate critical;acute respiratory disorders;associated pneumonia expected;pneumonia ecmo;ecmo eligible patients;provide risk score;ecmo eligible risk;suspected coronavirus disease;calculation patient risk;risk score based;endocardial respiratory;pneumonia expected better;suspected acute respiratory;risk score;coronavirus;presents risk score;cohort hospitalized patients;pneumonia expected"}, "ea2b138583e587850153f2825fe9e4339aa5f5f9": {"ta_keywords": "diarization single speaker;separation speech diarization;multiple speaker systems;speech diarization single;single speaker approach;speaker systems;multiple speaker;speech diarization;single speaker;approach separation speech;speaker approach extended;speaker approach;extended multiple speaker;separation speech;speaker;diarization single;diarization;speech;novel approach separation;separation;approach separation;systems;multiple;extended multiple;approach extended multiple;single;novel approach;present novel approach;approach extended;approach", "pdf_keywords": ""}, "ca1645abedae3b4caa3345aa8720c8b90f7c37db": {"ta_keywords": "voting rules rank;certain voting rules;rules certain voting;voting rules manipulable;voting rules;voting variety statistical;voting based concept;rank dependent scoring;voting variety;dependent scoring rules;rules rank;certain voting;rules rank dependent;model voting based;scoring rules based;negative scoring rules;simple model voting;rules based ordered;scoring rules certain;model voting;voting based;based ordered weighted;scoring rules;borda voting variety;dependent scoring;manipulable borda voting;family voting rules;ordered weighted average;concept negative scoring;borda voting", "pdf_keywords": ""}, "b2c3d660aaefb80085fe72c80ce81c5fa71980e9": {"ta_keywords": "pivot language words;pivot language tactic;pivot translation approaches;languages pivot translation;distinguishing pivot language;proposed pivot translation;phrase based pivot;languages pivot;subtrees pivot language;pivot language;ambiguities pivot language;syntactic subtrees pivot;semantic ambiguities pivot;pivot translation;combinations languages pivot;language words syntactic;pivot language avoid;pivot translation method;pivot target translation;phrase pairs interlingual;utilizes syntactic subtrees;superior pivot translation;words syntactic;target phrase pairs;syntactic subtrees;translation approaches;phrase pairs;combines phrase based;method translating languages;constituent words produces", "pdf_keywords": ""}, "b7d6829d9eccdbd3d3a5d6f5321a87158588033b": {"ta_keywords": "machine learning fluid;learning fluid dynamics;learning fluid;fluid dynamics videos;learn apparent personality;fluid dynamics video;dynamics videos;learn structure fluid;machine learning;videos humans continuous;dynamics video;dynamics videos used;trait challenge;images videos humans;trait challenge paper;personality trait challenge;apparent personality;videos humans;videos cha learn;based machine learning;machine learning based;learn structure;learning;humans continuous traits;videos;approach machine learning;large scale dataset;learn apparent;videos used learn;dataset", "pdf_keywords": ""}, "47adb249ce8f7f5f1e92112ba0f3757f8fbfbfc3": {"ta_keywords": "estimate semantic similarity;representations semantic similarity;alignment language models;lexical semantic models;semantic similarity;syntactic representations semantic;language models learn;learn term embeddings;semantic models investigated;semantic similarity lexical;similarity lexical semantic;semantic models;word syntactic representations;improves lexical semantic;language models word;models word syntactic;language models;estimate semantic;term embeddings unstructured;semantic models chain;network language models;question answer texts;neural network language;representations semantic;semantic similarity question;similarity lexical;lexical semantic;term embeddings;answer texts;embeddings unstructured text", "pdf_keywords": ""}, "21ac57d41843ac5367e11b8b784aa57f2ef7a1fc": {"ta_keywords": "convex stochastic optimization;stochastic convex optimization;smooth stochastic convex;convex optimization complexity;smooth convex stochastic;stochastic convex;convex stochastic;stochastic optimization;non smooth stochastic;convex optimization;stochastic optimization problems;smooth stochastic;high probability convergence;optimization complexity bounds;optimal non smooth;strongly convex problems;non smooth convex;optimal iteration oracle;optimization complexity;strongly convex;non sub gaussian;highly suboptimal objective;consider optimal iteration;smooth convex;optimal iteration;problems present stochastic;non smooth process;stochastic;method training large;iteration oracle complexity", "pdf_keywords": "words stochastic optimization;stochastic optimization convex;convex functions stochastic;stochastic optimization;optimization stochastic;sequence words stochastic;problem stochastic optimization;stochastic composite optimization;stochastic optimization stochastic;high probability complexity;stochastic approximation;functions stochastic approximation;words stochastic;optimization stochastic composite;probability complexity bound;optimization convex functions;functions stochastic;recurrent neural networks;problem stochastic;stochastic composite;stochastic approximation method;stochastic;probability complexity;consider problem stochastic;bound solving convex;learning recurrent neural;neural networks paper;optimization convex;learning recurrent;probability inequalities sum"}, "6cdff2505560390b28db5a96c2ae3070712077cf": {"ta_keywords": "games gradient descent;gradient games;gradient games gradient;games gradient dynamics;gradient based bandits;called gradient games;games called gradient;games gradient;gradient reinforcement learning;reinforcement learning gradient;quadratic games gradient;policy gradient reinforcement;gradient reinforcement;agents employing gradient;algorithms competitive gradient;policy gradient surely;policy gradient;learning gradient like;competitive gradient;learning gradient;competitive gradient based;including policy gradient;gradient descent;tool learning gradient;results policy gradient;gradient like flows;gradient dynamics;framework competitive gradient;learning converges;learning gradient based", "pdf_keywords": "games analyzing gradient;sum games gradient;games gradient based;games gradient;general continuous games;learning schemes games;gradient gradients deterministic;gradients deterministic;local nash equilibria;concave monotone games;general sum games;continuous games analyzing;gradient dynamics gradient;continuous games;stochastic policies player;gradients deterministic setting;nash equilibria strict;gradient dynamics;nash equilibria;dynamics gradient;dynamics gradient based;sum games;monotone games;deterministic setting gradient;games analyzing;behavior classical optimization;gradient based learning;class gradient based;learning schemes;bilinear games"}, "7301c7aba3c0824b91f69747e7e50f4db56d7fc1": {"ta_keywords": "model speech translation;translation paralinguistic information;translation sensitive paralinguistic;translation paralinguistic;paralinguistic translation performed;words paralinguistic translation;paralinguistic translation;express paralinguistic translation;paralinguistic information simply;paralinguistic information;model speech;spoken words paralinguistic;paralinguistic translation propose;space translation paralinguistic;paralinguistic information duration;speech translation;words paralinguistic;speech translation sensitive;express paralinguistic;sensitive paralinguistic information;paralinguistic;propose model speech;digit translation task;word dependent models;single neural network;sensitive paralinguistic;single neural;digit translation;model words;continuous space translation", "pdf_keywords": ""}, "8837530b23a2d51054d8752ae2f0ffef8998da8e": {"ta_keywords": "location recommendation framework;collective matrix factorization;recommendation framework;recommendation framework works;cross domain recommendation;matrix factorization;domain location recommendation;privacy preserving cross;factorization ccmf effectively;matrix factorization ccmf;domain recommendation typical;domain recommendation;privacy preserving;propose privacy preserving;protecting users privacy;factorization;field location recommendation;introduce privacy preserving;aware collective matrix;privacy preserving mechanism;confidence aware collective;factorization ccmf;users privacy;location recommendation;privacy;locations user assumption;users privacy introduce;alleviate data sparsity;locations user;recommendation typical", "pdf_keywords": ""}, "cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022": {"ta_keywords": "graph text generation;trainable graph text;graph text;generating texts;graph transforming encoder;novel graph transforming;constraints generating texts;knowledge graphs imposing;graphical knowledge representations;structure knowledge graphs;text generation;knowledge graphs;introduce novel graph;generating texts express;text generation techniques;challenges text generation;produces informative texts;structured representation content;information extraction;novel graph;end trainable graph;graph transforming;output information extraction;scientific text automatic;encoder leverage relational;relational structure knowledge;trainable graph;hierarchical constraints generating;sentences requires structured;graphical knowledge", "pdf_keywords": "learns generate texts;graph encoder;coherent text knowledge;dictionary learning known;dictionary learning framework;proposes dictionary learning;using graph encoder;generating coherent text;dictionary learning;generate texts;attention model graph;automatic information extraction;graph encoder setup;generating text;graph encoding;text knowledge;generate texts automatically;automatically extracted knowledge;problem dictionary learning;information extraction;model graph encoding;automatically generating text;sparse coding;attention based encoder;text knowledge requires;knowledge graphs;structure knowledge graphs;text generation;texts automatically extracted;extracted knowledge using"}, "a54019645dd8e9cfd8d71ab60155449307de3d83": {"ta_keywords": "quantum systems crowdsourcing;monetary expenditure crowdsourcing;crowdsourcing technique cheap;expenditure crowdsourcing;crowdsourcing;fundamental challenge crowdsourcing;systems crowdsourcing;crowdsourcing propose simple;challenge crowdsourcing;crowdsourcing reduce;expenditure crowdsourcing technique;data possible incentive;systems crowdsourcing gained;incentive compatible mechanisms;possible incentive compatible;crowdsourcing technique;use crowdsourcing;challenge crowdsourcing propose;way use crowdsourcing;crowdsourcing propose;crowdsourcing gained;mechanism incentive compatible;use crowdsourcing reduce;mechanism incentive;crowdsourcing reduce number;entanglement;incentive compatible;incentive;crowdsourcing gained immense;incentive compatible payment", "pdf_keywords": "crowdsourcing platform;crowdsourcing platform mturk;experiments commercial crowdsourcing;crowdsourcing platforms;crowdsourcing;crowdsourcing means collecting;commercial crowdsourcing platform;greater crowdsourcing;crowdsourcing platforms com;incentive compatible mechanism;crowdsourcing means;incentive compatible mechanisms;commercial crowdsourcing;incentive;numbers crowdsourcing;con\ufb01dences greater crowdsourcing;axiomatic framework incentive;greater crowdsourcing means;large numbers crowdsourcing;mechanism incentivize knowledgeable;compatible incentive;numbers crowdsourcing platforms;incentive compatible;payment compatible incentive;incentivize knowledgeable workers;incentivize knowledgeable;spammers workers answer;framework incentive;earning free;incentive payment"}, "2cc7db7b17ee7349800334b3a154f708850c6410": {"ta_keywords": "reliability archival storage;archival storage systems;data storage systems;data storage;data replication powerful;storage systems;data storage methods;reliability maximum storage;replication powerful erasure;data replication;improved reliability archival;storage efficiency;archival storage;storage methods;storage systems based;study data storage;simple data replication;storage systems understood;codes storing frequently;evolving data storage;maximum storage efficiency;storage efficiency use;reliability archival;storage methods simple;storage;storing frequently;hot data latency;reliability replication;level reliability replication;reliability replication significantly", "pdf_keywords": ""}, "0ff5b1e61bbebd2f077a4ef24c3afdb344e5b3d4": {"ta_keywords": "coupled pendulum oscillators;dynamics coupled pendulum;coupled pendulum;pendulum oscillators;numerical simulation dynamics;simulation dynamics coupled;pendulum;dynamics coupled;oscillators;numerical simulation;simulation dynamics;results numerical simulation;dynamics;simulation;present results numerical;numerical;results numerical;coupled;paper present results;present results;paper present;present;paper;results", "pdf_keywords": ""}, "8e992116bbc8afb075577a30672de7a90fbeba78": {"ta_keywords": "chunkwise attention;performance alternative recurrent;streaming based ablation;speech recognition asr;automatic speech;monotonic chunkwise attention;speech recognition;chunkwise attention mocha;attention propose streaming;alternative recurrent neural;recurrent neural networks;autonomous automatic speech;automatic speech recognition;streaming asr;streaming asr able;alternative recurrent;recognition asr;called streaming asr;recurrent neural;based ablation;ablation process;ablation process completed;based ablation method;ablation;ablation method;neural networks end;source target attention;streaming transformer;streaming transformer algorithm;batch models tasks", "pdf_keywords": ""}, "3163392f56cdffaa009fbc59f299989a1b8baec1": {"ta_keywords": "learning unlabeled data;learning unlabeled;dealing binary classification;unreliable unlabeled data;unlabeled data reliability;classification positive unlabeled;binary classification;unlabeled data unreliable;problem learning unlabeled;robust unreliable unlabeled;approaches binary classification;classification data labeled;labeled class data;unreliable unlabeled;far unlabeled data;classification;labeled data;binary classification data;data unlabeled data;data unlabeled;unlabeled data;data labeled class;unlabeled data unlabeled;2006 unlabeled data;labeled class;classification positive;oc classification;unlabeled pu learning;unlabeled data used;classification data", "pdf_keywords": "svm learning unlabeled;learning positive unlabeled;learning unlabeled;learning unlabeled examples;pu svm learning;svm learning;positive unlabeled examples;examples target recognition;unlabeled examples target;identify unlabeled examples;algorithms leverage unlabeled;automatically identify unlabeled;unlabeled examples;svm;vectors pu svm;positive unlabeled;target recognition;pu svm;leverage unlabeled data;unlabeled examples di\ufb00erent;labeled positive examples;identify unlabeled;unlabeled data;predictive predictive;recognition;new machine learning;machine learning;predictive predictive predictive;unlabeled examples paper;learning positive"}, "73472692b6090a72e36e03127bb99fc2e6bc8de0": {"ta_keywords": "derive networks communities;unsupervised machine learning;identifying groups communities;networks communities socio;networks communities;communities;social network;mixture models affinity;community relationships identifying;communities individual group;communities socio cultural;groups communities;latent network learn;network modularity maximization;structure community relationships;network learn structure;communities socio;socio cultural data;networks;methodology derive networks;unsupervised machine;groups communities individual;analyze networks;derive networks;network modularity;communities individual;analyze networks created;social network based;gaussian mixture models;mapping network modularity", "pdf_keywords": ""}, "2f6843f9345ca56af3fd9df5512daa1e7f80bedf": {"ta_keywords": "imitation learning;entanglement;entanglement context;order entanglement context;structured prediction;topological order entanglement;context quantum computation;order entanglement;entanglement context quantum;imitation learning approach;structure prediction;quantum computation;structured prediction relies;quantum computation present;based imitation learning;xmath0 body momentum;quantum;structure prediction approaches;context quantum;approach structured prediction;state run fermilab;xmath0 body;predictions non decomposable;previous tag predictions;fermilab tevatron;compared structure prediction;fermilab tevatron outperformed;random fields;set based imitation;conditional random fields", "pdf_keywords": ""}, "faa4468f2ad1c7cedaf04bf56ebb20ae4b349952": {"ta_keywords": "language models rnn;short term memory;recurrent neural network;term memory recurrent;memory recurrent neural;long term memory;memory neural network;term memory neural;memory recurrent;language models better;neural network language;experiments recurrent neural;recurrent neural;matrix adaptation cma;memory neural;term memory;language models;models rnn;models rnn lm;models various speech;matrix adaptation;speech recognition tasks;language model;model word history;network language models;speech recognition;language models various;based language models;neural networks tuning;covariance matrix adaptation", "pdf_keywords": ""}, "872c2d9d8b27ff49367854a7cf67b5dff2010406": {"ta_keywords": "fpu bio nlp;nlp 2009 event;bio nlp 2009;bio nlp;event detection task;2009 event detection;event detection;detection task paper;fermi pasta ulam;pasta ulam fpu;nlp 2009;nlp;ulam fpu model;pasta ulam;ulam fpu;detection task;fermi pasta;event;single event;ratio single event;fermi;event designed domain;2009 event;fpu model;fpu bio;xmath0;single event designed;performance fpu bio;xmath0 xmath1;fpu model used", "pdf_keywords": ""}, "c6713071291729386955586c6309778b1637b852": {"ta_keywords": "hvac resource allocation;energy management pricing;dynamic game;dynamic games;agent energy management;dynamic game theory;resource allocation buildings;strategy selfish agents;lqd dynamic games;standard dynamic game;simplification dynamic game;dynamic games promising;control strategy selfish;allocation social dynamically;pricing lqd dynamic;agents context hvac;pricing mechanisms;pricing design;allocation buildings;agent based game;pricing design problem;game cost function;selfish agents;construct pricing design;selfish agents context;resource allocation social;based game cost;construct pricing;use pricing mechanisms;propose neighborhood based", "pdf_keywords": ""}, "d5084f48212bed80e8c11e1e69669deea3ba2f83": {"ta_keywords": "script corpus;rules nlp community;parallel script corpus;rules nlp;script corpus cloze;structure rules nlp;nlp community;corpus cloze task;inducing commonsense;corpora tasks;captions;captions missing procedural;model water fountain;matches video captions;nlp community lacks;video captions;inducing commonsense knowledge;submerged water fountain;fountain water hierarchy;corpus;underwater water fountain;commonsense;fountain introduce simple;nlp;water fountain;lacks corpora tasks;corpora tasks evaluating;commonsense knowledge;water hierarchy rules;simple model water", "pdf_keywords": ""}, "8451d8e20bb9a94c6a576e52ca1a63470f8d2390": {"ta_keywords": "decentralized distributed optimization;distributed optimization;distributed optimization factor;convex composite optimization;order decentralized distributed;bound order decentralized;composite optimization;free method convex;composite optimization problems;decentralized distributed;method convex composite;convex composite;order decentralized;method convex;zeroth order oracle;distributed;optimization;optimization problems includes;bound order;derivative free method;optimization problems;order oracle calls;optimization factor;order oracle;convex;decentralized;state art bound;oracle calls node;free method;optimization factor proportional", "pdf_keywords": ""}, "a6a9c06d138537002aaca79dba359cc320b951df": {"ta_keywords": "bayesian speech language;bayesian speech;bayesian approach speech;language processing bayesian;processing bayesian;processing bayesian approach;based bayesian inference;recognition based bayesian;bayesian inference;based bayesian;speech language processing;approach speech recognition;speech recognition;bayesian;speech recognition based;speech language;bayesian approach;language processing;approach speech;speech;recognition based;processing;language;inference;recognition;approach;based", "pdf_keywords": ""}, "03bbbaa03cb57413c2581cc8dc5cbfa532bbea15": {"ta_keywords": "grade electroencephalogram eeg;electroencephalogram eeg investigate;electroencephalogram eeg;channel electroencephalograms eegs;consumer grade electroencephalogram;electroencephalograms eegs;use electroencephalogram eeg;grade electroencephalogram;user identification accuracy;electroencephalogram;based use electroencephalogram;accuracy user identification;channel electroencephalograms;electroencephalograms;use electroencephalogram;14 channel electroencephalograms;user identification based;electroencephalograms eegs 25;user identification authentication;eegs 25 subjects;analyze brain waves;demonstrate user identification;user identification;identification accuracy significantly;identification authentication;identification accuracy;identification based;eegs;dimensionality reduction technique;brain waves", "pdf_keywords": ""}, "73e1dcf5f0f3cf4e645b0bba62d9b1e2ef47b706": {"ta_keywords": "stochastic semantic parsing;semantic parsing;semantic parse;semantic parse tree;say semantic parse;lexical class identification;computer lexical class;semantic parsing main;lexical class;lexical classes;implementation lexical class;human computer lexical;lexical classes appropriate;free grammars parsing;parsing;based stochastic semantic;semantic analysis funda;stochastic semantic;implementation lexical;grammars parsing;computer lexical;semantic analysis;lexical class single;grammars parsing methods;speech recognition;parsing methods;speech recognition algorithm;parsing methods goal;semantic analysis based;lexical", "pdf_keywords": ""}, "122b75042daae44f93153dedda15b0fb11b3f279": {"ta_keywords": "answering qa datasets;question answering;question answering qa;popular question answering;learning qa datasets;question answering make;task question answering;question answering paper;answering qa;qa datasets squad;qa datasets evaluate;learning qa;shortcomings datasets evaluation;datasets evaluate models;qa datasets;outperform humans task;domain examples responses;really learning qa;answering;examples responses;datasets evaluation;datasets squad outperform;answering paper;answering make recommendations;datasets evaluation methods;models really learning;shortcomings datasets;questions teach;identify shortcomings datasets;datasets squad", "pdf_keywords": "question answering models;answering models;answering models used;comprehension qa datasets;answer answering models;question answering reading;question answering;answering reading comprehension;answer based models;performance question answering;answer negation questions;reading comprehension qa;answer answering;questions datasets analyzing;models answer negation;answering reading;impossible questions datasets;qa datasets models;reading comprehension including;learn reading comprehension;task reading comprehension;questions datasets;work question answering;reading comprehension;comprehension including;answer test;reading comprehension work;answer test set;comprehension qa;answering"}, "e6602786132e040e02df93f729f737f65a116677": {"ta_keywords": "speech recognition recognize;speech recognition als;speech recognition;home assistants spoken;automatic speech recognition;speech interaction digital;recognize human speech;home assistants digital;digital home assistants;processing deep learning;key speech processing;deep learning;deep learning dl;automatic speech;uses automatic speech;fields automatic speech;signal processing deep;speech processing;data deep learning;models speech;nonspecialist key speech;microphone;statistical models speech;assistants spoken language;speech interaction;home assistants;human speech;microphone array processing;recognition als computer;speech processing algorithms", "pdf_keywords": ""}, "d8682a269523a868f2bc9714b00f0519aa0e931f": {"ta_keywords": "text database like;text database;database like queries;use artificial neural;like queries structured;database like;deductive databases;style deductive databases;relational database;generation artificial neural;queries structured;information retrieval whirl;artificial neural;neural networks based;relational database allows;neural network;queries integrate information;deductive databases ranked;databases ranked retrieval;information retrieval;artificial neural network;database;new relational database;neural networks;fragments text database;like queries;artificial neural networks;databases;neural network new;database allows queries", "pdf_keywords": ""}, "8cb74fe4f598699c9c24d88acd4906e2489267af": {"ta_keywords": "teams simple robots;mapping navigation teams;adaptive mapping navigation;navigation teams simple;navigation teams;mapping navigation;adaptive mapping;simple robots;simple robots studied;robots;robots studied;mapping;teams simple;navigation;adaptive;teams;simple;studied", "pdf_keywords": ""}, "48aced0919e29722d6eed9544353d5507c541cfc": {"ta_keywords": "annotation gene mentions;association based phytoplanktonology;guidelines annotation gene;annotation gene;gene mentions texts;gene association;approach gene association;gene mentions;gene association association;gene;phytoplanktonology;phytoplanktonology article;annotation;based phytoplanktonology article;phytoplanktonology article present;guidelines annotation;based phytoplanktonology;flybase curation;approach gene;flybase curation paper;aid flybase curation;association;association based;presents approach gene;association association based;association association;flybase;mentions texts;present guidelines annotation;mentions texts outline", "pdf_keywords": ""}, "b3d9a0308ba6c4ca583a2b4e5be2b3eed466ccbc": {"ta_keywords": "markov decision process;environments millimeter wave;network environments millimeter;observable markov decision;markov decision;decision process pomdp;obstacles derive optimal;pomdp framework model;pomdp framework;millimeter wave mmwave;optimal threshold policy;network environments dynamic;millimeter wave;d2d communication highly;d2d communication;partially observable markov;wave mmwave device;device d2d communication;network environments;wave mmwave;observable markov;dynamic obstacles derive;environments dynamic obstacles;mmwave device;derive optimal threshold;process pomdp framework;dynamic obstacles;mmwave device device;uncertainty kind network;model uncertainty", "pdf_keywords": "relaying strategy based;relaying strategy;new relaying strategy;best relaying zone;packet relay;packet relay zone;able predict relay;idea best relaying;best relaying;predict relay able;relaying zone;detecting moving obstacles;relaying;propose new relaying;send packet relay;predict relay;relay able;relay;moving obstacles;wirelessly transmitting packet;relay zone;radars detecting moving;dynamic obstacles outperforms;relay able send;relaying zone zone;dynamic obstacles;moving obstacles simulations;obstacles outperforms state;optimal threshold policy;transmitting packet destination"}, "fc8e226c20800c8ccc095bb6a3c0f8dcb637b683": {"ta_keywords": "lung metastases consensus;lung metastases patients;lung metastases performance;lung metastases compared;study lung metastases;crc lung metastases;lung metastases crc;metastasis colorectal cancer;metastases metastasis colorectal;lung metastases metastasis;cancer metastases rectal;lung metastases;diagnosis lung metastases;metastases rectal cancer;effect lung metastases;metastasis colorectal;incidence lung metastases;metastases compared colon;cancer metastases;metastases rectal;metastases patients;metastases crc based;metastases crc;colorectal cancer ccrc;metastases metastasis;managing cancer metastases;colorectal cancer;metastases compared;compared colon cancer;rectal cancer", "pdf_keywords": ""}, "6c0a3029afd65c83982b3fb96f623da382344286": {"ta_keywords": "nlp applications tensor;topic models neural;natural language processing;tensor matrix factorization;matrix tensor factorization;tensor factorization;learning topic models;recent nlp applications;methods transductive learning;useful nlp;transductive learning;tensor factorization theory;semantics dependency parsing;useful nlp applications;positives natural language;transductive learning topic;topic models;discuss recent nlp;matrix factorization methods;nlp applications;nlp applications methods;recent nlp;matrix factorization;factorization methods attracted;likely useful nlp;factorization methods;factorization theory optimization;factorization methods recently;lexical semantics dependency;nlp", "pdf_keywords": ""}, "928f942baf03dd56aae662fa94d85d22b5600f83": {"ta_keywords": "paraphrases retrieve sentences;paraphrases retrieve;paraphrase pairs;paraphrases using;paraphrase pairs used;paraphrases using paraphrases;using paraphrases retrieve;use paraphrases using;using paraphrases;use paraphrases;paraphrases;paraphrasing;paraphrasing process;narrow domains paraphrasing;based use paraphrases;paraphrasing process animal;retrieve sentences statistical;paraphrase;domains paraphrasing;domains paraphrasing process;retrieve sentences meaning;retrieve sentences;parallel corpora;used retrieve sentences;parallel corpora used;sentences statistical framework;sentences statistical;tool translating texts;translating texts;corpora", "pdf_keywords": ""}, "e30b22e692b3c7d2653832bf2901abd8a9375b6e": {"ta_keywords": "contents cellular network;optimal content placement;placement contents cellular;optimal content;cellular network algorithm;content base station;cell networks optimal;gibbs sampling based;small cell networks;asymptotically optimal content;place popular contents;base station content;develop gibbs sampling;optimal placement contents;networks optimal place;cellular network;station content;contents cellular;learning optimal placement;decide store content;cellular;contents stored neighboring;cell networks;popularity distribution;store content base;popular contents base;content base;cache based knowledge;stations small cell;neighboring base stations", "pdf_keywords": "stations having caches;content placement cellular;caching contents content;having caches cooperate;cache update algorithms;corresponding cache based;network operators caching;caching contents;caching;cache based;sequential cache;sequential cache update;having caches;caches cooperate;content base station;cache based knowledge;placement cellular network;corresponding cache;cache;cellular network;develop sequential cache;approach cache;cellular network operators;cache level signal;cache level multi;cache level;base station content;caches cooperate cache;cooperate cache level;approach cache level"}, "2406cf39805c70264c4226b7325a09b506c70921": {"ta_keywords": "quantum bit existing;dot quantum;quantum dot quantum;quantum repeater;design quantum;quantum repeater quantum;repeater quantum;quantum repeaters based;repeater quantum dot;quantum repeaters;quantum bit;quantum;quantum state encoded;case quantum repeater;quantum dot;design quantum repeaters;dot quantum state;based concept quantum;single quantum bit;concept quantum;quantum state;quantum entanglement;synthetic corpus trained;case quantum;neural sql executor;learning neural sql;single quantum;approach design quantum;concept quantum entanglement;training synthetic corpus", "pdf_keywords": "deep understanding tables;languages tables achieve;training structured tabular;formal languages tables;question answering table;languages tables;table pre training;answering table based;training approach tabular;table based fact;tables achieve ef\ufb01cient;structured tabular data;ef\ufb01cient table pre;pre training corpus;question answering;based question answering;answering table;table based;pre training structured;table pre;corpus pre training;sql queries inference;tables;understanding tables;mimic sql;synthetic corpus pre;results deep understanding;tables achieve;pre trained language;predicting outcome sql"}, "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671": {"ta_keywords": "offline reinforcement learning;learns demonstrations weighting;generation offline reinforcement;text generation offline;algorithm learns demonstrations;learns demonstrations;text generation largely;machine translation;machine translation methods;algorithm learns;text generation;approaches text generation;offline reinforcement;learning algorithm learns;learning noisy data;learning noisy;learning models;frame text generation;translation methods automatic;reinforcement learning rl;automatic human evaluation;learning rl;learns;reinforcement learning;training analysis online;learning models called;art machine translation;generated histories models;problem learning noisy;demonstrations weighting", "pdf_keywords": "machine translation learns;translation learns predict;conditional text generation;deep reinforcement learning;based deep reinforcement;translation learns;generation based deep;deep reinforcement;text generation;machine translation;text generation based;learns predict;learning capable generating;learning objective evaluation;learns predict expected;reinforcement learning;policy state distributions;reinforcement learning capable;propose machine translation;conditional text;general learning;cnn dm;algorithm conditional text;learning objective;baseline learning;supervised baseline cnn;cnn dm paradigm;mismatched learning objective;propose general learning;based deep"}, "fdaad09b1a897c0a04b9a9579081d542e2b4546c": {"ta_keywords": "vaccination infection interval;neutralization potency viral;vaccination infection;infection interval determines;infection interval;vaccination;cross neutralization potency;viral infection;potency viral infection;determines cross neutralization;neutralization potency;potency viral;cross neutralization;interval determines cross;infection;neutralization;viral;interval determines;determines cross;potency;cross;interval;determines", "pdf_keywords": ""}, "74276a37bfa50f90dfae37f767b2b67784bd402a": {"ta_keywords": "based translation tasks;translation tasks;partially translate prediction;translate prediction;multilingual benchmarks;translation based translation;translation tasks work;state art translation;performance multilingual benchmarks;mt5 multilingual;translation based;introduce mt5 multilingual;mt5 multilingual variant;translation zero shot;translating text human;translating text;translate prediction wrong;performance multilingual;prevent accidental translation;multilingual variant t5;multilingual;translating;chooses partially translate;based translation;translation zero;translation;prediction wrong language;accidental translation zero;multilingual variant;accidental translation", "pdf_keywords": "training multilingual generative;multilingual generative models;multilingual generative;massively multilingual model;generate languages generative;pre trained multilingual;multilingual pre trained;trained multilingual language;trained multilingual;produce massively multilingual;lingual representation learning;languages generative;multilingual language model;unsupervised multilingual pre;multilingual model;pre training multilingual;trained language models;tasks unsupervised multilingual;training multilingual;language model trained;massively multilingual;unsupervised multilingual;generate languages;automatically generate languages;multilingual benchmark;tasks xtreme multilingual;multilingual pre;xtreme multilingual benchmark;language models;machine translation intermediate"}, "00b2afaf5935b4dea41f134fe11a21a1ed56fa0e": {"ta_keywords": "supervised sentiment analysis;supervised sentiment;sentiment analysis social;sentiment analysis;analysis social media;sentiment;analyze human behavior;supervised;analysis social;social media powerful;analyze human;human behavior;social media;tool analyze human;analysis;analyze;social;behavior;human;powerful tool analyze;tool analyze;media powerful tool;tool;powerful tool;media powerful;media;powerful", "pdf_keywords": ""}, "1bf36cb3453b51550ebadd904a840c75d59f171b": {"ta_keywords": "based avalanche rrrna;avalanche rrrna paper;avalanche rrrna;energy spectrum cosmic;speech recognition asr;robust automatic speech;cosmic microwave background;spectrum cosmic microwave;cosmic microwave;spectrum cosmic;speech recognition;recognition asr;automatic speech recognition;cosmic;recognition asr recently;deep neural network;robust asr;network based avalanche;robust asr chapter;components robust asr;robustness issues deep;automatic speech;rrrna paper present;cmb;recognition systems chapter;based avalanche;rrrna;rrrna paper;deep neural;issues deep neural", "pdf_keywords": ""}, "8b20173b98914f36302389e4c761c334fe867dcd": {"ta_keywords": "treebanks text generation;directly dependency treebanks;dependency treebanks;dependency treebanks text;generated subword language;treebanks;train robust parsers;robust parsers;translation morphologically rich;robust parsers propose;parsers propose;treebanks text;morphologically rich languages;translation morphologically;finitely generated subword;parsers;generated subword;parsers propose metric;language processing;text generation systems;subword language;natural language processing;text generation;morphologically;morphologically rich;language translated subword;metric translation morphologically;subword language translated;language processing applications;ubiquitous natural language", "pdf_keywords": "robust parsing morphological;treebanks models robust;robust parsers improve;robust parsing models;translation morphologically rich;synthetically noised treebanks;models robust parsing;robust parsers;robust parsing;treebanks models;systems robust parsers;directly dependency treebanks;machine translation systems;train robust parsing;parsing morphological;machine translation evaluation;translation text morphologically;languages machine translation;automated translation;machine translation;noised treebanks models;evaluate machine translation;translation morphologically;treebanks;dependency treebanks;parsing morphological feature;treebanks present;noised treebanks;treebanks present new;parsers improve"}, "3dcba175248d0e8d2da44e3731e4adbfb9f00e97": {"ta_keywords": "sentence level extractions;open information extraction;extracted individual sentences;enhance extracting entities;extracting entities relations;large corpus extract;corpora open information;extractions using corpus;massive text corpora;information extraction;information large corpus;extracting entities;corpus extract;entity relation phrases;sentence level tuple;mutually enhance extracting;information extraction systems;large corpus;tuple extractions using;entities relations text;learning subtask jointly;corpus extract high;level tuple extractions;tuple extractions;text corpora;corpus;using corpus;using corpus level;quality sentence level;corpus level statistics", "pdf_keywords": "open information extraction;extraction open information;information extraction open;domain information extraction;information extraction;dependent phrasal segmentation;information extraction systems;phrasal segmentation;extract relation tuples;remine extract relation;phrasal segmentation algorithm;extraction systems relation;context dependent phrasal;high quality phrases;phrases multiple types;framework remine extract;open domain information;extract relation;open information;relation tuples entity;extraction open;string relation sentences;tuples entity;tuples entity arguments;quality phrases;quality phrases multiple;open framework remine;extraction systems;phrases multiple;predicate string relation"}, "96dbffb71e4d62a985f826197845623b1415c267": {"ta_keywords": "metacognition learning preliminary;metacognition learning;metacognitive learning;metacognitive learning acquiring;yielding metacognitive learning;effect metacognition learning;algorithm yielding metacognitive;yielding metacognitive;metacognitive;metacognition;human learning;knowledge better learning;understand human learning;learning;better learning;future learning;assist future learning;learning preliminary;learning acquiring deep;better learning strategy;effect metacognition;learning acquiring;speed learning;free grammar induction;learning process;acquiring deep;human learning paper;model synthetic student;prior knowledge better;learning strategy", "pdf_keywords": ""}, "ce9919ffb9dab701babd67a945b1590917345789": {"ta_keywords": "explanation based learning;explanations training;multiple explanations training;explanations training instance;explanations constructed;explanations training example;incorrect explanations constructed;possible explanations training;inconsistent explanation problems;produces multiple explanations;applying explanation based;choose possible explanations;explanation problems occur;domain theories;mistake domain theory;domain theories suffer;incorrect incorrect explanations;domain theory tractable;explanations constructed short;multiple inconsistent explanation;inconsistent explanation problem;explanations;learning called abductive;theories multiple inconsistent;incorrect explanations;domain theory produces;inconsistent explanation;explanation based;imperfect theories;abductive explanation based", "pdf_keywords": ""}, "53e161d4434576355fc5f63fe56afd8e135174b2": {"ta_keywords": "learning script knowledge;learning script;script knowledge;high quality scripts;script knowledge given;scripts experiments models;script collection;script collection demonstrate;trained neural language;scripts experiments;neural language models;scripts;crowdsourced partially ordered;ordered scripts experiments;quality scripts;method learning script;large 4k crowdsourced;language generation;prediction generate;crowdsourced partially;language generation structure;4k crowdsourced partially;combines language generation;knowledge given script;neural language;4k crowdsourced;crowdsourced;language models;ordered scripts;partially ordered scripts", "pdf_keywords": "script generation tasks;text generation structured;script generation challenging;generation structured prediction;story generative models;plot story generative;story generative;automatic script generation;script generation;script generation human;generation tasks automatic;natural text generation;text generation;generation tasks;entire script generation;generation structured;prediction entire script;generation challenging task;structured prediction proscriptgen;level agenda plot;agenda plot story;approach automatically generating;structured prediction;applied script generation;language model generating;agenda plot;automatically generating;proscriptgen neural language;prediction proscriptgen neural;generative models successfully"}, "6c59a6ad00d82ca9f76fef92232ff3e2f3c1acc8": {"ta_keywords": "fusion multichannel diarization;voice activity detection;diarization systems;multichannel diarization;diarization systems majority;combining outputs diarization;speaker voice activity;outputs diarization systems;multichannel diarization compares;multiple speaker diagonals;voice activity;partite graph matching;diarization compares favorably;target speaker voice;diarization outputs;diarization;speaker voice;diarization compares;fusion multichannel;multiple speaker;networks target speaker;late fusion multichannel;outputs diarization;fusion methods;speaker diagonals;cost tensor;overlapping segments diarization;speakers speech method;target speaker;segments diarization outputs", "pdf_keywords": "automatic speaker diarization;speaker diarization based;speaker diarization;networks speaker diarization;speaker diarization important;diarization based deep;neural networks speaker;voice activity detection;speaker voice activity;diarize speech;overlap aware diarization;aware diarization systems;diarize speech applied;aware diarization;method automatic speaker;speaker voice;voice activity;combining outputs diarization;networks speaker;target speaker voice;diarization based;automatic speaker;speech applied resegmentation;networks target speaker;originally diarize speech;multi channel diarization;outputs diarization systems;diarization systems;diarization important task;diarization systems majority"}, "a5881560968963d0c845c468a273261fde0b7248": {"ta_keywords": "generate fragile interpretations;text interpretability methods;text interpretability;robust trustworthy nlp;interpretations need robust;trustworthy nlp;language model predictions;interpretability methods like;interpretability methods;trustworthy nlp applications;interpretability;fragile interpretations exploited;perform fragile interpretations;input text interpretability;natural language model;predicting response word;nlp datasets interpretations;fragile interpretations;fragile interpretations attack;predictions relative word;natural language;interpretation methods popular;nlp applications high;explaining natural language;nlp;nlp datasets;interpretations exploited;word perturbations input;language model;word importance scores", "pdf_keywords": "adversarially trained;adversarially;automated privacy policy;adversarially trained results;automated privacy;models adversarially trained;fake news identi\ufb01cation;detection 2016 text;attack interpretations using;models adversarially;text based automated;natural language processing;natural language;different models adversarially;based automated privacy;possible attack interpretations;computational language processing;toxic content detection;processing nlp gained;2020 explanations manipulated;privacy policy understanding;computational language;use natural language;text based;explanations manipulated;attack interpretations;privacy policy;nlp gained popularity;language processing nlp;language processing"}, "3004a3e4d8969dc3c36c9274b0f76ecc874f2e6a": {"ta_keywords": "boosted speech separation;direction separation speech;separation speech nonstationary;speech separation;networks incorporating speech;speech separation using;recognition separation promising;speech recognition outputs;recognition boosted speech;separation using deep;phase sensitive recognition;speech nonstationary interference;separation speech;recognition separation;speech recognition;incorporating speech recognition;deep recurrent neural;bidirectional recurrent networks;boosted speech;recurrent networks incorporating;recurrent networks;recurrent neural networks;speech nonstationary;using deep recurrent;deep recurrent;deep network;incorporating speech;spectral input features;using bidirectional recurrent;integration recognition separation", "pdf_keywords": ""}, "e29e43d9c0772d44cff53044484970599db30d5f": {"ta_keywords": "domain adaptation neural;domain adaptation;results domain adaptation;domain adaptation strategies;use domain adaptation;learned domain differentials;differential adaptation;adaptation neural;differential adaptation dda;adaptation neural machine;differential differential adaptation;adaptation dda;learned domain;neural machine translation;use learned domain;adaptation dda instead;domain differentials adapt;domain differentials;adaptation;modeling difference domains;target domains suboptimal;machine translation;labeled data domain;source target domains;adaptation strategies;difference domains;domains using models;task neural;machine translation demonstrate;difference domains using", "pdf_keywords": "translation based deep;unsupervised domain adaptation;neural machine translation;domain adaptation;domain adaptation based;rnn encoder statistical;domain differential adaptation;word representations domain;rnn encoder;machine translation necessitates;statistical machine translation;context machine translation;deep contextualized word;machine translation;machine translation investigated;using rnn encoder;generated domain subwords;generate domain subwords;deep contextualized;domain subwords generated;contextualized word representations;new unsupervised adaptation;subwords generated domain;learning phrase representations;unsupervised adaptation training;representations using rnn;machine translation based;unsupervised adaptation;adaptation training data;domain subwords"}, "f6eafb82d2450f28f668443b689c91e896a0d63e": {"ta_keywords": "uncertainty linear bandit;linear bandit problems;linear bandit problem;stochastic bandit problem;stochastic bandit;bandits stochastic bandit;stochastic bandits;case linear bandit;linear bandit;bandit problem learning;stochastic bandits stochastic;standard bandit problems;bandit problem studied;arms stochastic bandits;bandits stochastic;bandit problems set;bandit problem;bandit problems;bandit problems learning;horizon linear bandit;bandit problem seen;stochastic reward;stochastic reward algorithm;bandits;receives stochastic reward;reward algorithm;steps standard bandit;bandit;standard bandit;predicted reward", "pdf_keywords": ""}, "8225b047e0fe90c2d5f9bb77fd94396a9d0fd21e": {"ta_keywords": "gene identifiers documents;associating gene identifiers;gene identifiers;gene id ranking;id ranking systems;named entity recognition;identifiers documents;gene id finding;identifiers documents proposed;possible gene identifiers;entity recognition;entity recognition ner;dictionary gene synonyms;gene synonyms evaluate;gene synonyms;gene id;organism database;accurate gene id;id ranking;identifiers;framework associating gene;organism database curation;graph based ranking;dictionary gene;ner gene id;model organism database;named entity;evaluation named entity;recognition ner gene;graph search framework", "pdf_keywords": ""}, "3873e60de2d20aa33829e2d3d79221e716785546": {"ta_keywords": "semi supervised learning;discriminative language model;proximity unlabeled conversations;semi supervised;unlabeled conversations;used semi supervised;telephone speech corpora;unlabeled conversations used;supervised learning ssl;labeled data based;discriminative language;speech corpora;supervised learning;based labeled data;propose discriminative language;supervised;based label propagation;labeled data;labeled data available;label propagation graph;graph based label;label propagation;discriminative model propose;conversational telephone speech;speech corpora dev04f;analysis based labeled;gram features;telephone speech;labeled data experiments;discriminative model", "pdf_keywords": ""}, "229c0c13e5c2d8e189efccf77b8179ec16500212": {"ta_keywords": "string machine translation;machine translation engine;machine translation;machine translation possible;translation engine;learning travatar gravitational;translation engine training;tree string machine;greater accuracy translation;parsers alignment techniques;accuracy translation;syntactic parsers alignment;syntactic parsers;accuracy translation using;parsers;englishjapanese machine translation;decoding evaluation travatar;phrase based translation;extraction tuning decoding;different syntactic parsers;tuning decoding evaluation;travatar tree string;parsers alignment;decoding evaluation;decoder englishjapanese machine;gravitational wave detector;travatar gravitational wave;tree string;string machine;hierarchical phrase based", "pdf_keywords": ""}, "14fce3cfa503894f244fc6ea8a7a00fa0ddfd94e": {"ta_keywords": "concept weightless body;weightless body used;weightless body;reduce weight body;body weighting process;weightless;reduce weight;introduction concept weightless;body weighting;used reduce weight;concept weightless;weight body weighting;body used reduce;weight body;weighting;weighting process;weight;body;body used;reduce;simple self;used reduce;article presents simple;simple self contained;introduction concept;presents simple self;process;concept;introduction;self contained introduction", "pdf_keywords": ""}, "a54a3a7b02cacd92b3bc633be7ea54e4f365fa65": {"ta_keywords": "malware binary communities;distinct malware communities;malware communities;malware communities malware;characterize malware;communities malware;malware using socio;communities malware family;visualize analyze malware;characterize malware binaries;mapping scm malware;detecting malware;identify distinct malware;malware binaries;berlin characterize malware;malware binary;malware;distinct malware;analyze malware;detecting malware using;analyze malware binary;malware detection;scm malware detection;malware family;malware using;scm malware;malware binaries benign;malware family revealed;method detecting malware;malware detection systems", "pdf_keywords": ""}, "935c275868bec7301f4bd254159978d8ded138b9": {"ta_keywords": "energy cgmp signaling;cgmp signaling;cgmp signaling pathway;activation energy cgmp;rat nerve cell;nerve cell controlled;rat rat nerve;rat nerve;energy cgmp;nerve cell;signaling pathway rat;cgmp;nerve;pathway rat rat;signaling pathway;activation energy;demonstrate activation energy;pathway rat;signaling;cell controlled;cell controlled means;demonstrate activation;simple electrical circuit;electrical circuit;rat rat;rat;activation;circuit;simple electrical;pathway", "pdf_keywords": ""}, "b9f0c7e99bcc94c2cd75fd8e1cef45188f51270e": {"ta_keywords": "connectionist temporal classification;temporal classification loss;label transitions neural;temporal classification gtc;speech recognition task;speech recognition;temporal classification;speech recognition asr;graph speaker;speaker speech recognition;graph speaker information;based temporal classification;automatic speech;transitions neural network;nodes graph speaker;automatic speech recognition;graph based temporal;graph based supervision;speaker represented nodes;speaker asr tokens;label transitions;recognition asr;semi supervised;recognition asr systems;multi speaker speech;semi supervised learning;connectionist temporal;multi speaker asr;xmath1 transition fermilab;speaker asr", "pdf_keywords": "speaker speech dataset;speaker speech recognition;automatic speech;ordering automatic speech;speech dataset;automatic speech recognition;research automatic speech;speaker machine translation;speech recognition;learning multi speaker;librispeech automatic speech;multi speaker speech;speech recognition machine;toend speech dataset;speech recognition propose;multiple speakers sequence;speech recognition asr;speech dataset derived;speakers sequence intermingled;speakers sequence;recognize speaker example;speaker example application;speaker cascaded processes;multi speaker asr;speech recognition contrast;based temporal classifier;speaker speech;temporal classifier;multi speaker end;model multi speaker"}, "dfa34a10e2ba861545549c3188ef245b1e69bcdf": {"ta_keywords": "bio event extraction;text event extraction;word embedding;extracting bio event;word embedding methods;advances word embedding;word embeddings;event extraction;event extraction important;sequences word embeddings;event extraction using;latest word embedding;extracting biological networks;event sequences word;bio event sequences;word embeddings recent;word embedding make;word distribution;words text event;using bag ofwords;word distribution ef;extracting biological;bio event;bag ofwords;goal extracting biological;computation word distribution;embeddings recent advances;baseline bio event;words text;method extracting bio", "pdf_keywords": ""}, "2ea5b0f5e476ddc00ae4450f2888a51fa25dd1d3": {"ta_keywords": "tackling nlp tasks;trained language models;augmentation self training;nlp tasks pre;trained language;task augmentation self;pre trained language;tasks pre trained;nlp tasks;self training;training examples;self training approach;successes tackling nlp;task augmentation;training examples available;predicting performance self;task unlabeled texts;fine tuning train;achieved training examples;data auxiliary task;task fine tuning;training examples address;language models perform;tackling nlp;self training complementary;propose self training;training examples class;tuning target task;performance self propelled;trained", "pdf_keywords": "training task augmentation;sentence level pretraining;modeling task augmentation;task self training;data auxiliary training;effective task augmentation;training task self;task augmentation effectively;task augmentation self;self training task;task augmentation;auxiliary training task;augmentation self training;pretraining language modeling;task augmentation significantly;training task improve;task augmentation uses;implementation task augmentation;intermediate training task;task augmentation utilizes;training task;unlabeling examples improve;auxiliary intermediate training;augmentation utilizes unlabeled;methods self training;self training complementary;pretraining language;paper task augmentation;approach self training;task machine learning"}, "92f93c0014ba4da59180c4cd141ad0dcaad5803f": {"ta_keywords": "transfer learning multilingual;multilingual transfer learning;language multilingual transfer;multilingual transfer;transfer learning improve;transfer learning surprising;transfer learning;target language improvement;learning multilingual;based transfer learning;improve performance multilingual;learning multilingual data;target language multilingual;language improvement;target languages tasks;instance based transfer;multilingual data instance;target language;performance multilingual;target languages;languages tasks;auxiliary target languages;language improvement occur;common target language;effect transitive vocabulary;languages tasks tested;language multilingual;performance multilingual settings;multilingual;languages", "pdf_keywords": "multilingual transfer;multilingual multilingual transfer;language transfer learning;multilingual transfer transfer;target language transfer;language transfer;learning cross lingual;transfer learning promising;embedding unsupervised transfer;learning transfer learning;cross lingual embedding;target languages tasks;transfer learning;transfer learning transfer;lingual embedding unsupervised;unsupervised transfer learning;surprisingly effective multilingual;learning method multilingual;learning transfer;based transfer learning;languages tasks;transfer learning cross;transfer learning popular;task transfer learning;effective multilingual;lingual embedding;transfer possible vocabularies;transfer learning method;multilingual multilingual;cross lingual"}, "4a9c80e263fd0a88ad8220aa076ede4a3e77fcc1": {"ta_keywords": "vulnerable adversarial samples;confidence adversarial samples;high confidence adversarial;adversarial samples generated;confidence adversarial;adversarial samples crucial;adversarial samples;deep testing;vulnerable adversarial;deep testing methods;detect various adversarial;various adversarial samples;known vulnerable adversarial;adversarial samples introduce;various adversarial;adversarial;adversarial attacks;kinds adversarial attacks;kinds adversarial;testing predictive;vulnerability dnn systems;different kinds adversarial;vulnerability dnn;testing predictive predictive;proposed vulnerability dnn;dnn known vulnerable;networks dnn known;number deep testing;model mutation testing;deep neural", "pdf_keywords": "graph guided testing;input adversarial;adversarial program;input adversarial program;program adversarial;regulate input adversarial;adversarial program adversarial;adversarial;program adversarial attacks;adversarial attacks;adversarial attacks divided;knowledge dnn model;sparsity relational graphs;dnn model;dnn model information;detecting potential defects;deep learning;deep learning model;knowledge dnn;relational graphs;new deep learning;relational relational graph;deep learning framework;relational graph;propose deep learning;control sparsity relational;guided testing;respect knowledge dnn;graphs determine pruning;learning model automatically"}, "885fe11ed7ab81c8609ccddb3e10f62577c04ab9": {"ta_keywords": "learning agents dialogue;exploration deep learning;exploration deep;learning agents;efficiency exploration deep;learning noisy;algorithm learns;agents dialogue systems;learns;algorithm learns faster;learns faster;spiking replay;deep learning agents;replay buffer experiences;women algorithm learns;learning noisy noisy;episodes make learning;spiking replay buffer;agents dialogue;replay;exploration strategies epsilon;learning feasible;dialogue systems;intrinsic reward based;strategies epsilon greedy;improves efficiency exploration;exploration strategies;bootstrapping intrinsic reward;learning;greedy boltzmann bootstrapping", "pdf_keywords": "dialogue policy learning;learning dialogue policies;learning agents dialogue;learning dialogue;agents dialogue systems;learning learning dialogue;dialogue systems;dialogue policies;dialogue policies spoken;dialogue achieve task;dialogue policy;approach dialogue policy;deep reinforcement learning;policies spoken dialog;agents dialogue;policy learning;policy learning learning;dialog systems challenging;spoken dialog systems;learning agents;dialogue achieve;deep reinforcement;spoken dialog;dialog systems;deep learning agents;reinforcement learning dia;approach dialogue;dialogue;present deep reinforcement;reinforcement learning"}, "a1340029d8a5c57bee8a5995ac3beafd3d0ba96c": {"ta_keywords": "distributed sensor subset;sensor subset selection;complexity distributed sensor;kalman consensus filtering;sampling sensor subset;sensor subset;distributed sensor;gibbs sampling sensor;kalman consensus;constraint kalman consensus;consensus filtering process;algorithm tracking markov;filtering process estimation;subset selection stochastic;tracking markov chain;consensus filtering;process estimation algorithm;selection stochastic approximation;tracking markov;selection algorithm tracking;approximation meet sensor;estimation algorithm;sensor activation constraint;process estimation;sampling sensor;algorithm tracking;estimation algorithm evaluated;selection stochastic;sensor;stochastic approximation", "pdf_keywords": ""}, "c6854064cb5053e67d23394eee6d1646108f6d56": {"ta_keywords": "textual entailment task;textual entailment;novel textual entailment;setting textual entailment;standard textual entailment;entailment task;textual entailment define;premise sentences;multiple premise sentences;lexical inferences emphasizes;inferences emphasizes knowledge;entailment task requires;trivial lexical inferences;lexical inferences;entailment;inferences emphasizes;premise sentences paper;inference multiple premise;textual;entailment define novel;entailment define;task requires inference;premise task;inference multiple;inferences;multiple premise task;inference;strong neural baselines;requires inference multiple;sentences", "pdf_keywords": "textual entailment task;entailment task premise;entailment task;separate premise sentences;word attention neural;attention models;task premise text;single premise entailment;entailment task important;premise entailment labels;challenging realistic entailment;textual entailment;novel textual entailment;entailment labels premise;premise entailment;neural attention models;attention neural;multiple premise task;premise sentences;sentences describing scene;attention models shown;premise task;standard textual entailment;entailment labels;word attention;attention neural attention;attention model;realistic entailment problem;entailment problem solved;task premise"}, "74b05adf1ec74849a4f7963fe3f17fd61b92af4b": {"ta_keywords": "queries contextual;learn query languages;queries contextual information;mind query intents;like query languages;learn query;followup query analysis;query languages;multiple queries contextual;query languages paper;query analysis;having learn query;queries;approach search databases;involves multiple queries;termed followup query;like query;query analysis work;query languages multi;query intents;mind query;approach search;search databases;1000 query;databases using natural;new approach search;multiple queries;search databases using;followup query;contextual information", "pdf_keywords": "answering natural language;model answer answering;natural language follow;sequence sequence learning;answer answering;sequence learning;language follow queries;follow queries;answer answering natural;sequence learning model;follow query analysis;compositional semantic structures;follow queries furthermore;follow query scenarios;immediate follow query;based compositional semantic;natural language utterances;natural language;compositional semantic;approach natural language;follow query;answering;formulate follow query;natural language propose;method natural language;queries employs ranking;interpreted precedent queries;queries furthermore propose;follow queries users;learning model answer"}, "a5f214e23b8cd35a370a182c155ef333d77c5bb2": {"ta_keywords": "polarity correlates speech;stance natural speech;acoustic indicators stance;phonetic properties acoustic;correlates speech;neutral phonetic properties;phonetic properties;correlates speech rate;annotated stance strength;neutral phonetic;natural speech;stance complex activity;annotated stance;phonetic;hand annotated stance;negative neutral phonetic;stance natural;speech rate intensity;corpus collaborative conversational;acoustic phononetic;speech using corpus;speech rate;stance complex;stance strength correlated;natural speech using;speech;suggests increases stance;stance;speech using;stance strength", "pdf_keywords": ""}, "b33caf27fe5584b9b773c75fc35ee0e8b1421864": {"ta_keywords": "population potential game;strategies population type;continuous population potential;wardrop equilibrium type;equilibrium type based;potential game population;population potential;wardrop equilibrium;strategies population;artificial dynamics;equilibrium type;extension wardrop equilibrium;natural artificial dynamics;strategies potential game;equilibrium means optimizing;artificial dynamics dimension;subset strategies potential;preference strategies population;dynamics;potential game relative;game population mass;continuous population;population type;population type vector;potential game;equilibrium means;equilibrium;strategies potential;dynamics dimension type;importance strategies population", "pdf_keywords": ""}, "5ee96dd7e3395d8a53d6d3ceb62593477a4e0fe1": {"ta_keywords": "captions automatic colorization;color words captions;colorizations language;plausible colorizations language;language conditioned colorization;colorizations language agnostic;automatic colorization;colorizations;manipulating descriptive color;colorization;automatic colorization process;descriptive color words;plausible colorizations;colorizations simply manipulating;alter colorizations;accurate plausible colorizations;color words;colorizations simply;dramatically alter colorizations;alter colorizations simply;conditioned colorization;descriptive color;color dynamics birds;colorization process;colorization process adding;words captions automatic;words captions;color dynamics;colorization produce accurate;colorization produce", "pdf_keywords": "learns color spaces;colorization natural language;language conditioned colorization;learned image captions;color spaces dialogue;language based colorization;plausible colorizations language;visual question answering;colorizations language;task colorization natural;learns color;colorization algorithm learns;introduce task colorization;task colorization;colorizations language agnostic;algorithm learns color;video action recognition;action recognition;action recognition present;accurate plausible colorizations;based colorization augment;conditioned colorization;colorization augment;dialogue image;dialogue image pairs;color spaces;colorizations;representations learned image;plausible colorizations;video action"}, "53f1fb4dc887540ef134a8d08c152789c313aa5c": {"ta_keywords": "end speech recognition;triphone based dnn;speech recognition;speech recognition systems;speech data;performance track embedding;zero speech2017 track1;speech data using;language speech data;track embedding context;systems zero speech2017;track embedding;based phoneme transcripts;speech2017 track1;piano based hybrid;autoencoder;speech2017 track1 presented;zero speech2017;learn phoneme transcripts;normalize speaker characteristics;model learn phoneme;phoneme transcripts;piano based;attention based encoder;piano piano based;piano;embedding systems;using triphone based;large scale piano;computer based phoneme", "pdf_keywords": ""}, "7262bc3674c4c063526eaf4d2dcf54eecea7bf77": {"ta_keywords": "pendulum paraphrase generation;paraphrase generation;paraphrastic sentence embeddings;paraphrase generation paper;paraphrase generation provide;paraphrase pairs;used paraphrase generation;million paraphrase pairs;semantic textual similarity;paraphrase pair composed;paraphrase pairs pendulum;textual similarity competition;paraphrase pair;textual similarity;world paraphrase pair;sentence embeddings outperform;train paraphrastic sentence;paraphrastic;paraphrastic sentence;sentence embeddings;showing used paraphrase;paraphrase;train paraphrastic;pendulum paraphrase;semantic textual;pendulum pendulum paraphrase;world paraphrase;semeval semantic textual;50 million paraphrase;million paraphrase", "pdf_keywords": "paraphrastic sentence embeddings;evaluating sentence embeddings;sentence embeddings;sentence embedders model;sentence embedders;learning paraphrastic sentence;sentence embeddings translated;learning paraphrastic;train sentence embedders;sentence embeddings using;sentence similarity tasks;corpus sentential paraphrases;similarity sentence similarity;perform sentence similarity;sentence similarity;sentential paraphrases collections;embeddings translated;sentential paraphrases;sentence embeddings paper;semantic similarity datasets;train paraphrastic sentence;similarity tasks semantic;embeddings using learning;semantic similarity;embeddings offer advantages;train paraphrastic;tasks semantic similarity;embeddings;sentence similarity paper;machine translation"}, "6d2d86cf5e80b58a03360559095ea3603548248f": {"ta_keywords": "algorithm initialization quantum;prior algorithms;regularization;prior algorithms underlying;constraint continuous regularization;permutation based models;initialization quantum;continuous regularization;permutation constraint continuous;statistical literature permutation;initialization quantum state;combinatorial permutation constraint;shape constraint permutation;regularization term paper;determined quantum;permutation constraint;constraint permutation columns;rankedness signal noise;heuristic statistical seriation;outperforms prior algorithms;replaces combinatorial permutation;constraint permutation;statistical seriation classical;quantum state based;quantum determined quantum;quantum determined;regularization term;determined quantum state;permutation based;permutation columns propose", "pdf_keywords": ""}, "2d9769ce319a8acbe97438b45b0d381db2a538d1": {"ta_keywords": "3d reinforcement model;reinforcement model e18;3d reinforcement;model e18 team;dimensional 3d reinforcement;reinforcement model;e18 team;reinforcement;model e18;3d;performance dimensional 3d;dimensional 3d;team;e18;study performance dimensional;performance dimensional;model;performance;study performance;paper study performance;dimensional;study;paper;paper study", "pdf_keywords": ""}, "c143d2b09bdfc0dff784dce2668fd5657806dbf2": {"ta_keywords": "challenge explanations consist;challenge explanations;humans performing decision;shared task experiments;task experiments;team team competition;team competition;task experiments performed;task participants;results team team;results team;scientific knowledge world;performing decision making;speed challenge explanations;explanations consist average;training evaluation;performing teams significantly;performing decision;explanations consist;participants;competition multi decision;lipkin meshkov glick;training evaluation data;performance terms ranking;competition;competition multi;team competition time;performance lipkin meshkov;inference speed challenge;present results team", "pdf_keywords": ""}, "2873053aa18059a61ead5880d449f5bccda2d213": {"ta_keywords": "players interdependent services;undertake game theoretic;social dilemma player;game theoretic analysis;individuals social dilemma;game theoretic;interdependent services motivated;social dilemma;interdependent services;services motivated problems;players interdependent;reward player predecessor;dilemma player;player predecessor services;model players interdependent;equilibrium distribution family;equilibrium;theoretic analysis equilibrium;reward player;undertake game;services motivated;dilemma player allowed;services controlled;services controlled player;predecessor services controlled;schedule services;family individuals social;accrue reward player;model players;reward", "pdf_keywords": "interdependent scheduling games;schedule services game;scheduling games;player interdependent scheduling;scheduling games multiple;player scheduling;services game theoretic;player scheduling service;prevent player scheduling;schedule player interdependent;scheduling service;schedule tasks maximizes;scheduling;nash equilibrium;services game;interdependent scheduling;games multiple agents;pure nash equilibrium;schedule services;scheduling literature;game theoretic;schedule player;scheduling literature dependencies;scheduling service prerequisites;schedule tasks;maximizes reward player;response schedule player;tasks maximizes reward;services order maximizes;tasks maximizes"}, "fee62123e1d2ac56065675983475b079e1e9106f": {"ta_keywords": "trained greedy decoding;learned beam decoding;cross entropy trained;greedy decoding;neural sequence models;entityrecognition ccg supertagging;greedy decoding cross;new decoding;entropy trained;performance new decoding;decoding cross entropy;decoding baselines;cross entropy learned;decoding algorithm neural;entropy learned beam;beam decoding baselines;decoding;sequence tasks;entropy trained greedy;neural sequence;algorithm neural sequence;entropy learned;beam decoding;results sequence tasks;new decoding method;optimize beam search;sequence models avoids;sequence tasks named;ccg supertagging;decoding cross", "pdf_keywords": "sequence learning neural;sequence sequence learning;sequence learning;machine translation;search decoding;beam search decoding;machine translation summarization;training objective sequence;sequence seq2seq models;objective sequence sequence;translation summarization image;decoding algorithm models;decoding;summarization image captioning;test time decoding;learning neural;learning metric beam;seq2seq models sequence;translation summarization;search decoding procedure;objective sequence;sequence sequence seq2seq;summarization image;models sequence sequence;used machine translation;sequence seq2seq;itera tive;image captioning;captioning;decoding algorithm"}, "a96e05353032cc6f3d72eb5eca192295beac065e": {"ta_keywords": "stacked graphical learning;graphical learning meta;inference stacked graphical;graphical learning;class graphical learning;faster inference stacked;graphical learning methods;stacked graphical;graphical learning shown;inference stacked;graphical learning applied;demonstration lifelong learning;called stacked graphical;meta learning scheme;lifelong learning;base learner;meta learning;learning meta learning;graphical models faster;years stacked graphical;learning scheme;outperform graphical models;learning methods called;learning scheme base;lifelong learning process;base learner augmented;methods called stacked;learning methods;graphical models;learning", "pdf_keywords": ""}, "c52ac453e154953abdb06fc041023e327ea609a4": {"ta_keywords": "attention acoustic modeling;attentional acoustic models;self attention acoustic;self attentional acoustic;attention acoustic;attentional acoustic;acoustic models;acoustic modeling;context range models;acoustic modeling computational;acoustic models demonstrate;self attention method;attention method encoding;self attention heads;learn linguistically plausible;attention heads learn;acoustic modeling proposing;encoding sequences vectors;attention method;attention heads;propose gaussian biasing;self attention;representations hybrid models;self attentional;apply acoustic modeling;attentional;gaussian biasing;attention;demonstrate self attention;range models", "pdf_keywords": "attentional acoustic models;attentional acoustic modeling;deep recurrent neural;models rnns;attention encoder;attention encoders;encoders self attentional;attention encoders self;recurrent neural network;self attentional acoustic;attentional acoustic;hybrid models rnns;models rnns effective;previous attention encoders;deep recurrent;recurrent neural;learning architectures acoustic;sequences self attention;attention encoder interpretable;corpus attention encoder;deep learning architectures;rnns;propose deep recurrent;deep learning;acoustic modeling promising;acoustic models;acoustic models recently;rnns effective;attention applied sequence;corpus attention"}, "7ac4227d0b4d38b16da27ed55bd53ce240a32404": {"ta_keywords": "spin heisenberg antiferromagnet;recognition speech;spin heisenberg;spin spin;spin interaction;heisenberg antiferromagnet;spin spin interaction;automatic speech;spin polarized magnetic;speech recognition;spin interaction described;entanglement spins spin;spin;speech recognition speech;spins spin heisenberg;heisenberg antiferromagnet manipulated;spin polarized;recognition speech recognition;antiferromagnet;entanglement spins;demonstrate entanglement spins;various automatic speech;automatic speech recognition;spin direction spin;spin orbit interaction;spins spin;spin direction;antiferromagnet manipulated;spin orbit;end speech translation", "pdf_keywords": "end speech recognition;device automatic speech;automatic speech recognition;automatic recognition speech;speech recognition;speech recognition ar;speech recognition speech;automatic speech;recognition speech utterances;speech recognition recurrent;recognition speech recognition;performance automatic speech;speech recognition systems;recognition speech;speech recognition based;utterances recorded tablet;speech recognition central;setting automatic speech;industries automatic speech;speech utterances research;end utterances recorded;speech utterances;utterances recorded;end utterances;recurrent neural networks;end end utterances;utterances research;utterances;end speech;end end speech"}, "2f153172b92ea32f242d9cb6b94d162e52ef5f0b": {"ta_keywords": "functionfree prolog;functionfree prolog clauses;prolog clauses ground;hardness natural learning;prolog;learning description logic;determinate functionfree prolog;hard learning log;learning straightline programs;prolog clauses;learning log depth;natural learning problems;subconcepts learning arity;dual dfa learning;dfa learning;log depth boolean;binary representation;learning log;representation binary;dfa learning problem;representation binary representation;learning problems;problem learning straightline;learning straightline;description logic classically;learning arity determinate;depth boolean circuits;hard learning;problem learning representations;learning problem np", "pdf_keywords": ""}, "4a348e4725a2bc677e4aa40aa63c1421e8f335c9": {"ta_keywords": "performance skimming skimming;skimming skimming laminar;classifiers;classification;skimming skimming;performance skimming;random walk skidding;classification paper present;skidding;skimming laminar;field performance skimming;success binary classifiers;skid skidding;skimming;skidding skidding;skidding motion;multilabel classification;multilabel classification paper;classification paper;skidding surface;binary classifiers;documents f1 score;skidding skidding motion;flow skidding skidding;walk skidding;used multilabel classification;skid;flow skidding;skidding motion modeled;skid skidding trajectory", "pdf_keywords": "autonomous text classi;score autonomous text;rare labels classi\ufb01er;autonomous text;subset rare labels;achievable score optimal;rare labels;achievable score autonomous;metric maximize macro;maximizing performance metrics;optimal thresholds maximum;optimal thresholds;study multilabel;predicting instances positive;relationship optimal thresholds;tags controlled vocabulary;score optimal decision;case study multilabel;predicting instances;uninformative predicting instances;multilabel;score autonomous;score optimal;threshold classi\ufb01er uninformative;study multilabel setting;labels classi\ufb01er;titles abstracts;terms articles biomedical;labels classi\ufb01er uninformative;making threshold classi\ufb01er"}, "c2a79e2a65b721d4de5f6d4806323174b9f8f393": {"ta_keywords": "zero label learning;label learning natural;label learning train;labeled data achieving;augments labeled data;label learning;human labeled data;trained human labeled;explores zero label;human annotations;real human annotations;labeled labeled data;labeled data;labeled data labeled;zero label;human annotations method;labeled data approach;data labeled;data labeled data;labeled;labeled labeled;novel data augmentation;enables zero label;annotations;training data real;human labeled;learning natural language;data augmentation;training data creation;annotations method enables", "pdf_keywords": "text representations trained;language model learn;learning machine translation;unsupervised learning language;benchmarks shot inference;learning language unsupervised;language models underperforms;learn contextualized text;shot learning;language understanding benchmarks;machine translation;shot learning results;giant language models;language models;contextualized text representations;learn contextualized;translation language models;model learn contextualized;machine translation language;learning language;shot inference;text representations;memorizing facts training;language unsupervised achieves;small training data;language model unsupervised;language models shown;language understanding;directional language model;natural language understanding"}, "3b00e642de51d0f8378c7c35eca89f2ecb6f3af8": {"ta_keywords": "inherited microduplication;11 inherited microduplication;microduplication parent reportedly;inherited microduplication parent;18 individuals microduplications;microduplications region reported;microdeletion syndrome region;individuals microduplications;individuals microduplications 22q11;microdeletion syndrome;22q11 microdeletion syndrome;21 microdeletion syndrome;retardation congenital anomalies;congenital anomalies;study microduplication diaphragm;presents study microduplication;microduplication parent;microduplication diaphragm;23 microduplications region;congenital anomalies 12;study microduplication;microduplications region;mental retardation congenital;duplications predicted;microduplication;11 23 microduplications;microduplications 22q11 21;syndrome region frequency;syndrome region recently;23 microduplications", "pdf_keywords": ""}, "7a6c61b57bac074f7cd85963fd13da8f3321e087": {"ta_keywords": "topic model blogs;topic discovery estimation;unsupervised topic discovery;latent topic model;topic discovery;topic model;probabilistic latent topic;blogs informative popular;hyperlinks models topic;specific influence blogs;relevant user topics;blogs informative;influence blogs;unsupervised topic;influential blog postings;influence blogs paper;user topics;access blogs informative;blogs model;hyperlinks models;latent topic;blog postings topic;highly influential blog;link prediction;topic specific influence;influential blog;user topics propose;topic user work;link prediction extension;blogs", "pdf_keywords": ""}, "8c25e1c223fc70509172a32111c91fe4b9f86a56": {"ta_keywords": "software defined radio;sdr cognitive radio;wireless networks require;radio sdr cognitive;modern wireless networks;broad realization wireless;processor programmable routers;realization wireless networks;programmable routers;cognitive radio;defined radio sdr;defined radio;wireless networks;trends modern wireless;programmable routers device;modern wireless;ossification internet decentralized;radio sdr;realization wireless;ossification internet;device level programmability;radio cr programmable;wireless networks paper;programmability broad realization;programmable mac processor;programmable mac;mac processor programmable;programmability broad;support programmability broad;lack ossification internet", "pdf_keywords": "virtualization wireless networks;virtualization wireless;accomplished virtualization wireless;wireless programmable networking;network virtualization;network virtualization emerging;networks network virtualization;programmable wireless networks;programmable networking;virtual data center;new virtualization paradigm;future wireless programmable;future programmable wireless;programmable networking requires;virtualization virtualized devices;virtualized devices;propose new virtualization;programmable wireless;virtualized devices virtual;virtualization paradigm;wireless programmable;overview accomplished virtualization;devices virtual data;abstractions future wireless;virtualization virtualized;virtualization emerging;called virtualization virtualized;virtualization emerging technology;virtualized;new virtualization"}, "4a36a00db217fd98f1bd943aa2f2d6303adbc456": {"ta_keywords": "fair principal component;formulation fairness good;formulation fairness;mathematical formulation fairness;variance fairness runtime;fairness runtime present;defines fair principal;fair pca subject;fair pca;fairness runtime;principal component analysis;pca minimizing maximum;dimensionality reduced conditional;variance fairness;analysis pca minimizing;pca minimizing;fairness good statistical;explained variance fairness;optimization stiefel manifold;hyperparameter practical settings;hyperparameter practical;constraints nonconvex optimization;nonconvex optimization stiefel;mmd constraints nonconvex;nonconvex optimization;problem fair pca;component analysis pca;effect hyperparameter practical;principal component;reduced conditional distributions", "pdf_keywords": "unconstrained manifold optimization;fair pca constrained;fairness supervised learning;characterizations fair pca;manifold optimization;manifold optimization solver;unsupervised learning;matrices fairness research;fairness supervised;unsupervised learning algorithm;matrices fairness;new unsupervised learning;pca constrained optimization;function matrices fairness;propose fairness supervised;machine learning optimization;optimization stiefel manifold;new unconstrained manifold;unconstrained manifold;statistical characterizations fair;learning optimization;based fair pca;mmd based fair;fair pca;fair pca asymptotic;pca constrained;algorithmic bias prediction;characterizations fair;constrained optimization stiefel;conditions orthonnormality optimizing"}, "101d619b5911e9c2fda6f02365c593ae61617cb6": {"ta_keywords": "persuasive dialogue systems;cooperative persuasive dialogue;persuasive dialogue policies;persuasive dialogue humans;persuasive dialogue based;implementing dialogue policy;persuasive dialogues human;dialogue policies using;implementing dialogue;dialogue policies;tailored persuasive dialogue;common persuasive dialogue;persuasive dialogue;persuasive dialogues;learning cooperative persuasive;corpus persuasive dialogues;dialogue policy;method implementing dialogue;dialogue systems;dialogue policy office;dialogue humans;dialogues human interlocutors;dialogue based;dialogue based corpus;dialogue systems use;dialogue humans paper;constructing cooperative persuasive;dialogue rule;dialogues human;dialogue", "pdf_keywords": ""}, "d4762619b55c65120307ceebe4a0646984f6045a": {"ta_keywords": "accurately model translation;automatic speech;statistical machine translation;model translation probabilities;observed translation patterns;dependent translation model;frequently observed translation;machine translation;wfs automatic speech;translation probabilities log;automatic speech recognition;translation model;translation patterns;translation probabilities;model translation;context dependent translation;translation patterns given;translation model allows;automatically modeling dynamics;automatically modeling;machine translation smt;speech recognition;speech recognition asr;dependent translation;observed translation;context accurately model;state transducers;finite state transducers;translation smt analysis;readable transcripts", "pdf_keywords": ""}, "50ec3d960ac458573a1e4a1556420c5e96d58609": {"ta_keywords": "propelled particle viscous;annotations training;self propelled particle;propelled particle;evidence annotations training;performance fully supervised;supervised state art;models learn evidence;supervised;fully supervised;fully supervised state;neural approaches require;corpus distant supervision;self propelled;annotations training paper;dynamics self propelled;supervision answer labels;models learn;particles moving fluid;large corpus;learn evidence large;particle viscous flow;neural approaches;propelled;annotations;moving viscous flows;evidence annotations;supervised state;learn evidence;particle viscous", "pdf_keywords": "evidence annotations training;hop question answering;intermediate evidence annotations;evidence large corpus;question answering benchmarks;evidence annotations;acquiring labeled evidence;passage wikipedia pages;question answering;tasked evidence reranking;annotations training passage;labeled evidence pieces;evidence reranking span;annotations training;trained intermediate evidence;evidence reranking;intermediate evidence pieces;evidence labels training;wiki setting corpus;large corpus;wikipedia pages evaluate;large corpus reader;answer state art;evidence pieces;multi tasked evidence;labeled evidence;answering benchmarks;evidence labels;reranking span extraction;retriever \ufb01nds evidence"}, "d59c7b1c85f8c459863762361f251575785347a8": {"ta_keywords": "porous membranes nonequilibrium;model porous membranes;membranes nonequilibrium;permeability polydisperse pores;membranes nonequilibrium event;pore volume membranes;volume membranes highly;membranes highly permeable;porous membranes;volume membranes;permeability smaller particle;cylindrically porous membranes;polydisperse pores average;membranes consists hard;permeability polydisperse;molecular dynamics simulations;porous membranes consists;make membrane isoporous;separation hard sphere;permeability smaller;mean permeability polydisperse;separation processes;membrane isoporous;polydisperse pores;membranes highly;gases cylindrically porous;intrinsic permeability;molecular dynamics;intrinsic permeability selectivity;membranes", "pdf_keywords": ""}, "35750f1908f405bb38b0708972f33fe07b378b64": {"ta_keywords": "interpretability theory arithmetical;arithmetical semantics interpretability;logics relative interpretability;relative interpretability logicians;logic il interpretability;classical logic interpretability;interpretability logic;interpretability logicians;interpretability logic propositional;logic interpretability binary;logic interpretability;interpretability logic zagreb;interpretability logic based;interpretability axioms propositional;il interpretability logic;semantics interpretability logic;study interpretability logic;interpretability binary modal;interpretability logicians considered;relative interpretability studied;interpretability axioms;arithmetical semantics;relative interpretability;1981 interpretability logic;semantics interpretability;theory arithmetical semantics;modal logics relative;logicians considered modal;interpretability binary;principles interpretability axioms", "pdf_keywords": ""}, "90766546b29836eb96f54fe8fd70ec51a3e699ba": {"ta_keywords": "predicting protein structures;architecture predicting protein;protein structures convolution;recognition protein binding;3d structures proteins;predicting protein;structures convolution pooling;recognition protein;structures using deep;cnn constructed;protein binding interfaces;cnn constructed voronoi;example recognition protein;network cnn;structures proteins;structures proteins complexes;protein structures;network cnn constructed;convolutional neural networks;vorocnn example recognition;protein structures using;analysis protein structures;protein binding;convolutional neural network;structures convolution;neural network cnn;dimensional molecular structures;proteins complexes;deep convolutional neural;proteins complexes reviewed", "pdf_keywords": "tessellation 3d protein;dimensional 3d protein;protein folds convolutional;3d protein structures;3d cnns;3d protein;3d protein folds;graph convolutional networks;learning voronoi tessellations;deep learning voronoi;3d cnns combines;graphs based convolutional;called 3d cnns;versatility 3d protein;graph convolutional;convolutional networks work;convolutional networks;using graph convolutional;structure prediction protein;protein structure graphs;protein structure prediction;folds convolutional neural;predicting protein structure;voronoi tessellation 3d;prediction protein structure;convolutional networks paper;convolutional neural networks;based convolutional networks;tessellations using graph;learning voronoi"}, "7181a5139301c8a407da75a105dd457bf03d7057": {"ta_keywords": "stochastic network optimization;markovian network optimization;generalize network optimization;flows heterogeneous planning;network optimization;network optimization problems;network optimization primal;network optimization accommodate;stochastically extension wadrop;stochastic network;provide dynamic stochastically;dynamic programming;heterogeneous planning time;dynamic stochastically;class stochastic network;heterogeneous planning;markovian network;dynamic programming based;generalize network;dynamic stochastically extension;stochastic;design dynamic programming;wadrop equilibrium;emph markovian network;stochastically;markovian;multi commodity flows;wadrop equilibrium principle;flow multi commodity;game theoretic", "pdf_keywords": ""}, "af553d6121d338fc74dbd5faa43d5383a222198d": {"ta_keywords": "communication skills autism;skills autism;skills autism spectrum;social skills communication;social communication skills;skills communication difficulties;verbal communication skills;autism spectrum;people social skills;skills communication;communication skills use;social skills;communication skills;autism;training non verbal;non verbal communication;improve social communication;social communication;verbal communication;communication difficulties;autism spectrum quotient;safe activities indoor;verbal communication important;communication difficulties greater;difficulties improve social;computer based training;safe activities;social interactions;safe safe activities;enable people difficulties", "pdf_keywords": ""}, "a8315b5d3ff1b834fb58420397b13b9d169efad1": {"ta_keywords": "split citation based;split citation;citation based concept;proposed disambiguation mechanism;problem split citation;paper propose disambiguation;propose disambiguation mechanism;disambiguation mechanism;citation based;names publication;multiple publication attributes;concept soft clustering;proposed disambiguation;disambiguation mechanism uses;names publication venues;propose disambiguation;disambiguation;uses multiple publication;soft clustering;author names publication;distinguishing similar entities;multiple publication;efficiency proposed disambiguation;publication attributes metadata;soft clustering paper;publication venues titles;clustering paper propose;clustering;publication attributes;titles distinguishing similar", "pdf_keywords": ""}, "83ddc47f6dd0434c12eff9e4e42b727217a200a8": {"ta_keywords": "agents learn deterministic;learning dynamics convergence;learning dynamics;probability agents converge;learn deterministic;games agents learn;equilibrium finite time;agents learn;rates learning dynamics;agents converge neighborhood;learner initial condition;continuous games agents;non uniform learning;agents converge;convergence guarantees deterministic;consider continuous games;nonuniform learning rates;continuous games;nonuniform learning;uniform learning rates;deterministic version game;uniform learning;guarantees deterministic;learning rates;effects nonuniform learning;stable local equilibrium;learn deterministic settings;local equilibrium finite;probability agents;equilibrium finite", "pdf_keywords": "learning dynamics;learning games continuous;learning agents;non uniform learning;games consider gradient;multi agent learning;agent learning;learning dynamics called;autonomous learning agents;deep reinforcement learning;reinforcement learning self;agent learning algorithms;continuous games;uniform learning;reinforcement learning;learning algorithms autonomous;learning agents paper;deep reinforcement;imperfect information games;games continuous action;uniform learning rates;learning games;learning dynamics class;algorithms non cooperative;called deep reinforcement;player continuous games;individual gradients unbiased;learning algorithms non;learning self play;learning rules agent"}, "48ea80b65f42e9fbb96b856286d12347d1df52d2": {"ta_keywords": "evaluation commonsense reasoning;commonsense reasoning dataset;novel commonsense reasoning;commonsense reasoning facilitate;reasoning dataset dense;commonsense reasoning;reasoning models;language understanding reasoning;reasoning facilitate future;reasoning dataset;reasoning facilitate;understanding reasoning models;introduce tiered reasoning;evaluating underlying reasoning;language understanding tasks;tiered reasoning;better language understanding;tiered reasoning intuitive;verifiable evaluation commonsense;dense annotations;reasoning models paper;trained language models;reasoning intuitive physics;understanding reasoning;underlying reasoning;dataset dense annotations;reasoning intuitive;language understanding;underlying reasoning process;language models", "pdf_keywords": "physical commonsense reasoning;tieredreasoning intuitive physics;dataset physical commonsense;commonsense reasoning;structured commonsense knowledge;trained language models;commonsense reasoning called;reasoning language;reasoning process train;targeting physical commonsense;large scale language;reasoning language understanding;consistent reasoning language;introduce tieredreasoning intuitive;language models come;structured commonsense;language model predict;language models pre;trained language;trained language model;language understanding;procedural reading comprehension;semi structured commonsense;commonsense reasoning paper;commonsense knowledge;prediction procedural reading;models pre trained;tieredreasoning intuitive;language models 20;commonsense knowledge inject"}, "fa2657c0d66f048dee6b080536abbd1f947e822f": {"ta_keywords": "estimating brain age;brain age estimation;brain age correlated;brain age based;model brain age;age estimation bias;brain age;age based brain;age estimation accuracy;chronological brain age;effective age estimation;age estimation;age estimation paper;brain age curate;demonstrate age estimation;estimating brain;method estimating brain;age correlated chronological;age correlated;age distribution;mtri demonstrate age;age distribution adult;bias certain age;brain mris healthy;dataset structural brain;life span evaluation;brain magnetic resonance;brain mris;adult life span;age based", "pdf_keywords": "estimating brain age;aging analysis cognitive;brain age based;brain age derived;aging analysis;brain age;suggest brain age;estimating brain;method estimating brain;neuroimaging datasets representing;neuroimaging datasets;examine aging;neuroimaging datasets demonstrate;studying aging;open neuroimaging datasets;biomarker predictive cognitive;study examine aging;life span aging;aging process estimated;span aging analysis;utility studying aging;age derived models;center neuroimaging datasets;estimated age derived;aging;studying aging process;estimated age life;estimate age;examine aging process;estimate age healthy"}, "eb1b89751cac821792df36d3a1a2fb01dc4db2d1": {"ta_keywords": "dynamics coupled pendulum;coupled pendulum;coupled pendulum pendulum;pendulum oscillators case;oscillators case pendulum;pendulum driven external;pendulum pendulum oscillators;pendulum oscillators;external force pendulum;pendulum driven;temperature dynamics coupled;pendulum pendulum;pendulum described following;pendulum described;case pendulum driven;pendulum;case pendulum;force pendulum described;force pendulum;temperature dynamics;pendulum eq;eq pendulum;effect temperature dynamics;frac eq pendulum;eq pendulum eq;oscillators;oscillators case;dynamics coupled;effect temperature;temperature", "pdf_keywords": ""}, "120839995e64f8ed734b5249ab681328c4955f5d": {"ta_keywords": "mdp congestion game;entropic mutual information;optimal toll induced;congestion game;optimal toll;algorithms employed game;playing markov decision;determine optimal toll;optimal strategy clear;mutual information;mutual information eit;entropic mutual;optimal strategies formulate;congested stochastic network;strategies formulate toll;decision makers adaptively;markov decision process;optimal strategy;entanglement called entropic;consider toll design;designer congested stochastic;optimal strategies;toll synthesis problem;markov decision;toll induced equilibrium;toll design problem;mdp congestion;process mdp congestion;stochastic network decision;congestion", "pdf_keywords": ""}, "4d16a47fb6708704b155855045c9e5d2ea380bb0": {"ta_keywords": "sentiment analysis social;sentiment analysis languages;sentiment analysis;learning methods sentiment;methods sentiment analysis;approach sentiment analysis;social media dataset;approach sentiment;methods sentiment;sentiment;analysis social media;novel approach sentiment;classifiers;features classifiers;machine learning;machine learning methods;social media explore;various features classifiers;social media;movie product reviews;research machine learning;product reviews;created social media;analysis social;product reviews paper;automatic recognition article;learning methods;reviews paper present;reviews;reviews paper", "pdf_keywords": ""}, "7212cca9be971997434c2b3a27411a163bbd89c3": {"ta_keywords": "conditioning inference improve;previous inference conditioning;conditioning uses predictions;variable hidden markov;conditioned conditioning inference;hidden markov chain;hidden markov;conditioning inference;searched intermediate conditioning;predictions previous inference;inference conditioning inference;inference conditioning;conditioning framework;intermediate prediction latent;tractable conditioning framework;conditioning framework propose;new conditioning methods;conditioning methods based;conditioning methods;intermediate conditioning;conditioned conditioning;self conditioned conditioning;conditioning re\ufb01nes intermediate;intermediate conditioning re\ufb01nes;enable better conditioning;prediction latent;self conditioned ctc;multi pass conditioning;conditioning;tractable conditioning", "pdf_keywords": "end speech recognition;conditioning intermediate predictions;recurrent neural networks;automatic speech;neural networks conditioning;handles intermediate prediction;conditioning prediction;prediction tractable conditioning;improved inference conditioning;intermediate prediction tractable;conditioning prediction results;intermediate prediction latent;model intermediate prediction;path conditioning prediction;speech recognition;speech recognition machine;automatic speech recognition;recognition based recurrent;intermediate prediction;recurrent neural;end end speech;end speech;marginalized intermediate prediction;based recurrent neural;intermediate prediction similarly;inference self conditioned;conditioned ctc probabilistic;conditioning searched intermediates;self conditioned ctc;conditioning intermediate"}, "6bb2b856d9a9b873259ba9dc48bc450c96eb3318": {"ta_keywords": "semiconductor quantum dot;quantum dot qd;quantum dot;transcribing electrons semiconductor;dot qd proposed;electrons semiconductor quantum;semiconductor quantum;transcribing electrons;qd proposed promising;dot qd;electrons semiconductor;candidate spintronic devices;spintronic devices;promising candidate spintronic;qd proposed;quantum;semiconductor;candidate spintronic;qd;electrons;spintronic;dot;transcribing;devices;proposed promising;proposed promising candidate;promising candidate;proposed;promising;candidate", "pdf_keywords": "segmentation speech cost;updating segmentation speech;speech cost sensitive;speech cost;segmentation speech;automatic speech transcription;segmentation method transcribers;transcribes entire speech;speech transcription;automatic speech;new approach transcriber;approach transcriber;transcriber speci cost;approach transcriber speci;transcriber transcribes;segments convenient transcribe;allowing transcribers productive;transcribers;transcribers productive start;transcribers productive;transcriber speci;convenient transcribe;transcribes;transcriber;speci cost modeling;speech transcription process;transcribers maximizes;method transcribers maximizes;transcribe;segments asr transcript"}, "79b8ef3905a42b771248719495a2117271906445": {"ta_keywords": "energy cost carbon;optimize energy cost;estimating energy cost;improve energy efficiency;energy cost energy;effective energy cost;energy cost;energy efficient;energy efficiency;cost energy;optimize energy;energy cost machine;cost machine learning;measure energy cost;energy cost quantifies;cost energy harvesting;cost quantifies energy;energy cost measure;quantifies energy cost;possible optimize energy;cost measure energy;energy efficiency equivalent;large neural network;estimating energy;investigates energy cost;sparsely activated dnns;machine learning grown;cost carbon footprint;2x energy efficient;energy cost given", "pdf_keywords": "machine translation transformer;energy efficiency cloud;efficiency cloud;efficiency cloud datacenters;translation machine;machine translation machine;machine translation;propose machine translation;translation machine translation;cloud datacenters;estimates energy usage;cloud datacenters businesses;cost energy efficiency;translation transformer;cost energy;energy usage;energy efficiency;carbon footprint enterprise;enterprise datacenter;evaluate cost energy;energy usage carbon;sustainable energy energy;use artificial neural;datacenters;sustainable energy;datacenters businesses;cloud;datacenter;footprint enterprise datacenter;translation transformer sparsely"}, "1c709eef701d933af1383c790c13209f06806b60": {"ta_keywords": "greedy rationalization language;machine translation predictions;nlp systems predictions;rationalization language modeling;greedy rationalization best;predictions rna sequences;modeling machine translation;language modeling machine;greedy rationalization;language modeling;optimizing sequential;study greedy rationalization;machine translation;translation predictions;best optimizing sequential;model predictions rna;nlp systems;rationalization language;nlp;rna sequences critical;modern nlp systems;modern nlp;optimizing sequential objective;rna sequences;rationalization best optimizing;translation predictions incomplete;predictions rna;greedy algorithm approximate;sequential objective provides;sequential objective", "pdf_keywords": "attention based explanation;rationalization gradient attention;modeling machine translation;learning machine translation;word level explanations;rationalization machine translation;gradient attention methods;machine translation model;gradient attention;explanations detect;data gradient attention;model explanations detect;predictions interpretations interpretability;sequence models words;gradient attention based;language models;translation model trained;language models widely;machine translation;explanations detect biases;translation language models;interpret sequence models;encode predictions interpretations;model explanations;interpretability methods;language modeling machine;machine translation language;attention based;language modeling;attention methods"}, "0de86afbf91d0cf3e595a23a5b7a4d19deefb891": {"ta_keywords": "model bifurcations;known bifurcations;model bifurcations match;bifurcations;known bifurcations lie;minimal model bifurcations;bifurcations lie;points known bifurcations;optimal parameter regimes;bifurcations match;estimating parameters differential;bifurcations match specified;perturbation dynamical systems;dynamical systems;parameter regimes;perturbation dynamical;define cost landscape;biifurcation measure gradients;dynamical systems theory;bifurcations lie function;differential equation models;external perturbation dynamical;controlled condition space;theory change points;estimating parameters;optimisers optimal parameter;cost function;dynamical;cost landscape terms;cost landscape", "pdf_keywords": "models bifurcation;bifurcation analysis;bifurcation diagrams;bifurcations model;complexity study bifurcations;speci\ufb01ed bifurcation diagrams;bifurcations;node bifurcations;bifurcation diagrams demonstrate;bifurcation theory provides;study bifurcations;models bifurcation parameter;bifurcations model form;saddle node bifurcations;approach models bifurcation;bifurcation;study bifurcations model;bifurcation theory;bifurcation analysis paper;bifurcation parameter;approach bifurcation analysis;approach bifurcation;speci\ufb01ed bifurcation;user speci\ufb01ed bifurcation;based approach bifurcation;bifurcations damped oscillations;direction bifurcation theory;bifurcations damped;bifurcation parameter rise;direction bifurcation"}, "4f02d8775123624088a91fcfff20625463e5239a": {"ta_keywords": "education personalized prediction;intelligent learning analytics;driven collaborative filtering;collaborative filtering regression;propose collaborative filtering;experts learning analytics;learning analytics collaborative;collaborative filtering;learning analytics based;collaborative filtering algorithm;collaborative filtering fully;analytics collaborative filtering;personalized prediction test;learning analytics;collaborative filtering approach;personalized prediction;approach collaborative filtering;filtering regression experts;education personalized;filtering approach predict;predicting test outcomes;large education data;education data;englishlanguage learning experimental;data driven collaborative;predicting test;englishlanguage learning;applications englishlanguage learning;enormous education data;model intelligent learning", "pdf_keywords": ""}, "614dc4001ad68cac31484887f16542f04693eca4": {"ta_keywords": "computational complexity bribery;complexity bribery;complexity propose probabilistic;complexity bribery problem;bribery problem fluid;bribery criteria models;stochastic environment dynamics;influence voter preferences;choice evaluation bribery;probabilistic;voter preferences voting;probability vote desired;dynamics underwater bouncer;bribery;bribery criteria;dynamics computationally;bribery problem;criteria bribery;probability vote;evaluation bribery;dynamics computationally complex;propose probabilistic;voter preferences;preferences voting;stochastic;terms probability vote;influence voter;influence computational complexity;stochastic environment;dynamics underwater", "pdf_keywords": ""}, "3261728694c0a53a2e8f95326f94147a28e03a83": {"ta_keywords": "computed deep quantization;quantized training deep;deep quantization;deep quantized training;sinare deep quantized;deep quantization reduces;deep quantized;approach deep quantization;bitwidth assignment quantization;quantization reduces bitwidth;quantized training algorithms;assignment quantization large;sinareq enhancing quantized;quantized training;quantization large;enhancing quantized training;networks dnn;optimal bitwidth layer;training deep;computed deep;quantization;11 computed deep;assignment quantization;deep networks alexnet;training deep neural;networks dnn make;neural networks dnn;enhancing quantized;quantization reduces;performance deep neural", "pdf_keywords": "quantize neural networks;quantizing neural networks;quantization neural networks;deep quantization;automatically quantize neural;automatically quantizing neural;deep quantization used;problem deep quantization;quantizing neural;layer wise quantization;quantize neural;quantization neural;training automatically quantize;neural network regularizer;quantization method weighted;accuracy prone quantization;wise quantization;quantization;train deep neural;deep neural networks;automatically quantize;deep neural network;prone quantization neural;quantization method;new quantization method;new quantization;automatically quantizing;present new quantization;deep neural;neural networks large"}, "80ef8b8a1284790e0d8f7cbf9727c9e0b2a89332": {"ta_keywords": "black box predictors;predictive described;improved prediction;predictive;prediction;predictive predictive sudden;predictors;predictive predictive;predictive sudden change;prediction high dimensional;box predictors;predictors presented;estimates improved prediction;response predictive predictive;improved prediction high;classifiers;predictors presented introduce;predictive sudden;prediction high;predictive described follows;response predictive described;response predictive;box predictors presented;customer response predictive;classifiers test;dimensionality black box;datasets natural;quantify shift training;correct classifiers;classifiers test set", "pdf_keywords": "learning label shift;label shift estimation;learning label;label shift assumption;category learning estimated;human category learning;label distribution weights;label shift known;category learning;shifted label distribution;estimation shifted label;assumptions label shift;kernel mean matching;heuristic label shift;labeled data;estimated using labeled;using labeled data;label shift;shift label;shifted label;label shift label;label distribution;correction based kernel;labeled;shift label shift;learning estimated;using labeled;set label shift;label;labeled data source"}, "843966d4b567033abff9775c5958f7be4db5c0ad": {"ta_keywords": "child sudden change;response child sudden;child sudden;model response child;response child;change state mother;state mother children;sudden change state;mother children;child;state mother;children;sudden change;present simple model;model response;simple model response;change state;mother;simple model;response;model;present simple;sudden;change;present;state;simple;paper present;paper present simple;paper", "pdf_keywords": ""}}