{"9c61f5f51a232049635e6f3441e6397af4d91298": {"ta_keywords": "oscillator harmonic trap;trap harmonic oscillator;coupled harmonic trap;harmonic trap;harmonic trap harmonic;trap harmonic;generated harmonic oscillator;harmonic oscillator generated;harmonic oscillator coupled;dimensional harmonic oscillator;oscillator generated harmonic;oscillator coupled harmonic;harmonic oscillator;harmonic oscillator harmonic;oscillator harmonic;dynamics dimensional harmonic;generated harmonic;coupled harmonic;oscillator coupled;dimensional harmonic;oscillator generated;harmonic;oscillator;trap;dynamics;dynamics dimensional;study dynamics dimensional;coupled;study dynamics;generated", "pdf_keywords": ""}, "a05c3e8bd6dacbd192ffa28543e60e2c93c66d76": {"ta_keywords": "analyzing dynamics tweets;analyse dynamics tweets;analyze dynamics tweets;dynamics tweets micro;dynamics tweets generated;dynamics tweets;tweets micro world;graph tweets generated;graph tweets;analysis graph tweets;tweets generated;tweets micro;tweets generated users;tweets;users micro world;micro world;users micro;generated users micro;analyse dynamics;tool analyzing dynamics;used analyse dynamics;analyzing dynamics;micro world tool;analyze dynamics;dynamics;based analysis graph;new tool analyzing;world tool;world tool based;world tool used", "pdf_keywords": ""}, "bfa10ea6a4c9fa585f21f39858da517c31a76343": {"ta_keywords": "dialogue systems method;dialogue management;dialogue systems;management dialogue systems;method dialogue management;dialogue management dialogue;method dialogue;dialogue;management dialogue;propose method dialogue;decisions based user;decisions based knowledge;make decisions based;based user needs;process selecting user;selecting user;systems method based;based introduction user;based user;introduction user user;selecting;decisions based;make decisions;based knowledge assumed;process selecting;knowledge assumed process;assumed make decisions;selecting user list;systems method;based knowledge", "pdf_keywords": ""}, "0bf2a0a3216c79b62b3664c596f44d7a8add498a": {"ta_keywords": "curriculum introductory computer;computer science courses;introductory computer science;introductory computer;fiction reviews curriculum;reviews curriculum;curriculum introductory;science courses taught;computer science;science courses;curriculum;reviews curriculum divided;science fiction reviews;courses taught;introductory;courses;sections introductory;sections introductory focused;introductory focused;introductory focused old;student current knowledge;current knowledge subject;topics;science fiction;courses taught form;real science fiction;topics contents;description student current;research topics contents;curriculum divided", "pdf_keywords": ""}, "a4cb2a401c78bfafee69e823306b0cc9e4d673db": {"ta_keywords": "fair assignment papers;assignment papers fair;papers fair order;papers fair phase;papers fair;round robin mechanism;number papers fair;mechanism fair assignment;assignment papers;fair order relaxation;order relaxation submodularity;robin mechanism fair;relaxation submodularity;relaxation submodularity yields;novel round robin;mechanism fair;round robin;called round robin;phase round robin;submodularity yields;papers;possible number papers;submodularity;classic round robin;submodularity approximation;fair phase round;mechanism ensures minimum;submodularity yields weak;number papers;weak submodularity approximation", "pdf_keywords": "allocating fair reviewer;allocating papers reviewers;fair assignment reviewers;assignment reviewers papers;fair reviewer assignments;paper assigned reviewer;optimizing order reviewer;reviewers papers competitive;optimal assignment paper;optimally allocating papers;assignment reviewer search;set papers reviewers;assigning reviewers;given reviewer assignment;assigned reviewer theorem;assignment reviewer;reviewer assignment problem;reviewer round robin;algorithm fair assignment;papers reviewers;reviewer assignment instance;assignment reviewers;reviewer assignment;reviewers papers;papers reviewers based;consider reviewer assignment;possible assignment reviewer;paper score algorithms;assignment paper algorithm;reviewers set papers"}, "242c73ea34833910ad2643ec3a1096bb18c6d04d": {"ta_keywords": "speaker extraction recurrent;rnn speech extraction;target speaker extraction;speaker extraction;speaker extraction module;network rnn speech;target speaker speech;speech extraction;speech extraction proposed;extraction recurrent network;rnn speech;domain target speaker;extraction recurrent;rate target speaker;performance target speaker;target speaker;joint training framework;recurrent network rnn;speaker speech;recurrent network;network rnn;speaker;training framework combines;rnn;training framework;time domain target;joint training strategy;speech;extraction proposed framework;extraction module", "pdf_keywords": "speaker extraction encoder;target speech extractionin;target speech extraction;speech recognition cnn;target speech encoder;target speaker extraction;speech extraction convolution;target speaker recognition;speech extraction module;speech extraction robust;speaker speech extraction;speech encoder;speech extractionin;timedomain target speaker;target speech data;trained embedding speaker;speech extraction recurrent;predict enhanced speech;speaker extraction;speech data etarget;learning target speech;speech extraction;speaker recognition;speech encoder encoder;speaker recognition speech;speech data;able predict speaker;embeds target speech;module target speech;cnn uncertainty speech"}, "a4df5ff749d823905ff9c1a23b522d3f426a1bb6": {"ta_keywords": "page ranking similarity;ranking similarity measure;personalized page ranking;ranking similarity;page ranking;probabilities random fields;random fields evaluation;probability random fields;similarity measure;random fields;similarity measure evaluation;variable random fields;marginal probabilities random;associated personalized page;marginal probabilities;associated personalized;ranking;marginal probability random;personalized page;evaluation associated personalized;marginal probability discrete;random fields demonstrate;marginal probability;random fields reduced;variable marginal probabilities;fields evaluation associated;personalized;marginal;discrete variable marginal;probabilities random", "pdf_keywords": ""}, "f32108602fb0dbda29030cac780165a4b89048a3": {"ta_keywords": "comparison relations tables;tables use predict;predict comparison relations;tables tables;tables tables tables;predicting comparison relations;relations tables tables;tables;tables tables use;relations tables;tables use;tables form tables;tables tables form;method predicting comparison;relations tables form;tables use method;comparison relations;form tables tables;tables form;use predict comparison;predicting comparison;method predict comparison;predict comparison;form tables;form tables form;novel method predicting;relations;method predicting;method predict;use predict", "pdf_keywords": ""}, "04745fe1306d10c915d27a454c157c837dacefce": {"ta_keywords": "mixture target;phase difference mixture;difference mixture target;separating source target;mixture target introduce;separating source;discrete representation phasebook;discrete representations phase;directly estimates phase;difference mixture;target using discrete;mixture;phase difference;representations phase difference;estimates complex mask;estimates phase difference;phasebook directly estimates;phase;complex mask;problem separating source;representation phasebook directly;representation phasebook;phasebook;source target using;complex mask evaluate;estimates phase;phasebook directly;representations phase;mask evaluate proposed;separating", "pdf_keywords": ""}, "20140fcf0bdd932c1886ff1c7674c23649b1e3b8": {"ta_keywords": "speech synthesis based;based speech synthesis;speech synthesis;speech synthesis proposed;proposed speech synthesis;parameter generation hmm;generation hmm based;hmm based speech;components proposed speech;generation hmm;speech proposed method;component proposed speech;iterative parameter generation;parameter generation method;parameter generation;rich context models;applied generate spectral;synthesis based rich;generate spectral;generate spectral component;synthesis based;based speech;hmm based;generation method rich;context models proposed;context models;approach parameter generation;generation method;synthesis;synthesis proposed method", "pdf_keywords": ""}, "e32177e38060637ac8a2ebc9990d43d1ab8bdb8a": {"ta_keywords": "similarity communities social;based similarity communities;similarity communities;social networks similarities;communities social networks;recommendation systems;users social networks;networks similarities method;recommendation systems based;communities social;networks similarities;problem recommendation systems;social networks;systems based similarity;users social;social networks use;based similarity;similarity;similarities method;communities;similarities method used;performance recommendation systems;improve performance recommendation;information users social;cold start problem;similarities;problem recommendation;solve cold start;recommendation;cold start", "pdf_keywords": ""}, "6680b1e863c394f00307cb3818f7c7d75c9919aa": {"ta_keywords": "distributed network coding;network coding nonmulticast;coding applied distributed;network coding;network coding applied;distributed network;used network coding;distributed storage;distributed storage proposed;network proposed code;approach distributed network;distributed storage concept;applied distributed storage;concept distributed storage;storage proposed code;distributed;new approach distributed;data noise network;applied distributed;approach distributed;protect data noise;coding nonmulticast;interference alignment proposed;concept distributed;network proposed;coding applied;noise network proposed;using concept distributed;coding;alignment proposed code", "pdf_keywords": ""}, "c096ec97ecc4f8325f6db7f32398445d6a39f959": {"ta_keywords": "multiple fairness metrics;fairness metrics;fairness metrics demonstrate;aware multiple fairness;mechanism choosing fairness;choosing fairness concerns;multiple fairness;choosing fairness;performance evaluation recommender;fairness concerns;performance recommendation domains;recommender systems aware;demonstrating performance recommendation;evaluation recommender systems;fairness;evaluation recommender;recommender systems;recommendation domains;fairness concerns demonstrating;recommender;performance recommendation;metrics demonstrate utility;performance evaluation;recommendation;design performance evaluation;utility framework;aware multiple;metrics;providing mechanism choosing;mechanism choosing", "pdf_keywords": "recommendation fairness arbitrate;fairness recommendation algorithms;fairness recommendation ranking;optimal fairness recommendation;assigning fairness recommendation;recommendation fairness;fairness concerns recommender;fairness measure recommendation;measure fairness recommendation;fairness recommendation subgroup;fairness recommendation result;fairness recommendation;adaptation recommendation fairness;choice ranking fairness;ranking fairness scruf;dynamically rebalancing fairness;users fairness concerns;ranking fairness;optimal fairness measures;mechanisms choosing fairness;consider users fairness;fairness measures proposed;choice fairness criteria;fairness multiple representations;multiple fairness metrics;representing multiple fairness;multiple fairness criteria;users fairness;rebalancing fairness concerns;fairness criteria decisions"}, "27636090a87fab750fccff4c6ede161ab62bcab4": {"ta_keywords": "visibility traffic traffic;visibility traffic;traffic traffic networks;traffic traffic;traffic networks based;increasing visibility traffic;traffic networks;traffic;periodic safety messages;messages bes mechanism;safety messages bes;safety messages;reflected periodic safety;mechanism increasing visibility;increasing visibility;periodic safety;data reflected periodic;visibility;networks based data;based data reflected;www ppil;safety;http www ppil;ppil org ppil;ppil org;messages bes;data reflected;networks based;ppil;www ppil org", "pdf_keywords": "congestion 802 11;channel congestion 802;802 11;congestion 802;802 11 802;802 11 network;802;11 802;11 802 11;vehicle belonging network;visibility vehicular;increasing visibility vehicular;beacon transmit information;hoc networks using;ad hoc networks;vehicle protocol;communicating mobile vehicles;vehicle senses channel;vehicle broadcast;beacon transmit;vehicular ad hoc;provided beacon transmit;driver vehicle protocol;mobile vehicle broadcast;beacon inform driver;network improves collision;visibility network;receiving beacon based;beacons inserting neighbor;vehicle broadcast information"}, "c3fc0b1041dcdd5b47ffaa0d584e40aa841628bf": {"ta_keywords": "expands set entities;binary relational;learn binary relational;binary relational concepts;expands set;relational concepts seeds;set expansion;entities means wrappers;set expansion context;suzaku expands set;set entities;set entities means;level set expansion;entities;expansion context extended;relational;character level set;extended seal suzaku;relational concepts;expansion context;seal suzaku expands;binary;entities means;concepts seeds;able learn binary;suzaku expands;learn binary;context extended seal;expands;expansion", "pdf_keywords": ""}, "7f79ac114d30c2c7dae91075210fbfda90c9d76f": {"ta_keywords": "regret minimization agent;diplomacy combines supervised;anonymous games;agent press variant;press variant diplomacy;agent press;human players;variant diplomacy;new agent press;playing anonymous games;search external regret;anonymous games popular;learning human;lookahead search;external regret minimization;minimization agent achieves;128 human players;variant diplomacy combines;regret minimization;players playing anonymous;diplomacy;agent;learning human data;minimization agent;agent achieves;lookahead search external;step lookahead search;present new agent;new agent;supervised learning human", "pdf_keywords": "imitation learning games;agent imitation learning;strategy agent learn;imitation learning large;regret minimization agent;learning method game;imitation learning;supervised agent imitation;optimal strategy agent;players trained imitation;reward game agent;trained imitation learning;agent imitation;agent learn;optimal strategy human;learning optimal strategy;learning games;large corpus games;diplomacy game;search regret minimization;game use reinforcement;learning games accurate;strategy agent;arbitrary game results;agent searches strategy;bot arbitrary game;arbitrary game;strategy human player;press diplomacy game;games popular diplomacy"}, "70dc18bb6607e408ec1cd3f71c0fdac3534c288d": {"ta_keywords": "accurate speech enhancement;speech enhancement;speech enhancement using;robust accurate speech;enhancement using s__terms;term memory robust;short term memory;accurate speech;term memory;enhancement using;enhanced short term;memory robust;memory robust accurate;enhanced short;enhancement;speech;enhanced;s__terms s__terms s__terms;s__terms s__terms;s__terms s__terms proposed;s__terms proposed;using s__terms s__terms;s__terms proposed framework;s__terms;framework based s__terms;based s__terms s__terms;using s__terms;based s__terms;framework enhanced short;memory", "pdf_keywords": ""}, "6e07fb796c75cac6432cdf0c314b933d0f9f45e5": {"ta_keywords": "tagger perform parsing;gene recognition;data generated tagger;recognition use tagger;tagger training data;gene recognition use;generated tagger training;accuracy gene names;generate tagger;generated tagger;task gene recognition;use tagger;gene names;generate tagger performs;used generate tagger;tagger performs task;use tagger perform;tagger training;tagger;perform parsing task;perform parsing;parsing;parsing task train;parsing task;tagger perform;tagger performs;gene;high accuracy gene;training data generated;accuracy gene", "pdf_keywords": ""}, "24fcdaf969089e6a411f7cebc9274bbc53c25e42": {"ta_keywords": "counterfactually augmented data;counterfactually augmented;augmented data unsupervised;efficacy counterfactually augmented;data unsupervised learning;counterfactually;unsupervised learning;unsupervised learning noise;augmented data;efficacy counterfactually;analysis efficacy counterfactually;noncausal features;underlying causal;learning noise added;underlying causal model;added underlying causal;data unsupervised;causal;learning noise;added noncausal features;models robust domain;augmented;causal model degrades;causal model;empirical evidence;unsupervised;empirical evidence supports;noncausal features make;robust domain;learning", "pdf_keywords": "counterfactually augmented data;counterfactually augmented;biases natural language;models natural language;domain performance supervised;causal model predicting;natural language inference;predict semantics;efficacy counterfactually augmented;accurately predict linguistic;models causal inference;causal inference robust;accurate domain models;learning natural language;natural language sentiment;predict semantics text;predict linguistic structure;predict linguistic;language processing natural;learning dataset domain;causal features;optimally predict semantics;trained original sentiment;regression models causal;domain accuracy training;neural natural language;models causal;noise causal features;unsupervised learning;supervised learning natural"}, "1d5c07e7415a7e9be078717197ddf9f3c70a2875": {"ta_keywords": "training clinical models;clinical notes hics;models patients using;using clinical notes;clinical model using;clinical models patients;clinical models;trained model probe;clinical notes;clinical model;model probe data;models patients;training clinical;use trained model;deidentified clinical model;trained model;data associated patients;training deidentified clinical;approach training clinical;experimental approach training;notes hics project;hics project training;patients using clinical;model using hics;model probe;notes hics;using clinical;training process based;use trained;training process", "pdf_keywords": "privacy releasing models;leaking sensitive information;patient privacy releasing;releasing stealthiness documents;stealthiness documents containing;stealthiness documents;risk patient privacy;privacy releasing;patient privacy;data releasing models;privacy patient information;sensitive data releasing;pretrained data publicly;trained sensitive data;sensitive information;data discovery infectious;potential privacy;information potential risks;privacy;sensitive data;potential privacy implications;risks releasing stealthiness;predicting risk bert;text patient records;privacy implications;electronic health records;increasingly used predict;privacy implications sharing;privacy patient;patient records train"}, "ca9047c78d48b606c4e4f0c456b1dda550de28b2": {"ta_keywords": "recurrent convolutions;recurrent convolutions power;power recurrent convolutions;deep learning;novel deep learning;deep learning model;long range memory;memory structure layer;state space layers;memory initialized trainable;state space layer;approaches challenging speech;challenging speech;recurrent;power recurrent;layer preserved memory;combines power recurrent;memory structure;speech;novel deep;memory;introduce novel deep;layers structure state;state space;convolutions;layers;range memory structure;convolutions power;convolutions power linear;preserved memory initialized", "pdf_keywords": "advantages recurrent convolutional;recurrent model learning;recurrence rnn deep;recurrent neural networks;recurrent convolutional;recurrence based neural;generalizes recurrent neural;recurrent models;recurrent convolutional continuous;recurrent neural;model generalizes recurrent;networks rnns convolutions;neural networks rnns;rnns convolutions;networks rnns;rnns convolutions differential;long term memory;linear recurrence rnn;recurrent model;rnn deep nonlinear;long time convolutional;generalizations recurrence convolutions;recurrent models framework;present recurrent model;convolutional linear recurrences;theorems recurrent convolutional;modeling advantages recurrent;generalizes recurrent;sequence model representations;models flexibility recurrent"}, "b2a090506264bc9706dc9bcc5d61b4965ae919e7": {"ta_keywords": "entities knowledge graph;knowledge graph;identifying entities knowledge;logic identify entities;knowledge graph method;probabilistic soft logic;identify entities graph;soft logic identify;identifying entities;extractions identification graph;entities knowledge;identify entities;entities graph;set extracted facts;soft logic;uncertain extractions;uncertain extractions identification;logic identify;entities graph demonstrate;probabilistic soft;identification graph;uses probabilistic soft;extracted facts;approach synthetic data;task identifying entities;identification graph uses;entities;synthetic data set;transforms uncertain extractions;graph uses probabilistic", "pdf_keywords": ""}, "8ca5a1e6cec68ef515ac1eb28d069a23dc9c14df": {"ta_keywords": "clustering data sets;clustering data set;clustering probability clustering;probability clustering;probability clustering achieved;clustering probability computed;clustering probability;probability clustering probability;compute clustering properties;subsets fixed clustering;clustering probability used;fixed clustering probability;clustering data;clustering exponents;clustering properties;sets fixed clustering;clustering properties large;compute clustering;clustering;compute clustering exponents;method compute clustering;achieved clustering data;clustering clustering;use clustering;collection clustering data;clustering achieved;clustering framework;large collection clustering;fixed clustering;terms clustering probability", "pdf_keywords": ""}, "a75c2d26ca6a06cbee62a8d1dad5993356d96793": {"ta_keywords": "ranking algorithms used;iterative set expansion;various ranking algorithms;ranking algorithms;seeds set expansion;iterative set;ranking method;present iterative set;user selfgenerated seeds;provide seeds set;set expansion iseal;algorithms used iseal;seeds set;ranking method small;various ranking;set expansion bootstrapping;ranking;seeds compare performance;set expansion;iterative;performance various ranking;algorithms;selfgenerated seeds compare;algorithms used;choice ranking method;iseal choice ranking;user provide seeds;expansion bootstrapping;selfgenerated seeds;seeds compare", "pdf_keywords": ""}, "a2ce385fc8d5068e8c87ebe4699c8f9b295cad5e": {"ta_keywords": "adapting linguistic representations;adapting linguistic;linguistic representations new;representations new languages;linguistic representations;linguisticly motivated subword;problem adapting linguistic;subword units phonemes;new languages;new languages methods;entity rn languages;motivated subword units;phonemes graphemes able;phonemes graphemes;units phonemes graphemes;languages xmath0;rn languages xmath0;linguistic;use linguisticly motivated;languages methods;languages;units phonemes;subword units;use linguisticly;linguisticly motivated;phonemes;approaches named entity;named entity rn;rn languages;graphemes able adapt", "pdf_keywords": "lingual word embeddings;training word embeddings;word embeddings;word embedding;word embeddings phonological;train multilingual embeddings;multilingual embeddings;embed words;phonemes morphemes graphemes;linguistic representations adapting;embeddings phonological representation;phoneme ngrams embeddings;linguistic embedding;embeddings phonological;embedding given word;word representations;word representations linguistically;word embeddings neural;word embedding model;linguistic adaptation resource;linguistic embedding given;word embeddings apply;morphemes graphemes approach;phonemes morphemes;dimensional word embeddings;ngrams embeddings phonological;word representation;forward linguistic embedding;pattern recognition embeddings;method embed words"}, "a0e92f6e9564b8c38b6649ae71b892ddfb988faa": {"ta_keywords": "parsing generative seq2seq;parsing semantic data;semantic parsing generative;database semantic parsing;parsing semantic;semantic parsing;generative seq2seq models;application generative seq2seq;generative seq2seq;parsing generative;seq2seq models unstructured;method parsing semantic;generative seq2seq method;semantic data;database semantic;query representation;seq2seq models;augmented query representation;parsing;available database semantic;query representation applying;novel method parsing;semantic data based;semantic;seq2seq;models unstructured data;unstructured data;seq2seq method implemented;seq2seq method;controlled augmented query", "pdf_keywords": "semantic parsers exemplar;parsers exemplar retrieval;generative semantic parser;predict semantic queries;disrupting semantic parsers;semantic parsers;semantic parser;related exemplars retrieval;semantic parser able;model semantic parsing;parser using retrieval;exemplars retrieval;semantic parser using;semantic parsing relies;semantic queries;semantic parsing;semantic queries high;query parser;trained recognize queries;query parser retrieves;parsers exemplar;queries domain parser;parser trained;generative semantic;predict query embedding;domain parser trained;exemplars retrieval index;parser learn;discovery generative seq2seq;generative seq2seq model"}, "a30d7a3aa5e50d0b7abb448b6692e419b84018b8": {"ta_keywords": "prefix boosting sequences;accurate prefix boosting;prefix boosting;boosting sequences training;prefix boosting papb;boosting sequences;sequences training procedure;sequences training;sequence improved better;present accurate prefix;scheme accurate prefix;boosting;sequence improved;accurate prefix;prefix;boosting papb;sequence obtained beam;sequences;boosting papb new;correct sequence improved;sequence;beam search;beam search compared;obtained beam search;partial correct sequence;minimization token character;compared hypotheses training;plausibly correct sequence;hypotheses training objective;correct sequence", "pdf_keywords": "decoder automatic speech;speech recognition;automatic speech;risk mbr softmax;sequence based decoder;mbr softmax;language processing natural;softmax;speech recognition problem;models natural language;end speech recognition;automatic speech recognition;search decoding;speech recognition method;training seq2seq models;speech recognition ewrs;natural language processing;processing natural language;word level recognition;search decoding seq2seq;exploiting capabilities softmax;capabilities softmax;mbr softmax margin;training seq2seq;problem training seq2seq;decoding seq2seq model;metric choice training;language processing;word level rnnlm;decoder"}, "a41c81e5c3f86e18217069b94b44ceaf281e451c": {"ta_keywords": "assigning location sensor;assigning location;policies placement sensor;heuristic placement policies;placement policies;placement sensor;placement sensor given;simple heuristic placement;location sensor wireless;heuristic placement;set policies placement;location sensor;sensor wireless network;sensor given network;problem assigning location;policies placement;optimal policies;obtain optimal policies;sensor wireless;optimal policies various;obtain optimal;process obtain optimal;optimal values policies;wireless network;optimal;placement;formulate problem decision;simple heuristic;wireless network provide;optimal values", "pdf_keywords": "relay optimal decision;deployed relay optimal;relay optimal;networks deployment relays;deployment wireless relay;relay placement;placement relay network;cost relay deployment;decision place relay;path fading channel;cost placement relay;cost relay position;wireless networks deployment;relay deployment method;relay deployment process;outage probability path;deployment relays method;deployment relay given;relay deployment;stochastic process relay;placement relay;relay deployment depends;relay deployment proposed;deployment relays;relay placement consists;estimating cost relay;deployment relay;relay located distance;relay cost placing;greater distance relay"}, "f20654f481843ec9eb11bcd00e418aec2470dfa5": {"ta_keywords": "distributed storage codes;storage codes construct;storage codes efficient;distributed storage;storage codes;downloading distributed storage;framework distributed storage;storage;reading downloading distributed;data centers design;efficient data downloaded;codes efficient data;downloading distributed;data downloaded rebuilding;piggybacking design framework;efficient data read;considerations data centers;data centers;design framework distributed;codes construct codes;codes efficient;framework distributed;codes construct;data read downloaded;piggybacking design;construct codes efficient;data read;data downloaded;distributed;known piggybacking design", "pdf_keywords": "distributed storage codes;designing storage codes;storage codes efficient;hyper mesh codes;storage codes effectively;distributed code storagewe;grid storage codes;storage codes;mesh codes codes;code storagewe;storage codes valid;storage codes require;mesh codes;distributed storage;storage codes maximum;code suitable distributed;designing distributed storage;distributed storage systems;storage distributed storage;construction storage codes;nodes code piggybacking;distributed code;storage data distributed;constructing piggybacked codes;arbitrary storage configurations;storage distributed;codes efficient data;design efficient codes;code storagewe present;code able store"}, "6e05d35d072cd73fa039fd60696a8fe110f1d6cd": {"ta_keywords": "recommendation publication databases;publication databases;publication databases based;publication database reference;recommendation publication;publication database proposed;publication database;representation publication database;connections publication database;approach recommendation publication;databases based discovery;database reference;database reference list;discovery connections publication;data representation publication;based discovery connections;databases based;reference list approach;databases;connections publication;based discovery;rich meta data;discovery connections;reference list;meta data representation;novel approach recommendation;database;random walk model;meta data;database proposed", "pdf_keywords": ""}, "3c4dfc252c214d559fadb5e3159bcc9c7db08fbc": {"ta_keywords": "dehydration dynamics fluorosis;fluorosis extracted teeth;dynamics fluorosis significantly;dynamics fluorosis extracted;dynamics fluorosis;fluorosis inferred surface;extracted teeth study;influence fluorosis inferred;fluorosis extracted;tooth results study;fluorosis significantly;fluorosis inferred;fluorosis significantly influenced;teeth study performed;inelastic ray imaging;influence fluorosis;extracted teeth;teeth study;investigate influence fluorosis;layers tooth results;fluorosis;tooth results;study dehydration dynamics;dehydration dynamics;ray imaging;surface layers tooth;presence fluorosis;imaging dsdis;teeth;present study dehydration", "pdf_keywords": ""}, "294f8307f26eb3ec7bbf19f15092f3c473ece821": {"ta_keywords": "entities using imitation;named entity classification;imitation learning proposed;entity classification;named entity classifier;entity classification points;using imitation learning;entity classifier;imitation learning;quality named entity;predicting relation non;using imitation;named entity;entity classifier points;method predicting relation;non standard entities;imitation;improve quality named;predicting relation;standard entities using;improves quality named;entities using;entities;classifier;entities points proposed;entities points;problem predicting relation;standard entities;standard entities points;entity", "pdf_keywords": ""}, "6a2c4a0f04c6ba2f6fbc171dcea8730423a298e5": {"ta_keywords": "learning metric non;learning metric;approach learning metric;learned pruner efficient;learned pruner compare;approaches learned pruner;metric based;spaces based learned;spaces based metric;non metric;learned pruner;based learned pruner;metric non metric;metric;metric spaces based;metric based parameter;metric non;non metric spaces;based metric;based metric based;metric spaces;pruner efficient;pruner compare;pruner compare performance;locality;probe locality;pruner;learning;parameter probe locality;effective approach learning", "pdf_keywords": ""}, "20d4105b276da6d6d38ed3c1bfc436f76198c240": {"ta_keywords": "pairwise associations events;association pairs events;large event datasets;event datasets introduce;event datasets;effect association pairs;associations events evaluate;pairs events;associations events;pairs events large;events;cause effect association;estimating cause effect;algorithms estimating cause;effect association;events evaluate proposed;events large event;human raters;conditional intensity rates;pairwise associations;large event;events large;estimating cause;associations events use;characterize pairwise associations;events evaluate;assessments human raters;event;human raters ground;intensity rates characterize", "pdf_keywords": ""}, "695d4c04f6e4f7ba5f771ac7853fdbaa81713ae8": {"ta_keywords": "social knowledge graphs;knowledge graphs learn;learning social knowledge;knowledge graphs;knowledge graphs method;search knowledge base;word network embeddings;structure social knowledge;learn latent topics;graphs learn latent;academic search knowledge;social knowledge;data embeddings;latent topics generate;network embeddings;search knowledge;knowledge base;embeddings represent data;data embeddings represent;latent topics;learning social;unlabeled data embeddings;embeddings;method learning social;knowledge base 35;graphs learn;topics generate word;topics generate;online academic search;word network", "pdf_keywords": "social knowledge graphs;social knowledge graph;word embeddings network;knowledge graphs datasets;modal probabilistic embedding;probabilistic embedding model;embeddings topic models;topic models embedding;knowledge graph;knowledge graphs generate;knowledge graphs;embedding model learn;topic sampling embedding;embedding model social;probabilistic embedding;knowledge graphs knowledge;word embeddings;embedding based models;embeddings network embeddings;knowledge graph use;embedding model learning;knowledge graph researchers;models embedding based;modal embedding model;models embedding;network embeddings;embeddings network;learning social knowledge;knowledge graph proposed;model social knowledge"}, "3b8494614903dc47579da30477b21b109b29f8cd": {"ta_keywords": "zero xmath0 xmath1;xmath1 xmath2 xmath3;xmath2 xmath3 xmath4;xmath0 xmath1 xmath2;xmath1 xmath2;xmath3 xmath4;xmath0 xmath1;xmath2 xmath3;zero xmath0;xmath8 xmath9 xmath10;xmath7 xmath8 xmath9;xmath1;xmath3 xmath4 xmath5;xmath9 xmath10 xmath11;xmath0;xmath9 xmath10;xmath10 xmath11 xmath12;xmath4;xmath6 xmath7 xmath8;xmath4 xmath5 xmath6;xmath2;xmath10 xmath11;xmath4 xmath5;xmath8 xmath9;xmath5 xmath6 xmath7;non zero xmath0;xmath10;xmath12 xmath13 xmath14;xmath6 xmath7;xmath3", "pdf_keywords": ""}, "74c881830a9cd7ea49795faa5c582b7ec56bd0bf": {"ta_keywords": "noisy speech recognition;neural beamforming module;speech recognition asr;neural beamforming;denoising noisy speech;resulting neural beamforming;neural beamforming jointly;speech recognition;automatic speech recognition;ability neural beamforming;speech recognition task;noisy speech;recognition asr systems;end automatic speech;beamforming jointly optimize;automatic speech;beamforming module;recognition asr;far field denoising;beamforming;field denoising noisy;nodes denoising module;beamforming jointly;denoising noisy;asr systems;sequence nodes denoising;denoising module;beamforming module extends;nodes denoising;field denoising", "pdf_keywords": "training dereverberation beamforming;speech enhancement advanced;multichannel speech enhancement;dereverberation beamforming based;e2e automatic speech;speech recognition asr;dereverberation beamforming modules;speech enhancement;extension dereverberation beamforming;dereverberation beamforming;multichannel speech;speech recognition;optimizes beamforming dereverberation;scattering dss beamforming;beamforming dereverberation;entire multichannel speech;automatic speech recognition;automatic speech;beamforming modules s2sin;beamforming dereverberation components;dss beamforming;beamforming based;sound end architecture;deep inelastic scattering;beamforming modules;reverberant data;microphone results proposed;noisy reverberant data;reference microphone results;noise ratio dereverberation"}, "3d3b1300c7cd6a820a6d08605248f875a3ad20b9": {"ta_keywords": "interpretability multi hop;hop reasoning models;multi hop reasoning;metrics using interpretability;quantitatively evaluate interpretability;interpretability multi;interpretability global interpretability;evaluate interpretability multi;global interpretability evaluation;interpretability scores rules;interpretability evaluation;interpretability evaluation design;hop reasoning;global interpretability;evaluate interpretability;interpretability global;recall local interpretability;using interpretability;reasoning models;using interpretability scores;interpretability;rule based models;reasoning models particular;interpretability scores;local interpretability global;local interpretability;annotate possible rules;path recall;rules manually annotate;multi hop", "pdf_keywords": "hop reasoning models;interpretability multi hop;rules interpretability evaluation;rules based interpretability;extract rules interpretability;rule interpretability score;benchmark detect interpretability;prediction interpretability evaluation;reasoning annotated knowledge;rules interpretability;obtain rule interpretability;hop reasoning model;quantitatively evaluating interpretability;reasoning knowledge graphs;built annotated rules;evaluating interpretability multi;detect interpretability multi;rule based reasoning;model interpretabilitywe;interpretability rule based;rule interpretability;measuring interpretability;quantitatively evaluate interpretability;evaluate model interpretabilitywe;detect interpretability;method measuring interpretability;prediction interpretability;global interpretability evaluation;interpretability evaluation explore;evaluate interpretability rule"}, "e0c54e18cf2372414042bf67eed0272b0a432190": {"ta_keywords": "propagation charged particle;zeeman field propagation;particle medium zeeman;medium propagation charged;propagation charged;particle medium propagation;charged particle medium;control propagation charged;field propagation charged;effect zeeman field;medium zeeman field;particle medium;charged particle;medium propagation;charged particle background;field propagation;particle medium topic;particle background medium;propagation;zeeman field used;effect zeeman;zeeman field;control propagation;medium zeeman;zeeman;main study propagation;medium field;used control propagation;particle;study effect zeeman", "pdf_keywords": ""}, "7da967be8f6367f6174bf99d0d019ff545ac5966": {"ta_keywords": "semantic annotation lingvosemantic;annotation lingvosemantic project;semantic annotation;annotation lingvosemantic;method semantic annotation;annotation;annotated data;create annotated;annotated data presented;create annotated data;creation annotated data;annotated data purpose;annotated;used create annotated;creation annotated;recommendations creation annotated;semantic;method semantic;lingvosemantic project;novel method semantic;sentences recommendations creation;lingvosemantic project method;user questions methodology;lingvosemantic;questions methodology;method based bootstrapping;sentences;based bootstrapping;based bootstrapping method;sentences recommendations", "pdf_keywords": ""}, "49db57f300b270f16cbcb1891ca39e16981d42b5": {"ta_keywords": "indicators pandemic activity;time indicators pandemic;pandemic activity;indicators pandemic;pandemic activity united;real time indicators;repository available covidcast;health data central;public health data;available covidcast;pandemic;health data;states available covidcast;covidcast;available covidcast open;called covidcast;covidcast open access;states called covidcast;time indicators;repository public health;covidcast open;called covidcast open;repository real time;public health;data central;data central repository;real time;indicators;central repository available;open access repository", "pdf_keywords": ""}, "bad416f073a08086ee428e5a264eac3a7d3251e5": {"ta_keywords": "point source image;finding position point;method finding position;image depth method;image shape obtained;finding position;analysis image shape;position point source;image method;image method based;image depth obtained;source image method;image shape;shape image depth;image image depth;image shape image;position point;image depth;shape image;use image depth;obtained using image;image image;depth method;analysis image;depth method applied;depth method demonstrated;combination image shape;using image image;source image;shape obtained using", "pdf_keywords": ""}, "d9dbdd254b02ef1af2769af403cba373c1b1bcb1": {"ta_keywords": "speaker diarization method;speaker diarization problem;speaker diarization;novel speaker diarization;end speaker diarization;extraction clustering speaker;clustering speaker representations;clustering speaker;formulates speaker diarization;speaker representations proposed;speaker representations;method formulates speaker;formulates speaker;diarization method based;speaker label;diarization problem permutation;end speaker;speaker;diarization method;directly minimize diarization;end end speaker;minimize diarization errors;novel speaker;minimize diarization;diarization problem;errors suffered speaker;diarization;diarization method does;representations proposed method;suffered speaker label", "pdf_keywords": "neural speaker diarization;efficient speaker diarization;based speaker diarization;segmentation clustering speaker;speaker diarization proposed;separation neural speaker;speaker diarization method;speaker diarization;speaker diarization problem;clustering speaker;novel speaker diarization;problem speaker diarization;speaker label permutation;clustering speaker representation;end neural speaker;overlap speaker labels;speech neural network;label separation neural;overlapping speech;considers overlap speaker;formulates speaker diarization;overlapping speech simply;overlap speaker;overlapping speech input;speaker diarization time;architecture overlapping speech;trained speech mixtures;handle overlapping speech;speaker representation proposed;neural speaker"}, "5931c8ac145baf17cec9effc25c051049b7dfd4c": {"ta_keywords": "spatial grounding dialogue;grounded neural dialogue;grounding dialogue task;grounding dialogue;neural dialogue;dialogue task;neural dialogue model;grounded neural;dialogue agent;dialogue model;dialogue agent outperforms;spatial grounding;dialogue;dialogue model successfully;common spatial grounding;present grounded neural;dialogue task udagawa;observable reference game;shades dialogue agent;reference game;grounding;shades dialogue;spatial;grounded;common spatial;present grounded;reference game evaluate;agent;board;collaborates people", "pdf_keywords": "reference dialogue games;grounding dialogue neural;grounding dialogue task;dialogue task generation;dialogue model reference;collaborative reference dialogue;combines neural dialogue;neural dialogue model;grounded neural dialogue;models predict dialogue;dialogue games combines;dialogue neural;dialogue tasks;dialogue tasks implementations;based neural dialogue;referents human dialogue;spatial grounding dialogue;dialogue game;dialogue task;method grounding dialogue;dialogue games;grounding dialogue;predict dialogue;dialogue model;neural dialogue;dialogue game using;reference dialogue;grounding language referents;dialogue systems;structured referent grounding"}, "6c4258f6a6a4bee7b9d914379c44aea6073cdc37": {"ta_keywords": "energy disaggregation adaptive;energy level disaggregation;energy disaggregation problem;energy disaggregation;disaggregation problem energy;problem energy disaggregation;disaggregation adaptive filtering;disaggregation adaptive;filtering new energy;disaggregation problem reformulated;adaptive filtering;disaggregation;level disaggregation;disaggregation problem;adaptive filtering problem;level disaggregation problem;adaptive filtering new;reformulated adaptive filtering;energy level;new energy level;problem energy;problem reformulated adaptive;filtering;reformulated adaptive;filtering problem discuss;energy;new energy;adaptive;filtering problem;optimality algorithm", "pdf_keywords": "disaggregation energy data;energy disaggregation;energy disaggregation determination;energy disaggregation problem;harvesting energy disaggregation;algorithm disaggregation energy;framework energy disaggregation;energy disaggregation requires;considers energy disaggregation;problem energy disaggregation;method energy disaggregation;disaggregation energy;energy disaggregation presence;finally energy disaggregation;disaggregation algorithm;novel algorithm disaggregation;disaggregation consumer data;power consumption data;disaggregation based;disaggregation algorithm used;power consumption patterns;algorithm disaggregation consumer;disaggregation based use;recover power consumption;data set disaggregation;disaggregation consumer;disaggregation determination;disaggregation paper propose;filter bank disaggregation;algorithm disaggregation"}, "3dcf9c900f5f28e082a2fcdea4763b6063a76f09": {"ta_keywords": "defeasible reasoning leverages;defeasible reasoning;model scenario answering;scenario answering;approach defeasible reasoning;reasoning leverages;reasoning leverages fact;scenario answering question;problem scenario answering;answering question leverage;answering question;reasoning;person answer question;answer question defeasible;answering;mental model;mental model problem;answer question;answering question curious;question defeasible way;leverages fact person;leverage fact person;forms mental model;approach defeasible;model scenario;influences model scenario;person answer;defeasible way demonstrate;person forms mental;fact person forms", "pdf_keywords": "inference natural language;defeasible reasoning datasets;inference query graph;inference graph;learning defeasible inference;defeasible inference based;graph based inference;reasoning datasets;defeasible inference query;defeasible inference modeling;answer defeasible inference;improves defeasible inference;defeasible inference;scenario inference graph;reasoning mechanism model;inference graph adapted;inferencewe introduce;reasoning mechanism;reasoning datasets work;defeasible inference natural;defeasible reasoning using;defeasible inference model;reasoning using hierarchical;inferencewe;optimal inference graph;model answering;inferencewe introduce novel;reasoning datasets method;defeasible reasoning;reasoning mixture experts"}, "dbdb7f25f1538c2a2885d3992e5320e2ee5c23a1": {"ta_keywords": "environments use classroom;class learning environments;learning environments;use classroom class;learning environments use;models defined student;use classroom;classroom class learning;student perspectives class;classroom class;classroom;learning models defined;perspectives class learning;classes defined student;student perspectives;class learning models;student perspectives respectively;defined student time;classes;time student perspectives;student time;student time student;perspectives respectively classes;time student;perspectives class;class learning;defined student;learning models;class;student", "pdf_keywords": ""}, "660119405bb48777cd71d85caa5ec2e90a336caf": {"ta_keywords": "historical text normalization;text normalization;text normalization using;normalization using;normalization;normalization using different;neural encoder;historical text;best method neural;method neural encoder;encoder;languages dataset literature;text;languages dataset;neural encoder advantage;context training;art historical text;different languages dataset;neural;methods context training;encoder advantage;method neural;encoder advantage able;dataset literature compare;different languages;dataset literature;dataset;languages;using different languages;context training process", "pdf_keywords": "historical text normalization;text normalization systems;text normalization task;text normalization assess;normalizing historical text;text normalization;normalization historical text;normalize historical text;texts normalized;normalisation historical text;normalization historical texts;text normalization starting;learning normalization historical;historical texts normalized;statistical machine translation;historical spelling normalization;machine translation models;spelling normalization;spelling normalization use;texts normalized high;neural machine translation;learning approach normalize;method normalizing historical;normalizing historical;natural language processing;learning normalization;processing nlp;accuracy natural language;machine translation;language processing nlp"}, "98554bd8a15172e9a6ef3cc3db3bc52504110fc9": {"ta_keywords": "stochastic bandits optimal;strategy stochastic bandits;bandits optimal strategy;stochastic bandits;bandits optimal;optimal strategy stochastic;strategy stochastic;strategy optimal variant;optimal strategy variant;identifying optimal strategy;original strategy optimal;strategy robust distributed;optimal strategy;optimal strategy optimal;strategy optimal;demonstrate optimal strategy;bandits;optimality optimal strategy;optimal strategy task;original strategy robust;identifying optimal;strategy robust;strategy variant;variant demonstrate optimal;variant original strategy;robust distributed variant;optimal variant;strategy variant original;task identifying optimal;optimality optimal", "pdf_keywords": "optimality stochastic bandits;optimal stochastic bandits;stochastic bandits optimal;stochastic bandits;stochastic bandits possible;bandits optimal optimal;bandits optimal;bandits possible optimal;stochastic bandits single;stochastic bandits additionally;performance stochastic bandits;bandits prove optimality;stochastic bandit statistically;stochastic bandit;bandits optimal version;distributed stochastic bandits;bandits optimal stopping;stochastic bandit models;study stochastic bandits;stochastic bandits following;stochastic adversarial bandits;algorithms optimal regret;stochastic bandits fixed;optimal stochastic reward;optimal regret minimization;distributed stochastic bandit;optimal item regret;item regret optimality;regret optimal item;regret optimal optimal"}, "6b3fa9157a8120a6eb86ae06a93611a1fcd9e219": {"ta_keywords": "underlying hard database;particular soft database;soft database;soft database approach;hard database;database formulate hardening;soft database formulate;hard database given;given particular soft;database given particular;hardening optimization;particular soft;database;hardening optimization problem;approach based hardening;formulate hardening optimization;database approach;based hardening problem;database formulate;database approach based;underlying hard;database given;soft;likely underlying hard;hardening problem;formulate hardening;hardening problem problem;inferring likely underlying;nding local optimum;algorithm nding local", "pdf_keywords": ""}, "ef9ddbc35676ce8ffc2a8067044473727839dbac": {"ta_keywords": "language based softmax;model natural language;softmax;limited softmax;based softmax;softmax model;based softmax model;softmax model matrix;softmax bottleneck propose;method limited softmax;factorization problem expressiveness;limited softmax bottleneck;softmax bottleneck;natural language;model matrix factorization;natural language proposed;natural language based;matrix factorization;matrix factorization problem;factorization;factorization problem;language proposed method;problem expressiveness proposed;problem expressiveness;language;language proposed;model natural;language based;expressiveness proposed method;model matrix", "pdf_keywords": "softmax propose;softmax bottleneck propose;neural language models;models limited softmax;limited softmax;softmax propose new;softmaxes;problem expressiveness softmax;softmax bottleneck proposed;models based softmax;softmax;important softmax;important softmax bottleneck;softmaxes furthermore expressiveness;named softmax bottleneck;softmax based models;high dimensional softmax;softmaxes furthermore;model important softmax;limited softmax bottleneck;expressiveness softmax;named softmax;low dimensional softmax;modeling natural language;models named softmax;softmax bottleneck;dimensional softmax propose;dimensional softmax bottleneck;softmax based;language models"}, "d29f155060f96becef0247ee77dc038f96b2d983": {"ta_keywords": "shortening translation unit;translation translation processing;shortening translation;translation translation speed;method shortening translation;unit machine translation;machine translation translation;translation processing;translation unit machine;translation processing time;machine translation module;machine translation;translation speed;translation speed accuracy;base translation translation;phrase base translation;translation module reduced;translation module;translation unit;base translation;time machine translation;translation translation;translation;phrase base;phrase table;reduced using phrase;propose method shortening;table phrase base;shortening;method shortening", "pdf_keywords": ""}, "d415b724fbc35afcc8dd91738123edfa6a5db634": {"ta_keywords": "deep policy gradient;policy gradient algorithms;progress deep policy;policy gradient;algorithms proximal policy;proximal policy optimization;region policy optimization;policy optimization;deep policy;algorithmic progress deep;policy optimization pp;optimization pp trust;policy optimization tt;trust region policy;progress deep;gradient algorithms case;gradient algorithms;optimizations responsible gain;proximal policy;code level optimizations;region policy;popular algorithms proximal;algorithms proximal;algorithm optimizations responsible;optimizations algorithm augmentations;level optimizations;trust region;algorithmic progress;core algorithm optimizations;cumulative reward", "pdf_keywords": "deep policy gradient;gradient algorithms policy;powering deep policy;policy gradient algorithms;policy gradient applications;progress deep policy;policy gradient methods;experimental policy gradient;proposed policy gradient;policy gradient;implementation policy gradient;enforced policy gradient;prominent policy gradient;gradient methods deep;methods policy gradient;algorithmic progress deep;standard policy gradient;deep policy;underlying policy optimizations;policy optimization layer;regions policy gradient;policy optimizations provide;algorithms policy optimization;disjoint policy gradient;policy optimizations;policy optimization algorithms;algorithms proximal policy;proximal policy optimization;algorithms used deep;choices policy gradient"}, "3315dee45b1edb8f8286816629de7b8c31d270d6": {"ta_keywords": "information social cues;search behavior participants;affect search behavior;social signals information;cues affect search;social signals;search behavior;participants neutral information;disliked information social;strategies subjects voting;social cues;social cues affect;participants subjects social;simple social signals;search strategies subjects;subjects voting;affect search;information social;information search strategies;subjects voting task;voting task randomly;information search;liked disliked information;search did subjects;influence simple social;voting task;behavior participants subjects;search strategies;subjects social environments;disliked information", "pdf_keywords": ""}, "387754dc8d4185fadd7c3c15e43956a4d085e8fe": {"ta_keywords": "nearest neighbor search;approximate nearest neighbor;nearest neighbor;neighbor search methods;methods approximate nearest;benchmark search;benchmark search methods;distance permutations good;neighbor search;based benchmark search;approximate nearest;distance permutations;assumption distance permutations;nearest;benchmark datasets;methods benchmark datasets;permutation based methods;machine based benchmark;permutation methods;benchmark datasets literature;support permutation methods;distance original points;methods benchmark;search methods;permutation based;benchmark;search methods based;based benchmark;survey permutation based;test methods benchmark", "pdf_keywords": "ranking data points;permutation nearest neighbor;neighbor search based;search closest neighbor;distance based search;nearest neighbor;based pivots nearest;pivots nearest neighbors;approximate knearest neighbor;nearest neighbors;closest neighbor query;function nearest neighbor;ranking data;nearest neighbors method;approximate similarity search;search closest;nearest neighbor defined;efficiently search;search algorithm distance;methods approximate knearest;permutations search closest;knearest neighbor search;neighbor search;filtering highly ranked;similarity search nonmetric;based permutation search;nearest neighbor graph;efficiently search entries;method ranking data;search entries metric"}, "60a121c55b5144bfe3aef5b6ea8959a9f6dd12ae": {"ta_keywords": "speech enhancement ensemble;enhancement ensemble learning;enhancement ensemble;ensemble learning;ensemble learning framework;speech enhancement;problem speech enhancement;ensemble;averaging voting techniques;averaging voting;simple averaging voting;performance simple averaging;improvement enhancement;simple averaging;voting techniques;enhancement performance;improvement enhancement performance;enhancement;enhancement performance simple;averaging;combination algorithms;speech;algorithms lead improvement;combination algorithms lead;algorithms;problem speech;learning framework combination;formulating problem speech;framework combination algorithms;improvement", "pdf_keywords": ""}, "ba56bb1eb67b188a89060058ef8ad02ce3c660ac": {"ta_keywords": "polarized xmath3he interaction;spin polarized xmath3he;interaction spin polarized;spin polarized xmath2he;spin polarized xmath0he;interaction spin;xmath3he interaction interaction;interaction dominated spin;dynamics spin polarized;xmath2he xmath1he interaction;xmath1he interaction dominant;xmath0he xmath1he interaction;xmath3he interaction;xmath1he interaction;component interaction spin;relativistic quantum mechanics;polarized xmath2he xmath1he;polarized xmath3he;non relativistic quantum;polarized xmath0he xmath1he;relativistic quantum;polarized xmath2he;dynamics spin;polarized xmath0he;xmath1he interaction framework;spin polarized;quantum mechanics nrqm;dominated spin polarized;study dynamics spin;quantum", "pdf_keywords": ""}, "9fc33c53d1f59aa9fd7f1b642c3859900865b0e3": {"ta_keywords": "sets closed systems;systems closed set;closed systems;closed systems closed;closed systems based;closed sets;closed systems estimate;systems closed;closed set;set closed sets;closed set set;closed sets set;closed sets closed;compute set closed;sets closed;closed systems use;set closed;set sets closed;sets set closed;set set closed;representation set closed;systems estimate set;estimate set sets;representation compute set;sets;set sets;closed;compute set;sets set;representation set", "pdf_keywords": ""}, "d7729f2ff21f97d56d10c54adc1f1f5ffbec9e5c": {"ta_keywords": "lesions using lasers;treat lesions oral;oral premalignant lesions;lesions oral premalignant;using scalpels lasers;lesions oral;treatment oral premalignant;using lasers use;scalpels lasers present;scalpels lasers;treat oral premalignant;using lasers;lasers present study;premalignant lesions using;lasers use tools;study treat lesions;lesions using;treat lesions;lesions using scalpels;oral premalignant;results treatment oral;lasers use;treatment oral;used treat oral;lasers present;treat oral;premalignant lesions;lasers;surgical techniques used;lesions", "pdf_keywords": ""}, "649eb9fd9a18f9601270b7fcde8d6548bfc6ec75": {"ta_keywords": "recognize speech mixture;dataset speech separation;speech mixture corresponding;speech separation benchmark;speech mixture;2mix dataset speech;monaural multi speaker;mixture speech corresponding;speech recognition;dataset speech;automatic speech;automatic speech recognition;speech recognition model;trained recognize speech;non mixture speech;speech separation;mixture speech;speech generated wsj0;recognize speech;end automatic speech;speaker mixed speech;multi speaker end;mixed speech generated;speech corresponding labels;speaker mixed;multi speaker;experiments speaker mixed;speech generated;speaker end;speech corresponding", "pdf_keywords": "speech recognition permutation;speech recognition end;end speech recognition;speaker speech recognition;speech recognition joint;speech recognition distributed;monaural multi speaker;talker speech recognition;automatic speech;speech decoder trained;speech recognition task;separated speech decoder;speech decoder;speech corresponding labels;multi speaker speech;speech recognition;decoder network speech;end automatic speech;speech recognition model;automatic speech recognition;speaker parallel attention;end multi speaker;speech recognition thewe;multi talker speech;monaural multi talker;speech corresponding;network speech recognition;multi speaker end;network speech;filter separated speech"}, "5884948777dfc003ba49e1513420830616281839": {"ta_keywords": "representations multilingual bert;learning multilingual representations;multilingual representations combines;multilingual representations;contextualized representations multilingual;multilingual bert;representations multilingual;multilingual bert produces;learning multilingual;lingual tasks alignment;cross lingual tasks;diverse cross lingual;generalize contextualized representations;contextualized representations;lingual tasks;multilingual;framework learning multilingual;cross lingual;lingual;representations combines mutually;representations combines;comparisons representations learned;representations learned using;representations learned;generalize contextualized;alignment joint training;direct comparisons representations;bert produces;comparisons representations;contextualized", "pdf_keywords": "learns unified multilingual;learning multilingual representations;unified multilingual representations;trained monolingual representations;accurate monolingual representations;multilingual representations;embeddings preserves monolingual;word representations multilingual;learning multilingual;multilingual representations text;monolingual representations shared;unsupervised cross lingual;bilingual word embeddings;multilingual representations using;representations multilingual corpora;multilingual word representation;methods crosslingual nernst;monolingual representations;representations using monolingual;embeddings shared vocabulary;representations multilingual;unified multilingual;embeddings alignment refinement;cross lingual nernst;diverse crosslingual tasks;independently trained monolingual;learning embeddings single;trained monolingual;methods diverse crosslingual;unsupervised supervised alignment"}, "56bc2a1eebedab3e452a7ca3969aa1e4dd5946c3": {"ta_keywords": "diversified influence maximization;influence maximization;influential nodes network;influential nodes;influence maximization im;identify influential nodes;diversified influence;node diversity;framework diversified influence;algorithms identify influential;diverse ranking;incorporates node diversity;approaches diverse ranking;submodular optimization;diversity original optimization;submodular optimization problem;diverse ranking experimental;node diversity original;maximization im incorporates;maximization;nonmonotonic submodular optimization;diversified;maximization im;identify influential;nodes;ranking;propose framework diversified;influential;framework diversified;influence", "pdf_keywords": ""}, "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30": {"ta_keywords": "shot language coordination;language coordination;language coordination explicitly;language coordination fundamental;task shot language;game language navigation;shot language;language navigation task;referential game language;game language;language navigation;coordination;coordination explicitly;language theory;coordination fundamental;language;referential game;performance referential game;problem language theory;language theory study;coordination explicitly modeling;communication;brain ability coordinate;communication performance referential;brain ability;better communication performance;better communication;coordination fundamental problem;navigation task;problem language", "pdf_keywords": "shot language coordination;language coordination game;language coordination agents;language coordination speaker;better language coordination;language coordination;coordination referential game;language coordination perform;language coordination context;language coordination task;language coordination modeling;listener language shot;task shot language;referential game language;coordination speaker predicts;quickly adapting conversational;coordination game ability;referential game speaker;adapting conversational;language coordination referential;listeners shot language;language games optimal;learning language games;maximised shot language;game language;coordination context shot;coordination agents quickly;coordination task reinforcement;perform shot language;simple shot language"}, "3c57a1aa483d8bffe1339914b80d2913f2dc8376": {"ta_keywords": "gans improves feature;adversarial networks gans;generative adversarial networks;networks gans improves;generative adversarial;gans improves;networks gans;gans;formulation generative adversarial;adversarial networks;adversarial;good semi supervised;improves feature matching;semi supervised;generative;semi supervised learning;supervised learning bad;learning bad generator;new formulation generative;benchmark datasets;feature matching;formulation generative;supervised;substantially improves feature;improves feature;feature matching obtain;benchmark datasets given;feature matching obtaining;supervised learning;multiple benchmark datasets", "pdf_keywords": "semi supervised;propose semi supervised;semi supervised classification;supervised semi;good semi supervised;semi supervised learning;supervised semi supervised;standard semi supervised;matching based generative;approach semi supervised;adversarial feature matching;supervised learning generative;method supervised semi;semi learning;semi learning model;generative models;learning generative;desired supervised;adversarial feature;supervised;learning based generative;discriminator objective good;achieve desired supervised;learning clear discriminator;generative model;perform supervised learning;generative learning;learning propose semi;generated generative adversarial;generative"}, "866f231970f93f4a201febc2fb46aff06f501e4b": {"ta_keywords": "normalization historical language;automatic normalization historical;automatically normalize historical;normalize historical forms;normalize historical;automatic normalization;normalization historical;tool automatic normalization;automatically normalize;historical language data;used automatically normalize;normalization;normalize;language data;language data tool;historical language;interprets historical forms;historical forms way;historical forms;historical forms close;modules interprets historical;rule entries;interprets historical;set rule entries;rule entries define;historical;norma new tool;present norma new;form present norma;new tool automatic", "pdf_keywords": ""}, "4d96ec46cda5d3b223fc7d33a920ab85864ea36d": {"ta_keywords": "functional motifs proteins;predict features proteins;learns underlying protein;motifs proteins amino;motifs proteins;protein sequence predict;identify functional motifs;features proteins amino;features proteins;reductase superfamily discover;novel functional motifs;underlying protein sequence;oxygen reductase superfamily;copper oxygen reductase;functional motifs;reductase superfamily;protein sequence;neural network architecture;amino acid sequences;acid sequences architecture;proteins amino acid;deep learning;sequence predict functional;proteins;superfamily discover;proteins amino;architecture based deep;introduce neural network;underlying protein;neural", "pdf_keywords": ""}, "6c170fe3fec5a477c938d07fa00935bb6f7b87cc": {"ta_keywords": "conversion speech parameters;speech vectors mixture;speech parameters proposed;speech parameters;quality converted speech;approach conversion speech;individual speech vectors;converted speech;speech vectors;conversion speech;features individual speech;vectors mixture converted;mixture converted;individual speech;converted novel multivariate;mixture converted novel;speech;vectors mixture;rich context model;quality converted;context model conventional;context model;conversion;converted;random matrix model;method based mixture;improvements quality converted;based mixture novel;approach conversion;based mixture", "pdf_keywords": ""}, "adac290d72c86c186837a884aae922bee4dee684": {"ta_keywords": "errors human readers;misspellings error rates;readers unimpaired comprehension;report eye tracking;eye tracking study;reading difficulty;words cause reading;human readers unimpaired;cause reading difficulty;eye tracking;occurring misspellings;words contain errors;reading difficulty correct;human readers;difficulty correct words;word based surprisal;contain errors human;error words;misspellings;errors human;misspellings error;naturally occurring misspellings;unimpaired comprehension;occurring misspellings error;error words cause;errors error words;words present computational;readers unimpaired;uses character based;word based", "pdf_keywords": "predict words misspellings;words misspellings affected;words cause reading;treats words errors;words errors vocabulary;words errors affect;misspellings human readers;fixated words mathematical;reading behavior;compared fixated words;behavior human reading;experiment reading texts;words misspellings;fixated words;readers unimpaired comprehension;errors affect reading;reading texts errors;essentially treats words;increases difficulty words;errors vocabulary;reading texts letters;words mathematical model;cause reading difficulty;misspellings affected;letters transpositions misspellings;reading difficulty;word length reading;predict words;reading behavior word;difficulty behavior words"}, "23918ed366c60ae0ef85b0c80def63127f035e02": {"ta_keywords": "shared data cloud;learn noise distributions;spread lossy information;noisy data;shared data;estimation noisy data;machine learning security;reliable estimation noisy;data cloud;performance shared data;shared data demonstrate;noise distributions superior;learn noise;noisy data key;information remote servers;estimation noisy;noise distributions;cloud;lossy information remote;cloud storage cloud;cloud storage;cloud access;cloud access mechanism;storage cloud;data cloud storage;storage cloud access;mechanism learn noise;lossy information;learning security;machine learning", "pdf_keywords": ""}, "e7e1f5a713d20cdf31e732022731fdf0d8fb4fc5": {"ta_keywords": "generate explanations sentences;generating explanations sentences;natural language inference;explanations sentences natural;generating explanations;explanations sentences;explanations sentences np;generate explanations;approach generate explanations;language inference;language inference np;method generating explanations;sentences natural language;sentences approach generate;description sentences approach;natural language;description sentences;sentences np;inference;sentences natural;sentences np accurate;inference np;sentences approach;explanations;sentences;inference np approach;respectively description sentences;low order;order respectively description;box low order", "pdf_keywords": "attention based explanations;generating explanations entailment;explanations entailment task;tokenlevel explanations entailment;generating explanations;generate accurate explanations;neural entailment model;generate tokenlevel explanations;based explanations entailment;task sentence classification;neural entailment;explanations recall method;explanations task explanations;task explanations generated;explanations entailment relation;level explanations entailment;level explanations recall;generating attention based;explanations entailment;supervised description model;sentences neural network;accurate explanations task;explanations recall;supervised description features;description features supervised;matrix neural entailment;sentence classification;sentences neural;induces explanations accompanying;supervised description use"}, "37ef7941909527aaf123d7b8f90adbf4606f4917": {"ta_keywords": "learning algorithm parallel;implementation parallel distributed;distributed quadratic learning;ihmm parallel distributed;data parallel implementation;data parallel;parallel implementation;algorithm parallel description;parallel distributed;parallel implementation based;algorithm parallel;parallel implementation parallel;implementation parallel;parallel distributed quadratic;parallel description data;version ihmm parallel;parallel implementation underlying;description data parallel;ihmm parallel;parallel distributed version;demonstrate parallel implementation;quadratic learning algorithm;distributed implementation;scale parallel implementation;present parallel implementation;increases distributed implementation;learning algorithm;parallel;parallel implementation scales;quadratic learning", "pdf_keywords": ""}, "58e5ce12c23f815e9b394220044eaf99b28cfffe": {"ta_keywords": "bose einstein condensate;interaction repulsive condensate;repulsive interaction condensate;hamiltonian interaction repulsive;condensate stable interaction;dynamics component bose;repulsive condensate stable;einstein condensate presence;component bose einstein;interaction attractive condensate;interaction condensate described;interaction condensate;einstein condensate;repulsive condensate;condensate presence repulsive;bose einstein;repulsive interaction;dependent hamiltonian interaction;hamiltonian interaction;attractive condensate stable;condensate stable;interaction repulsive;condensate described dimensional;schrdinger equation;condensate presence;time dependent hamiltonian;condensate described;attractive condensate;dimensional schrdinger equation;schrdinger equation solved", "pdf_keywords": ""}, "cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a": {"ta_keywords": "deceptive attention mechanisms;deceptive attention;method deceptive attention;trained explain predictions;design attention masks;modified attention;attention masks;using modified attention;attention mechanisms model;modified attention mechanism;attention mechanism modified;attention masks hide;method deceptive;attention mechanisms;design attention;explain predictions using;attention mechanism;simple method deceptive;attention;model trained explain;predictions using modified;deceptive;predictions using;inform consumers;inform consumers choices;feedback model demonstrate;model demonstrate modified;model trained;feedback model;explain predictions", "pdf_keywords": "adversarial attention;words train adversarial;deceptive attention masks;effective adversarial attention;adversarial attention alsoneural;deceptive attention;produce deceptive attention;manipulated attention models;manipulating attention prespecified;attention masks models;deceive human annotators;manipulating attention supervised;models produce deceptive;models distort attention;attention modifying;adversarial models distort;train adversarial;manipulating attention;attention manipulated model;representations attention turned;attention turned inference;attention supervised;attention encoder;attention mass impermissible;predictions attention;manipulate attention;attention models;attention modifying way;attention sender decode;use attention mechanisms"}, "79655bfc45039b4d7cfe6cc86d52a4ced492f43a": {"ta_keywords": "performance learning rank;learning rank methods;rank methods web;learning rank;experiments learning rank;generative learning rank;web track adhoc;learning rank formula;methods web track;rank methods;rank methods generative;rank methods handtuned;web track;compare performance learning;rank;performance learning;track adhoc task;rank formula;track adhoc;report experiments learning;adhoc task compare;methods generative learning;adhoc task;task compare performance;generative learning;experiments learning;methods web;task compare;methods generative;web", "pdf_keywords": ""}, "5c3cc301a892094d5bfca3c41a78a3a8ebd755f8": {"ta_keywords": "dropouts multipletress trees;dropouts;employing dropouts;employing dropouts multipletress;brane regression;novel brane regression;dropouts multipletress;method employing dropouts;multipletress trees;brane regression model;multipletress trees mart;regression classification tasks;regression classification;trees mart resulting;trees;set regression classification;trees mart;regression;models;large set regression;resulting novel brane;brane;novel brane;inaccessible models;regression model;set regression;classification;model large set;level inaccessible models;model", "pdf_keywords": "dropouts ensemble trees;regression trees mart;regression trees;additive regression trees;ranking regression classification;ensemble trees;regression evaluation trees;outperforms random forest;method training trees;regression classification tasks;algorithm ranking regression;random forest;training trees;trees linear regression;ensemble trees muting;training trees linear;based random forest;efficiently train trees;employing dropouts ensemble;regression classification;dropouts ensemble;ranking regression;evaluation trees;random forest linear;random forest eachwe;available datasets outperforms;efficiently classify;datasets outperforms;datasets outperforms mart;regression classification using"}, "9eecfdb7c8ad9af4f3863e9f6ed857211fb710e7": {"ta_keywords": "lieb liniger strategy;strategy optimal decision;optimal decision making;domain specific explanations;financial services designed;explanations explanation interactive;making domain financial;decision making domain;novel explanation lieb;optimal decision;domain financial services;domain financial;strategy optimal;liniger strategy optimal;financial services;explanation interactive;explanation lieb;liniger strategy;domain independent techniques;explanation designed;decision making;explanation lieb liniger;financial;explanation interactive does;explanations explanation;explanations;techniques generate domain;time explanation designed;lieb;lieb liniger", "pdf_keywords": ""}, "dfd8fc9966ca8ec5c8bdc2dfc94099285f0e07a9": {"ta_keywords": "empirical privacy;preserve privacy text;privacy text proposed;desired privacy guarantees;empirical privacy guarantee;privacy text;differentially private mechanisms;privacy guarantees experiments;mechanisms preserve privacy;maintaining desired privacy;preserve privacy;differentially private;desired privacy;privacy;privacy guarantees;private mechanisms preserve;private mechanisms;class differentially private;art empirical privacy;privacy guarantee;private;real text classification;class constrained empirically;text classification datasets;text classification;proposed class constrained;constrained empirically;maximizing utility;classification datasets;experiments real text", "pdf_keywords": "empirical privacy utility;maximize empirical privacy;privacy metric reconstruction;empirical privacy metric;privacy machine learning;maintaining empirical privacy;privacy text generation;estimate empirical privacy;empirical privacy machine;satisfy empirical privacy;empirical privacy;quantify empirical privacy;empirical privacy guarantee;privacy natural language;privacy metric;privacy measure adversary;privacy utility;privacy utility tradeoff;preserve privacy text;mechanisms empirical privacy;preserving privacy;preserving privacy data;desired privacy guarantees;privacy utility utility;mechanism privacy measure;privacy preserved data;preserve desired privacy;preserve privacy data;privacy text;privacy measure"}, "e8c4a4e81084e17b0c71a6a69bdf1e4e2b6f6af1": {"ta_keywords": "speech separation multi;models speech separation;speech separation;sources mixture sequence;contextual output sequences;multi speaker speech;mixture sequence model;speaker speech recognition;separation multi speaker;multiple sources mixture;speech recognition;sequence model extracting;models speech;speech recognition experiments;estimated contextual output;mixture sequence;infer output sequences;evaluate models speech;sources mixture;contextual output;conditional multi sequence;extracting multiple sources;multi sequence model;speaker speech;multi speaker;estimated contextual;separation multi;output sequences;multi sequence;previously estimated contextual", "pdf_keywords": "contextual output sequences;chain model speech;sequence transduction;tackle sequence transduction;speech conditional model;infer output sequences;sequence models conditional;multiple output sequences;output sequences probabilistic;sequence model conditional;model speech separation;sequence transduction problem;conditional signal transduction;conditional multi sequence;model speech;multi sequence models;sequences experiments speech;novel conditional chain;sequence modeling;sequences probabilistic chain;sequences explicit modeling;based sequence mapping;long sequence modeling;output sequences input;output sequences;sequence models;sequence modeling proposed;model conditional chain;speech separation multi;multi sequence model"}, "59121b847fd7eb4cf92cbfccb54f1705733d8b65": {"ta_keywords": "dereverberation acoustic features;method dereverberation acoustic;dereverberation acoustic;reverberation method based;reverberation method;reverberation;reverberation effect;preprocessor reduces reverberation;features presence reverberation;presence reverberation method;reduces reverberation;reduces reverberation effect;presence reverberation;reverberation effect factor;acoustic features presence;acoustic features;novel dereverberation preprocessor;acoustic;novel method dereverberation;dereverberation preprocessor;use novel dereverberation;novel dereverberation;dereverberation;method dereverberation;dereverberation preprocessor reduces;dynamic mismatches paper;dynamic mismatches model;static dynamic mismatches;dynamic mismatches;static dynamic", "pdf_keywords": ""}, "cec37cd54a940bec818db7216cc1086672f3fec0": {"ta_keywords": "synsets representing concepts;lexical substitutions crowdsourcing;based concept lexical;synsets individual word;automatically deriving synsets;concept lexical;concept lexical substitutions;concept inventory induction;representing concepts based;representing concepts;substitutions crowdsourcing approach;substitutions crowdsourcing;deriving synsets;concept inventory;crowdsourcing approach able;based concept oor;deriving synsets representing;lexical;lexical substitutions;crowdsourcing approach;synsets representing;crowdsourcing;concept oor;introduced concept inventory;concept oor recently;concepts based concept;synsets;duplicate synsets;potentially duplicate synsets;concepts based", "pdf_keywords": ""}, "fa10752ab1768d1633001420b48be5e2518a4f80": {"ta_keywords": "collision rhic proton;relativistic heavy ion;ion collision rhic;proton cross section;heavy ion collision;proton proton cross;rhic proton proton;ion collision;proton cross;calculation proton;applied calculation proton;calculation proton proton;section relativistic heavy;cross section relativistic;rhic proton;method calculation proton;relativistic heavy;collision rhic method;relativistic mean field;section relativistic;field theory relativistic;proton proton;rhic heavy ion;combination relativistic mean;relativistic mean;theory relativistic mean;collision rhic;theory relativistic;proton;combination relativistic", "pdf_keywords": ""}, "9fcfbc662d4095d72eb9a4e1c4f5ae8f0ffc4222": {"ta_keywords": "teeth measurements dehydration;dehydration secondary lesions;tomography results dehydration;lesions extracted teeth;extracted teeth measurements;measurements dehydration secondary;extracted teeth performed;teeth average lesion;dehydration rate secondary;dehydration secondary;rate secondary lesions;measurements dehydration;extracted teeth;results dehydration;results dehydration correlated;secondary lesions extracted;teeth performed using;dehydration correlated;dehydration correlated rate;teeth performed;investigates relationship dehydration;dehydration rate;relationship dehydration rate;results secondary lesions;lesions results secondary;secondary lesions results;lesions extracted;coherence tomography results;teeth measurements;dehydration", "pdf_keywords": ""}, "5801974fcebc11b4a8085fb02e77f792454caf7c": {"ta_keywords": "automated social skills;social skills training;accuracy social skills;social skills;provides social skills;automated social;novel automated social;improve accuracy social;skills training simulates;accuracy social;training simulates human;skills training;training means human;human computer interaction;skills training measuring;skills training means;training simulates;interaction provides social;computer interaction provides;accuracy training;social;simulates human computer;training accuracy training;provides social;computer interaction;effect training accuracy;training accuracy;simulates human;skills;computer interaction used", "pdf_keywords": ""}, "20f166f7809d1af9065cd1c71ec1e38d5d92993f": {"ta_keywords": "intrinsic fear learned;fear learned reward;learned reward shaping;deep reinforcement learning;reinforcement learning guards;fear learned;deep reinforcement;reinforcement learning;intrinsic fear;introduce intrinsic fear;catastrophic states improve;policies periodic catastrophes;accelerates deep reinforcement;learned reward;avoid catastrophic states;intrinsic fear able;reward shaping;reward shaping accelerates;probability imminent catastrophe;assumptions intrinsic fear;reinforcement;predict probability imminent;catastrophic states;able avoid catastrophic;fear able avoid;learning predict probability;avoid catastrophic;learning predict;learning guards oscillating;imminent catastrophe", "pdf_keywords": ""}, "c43d9d868f5288738cd625d365f0b3a5c18d4a20": {"ta_keywords": "professional simultaneous interpreters;experience based translation;experience corpus;experience corpus available;translation corpus;simultaneous interpreters;simultaneous interpretation used;interpreters;simultaneous interpretation able;simultaneous interpreters different;simultaneous interpretation;based translation corpus;recorded interpretation results;interpretation results professional;japan simultaneous interpretation;translation corpus contains;interpreters different;interpretation able compare;recorded interpretation;corpus;present simultaneous interpretation;interpretation used;interpretation able;amounts experience corpus;interpretation;interpretation used extract;contains recorded interpretation;corpus available;translation;interpretation results", "pdf_keywords": ""}, "3be5e7310b1bec9b4431ad0f1264f536b6a39f14": {"ta_keywords": "machine translation models;pivot improve translation;translation models;sparse noisy translation;translation quality significantly;machine translation;improve translation quality;translation quality;optimized improve translation;use machine translation;noisy translation;noisy translation compares;translation quality demonstrate;translation models context;improve translation;pivot languages;use pivot languages;pivot languages use;translation compares use;translation compares;translation;single pivot improve;pivots works significantly;pivot improve;models context sparse;word character level;single pivot;character level models;context sparse noisy;pivot", "pdf_keywords": "trained translation models;robust translation models;machine translation models;translation models;translation models results;languages quality translation;pivots trained translation;boost translation quality;level machine translation;translation quality crucial;translation prediction;statistical machine translation;crucial robust translation;machine translation prediction;translation models pivot;machine translation;translation quality;optimizing translation;translation models able;robust translation;pivot based translation;quality translation improved;characterizing optimal translation;optimally optimizing translation;trained translation;translation model;optimally perform translation;models joint translation;quality translation related;optimal translation"}, "6e7e095f46deb297713dcde05991faf635768d29": {"ta_keywords": "race algorithmic fairness;algorithmic fairness research;algorithmic fairness;fairness research;conceptualizations race algorithmic;race theory sociological;race algorithmic;conceptualizations race;race theory;ground conceptualizations race;critical race theory;sociological work race;race ethnicity ground;fairness;race ethnicity;work race;ethnicity ground conceptualizations;critical race;work race ethnicity;race;present critical race;theory sociological;algorithmic;ethnicity ground;theory sociological work;ethnicity;sociological;sociological work;theory;ground conceptualizations", "pdf_keywords": "race algorithmic fairness;racial categories algorithmic;adopted algorithmic fairness;categories algorithmic fairness;algorithmic fairness subject;purposes algorithmic fairness;algorithmic fairness research;algorithmic fairness;andthe algorithmic fairness;algorithmic unfairness focus;use algorithmic fairness;algorithmic unfairness;fairness research race;race algorithmic;algorithmic fairness use;limitations algorithmic fairness;algorithmic fairness frameworks;algorithmic fairness methodologies;algorithmic fairness literature;effectiveness algorithmic fairness;conceptualization race fixed;algorithmic fairness larger;algorithmic fairness cambridge;operationalizing race conceptualizing;conceptualization race;central algorithmic fairness;algorithmic fairness analysis;race racial categories;algorithmic fairness note;race critical concept"}, "4d10d7c02ce01d71f11c296b09b389c6f20b354b": {"ta_keywords": "aggregation data crowdsourcing;data crowdsourcing marketplaces;data crowdsourcing;data labeling aggregation;labeling aggregation data;labeling aggregation;crowdsourcing marketplaces;efficient data labeling;real data labeling;data labeling;crowdsourcing;data organized labeled;crowdsourcing marketplaces focus;data labeling process;aggregation incremental relabeling;aggregation data;results efficient aggregation;efficient aggregation;labeling;efficient aggregation incremental;aggregation;organized labeled;data organized;relabeling dynamic pricing;aggregation incremental;efficient data;labeling process;tutorial efficient data;labeled;resulting data organized", "pdf_keywords": ""}, "191169031c7646c02ecb1aaa9c8a6b6e05009730": {"ta_keywords": "reduced graphene oxide;graphene rgo ng;graphene rgo;graphene oxide;synthesis hollow graphene;oxide doped graphene;graphene spheres graphene;doped graphene rgo;based reduced graphene;graphene spheres;graphene;hollow graphene spheres;reduced graphene;spheres graphene;graphene sheets achieved;graphene oxide doped;hollow graphene;doped graphene;spheres graphene sheets;graphene sheets;electromagnetic shielding film;shielding film based;shielding film;new electromagnetic shielding;electromagnetic shielding;average shielding;synthesized rgo ng;shielding;exhibits average shielding;shielding effectiveness", "pdf_keywords": ""}, "19b6e7158ee4f13caa004a0b6c6a6e0ef965ea8f": {"ta_keywords": "chime challenges robust;robust automatic speech;speech recognition everyday;automatic speech;chime challenges;automatic speech recognition;speech recognition;describes chime challenges;robust recognition;challenges robust automatic;describes chime;robust automatic;developed robust recognition;chime;recognition everyday environments;chapter describes chime;robust systems automatically;developed robust;techniques developed robust;recognition;challenges robust;robust;robust systems;recognition everyday;automatically autonomously perform;automatically autonomously;challenges build robust;build robust systems;systems automatically autonomously;autonomously perform tasks", "pdf_keywords": ""}, "53f6c82035d43a19b9c8be0de651cae25bdd4bda": {"ta_keywords": "spoken word transcriptstyle;automatic transformation spoken;transformation spoken word;word transcriptstyle;word transcriptstyle text;transcriptstyle text;dropped words implemented;transcriptstyle;transcriptstyle text enables;finite state transducers;substitutions dropped words;state transducers;spoken word;words implemented;state transducers wfsts;dropped words;words implemented using;automatic transformation;word error rate;deletion disfluencies substitutions;transformation spoken;transducers wfsts;transducers wfsts achieves;introduces automatic transformation;transducers;text;deletion disfluencies;word error;disfluencies substitutions;enables deletion disfluencies", "pdf_keywords": ""}, "821532ecef5bc2252823b190c35f1e4c44ddc41c": {"ta_keywords": "word embeddings parallel;multilingual word embeddings;embeddings parallel text;align multilingual word;parallel text leveraging;align multilingual;embeddings parallel;train multilingual word;tuning parallel text;word embeddings;methods align multilingual;word embeddings obtain;multilingual word;parallel text objectives;parallel text;language pairs;train multilingual;multilingual;trained language models;different language pairs;experiments language pairs;language models fine;language models;able train multilingual;language pairs demonstrate;text leveraging pre;parallel;performance different language;embeddings;text leveraging", "pdf_keywords": "alignments multilingual corpora;multilingual word alignment;monolingual word alignment;extract alignments multilingual;word alignments monolingual;lingual word alignment;alignments multilingual;multilingual corpora model;multilingual word aligners;multilingual language models;parallel text multilingual;word alignment improves;align multilingual word;alignments monolingual;multilingual corpora;aligned representations multilingual;multilingual word corpora;crosslingual machine translation;neural word alignment;language models multilingual;machine translation optimize;quality word alignments;unconstrained words alignment;multilingual corpora using;align multilingual;multilingual word performance;alignments monolingual setting;model word alignment;alignments target corpora;objectives parallel corpus"}, "2b4edb9515a26561ea3f9ee2a63a506721c8369e": {"ta_keywords": "dataset aspect prediction;sentiments analysis scientific;aspect based sentiments;aspect prediction;reviews extract useful;scientific reviews extract;reviews extract;sentiments analysis;based sentiments analysis;analysis scientific reviews;dataset aspect;aspect based;training dataset aspect;active learning;machine learning;based sentiments;workshop machine learning;scientific reviews;use aspect based;sentiments;field machine learning;machine learning use;active learning framework;machine learning ilc;use aspect;use active learning;aspect;decision reviewers;reviewers;extract useful information", "pdf_keywords": "annotate sentence sentiment;sentiment features review;automatically annotation corpus;text annotate corpus;annotation corpus;identify annotate sentences;sentiment features;annotate corpus;annotation corpus papers;annotate sentences text;review corpora sentiments;reviews extract useful;annotate corpus features;sentiments extracted review;annotate sentences;sentiment features useful;classify sentiment;peer review corpora;sentence sentiment patterns;reviews design annotation;reviews extract;scientific reviews extract;sentiment analysis scientific;automatically annotation;extracted review text;perform sentiment;sentiment analysis;impact sentiment analysis;method classify sentiment;review corpora"}, "f6d6c4dd0115386c234a0b027dd38f7aa9d9df2f": {"ta_keywords": "nonlinear maps;study nonlinear map;nonlinear maps illustrate;nonlinear map;form nonlinear maps;map certain problem;nonlinear map certain;maps illustrate case;certain tasks represented;map;map certain;maps;tasks form documents;tasks represented;nonlinear maps order;maps illustrate;tasks represented form;represented form nonlinear;form documents;field challenges writing;documents particular form;challenges writing executing;case study nonlinear;study nonlinear;nonlinear;form documents particular;challenges writing;certain tasks form;form nonlinear;problem represented", "pdf_keywords": ""}, "31c53acd2a43dcec4342d9c42d0ffbfbef36e855": {"ta_keywords": "estimation noisy label;noisy label using;noisy label;likelihood estimation noisy;estimation noisy;label using moment;miscalibration estimation;classifier confusion matrix;label using;label;maximum likelihood estimation;moment matching approach;moment matching;miscalibration estimation error;classifier;calibration classifier;likelihood estimation;using moment matching;maximum likelihood;estimation error support;view maximum likelihood;calibration classifier confusion;confusion matrix;classifier confusion;includes calibration classifier;synthetic data sirtori;synthetic data;confusion matrix invertibility;miscalibration;impacts miscalibration estimation", "pdf_keywords": "learning label shift;estimating label shifts;label shift estimation;adaptation algorithm mlls;estimation label shift;label distributions methods;domain adaptation algorithm;domain adaptation;minimum learning label;label shift mlls;unsupervised domain adaptation;learning label;domain label distributions;estimating label marginal;label distributions;label estimation;estimate label distribution;label distribution novel;approaches estimating label;shift mlls rigorously;characterize label distribution;generalized distribution matching;classifiers based distribution;label distribution;estimating label;methodology label estimation;method estimating label;describing label distribution;estimating importance weights;based distribution matching"}, "ccfaccf36b9cd7c0c05af2285ec90ecf5f51a34c": {"ta_keywords": "optimal relay location;relay location optimal;optimal relay;allocation source relay;optimal placement relay;formulas optimal relay;placement relay nodes;optimal power allocation;source relay channel;relay channel;relay nodes multi;relay channel exponential;relay nodes;location optimal power;placement relay;source node relay;relay location;source relay;power allocation;power allocation source;node relay;relay node;multi hop network;nodes multi hop;optimal power;node relay node;relay node provide;single relay;channel exponential power;relay", "pdf_keywords": ""}, "d8d12c922fc571d081bae27c67fcf50cdbb17d90": {"ta_keywords": "summarisation model trained;text summarisation dataset;lingual transfer learning;historical text summarisation;summarisation documents historical;summarisation dataset;text summarisation documents;standard text summarisation;language parallel data;text summarisation;summarisation dataset consists;summarisation documents;summarisation model;propose summarisation model;techniques propose summarisation;cross lingual historical;cross lingual transfer;based cross lingual;transfer learning techniques;trained cross lingual;summarised corresponding modern;lingual transfer;modern language parallel;cross lingual;transfer learning;lingual historical;summarisation;language parallel;summarised corresponding;task historical text", "pdf_keywords": "cross lingual summarisation;large monolingual summarisation;lingual summarisation support;monolingual summarisation dataset;summarizing historical documents;summarisation corpus historical;summarisation model trained;text summarisation historical;historical text summarisation;hybrid annotated summarisation;lingual summarisation;annotated summarisation model;lingual summarisation use;high quality summarisation;monolingual summarisation;summarisation support historians;summarisation corpus;text summarisation dataset;constructed summarisation corpus;summarizing historical;summarisation fundamental routine;task summarizing historical;summarisation historical historical;standard text summarisation;summarisation support;text summarisation;summarisation dataset train;text summarisation fundamental;annotated summarisation;quality summarisation corpus"}, "c4607387ee863d5c5e5dc9f8adfbe7930508e286": {"ta_keywords": "machine learning icml;machine learning society;international machine learning;conference machine learning;machine learning;learning icml;conference international machine;learning icml held;2008 conference machine;review 2008 conference;review 2008;international machine;icml held helsinki;icml;learning;machine;conference machine;brief review 2008;2008 conference;icml held;accepted 2008 conference;2008 conference including;conference international;annual conference international;learning society iml;review;held helsinki finland;finland;papers accepted 2008;helsinki finland", "pdf_keywords": ""}, "7f4fa7c6f16f2965a104fa45071ea0c92b4366fe": {"ta_keywords": "model polymer repulsive;polymer repulsive interaction;monomers polymer stiffness;polymer stiffness simple;dimensional polymer repulsive;polymer stiffness;polymer repulsive;simple model polymer;stiffness simple model;model polymer;repulsive interaction depend;model dimensional polymer;stiffness simple;repulsive interaction;stiffness;dimensional polymer;number monomers polymer;monomers polymer;polymer;repulsive;depend number monomers;interaction depend number;interaction depend;monomers number monomers;simple model;monomers;number monomers;monomers number;interaction;simple model dimensional", "pdf_keywords": ""}, "7634b0cf93169d2a95d4d7193f47f97a61e3b4b2": {"ta_keywords": "behavior data surveys;able distinguish players;distinguish players different;games used distinguish;distinguish players;distinguish different players;human behavior data;dimensional games used;dimensional games;players different traits;high dimensional games;traits human behavior;behavior data;game design;players validate approach;identifying traits human;data surveys framework;surveys framework;data surveys;game design framework;surveys framework based;present game design;human behavior;players validate;identifying traits;players different;surveys;different players validate;framework identifying traits;games", "pdf_keywords": "behavior diagnostic games;games elicit distinguishable;behavior diagnostic game;diagnostic games elicit;diagnostic game design;probabilities game learning;diagnostic games based;games purpose distinguishing;diagnostic games;decision making games;players seek maximize;optimization parameterize games;games validate approach;games successfully distinguish;player traits optimization;game learning parameters;approach game design;distinguish players game;game decision makers;games elicit;approach game;parameterize games;behavior mutual information;diagnostic game;behavioral observations formalize;quantify effectiveness game;quantitatively capture game;theory setting game;expected behavior players;effectiveness game design"}, "1ccf412212873ae1b020762b8b86291e1fb11f65": {"ta_keywords": "dataset crowdsourced audio;crowdsourced audio transcriptions;crowdsourced audio;data collection crowdsourcing;crowdsourced data collect;dataset crowdsourced;crowdsourced data;collection crowdsourcing;collection methods crowdsourced;methods crowdsourced data;scale dataset crowdsourced;crowdsourced;methods crowdsourced;crowdsourcing;audio transcriptions;audio transcriptions applicability;transcriptions applicability resource;dataset;resource language russian;reliable data collection;data collection;data collect;russian language contribute;large scale dataset;principled data collection;transcriptions applicability;data collection methods;language russian language;transcriptions;russian language", "pdf_keywords": "transcriptions dataset crowdsourced;crowdsourced audio transcriptions;datasets crowdsourced audio;dataset crowdsourced audio;crowdsourced audio annotations;crowdsourced audio;crowdsourced audio recordings;collection crowdsourced audio;datasets crowdsourced text;analysis crowdsourced datasets;audio transcriptions dataset;crowdsourced datasets;constructing datasets crowdsourced;audio transcriptions crowd;synthetic datasets crowdsourced;crowdsourced datasets facilitate;datasets crowdsourced;available dataset crowdsourced;sample datasets crowdsourced;dataset crowdsourced;datasets crowdsourced translations;collection datasets crowdsourced;transcriptions crowd;data collection crowdsourcingthis;audio annotations prevalence;current state crowdsourcing;transcriptions dataset;crowdsourced annotations text;audio annotations;crowdsourced annotations"}, "3ba529f732d3c4a31e9ce57f1c78ddf911846bf4": {"ta_keywords": "evaluation weak supervision;generated weak supervision;weak supervision sources;weak supervision;weak supervision sbc;supervision sources modular;supervision sbc approaches;supervision sources;sequence tagging;classification sequence tagging;supervision sbc;sequence tagging range;supervision;evaluation weak;standardized evaluation weak;generated weak;datasets classification sequence;thorough standardized evaluation;tagging;weak;classification sequence;approaches benchmark;tagging range;sbc approaches benchmark;sequence;approaches benchmark platform;tagging range real;benchmark;extensive comparisons;framework sbc evaluation", "pdf_keywords": "weak supervision benchmark;learning weak supervision;accuracy weak supervision;supervision benchmark platform;weak supervision sources;weak supervision feature;generated weak supervision;framework weak supervision;supervision benchmark;platform weak supervision;models weak supervision;multiple weak supervision;weak supervision wf;weak supervision class;approach weak supervision;model weak supervision;standardized weak supervision;supervised learning weak;weak supervision model;strong supervision method;supervision method scalable;weak supervision;weak supervision named;weak supervision proposed;weak supervision novel;weak supervision problem;weak supervision provide;weak supervision ws;types weak supervision;present weak supervision"}, "64bc7fe1c46c4d4106afba4621ff1bd4376c077a": {"ta_keywords": "patient aaryngeal model;aaryngeal model method;aaryngeal model;estimate ofaryngeal distortion;ofaryngeal distortion induced;patient aaryngeal;ofaryngeal distortion;induced patient aaryngeal;aaryngeal;coupled single mode;mode minimally coupled;distortion induced patient;peaks spectrum patient;spectrum patient saryngeal;patient saryngeal packet;model single mode;peaks spectrum;mode mode method;method estimate ofaryngeal;saryngeal packet method;mode method;single mode mode;distortion induced;estimate ofaryngeal;estimation number peaks;mode method applied;mode mode minimally;single mode minimally;single mode;spectrum patient", "pdf_keywords": ""}, "04db62a14f78f693d6bd14a4803b9b73325b36bb": {"ta_keywords": "detecting fake news;detect fake news;fake news detection;fake news subgraph;news subgraph classification;classify subgraph news;news subgraph;news detection;subgraph news item;knowledge based fake;news detection problem;subgraph news;news based knowledge;modal fake news;news detection does;fake news based;based fake news;knowledge graph;based knowledge news;detecting fake;knowledge news;knowledge graph transform;fake news;detect fake;news item learned;graph neural network;external knowledge graph;news based;graph neural;subgraph classification", "pdf_keywords": "fake news subgraph;news subgraph classification;detecting fake news;classify subgraph news;fake news detection;detect fake news;news subgraph;nodes subgraph news;subgraph news item;learning subgraph embedding;false news detection;subgraph embedding;subgraph news;subgraph embedding function;detection false news;entities train subgraph;neural networks news;news detection social;news detection combines;knowledge graphs extract;identifying classifying news;knowledge graph;news social networks;knowledge graph transform;uncover fake news;news detection algorithm;knowledge graphs;extract relations news;news detection;classifying news"}, "9bd6cdae71506eb307507e44df7abe0c285b3ca7": {"ta_keywords": "translation transformation model;translation translation model;translation model;translation translation transformation;coupled machine translation;translation transformation;machine translation translation;translation model tested;machine translation;result machine translation;neural model quality;translation translation;transformation model;diffusion network neural;transformation model based;neural model;action diffusion network;translation;neural model coupled;neural action diffusion;network neural model;activation neural action;network neural;neural action;activation neural;transformation;model based activation;action diffusion;diffusion network;effect neural model", "pdf_keywords": "neural machine translation;machine translation improvement;neural model translation;translation improvement accuracy;machine translation;language reranking neural;translation quality;translation improvement;translation nnt ranking;standard machine translation;translation engine evaluated;translation engine;machine translation mt;neural language model;translation engine given;generic translation engine;training generic translation;perception translation quality;performance translation engine;predict translation;model translation languages;machine translation nnt;translation languages compare;translation mt models;predict translation output;model translation;neural models rerank;translation languages;language reranking;ranking non neural"}, "e0a0b3438aef008fece5b8bbf76105b470f10f25": {"ta_keywords": "quantum dots entanglement;quantum dot entanglement;dots entanglement;dot entanglement;dots entanglement generated;dot entanglement fundamental;separated quantum dots;entanglement spatially separated;single quantum dot;state entanglement created;quantum entanglement;quantum dot;processing quantum entanglement;coherent state entanglement;quantum dots;entanglement spatially;state entanglement;quantum entanglement entanglement;entanglement entanglement;entanglement;entanglement created;entanglement entanglement spatially;entanglement generated;entanglement created single;quantum dot initially;entanglement generated single;quantum entanglement key;entanglement key;type quantum entanglement;quantum dot transferred", "pdf_keywords": ""}, "1817c9f0fd8a17e31c65963dd8cee9783059495b": {"ta_keywords": "xmath1 xmath2 xmath3;xmath2 xmath3 xmath4;xmath16 xmath17 xmath18;xmath15 xmath16 xmath17;xmath17 xmath18 xmath19;xmath0 xmath1 xmath2;xmath3 xmath4 xmath5;xmath14 xmath15 xmath16;xmath12 xmath13 xmath14;xmath13 xmath14 xmath15;xmath4 xmath5 xmath6;xmath1 xmath2;xmath11 xmath12 xmath13;xmath10 xmath11 xmath12;xmath6 xmath7 xmath8;xmath17 xmath18;xmath9 xmath10 xmath11;xmath18 xmath19 xmath20;xmath3 xmath4;xmath5 xmath6 xmath7;xmath16 xmath17;xmath7 xmath8 xmath9;xmath2 xmath3;xmath8 xmath9 xmath10;xmath19 xmath20 xmath21;xmath15 xmath16;xmath18 xmath19;xmath13 xmath14;xmath14 xmath15;xmath11 xmath12", "pdf_keywords": ""}, "167adafac25ee108ca99c688cceded8bca710bb1": {"ta_keywords": "age human brain;brain size stimuli;human brain size;statistical analysis age;brain size;size stimuli investigated;age dependence size;analysis age dependence;age results analysis;control sample age;sample age results;size stimuli;age dependence;age human;analysis age;sample age;age results;human brain;relationship age human;stimuli investigated using;using statistical;stimuli investigated;statistical;investigated using statistical;statistical analysis;relationship age;mouse presented results;using statistical methods;age;compared control sample", "pdf_keywords": ""}, "538466f2a69271617bf4f5b0df4e5fd854c11c35": {"ta_keywords": "sparse graph codes;adaptive group testing;graph codes reliably;group testing scheme;codes reliably recover;group testing;graph codes;codes reliably;based sparse graph;sparse graph;complexity decoding;scheme based sparse;decoding algorithm optimal;complexity decoding algorithm;defective items 6c;computational complexity decoding;decoding algorithm;defective items;based sparse;small defective items;non adaptive group;10k defective items;sparse;testing scheme;adaptive group;decoding;defective items high;testing scheme based;recover 10k defective;arbitrarily small defective", "pdf_keywords": "adaptive group testing;sparse graph codes;group testing schemes;noisy group testing;testing problems sparse;group testing scheme;group testing code;codes group testi;group testing noisy;solving group testing;finding group testing;graph codes group;testing code recovers;consider group testing;group testing;optimal decoding complexity;group testing based;nonadaptive group testing;group testing theorems;groups items efficiently;decoding complexity;group testing problems;group testing framework;group testing problem;new algorithm recovery;optimal decoding;complexity decoding;codes group;improved detect decode;order optimal decoding"}, "7e358ffc2731a82420d84a7f0bedb155a487c39d": {"ta_keywords": "question answering dataset;multihop question answering;answering dataset muss;answering dataset;multihop muss dataset;question answering;dataset muss improved;consists 25k questions;muss dataset;dataset muss;muss dataset muss;single hop datasets;hop datasets goal;hop datasets;datasets;dataset muss consists;existing datasets;25k questions existing;datasets goal;new multihop muss;dataset;performance existing datasets;goal use dataset;multihop muss;use dataset;25k questions;datasets use;existing datasets use;dataset construct new;answering", "pdf_keywords": "targeting connected reasoning;connected reasoning consensus;connected reasoning reducingunderstanding;connected reasoning;connected reasoning use;connected reasoning important;generating multihop questions;connected reasoning present;using connected reasoning;reasoning consensus connected;requires connected reasoning;reasoning graphs;question decompositions improved;question decompositions;consensus connected reasoning;reasoning graphs unsourceable;improved question decompositions;multihop questions systematically;traditional question decompositions;datasets simpler questions;representations reasoning achieved;open domain comprehension;refer connected reasoning;problems reasoning achieved;dataset called understanding;network representations reasoning;challenging questions graph;questions connected methodology;building multihop datasets;knowledge generation tasks"}, "61cce75554a6d1bb802f26758c3b0ba97de6918d": {"ta_keywords": "graph attentional;graph attentional networks;termed graph attentional;networks positional embeddings;attentional networks positional;graph framework positional;predictive graph context;attentional networks;positional embeddings learned;networks positional;graph context;graph context plugged;positional information nodes;positional embeddings;gats positional embeddings;positional embeddings gat;predictive graph;attentional;model predictive graph;embeddings learned;embeddings capture structural;positional content information;graph framework;nodes graph framework;positional embeddings capture;nodes graph;information nodes graph;positional content;framework positional embeddings;embeddings learned model", "pdf_keywords": "graph attention layers;graph attentional layer;graph attention networks;graph attentional;graph attentional networks;graph attention;enhanced graph attentional;networks positional embeddings;modify graph attention;termed graph attentional;gnns positional embeddings;networks gnns positional;called graph attention;described graph learning;graph learning;graph convolutional networks;attention networks gnns;positional embedding trained;attention networks positional;attentional networks positional;graph networks;attention layers;positional embeddings node;positional embeddings learning;graph networks propose;attention networks;attentional networks;networks gnns;classification tasks gnns;embedding trained"}, "b81acc013c42796a5eea0fc20cfb04846da3a589": {"ta_keywords": "multilingual language discovery;language discovery;language discovery documentation;documentation combines linguistic;linguistic features language;language data efficient;language data;linguistic processing tools;combines linguistic features;large scale linguistic;multilingual language;novel multilingual language;linguistic networks;linguistic features;manipulate language data;linguistic networks able;linguistic processing;combines linguistic;multilingual;scale linguistic networks;discovery documentation combines;novel multilingual;features language theory;present novel multilingual;linguistic;discovery documentation;predict manipulate language;features language;powerful linguistic processing;language theory", "pdf_keywords": "linguistic annotation backend;annotation linguists using;linguistic annotation linguists;provide linguistic annotation;processing linguistic annotation;annotated linguistic data;linguistic annotation;annotation linguists;annotated linguistic;annotation candidates linguist;data linguisto corpus;partially annotated linguistic;user machine translation;linguisto corpus;linguisto corpus systems;machine translation pipeline;annotation backend technology;process linguistic annotation;translates language data;linguistic data provide;linguists using;language data;machine translation;language user machine;data linguisto;annotation backend;machine machine translation;language processing linguistic;annotation candidates;language processing"}, "398a0625e8707a0b41ac58eaec51e8feb87dd7cb": {"ta_keywords": "agents text based;train agents text;agents learn abstract;agents learn;agents text;text based learning;enables agents learn;based learning pipeline;learning pipeline;text based environments;agents abstract knowledge;learn abstract policies;simulator enables agents;learning pipeline method;text based;generalize train agents;policies text based;model text based;creation new agents;new agents abstract;agents abstract;based learning;model text;learn abstract;effective model text;text;policies text;learning;train agents;abstract policies text", "pdf_keywords": "textworld language embodied;text embodied based;text based alfworld;building understanding textworld;alfworld text based;framework text embodied;text embodied;text based environments;interactive text environment;text based environment;textworld language;understanding textworld;understanding textworld language;framework novel alfworld;embodied based learning;alfworld enables agents;abstract text world;action textual environment;language embodied;agents tasks text;training agents text;framework unseen embodied;based alfworld framework;text based context;text agent knowledge;alfworld framework;alfworld text;called alfworld text;interactive text;learning tasks abstract"}, "4f7bbcef3d40cafad17936fdf562a121667af1e8": {"ta_keywords": "divergence geometric regularization;vessel tree reconstruction;reconstructing vector fields;flow divergent arteries;regularization principle reconstructing;divergent arteries;principle reconstructing vector;arteries convergent veins;geometric regularization principle;reconstructing vector;divergent arteries convergent;geometric regularization;new geometric regularization;blood flow divergent;convergent veins;model blood flow;arteries convergent;vector fields based;vector fields;divergence geometric;flow divergent;regularization principle;geometric regularization constraint;arteries;regularization;reconstruction particularly bifurcations;principle reconstructing;vessel tree;blood flow;tree reconstruction", "pdf_keywords": "vessel tree reconstruction;approximation vessel tree;regularization oriented vessel;vessel reconstruction problems;unsupervised vessel tree;vessel tree extraction;vessel reconstruction;divergence disambiguate vessel;problems vessel reconstruction;class vessel reconstruction;vessel tree estimation;tangent approximation vessel;flow pattern vessel;vessel divergence use;complete vessel trees;filtered vessel trees;approach unsupervised vessel;pattern vessel tree;vessel divergence;approximation vessel;disambiguate vessel directions;filtered vessel tree;reconstructing vector fields;vessel tree extracted;vessel trees based;reconstruction problems vessel;based curvature regularization;reconstruct complete vessel;method vessel tree;vessel pathline tangents"}, "0431f60546381a9e91fb156236c3c7056f57081f": {"ta_keywords": "singing voice synthesis;pitch augmentation mix;training singing voice;based pitch augmentation;pitch augmentation;voice synthesis;training singing;boost training singing;voice synthesis ss;data augmentation methods;augmentation methods boost;singing databases demonstrate;singing databases;data augmentation;augmentation mix augmentation;augmentation methods;singing voice;augmentation stabilize training;proposed augmentation methods;augmentation mix;different data augmentation;mix augmentation;augmentation methods stabilizing;augmentation;mix augmentation stabilize;public singing databases;proposed augmentation;experiments public singing;singing;augmentation stabilize", "pdf_keywords": "singing datasets demonstrate;singing voice synthesis;singing datasets;singing voice augmentation;predicting acoustic features;prediction acoustic features;acoustic features improved;public singing datasets;voice synthesis;voice augmentation supervised;voice synthesis svs;framework predicting acoustic;synthesized singing;predicting acoustic;learning based singing;voice augmentation;synthesized singing extensive;methods pitch augmentation;quality synthesized singing;input predicts music;augmentation methods pitch;new method singing;acoustic feature local;prediction acoustic;acoustic features;predicts music;singing extensive experiments;method singing voice;method singing;method prediction acoustic"}, "dbe87b171bfb789e1d22a047aeeee69105e6fd02": {"ta_keywords": "sentence embeddings encoder;performance sentence embeddings;sentence embeddings achieves;sentence embeddings;sentence embeddings extracted;sentences encoder;sentences encoder decoder;extracted sentence embeddings;sequence sentences encoder;embeddings extracted sentence;embeddings encoder decoder;characterization sentence embeddings;embeddings encoder;embeddings achieves;emission sequence sentences;embeddings;embeddings extracted;encoder decoder models;encoder decoder;encoder;embeddings achieves state;encoder decoder model;decoder models spontaneous;decoder model significantly;language processing tasks;decoder models;language processing;decoder;sentences;based encoder", "pdf_keywords": "encode sentence representations;sentence embeddings transfer;sentence embeddings encoder;sentence embedding tasks;sentence embedding models;sentence encoders;sentence encoder;model sentence embeddings;sentence representations enhanced;tuning sentence embedding;sentence encoder model;sentence embeddings extract;representations sentence encoder;embeddings transfer tasks;sentence transfer tasks;scaling sentence embeddings;sentence embeddings;embeddings encoder text;sst5 sentence embedding;sentence embeddings based;architectures sentence encoders;sentence embedding;sentence encoders analyze;sentence representations;text transformers sentences;encoder contrastive learning;encoder text transformers;sentencet5 consists encoder;model sentence transfer;embeddings encoder textwe"}, "95788ed8affd06c0c2c6159c26ff7c123c4f2e0a": {"ta_keywords": "speaker wise conditional;speaker diarization methods;speaker wise parametric;speaker diarization;end speaker diarization;speakers stop sequence;speaker wise;variable number speakers;issue speaker wise;end speaker;multi speaker;fixed number speaker;number speaker;results multi speaker;speaker issue speaker;speaker audio recordings;end end speaker;speaker audio;issue speaker;speaker;multi speaker audio;novel speaker wise;number speaker issue;speaker issue;number speakers stop;number speakers;novel speaker;speakers stop;propose novel speaker;conditional inference method", "pdf_keywords": "model speech separation;speaker diarization stream;speaker diarization method;speaker diarization;speech separation task;based separation speaker;method speaker diarization;speakerwise conditional signal;applied speaker diarization;speech separation;speaker outputs computation;speaker wise conditional;speaker diarization essential;outputs speakerwise conditional;separation speaker source;speaker speech activity;task speech separation;speakerforcing method based;conditioning speaker speech;separation speaker;speaker deep learning;speakerwise conditional;method speaker speechin;speaker wise decoding;speakerforcing proposed method;task speakerforcing method;task speakerforcing;task speakerforcing proposed;speech separation 36;speaker speechin"}, "84908a28a03d0d7c467d9556ed36f0e416de7171": {"ta_keywords": "hybrid semantic analysis;meaning recognition based;perform semantic analysis;describes hybrid semantic;semantic analysis;hybrid semantic;semantic analysis meaning;semantic analysis remaining;perform semantic;semantic analysis large;analysis meaning recognition;able perform semantic;semantic;context free grammars;utterances;meaning recognition;semantic analysis fraction;free grammars able;utterances able;utterances able perform;free grammars;extended context free;large number utterances;grammars;grammars able perform;grammars able;analysis fraction utterances;extended context;context free;number utterances able", "pdf_keywords": ""}, "b46be3ac246499655cc442e93c5878e7a9640ae3": {"ta_keywords": "constructing timeline;saga like events;methods constructing timeline;produced timelines;constructing timeline compare;events define timeline;timelines;define timeline;timeline running sequence;timeline running;produced timelines competitive;hand produced timelines;timeline;timelines competitive;timeline compare;define timeline running;representations saga like;timeline compare performance;representations saga;explicit representations saga;saga;sequence related events;events present;saga like;related events;related events present;events;like events;events define;events present unsupervised", "pdf_keywords": ""}, "8da992b611df508b1803f66ffa53bd1fb741a76c": {"ta_keywords": "question answer game;neural language model;conditional neural language;answer game;quality generated hierarchies;generated hierarchies experiments;generated hierarchies;language model generate;neural language;generate hierarchy hierarchies;generate hierarchy;input document hierarchy;document hierarchy;hierarchies based;hierarchies;hierarchies experiments empirical;hierarchies experiments;question answer pairs;language model;model generate hierarchy;answer game converts;hierarchy hierarchies;hierarchy;hierarchy hierarchies based;conditional neural;document hierarchy question;neural;hierarchy question answer;use conditional neural;question answer", "pdf_keywords": "learning question generation;question generation systems;challenging text generation;question generation;question generation using;text generation task;question generation applications;new question generation;generating meaningful sentences;question generation experiment;text generation pipeline;text generation;neural question generation;generated questions representative;text generation process;question answering task;novel text generation;problem text generation;neural text generation;control question generation;generate sequence questions;answer language modelling;generated questions;question answering;generation task;questions discovery natural;quality document sequences;discovery natural language;paragraph answer language;accuratelyun answerable questions"}, "2eea63f896deed47cc0c0000e1482ec5c860fd0b": {"ta_keywords": "level controversy detection;controversy detection;controversy detection problem;reply structure sentiment;structure sentiment information;post level controversy;structure sentiment;sentiment information;sentiment information structure;information reply structure;semantic information reply;reply structure;controversy;reply structure structure;structure based semantic;components reply structure;information structure based;based semantic information;level controversy;based semantic;sentiment;semantic information;information structure;semantic;components reply;method post level;structure based;structure;following components reply;structure structure based", "pdf_keywords": ""}, "881ce19455a9923e4798e9d77d2d8623ca9d2e03": {"ta_keywords": "statistical framework speech;speech recognition;framework speech recognition;speech recognition based;approximations predictive distribution;predictive distribution bpc;predictive distribution;based variational bayes;predictive distribution experimentally;framework speech;variational bayes vb;bpc based variational;recognition based discrete;variational bayes;sparseness robust clustering;version predictive distribution;approximations predictive;based variational;data sparseness robust;robust data sparseness;analytical approximations predictive;data sparseness;novel statistical framework;distribution bpc discrete;robust clustering;bpc robust data;sparseness robust;predictive;data version predictive;novel statistical", "pdf_keywords": ""}, "6d9603be7e79ff33677327a0edd5bd3f7da6347b": {"ta_keywords": "propagation charged particle;charged particle turbulent;zeeman field propagation;turbulent medium particle;particle turbulent medium;particle turbulent;effect zeeman field;field propagation charged;propagation charged;turbulent medium;stationary medium particle;medium particle assumed;medium particle;zeeman field;charged particle;field propagation;effect zeeman;particle assumed vicinity;turbulent;particle assumed;particle;zeeman;propagation;vicinity stationary medium;study effect zeeman;stationary medium;vicinity stationary;assumed vicinity stationary;stationary;field", "pdf_keywords": ""}, "81af4e14050c410e2afee226be583088a9791ddf": {"ta_keywords": "learn argument embeddings;argument embeddings context;argument embeddings;argument embeddings according;bias argument embeddings;semantic role induction;embeddings unsupervised semantic;unsupervised semantic role;embeddings according dependency;learn argument;embeddings context explicitly;embeddings context;unsupervised semantic;embeddings according;incorporating dependency relations;embeddings unsupervised;semantic role;model unsupervised semantic;embeddings;state art embeddings;role induction conll;dependency relations;dependency relations multiplicative;model learn argument;role induction;role induction slc;context explicitly incorporating;sim999 word similarity;word similarity task;dataset sim999 word", "pdf_keywords": ""}, "ec99cf93ef22a0c0d669abe90c9509f642b2cf69": {"ta_keywords": "downsampling technique learns;adaptive downsampling;sampling improving;uniform sampling improving;novel adaptive downsampling;adaptive downsampling technique;downsampling;downsampling technique;sampling locations;sampling;outperforms uniform sampling;semantic boundaries target;favor sampling locations;sampling locations near;near semantic boundaries;learns favor sampling;favor sampling;uniform sampling;semantic boundaries;locations near semantic;boundaries target classes;sampling improving balance;near semantic;target classes;technique learns;boundaries target;semantic;learns;propose novel adaptive;target classes method", "pdf_keywords": "sampling deep;semantic segmentation reduce;good semantic segmentation;downsampling technique learns;sampling deep learning;semantic segmentation critical;semantic segmentation rely;accurate semantic segmentation;including semantic segmentation;downsampling input image;adaptive downsampling;adaptive downsampling high;adaptive adaptive downsampling;semantic segmentation;object deep learning;learning semantic boundaries;improves quality segmentation;segmentation rely good;nonuniform sampling deep;downsampling high resolution;object semantic boundaries;adaptive downsampling approach;based adaptive downsampling;object based deep;required semantic segmentation;state art segmentation;semantic segmentation use;downsampling approach extension;downsampling approach;downsampling"}, "48e8e8085907192d501eb2bcc582035e90431a2f": {"ta_keywords": "sequence tagging;specific sequence tagging;sequence tagging tags;tags sequence words;tagging tags sequence;tags sequence;predict tags;sequence words model;tagging;layers predict tags;deep gated recurrent;sequence words;predict tags demonstrate;task specific sequence;tagging tags;tasks including chunking;encode tags;tags;recurrent unit encode;unit encode tags;words model;gated recurrent;encode tags applies;words model employs;benchmark tasks;chunking ner;recurrent;including chunking ner;tasks;gated recurrent unit", "pdf_keywords": "tagging based deep;deep gated recurrent;deep hierarchical recurrent;model sequence tagging;predict sequence tags;sequence words model;recurrent neural network;word embeddingswe model;sequence tagging based;sequence tagging;hierarchical gated recurrent;hierarchical recurrent neural;recurrent neural;hierarchical recurrent;tag sequence model;sequence tagging given;tags sequence model;network sequence tagging;training corpus;based gated recurrent;encode morphology context;model capture linguistic;neural network sequence;deep hierarchical;words model;tagging given sequence;based deep hierarchical;corpus trained;word embeddingswe;sequence tags"}, "5431098723db5858c4553f0259921cbbdd6492d5": {"ta_keywords": "diseases transition predator;prey transition;prey prey transition;prey transition prey;transition predator prey;infectious diseases transition;transition prey prey;transition prey;prey attack transition;key transition prey;prey key transition;attack transition prey;diseases key transition;transition prey attack;infectious diseases key;prey key step;step spread infectious;transition predator;spread infectious diseases;diseases transition;predator prey prey;prey attack;predator prey;prey prey key;prey prey;prey attack key;key transition;spread infectious;prey;attack key transition", "pdf_keywords": "translation infrastructure;translation initiative covid19;translation infrastructure able;translate pandemic data;community translate pandemic;machine translation mt;translation initiative;translation systems;translation mt tools;translate pandemic;provide translation data;forming translation initiative;provide translation benchmark;translation data;development translation data;concept translation infrastructure;translation benchmarked;machine translation mwe;infrastructure provide translation;translation development;machine translation;translation development translation;translation benchmark;use machine translation;resourced languages translation;translation data resourced;translation data tesdespite;languages machine translation;machine translation results;machine translation systems"}, "a72e732f2d11075aa0103b72b4f9884ddcaaaa85": {"ta_keywords": "negative learnability logic;learnability logic programs;learnability logic;inductive logic programming;positive negative learnability;negative learnability;learnability;logic programs;logic programming theory;logic programs equivalent;logic programming;inductive logic;logic programs use;combination inductive logic;theory polynomial learning;programming theory polynomial;polynomial learning;polynomial learning derive;programming theory;logic;programs equivalent;performance positive negative;programs equivalent sum;learning derive;programming;learning;probabilities success worst;learning derive following;inductive;combination inductive", "pdf_keywords": ""}, "015dc5b71894dd4d05e7668d015e545ab2e162ba": {"ta_keywords": "speech processing toolkit;text speech processing;open speech processing;text speech;speech processing;speech processing including;based speech processing;end text speech;s__ based speech;toolkit named espnet;extension open speech;processing toolkit espnet_;based speech;espnet_ new toolkit;toolkit espnet_;named espnet extension;open speech;toolkit espnet_ new;espnet extension;speech;espnet;named espnet;processing toolkit;espnet extension open;text;espnet_;new toolkit;espnet_ new;end text;toolkit", "pdf_keywords": "speech processing toolkit;generating speech;speech e2e toolkit;source speech processing;generating speech using;generation speech input;generate accurate speechwe;generated speech;text speech;open source speech;approach generating speech;desired speech generative;generated speech proposed;generate desired speech;speech generative;software generation speech;speech synthesis generative;end text speech;text speech e2e;parametrically generated speech;speech synthesis;speech input text;speech processing;deep learning string;apply generated speech;speech synthesis based;neural network big;toolkit espnet sourcenet;novel speech synthesis;source speech"}, "3122a2d7799ba585b993e432b3deb47659b3f3c1": {"ta_keywords": "question answering lfqa;question answering;sparse attention contrastive;form question answering;answering lfqa task;sparse attention;based sparse attention;attention contrastive retriever;contrastive retriever learning;attention contrastive;lfqa task based;answering lfqa;lfqa task;task based sparse;answering;retriever learning;attention;retriever learning approach;retriever learning heuristic;learning heuristic extract;contrastive retriever;apply retriever learning;learning heuristic;learning approach;learning approach relies;learning;approach long form;task;lfqa;long form question", "pdf_keywords": "question answering model;question answering lqa;question answering;answer models;length answer models;answering model;attention contrastive retriever;answering lqa task;answer models recently;attention model;sparse attention model;answering model based;sparse attention;uses sparse attention;generated retrieval;attention model train;generated retrieval model;relies sparse attention;domain question answering;attention;form question answering;answer question long;sparse attention contrastive;question long document;retrievals resultingwe introduce;probabilistically generated retrieval;generators correct retrievals;end quantum;answering;answering lqa"}, "15d643f4c27d373aa46f26a760051e76fde81dc2": {"ta_keywords": "question answering;end entity resolution;entity resolution;differentiable knowledge graphs;entity resolution e2e;answering problem model;question answering problem;knowledge graphs;knowledge graphs kgs;building differentiable knowledge;context question answering;entity;end entity;end end entity;problem model trained;answering;model public datasets;differentiable knowledge;answering problem;model task;model task end;knowledge;differentiable model task;problem model;public datasets;public datasets compares;model trained using;resolution e2e context;model trained;fully differentiable model", "pdf_keywords": "answer entity train;answering knowledge graphs;answer questions entities;question answering knowledge;model question answering;models question answering;knowledge graph scalable;questions entities;answer entity;knowledge base queries;query billions entities;entities knowledge base;questions entities entity;external knowledge graphs;best answer entity;entangling query billions;question answering;answering knowledge;learning representations entities;entities knowledge;entity answer;task entity resolution;knowledge entities;entities scalable representations;knowledge entities knowledge;knowledge graphs extract;knowledge graphs;differentiable knowledge graphs;graphs extract entities;knowledge graph"}, "5f9d8fe21efb3c2b241427869a333472ab09a22d": {"ta_keywords": "single atom laser;atom laser pulse;laser atom interaction;atom interaction laser;interaction atom laser;atom laser model;interaction laser atom;atom laser;laser atom;pulse single atom;applied laser atom;single photon laser;interaction laser pulse;photon laser pulse;idea laser atom;laser pulse single;laser pulse generated;photon laser;laser pulse;interaction laser;generation single photon;based interaction laser;laser pulse based;model applied laser;laser model;single photon;laser model applied;single atom;laser;applied laser", "pdf_keywords": ""}, "ea6547e877c1cc3d37229a6f488ac04e9a11de18": {"ta_keywords": "protein interfaces predictions;water positions protein;protein interfaces;protein protein interfaces;protein interfaces preferential;positions protein protein;predictions water positions;colic protein testing;colic protein;positions protein;blind predictions water;domain colic protein;protein testing accuracy;predictions water;accuracy docking models;descriptions features protein;water contacts simulation;features protein portion;protein testing;water positions;preferentially water positions;features protein;protein portion complex;water contacts;protein protein;accuracy docking;docking models groups;fraction water contacts;testing accuracy docking;discovery domain colic", "pdf_keywords": ""}, "fd1b59d22eb1fb32e0360d4fcbe58dc4ebb25af9": {"ta_keywords": "detection audio deepfakes;audio deepfakes;deep fakes;deep fakes survey;deepfakes particularly detection;fakes survey deepfakes;deepfakes created detected;audio deepfakes recent;research deepfakes;deepfakes created;research deepfakes particularly;deepfakes;deepfakes recent trends;following generation deepfakes;detection audio;particularly detection audio;deepfakes particularly;survey deepfakes created;generation deepfakes;fakes survey;fakes;deepfakes important;survey deepfakes;deepfakes recent;need research deepfakes;generation deepfakes important;deep;deepfakes important generation;detection methods;shortcomings detection methods", "pdf_keywords": "speech deep fakes;fakes recording speaker;detecting fakes recording;audio deepfakes;detect fakes recording;audio deepfake research;fakes recording;audio deepfake;introduce audio deepfake;source audio deepfake;audio deepfake generation;fakes recording target;deep fakes;fakes detecting fakes;deep fakes focusing;audio deepfakes overlooked;creating detecting deepfakes;detecting fakes;detecting deepfakes use;detect fakes;fakes focusing deep;detecting deepfakes;fakes detecting;approaches detect fakes;predict real speech;detecting deepfakes developed;generate detect deepfakefakes;detect deepfake news;focuses audio deepfakes;detect deepfakefakes"}, "86b91922923b03c66497accfa88c638299fc8d26": {"ta_keywords": "variational encoder decoder;variational encoder;decoder trained variational;trained variational encoder;employs variational encoder;encoder discrete latent;encoder decoder represented;encoder decoder;encoder decoder trained;encoder decoder method;approach encoder decoder;variable encoder decoder;latent variable encoder;decoder method encoder;approach encoder;discrete encoder;method encoder decoder;represented discrete encoder;encoder;encoder decoder problem;encoder discrete;decoder trained;variable encoder;novel approach encoder;decoder represented;decoder represented discrete;decoder;decoder method;trained variational;decoder problem employs", "pdf_keywords": ""}, "f35a01c1e5d5375453c39e6161526633492fb574": {"ta_keywords": "erasure code queue;data erasure distributed;erasure code;standard erasure code;xmath0 erasure code;minimum xmath0 erasure;erasure distributed environment;erasure distributed;perform data erasure;latency minimum xmath0;codes latency minimum;xmath0 erasure;codes latency;data erasure;code queue;improvement standard erasure;standard erasure;erasure;latency provide closed;effect codes latency;code queue provide;latency minimum;latency provide;queue;latency;throughputs various systems;average latency provide;bounds average latency;queue provide analytical;average latency", "pdf_keywords": "distributed storage batch;storage consider queue;queueing systems;distributed storage systems;distributed batch scheduling;jobs distributed storage;distributed storage consider;distributed storage;translation distributed storage;underlying distributed storage;performance proposed queues;process distributed batch;latency distributed memory;distributed storage data;distributed storage provide;storage data distributed;distributed batch;storage processing;data distributed memory;used distributed storage;process analogous queueing;data storage processing;analogous queueing;queueing provide scheduling;proposed queues based;queueing theory;queues based notion;performance associated queues;performance associated queue;storage processing theorems"}, "9b71542ef5d5178041048b9a330309053bb0bcfc": {"ta_keywords": "speech combining discrete;synthesize ground truth;speech mixture;speech proposed method;input target speech;target speech combining;speech combining;truth speech mixture;ground truth speech;speech mixture different;combining synthesis;synthesize ground;synthesis model input;achieved combining synthesis;combining synthesis model;mixture different languages;target speech proposed;method synthesize ground;input discrete symbols;synthesis model;combining discrete symbols;synthesis;discrete symbols input;speech quality;high speech quality;recognition discrete symbols;speech proposed;languages using recognition;truth speech;model input discrete", "pdf_keywords": "speech discretization vocoder;speech separation enhancement;separation speech enhancement;enhancement speech separation;vocoder speech separation;speech discretization synthesis;trained speech discretization;speech separation models;extracted speech discretization;speech enhancement based;based speech separation;speech separation implemented;speech discretization;developments speech separation;speech enhancement speech;speech discretization discretization;trained speech separation;speech enhancement given;methods speech enhancement;problem speech enhancement;speech separation proposed;discretization vocoder;evaluation speech enhancement;discretization vocoder discrete;speech enhancement;novel speech separation;problem speech discretization;framework speech enhancement;sequence speech separation;speech separation predicted"}, "79ab3a0d6dc5d6fd3b466ea2814fdbb93a3672d0": {"ta_keywords": "zeeman field spin;spin polarized xmath0he;dynamics spin polarized;xmath1he spin chain;spin polarized xmath0;chain spin polarized;chain spin dynamics;spin dynamics spin;xmath1he spin;spin chain spin;xmath0he xmath1he spin;spin dynamics;field spin dynamics;dynamics spin;spin polarized;spin chain;study spin dynamics;chain spin;field spin;spin chain used;spin;polarized xmath0he xmath1he;polarized xmath0he;effect zeeman field;polarized xmath0;study spin;effect zeeman;zeeman;zeeman field;used study spin", "pdf_keywords": ""}, "da10c4bc1de7b9b7ddbb21d70ff5092a15cb866f": {"ta_keywords": "transfer learning unsupervised;domain adaptation;domain adaptation based;transfer learning based;transfer learning;problem domain adaptation;methods transfer learning;directions transfer learning;learning unsupervised;learning based unsupervised;learning unsupervised unsupervised;unsupervised approach;based unsupervised approach;unsupervised unsupervised;adaptation based;unsupervised unsupervised setting;based unsupervised;adaptation based maximum;unsupervised setting;adaptation;methods unsupervised;transfer;unsupervised;unsupervised setting novel;comparison methods transfer;learning;methods transfer;methods unsupervised setting;art methods unsupervised;unsupervised setting conclude", "pdf_keywords": ""}, "9db77e925015ad02efc9beeab233bedbfe04e4b7": {"ta_keywords": "smooth strategy reinforcement;benchmark reinforcement learning;benchmark reinforcement;method benchmark reinforcement;novel reinforcement learning;strategy reinforcement learning;smooth strategy;reinforcement learning method;reinforcement learning tasks;strategy reinforcement;reinforcement learning;novel reinforcement;present novel reinforcement;learning method accelerates;reinforcement;ursa smootheding method;smootheding method accelerates;dula ursa smootheding;learning tasks method;ursa smootheding;learning tasks;smootheding method;smootheding;learning method;development smooth strategy;learning tasks demonstrating;strategies incorporating accurate;new strategies;strategies;benchmark", "pdf_keywords": "benchmark reinforcement learning;training benchmark reinforcement;reinforcement learning strategy;benchmark reinforcement;strategy reinforcement learning;learning reinforcement learning;train reinforcement learning;policy search reinforcement;learning reinforcement;approach reinforcement learning;standard reinforcement learning;novel reinforcement learning;reinforcement learning reinforcement;search reinforcement learning;reinforcement learning;reinforcement learning es;reinforcement learning discuss;reinforcement learning extends;scalable reinforcement learning;reinforcement learning evolution;problem reinforcement learning;new strategy reinforcement;strategy reinforcement;method reinforcement learning;reinforcement learning task;reinforcement learning approach;reinforcement learning problems;learning evolution strategy;reinforcement learning framework;learning task smooth"}, "af0aa62d243c761b56a83369bc9b1f75805003cf": {"ta_keywords": "extraction text mining;coordinate term extraction;text mining;term extraction text;extraction coordinate term;features text mining;term extraction;text mining resulting;extraction text;coordinate term text;mining resulting similarity;extracted similarity;text mining improve;extraction coordinate;extracted similarity measure;term text based;accuracy extracted similarity;framework extraction coordinate;similarity words framework;term text;framework extraction;similarity words;notion similarity words;resulting similarity measure;propose framework extraction;features text;based notion similarity;similarity measure;coordinate term;text based notion", "pdf_keywords": ""}, "f765b23f0b0d2a196bc0fe562ad24278d0c9cee4": {"ta_keywords": "adaptively learning parameter;parameter deep learning;learning method adaptive;learning parameter deep;adaptive learning method;global adaptive learning;method adaptively learning;adaptively learning;deep learning model;method adaptive learning;learning parameter;adaptive learning;learning model convolutional;deep learning;adapting stochastic optimization;neural network adapting;gradient method learning;learning rate parameter;global adaptive;method global adaptive;stochastic optimization method;adapting stochastic;learning method;stochastic optimization;model convolutional neural;new method adaptively;learning model;method adaptively;adaptively;method adaptive", "pdf_keywords": ""}, "ccbc17d42f2b260079eee702fd97a75de705d8ac": {"ta_keywords": "sentences form vector;vector sentences set;input vector sentences;vector sentences;generate entropic vector;vector set sentences;vector generated;sentences language method;generation vector;based generation vector;generation vector set;set sentences language;sentences language;sentences set method;generate entropic;sentences set;method generate entropic;sentences form;entropic vector set;set sentences form;entropic vector matched;entropic vector;vector generated presence;resulting entropic vector;set sentences;form vector generated;generate;sentences;language method;generated", "pdf_keywords": ""}, "75b13e7131997ff6fd21325d68a2222d2c1b7157": {"ta_keywords": "spatiotemporal beamforming neural;beamforming neural networks;beamforming neural;spectral separation beamforming;separation beamforming based;separation beamforming;spatiotemporal beamforming;method spatiotemporal beamforming;beamforming based spatial;beamforming based;context size beamforming;beamforming formulations;beamforming;beamforming formulations demonstrate;size beamforming formulations;size beamforming;based spectral separation;based spatial separation;spatial separation;spectral separation;novel convolutional architecture;spatial separation method;network based spectral;convolutional architecture;novel method spatiotemporal;trained novel convolutional;novel convolutional;stabilized signal noise;scale invariant signal;neural network", "pdf_keywords": ""}, "00b874f8346cedadc2a6366c4b72e60140f99556": {"ta_keywords": "predicting semantic similarity;similarity words text;semantic similarity words;semantic similarity;predicting semantic;similarity words;text based attention;model predicting semantic;attention based neural;words text;words text based;similarity;semantic;neural;attention based;neural networks model;text;neural networks;based attention based;longitudinal transverse data;longitudinal transverse;transverse;longitudinal data transverse;transverse data longitudinal;text based;attention;words;longitudinal;data transverse;based neural", "pdf_keywords": ""}, "43c844c30765f3fa25bfabd83490ef826b9ceca1": {"ta_keywords": "word recognition model;word recognition;novel word recognition;rare unseen words;recognition model trained;words form random;keyboard mistakes pipeline;unseen words;unseen words form;recognition model;handling rare unseen;unseen words impart;semi character model;keyboard mistakes;character model;accuracy reduction vanilla;rare unseen;recognition;accuracy reduction;swaps keyboard mistakes;words;character model introduce;absolute accuracy reduction;word;spread rare unseen;model trained;keyboard;vanilla semi character;absolute accuracy;robustness competing models", "pdf_keywords": "word recognition adversarial;suppress adversarial spelling;adversarial spelling mistakes;adversarial spelling;word recognition safeguard;robustness word recognition;word recognition robust;accurate word recognition;novel word recognition;characterizing word recognition;character word recognition;character word recognizers;word recognition models;word recognizers;recognition adversarial;word models robust;word recognition;improved correct adversarial;word recognition achieved;trained suppress adversarial;recognition adversarial attacks;suppress adversarial;suppress adversarially;trained recognize words;word recognition model;word recognition able;word recognizers discover;suppress adversarial examples;class word recognition;attacks misspellings dropwords"}, "25ae911c13da7ef9def56ee30170920ebd48a668": {"ta_keywords": "computational argumentation;web arguments convincingness;computational argumentation task;new computational argumentation;argumentation task investigate;argumentation task;web arguments;argumentation;arguments having stance;arguments;arguments having;classification pair arguments;arguments 32 topics;problem relation classification;constraints cleaning crowdsourced;arguments convincingness;pair arguments having;arguments convincingness cast;relation classification;pair arguments;cleaning crowdsourced;pairs arguments;relation classification pair;crowdsourced data;crowdsourced;cleaning crowdsourced data;properties web arguments;16k pairs arguments;stance judged annotate;qualitative properties web", "pdf_keywords": ""}, "b48f0652605f981b5d407496aba3d9756725264f": {"ta_keywords": "user based preferences;preferences user impact;based preferences users;preferences users choice;determined preferences;determined preferences user;choice user based;based preferences user;solely determined preferences;solely based preferences;based preferences;choice choice user;preferences performance;impact preferences performance;choice user solely;users choice user;preferences users;controls choice user;choice user;preferences performance controls;impact preferences;preferences user;preferences;user impact decision;decision maker choice;users choice;user solely determined;choice choice;study impact preferences;user solely based", "pdf_keywords": ""}, "a636768c2fc6cadccd8bb4d704f651dd54dad395": {"ta_keywords": "emotion recognition indonesian;recognition indonesian speech;emotion recognition;study emotion recognition;classifier sdb corpus;corpus colorful utterances;colorful utterances;colorful utterances television;indonesian speech;indonesian speech construct;colorful speech;colorful speech build;yielding colorful speech;speech construct corpus;sdb corpus;utterances television;recognition indonesian;construct corpus colorful;machine classifies utterance;utterances;classifier;classifier sdb;utterances television talk;corpus colorful;speech build support;classifier optimize recognition;classifies utterance;support vector machine;acoustic features;utterance terms acoustic", "pdf_keywords": ""}, "f664e6635d0514b0cb398a713f08bab90b4a3d81": {"ta_keywords": "arcsecond sparse word;arcsecond word graph;sparse word graph;word graph based;word graph algorithm;sub arcsecond word;word graph;word correla tions;sub arcsecond sparse;arcsecond word correla;sparse word;arcsecond sparse;arcsecond word;characterization sub arcsecond;based sub arcsecond;tions large collections;sub arcsecond;correla tions;correla tions large;documents algorithm;word correla;arcsecond;documents algorithm based;collections documents algorithm;sparse;graph based sub;version sub arcsecond;tions;large collections documents;novel algorithm characterization", "pdf_keywords": ""}, "bd1bdb3c5f28001a4cee92c0e1669512d0f06a35": {"ta_keywords": "periodic potential model;dynamics particle periodic;particle periodic potential;hamiltonian particle periodic;particle periodic;periodic potential;periodic potential generalization;usual periodic potential;limit periodic potential;periodic potential resulting;model dynamics particle;dynamics particle;based hamiltonian particle;hamiltonian particle;study dynamics particle;generalization usual periodic;limit periodic;periodic;potential resulting particle;potential model;hamiltonian;model based hamiltonian;solved limit periodic;usual periodic;based hamiltonian;potential model based;particle density;particle;potential model solved;resulting particle density", "pdf_keywords": "generalized zipf law;zipf law generalized;derivation generalized zipf;hypergraph function;generalized zipf;hypergraph function hypergraph;function hypergraph;function hypergraph function;hypergraph;free energy density;gaussian free energy;evaluation function hypergraph;energy density free;diagram gaussian free;density free energy;function hypergraph use;combination function hypergraph;energy density gaussian;value function hypergraph;hypergraph linear;density gaussian free;zipf law;function hypergraph linear;density functional gdf;energy density functional;gaussian free;function hypergraph andwe;hypergraph use;functional gdf;hypergraph linear combination"}, "a9b9404962760731d6d2fc2ecbc6da7bc2f21be7": {"ta_keywords": "signal processing snp;distribution signal;reduction probability distribution;processing snp method;model based signal;processing snp;gaussian distribution form;snp method;uses gaussian distribution;distribution signal xmath2;signal processing;gaussian distribution;signal method based;method reduction probability;snp method uses;based signal processing;reduction probability;given distribution model;xmath1 distribution signal;uses gaussian;output signal method;signal method;distribution form xmath0;method uses gaussian;distribution model based;distribution form;gaussian;probability distribution function;given distribution;function given distribution", "pdf_keywords": ""}, "32367e7587d5b2de0391cff9ad2d600ff8624e60": {"ta_keywords": "human social skills;training presence audiovisual;users social skills;social skills training;perform social skills;audiovisual information proposed;social skills;audiovisual information;audiovisual;presence audiovisual information;presence audiovisual;human social;method perform social;perform social;simulation human social;skills training presence;improve users social;users social;social;training presence;skills training experimental;skills training;simulation human;based simulation human;smiling yaw pitch;skills;ratio smiling;considering ratio smiling;training experimental evaluation;training experimental", "pdf_keywords": ""}, "22f4eb19be4031e63194bbd7c355914533004918": {"ta_keywords": "dedicated hybrid transmission;hybrid transmission dht;hybrid transmission;future electrified vehicles;transmission dht electrified;optimized dedicated hybrid;transmission dht;dedicated hybrid;dht electrified segment;low energy driving;optimized transmission;optimized transmission based;segment optimized transmission;electrified segment optimized;energy driving gear;electrified vehicles unprecedented;dht electrified;gear shift powertrain;electrified vehicles;low impact road;shift powertrain;hybrid;shift powertrain model;vehicles unprecedented energy;powertrain model based;energy driving;driving gear shift;transmission;transmission based;powertrain", "pdf_keywords": ""}, "ea77b71385648f5c6ea533a0e3685f0e76302eba": {"ta_keywords": "lingual transfer learning;lingual transfer model;entity recognizers;entity recognizers millisecond;entity recognizers low;quality entity recognizers;languages active learning;predictions cross lingual;cross lingual transfer;transfer learning;based cross lingual;lingual transfer;highly resourced languages;transfer learning learns;cross lingual;learns highly resourced;millisecond sized languages;high quality entity;resourced languages;entity;resourced languages active;languages active;sized languages approach;lingual;active learning;learning learns highly;effective training data;learning learns;learns highly;sized languages", "pdf_keywords": "entity recognition ner;named entity recognition;lingual transfer learning;entity recognition;annotation based ner;model trained corpus;trained partially annotated;annotation data training;annotation approach outperforms;partially annotated sequences;corpus model trained;annotation task natural;annotation natural language;sequence annotation;annotation natural;sequence annotation approach;language processing ner;tokens sequence annotation;trained corpus;improve annotation;annotators trained nernst;annotated sequences spans;trained corpus predict;accurate annotators;annotators trained;annotated sequences;improve annotation based;large corpus;entity accuracy prediction;named entity text"}, "0822f8d7e6a72a65e65f147d3a8d8fccd485da40": {"ta_keywords": "recurrence methods transformers;sequence words trained;language modeled;perplexity model language;language modeled using;improving perplexity model;trained short subsequences;sequence words;methods improving perplexity;modeled using transformers;short sequence words;model language modeled;improving perplexity;model language;transformers let models;substantially improve perplexity;using transformers;methods transformers;transformers;tokens generating sequences;transformers method;improve perplexity;perplexity model;transformers let;methods transformers let;transformers method uses;uses short sequence;words trained short;using transformers method;short subsequences", "pdf_keywords": "training long sequences;training subsequences shorter;ofencoded pre transcription;subsequence length training;language transformers efficient;trained subsequence length;transcription sequence;pre transcription sequence;efficiency training subsequences;simple pre transcription;encoded pre transcription;transcription transcription sequence;transformers efficient learning;pre transcription transcription;transcription sequence process;sequence significantly faster;pre transcription;transcription;training subsequences;long sequences expensive;sequence learning;trained subsequence;shorter subsequences progressing;transcription sequence method;transcription sequence significantly;transcription transcription;range sequence learning;model trained subsequence;model short subsequences;subsequences faster"}, "253ad629cd2d396201d71aa605bec233bff66dca": {"ta_keywords": "clustering vbec speech;model speech recognition;acoustic model speech;vbec speech recognition;variational estimation clustering;model speech;estimation clustering vbec;speech recognition;decision tree clustering;speech recognition propose;probabilistic mixture model;acoustic model topology;vbec speech;complicated acoustic model;based probabilistic mixture;finding appropriate acoustic;acoustic model;speech recognition using;variational estimation;recognition using variational;clustering based probabilistic;using variational estimation;appropriate acoustic model;probabilistic mixture;estimation clustering;clustering vbec;mixture model;tree clustering based;tree clustering;mixture model gmm", "pdf_keywords": ""}, "fac95cc5f52f954fe89b3aa4b75895568ff6a6d4": {"ta_keywords": "normalization language data;normalization language;approach normalization language;based approach normalization;normalization;approach normalization;language data approach;language data;texts early high;historical texts early;variety historical texts;historical texts;texts early;historical texts shown;texts;rule based approach;texts shown competitive;rule based;frequency data results;frequency data;high frequency data;exact matches method;data results approach;data results;texts shown;baseline 65 method;data approach;terms exact matches;baseline;better performance baseline", "pdf_keywords": ""}, "8b73e226815d57bf66fc94905ebd063e4957b449": {"ta_keywords": "calibrating peer review;privacy inferring reviewer;inferring reviewer identity;peer review;reviewer identity utility;inferring reviewer;peer review challenges;reviewer identity;privacy inferring;tradeoff privacy inferring;reviewer;calibrating peer;privacy;peer;review challenges preventing;preventing inaccurate identify;review challenges;tradeoff privacy;review;frontier tradeoff privacy;problem calibrating peer;explicit computationally efficient;efficient algorithms optimal;computationally efficient algorithms;efficient algorithms;identity utility design;identity utility;explicit computationally;preventing inaccurate;algorithms optimal", "pdf_keywords": "calibration privacy optimally;optimal calibration privacy;algorithms calibration privacy;privacy calibration peer;calibration privacy;privacy calibration;reviewers papers adversary;privacy optimally;calibration privacy noisy;randomized reviewers;differential privacy;problem privacy calibration;randomized reviewers determine;decisions randomized reviewers;problem differential privacy;privacy optimally trades;differential privacy method;calibration peer review;reviewers paper uniformly;adversary optimal calibration;reviewers underlying distribution;optimality peer review;ensuring privacy;reviewers peer;reviewers optimal;privacy;privacy method;based differential privacy;distribution assignment reviewers;adversary assignment conference"}, "f249e3a7d4f7f964e9a4ca6e633ac31410a91dd8": {"ta_keywords": "monolingual data inflection;data inflection decoders;architecture inflection decoders;inflection decoders;inflection decoders enable;inflection decoders scale;low resource inflections;resource inflections models;monolingual data;resource inflections;languages monolingual data;data inflection;inflections models based;architecture inflection;attention architecture scales;lingual transfer;attention architecture;inflections models;novel architecture inflection;effects cross lingual;lingual;lingual transfer single;cross lingual transfer;inflections;inflection;step attention architecture;cross lingual;languages monolingual;languages;multiple languages monolingual", "pdf_keywords": "sequence model attention;language task predicting;novel language representations;morphological inflection powerful;language representations;novel attention model;state art morphological;powerful lexical processing;predict semantics languages;inflection powerful lexical;attention architecture;model attention;morphological inflection low;morphological inflection;interpretable attention;predicting message language;attention model;predict semantics;incorporate morphological;model monotonic attention;lexical processing;language used predict;generation interpretable attention;input incorporate morphological;model attention augments;incorporate morphological pattern;language representations approach;language task;labeling languages high;attention model task"}, "a309cb82c27233948f9b09f440be171a8d24ffff": {"ta_keywords": "peer selection algorithms;peer selection algorithm;peer selection accuracy;peer selection;based peer selection;peer selection problem;algorithm based peer;novel peer selection;competitive peer selection;generation peer selection;competitive peer;peer;achieve competitive peer;selection algorithm accurate;based peer;selection algorithms;selection algorithm;recent generation peer;partitioning agents;partitioning agents able;explicit partitioning agents;novel peer;selection accuracy;generation peer;algorithm accurate efficient;efficient existing algorithms;algorithm accurate;existing algorithms algorithm;algorithms;existing algorithms", "pdf_keywords": "peer selection algorithms;unbiased peer selection;peer selection algorithm;algorithm peer selection;selection unbiased peer;rank peer selection;peer selection method;peer selection based;exact peer selection;peer selection accurate;reviewers using probabilistic;automatic ranking networked;ranking networked agents;ranking assume review;given peer selection;peer selection strategyproof;peer selection;reviews assume ranking;ranking networked;selecting agents review;algorithm automatic ranking;impartial peer selection;automatic ranking;new peer selection;peer selection decision;present peer selection;selection algorithms empirically;majority reviewers using;review pool selection;majority reviewers"}, "04a94c15fec43e7563d58be697246a0dd6c57021": {"ta_keywords": "virtual content;virtual content web;generation virtual content;content web;content web argue;micro content post;social media platforms;generation spatial coherence;emergence spatial coherence;pre shared content;shared content;social media;spatial coherence;web argue remarkably;content;content post remarkably;spatial coherence generation;micro content;shared content able;spatial;generation spatial;spatial coherence results;argue generation spatial;media platforms;coherence generation virtual;content able;emergence spatial;web argue;web;content post", "pdf_keywords": "content provider creators;media creators;content providers platforms;created media creators;technology used curate;curate content nature;intermediaries creators;information content providers;content providers;curate content;content curated;users internet platforms;content nature;intermediaries creators breakthroughs;internet platforms;underlying content curated;media creators publishers;creation media;used curate content;information content provider;created media;algorithmic promotion extremism;content provider;creation activities internet;curate;user content rapidly;content curated experiences;algorithmic promotion;internet platforms continue;creators"}, "a99de68ee8d6729eee5ca5943b152aba7e4738ee": {"ta_keywords": "edits neural network;edits neural;information edits neural;edit represented neural;structure edits models;salient information edits;edits models;inputs structure edits;structure edits;structure edit represented;edits models used;model structure edits;edits encoded structure;neural network structure;structure edit;edits new inputs;structure edits encoded;form edits;edit represented;information edits;neural network;neural networks;neural networks model;represented neural network;neural network models;edits new;learn apply edits;new inputs structure;class neural networks;neural", "pdf_keywords": "learns edit representation;neural editor models;introduce neural editor;neural editor;learned edit representations;novel neural editor;computational neural editor;neural editor edit;neural editor model;distributed representations edits;neural editing;accurately neural editor;tree neural editor;prediction preserving editing;combining neural editor;neural editing model;edits combining neural;neural editor encodes;neural editor solving;efficiently generalize editing;learns edit;editing model learns;edit encoder neural;novel edit patterns;predicted code edits;based edit encoder;similar edits encoder;semantically similar edits;code edits database;edits encoder trained"}, "7f20366098665cd508fe82255cc1a65e1e733a14": {"ta_keywords": "variance acoustic features;estimating variance acoustic;features speech recognition;acoustic features speech;database automated speech;speech recognition;automated speech recognition;speech recognition proposed;speech recognition ar2;variance acoustic;acoustic features;automated speech;features speech;discriminative criterion adaptation;estimating variance;estimate variance;estimate variance proposed;estimation estimate variance;method estimating variance;variance proposed method;criterion adaptation;acoustic;based discriminative criterion;criterion adaptation uses;method based discriminative;maximum likelihood estimation;discriminative criterion;variance proposed;based discriminative;likelihood estimation", "pdf_keywords": ""}, "22d2f8030221bd0c27bfb9416eeffe4e86633780": {"ta_keywords": "accurate ranking documents;ranking documents;ranking documents using;semantic lexical scores;interpolation semantic lexical;based semantic similarity;semantic similarity computation;accurate ranking;method accurate ranking;lexical scores;semantic similarity;improvements hybrid indexes;interpolation semantic;ranking;using interpolation semantic;lexical scores method;query processing efficiency;improve query processing;performance query processing;indexes performance query;based semantic;semantic lexical;techniques improve query;hybrid indexes performance;hybrid indexes;documents using interpolation;semantic;query processing;indexes performance;indexes", "pdf_keywords": "document retrieval ranking;ranking long documents;retrieval ranking;dense retrieval models;document passage ranking;document retrieval improve;dense document retrieval;retrieval ranking method;sparse retrievalwe investigate;dense retrieval model;increasing sparse retrievalwe;fast forward indexes;ranking document;retrieval improve efficiency;sparse retrievalwe;fast forward index;based dense retrieval;ranking document current;best ranking document;index referred fast;document retrieval;improving ranking efficiency;passage ranking;combines document retrieval;fast query processing;fast query;retrieval models;dense retrieval;faster query processing;indexes dense document"}, "4e0610ac4c5e055ac56b2ae0d91386a10ffbd325": {"ta_keywords": "specific learning agent;student learning agent;learning agent used;learning agent;machine learning agent;deep feature learner;feature learner;feature learner student;learning agent able;domain specific learning;integration deep feature;learning agent evaluate;based integration deep;deep feature;specific learning;machine student learning;student machine learning;integration deep;machine student;learner;prior knowledge robust;knowledge robust;student learning;student machine;learner student;student student machine;learning;knowledge robust effective;prior knowledge;agent able integrate", "pdf_keywords": ""}, "8d019c77989100a51385e4b4a5fa5250445d8f1d": {"ta_keywords": "complementary systems combination;complementary systems;speech recognition spontaneous;performance complementary systems;speech recognition;acoustic modeling deep;tasks speech recognition;systems combination proposed;systems combination;improving performance complementary;combines acoustic modeling;complementary;deep neural networks;method combines acoustic;new approach boosting;neural networks;boosting methods;modeling deep neural;deep neural;approach boosting methods;approach boosting;combines acoustic;acoustic modeling;modeling deep;neural networks produce;performance complementary;boosting methods effectiveness;boosting;systems;neural", "pdf_keywords": ""}, "1b97c38d0156dc8bf300b41c2ba5c0463c3a2c00": {"ta_keywords": "training data augmentation;data augmentation strategy;data augmentation;best data augmentation;data augmentation workshop;data augmentation augmentation;results data augmentation;augmentation strategy;augmentation workshop aim;augmentation workshop np;augmentation strategy given;augmentation workshop;germany data augmentation;augmentation augmentation;augmentation;augmentation new method;augmentation augmentation new;training data;augmentation new;given training data;conducted training data;data;given training;training;best data;selecting best data;results data;strategy given training;workshop np 2015;conducted training", "pdf_keywords": ""}, "3df825e086b00dd4132c34ecbf638f9a6dc4320d": {"ta_keywords": "transfer learning module;transfer learning machine;integrating transfer learning;transfer learning;separate transfer learning;artificial intelligence agent;machine artificial intelligence;learning machine artificial;learning module machine;learning module;artificial intelligence;learning machine;intelligence agent method;intelligence agent;integrating transfer;integration separate transfer;integrate module machine;intelligence agent illustrate;separate transfer;learning;method integrating transfer;agent illustrate integration;module machine artificial;transfer;intelligence agent discuss;integration module student;module machine;module student illustrative;complex machine;machine artificial", "pdf_keywords": ""}, "3e59b3e1e3ef65f9574a0fe30f18ba7a815ea0af": {"ta_keywords": "efficiently explore dialogue;explore dialogue tasks;explore dialogue;dialogue tasks;dialogue tasks approach;quality dialogue;quality dialogue leverage;quantum networks;quantum networks used;dialogue leverage;dialogue;evaluate quality dialogue;repertoire quantum networks;dialogue leverage rich;quantum;networks improve;efficiently explore;networks;approach efficiently explore;repertoire quantum;structure repertoire quantum;repertoire networks improve;improve efficiency exploration;demonstrate improvement exploration;improvement exploration;networks improve efficiency;structure repertoire networks;networks used evaluate;nodes;exploration demonstrate improvement", "pdf_keywords": ""}, "790eb7e93f1d3fce470c0222fd2be83bab55a428": {"ta_keywords": "automatic speech;end automatic speech;automatic speech nn;based language model;language model;language model nn;word based language;word character based;speech nn combination;recognition end end;recognition end;speech nn;nn recognition end;model nn recognition;word character;end end automatic;speech;end automatic;character based nns;nn recognition;based language;word based;novel word based;combination word character;feature nn models;nn models;language;models proposed nn;nn models proposed;end end", "pdf_keywords": "language models rnn;rnn language models;nnn word representations;rnn language model;word based rnn;word based rnnlms;neural network nnn;end speech recognition;models rnn;speech recognition deep;rnn language;language hybrid attention;based rnn language;end deep cnn;language model trained;speech recognition asr;speech recognition encoder;radar neural network;attention convolutional neural;models rnn lms;ctc attention based;end speech processing;based rnnlms combined;hybrid attention ctc;external rnn language;end speech based;decoder external rnn;attention ctc network;approach based nnn;radar neural"}, "83cf7b9611fabe9da2d08722445039023f1b19e9": {"ta_keywords": "noise increase electron;gaussian noise increase;gaussian noise;effect gaussian noise;gaussian noise dynamics;increase electron density;electron density;noise dynamics;noise dynamics dimensional;dimensional electron gas;electron gas presence;electron gas;dynamics dimensional electron;increase electron;effect gaussian;noise increase;gaussian;electron density factor;strong magnetic field;magnetic field effect;electron;noise;dimensional electron;strong magnetic;field effect gaussian;presence strong magnetic;magnetic field;study effect gaussian;used increase electron;magnetic", "pdf_keywords": ""}, "9b09ff09b88bb793b161f284ca6e66031bc5a992": {"ta_keywords": "repulsive fluid interaction;viscosity repulsive fluid;fluid repulsive interaction;viscous fluid repulsive;2d fluid viscous;fluid interaction;viscosity repulsive;viscosity fluid xmath2;fluid interaction used;fluid xmath3 viscosity;xmath1 viscosity fluid;xmath3 viscosity repulsive;dimensional 2d fluid;2d fluid;repulsive fluid;fluid repulsive;viscosity fluid;fluid viscous fluid;viscous fluid;fluid viscous;density states fluid;xmath1 viscosity;xmath0 xmath1 viscosity;xmath3 viscosity;fluid xmath2 density;repulsive interaction interaction;viscosity;fluid xmath2;repulsive interaction;dynamics dimensional 2d", "pdf_keywords": ""}, "68f2f32e0e8fc868920971077a11042784be2616": {"ta_keywords": "ranking rating book;ranking rating;ranking;ranking rating covers;field ranking rating;learn field ranking;field ranking;data mining;data mining methodology;mining data mining;data mining data;use data mining;book data mining;mining data;mining methodology tuning;data mining book;rating book;algebra data mining;introduction field ranking;rating covers fundamentals;mining methodology;linear algebra data;data mining course;rating covers;linear algebra;rating book supplemental;rating;mining book;mining book intended;tuning parameters", "pdf_keywords": ""}, "a3ca4893ae941bd1601322aface4840e47339761": {"ta_keywords": "crowdsourcing scores mechanism;agents based crowdsourcing;based crowdsourcing scores;crowdsourcing scores;based crowdsourcing;crowdsourcing;agents evaluate reviews;selecting agents based;aggregate contributions agents;mechanism selecting agents;selecting agents;select agents based;automatically select agents;agents evaluate;contributions agents;having agents evaluate;evaluate reviews decentralized;select agents;agents based;idea having agents;contributions agents independent;reviews decentralized;agents;scores mechanism based;strategyproof mechanism selecting;having agents;agents independent;scores mechanism;evaluate reviews;mechanism selecting", "pdf_keywords": ""}, "a810d2f4a1fefd4175d8cdda9702ee1b829e5831": {"ta_keywords": "obesity population pseudogap;pseudogap adipose area;pseudogap adipose;population pseudogap adipose;mechanism suppressing obesity;pseudogap distribution subcellular;called pfaloproteins;pfaloprotein pfalo xmath0;called pfaloprotein;called pfaloprotein pfalo;excitons called pfaloproteins;pfaloproteins;called pfaloproteins pdcs;suppressing obesity;pfalo xmath0 mechanism;pfaloprotein;pfaloprotein pfalo;pfaloproteins pdcs responsible;pfaloproteins pdcs;adipose area;adipose;adipose area called;area called pfaloprotein;suppressing obesity population;pdcs responsible metabolism;obesity;metabolism;pfalo xmath0;responsible metabolism;subcellular excitons called", "pdf_keywords": ""}, "66081634c17b089cb47fd1b0ad7ad842c7fb3f87": {"ta_keywords": "simple learning agent;teachingable agent;use teachingable agent;mock student teaching;student mock;teaching mock student;teachingable agent called;learning agent;student mock student;mock student;mock student mock;mock student study;tutee student learning;student learning;teaching mock;student learning student;enhanced teaching mock;learning student learning;learning agent able;learning student;called mock student;student able solve;learning use teachingable;student learning enhanced;examples student able;student study tutee;simple learning strategy;student learning use;examples student;student teaching", "pdf_keywords": ""}, "d706645fbbc6edfad5fb642b1dfc3019fcabbd99": {"ta_keywords": "text generation papers;story evaluation experiments;text generation;ended text generation;story evaluation;generation papers;model generated text;human generated references;generation papers aim;text human generated;generated text;generated references;generated text human;collecting descriptions quantifying;series story evaluation;collecting descriptions;open ended text;generation;model generated;generated;descriptions;human generated;descriptions quantifying;evaluation experiments;tasks hindering reproducibility;reproducibility;evaluation;distinguish model generated;aim collecting descriptions;reproducibility run", "pdf_keywords": "text generation tasks;generated text evaluation;text generation;text generation task;machine generated texts;generated texts;ended text generation;text generation quality;openended text generation;generated text compared;machine generated text;text generation papers;generated text use;text generation amt;human generated text;generated text;generated text used;prompt generation;generate text computational;generate text;generated text researchers;behavior generated text;generation prompt description;model generated text;text used generate;generated text report;generated texts literature;quality generated text;used generate text;generated text paper"}, "ad7129af0644dbcafa9aa2f111cb76526ea444a1": {"ta_keywords": "neural fake news;generate real text;neural fake;classify neural fake;controllable text generation;text generation;real text;text generation called;news real human;text link vaccines;real human news;grover generate real;news real;link vaccines autism;human news;generate real;called grover generate;vaccines autism;controllable text;real text link;vaccines autism best;text;bias sampling;fake news real;bias sampling strategies;grover generate;generate;generation called grover;vaccines;model controllable text", "pdf_keywords": "predict fake news;neural fake news;news neural fake;news trustworthy generated;generated news trustworthy;generated false news;fake news generated;fake news encoded;dynamics fake news;generated news source;news accurate generated;false news neural;discriminator generated news;news neural;generating neural fake;generated news accurate;predict quality propaganda;information generated news;method generate propaganda;suggesting generated news;generating propaganda;trustworthy generated text;generating propaganda using;method generating propaganda;news articles generated;generated targeted propaganda;generated news;fake news articles;generate propaganda;fake news implications"}, "946e5e31b0779fc33550e8681994e7afd8d549a5": {"ta_keywords": "measurement motion patient;automated measurement motion;measurement motion;applied measurement motion;based measurement motion;measurement motion central;motion patient method;measurement time motion;target measurement motion;motion patient based;motion central patient;motion patient clinical;patient based measurement;motion patient;time motion patient;automated measurement;method automated measurement;motion;method based measurement;time motion;measurement time;method applied measurement;patient vicinity target;based measurement time;motion central;applied measurement;central patient vicinity;patient vicinity;target measurement;vicinity target measurement", "pdf_keywords": ""}, "81d4357afae9680e64a645cbb36aa090c3619b19": {"ta_keywords": "slowing particle;energy particle slowing;critical slowing;critical slowing slowing;particle slowing loss;particle slowing particle;particle slowing;description critical slowing;moving particle slowing;slowing particle loss;particle slowing caused;slowing loss energy;fast moving particle;loss energy particle;energy particle loss;slowing slowing slowing;slowing slowing fast;particle loss energy;slowing fast moving;slowing slowing;particle loss;slowing fast;slowing loss;slowing;slowing caused loss;moving particle;loss energy;loss energy loss;energy loss energy;energy loss", "pdf_keywords": ""}, "a7d6b5e61024127bf4fe8f04c0182a16ff97bccf": {"ta_keywords": "lobbying stochastic;lobbying stochastic environment;dynamics lobbying stochastic;lobbying issue weighting;lobbying issue;dynamics lobbying;depending given bribery;lobbying;model dynamics lobbying;bribery methods consider;bribery methods;given bribery;bribery;forms lobbying;criteria bribery;resulting forms lobbying;forms lobbying issue;criteria bribery methods;evaluation criteria bribery;influence voters preferences;influence voters;seeks influence voters;voters preferences voting;voters preferences;voting multiple issues;preferences voting;voting;voting multiple;preferences voting multiple;voters", "pdf_keywords": "complexity stochastic lobbying;stochastic lobbying problem;probabilistic lobbying stochastic;probabilistic lobbying problems;problem probabilistic lobbying;stochastic lobbying;lobbying stochastic;probabilistic lobbying;hard probabilistic lobbying;lobbying stochastic environment;lobbying problem presented;lobbying problem;efficient lobbying;efficient lobbying strategy;lobbying problems;model lobbying;effective efficient lobbying;lobbying problems issue;finding effective lobbying;lobbying strategy;model lobbying american;problem voter bribery;lobbying strategy winning;lobbying;lobbying american political;effective lobbying;extend model lobbying;effective lobbying strategy;lobbying strategy class;model lobbying bywe"}, "419e714f22c3fa2599abebd630cae5595c70bdef": {"ta_keywords": "speech recognition asr;linear chime training;speech input self;speech recognition;recognition asr model;automatic speech recognition;recognition asr;chime training protocol;enhanced speech input;input self supervised;automatic speech;speech input;learning representation iris;model enhanced speech;trained monaural chime;channel chime benchmark;chime training;chime benchmark proposed;single channel chime;end automatic speech;self supervised learning;chime benchmark;channel chime;linear chime;asr model enhanced;monaural chime task;self supervised;asr model;proposed model trained;chime task", "pdf_keywords": "kakuded speech recognition;speech recognition enhanced;recognition enhanced speech;enhanced speech input;end speech recognition;speech recognition asr;speech enhancement model;channel speech enhancement;speech recognition;speech datasets;speech enhancement;speech enhancement based;speech enhancement ssp;automatic speech recognition;robust speech recognition;approach speech enhancement;enhanced speech;scale speech recognition;speech datasets approach;speech recognition task;speech recognition proposed;modules speech enhancement;chime speech recognition;model robust speech;hybrid deep learning;scale speech datasets;automatic speech;speech input self;deep learning extension;scalable approach speech"}, "888c81cd3d1e953e2b7f8cc4ce68ca9f908c1e8d": {"ta_keywords": "differential privacy;using differential privacy;text representation learning;differential privacy dp;privacy dp simple;proposing text representation;text representation;privacy loss guarantees;privacy;privacy dp;privacy loss;representation learning;representation learning using;learning using differential;violates privacy loss;learning using;violates privacy;text;papers proposing text;representation;learning;proposing text;certainly violates privacy;implementation dp;dp simple general;implementation;recent papers;general empirical;analyze recent papers;formally analyze recent", "pdf_keywords": "differential privacy text;using differential privacy;differential privacy;differential privacy propose;differential privacy ii;differential privacy model;partial differential privacy;differential privacy following;applying differential privacy;differential privacy partial;preserved differential privacy;privacy partial differential;differential privacy dp;differential privacy public;concept differential privacy;differential privacy mechanism;guarantee differential privacy;implementation differential privacy;privacy text representation;deep datasets arbitrary;privacy dynamic data;differential privacy ps;differentially private;privacy dynamic;differentially private simple;learning using differential;generate deep datasets;deep datasets;privacy partial;networksampling distribution fundamental"}, "596b46dbe4fa8eee72e517ea9fd5f8ef83c9c64e": {"ta_keywords": "selection round;overview selection round;selection;round;overview;overview selection;brief overview selection;brief overview;present brief overview;brief;present;present brief", "pdf_keywords": "question answering;answer machine learning;qb question answering;question answering sequential;questions answering;task question answering;generating qb questions;answer probabilities selection;qb answer questions;question answering method;machine answer question;generated questions qb;simple question answering;decides answer qb;qb framework answering;question answering uses;answer machine;make qb questions;machine answers;qb answer strings;selection trivia game;predict answer question;learning machine answer;answering uses neural;trained question opponent;answering questions;machine answers question;questions answered answering;traditional questions answering;selection best answer"}, "650f2afca6d72d6b6e2e08849e1224f1e8b7900c": {"ta_keywords": "rating estimation graph;binary rating estimation;estimation graph information;rating estimation;quality graph information;graph information propose;graph information;graph information characterize;binary rating;rating matrix;recover rating matrix;problem binary rating;estimation graph;quality graph;rating matrix called;optimal sample complexity;function quality graph;graphs propose;real world graphs;information propose computationally;graphs;graphs propose novel;rating;sample complexity;graph;recover rating;sample complexity function;world graphs propose;efficient algorithm achieves;number observed entries", "pdf_keywords": ""}, "932404745d960291925b3f27b71734dff5b23633": {"ta_keywords": "impact disparity learning;disparity learning processes;group blind classifier;disparity learning;mitigating impact disparity;blind classifier;blind classifier applies;group blind;classifier applies policy;impact disparity;learning processes proposed;mitigating impact;classifier;classifier applies;strategy mitigating impact;learning processes;mitigating;employs group blind;learning;datasets highlight practical;strategy mitigating;protected subgroup differently;effective strategy mitigating;datasets empirical results;treating members protected;protected subgroup;datasets empirical;policy treating members;datasets;members protected subgroup", "pdf_keywords": ""}, "7bbd132f40c7630aeebf6379b00e307c3fff738c": {"ta_keywords": "simulation distributed data;distributed data sets;distributed data;distributed data storage;problem distributed data;simulation distributed;distributed algorithms method;distributed algorithms;use distributed algorithms;method simulation distributed;use distributed;distributed;data storage method;based use distributed;based simulation data;simulation data;simulation data using;data storage;data algorithm;data algorithm applied;problem distributed;storage method;applied problem distributed;data sets based;storage method applied;data sets;minimizing bandwidth data;method based simulation;initialize data algorithm;based simulation", "pdf_keywords": "proposed data regeneration;data regeneration;data collection regeneration;node regeneration code;data regeneration systematic;node equivalent regeneration;regenerating code optimal;data storage nodes;matrix systematic regeneration;algorithm applied regeneration;regeneration code arbitrary;node regeneration;construction regeneration matrix;bandwidth exact regeneration;regeneration algorithm;matrix systematic nodes;storage nodes;node regeneration model;systematic node regeneration;regeneration matrix systematic;regeneration systematic nodes;nodes regeneration;reconstruct entire data;minimizing repair bandwidth;regeneration systematic node;nodes data;node problem regeneration;systematic nodes regeneration;systematic regeneration algorithm;storage nodes distributed"}, "e8c3090e66fdb05a2c169a12c52dd94bb8786fb5": {"ta_keywords": "natural language explanations;language explanations proposed;generating explanations;language explanations word;language explanations;explanations word;methods generating explanations;explanations word develop;explanations proposed features;subreddit algorithmic architecture;explanations proposed;echoing word explanation;develop subreddit algorithmic;subreddit algorithmic;word explanation;word explanation enhance;word develop subreddit;providing natural language;natural language;explanation enhance neural;explanations;set natural language;architecture allows explain;neural methods generating;predictive;algorithmic;develop subreddit;predictive power echoing;neural;algorithmic architecture allows", "pdf_keywords": "explanations argument persuasive;natural language explanations;argument persuasive develop;explanations context persuasion;persuasive comment similar;predicting used persuasive;argument persuasive;persuasive comment propose;post persuasive comment;explaining natural language;used persuasive comment;persuasive comment;natural language subreddit;language explanations;language explanations essential;view natural language;generating explanationswe;persuasive develop features;natural language explanation;persuasive comment original;performance generating explanationswe;generating explanationswe novel;explanation used persuasive;context persuasion using;predicting word used;arguments context persuasion;context persuasion assemble;naturally occurring explanations;post persuasive;natural language"}, "ed535e93d5b5a8b689e861e9c6083a806d1535c2": {"ta_keywords": "systematic generalization transformers;generalization transformers;embedding universal transformers;generalization elementary models;models devil tricks;positional embedding universal;tricks revisiting model;elementary models;models devil;universal transformers variant;elementary models devil;accuracy systematic generalization;revisiting model configurations;embeddings early stopping;systematic generalization elementary;embedding universal;universal transformers;models;positional embedding drastically;systematic generalization;transformers variant;basic scaling embeddings;model configurations;scaling embeddings early;transformers variant demonstrate;model configurations basic;embeddings early;positional embedding;generalization elementary;scaling embeddings", "pdf_keywords": "systematic generalization neural;trained systematic generalization;generalizing neural network;systematic generalization transformers;generalizing neural;method generalizing neural;generalization neural;generalization pretrained neural;generalization neural networks;generalization pretrained;accuracy generalization models;accuracy systematic generalization;baseline transformers datasets;generalization transformers revisiting;trained model transformer;tuning accuracy generalization;embeddings significantly improve;networks trained systematic;systematic generalization tasks;accuracy models generalization;compositional generalization pretrained;embedding improve accuracy;generalization transformers;improvements systematic generalization;accuracy generalization;embedding significantly improve;neural networks trained;neural networks stage;embeddings demonstrate accuracy;improve accuracy generalized"}, "9abb50813e05de849dbbd89535bc7d0206f5e36a": {"ta_keywords": "reconstruction verb data;verb data sample;verb data method;verb data;sample verb classes;generate sample verb;partial reconstruction verb;verb classes method;reconstruction verb;verb classes;sample verb;empirical estimation;empirical estimation maximum;maximum likelihood estimation;verb;problem partial reconstruction;data sample;estimation maximum likelihood;partial reconstruction;based empirical estimation;likelihood estimation;data used generate;likelihood estimation method;likelihood estimator data;predictions maximum likelihood;method based empirical;maximum likelihood;data method;generate sample;data method based", "pdf_keywords": ""}, "0bbfa6ab7451aea5cbb842cce97b54500bafdfc7": {"ta_keywords": "preference learning consider;preference learning;optimal decision maker;identifying optimal decision;unknown optimal decision;preferences unknown decision;optimal decision;human preferences unknown;decision maker unknown;decision maker popular;framework preference learning;unknown decision maker;human preferences;decision maker;decision maker identified;case human preferences;identifying optimal;decisions human making;decisions human;number decisions human;problem identifying optimal;maker unknown optimal;preference;unknown optimal;unknown decision;identified number decisions;learning consider case;learning consider;preferences unknown;decisions", "pdf_keywords": "inverse decision theory;consider inverse decision;inverse decision;study inverse decision;setting inverse decision;inverse decision problem;properties inverse decision;binary decisions uncertainty;parameter inverse decision;inverse reinforcement learning;investigates optimal decision;suboptimal decision maker;decision maker suboptimal;decisions optimal decision;decision theory;suboptimal decision;binary decisions;decisions suboptimal sense;observed decisions optimal;optimal decision;decisions estimation;decisions uncertainty idt;decisions suboptimal;uncertainty decisionsreinforcement learning;preferences uncertainty;decision theory idt;existence suboptimal decision;preference learning decision;existence optimal decision;preferences uncertainty case"}, "a31ab366b0a349ee5f341f1179810bc9805d32a4": {"ta_keywords": "secure regenerating codes;regenerating codes security;regenerating codes operating;regenerating codes;secure regenerating;repair regenerating codes;codes security;matrix construction secure;minimum storage regenerating;security storage;data file securely;security storage data;codes security exact;file securely stored;codes operating minimum;investigated security storage;storage regenerating;securely stored;construction secure regenerating;file securely;operating minimum storage;secure upper bound;storage data achieved;codes operating;securely stored holds;stored holds l2;security exact repair;storage data;minimum storage;storage regenerating msr", "pdf_keywords": ""}, "04d18fc81cc232b3d3dece0994c0fa8aaabaf4b7": {"ta_keywords": "domain adaptation;effective domain adaptation;domain adaptation strategy;domain adaptation minimum;cost domain adaptation;predicting shape domain;tagging prediction;adaptation minimum annotations;word segmentation;word segmentation speech;tagging prediction results;process word segmentation;speech ssp tagging;shape domain;domain proposed method;segmentation speech;domain proposed;shape domain proposed;ssp tagging prediction;tagging;estimate domain;estimate domain proposed;segmentation speech ssp;domain;annotations using;achieves effective domain;effective domain;annotations;minimum annotations using;minimum annotations", "pdf_keywords": ""}, "d5dcbb144a2be999610b4838d94cc3fb228f837c": {"ta_keywords": "network slicing 5g;slicing 5g networks;slicing 5g;network slicing;deployment network slicing;5g networks model;5g networks;5g network;real time 5g;5g;deployment network;5g network results;time 5g network;model deployment network;time 5g;slicing;deployment real time;networks model based;networks model;network;thelatency backup configurations;thelatency backup;reinforcement learning approach;reduce deployment costs;reinforcement learning;deployment real;reduce deployment;networks;based reinforcement learning;model reduce deployment", "pdf_keywords": ""}, "df689bdc6c497949e9ab3b7ba19950d9fade7180": {"ta_keywords": "spin orbit interaction;orbit interaction spin;interaction spin orbit;orbit coupled spin;coupled spin orbit;spin orbit coupled;dynamics spin orbit;enhance spin orbit;enhancement spin orbit;effect spin orbit;interaction dynamics spin;spin orbit;suppression spin orbit;coupled spin;interaction spin;dynamics spin;orbit interaction dynamics;orbit interaction;orbit interaction responsible;orbit interaction used;enhance spin;effect spin;enhancement spin;suppression spin;orbit coupled;responsible enhancement spin;responsible suppression spin;spin;used enhance spin;study effect spin", "pdf_keywords": ""}, "d7fe9b46f96ae9df7fa64e1c575c7114e5ef0aaa": {"ta_keywords": "accelerated tensor method;new tensor method;tensor method;tensor methods;analysis tensor methods;tensor methods proposed;tensor method solving;accelerated tensor;complexity analysis tensor;known accelerated tensor;tensor method classes;analysis tensor;uniformly convex optimization;propose new tensor;tensor;new tensor;convex optimization;convex optimization problems;solving uniformly convex;optimization problems introduce;optimization problems objective;proposed method accelerated;optimization;method accelerated;optimization problems;uniformly convex;method accelerated additional;method classes optimization;problems objective function;convex", "pdf_keywords": "accelerated tensor method;accelerated tensor methods;fast convergence tensor;convex optimization;theorem optimization partial;propose accelerated tensor;convergence tensor methods;tensor method proposed;new tensor method;derivative convex objective;unconstrained convex optimization;certain optimization;based accelerated tensor;optimization partial;iterations certain optimization;tensor methods present;convex optimization problem;tensor methods;tensor method;theorem optimization;accelerated tensor;unconstrained convex;tensor methods method;valid iterations optimization;optimization partial derivatives;optimization problem theorems;tensor method finding;method optimallyfresco;type theorem optimization;tensor method closes"}, "4c6f7fb5c2e1bd12899c3ec2788f9ce7eb2f8a5c": {"ta_keywords": "speech tagging dependency;tagging dependency parsing;nlu speech tagging;paraphrase identification models;speech tagging;parsing paraphrase identification;dependency parsing paraphrase;encode syntactic knowledge;data encode syntactic;dependency parsing;models pretrained data;programming model nlu;tagging dependency;pretrained data;paraphrase identification;pretraining data;syntactic knowledge;impact pretraining data;encode syntactic;pretrained data encode;parsing paraphrase;model nlu speech;parsing;identification models pretrained;models pretrained;pretraining data size;tagging;syntactic;nlu speech;known nonlinear programming", "pdf_keywords": "language models downstream;syntactic generalization performance;data models pretrained;models pretrained data;structural probing syntactic;models pretraining models;bidirectional linguistic models;probing syntactic evaluation;models pretraining;models pretrained;analysis pretraining models;language models;pretraining models;source pretraining models;pretrained data;pretraining models beneficial;deep bidirectional linguistic;performance different syntactic;applications pretraining models;pretraining data;pretraining models pretraining;size syntactic generalization;syntactic structures using;syntactic knowledge perform;characterization syntactic structures;encode syntactic knowledge;structural syntactic;syntactic structures;pretrained data training;different language models"}, "b73191adcc938cfcf20ce0327cf5cd1f539f7f81": {"ta_keywords": "neural tagging;semi supervised learning;novel semi supervised;based neural tagging;semi supervised;neural tagging apply;tagging;supervised learning;supervised;supervised learning method;unannotated articles;unannotated articles obtain;set unannotated articles;tagging apply;unannotated;learning method;data set unannotated;2017 semeval task;tagging apply method;novel semi;neural;semi;introduce novel semi;set unannotated;learning method based;learning;semeval task 10;semeval task;method based neural;task 10", "pdf_keywords": "neural tagging;neural tagging model;entity recognition annotated;named entity recognition;efficiently classify keywords;entity recognition;task extracting keyphrases;classify keywords;classify keywords scientific;keyphrases scientific articles;extracting keyphrases scientific;semisupervision tag words;methods neural tagging;sequence tagging;similarity semi supervised;introduce semi supervised;semi supervised learning;supervised key classification;extracting keyphrases;unannotated articles;keywords scientific articles;characterizing keyphrases;classify semi supervised;semi supervised;keyphrases scientific;unannotated articles including;based semi supervised;sequence tagging introduce;tagging introduce semi;problem sequence tagging"}, "06d77cc8970b59102a0caffb5e4c5b7a3242563a": {"ta_keywords": "energy particle quantum;particle quantum field;calculation energy particle;quantum field model;function energy particle;quantum field;particles energy particle;energy particle;particle quantum;wave function energy;quantum field determined;particles energy;energy wave function;field determined energy;number particles energy;schrdinger equation;quantum;function energy;determined energy wave;schrdinger equation solved;calculation energy;particle;model calculation energy;energy wave;solution schrdinger equation;determined energy;wave function;schrdinger;large number particles;particles", "pdf_keywords": ""}, "4715ee17ca4f52762fdf67c9a8ef8fb751c88484": {"ta_keywords": "appliances arxes measurements;consumption electrical appliances;formulation rank minimization;electrical appliances arxes;rank minimization;rank minimization problem;identification arxes models;convex formulation rank;electrical appliances;identify power consumption;appliances arxes;consumption electrical;power consumption electrical;method identify power;appliances;method based identification;arxes models piecewise;corresponding arxes models;convex formulation;relaxed convex formulation;based identification arxes;minimization problem proposed;minimization;formulation rank;minimization problem;arxes measurements method;method relaxed convex;arxes models;measurements method based;power consumption", "pdf_keywords": "model blind deconvolution;blind deconvolution method;deconvolution method solution;estimating impulse response;deconvolution method;using blind deconvolution;blind deconvolution;channel utility approach;deconvolution;recovery sparse signal;convolution model blind;identification multiple output;channel using convolution;noisy channel utility;estimating impulse;identifying noisy channel;model output measurements;convolution model;channel utility;noisy channel algorithm;deconvolution method andwe;described using convolution;method estimation quadratic;regressive exogenous input;output systems;estimation quadratic estimators;sparse signal representations;problem estimating impulse;input arxiv model;arxiv model output"}, "cac008e541af58f738407c7f2ee86d547053188f": {"ta_keywords": "heisenberg antiferromagnet spin;spin heisenberg antiferromagnet;antiferromagnet spin orbit;antiferromagnet spin;heisenberg antiferromagnet;zeeman field spin;coupling spin heisenberg;spin orbit coupling;coupling spin;orbit coupling spin;antiferromagnet;spin heisenberg;effect zeeman field;field spin orbit;field spin;spin orbit;affected field spin;determined spin orbit;effect zeeman;zeeman field;orbit coupling strongly;determined spin;strength determined spin;spin;zeeman;orbit coupling strength;coupling strength xmath0;orbit coupling;coupling strongly affected;coupling strongly", "pdf_keywords": ""}, "1e2ef0c9a494c7949f38940ee735a88c56355202": {"ta_keywords": "process help sensors;sensors proposed algorithms;sensors;characterization process help;sensors proposed;characterization process;method characterization process;process high accuracy;help sensors;characterize process;able characterize process;characterize process high;help sensors proposed;novel method characterization;process;method characterization;process help;algorithms able characterize;algorithms;proposed algorithms;high accuracy possible;proposed methods algorithms;proposed algorithms able;algorithms able;methods algorithms able;accuracy possible;methods algorithms;high accuracy;accuracy;accuracy possible thanks", "pdf_keywords": ""}, "00c3a86551f1bc812b676025210e295021853f66": {"ta_keywords": "rank big bang;big bang important;big bang big;big bang;make big bang;bang important facts;big pacs numbers;bang big pacs;questions historical figures;volume questions historical;questions historical;bang big;numbers rank big;pacs numbers 03;entertaining collection questions;pacs numbers;bang important;facts make big;big pacs;numbers 03;numbers rank;questions;bang;amusing entertaining collection;rank big;numbers 03 65;numbers;important facts make;answered authors volume;authors volume questions", "pdf_keywords": ""}, "69d5579955a5a8859d78a70b3d1afede0f91fa09": {"ta_keywords": "perform energy disaggregation;energy disaggregation;energy disaggregation separating;disaggregation separating energy;separating energy data;device energy data;building energy data;individual device energy;energy data building;energy data individual;energy data;observed power consumption;power consumed device;data individual appliances;device energy;device task disaggregation;power consumption;data building energy;power consumed;individual appliances;disaggregation separating;building energy;task disaggregation;output power consumed;disaggregation;separating energy;generate observed power;individual appliances model;energy;perform energy", "pdf_keywords": "disaggregation power consumption;energy disaggregation task;perform energy disaggregation;energy disaggregation;power consumption data;energy data collection;building energy data;aggregate energy data;disaggregation power;energy data;represent power consumption;disaggregate data;energy data individual;disaggregated data;energy data building;obtain disaggregated data;power consumption dynamics;disaggregate data data;disaggregated data set;power consumed device;disaggregating data;problem disaggregation power;disaggregating data data;able disaggregate data;measurements power consumption;data individual appliances;power consumption individual;disaggregation results powerwe;power consumption;disaggregation task"}, "834d68b9befcc6c68415b460b33435a1822799fb": {"ta_keywords": "argumentation mining;argumentation mining context;argumentation mining user;problem argumentation mining;web discourse analyze;method argumentation mining;license argumentation mining;generated web discourse;web discourse feasible;analyze problem argumentation;discourse analyze;free license argumentation;web discourse;argumentation;discourse analyze problem;discourse feasible;discourse feasible challenging;license argumentation;problem argumentation;standard corpus oriented;novel method argumentation;discourse;corpus oriented corpus;corpus oriented;gold standard corpus;mining context new;oriented corpus;oriented corpus oriented;mining context;standard corpus", "pdf_keywords": "content argumentation mining;argumentation mining introduces;annotated argumentation mining;argumentation mining;argumentation mining based;argumentation web analyze;argumentation mining user;presents argumentation mining;argumentation mining reached;argumentation mining literature;challenges argumentation mining;addressed argumentation mining;argumentation web;argumentation mining field;modeling argumentation web;argumentation mining ways;introduction argumentation mining;overview argumentation mining;users aware argumentation;examples argumentation mining;argumentative web;approach argumentation mining;documents analyze argumentation;argumentation discussions online;argumentation internet;argumentation content;generated argumentation content;research argumentation mining;argumentation highly informative;mining literature argumentation"}, "972a74968d2522908b06c5bd1e26266194c5a9ee": {"ta_keywords": "automatically decontextualize sentences;decontextualize sentences;sentence automatically decontextualize;decontextualization sentences;use decontextualization sentences;sentences use decontextualization;decontextualization sentences define;decontextualize sentences means;user oriented decontextualization;automatically decontextualize;context sentence automatically;decontextualization task;decontextualization task aims;use decontextualization;sentence proposed framework;decontextualization;decontextualize;means understanding sentences;sentence automatically;oriented decontextualization task;sentences means models;sentences define;user facing context;sentences means;understanding sentences use;understanding sentences;new context context;process text;context context;sentences", "pdf_keywords": "question answering decontextualization;decontextualization sentences decontextualization;sentences decontextualization;sentences decontextualization useful;decontextualizing sentences;decontextualized sentence decontextualization;decontextualization sentences;decontextualization extract sentences;sentences easily decontextualized;decontextualize sentences;decontextualizing sentences relies;decontextualized sentences;sentence decontextualization;decontextualization sentence process;sentences decontextualized;decontextualize sentences present;automatically decontextualize sentences;sentence decontextualization taking;method decontextualizing sentences;using decontextualized sentences;uncontextualized sentences decontextualized;decontextualization phenomenon sentences;annotation based decontextualization;answering decontextualization;lexical tasks decontextualization;decontextualized sentence train"}, "6b387d18bae978202af501c4795f37a0c73781a6": {"ta_keywords": "proximal extragradient method;extragradient method;hybrid proximal extragradient;extragradient method step;proximal extragradient;numerical convex optimization;smooth convex optimization;convex optimization problems;hessian optimized;gradient hessian optimized;recent tensor method;tensor method proposed;convex optimization;tensor method;hessian optimized function;convex optimization problem;function tensor method;methods using gradient;tensor method author;extragradient;optimal smooth convex;optimized function tensor;numerical convex;problem hybrid proximal;optimization problem hybrid;approximate solution auxiliary;gradient hessian;considers numerical convex;optimization problems class;hessian", "pdf_keywords": ""}, "2bb1e1a5b9a16f6828fe94736cea5dab264533a6": {"ta_keywords": "ungrounded language models;assertions language models;language models acquire;underlying semantics;ability ungrounded language;assertions language;ways assertions language;understand underlying semantics;semantics;language models;ungrounded language;language models appear;formalize ways assertions;models acquire meaning;formalize;meaning formalize ways;models appear fundamentally;ways assertions;assertions;formalize ways;meaning formalize;ability understand underlying;language;acquire meaning formalize;investigate ability ungrounded;ability ungrounded;models;formally investigate ability;formally;understand underlying", "pdf_keywords": "underlying semantics assertions;assertions natural language;semantics assertions;formalize assertions semantic;semantics assertions enable;emulate underlying semantics;language assertions;notion emulation assertions;assertions enable semantic;emulate semantics;emulate semantics natural;denotation based assertions;assertions semantic;learning language assertions;learn language assertions;assertions textual contexts;emulation assertions;formalize assertions;semantic emulation;assertions semantic relations;emulation assertions using;assertions given language;assertions formalize;assertions used emulate;semantic emulation languages;language semantics;notion semantic transparency;language assertions given;formalize notion emulation;assertions formalize assertions"}, "96b32b204a62777bef66eea595de2c47b4e9d6e9": {"ta_keywords": "learning representation domain;domain dependent data;data domain dependent;domain data set;method learning representation;domain data;learning representation;representation domain;data domain;dependent data sets;data sets method;set data domain;projection model representation;domain dependent;model representation method;method learning;data sets;model representation;original domain data;domain dependent performance;reconstruct original domain;representation domain small;data set data;data set;number domain dependent;battery domain dependent;battery domain;reverse gradient method;gradient method subspace;representation method", "pdf_keywords": "learning domain representation;data neural;deep neural;objects using neural;learning domain;neural network structured;train neural;data neural building;data set neural;textural representation learn;deep neural network;neural network;using neural;neural network differentiable;train neural network;representation learn textural;neural;using neural network;learn textural representation;predict textural information;information data neural;synthetic datasets;domain classification;synthetic datasets able;textural information synthetic;predict semantic labels;neural network predict;train method neural;type deep neural;predict semantic"}, "636611068825cb4b7bdab6ad16ef415adf4fb96c": {"ta_keywords": "existing multidomain learning;multidomain learning approaches;multidomain learning;biases multidomain methods;multidomain methods improving;class biases multidomain;multidomain methods;biases multidomain;approaches performance multidomain;analysis existing multidomain;multidomain;performance multidomain;multidomain setting discuss;existing multidomain;domain specific class;multidomain setting;performance multidomain setting;domains;domains domain specific;domain specific;questions domains;learning approaches;domains domain;questions domains domain;domain;capture domain specific;respect questions domains;improving capture domain;learning approaches respect;capture domain", "pdf_keywords": ""}, "6f902b8128b218563b276c1ebff46ef668dcb185": {"ta_keywords": "mechanism incentivize agents;engage arbitrary collusions;incentivize agents engage;arbitrary collusions homogeneous;arbitrary collusions;incentivize agents;propose incentive mechanism;incentive mechanism incentivize;incentive mechanism;collusions homogeneous;mechanism incentivize;collusions homogeneous setting;agents engage arbitrary;propose incentive;agents noncolluding mechanism;collusions;incentive;agents noncolluding;mechanism optimal;agents preferences priori;optimal mechanism;engage arbitrary;incentivize;mechanism optimal mechanism;preferences priori agents;agents engage;agents preferences;priori agents noncolluding;knows agents preferences;mechanism knows agents", "pdf_keywords": "maximized crowdsourcer agents;tasks maximized crowdsourcer;maximized crowdsourcer;crowdsourcer agents;crowdsourcer achieve maximizes;crowd sensing heterogeneous;incentive mechanism crowd;optimal strategy crowdsourcer;mechanism crowd sensing;crowdsourcer agents report;dynamics crowdsourcer;incentive mechanism agents;strategy crowdsourcer;crowd sensing game;crowdsourcer aims optimize;crowdsourcer achieve;crowdsensing systems;strategy crowdsourcer achieve;insensetive mechanism crowd;crowd sensing;crowdsourcer;crowd sensing problem;dynamics crowdsourcer aims;encourage agents participate;crowdsensing systems proposed;crowdsourcer achieve hypothetical;sensing heterogeneous agents;incentive prevents agents;mechanism incentive based;agents participate data"}, "7650d705b85dc399112a5b6a79e9c6f81c7c6146": {"ta_keywords": "discrete optimization;discrete optimization problems;solving discrete optimization;optimization problems large;automatically constructing;constructing semi efficiently;semi efficiently solving;automatically constructing semi;neural network cloze;optimization problems;specific corpora approach;efficiently solving discrete;optimization;propose automatically constructing;corpora approach;corpora approach based;specific corpora;semi efficiently;domain specific corpora;diverse datasets;efficiently solving;corpora;evaluate diverse datasets;large domain specific;constructing semi;datasets;large domain;automatically;powerful neural network;training powerful neural", "pdf_keywords": "semi supervised qa;comprehension supervised;reading comprehension supervised;extractive questions answer;large scale supervised;predict learned questions;comprehension supervised learning;triples cloze corpus;extractive questions;cloze corpus best;cloze corpus;specific annotated corpora;annotated corpora;propose semi supervised;reading comprehension scalable;learned questions model;annotated corpora limited;supervised qa;supervised learning large;semi supervised;learning large scale;beneficial semi supervised;semi supervised learning;ly semi supervised;semi supervised simple;supervised learning tasks;learned questions variety;learning large;comprehension scalable accurate;learned questions outperform"}, "58834a447c749758e7f57498c6dd88a281af41a0": {"ta_keywords": "training constituency parsers;oracle training parsers;training parsers;constituency parsers;constituency parsers approach;constituency parsers utility;subset constituency parsers;constituency parsers fixed;parsers;training parsers define;parsers languages;demonstrated analyzing parsers;parsers define dynamic;analyzing parsers;parsers approach;analyzing parsers languages;parsers approach based;oracle training constituency;parsers utility approach;parsers utility;parsers define;dynamic oracle training;parsers fixed;parsers fixed subset;novel dynamic oracle;static oracle training;oracle training;dynamic oracle;training constituency;alternative static oracle", "pdf_keywords": "training parsers;training parser;parser training;based parsers trained;training parsers using;training constituency parsers;parsers trained;parser training methods;transition based parsers;train constituent parser;parsers exploration;present parser training;training parser use;constituency parsers exploration;effective supervising parser;parsers trained english;parser training method;dynamic oracle parser;constituent parser improved;method training parsers;training class parsers;novel constituent parser;methods parser training;supervising parser;parsers exploration method;guide training parser;language parsers;based parsers model;parsers policy gradient;based parsers obtain"}, "1fa32503bce4f01ab2ccb65dedd374310c488fe8": {"ta_keywords": "partial compliance incentive;compliance incentive effects;compliance incentive;market partial compliance;partial compliance based;partial compliance;achieve partial compliance;partial compliance level;compliance based;effects fair employers;compliance;used partial compliance;compliance based local;compliance level;incentive effects fair;compliance level far;employment market partial;compliance partners;fair employers;regulatory frameworks;fair employers achieve;employment market;design regulatory frameworks;regulatory frameworks discussed;model employment market;employers achieve partial;proportional number compliance;compliance partners global;number compliance partners;number compliance", "pdf_keywords": "fair machine learning;fairness compliant employers;discrimination partial compliance;employers able fair;fair employers;compliance regulation hiring;partial compliance employers;employers partial compliance;compliance occurring employment;fairness compliant;compliance employers;compliance employers spread;regulation hiring;severe fair employers;fairness role regulation;algorithmic fairness;bias fairness;bias fairness potential;partial fairness compliant;fair employers match;non compliance employers;applications fair policies;compliance compliant employers;market partial fairness;algorithmic fairness role;policy compliant employers;present fairness framework;compliant employers adopt;algorithms claim fairness;fairness framework fair"}, "6ea353ada2b89763f58d8068a74b2e6def526948": {"ta_keywords": "extracted biomedical text;biomedical texts extracted;biomedical text corpus;extracted corpus;extracted corpus used;texts extracted corpus;corpus pharmacological substances;corpus pharmacological;texts extracted;information extraction;information extraction techniques;text corpus;novel corpus pharmacological;ddis biomedical texts;biomedical text;pharmacological substances detection;interactions ddis extracted;extracted biomedical;biomedical texts;evaluation information extraction;text corpus used;ddis extracted biomedical;corpus;drug interactions ddis;extraction techniques applied;pharmacological substances drug;extraction techniques;extraction;drug interactions;pharmacological substances", "pdf_keywords": "corpus annotated pharmacological;annotated text drugbank;corpus based annotated;annotated corpus;corpus annotated;presents annotated corpus;protein interaction text;annotated corpora pharmacological;annotated corpus paper;interactions biomedical text;interaction protein text;annotated corpora;annotation schema drug;descripion induction corpus;easy use corpus;interactions proposed corpus;proposed corpus;text drugbank database;processing nlp;natural language processing;annotated pharmacological substances;corpus based;induction corpus annotated;use corpus;annotated text;protein text;corpus;gold standard corpus;based annotated text;corpus developed descripion"}, "c507ad8b7bec5d29da7cf0ee92e2bf4361a5c92f": {"ta_keywords": "deep quantization neural;deep reinforcement learning;quantization neural networks;deep quantization;quantization neural;deep reinforcement;problem deep quantization;end deep reinforcement;policy optimization;reinforcement learning;discovering quantization levels;discovering quantization;quantization;quantization levels end;policy optimization methods;quantization levels;quantization focus;reinforcement learning framework;adapt policy optimization;problem quantization;network architecture training;problem quantization focus;learning framework releq;neural networks automating;action spaces network;state action spaces;reinforcement;process discovering quantization;choosing state action;quantization focus finding", "pdf_keywords": ""}, "2899eb53cddf050e3a34f07bbc0bc0ee7907d5d0": {"ta_keywords": "tagging problem japanese;additions word segmentation;speech tagging;speech tagging problem;problem speech tagging;language resource additions;additions training corpus;word segmentation;corpus efficient added;efficient added language;word segmentation problem;segmentation problem speech;training corpus efficient;training corpus;adding language resource;added language resource;adding language;corpus efficient;tagging;problem japanese;language resource;resource additions training;added language;resource additions word;tagging problem;effect adding language;corpus;segmentation;japanese;additions word", "pdf_keywords": ""}, "626f8a50a7bd24d869f25bddb6fbaa59b090268c": {"ta_keywords": "pattern recognition device;pattern recognition;signal scene method;recognition device;constructing pattern recognition;signal scene;detect;recognition device able;recognition;signal detected;detect presence signal;device able detect;presence signal scene;scene method based;able detect presence;signal;signal detected previously;presence signal;detect presence;scene method;level systems connected;constructing pattern;able detect;idea signal;idea signal detected;detected method;level systems;method constructing pattern;detected previously detected;detected method applied", "pdf_keywords": ""}, "3681456f29398e42cc2baafb0b72d166070a3cf1": {"ta_keywords": "leader linear game;dynamic leader linear;updating dynamic leader;dynamic leader;sequential updating strategy;game based sequential;sequential game;updating strategy;updating strategy strategy;linear game based;linear game;standard sequential game;form linear game;leader linear;linear games;linear games form;based sequential updating;applied linear games;sequential game xmath1;linear game xmath0;games form linear;strategy strategy based;sequential updating;strategy based;strategy strategy;based sequential;policy gradient based;strategy;policy gradient;use policy gradient", "pdf_keywords": "quadratic dynamic games;linear quadratic games;quadratic control existence;quadratic quadratic control;quadratic control;linearly stable quadratic;linear quadratic dynamic;quadratic games;quadratic games issues;optimal control existence;convergence dynamic games;linear convergence game;sequential game theorem;theorems optimal control;optimal control;strategies linearly driven;gradient game linear;quadratic regulator strategy;theorem optimal control;control strategy equivalent;leader follower algorithm;updates linear quadratic;quadratic dynamic;sequential algorithms linear;stabilization lq games;linear policies;stable stabilizability linear;problem optimal control;sequential game;sum strategies linearly"}, "a711e02f85fa52c15df0a830a8ba88df2c3928ec": {"ta_keywords": "calculation spectral density;spectral density single;particle wave function;spectral density;single particle wave;calculation spectral;method calculation spectral;density single particle;ab initio hamiltonian;particle wave;wave function;wave function based;single particle;initio hamiltonian combination;hamiltonian initio;initio hamiltonian;combination initio hamiltonian;hamiltonian combination initio;hamiltonian initio hamiltonian;spectral;ab initio method;initio hamiltonian initio;density single;hamiltonian;hamiltonian combination;particle;use ab initio;initio method method;wave;density", "pdf_keywords": ""}, "4c67c129dab9805ab248407b77a6d542c2e40d41": {"ta_keywords": "reconstructing rank matrix;reconstructing rank;problem reconstructing rank;rank matrix revealed;matrix revealed subset;original rank matrix;rank matrix proposed;optimal set revealed;matrix proposed algorithm;rank matrix;revealed subset entries;recover original rank;matrix revealed;alternating minimization;alternating minimization extreme;algorithm optimal set;subset entries revealed;set revealed entries;combining alternating minimization;consider problem reconstructing;revealed entries given;revealed entries;minimization extreme;extreme value filtering;renyi random graph;revealed subset;entries revealed entries;matrix proposed;algorithm optimal;reconstructing", "pdf_keywords": "rank matrix completion;rankone matrix completion;matrix completion problem;reconstructing rank matrix;matrix completion problems;matrix completion;corruptions rank matrix;crowdsourcing adversarial corruptions;rank matrix completionwe;matrix completion accurate;rank matrix reconstructed;completion problem crowdsourcing;adversarial corruptions rank;robust rank matrixwe;reconstructing rank;crowdsourcing problem recovering;robust rank;large rank matrix;problem reconstructing rank;problem crowdsourcing adversarial;assignment problem robust;rank matrix revealed;approaches rank matrix;rank matrix factorization;algorithm rank matrix;matrix completionwe consider;rank matrix particular;crowdsourcing task assignment;optimal bounds rankone;matrix completionwe"}, "840fabc2a7773e1bd771f152f76210b2ea5845b9": {"ta_keywords": "stochastic beam search;conditional stochastic beam;stochastic beam;beam search;conditional stochastic sampling;efficient conditional stochastic;probabilistic conditional stochastic;items noisy beam;conditional stochastic;beam search pmsn;beam search sst;conditional conditional stochastic;noisy beam proposed;stochastic sampling;noisy beam;stochastic sampling underlying;sampling underlying stochastic;stochastic;underlying stochastic;beam proposed method;novel probabilistic conditional;probabilistic conditional;best items noisy;beam;identifying best items;beam proposed;optimally identify best;optimally identifying best;identify best items;stochastic process", "pdf_keywords": "stochastic beam search;beam search stochastic;perform stochastic beam;conditional stochastic beam;stochastic beam;search stochastic;beam search strategy;beam search strategies;present stochastic beam;beam search samples;beam search;given beam search;search stochastic process;turning beam search;beam search method;underlying distribution seek;distribution seek noisy;beam search node;perform stochastic;construction stochastic;random distribution network;distribution seek;range beam search;beam search strategieswe;network perform stochastic;beams given algorithm;beam search produces;seek noisy element;efficiently sampling;random walk algorithm"}, "509b42fc150a057a64c4608f64e779ef04fdff47": {"ta_keywords": "entity recognition empirically;tweets annotated named;english tweets annotated;named entity recognition;tweets annotated;entity recognition;named entities empirically;annotated named entities;temporal information documents;use temporally diverse;english tweets;entities empirically;set english tweets;temporally diverse;annotated named;temporal information introduce;diverse training data;temporally diverse training;tweets;named entities;better use temporally;temporal information;performance temporal information;entities empirically demonstrate;temporal drift performance;named entity;annotated;use temporally;temporal drift;entities", "pdf_keywords": ""}, "11c6d0851152b6bec34726be40d90bea8d8a90f0": {"ta_keywords": "annotation noise common;based annotator expertise;difficulty annotator expertise;annotator expertise perinstance;annotation noise;confusions based annotator;novel crowdsourcing model;annotator expertise;crowdsourcing;crowdsourcing model;expertise perinstance annotator;crowdsourcing model modeling;propose novel crowdsourcing;modeling common confusions;novel crowdsourcing;difficulty annotator;instance difficulty annotator;decompose annotation noise;based annotator;annotation;common confusions based;annotator;common noise adaptation;expertise perinstance;perinstance annotator;annotator basis experiments;confusion based instance;noise adaptation;noise adaptation solution;decompose annotation", "pdf_keywords": "modeling annotation noise;crowds noisy annotations;noisy annotations model;classify noisy annotations;predict noisy annotations;annotators crowdsourced;annotation noise annotation;noisy annotations based;crowds annotated annotators;noisy annotations;annotation noise common;annotators crowd sourced;generate annotators crowdsourced;noisy annotation;annotation noise;learning crowds annotated;noisy annotations realize;annotators crowdsourced data;quality annotations learning;annotations annotators crowd;model noisy annotations;annotations learning;model noisy annotation;noisy annotation process;annotations learner learning;noise annotation;confusion annotators model;crowdsourced data annotators;annotators model noisy;noise annotation use"}, "6c520d983923dbe1e437c01086424fdcdd8f430a": {"ta_keywords": "parametric speech synthesis;statistical parametric speech;speech synthesis;speech synthesis including;speech synthesis process;parametric speech;speech parameter;speech parameter parameter;trajectories speech parameter;based trajectories speech;freedom statistical parametric;trajectories speech;post filters modify;quality statistical parametric;proposed post filters;statistical parametric;post filters;post filters applicable;speech;post filters based;filters modify;freedom statistical;synthesis process proposed;synthesis process;filters applicable;filters based;synthesis;synthesis including;filters;high quality statistical", "pdf_keywords": ""}, "5f46d8e18138fe572b6fae897475a2ad645a3e1a": {"ta_keywords": "neurons depthless representation;neurons depthless;depthless network neurons;depthless representation biological;ratio neurons depthless;network neurons described;biological process depthless;neurons described;neurons described proposed;depthless representation;depthless representation proposed;network neurons;combines depthless representation;neurons;structure biological tissues;density ratio neurons;tissues using representation;representation underlying network;neuron;ratio neurons;representation biological process;representation underlying biological;single neuron;characterizing structure biological;process depthless network;depthless network;biological process representation;case single neuron;process depthless;structure biological", "pdf_keywords": "speech recognition deep;combines deep learning;model automatic speech;hybrid neural networks;deep learning decode;deep learning models;extension deep learning;deep learning deep;recognition deep neural;speech recognition incorporates;deep learning;deep network;deep neural network;deep neural;context speech recognition;automatic speech;speech recognition model;train deep neural;learning deep;learning deep network;speech recognition;andtranscript generalizations deep;deep learning process;automatic speech recognition;recognition deep;combination deep learning;deep network representations;voice search;neural networks nnn;speech data"}, "2ce3428ba8777c723b9b12e9f8eaeb2c87a5a793": {"ta_keywords": "sentence rationales supervision;predict faithful sentence;rationales supervision model;learning predict faithful;rationales supervision;rationales sentence level;supervision target task;faithful sentence rationales;model outperforms reinforcement;learning predict;predict correct rationales;supervision model produces;supervision model;outperforms standard bert;bert blackbox;sentence level;outperforms reinforcement;outperforms reinforcement learning;sentence rationales;supervision target;models able predict;standard bert blackbox;able predict correct;bert;bert blackbox exceeding;relies nondifferentiable models;able predict;learning approach relies;sentence level applying;learning", "pdf_keywords": "trustworthiness sentence prediction;able predict sentences;sentence detection rationales;rationale supervised;sentence able predict;rationale supervision detection;decisions sentence prediction;predict sentences precision;supervised gold rationales;predict sentences;optimal rationale supervision;supervised sentence models;blackbox approach supervision;sentence prediction task;problem rationale supervised;sentence prediction;sentence prediction essential;bertarian reasoning tasks;rationale supervised assigned;rationales faithful prediction;supervised sentence;rationale supervision;sentence models;predict correct rationale;detection rationales;movie review predicting;compete bertarian reasoning;faithful prediction evaluate;improved supervised gold;rationale sentence level"}, "cd1d915604826e5fb0ba2bbcdf8479a9b90fb289": {"ta_keywords": "generation random lattices;generated random lattice;random lattice generated;random lattices model;random lattices;based random lattice;lattice generated random;random lattice;particles random lattice;random lattice random;path random lattice;random lattice path;lattice random;lattice path random;lattice random number;lattice model generation;lattice generated;lattices model;scalable lattice model;lattice model;lattices model based;scalable lattice;lattices;propose scalable lattice;lattice;lattice path;model generation random;generation random;generated random;particles random", "pdf_keywords": "optimal placement relay;optimal placement relays;optimal place relay;relays random lattice;placement relay nodes;placement relay network;lattice placing relays;relay placement nlos;relay given placement;relay placement;relay optimal;placement relay;placement relays;placement relays given;consider deployment relays;relays network deployment;relay network deployment;placement relay given;deployment relays network;place relay deployment;placement sets relay;placement relays determined;deployed relays random;deployment relays;relay optimal range;decide place relay;problem relay placement;relay deployment;place given relay;relay proposed objective"}, "a425a11b9b249cb768d0f54d4a32f4f1d007e279": {"ta_keywords": "svm online learning;linear svm online;svm online;pass online learning;linear svm;rform feature selection;online learning tasks;online learning;learning based margin;svm;comparable linear svm;online learning based;single pass online;balanced winnow algorithm;feature selection;pe rform feature;margin balanced winnow;pass online;rform feature;based margin balanced;winnow algorithm proposed;winnow algorithm;margin balanced;balanced winnow;single pass;learning tasks propose;learning based;present novel algorithm;feature;learning", "pdf_keywords": ""}, "2d71fb62c71e49479c1b6ce832ee1bb88df20556": {"ta_keywords": "computing common subsumer;descriptions common subsumer;relating common subsumer;common subsumer set;common subsumer pair;common subsumer known;common subsumer;subsumer set descriptions;subsumer pair descriptions;subsumer set chain;subsumer known problem;subsumer pair;subsumer set;computing common;method computing common;subsumer known;set descriptions computed;testing subsumption;testing subsumption present;set chain equalities;descriptions computed relating;tractability computing common;subsumer;descriptions computed;equalities analyze tractability;computed relating common;pair descriptions common;chain equalities analyze;subsumption;subsumption present", "pdf_keywords": ""}, "6fa85c46ea68c754ef903edc70058ba525f1fc4d": {"ta_keywords": "copies generated algorithm;number copies generated;copies given computer;algorithm able generate;number copies computer;copies generated;generated algorithm;generated algorithm determined;numbers copies computer;large number copies;number copies given;determined number copies;generated algorithm algorithm;large numbers copies;computer number copies;generate;copies computer;generate large numbers;able generate large;generate large number;generation large number;given computer;given computer method;able generate;copies computer number;generate large;computer method;number copies;numbers copies;copies given", "pdf_keywords": ""}, "0c89b1ec80de46222ed7efc6261c03e52a1e2c54": {"ta_keywords": "description decoders;encoders description decoders;description encoders;description decoders demonstrate;description encoders description;natural words based;class natural words;words based local;encoders description;natural words;combination description encoders;describing;local global contexts;global contexts model;decoders;global contexts;words based;contexts model;contexts model combination;contexts;encoders;decoders demonstrate;decoders demonstrate effectiveness;describing large class;description;words;method describing;based local global;novel method describing;local global", "pdf_keywords": ""}, "b2b9b0d7afd85c5d708b79a61d9a000c6c906d8c": {"ta_keywords": "graph represent syntactic;syntactic relations nodes;text problem nodes;approach parsed text;labelled directed graph;nodes labelled directed;parsed text;similarity measure graph;novel approach parsed;represent syntactic relations;relations nodes graph;directed graph represent;task specific similarity;relations nodes;directed graph;graph walk;syntactic relations;constrained graph walk;problem nodes labelled;nodes labelled;graph graph walk;represent syntactic;parsed text problem;method graph walk;graph walk process;path constrained graph;syntactic;graph walk method;parsed;labelled directed", "pdf_keywords": ""}, "3042bc348d6cc7959cd574756f720e5afad236de": {"ta_keywords": "draw paperboard tray;paperboard tray;indented paperboard method;method indented paperboard;indented paperboard;paperboard method;draw paperboard;paperboard method based;anisotropic tray indented;ability draw paperboard;indented corners tray;paperboard tray key;deep drawing paper;drawing paper;tray shallow rectangle;tray indented;simple anisotropic tray;paperboard anisotropic;paperboard;tray indented indented;corners tray shallow;anisotropic tray;paperboard anisotropic mechanical;corners tray;walls paperboard;walls paperboard anisotropic;drawing paper presents;tray shallow;tray key aspect;tray", "pdf_keywords": ""}, "39c5cfc0ff6660a17364cb4af1eb071d6efa463d": {"ta_keywords": "mechanical turk variety;mechanical turk;amazon mechanical turk;comparative ordinal measurements;tasks pairwise comparative;turk variety tasks;humans unknown quantity;eliciting humans unknown;cardinal comparative measurements;empirical evidence;comparative measurements provide;comparative measurements;comparative ordinal;comparative;making direct comparative;provide empirical evidence;pairwise comparative;pairwise comparative ordinal;direct comparative;empirical evidence based;provide empirical;measurements provide empirical;comparative cardinal comparative;empirical;humans unknown;comparative cardinal;based experiments;lower sample noise;cardinal comparative;sample noise typically", "pdf_keywords": "inequality estimation ordinal;ordinal samples estimation;comparative ordinal measurements;estimation ordinal samples;estimation ordinal sample;mechanical turk variety;samples estimation ordinal;estimation paired comparisons;mechanical turk;estimation ordinal;amazon mechanical turk;selection measurement scheme;estimation paired;selection measurement;comparisons estimation cardinal;paired comparisons estimation;tasks pairwise comparative;processing inequality estimation;estimation items;ordinal approaches evaluation;comparisons estimation;crowdsourcing choice evaluation;scoring cardinal comparative;samples estimation paired;humans unknown quantity;cardinal samples estimation;lower sampleestimation fundamental;inequality estimation;ordinal samples;estimation cardinal samples"}, "67b29c3fe6f110125a8892e8ed128d20b23957ea": {"ta_keywords": "lingual entity linking;disambiguation cross lingual;entity linking;improve disambiguation cross;entity linking model;cross lingual entity;entity candidates disambiguation;entity linking takes;improve disambiguation;lingual entity;disambiguation cross;candidates disambiguation cross;model cross lingual;higher linking accuracy;disambiguation;higher linking;linking;linking model used;linking takes account;achieve higher linking;candidates disambiguation;linking model;linking accuracy;cross lingual;generation entity candidates;linking accuracy existing;used improve disambiguation;resources languages;entity candidates;linking takes", "pdf_keywords": "crosslingual entity disambiguation;multilingual entity disambiguation;lingual entity linking;disambiguation resource heavy;disambiguation resource;cross reference disambiguation;disambiguation entity disambiguation;entity disambiguation powerful;improve entity disambiguation;accurate flexible disambiguation;entity disambiguation;reference disambiguation proposed;disambiguation entity;entity disambiguation xel;reference disambiguation;entity disambiguation entity;flexible disambiguation model;flexible disambiguation;disambiguation method effective;entity disambiguation natural;entity linking propose;disambiguation method;disambiguation model;reference disambiguation xel;entity linking;entity linking xel;model disambiguation resource;disambiguation powerful tool;disambiguation xel sources;disambiguation model demonstrate"}, "2a64da1ed300e49f2d665312146c8bb2f66920b7": {"ta_keywords": "machine translation smt;statistical machine translation;translation smt systems;machine translation;translation smt;optimization smt parameters;optimization smt;optimization statistical machine;optimization statistical;issues optimization smt;research optimization statistical;smt parameters;statistical machine;optimization;translation;smt parameters outline;smt systems;research optimization;issues optimization;smt systems discuss;years research optimization;discuss issues optimization;smt;statistical;parameters;machine;parameters outline;systems discuss;systems;survey", "pdf_keywords": ""}, "d530a007ae0493ef6a8167c25bd007104623c504": {"ta_keywords": "decompiled code renaming;source code binary;names source code;code binary;code renaming;variable names source;identifying variable names;decompiled code;code renaming use;decompiler lexical information;binaries generated;binaries generated projects;code binary method;source code;recovered decompiler lexical;generating corpora;decompiler lexical;models decompiled code;technique generating corpora;generating corpora suitable;variable names;64 binaries generated;binary;lexical information recovered;binaries;information recovered decompiler;corpora;lexical information;corpora suitable;corpora suitable training", "pdf_keywords": "names decompiled code;decompiled code renaming;decompiled identifier renaming;meaningful names decompiled;variable names decompiled;names decompiled;names decompiled variables;propose decompiled identifier;decompiled identifier;variable names decompiler;code renaming;identifiers given program;identifier dire decompiled;code decompilers;identifier renaming engine;identify identifiers encoding;identifiers freely available;identifiers encoding process;predicting variable names;names decompiler;identifying variable names;identifier renaming;code renaming use;dire decompiled identifier;encoding information identifiers;identifier engine;extract binary namesin;characterizing source code;information identifiers freely;identifiers encoding"}, "bc1bf0a21d7838ec167e77c76163afc1f5f76c3d": {"ta_keywords": "multi channel electroencephalogram;channel electroencephalogram;channel electroencephalogram eweg;electroencephalogram;electroencephalogram eweg proposed;electroencephalogram eweg;spectral transform stft;covariance matrices frequency;background noise single;short time spectral;removing background noise;noise single trial;time spectral transform;recorded multi channel;multi channel signal;potentials recorded multi;spectral transform;related potentials recorded;event related potentials;pseudo erp data;noise single;potentials recorded;time spectral;background noise;using pseudo erp;channel signal;transform stft;transform stft representing;matrices frequency bins;multi channel", "pdf_keywords": ""}, "4c9f20ce99f9b93527fd76ec04a44fcef9082005": {"ta_keywords": "fairness interactive recommenders;fairness aware recommender;fairness aware recommenders;recommenders based reinforcement;aware recommender high;recommender accuracy;fairness interactive;accuracy fairness interactive;interactive recommenders;aware recommender;aware recommenders based;aware recommenders;interactive recommenders proposed;fairness aware;recommenders evaluate;recommenders based;recommenders evaluate performance;improve accuracy fairness;recommenders proposed;accuracy fairness;provide fairness aware;recommender high;recommenders proposed framework;recommender;named fair ed;named fair;provide fairness;recommenders;fairness;new framework fairness", "pdf_keywords": "fairness interactive recommender;fairness recommender systems;fairness aware recommendation;fairness recommender;considers fairness recommender;recommendation framework fairrec;reinforcement learning fairrec;recommendation framework reinforcement;personalized fairness aware;fairness status recommendation;preference state fairness;personalized fairness;fairness composed user;fairness aware state;user preferences fairness;propose personalized fairness;accuracy fairness interactive;fairness interactive;preferences fairness status;fairness state proposed;preferences fairness;fairness accuracy framework;fairness state;accuracy fairness propose;novel fairness aware;fairrec dynamically;propose fairness accuracy;fairness accuracy;fairness measure deep;fair fairness aware"}, "cb153d8469ac466606032ea457b934bc61ae86ae": {"ta_keywords": "emotion fake news;dual emotion fake;detecting fake news;fake news detectors;emotion fake;dual emotion representation;emotion features effectively;emotion features;related emotion features;news detectors;emotion representation;based dual emotion;emotion representation emotion;emotions dual;news detectors enhancement;existing fake news;emotions dual emotion;news proposed feature;dual emotion;detecting fake;representation emotion;fake news proposed;emotion related signal;relationship emotions dual;representation emotion related;emotions;fake news;emotion;emotion related;task related emotion", "pdf_keywords": ""}, "029fa34b291c3f60b8a00cdf386e6048d45c394d": {"ta_keywords": "mixed membership clustering;membership clustering;membership clustering proposed;clustering;clustering proposed;clustering proposed approach;graph based representation;approach mixed membership;representation data graph;mixed membership;graph based;node particle proposed;data graph based;scalable approach mixed;node particle;data represented weighted;scalable large scales;based representation data;proposed approach scalable;approach based representation;approach scalable large;data graph;scalable approach;represented weighted;representation data;terms node particle;scalable large;membership;data represented;present scalable approach", "pdf_keywords": ""}, "04b876e95ac3e4754c8f0c8a9355e7acc3dc70b9": {"ta_keywords": "adding dictionaries corpora;training corpus adding;dictionary adding annotated;corpus adding;adding dictionaries;effect adding dictionaries;language resource addition;dictionaries corpora language;corpus adding new;adding new words;dictionary adding;dictionaries corpora;adding annotated sentences;add annotated sentences;corpora language resource;efficient add annotated;new words contexts;training corpus;corpora language;annotated sentences training;sentences training corpus;annotated sentences word;adding entries dictionary;adding annotated;add annotated;annotated sentences;corpora;dictionaries;corpus;entries dictionary adding", "pdf_keywords": ""}, "7393d2618c7478d937112865458862e8d5f10475": {"ta_keywords": "generation domain domains;generation domain;pretrained sequence models;domains set pretrained;field model generated;sequence models;pretrained sequence;sequence models model;model generated finite;model generated;set pretrained sequence;models model generated;domain domains;study generation domain;domain domains set;domains;field model;vector field model;domains set;models;generated finite size;domain;dimensional vector field;generated finite;generated;case study generation;generation;field;vector field;model", "pdf_keywords": "domain knowledge generation;domain reasoning collect;seq reasoning tasks;domain reasoning ability;domain reasoning challenge;cross domain reasoning;cross domain knowledge;crossdomain reasoning seds;domain reasoning approach;domain reasoning;reasoning cross domain;domain knowledge;sequence model trained;crossdomain reasoning approach;reasoning tasks;reasoning collect dataset;sequence model template;reasoning seds;sequence sequence models;challenging task reasoning;seq models task;problem structured knowledge;reasoning tasks approach;perform crossdomain reasoning;task reasoning cross;structured knowledge base;sequence models perform;generation machine learning;underlying knowledge base;sequence models"}, "7137a842d496a1a5581db31ad946fa0c0827e663": {"ta_keywords": "nonlocality xmath0 model;model nonlocality xmath0;nonlocality xmath0;xmath0 model;based xmath1 model;model xmath2 xmath3;xmath0 model model;model xmath2;xmath1 model;model based xmath1;xmath1 model xmath2;new model nonlocality;model nonlocality;based xmath1;xmath0;xmath10 xmath11;xmath10 xmath11 xmath;xmath9 xmath10 xmath11;xmath2 xmath3;xmath3 xmath4;xmath11 xmath;xmath3 xmath4 xmath5;xmath2 xmath3 xmath4;xmath11;xmath8 xmath9 xmath10;xmath10;xmath3;xmath9 xmath10;xmath5 xmath6 xmath7;xmath7 xmath8 xmath9", "pdf_keywords": ""}, "f735f5f55cbc5a9d372ea1cd9b4e81d35f043a00": {"ta_keywords": "broader stochastically transitive;study stochastically transitive;stochastic transitivity;stochastically transitive model;stochastically transitive;stochastic transitivity provide;stochastically transitive class;transitive model pairwise;pairwise comparisons probabilities;form stochastic transitivity;model pairwise comparisons;comparisons probabilities outcomes;transitive model;models broader stochastically;comparisons probabilities;pairwise comparisons;broader stochastically;transitivity provide various;transitivity;stochastically;transitive;stochastic;parametric models provide;classical parametric models;model pairwise;study stochastically;transitive class;transitivity provide;parametric models logarithmic;standard parametric models", "pdf_keywords": "soft thresholding estimators;estimating latent quality;estimates latent quality;asymptotic behavior thresholding;estimation stochastic transitivity;thresholding estimators;thresholding estimators provide;robust stochastic transitivity;threshold optimally weighted;latent quality vector;threshold optimally;class pairwise comparisons;soft thresholding;transitivity model minimax;estimating probability pairwise;broader stochastically transitive;hard soft thresholding;models estimation pairwise;threshold estimate;behavior thresholding estimator;optimal estimates latent;thresholding estimator;models broader stochastically;bound robust stochastic;error thresholding estimator;close threshold optimally;stochastic transitivity model;pairwise comparison probabilities;estimators achieves minimax;error thresholding"}, "380278716f4d78ad9dcc3ece9e12b235ca1d1569": {"ta_keywords": "probabilistic logic neural;parallelization deep;suitable parallelization deep;parallelization deep learning;logic neural network;logic neural;deep learning problem;deep learning;approach deep learning;learning problem solving;neural network infrastructure;novel approach deep;approach deep;neural network;probabilistic logic;learning problem essential;learning problem;neural;solving problems;hundreds thousands examples;thousands examples approach;problem solving problems;integration probabilistic logic;parallelization;examples suitable parallelization;learning;involving thousands examples;suitable parallelization;thousands examples;thousands examples suitable", "pdf_keywords": "probabilistic logics deep;logics deep learners;learning stochastic logic;embedding probabilistic logics;probabilistic deductive databases;logic embeddings significantly;stochastic deductive knowledge;logic embeddings;deductive databases;logic tensor network;deductive knowledge graphs;logic embeddings able;deductive knowledge graph;stochastic logic programs;probabilistic logics;proposed logic embeddings;logics deep;probabilistic deductive;probabilistic logic framework;limited stochastic deductive;deductive databases ddbs;stochastic logic;probabilistic logic;deductive knowledge;stochastic deductive;probabilistic reasoning outputs;predicates tensor network;contained deductive knowledge;enable probabilistic reasoning;reasoning outputs deep"}, "8a880680b28dee5642ac88431b3ae1085b911f96": {"ta_keywords": "improving translation quality;low resource translation;improves translation quality;translation quality low;regularizer improves translation;translation quality;translation quality multiple;improving translation;method improving translation;improves translation;regularizer penalizes translation;penalizes translation length;penalizes translation;translation tasks;resource translation tasks;translation data;resource translation;translation data use;translation length;word frequencies translation;translation tasks method;translation length word;low resource languages;translation english;study translation english;study translation;translation;frequencies translation data;frequencies translation;quality low resource", "pdf_keywords": ""}, "4f7b108830de2e7964b6e1a89bf1c2da60140a34": {"ta_keywords": "posterior collapse predictive;training posterior collapse;posterior collapse data;posterior collapse;collapse predictive model;collapse predictive;fit posterior collapse;collapse data;method training posterior;posterior surrogate;posterior surrogate objective;considers posterior surrogate;collapse data previously;training posterior;collapse;predictive model method;predictive;predictive model;lower bound surrogate;posterior;considers posterior;surrogate objective;bound surrogate objective;heuristic considers posterior;bound surrogate;novel method training;method training;surrogate objective heuristic;fit posterior;state art methods", "pdf_keywords": "posterior language modeling;language models low;known language models;language model effective;language models;autoencoder language model;powerful language model;language modeling reconstruction;language modeling;language model;suboptimal language modeling;trained effectively generative;training autoencoder language;encoder inference;encoder inference network;posterior language;language modeling propose;variable encoder inference;likelihood suboptimal language;effectively generative model;language modeling setting;language model consider;autoencoder language;inference network powerful;effective representation learning;generative model latent;latent variable encoder;representation learning framework;drive posterior language;method training autoencoder"}, "14119210e5f9e0d962e329c833557dfb5524c4bd": {"ta_keywords": "electrolyte structure ices;electrolyte matrix scaffold;lithium oxygen batteries;cathode electrolyte structure;based cathode electrolyte;integrated cathode electrolyte;cathode electrolyte matrix;oxygen batteries sslobs;sslobs battery constructed;cathode electrolyte;batteries sslobs battery;performance lithium oxygen;battery constructed 3d;lithium oxygen;electrolyte structure;polymer nanofibers nss;batteries sslobs;sslobs battery;oxygen batteries;electrolyte matrix;novel integrated cathode;silicate polymer nanofibers;high performance lithium;polymer nanofibers;electrolyte;battery constructed;nanofibers nss novel;performance lithium;nanofibers nss;matrix based cathode", "pdf_keywords": ""}, "de8ded0d66f3227d99751a89fdd5f4b438d6e8ee": {"ta_keywords": "harmonic trap coupling;particle harmonic trap;trap coupling;coupling spin orbit;spin orbit coupling;trap coupling strength;harmonic trap;frequency particle coupling;coupling motion particle;orbit coupling motion;coupling spin;coupling motion;effect coupling spin;orbit coupling;particle coupling;spin orbit;particle coupling strength;motion particle harmonic;coupling strength tuned;coupling;particle harmonic;effect coupling;tuned varying amplitude;amplitude particle motion;coupling strength;frequency particle;varying frequency particle;trap;tuned varying frequency;spin", "pdf_keywords": ""}, "e74d7523d7d96ab65f05f059284f9d0a994bb074": {"ta_keywords": "new treebank talks;treebank talks;present new treebank;treebank talks proceedings;new treebank;treebank;proceedings talks;talks proceedings;talks international workshop;talks proceedings talks;proceedings talks international;talks;international workshop proceedings;talks international;proceedings;international workshop;workshop proceedings;workshop;international;present new;new;present", "pdf_keywords": ""}, "3efee0095cb578659dfaaf0d87a616f133ecf85c": {"ta_keywords": "study acoustic modeling;acoustic modeling;comparative study acoustic;study acoustic;acoustic;modeling;present comparative study;comparative study;comparative;present comparative;study;present", "pdf_keywords": ""}, "9896a68e999298410bf16ffd08e8e67a54ad6a91": {"ta_keywords": "processing tools corpora;language processing tools;natural language processing;tools corpora;tools corpora presented;corpora;language processing;corpora presented;distributed cloud computing;framework distributed cloud;cloud computing evaluation;distributed cloud;cloud computing;various natural language;tools framework distributed;natural language;selected natural language;processing tools framework;processing tools;framework distributed;cloud;tools framework;distributed;processing;tools;computing evaluation;computing;language;performance various natural;framework", "pdf_keywords": ""}, "52c040c4b1786166325a0d930af94a529e2b5023": {"ta_keywords": "network summarize speaker;summarize speaker message;summarize speaker;neural network summarize;speaker message method;speaker message;sequence neural networks;training sequence neural;training neural network;resulting neural network;neural networks combined;neural network;train neural network;sequence neural;training neural;neural networks;training network vector;speaker;network vector extractor;network summarize;method train neural;train neural;vector extractor;message method;training network;input resulting neural;standard training neural;single training network;training sequence;neural network shown", "pdf_keywords": ""}, "da564ff902a5490088f60c9fb100531fc9f97288": {"ta_keywords": "learning probabilistic logic;probabilistic logic;probabilistic logic approach;entity resolution classification;based personalized pagerank;approach learning probabilistic;learning probabilistic;pagerank designed efficient;personalized pagerank;databases facts approach;pagerank;personalized pagerank designed;entity resolution;classification joint inference;pagerank designed;large databases facts;logic approach;probabilistic;logic approach based;including entity resolution;databases facts;joint inference;inference;efficient large databases;logic;large databases;facts approach;entity;facts approach illustrated;approach based personalized", "pdf_keywords": "stochastic logic programs;nodes stochastic logic;stochastic logic program;stochastic logic network;learning entity resolution;including stochastic logic;based stochastic logic;stochastic logic;grounding procedure learning;probabilistic language;new probabilistic language;probabilistic language suited;learning inference tasks;logic programs suited;logic programs;probabilistic order language;weight learning entity;order probabilistic language;learning entity;learning query;logic network;database predicate;purpose learning query;efficient inference rapid;inference tasks;logic network approach;highly efficient inference;probabilistic language called;database predicate use;inference rapid learning"}, "27724bd19946d6a824d06cdca3cdfe5d40f71003": {"ta_keywords": "predicting edit code;predicting edit;edit code snippet;code snippet model;approach predicting edit;snippet model based;trained past edits;snippet model;edit code;code snippet;higher accuracy syntactic;accuracy syntactic;past edits;snippet;code;novel approach predicting;neural models experiments;model based learned;learned model;models l3po neural;syntactic;l3po neural models;based learned model;past edits conduct;learned model trained;neural models;predicting;model trained past;approach predicting;edits conduct thorough", "pdf_keywords": ""}, "0cee58946a13a5c2845647b4af8b9d2bf52a8b6b": {"ta_keywords": "predictions entity recognition;entity recognition ner;entity recognition;accurate predictions entity;predictions entity;trained language model;recognition ner tasks;language model ner;accuracy predictions adapting;improves accuracy predictions;language model;entity;recognition ner;model ner tasks;predictions adapting pre;pre trained language;ner tasks;ner tasks proposed;based distance labeled;accuracy predictions;distance labeled;ner tasks demonstrate;framework accurate predictions;trained language;predictions adapting;training algorithm improves;novel training algorithm;distance labeled approach;adapting pre trained;labeled approach new", "pdf_keywords": "entity recognition ner;entity recognition nerns;distantly supervised ner;distantly supervised named;named entity recognition;label distant entities;entity recognition;entity recognition wf;supervised named entity;entity deep learning;usingnamed entity recognition;distant labels entitieswe;distantly supervised;predicting named entity;novel distantly supervised;entity deep;given entity deep;supervised ner models;distant entities based;supervised ner;adapt distantly supervised;ner classification tasks;supervised language model;distant entities;scalable distantly supervised;supervised ner model;predicting distant labels;distant labels ner;approach label entities;supervised language"}, "8274799029bfac4402685e1efd995a8aeb9e7426": {"ta_keywords": "unsupervised sentence compression;unsupervised text compression;unsupervised encoder decoder;sequence encoder decoder;novel unsupervised encoder;unsupervised encoder;sequence encoder;sequence sequence encoder;sentence compression set;sentence compression;decoder model unsupervised;encoder decoder;text compression;encoder;text compression based;encoder decoder model;encoder decoder pair;decoder;compression based sequence;unsupervised text;model unsupervised text;unsupervised sentence;decoder model;sentences large corpus;large corpus data;large corpus;decoder pair;compression based;applied unsupervised sentence;information sentences large", "pdf_keywords": "unsupervised sentence compression;sentence compression model;abstractive sentence compression;sentence compression based;sentence compression task;encode summary topic;compressed sentences;sentence compression;machine translation encoder;length compressed sentences;sequence encoder decoder;summaries varying compression;machine translation;machine translation paradigm;sequence encoder;encode summary;sequence sequence encoder;describing sentences text;compressed sentences forces;paraphrasing compressing text;sentences text neural;supervised abstractive sentence;encoding sum words;latent word sequences;novel machine translation;autoencoder unsupervised abstractive;translation encoder;paraphrasing compressing;decoder autoencoder;capable producing summaries"}, "927efd299cffcfca3716efefcc904331b70c153e": {"ta_keywords": "interpretable reasoning graph;reasoning graph generated;reasoning graph;logical graph generated;logical graph;form logical graph;novel interpretable reasoning;interpretable reasoning;interpret answer question;mathematical expressions use;generate novel interpretable;natural language;language processing np;mathematical expressions;question form logical;interpret answer;case natural language;mathematical expressions mathematical;natural language processing;mathematical expressions case;expressions use;expressions mathematical;set mathematical expressions;graph generated;expressions mathematical expressions;form logical;reasoning;expressions;graph;interpret", "pdf_keywords": "natural language mathematically;literature characterization reasoning;explaining complex questions;reasoning process noahqa;conversation provide annotators;mathematical description interpretation;important understanding reasoning;problem mathematical language;mathematical language;description mathematical description;orthogonality natural language;characterization reasoning;analyzing conversation;description mathematical;description interpretation graph;improved using interpretation;understanding reasoning;proof generation understanding;mathematical description present;structure natural language;mathematical language approach;natural language np;description problem approach;mathematical description;mathematical language presented;natural language;method analyzing conversation;mathematical description mathematical;description based approaches;approach combines description"}, "6a116b897569fe4d6ea9ad4c3ba9a18825b96f49": {"ta_keywords": "rules model learns;neural controller learns;learns sequentially;model learns sequentially;differentiable model learning;knowledge base completion;model learns;completion neural controller;neural controller;controller learns sequentially;learns sequentially compose;controller learns;perform knowledge base;rules model;base completion neural;model learning sets;knowledge base;order rules model;completion neural;operations perform knowledge;model learning;primitive differentiable operations;differentiable operations;completely differentiable model;compose differentiable operations;learns;learning sets;differentiable model;differentiable operations perform;learning sets order", "pdf_keywords": "base reasoning learning;neural logic programming;reasoning learning;knowledge base reasoning;neural logic;learning order logical;reasoning learning problem;base reasoning tasks;relations neural rules;rules knowledge base;logical rules knowledge;reasoning natural language;learning relations neural;learning learns relations;knowledge base;inductive logic programming;framework neural logic;relation learning learns;probabilistic differentiable logic;learning relations;rules learned neural;relational learning;relations neural network;knowledge base relation;learns relations solving;problem learning relations;relational learning method;rules neural;learn structure query;relation learning"}, "3c0e8f7337491ca4f714de14021eb23ca43d1d5e": {"ta_keywords": "speech recognition reverberant;robustness automatic speech;recognition reverberant environments;aspire automatic speech;recognition reverberant;implemented multidimensional speech;speech recognition systems;reverberant environments challenge;multidimensional speech representation;automatic speech recognition;speech recognition;automatic speech;multidimensional speech;speech representation;reverberant environments;acoustic models performance;acoustic models;reverberant;recognition systems challenge;speech representation represented;noisy environments challenge;set acoustic models;robust performs noisy;challenge develop robust;performs noisy environments;recognition systems;robustness automatic;noisy environments;challenge implemented multidimensional;data noisy environments", "pdf_keywords": ""}, "6c78bac2dd71efb89951d9bab72c8129bbc07f67": {"ta_keywords": "regularize topic models;topic models latent;regularize topic;used regularize topic;regularization technique latent;variables regularization framework;observed variables regularization;variable based regularization;topic models;variables regularization;mixed membership models;modeling experiments regularization;models latent variable;regularization framework;based regularization;language modeling experiments;membership models language;regularization;latent observed variables;models latent;regularization framework used;models language modeling;latent variable mixed;experiments regularization;language modeling;regularization leads better;membership models;experiments regularization leads;variable mixed membership;regularization technique", "pdf_keywords": ""}, "ce45aa1c64da82bfd02db0e147efa268da6980e4": {"ta_keywords": "multiple access ofdm;access ofdm;multiple access orthogonal;orthogonal multiple access;ofdm analyze performance;ofdm analyze;bit loading algorithms;access ofdm analyze;ofdm;access orthogonal multiple;access orthogonal;bit loading;multiple access;classic bit loading;algorithms framework orthogonal;entropy method algorithm;entropy method propose;study bit loading;loading algorithms greedy;maximum entropy method;loading algorithms;use maximum entropy;maximum entropy;entropy method;framework orthogonal multiple;orthogonal multiple;framework orthogonal;loading algorithms framework;performance classic bit;entropy", "pdf_keywords": ""}, "d32fb57467d64bb82dce60e904ddc5c18b3f0f91": {"ta_keywords": "predicting spatial distribution;spatial distribution parking;distribution parking spots;parking spots;predicting spatial;parking spots city;method predicting spatial;spatial distribution;spatial distribution data;parking;distribution parking;use spatial distribution;stochastic mixture;stochastic mixture model;spots city method;mixture model predictive;combination stochastic mixture;optimal point view;approach use spatial;based combination stochastic;use spatial;spatial;city method based;stochastic;spots city;predictive approach;view use spatial;combination stochastic;city method;mixture model", "pdf_keywords": "spatial distribution parking;characteristics parking demand;parking demand illustrate;parking demand study;parking demand;spatiotemporal structure parking;demand parking;demand parking significantly;parking using probabilistic;demand parking zones;parking demand city;parking demand using;study demand parking;demonstrate parking demand;demand parking popular;mobility demand parking;consistency parking demand;distribution parking demand;structure parking demand;generalized gaussian mixture;characteristics parking;parking demand belltown;demand demonstrate parking;parking demand displays;temporal characteristics parking;present demand parking;parking dynamics;parking demand bounded;problem parking demand;gaussian mixture model"}, "ab5c6703fceb3dce6558be309cc65a4a8615c774": {"ta_keywords": "graph based neighborhood;neighborhood graph algorithm;constructing neighborhood graph;fast neighborhood graphs;neighborhood graph;neighborhood graph using;based neighborhood graphs;neighborhood graphs graph;neighborhood graphs;construction neighborhood graph;neighborhood graphs guaranteed;algorithm constructing neighborhood;graph based search;graph based distance;distance graph based;constructing neighborhood;based distance graph;graph algorithm based;graphs graph based;accurate fast neighborhood;graph algorithm;distance graph;graph based;fast neighborhood;using graph based;neighborhood;based construction neighborhood;construction neighborhood;search algorithm constructing;graphs guaranteed accurate", "pdf_keywords": "searching generic distances;searching non metric;nearest neighbor search;nearest neighbor graphs;graph based retrieval;metric data neighborhood;non symmetric distances;generic distances graph;similarity search equivalent;nearest neighbor queries;similarity search;graph based search;symmetrization distance learning;distance symmetrization;mapping distance symmetrization;metric learning;methods symmetrization distance;graph method searching;symmetric distances resorting;symmetric distances use;symmetrization graph neighborhoods;navigable nearest neighbor;nearest neighbor;metric learning accomplished;symmetric distances;data neighborhood graphs;distances graph based;approaches nearest neighbor;neighbor search;queries non metric"}, "d6741241efb9ffd933df974b43d7109c72238371": {"ta_keywords": "generative generation music;generation music capable;generation music based;multi track generative;control generation music;generation music;track generative;track generative model;generative generator capable;novel generative generation;generative generator;capturing structure music;control structure music;generative generation;displaying structure music;model generative generator;structure music choosing;generative model;music based multi;generative model generative;structure music;music choosing desired;music capable displaying;model generative;music capable;generative;music choosing;propose novel generative;novel generative;music offers control", "pdf_keywords": "generating musical tracks;music processing generation;processing generation music;generating music based;generating musical;generating music;construct music representation;music processing;generating multiple tracks;music representation;excerpt trained midi;method generating musical;method generating music;process based onmusic;music based representation;easily construct music;generating multi track;midi;music generation combines;music machine;midi instruments;track music machine;sequence musical events;representation musical material;trained midi;musical tracks;musical excerpt trained;generating set tracks;musical tracks simple;trained midi instruments"}, "d41216f2f809e9fe26a684392f0ded4778f79e74": {"ta_keywords": "speech recognition asr;tracking language utterance;automatic speech recognition;speech recognition;automatic speech;language utterance model;utterance model based;tracking language;utterance model;utterance able recognize;enables tracking language;end automatic speech;recognition asr;language utterance;mixed language speech;recognize mixed language;language speech;language speech experiments;switches utterance;able recognize language;language dependent models;recognize language mixed;switches utterance able;recognition asr enables;recognize language;language switches utterance;asr enables tracking;speech experiments;language independent neural;neural network architecture", "pdf_keywords": ""}, "66cbda3e730285cb572c4792edcef209af32c564": {"ta_keywords": "question answering;answer questions data;model answer questions;questions data;question answering able;supervised data generative;data generative model;data generative;generate supervised model;learn retriever model;generate supervised;including question answering;method learn retriever;generative model;structured data;structured data using;supervised model;supervised model data;use neural;generative model use;supervised data;questions data evaluate;neural network generate;answer questions;retriever model;answering able answer;supervised;model use neural;questions including question;use neural network", "pdf_keywords": "retriever question answering;question answering benchmarks;question answering task;question answering;learning approximate attention;large scale retrieval;answering task retriever;attention scores reader;retrievers fronts learning;learn retriever models;train information retrieval;trained structured semantic;approximate attention;models attention;queries documents annotations;retrievers based knowledge;question answering problem;training large reader;approaches retriever classification;questions retrieved;retrieval module downstream;reader training based;information retrieval;hereinafter question answering;models attention scores;training retriever module;approximate attention scores;reader training;method learn retriever;retriever classification"}, "7b96f6165ce5f686e46868c53b111b8e43b93de3": {"ta_keywords": "older papers cited;citations older papers;citations older;natural language processing;rate older papers;older papers;age old papers;papers cited;number citations older;language processing;old papers published;old papers;citations;older papers remained;natural language;older papers increased;papers cited increased;literature natural language;stable number citations;language processing rate;rate older;number citations;2019 rate older;processing rate older;cited;cited increased;papers published literature;older;papers remained relatively;old", "pdf_keywords": ""}, "863b2b38b33ffc4e5462adbc7aaf84aeb93adda8": {"ta_keywords": "annotation multiple languages;projecting annotation multiple;annotation multiple sources;projecting annotation single;projecting annotation;simultaneous annotation;simultaneous annotation multiple;annotation single source;algorithm simultaneous annotation;annotation multiple;methods projecting annotation;method projecting annotation;annotation single;problem projecting annotation;annotation method;annotation;annotation method novel;algorithm based annotation;based annotation method;based annotation;single source languages;multiple languages;multiple languages method;source languages algorithm;tasks multiple languages;source languages;languages algorithm;languages algorithm based;multiple languages improves;languages method", "pdf_keywords": "parallel corpus;relying parallel corpora;parallel corpus present;using parallel corpus;parallel corpus 18;using parallel corpora;parallel corpora;parallel corpora algorithm;parallel corpora use;decoding dependency graph;parsers separate corpora;target sentences graph;parallel corpora available;sentences graph constructed;jointly projecting annotation;joint annotation projection;source sentence graph;sentence graph encodes;sentence splitting tokenization;corpora approach fully;segment parallel corpora;dependencies represent corpora;parallel text based;corpora algorithm;corpora approach;corpus;represent corpora approach;sentence graph;parallel corpora evaluate;corpora algorithm obtains"}, "d16d24dd5135f148556df1b2304b3747eee19e00": {"ta_keywords": "text email messages;automatically identifying signature;reply lines;reply lines plain;signature blocks reply;sequential representation email;plain text email;text email;email message represented;identifying signature blocks;representation email message;blocks reply lines;email messages;message email message;signature blocks;message email;identifying signature;message represented sequence;email message email;representation email;email message;email messages method;message represented;sequential machine learning;automatically identifying;signature;represented sequence lines;messages;sequence lines;machine learning algorithms", "pdf_keywords": ""}, "7f54429be66319dc19a42c0c9fceda3ac33fc92d": {"ta_keywords": "inductive logic programming;rules simulation student;teaching language;simulation student demonstration;approach teaching language;simulation student;language classroom approach;teaching language classroom;simulation student work;logic programming;language classroom;logic programming path;inductive logic;use inductive logic;rules simulation;student demonstration;student demonstration set;based use inductive;classroom approach based;classroom approach;programming;classroom;set rules simulation;demonstration set rules;teaching;approach teaching;use inductive;novel approach teaching;simulation;programming path", "pdf_keywords": ""}, "c97500763de8a0871f1b83b1f968fcf4a8b31aee": {"ta_keywords": "speaking style tasks;stance taking;characterize standing taking;unscripted conversations;conversations pairs dyads;unscripted conversations pairs;characterize standing;standing taking unscripted;speaking style;study unscripted corpus;unscripted corpus;conversations pairs;unscripted corpus built;different speaking style;stance;conversations;stance taking increasing;standing taking;detect characterize standing;corpus;density stance taking;settings unscripted conversations;corpus built;different speaking;corpus built tool;unscripted activity enhanced;style tasks;standing;activity enhanced presence;unscripted activity measure", "pdf_keywords": ""}, "2ded680be56e03c8c17a04065deaac8ea6d4fa12": {"ta_keywords": "molecule optical depth;molecule sample optical;single molecule optical;molecule optical;sample optical spectra;optical depth sample;identify optical depth;depth single molecule;spectra single molecules;spectra single molecule;identification optical depth;optical spectra single;optical spectra;optical spectra method;single molecule sample;optical depth;optical depth single;single molecules sample;molecule sample;sample optical;molecules sample;applied sample optical;method identify optical;optical lines sample;single molecule method;depth sample determined;single molecules;depth sample;single molecule;optical lines", "pdf_keywords": ""}, "753d10503a3cf340e41552109087ffd15ec96446": {"ta_keywords": "harmonic trap electron;trap electron gas;electron gas harmonic;electron gas trapped;gas harmonic trap;confined harmonic trap;trapped harmonic trap;trap electron;gas trapped harmonic;harmonic trap trap;harmonic trap;trap excited electron;gas confined harmonic;trapped harmonic;motion electron gas;electron gas confined;electron gas;dimensional electron gas;gas harmonic;dynamics dimensional electron;excited electron motion;confined harmonic;electron motion;electron motion electron;motion electron;trap trap excited;excited electron;gas trapped;trap excited;trap trap", "pdf_keywords": ""}, "998bd8862ab4193e672bb16fe1aae4d446f7536e": {"ta_keywords": "image synthesis;learn images unpaired;image image translation;image translation;learning images;using image synthesis;image synthesis efficient;learn images;perform image synthesis;images based contrastive;unpaired image image;method learning images;unpaired image;learning images based;images unpaired image;contrastive learning method;contrastive learning;images unpaired;image map;image map similar;input image map;input image;based contrastive learning;image image;patches input image;image translation setting;used learn images;images;perform image;way using image", "pdf_keywords": "learning unpaired image;image translation learns;conditional image synthesis;unpaired image translation;image synthesis;encoder contrastive learning;contrastive learning encoders;visual representation learning;representation learning unpaired;patchwise contrastive learning;learns embedding;image training;function image translation;encoder contrastive;learning unpaired;predicting image;image translation;predict image;image synthesis using;image translation input;image translation able;paired image translation;learning generative;unsupervised visual representation;multilayer contrastive learning;learning encoders generative;image image translation;unsupervised visual;image translation based;single image translation"}, "d1206ccabd1980848f14472d6548251c2fab7963": {"ta_keywords": "labeling transfer learning;speech tagging transfers;transfer learning;transfer learning beneficial;sequence labeling transfer;task speech tagging;tagging transfers np;labeling transfer;target task speech;classification question answering;tagging transfers;transferability;answering sequence labeling;question answering;speech tagging;especially target task;transferability 33;problems text classification;question answering sequence;task speech;source task;study transferability;extensive study transferability;study transferability 33;text classification;transfers np compliant;source task small;target task;sequence labeling;transferability 33 different", "pdf_keywords": "task based quantum;tasks distributed quantum;quantum computing transfer;task embeddings optimization;computation distributed quantum;demonstrate task embeddings;task embeddings predictive;embeddings predictive task;task embeddings;quantum information processing;quantum computation distributed;explicit task embeddings;distributed quantum computing;task embeddings extract;task embeddings simple;quantum computing;quantum computation;task embeddings gradients;importance task embeddings;quantum search algorithms;embeddings extract transferable;present quantum computing;task predict linguistic;relationships task embeddings;identifying task transferability;task transferability task;representations tasks;large scale quantum;quantum computing framework;qkd quantum computation"}, "e9d26b9f5e6b619bbb759a67560cb949a9f034ba": {"ta_keywords": "ascent learning timescale;dynamics gradient descent;unconstrained continuous action;nonconvex optimization;ascent learning;gradient descent ascent;nonconvex optimization problem;descent ascent learning;zero sum games;sum games minimizing;descent ascent;gda unconstrained continuous;gradient descent;maximizing player;maximizing player optimizes;concave sc objective;learning timescale separation;problem maximizing player;ojasiewicz strongly concave;games minimizing;dynamics gradient;study dynamics gradient;unconstrained continuous;continuous action zero;faces nonconvex optimization;games minimizing player;optimizes polyak ojasiewicz;ascent;optimization problem maximizing;minimizing player", "pdf_keywords": ""}, "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91": {"ta_keywords": "multimodal machine learning;taxonomy multimodal machine;new taxonomy multimodal;recent advances multimodal;multimodal;advances multimodal;taxonomy multimodal;advances multimodal machine;multimodal machine;challenges faced multimodal;faced multimodal machine;faced multimodal;fusion categorization;late fusion categorization;fusion categorization identify;machine learning present;categorization identify broader;categorization;machine learning survey;categorization identify;learning survey recent;learning survey;machine learning;fusion;learning present common;learning;common taxonomy;taxonomy;late fusion;identify broader", "pdf_keywords": "multimodal machine learning;machine learning multimodal;multimodal alignment supervised;multimodal representation fusion;supervised multimodal alignment;learning multimodal;training multimodal representation;learning multimodal representation;multimodal representations;multimodal alignment model;challenges facing multimodal;multimodal representation important;neural networks multimodal;representations multimodal data;supervised multimodal;joint multimodal representation;challenges multimodal fusion;natural language multimodal;training multimodal fusion;multimodal representation;concepts multimodal representations;multimodal fusion kernel;recent advances multimodal;multimodal feature;concepts multimodal;multimodal translation research;approaches multimodal;propose supervised multimodal;model training multimodal;multimodal alignment"}, "480d545ac4a4ffff5b1bc291c2de613192e35d91": {"ta_keywords": "network models computational;network models;implementing neural network;network models based;complex network models;network structure;implement complex network;network structure toolkit;neural network models;dynamic declaration network;neural network;toolkit implementing neural;complex network;models computational graph;declaration network structure;implementing neural;computational graph defined;opticsinfobase;opticsinfobase org abstract;models based dynamic;http www opticsinfobase;opticsinfobase org;www opticsinfobase;graph defined;network;graph defined explicitly;models;www opticsinfobase org;computational graph;models computational", "pdf_keywords": "tools implementing deep;implementing deep neural;declaration toolkit neural;scalable computation neural;computation neural;computation networks provide;implementations neural;computation networks;implementing deep;framework implementing neural;computation neural networks;dynamic computation toolkit;implementations neural networks;implementing neural networks;neural network toolkit;defining computational architecture;defines computation graph;toolkit neural;dynamic execution network;implementation tensor network;performance neural network;toolkit neural network;computation toolkit;models deep learning;estimating computation networks;high performance neural;combines deep learning;performance neural;deep learning;deep neural network"}, "aeccb1d53e08adcfe271d1e4b08c0a2cdc3c42b4": {"ta_keywords": "open information extraction;sentence level extractions;information extraction;information extraction called;entity relation phrases;novel open information;open information;segmenting entity relation;extractions translating;extractions translating based;unify segmenting entity;extraction called remine;sentences based local;framework distant supervision;segmenting entity;level extractions translating;relation phrases individual;phrases individual sentences;individual sentences based;unify segmenting;phrases individual;real world corpora;sentences based;individual sentences;sentences;translating based objective;corpora;relation phrases;remine;extraction", "pdf_keywords": ""}, "824cd8db8a68732db04f4d8b7139eb4475e59ff2": {"ta_keywords": "natural language generation;language generation nlg;generation nlg natural;generation nlg;benchmark natural language;language generation models;natural language benchmark;nlg natural language;language benchmark natural;language generation;evaluation natural language;language benchmark goal;nlg natural;generation models;generation models particular;new natural language;nlg;language benchmark;natural language;generation;benchmark natural;benchmark goal provide;arise using benchmark;benchmark provide description;benchmark goal;framework evaluation natural;evaluation natural;benchmark provide;benchmark;description data", "pdf_keywords": "generation nlp challenges;generation nlp;language generation benchmark;models nlp tasks;natural language generation;language generation nlp;evaluation models nlp;generation conversational models;models nlp;evaluation natural language;learning nlp tasks;programming nlp models;generation benchmark introduce;nlp tasks;model generation conversational;nlp tasks human;generation benchmark;model natural language;models natural language;machine translation evaluation;text predict semantics;nlp models real;nlp models;generation benchmark np;nlp challenges;language generation np;large scale generative;learning nlp;machine learning nlp;predict semantics text"}, "508e9bb13fcb1fa0c4dbac47288e8a3c2487bfc2": {"ta_keywords": "generalize proof tree;proof tree generalization;algorithms generalize proof;generalize proof;proof tree;generalization algorithm provably;algorithms generalize;properties algorithms generalize;tree generalization algorithm;generalizing number;framework generalizing number;generalizing number consists;tree generalization;algorithm provably;generalization algorithm;properties algorithms;algorithm provably meets;desired properties algorithms;generalization;algorithms;generalizing;algorithm;generalize;framework generalizing;number consists desiderata;number consists;present framework generalizing;proof;number;tree", "pdf_keywords": ""}, "7a733a8d8f8649cc07e3ea9091f454ae117573af": {"ta_keywords": "biomedical text attention;attentionmesh utilizes deep;model attentionmesh;text attention;end model attentionmesh;model attentionmesh utilizes;deep learning attention;attentionmesh utilizes;text attention mechanism;biomedical text;index biomedical text;attentionmesh;learning attention;attention;attention mechanism index;learning attention mechanism;word level model;model associate textual;textual evidence annotations;attention mechanism;associate textual;evidence annotations providing;annotations providing interpretability;annotations;associate textual evidence;evidence annotations;textual;attention mechanism enables;annotations providing;text", "pdf_keywords": ""}, "abb9b27440719ca44db5947a537fde07f0547973": {"ta_keywords": "repair bandwidth nodes;problem distributed storage;distributed storage;distributed storage data;reduces repair bandwidth;storage data code;repair bandwidth;code based kuramoto;bandwidth nodes feasible;based kuramoto code;nodes approximately code;kuramoto code fully;code reduces repair;kuramoto code;bandwidth nodes;bandwidth nodes approximately;nodes feasible;storage data;distributed;data code;nodes;storage;applied problem distributed;problem distributed;data code applied;nodes feasible values;parameters code based;nodes approximately;reduces repair;code fully", "pdf_keywords": ""}, "808a9c9dece4c21be50f41e6caf50101f2b24b47": {"ta_keywords": "preferences argue experiments;properties human preferences;human preferences;preferences argue;mathematical model preferences;model preferences argue;testing decision makers;testing decision;human preferences highlight;preferences highlight computational;empirical properties human;decision makers consistent;experimental psychologists artificial;artificial intelligence researchers;argue experiments;model preferences;experiments used validate;study empirical properties;decision makers;preferences;argue experiments used;experiments;experimental psychologists;artificial intelligence;psychologists artificial intelligence;psychologists artificial;intelligence researchers;experimental;study empirical;comparative study empirical", "pdf_keywords": ""}, "104f75283ae9027eb478e7984bd26b680277ce6f": {"ta_keywords": "sequential action decoding;training sampled actions;navigation method stochastic;action decoding;actions training sampled;challenge robust navigation;learn text representations;robust navigation;sampled actions agent;sampling scheme learn;actions agent learn;scheme learn text;agent learn correct;stochastic sampling;robust navigation method;agent learn;method stochastic sampling;text representations;training sampled;text representations generalize;stochastic sampling scheme;sampled actions;expert actions training;learn text;decoding;navigation;address challenge robust;large scale stochastic;navigation method;actions training", "pdf_keywords": "sampling pretrained language;pretrained language models;action sampling pretrained;language models learn;language model reconstruction;models learn text;natural language navigation;vision language navigation;training sampled actions;actions training sampled;natural language instruction;language navigation agent;learns accurate;language navigation;training sampled;pretrained language;instruction level navigation;embedded pretrained language;learns;training agents navigate;sampling pretrained;stochastic action sampling;learns accurate flexible;memory language model;pretrained language encoder;memory language;learn text representations;test agent learn;action sampling;propose stochastic sampling"}, "f83ef3250ba1166d7c1c7585da7dd78e0641fae7": {"ta_keywords": "music applied generate;music generation;music generation framework;track music generation;multi track music;networks gans proposed;networks gans;generative adversarial networks;generative adversarial;gans proposed models;track music;symbolic multi track;applied generate piano;generate piano;rock music;generation framework generative;framework generative adversarial;gans proposed;music;adversarial networks gans;generative;rock music applied;multi track;gans;jamming model composer;tracks;bars rock music;music applied;adversarial networks;generate piano rolls", "pdf_keywords": "model generate music;track music generation;generate music;music generated;method generate music;model generation music;music generation framework;music model;generate music polyphonic;music generation;music model based;polyphonic music model;generate music multi;track music;music polyphonic music;cooperative generation music;data preprocessing music;networks gans proposed;networks gans;generation music;multi track music;polyphonic music;gans generating;networks gans known;scratch music generated;music polyphonic;musical performance models;generator captures musical;generating multi track;music multi track"}, "199f383e9acd62649121ccde1e06631ce62c89e9": {"ta_keywords": "complexity secret sharing;secret sharing network;algorithm secret sharing;secret sharing general;secret sharing;node secret sharing;sharing dealer network;secret sharing dealer;communication complexity secret;sharing general networks;efficient algorithm secret;sharing dealer;complexity secret;algorithm secret;sharing network;dealer network;node secret;bounds communication complexity;network derive information;general network;networks satisfy condition;node node secret;communication complexity;general networks satisfy;dealer node node;general network derive;dealer node;networks satisfy;dealer participants;lower bounds communication", "pdf_keywords": ""}, "645bc7a5347a299a1e8aa965867bd097f6f4bddd": {"ta_keywords": "agents language guided;agent simulate navigation;agent answers guiding;guiding agents language;answers guiding agent;models navigation agent;navigation agent simulate;navigation agent;model guiding agents;agent assumed navigate;agent model guiding;agents language;demonstrate guiding agent;guiding agents;language guided;agent answers;language guided environment;second agent answers;guiding agent;questions second agent;guiding agent turn;guiding agent assumed;agent simulate;reinforcement learning reward;guiding agent make;introduce agent model;agent model;making reinforcement learning;simulate navigation;environment guiding agent", "pdf_keywords": "guide dialogue model;dialogue model;dialogue process;dialogue navigation;human dialogue paradigm;agent simulate answers;recursive mental model;agent dialogue context;human dialogues;dialogue navigation using;human dialogue;dialogue paradigm;agent answer questions;agent dialogue;propose recursive mental;language navigation agent;dialogue process used;integral dialogue process;human dialogues common;describing language navigation;based recursive mental;dialogue exchanges based;recursive mental;dialogues;questions guiding agent;formulation dialogue exchanges;dialogue exchanges;dialogue guide;dialogue paradigm augment;dialogue guide seeks"}, "d00a403028eb0786915dab7a76692e5eeadf60be": {"ta_keywords": "transductive transfer learning;unsupervised transductive transfer;transfer learning protein;transfer learning data;transfer learning;learning protein extraction;problem transfer learning;transductive transfer;inductive transductive approaches;learning protein;unsupervised transductive;problem unsupervised transductive;inductive transductive;transductive approaches;learning data target;transductive approaches adapt;transductive;iterative feature transformation;art inductive transductive;models problem transfer;learning data;feature transformation;protein extraction;domain available training;feature transformation ift;protein extraction process;iterative feature;data target domain;transfer;technique iterative feature", "pdf_keywords": ""}, "7abcc79e10ff651ef59dea84d347fa64c51e11b2": {"ta_keywords": "assembly organic heterostructures;organic heterostructures approach;organic heterostructures;self assembly organic;self assembly types;proposed self assembly;approach self assembly;self assembly strategy;sequential self assembly;self assembly;simulation self assembly;branching axial structures;heterostructures;assembly geometries;heterostructures approach;assembly geometries approach;assembly organic;structures proposed self;heterostructures approach based;branch axial structures;assembly types branching;rates assembly geometries;single branch axial;structures bilateral multi;multi branch axial;assembly types;branching axial;bilateral single branch;structures including single;axial structures bilateral", "pdf_keywords": ""}, "31b3e84f0a66e27c53c7fe403a0c6cd2319ed797": {"ta_keywords": "parallel learning;parallel learning framework;novel parallel learning;novel probabilistic logic;learning framework solving;probabilistic logic;novel probabilistic;probabilistic logic called;learning framework;use novel probabilistic;probabilistic;learning;novel parallel;solving problems;parallel;present novel parallel;involving thousands unknowns;framework solving;thousands unknowns;thousands unknowns framework;framework solving problems;unknowns framework based;unknowns;logic;logic called;solving problems involving;unknowns framework;solving;problems involving thousands;logic called t_", "pdf_keywords": ""}, "805e49c7282b847faee048a63c1f43ceb08f5257": {"ta_keywords": "voting systems;voting systems proposed;study voting systems;preferences number candidates;lower number candidates;candidates significantly larger;number candidates significantly;candidates ratio significantly;candidates ratio lower;candidates ratio;larger number candidates;number candidates;candidates significantly;number candidates ratio;study voting;candidates ratio number;voting;number reported preferences;candidates;reported preferences number;reported preferences;preferences number;ratio number candidates;significantly lower number;preferences;significantly larger number;number reported;discrepancy number reported;observed discrepancy number;systems proposed", "pdf_keywords": ""}, "641af3bc3cc17993dc72098725d2eb9c0d98049d": {"ta_keywords": "non trivial graphs;trivial graphs related;trivial graphs;trivial graphs power;non trivial graph;graph spacing density;clustering properties topologically;graphs related density;trivial graph;graph spacing;xmath2 graph spacing;graph spacing xmath3;xmath3 graph spacing;spacing xmath3 graph;xmath1 graph size;trivial graph form;graph size xmath2;size xmath2 graph;graphs related;graphs power law;xmath1 graph;xmath2 graph;topologically non trivial;graphs;graphs power;clustering properties;graph size;xmath0 xmath1 graph;xmath3 graph;density non trivial", "pdf_keywords": ""}, "84f2cfbc142ad3165ea3bcacd189a3d1110660e0": {"ta_keywords": "scalable segmentation data;scalable segmentation;segmentation data represented;scalable scalable segmentation;monotone hierarchical segmentation;hierarchical segmentation data;segmentation data;hierarchical segmentation;segmentation;segmentation data presented;corresponding clustering;separate encoders proposed;clustering;represented separate encoders;separate encoders;nodes corresponding clustering;clustering nodes;data represented separate;clustering nodes corresponding;nodes clustering;characterized clustering;clustering relevant nodes;characterized clustering relevant;encoders proposed framework;clustering relevant;nodes clustering relevant;framework characterized clustering;relevant nodes clustering;framework illustrated clustering;encoders proposed", "pdf_keywords": "neural network speech;attention network;attention model speech;hierarchical attention network;sensors attention network;based attention network;network speech recognition;attention model trained;speech recognition;attention network used;scale speech recognition;combines deep neural;attention model;model speech recognition;automatic speech recognition;speech recognition proposed;speech recognition asr;attention model flexible;speech processing;deep neural network;attention channel approach;speech encoders;model attention model;speech recognition propose;ctc attention model;attention attention based;combination attention based;based hierarchical attention;speech recognition problem;network speech"}, "ceefd51b4b391668e313afe8edb3588197002e37": {"ta_keywords": "generate filtered speech;speech synthesis proposed;based speech synthesis;filtered speech parameter;speech synthesis hmm;speech synthesis;conventional speech synthesis;speech parameter sequence;speech parameter;synthesis hmm based;filtered speech;synthesis hmm;modulation spectrum trajectory;sequence based modulation;hmm based speech;hybrid conventional speech;spectrum trajectory proposed;based modulation spectrum;parameter sequence based;spectrum trajectory new;spectrum trajectory;based speech;method based modulation;generate filtered;modulation spectrum;method generate filtered;based modulation;parameter sequence;synthesis proposed method;synthesis proposed", "pdf_keywords": ""}, "2ec99c834bd67ac64ec04b426e5f9fd04f639024": {"ta_keywords": "crowdsourced audio transcriptions;datasets crowdsourced audio;crowdsourced audio;constructing datasets crowdsourced;datasets crowdsourced;quality datasets crowdsourced;data collection crowdsourcing;audio transcriptions;audio transcriptions build;collection crowdsourcing;crowdsourced;datasets formalized language;crowdsourcing;replicating datasets formalized;audio transcriptions novel;datasets formalized;high quality datasets;constructing datasets;constructing replicating datasets;pipeline constructing datasets;quality datasets;replicating datasets;datasets;transcriptions;transcriptions build;resourced language constructing;transcriptions novel domain;transcriptions build principled;audio;transcriptions novel", "pdf_keywords": ""}, "4375cccdfaf2ce3d013e4129d39f7801ef8a468e": {"ta_keywords": "parallel translation tasks;parallel translation shared;parallel translation;results parallel translation;workshop parallel translation;parallel translation held;translation shared tasks;translation tasks presented;translation tasks;translation shared;tasks translation translation;tasks translation;translation translation translation;translation present results;translation translation present;following tasks translation;translation held;translation translation;translation present;translation held beijing;present results parallel;results parallel;translation;parallel;6th workshop parallel;workshop parallel;shared tasks following;tasks following;following tasks;shared tasks", "pdf_keywords": ""}, "50a1dd504037463578f6ba8ee40afe4143f3d6fa": {"ta_keywords": "planar sphere bound;optimal bound norm;sphere bound positive;spheres bound positive;sphere bound;spheres bound obtained;planar spheres positive;spheres bound;vector planar sphere;class spheres bound;bound norm vector;norm vector planar;class planar spheres;planar sphere;planar spheres;spheres positive small;class spheres positive;spheres positive;bound norm;optimal bound;large class spheres;small class spheres;finding optimal bound;norm vector;class spheres;norm;sphere;bound positive;vector planar;spheres", "pdf_keywords": "bose einstein condensate;bosonic xmath2 state;phase transition bose;interaction investigated bosonic;einstein condensate periodic;transition bose einstein;bosonic presence repulsive;transition bose;bosonic described xmath1;condensate periodic potential;bosonic xmath2;condensate atoms;xmath2 condensate;condensate periodic;einstein condensate;xmath1 condensate;xmath0 xmath1 condensate;represented condensate atoms;investigated bosonic described;described xmath1 bosonic;condensate atoms condensate;xmath0 bosonic presence;behavior xmath0 bosonic;condensate ground state;xmath1 bosonic;xmath1 bosonic xmath2;atoms condensate;investigated bosonic;studied represented condensate;condensatethe behavior xmath0"}, "fac2368c2ec81ef82fd168d49a0def2f8d1ec7d8": {"ta_keywords": "entity recognition relation;entity recognition;named entity recognition;relation extraction;recognition relation extraction;information extraction;information extraction including;studying connections entities;database mining_;language database mining_;entities given context;connections entities given;connections entities;named entity;extraction including named;entities given;entities;database mining_ develop;including named entity;database mining_ apply;database;language database;framework studying connections;entity;tasks information extraction;using language database;relation;mining_ apply framework;studying connections;context using language", "pdf_keywords": "event extraction prediction;event extraction;relation extraction event;extraction event extraction;natural language processing;event extraction combines;event extractionwe propose;entity recognition;extraction event detection;recognition relation extraction;named entity recognition;event extraction event;perform event extraction;improves event extractionwe;entity recognition relation;event extractionwe;event extraction perform;methods event extraction;extracting information sentences;sentence representations;relation extraction;relation extraction performance;framework event extraction;improves relation extraction;relation spans text;sentence representations graph;entity recognition performance;method event extraction;task event prediction;multi sentence representations"}, "25ee819bc444b02db43fcbeced982c975edee033": {"ta_keywords": "characterizing crowdsourced tasks;crowdsourced tasks;crowdsourced tasks design;characterizing crowdsourced;problem characterizing crowdsourced;binary task labels;crowdsourced;clustering weighted majority;voting proposed inference;worker clustering weighted;weighted majority voting;recovers binary task;binary task;inference algorithm recovers;tasks design inference;worker clustering;task labels;weighted majority;task labels given;clustering weighted;worker task;using worker clustering;information worker task;labels given recovery;tasks;algorithm recovers;worker task types;task;tasks design;accuracy using worker", "pdf_keywords": "crowdsourced binary tasks;algorithm recovers crowdsourced;task propose clustering;matched type tasks;crowdsourced labeling type;voting optimal baselines;worker task specialization;labeling type worker;clustering selecting workers;recovers crowdsourced;problem crowdsourced labeling;voting optimal oracle;cluster voting;cluster given task;crowdsourced labeling;oracle weighted majority;matching task based;recovers crowdsourced binary;based cluster voting;estimation weighted majority;weighted majority voting;performance weighted majority;task characterizing clustering;type worker task;tasks literature clustering;clustering worker skill;tasks matched type;jority voting optimal;worker clustering;majority voting workers"}, "6b13c4ac18f621155a550238a037a670bdce8969": {"ta_keywords": "zeeman field stability;zeeman splitting model;cluster framework zeeman;xmath0he ep cluster;field stability xmath0he;stability xmath0he ep;framework zeeman splitting;zeeman splitting;zeeman field strength;stability xmath0he;effect zeeman field;framework zeeman;strength stability xmath0he;zeeman field;field stability;xmath0he ep;effect zeeman;choice zeeman field;cluster determined interplay;zeeman;cluster determined;ep cluster framework;suitable choice zeeman;ep cluster;ep cluster determined;cluster framework;ep cluster minimized;effect field stability;splitting model;field strength stability", "pdf_keywords": ""}, "06d0af396fb08caa6a665dd476380aa16b6199b2": {"ta_keywords": "phase space quantum;computing phase space;quantum based application;interacting particles qubit;quantum;particles qubit;reversible measurement method;local reversible measurement;quantum based;phase space;computing phase;method computing phase;space quantum;space quantum based;reversible measurement;qubit;local measurement method;non local measurement;local measurement;local measurement evaluated;measurement method;measurement method applied;measurement method based;phase;measurement evaluated;non local reversible;local reversible;measurement evaluated suitable;applied interacting particles;measurement", "pdf_keywords": ""}, "3e33c988969b4c9f1d9af8c1c0f7644a30d0311f": {"ta_keywords": "high speed translation;speed translation signal;speed translation able;translation signal proposed;speed translation;translation signal;translation able translate;able translate sentences;phase shift sensor;nonlinear phase shift;translate sentences natural;translation able;translate sentences;phase shift sensors;shift sensor;phase shift;shift sensors nonlinear;shift sensor provide;shift sensors;shift sensors provide;sensors nonlinear phase;using nonlinear phase;able translate;translate;shift;design high speed;translation;nonlinear phase;high speed;provide high speed", "pdf_keywords": ""}, "6fb3d5a48be16fe1a4cff5e83093b77fbcd1013b": {"ta_keywords": "smart grid adversary;privacy metric adversary;adversary infer private;private parameter data;grid adversary infer;infer private parameter;collected smart grid;grid adversary;smart grid;smartmeter data restricted;privacy metric;new privacy metric;small adversary infer;private parameter;metric adversary infer;thermostatically controlled smartmeter;controlled smartmeter data;adversary infer;metric adversary;collected smartmeter;relatively small adversary;collected smartmeter data;privacy;infer private;small adversary;data collected smartmeter;new privacy;data collected smart;introduce new privacy;data collected thermostatically", "pdf_keywords": ""}, "032e660447156a045ad6cf50272bca46246f4645": {"ta_keywords": "machine translation adaptation;bias output softmax;translation adaptation;translation adaptation achieved;softmax;output softmax;improvements translation accuracy;output softmax user;machine translation;user machine translation;softmax user;softmax user directly;output softmax particular;translation accuracy better;translation accuracy;softmax particular;softmax particular user;adapting bias output;adaptation bias output;demonstrate improvements translation;improvements translation;adapting bias;efficient adaptation bias;achieved adapting bias;adaptation bias;parameter efficient adaptation;speaker traits target;translation;bias output;target text", "pdf_keywords": "speaker adaptation;speaker adaptation process;machine translation;user machine translation;machine translation directly;softmax;allocated speaker adaptation;improvements translation accuracy;modeling speaker explicitly;challenge modeling speaker;preserving linguistic accuracy;predict target sentence;linguistic variations translation;talk based linguistic;output softmax;linguistic features talks;softmax particular;softmax bias;preserving linguistic;translation accuracy better;translation accuracy;parameters softmax;demonstrate improvements translation;parameters softmax bias;output softmax particular;based translation output;given talk based;bias output softmax;parameter efficient adaptation;method based translation"}, "2aad7765250f7d9e312c9382f929ea5239b0fd73": {"ta_keywords": "nonlinear dna molecule;nonlinear dynamics biological;nonlinear dna;use nonlinear dna;simulation nonlinear dynamics;dna molecule coupled;dynamics biological systems;nonlinear dynamics;simulation nonlinear;dynamics biological;molecules coupled environment;coupled bath dna;method simulation nonlinear;dna molecule;molecules coupled;molecule coupled bath;dna like molecules;molecule coupled;molecules molecule coupled;coupling achieved dna;like molecules coupled;achieved dna molecule;nonlinear;dynamics;biological systems method;use nonlinear;biological systems;molecules molecule;like molecules molecule;coupled environment", "pdf_keywords": ""}, "a1d578646cf42f2f69ee996742af484d03cc9121": {"ta_keywords": "machine translation pre;efficient smp5 language;translation pre training;smp5 language model;performance downstream multilingual;data machine translation;smp5 language;parallel data mt5;mt5 pre training;downstream multilingual crosslingual;machine translation;data mt5 pre;downstream multilingual;multilingual crosslingual tasks;translation pre;np efficient smp5;language model;parallel data essential;mt5 pre;language model combination;incorporating parallel data;multilingual;multilingual crosslingual;crosslingual tasks;suggesting parallel data;efficient smp5;parallel data machine;data mt5;training using np;parallel data", "pdf_keywords": "pretraining multilingual;effectiveness pretraining multilingual;training massively multilingual;pretraining multilingual encoderde;language models mt5;training multilingual neural;multilingual neural machine;pre training multilingual;training multilingual;performance downstream multilingual;multilingual language models;multilingual neural;cross lingual tasks;performance crosslingual transfer;corpus present nmt5;massively multilingual language;high performance crosslingual;massively multilingual;performance crosslingual;performance translation task;model cross lingual;multilingual encoderde models;tasks mc4 corpus;training multi tasking;translation encoder training;downstream multilingual cross;multilingual cross lingual;neural machine translation;downstream multilingual;best performance translation"}, "0ba3e29dac0857100935b6eb22bce9cee4afcf17": {"ta_keywords": "faster naive inference;web faster naive;domains normalization faster;global domains normalization;way naive inference;naive inference methods;domains normalization;naive inference;faster way naive;global domains;faster naive;names mapped global;mapped global domains;local names mapped;explains local names;wide web faster;normalization faster;world wide web;data extracted world;local names;whirl explains local;names mapped;normalization faster way;normalization;methods short queries;inference methods short;logic data;web faster;short queries;logic data extracted", "pdf_keywords": ""}, "cec6de30eea5b4a5a414cf99830fbdb5c56a481c": {"ta_keywords": "mixed data storage;vd content distribution;data storage communication;scalable architecture massively;architecture massively parallel;storage communication;storage communication vd;communication vd content;scalable architecture;content distribution;scalable scalable architecture;content distribution goal;parallel mixed data;large scale multi;data storage;propose scalable scalable;network multi;network multi user;multi network multi;multi user multi;scale multi user;vd content;propose scalable;scalable scalable;multi network;massively parallel mixed;multi device multi;user multi network;scalable;architecture massively", "pdf_keywords": ""}, "987658ba918710bbce5de8d92eb44bd127cf72c5": {"ta_keywords": "probabilistic phone phoneme;phoneme mappings approach;phoneme mappings;phone phoneme mappings;interpretable probabilistic phone;phone based speech;speech recognition systems;allophone graphs;differentiable allophone graphs;speech recognition;probabilistic phone;based speech recognition;allophone graphs demonstrate;universal phone based;phone phoneme;differentiable allophone;approach differentiable allophone;construction universal phone;universal phone;allophone;rich pronunciation variations;captures rich pronunciation;recognition systems interpretable;phoneme;pronunciation variations;training diverse languages;recognition systems;diverse languages;diverse languages captures;phone based", "pdf_keywords": "phoneme mappings language;phoneme mappings phonetic;language specific phonemes;probabilistic phone phoneme;language specific phonemic;specific phonemes learn;phoneme level language;multiple phonemes multilingual;phonemes multilingual;predicting phoneme;language specified phoneme;language phoneme specific;phonemes learn probabilistic;phoneme prediction;languages known phoneme;phoneme disambiguations improves;supporting language phoneme;phone phoneme disambiguations;phonetic realizations phonemes;phones language specific;multilingual phone based;phoneme mappings;phoneme disambiguations;phone phoneme prediction;phones language;lexicons allophone discovery;phoneme annotations;multilingual phone;phonemes multilingual setting;specific phonemes approach"}, "c0a32c68b992b44f1812492c95ac91fb62a6df37": {"ta_keywords": "gradient based bandits;guarantees competitive gradient;gradient reinforcement learning;competitive gradient;reinforcement learning gradient;online convex optimization;competitive gradient based;policy gradient reinforcement;gradient reinforcement;framework competitive gradient;policy gradient;online convex;algorithms information gradient;certain online convex;including policy gradient;convex optimization algorithms;learning gradient;morse smale games;information gradient free;bandits certain online;algorithms including policy;learning gradient based;information gradient;convex optimization;reinforcement learning;based bandits;gradient based learning;based bandits certain;gradient free;gradient based", "pdf_keywords": ""}, "01138945dc9de691cd559d09a46597cca7659efb": {"ta_keywords": "fairness data collection;bias fairness data;fairness data;use bias fairness;bias fairness;use bias;basics use bias;bias;fairness;data science social;data science;data collection management;data collection;fields data science;data;collection management;social science;algorithmic advances;future algorithmic;algorithmic;future algorithmic advances;science social science;algorithmic advances fields;management;advances fields data;collection;collection management eye;science social;fields data;social", "pdf_keywords": ""}, "ca7cd3a90d2953b2f8e45686afa3e79eb3a39add": {"ta_keywords": "predicting edit completion;predict edit completion;predicting edit;approach predicting edit;edits predict edit;predict edit;trained past edits;edit completion code;past edits predict;edits predict;completion code snippet;edit completion use;code snippet approach;edit completion;completion code;snippet approach;edits generated;code snippet;past edits;snippet approach use;edits generated publicly;approach dataset edits;completed code publicly;dataset edits generated;completion use;novel approach predicting;dataset edits;fully completed code;completion use model;code", "pdf_keywords": "predicting edit completions;predicting edits code;novel approach editcompletion;edits code completion;predicting edits program;editcompletion computationally;predict edit program;task editcompletion computationally;predict additional edits;editcompletion computationally challenging;editcompletion predict;prediction edits program;editcompletion task predict;editcompletion predict additional;predict edit needs;predict edits;approach predicting edits;code completion examples;code completion;representation predict edits;predicting edits;approach predicting edit;method predicting edit;able predict edits;model predicting edits;predicting edit;edit completions code;learning edit code;task predict edits;learns predict edit"}, "d7bebb71635cb818d2f5e0ca0a70434283deb4b6": {"ta_keywords": "humanlike behavior multiagent;behavior multiagent decision;regret minimization algorithm;regret minimization;novel regret minimization;multiagent decision;multiagent decision making;human behavior accuracy;behavior multiagent;accuracy inferred actions;inferred actions accuracy;inferred actions person;behavior accuracy;strong humanlike behavior;humanlike behavior;inferred actions;behavior accuracy inferred;actions accuracy inferred;actions depends accuracy;human behavior;actions accuracy;modeling strong humanlike;task modeling strong;inferred actions depends;multiagent;consider task modeling;task modeling;decision making problems;inferred actions introduce;minimization algorithm regularized", "pdf_keywords": "strategy modeling humans;learning models games;behavior imitation learning;human strategy players;strong reinforcement learning;imitation learning il;imitation learning;policy large game;modeling behavior players;search based reinforcement;reinforcement learning agents;human behavior imitation;strategy models players;strategy search player;modeling humans games;learning agents;model strong reinforcement;model behavior game;games use reinforcement;strong humanlike policies;strategy predicting behavior;method learning games;strategy search;extending reinforcement learning;human strategy;reinforcement learning new;strategy modeling strong;empirical behavior game;players choose strategies;implementation human game"}, "4d7a50f6cfd8f27ebd4d5201fad6c5ef42c33733": {"ta_keywords": "predictions patient information;hospital mortality predictions;embed patient information;accurate hospital mortality;mortality predictions patient;patient information represented;hospital mortality;information clinical notes;patient information;patient information clinical;health records ehealth;mortality predictions;predictions patient;model embed patient;clinical notes collected;information clinical;bert model;electronic health records;make accurate hospital;bert model embed;intensive care;clinical notes;records ehealth;intensive care unit;clinical notes combine;records ehealth use;tuned bert model;collected intensive care;health records;accurate hospital", "pdf_keywords": ""}, "1cf2e9e198feef3893da2800a7949f6880ddc084": {"ta_keywords": "learning based explainaboard;summaries explainable graphs;based explainaboard allows;explainable graphs;explainaboard allows users;plots leaderboards possible;plots leaderboards;based explainaboard;functional learning based;plot plots leaderboards;functional learning;explainaboard allows;leaderboards;leaderboards possible extract;graphs;leaderboards possible;plots;learning based;extract meaningful insights;extract interesting insights;insights data form;tasks possible;tasks possible extract;summaries explainable;insights data;novel functional learning;implementation novel functional;interesting insights data;meaningful insights data;different tasks possible", "pdf_keywords": "performance natural language;nlp systems;able predict linguistic;leaderboards predicting performance;nlp systems simple;processing nlp systems;leaderboards predicting;new leaderboards predicting;predict linguistic;predict structure linguistic;predicting linguistic;computational linguistic performance;approach predicting linguistic;processing nlp;prediction computational linguistic;large corpus nonlinear;natural language processing;linguistic performance large;predict linguistic structure;leaderboard project;language processing nlp;corpus nonlinear;natural language;new leaderboard project;computational linguistic;leaderboards;predicting linguistic structure;future leaderboards;ability predict linguistic;functionality standard leaderboards"}, "5665d864d0f1bce6672d6d2bf9f8d8646093cb37": {"ta_keywords": "semantic parsing challenge;semantic parsing;parsing challenge;parsing;represented sequence sequences;claim represented sequence;version semantic parsing;sequences numbers context;numbers generally sequence;sequences numbers generally;generally sequence sequences;numbers context fast;semantic;sequence sequences;generally sequence;represented sequence;sequence sequences numbers;sequences;sequences numbers;sequence;furious version semantic;numbers context;version semantic;numbers;numbers generally;given claim represented;context fast;claim represented;challenge;verify given claim", "pdf_keywords": ""}, "e050cd9cec5eed73bd56cb2c9726ea85e985384b": {"ta_keywords": "incremental sentence compression;sentence compression using;sentence compression;short term memory;recurrent neural networks;memory recurrent neural;pretrained autoencoder;autoencoder;term memory recurrent;recurrent neural;pretrained autoencoder experimental;network pretrained autoencoder;memory recurrent;performing incremental sentence;compression using long;autoencoder experimental;incremental sentence;autoencoder experimental results;neural networks rsns;term memory;recurrent;compression using;using long short;compression;pretraining method network;obtains compression;long short term;pretraining method;compression rates;human references better", "pdf_keywords": ""}, "63567f348231abed171c02f99d4c49c2892a2ade": {"ta_keywords": "private deep learning;differentially private deep;accuracy results privacy;differentially private;results privacy;impact differentially private;model privacy;privacy data guaranteed;private deep;results privacy model;privacy model guaranteed;model privacy data;results depends privacy;privacy;privacy model privacy;privacy data;privacy model;deep learning accuracy;depends privacy model;private;deep learning;depends privacy;trained data accuracy;learning accuracy results;learning accuracy;data guaranteed;guaranteed accuracy;trained data;data guaranteed demonstrate;model guaranteed accuracy", "pdf_keywords": "learning differential privacy;differential privacy accuracy;differential privacy crucial;privacy accuracy deep;sgd differential privacy;model differential privacy;differential privacy;differential privacy model;differential privacy huge;differential privacy guarantees;differential privacy degrades;impacts differential privacy;impact differential privacy;imposing differential privacy;privacy model accuracy;privacy accuracy;deep learning differential;differentially logistic regression;regression differential sgd;differential sgd;privacy large class;imbalances loose privacy;guarantees increasing privacy;privacy crucial;privacy guarantees huge;increasing privacy;privacy large;privacy;privacy budget;differential sgd study"}, "d558c6b953e0267781ed5da90a35c122ba360f10": {"ta_keywords": "lingual neural networks;complex text identification;cross lingual neural;neural networks parallel;monolingual cross lingual;lingual neural;text identification problem;complex text;cross lingual;text identification;lingual;performance monolingual cross;parallel version complex;monolingual cross;performance monolingual;networks parallel;neural networks;parallel;parallel version;monolingual;version complex text;simple learning models;networks parallel version;simple learning;learning models;features simple learning;problem simple learning;text;neural;study performance monolingual", "pdf_keywords": "word identification shared;monolingual approach feature;language shared task;regression languages text;trained crosslingual dataset;word identification task;predict words;model predict words;predicting complex words;predict words target;crosslingual model predict;predict complex words;features target word;model trained crosslingual;abstract word identification;language similar accuracy;words dataset crucial;crosslingual dataset;regression languages;identification shared task;languages crosslingual model;perform regression languages;text identification models;languages use predict;crosslingual dataset lps;words languages crosslingual;monolingual cross lingual;text model trained;lingual complex text;language shared"}, "d33d6c16d7c34dd387841efca74b457b7e60933a": {"ta_keywords": "learning recursive logic;recursive logic programs;recursive logic;logic programs force2;linear recursive logic;logic programs;learning recursive;logic programs given;algorithm learning recursive;recursive;programs force2 based;linear recursive;examples force2 quickly;forced simulation;new algorithm learning;programs force2;force2 quickly;force2 based;technique forced simulation;algorithm learning;forced simulation given;force2 quickly successfully;examples force2;programs given;force2 based technique;logic;quickly successfully learn;simulation;class linear recursive;force2", "pdf_keywords": ""}, "68ca176c7566067ae4b3311957cc4a134bfbc819": {"ta_keywords": "neural cognitive architecture;cognitive architecture;cognition learn continuously;propose neural cognitive;cognitive architecture nca;neural cognitive;cognition learn;human cognition learn;propose neural;learning solve evolving;cognition;learn continuously solve;human cognition;inspired human cognition;learn continuously;neural;thesis propose neural;cognitive;learning solve;combined self supervision;self supervision;self supervision problems;solve multiple problems;learning;nca learning solve;supervision combined self;architecture;solve evolving automatically;evolving automatically;distinct perception action", "pdf_keywords": ""}, "163c6b06d948d0869eb8173b537c441c9a786977": {"ta_keywords": "payoffs congestion game;congestion game based;constructing payoffs congestion;congestion game public;congestion game;game public transportation;payoffs congestion;public transportation payoffs;version congestion game;transportation payoffs;game payoffs computed;transportation payoffs computed;game payoffs;congestion;method constructing payoffs;constructing payoffs;used game payoffs;public transportation;constructing payoff;payoff strategy;constructing payoff payoff;payoff strategy used;payoff payoff strategy;payoffs computed using;use population constraints;population constraints;payoffs computed;illustrated constructing payoff;simulated version congestion;game public", "pdf_keywords": "monotone congestion game;equilibrium congestion game;stochastic population games;game equilibrium congestion;games monotone congestion;continuous population games;congestion games equivalent;equilibrium congestion;congestion game;congestion games;agents solve congestion;population games continuous;mdl congestion games;congestion game augmenting;monotone congestion;game satisfy population;solve congestion;congestion game mdl;population selfish agents;population games;population games strategies;equilibrium routing games;games continuous population;congestion process population;congestion game known;continuous population selfish;equilibrium dynamics game;equilibrium game;solve congestion process;multiplicative game population"}, "2b63812db40152b12925ce4a848b929fa591b858": {"ta_keywords": "sequence xmath0 bits;sequence xmath2 bits;sequence xmath4 bits;bits sequence generated;sequence xmath1 bits;bits generated sequence;sequence xmath3 bits;translation data sequences;xmath1 bits sequence;bits sequence;sequences sequence xmath0;sequence xmath0;generated sequence xmath1;generated sequence xmath2;generated sequence xmath4;xmath0 bits generated;data sequences method;generated sequence xmath3;data sequences;sequence xmath2;xmath0 bits;sequence xmath4;sequence xmath1;sequence xmath3;xmath2 bits generated;xmath3 bits generated;xmath4 bits;xmath1 bits;xmath2 bits;bits generated", "pdf_keywords": "statistical machine translation;machine translation sequence;neural machine translation;translation sequence prediction;machine translation neural;machine translation method;machine translation;translation neural sequence;machine translation algorithms;output machine translation;machine translation algorithm;techniques machine translation;translation neural;machine translation standard;sequence languages models;task machine translation;machine translation applications;translation algorithms;methods machine translation;generating translation;dependency machine translation;translation output machine;probabilities language model;translation model;language modeling;machine translation smd;generating translation output;machine translation followed;machine translation nnt;accuracy language models"}, "8b608ad2ec6d0300b6a0bb8f616d4a2b01150693": {"ta_keywords": "track changes language;topic tracking technique;topic tracking;language content meeting;based topic tracking;changes content meeting;content meeting event;track changes content;track changes;content meeting method;meeting event;meeting event proposed;speech recognition;changes language;meeting method based;speech recognition performance;changes language content;method track changes;content meeting;speech;meeting method;language content;event proposed method;event proposed;method illustrated speech;tracking technique used;used track changes;illustrated speech recognition;tracking;tracking technique", "pdf_keywords": ""}, "0a4b8b161931799d5c6bc3ecf07c53bae0e9e502": {"ta_keywords": "quality text corpora;quality content text;quality text;quality text text;high quality text;quality content;text corpora high;high quality content;measuring quality high;measuring quality;corpora high quality;select high quality;method measuring quality;text corpora;factuality literary acclaim;metrics factuality literary;quality high;preferred quality;quality high quality;text use novel;content text;quality;quality fil011 ter;quality fil011;preferred quality fil011;text text;use novel filtering;novel filtering;literary acclaim;factuality literary", "pdf_keywords": "news sources evaluation;corpus high quality;quality text language;quality news articles;quality news sources;text language ideologies;factual filtered articles;translation wikipedia newswire;evaluates quality text;language ideologies robust;pronouns document quality;corpora language models;filter published literature;corpora text collection;quality text based;privileging corpus;linguistic properties corpora;training corpora language;factuality literary acclaim;corpus;correlates news sources;effective characterizing linguistic;text collection newspaper;language models better;corpora language;corpora text;filtered articles predominantly;quality news;wikipedia newswire;high quality news"}, "7a737872a6693ba3f0c99651191b93dad0dadcee": {"ta_keywords": "laplacian xmath0 ray;xmath2 ray data;xmath0 ray data;xmath1 ray data;data xmath2 ray;ray data xmath2;data xmath1 ray;ray data xmath3d;xmath3d data;ray data xmath1;xmath3d data xmath4d;xmath8d data;data xmath3d data;xmath4d data;xmath5d data;xmath9d data;data xmath3d;xmath8d data xmath9d;xmath7d data xmath8d;xmath7d data;xmath6d data;laplacian xmath0;xmath2 ray;xmath4d data xmath5d;data xmath8d data;xmath5d data xmath6d;xmath0 ray;xmath9d data xmath;xmath6d data xmath7d;data xmath4d data", "pdf_keywords": "training speech models;training dereverberation audio;diarization voice activity;outputs speaker diarization;speaker diarization;empirical speech representation;speaker diarization approximated;method training speech;training speech;speech representation;diarization voice;speech models;speech models based;representation measured speech;speaker diarization large;audio model train;speaker diarization voice;deep learning;speech representation eend;speech estimate;dereverberation audio;neural network tdnn;measured speech estimate;problem speaker diarization;convolutional neural network;speaker diarization sequence;state art convolutional;neural network snn;dereverberation audio recording;convolutional neural"}, "4d44f2c3f269ea6cbc840b99c3f8119a13829509": {"ta_keywords": "polarized particle beam;spin polarized particle;dynamics spin polarized;polarized particle;background polarized particle;spin polarized;beam vicinity polarized;determine spin polarized;continuum particle beam;particle beam;polarized background polarized;particle beam assumed;particle beam vicinity;continuum polarized background;dynamics spin;assumed continuum polarized;continuum polarized;study dynamics spin;vicinity polarized;vicinity polarized background;polarized background assumed;polarized background;beam assumed continuum;polarized background used;continuum particle;polarized;assumed continuum particle;background polarized;determine spin;spin", "pdf_keywords": ""}, "94c0a8be74d69787a1f3f6e91dcb480a2fd0dd56": {"ta_keywords": "natural language benchmarks;language models based;language benchmarks demonstrate;language models;pretraining paradigm language;discovery language model;program model pre;data driven theorems;language model;language benchmarks;language model able;pre trained program;model pre trained;discovery language;variety natural language;paradigm language models;natural language;relies discovery language;pretraining paradigm;program model;new pretraining paradigm;pretraining;driven theorems approach;trained program;result program model;pre trained;data driven;new pretraining;present new pretraining;program resulting architecture", "pdf_keywords": "boosting reasoning capabilities;program reasoning instantiations;boosting reasoning capability;exemplary program reasoning;reasoning context computational;reasoning enhancement pre;advanced reasoning capabilities;various reasoning capabilities;reasoning instantiations various;reasoning capability language;reasoning capabilities;reasoning instantiations;reasoning tasks including;paradigm boosting reasoning;reasoning capability;reasoning tasks;reasoning capability present;reasoning enhancement;improves performance reasoning;reasoning capabilities work;framework boosting reasoning;reasoning dataset;reasoning capacities;language models programs;reasoning capacities executor;executors natural language;reasoning context dataset;boosting reasoning;program reasoning;reasoning capabilities linear"}, "de41f897ea6ca5447cfae81e9505f94ccf50e6a5": {"ta_keywords": "heisenberg antiferromagnet spin;spin heisenberg antiferromagnet;antiferromagnet spin orbit;antiferromagnet spin;heisenberg antiferromagnet;zeeman field spin;coupling spin heisenberg;spin orbit coupling;coupling spin;orbit coupling spin;antiferromagnet;spin heisenberg;coupling strength spin;effect zeeman field;field spin orbit;field spin;strength spin orbit;spin orbit;control spin orbit;affected field spin;effect zeeman;control spin;zeeman field;spin;orbit coupling strongly;zeeman;orbit coupling used;orbit coupling;strength spin;orbit coupling strength", "pdf_keywords": ""}, "0a4dd1e51616b422aa2d437610dbfbdd3733a114": {"ta_keywords": "dialog agent;novel dialog agent;dialog agent utilizes;dialog management techniques;dialog management;approaches dialog management;dialog management including;including dialog management;human conversation examples;dialog;conversation examples movies;management including dialog;human human conversation;approaches dialog;human conversation;examples movies twitter;novel dialog;including dialog;semantic similarity retrieval;driven approaches dialog;conversation examples;syntactic semantic similarity;similarity retrieval tf;movies twitter;similarity retrieval;statistical machine translation;similarity retrieval statistical;semantic similarity;present novel dialog;machine translation", "pdf_keywords": ""}, "d5ec188a5a39e504788c1fe33457eeb816a99f31": {"ta_keywords": "models parsing constituency;unsupervised grammar induction;dependency structure grammars;grammar induction model;structure grammars;grammar induction;unsupervised grammar;parsing constituency;constituency smaller grammar;learn constituency structure;propose unsupervised grammar;structure grammars experiments;constituency structure dependency;models parsing;parsing constituency smaller;grammars;smaller grammar;word concreteness structural;grammars experiments;grammar;grammars experiments proposed;jointly learn constituency;constituency structure;parsing;structure dependency structure;smaller grammar size;learn constituency;grammar size;structure dependency;leverages word concreteness", "pdf_keywords": "learns constituency grammar;constituency grammar induction;model parsing sentences;grammar induction focuses;dependency structure grammars;constituency parsing accuracy;grammar induction model;improve constituency parsing;unsupervised grammar induction;constituency parsing;structure sentence neural;syntax capable learning;constituency parsing performance;learning syntax constraints;learning phrasal dependency;grammar induction pure;accuracy grammar induction;learning syntax corresponding;sentence neural;learning syntax;grammar induction;constituency grammar;predict syntactic structure;syntactic parsing;learns constituency;sentence neural pcfg;grounded syntax models;parsing sentences;dependency parsing;structure grammars"}, "d878828c2345b665ab9651f20fb0e60e1ffe9de5": {"ta_keywords": "detection strains heterostructures;detection strains heterostructure;heterostructure frequency xmath0;lattice mismatch heterostructure;strains heterostructures;strains heterostructures method;strains heterostructure;strains heterostructure frequency;mismatch heterostructure;heterostructures method based;mismatch heterostructure interface;heterostructures method;heterostructure interface sensitivity;heterostructures;heterostructure;heterostructure frequency;heterostructure interface;magnetometer capable probing;magnetometer demonstrated detection;interface sensitivity magnetometer;probing lattice mismatch;magnetometer capable;sensitivity magnetometer demonstrated;magnetometer;sensitivity magnetometer;magnetometer demonstrated;capable probing lattice;custom magnetometer capable;custom magnetometer;use custom magnetometer", "pdf_keywords": ""}, "84e566e326b64b105cabf0c47dff336c4f632a1c": {"ta_keywords": "gaussian particle interacting;harmonic potential particle;harmonic potential model;gaussian particle;dimensional gaussian particle;field harmonic potential;potential particle confined;scalar field harmonic;particle interacting;particle interacting scalar;boltzmann equation particle;harmonics phase;mode harmonic potential;potential particle;particle confined region;harmonics phase diagram;field harmonic;harmonic potential;number harmonics phase;particle confined;potential model consists;coupled mode harmonic;potential model;boltzmann equation;model dimensional gaussian;obtained solving boltzmann;solving boltzmann;interacting scalar field;phase diagram model;dimensional gaussian", "pdf_keywords": ""}, "18268bdfc8a6e0a51f373bc4acf65c8b9a7bd6a0": {"ta_keywords": "particle neural network;prediction behavior particle;predict particle state;predict particle;behavior particle neural;particle neural;able predict particle;neural network model;neural network;particle state;neural network coupled;particle state presence;behavior particle;particle;network coupled noisy;noisy environment model;coupled noisy environment;noisy environment;coupled noisy;state presence noisy;concept neural network;prediction behavior;neural;model prediction;prediction;model prediction behavior;model able predict;noisy;based concept neural;concept neural", "pdf_keywords": ""}, "9e77f94e5a12cb33b8b464dc834fd81da1a609e2": {"ta_keywords": "delay dynamical;dynamics delay dynamical;xmath0 type delay;delay product type;dynamics delay;type delay product;delay product;delay dynamical dds;use delay product;dynamical dds delay;type delay;studies dynamics delay;delay generated;delay generated tex;use delay;delay;based use delay;dds delay generated;product type functional;dds delay;tex xmath0 type;xmath0 type;type functional lkf;xmath0 type lkf;functional lkf tex;tex xmath0;lkf tex xmath0;type functional;functional lkf;generated tex xmath0", "pdf_keywords": ""}, "24219135d563b1cb24523bf522366c91a55d7604": {"ta_keywords": "f1 multilabel classification;thresholding multi label;averaged f1 multilabel;multilabel classification;f1 multilabel;f1 optimal thresholding;multi label setting;multilabel;optimal thresholding multi;multi label;label setting maximum;thresholding multi;label setting;optimal thresholding;skill score classifier;extreme thresholding;thresholding;classifier macro averaged;score classifier macro;f1 optimal;label;classifier;score classifier;thresholding behavior finally;f1 score related;classification;macro averaged f1;possible f1 score;f1 score;maximum possible f1", "pdf_keywords": ""}, "274b4ad4840b0a8a70c5bac3fe4b4861ce5fbb95": {"ta_keywords": "shot relation classification;relation classification;relation classification based;relation classification problem;model relation classification;shot relation;relation;learning model relation;recent shot learning;simple shot learning;methods shot learning;shot learning methods;solve shot relation;shot learning;shot learning problem;shot learning model;present shot learning;classification based recent;supervised learning approach;supervised learning;model relation;learning approach;simple shot;learning methods;classification problem recent;problem recent shot;recent shot;supervised;use supervised learning;learning model", "pdf_keywords": "shot relation classification;supervised shot relation;relation classification task;relation classification new;supervised shot classification;relation classification;relation classification results;related shot tasks;recent shot learning;shot classification tasks;supervised relation extraction;dataset shot relation;task shot learning;distantly supervised relation;supervised relation;view relation classification;shot relation fewrel;shot relation;relation classification problem;shot learning models;related shot;shot classification;learning task shot;shot learning methods;shot learning task;benchmark fewshot learning;shot learning;methods relation classification;new fewshot classification;formulate fewshot learning"}, "78e838bcd2268260ddce6be6db4907df6f29f04f": {"ta_keywords": "expressive text;convey emotional speech;expressive text preferable;expressive text novel;generate text balloons;text balloons;text balloons exist;reveal expressive text;convey emotional information;shape text balloons;emotional speech;convey emotional;text balloons propose;emotional speech means;approach convey emotional;develop convey emotional;expressive;means text balloons;speech means text;linguistic acoustic features;text balloons 87;features based comic;utilize linguistic acoustic;emotional information;text;shape text;results reveal expressive;static text;emotional information means;reveal expressive", "pdf_keywords": ""}, "66f7d22d6373af5032074b25828331958b07e7f9": {"ta_keywords": "networks dnns;networks dnns based;learning networks dnns;dnns based use;deep learning networks;dnns based;deep learning;behavior deep learning;networks;learning networks;nodes network;nodes network model;network model characterized;network model;dnns;characterize nodes model;number nodes network;network;characterize nodes;nodes model;nodes;describing behavior deep;large number nodes;deep;parameters characterize nodes;behavior deep;nodes model able;interpret data large;number nodes;interpret data", "pdf_keywords": "private convolutional neural;differentially private convolutional;explainability convolutional neural;beneficial interpretability noise;explainability convolutional;interpretability noise used;cams explainability convolutional;interpretability noise;differentially private;deep neural;noise convolutional neural;deep neural networks;private convolutional;dnns increasing importance;neural networks dnns;explanation quality privacy;networks dnns;training deep neural;neural networks convolutional;networks dnns increasingly;networks convolutional neural;differentially private dp;accuracy measure privacy;data training deep;training deep;measure privacy model;privacy dimensional space;networks dnns increasing;interpretability models;privacy dimensional"}, "26a238217321008cd1daaa649683d461e16e7574": {"ta_keywords": "policy gradient training;neural networks policy;gradient training;training neural;policy gradient;networks policy gradient;training neural networks;gradient training use;microwave signal processors;improve accuracy transcription;accuracy transcription;improve performance transcription;neural networks;performance transcription transcription;transcription sequences improve;performance transcription;accuracy transcription transcription;text microwave signal;method training neural;transcription;transcription transcription;transcription transcription transcription;transcription transcription sequences;microwave signal particular;novel method training;transcription sequences;text microwave;transcription sequences method;normalizations text microwave;future microwave signal", "pdf_keywords": ""}, "eec490a41bdc716fccf98f4a7996c1d31334985a": {"ta_keywords": "open source library;source library;source library named;present open source;open source;library named;library;open;present open;source;named;present", "pdf_keywords": "datasets music generation;symbolic music generation;datasets music;muspy symbolic music;music generation muspy_;generate music arbitrary;music generation library;large datasets music;generate music;music structure data;possible generate music;symbolic music;music generation systems;tools developing music;symbolic music compare;music generation including;framework music generation;generative representation music;developing music generation;music database;music generation;models music generation;routines music generation;generation music identify;music generation use;case symbolic music;learning models music;music generation called;music arbitrary structure;music arbitrary"}, "dc26c3775d233a5fa9516d21fee12aa5b46f8a25": {"ta_keywords": "recommendation scientific terms;unannotated scientific papers;scientific term extractor;leveraging large unannotated;terms based unsupervised;collection unannotated scientific;large unannotated papers;semi supervised approaches;knowledge graph;unsupervised relational;based unsupervised relational;unannotated scientific;knowledge graph way;unannotated papers;unsupervised relational signals;recommendation scientific;semi supervised;term extractor;unannotated papers obtained;relational signals extracted;small unannotated papers;scientific terms based;multiple semi supervised;term extractor using;construct knowledge graph;unannotated papers applying;approach recommendation scientific;scientific papers trained;scientific papers;scientific terms", "pdf_keywords": "relation extraction scientific;knowledge driven extraction;relation extraction;scientific knowledge graph;extraction knowledge web;effective extracting relations;relation extraction model;relation extraction based;extract knowledge semantic;scientific information extraction;leveraging large unannotated;relation extraction paths;relations knowledge bases;based relation extraction;extracting knowledge knowledge;terms knowledge graph;notion relation extraction;extraction knowledge;extracting relations;improve extraction knowledge;dictionary relation extraction;knowledge bases;knowledge base completion;knowledge knowledge graph;knowledge graph leveraging;knowledge graph;relations knowledge base;extraction scientific literature;knowledge base prediction;information extraction"}, "ef2e2f3a847667000b591c8708b543eaf259113b": {"ta_keywords": "threshold speech recognition;chime distant recognition;speech recognition problem;speech recognition;near threshold speech;distant recognition challenge;threshold speech;hybrid beamforming language;beamforming language model;distant recognition;applied chime distant;bidirectional hybrid beamforming;beamforming language;chime distant;hybrid beamforming;beamforming;language model rescoring;recognition challenge achieves;recognition challenge;distance improvement xmath0;recognition problem;speech;state near threshold;proposed applied chime;recognition;shallow state near;recognition problem employs;language model;distant;model rescoring proposed", "pdf_keywords": ""}, "b176a46ec214b9f75df751dcd2c894f0a7a72a9a": {"ta_keywords": "supervised unlabeled discourse;unlabeled discourse;unlabeled discourse approach;supervised learning unlabeled;learning unlabeled data;supervised unlabeled;learning unlabeled;supervised unlabeled data;bounded supervised unlabeled;debate portals;debate portals relies;semi supervised;discourse;data semi supervised;clustering unlabeled;unlabeled data debate;semi supervised manner;unlabeled data evaluation;discourse approach;unlabeled data semi;data debate portals;clustering unlabeled data;baselines supervised unlabeled;domain bounded supervised;discourse approach relies;relies clustering unlabeled;unlabeled data;bounded supervised;supervised learning;supervised", "pdf_keywords": ""}, "83145b7a391b792e24d8d38f74ed6b6ae7a149dc": {"ta_keywords": "referenced machine translation;context aware word;increases context usage;referenced context aware;translation systems use;machine translation systems;translation systems;machine translation introduce;machine translation;context referenced machine;aware word dropout;lexical cohesion contrastive;increases context;dropout increase context;anaphoric lexical cohesion;increase context referenced;lexical cohesion;context referenced context;increase context;context aware models;context aware;word dropout;referenced context;word dropout increase;context referenced;cohesion contrastive datasets;experiments anaphoric lexical;anaphoric lexical;lexical;context", "pdf_keywords": "context neural translation;aware machine translation;machine translation models;neural translation model;neural machine translation;translation models;harnesses context translation;translation predicting;translation models using;machine translation improve;translation models used;quality translation predicting;machine translation;generative machine translation;neural translation;context translation process;model learn translation;machine translation increases;translation model;machine translation framework;translation increases context;use context translation;machine translation technique;wemachine translation models;translation predicting higher;able translate context;context translation;machine translation presented;sentential context sentences;translation models method"}, "45eea76ac46b402f3a209de57e469275419fdc9e": {"ta_keywords": "graph generated gaussian;random graphs combining;gaussian states generated;random graphs;collection random graphs;generated gaussian state;gaussian state generated;generated gaussian;combination gaussian states;state generated gaussian;dimensional gaussian states;graph generated;gaussian states;states dimensional gaussian;dimensional gaussian state;gaussian states dimensional;gaussian state;combining dimensional gaussian;node graph generated;linear combination gaussian;combination gaussian;graphs combining dimensional;dimensional gaussian;gaussian state resulting;state resulting graph;graphs;graphs combining;states generated;gaussian;single node graph", "pdf_keywords": ""}, "94a11c9425bf5f4f9b8ed1b07ea1d15a81b96e9f": {"ta_keywords": "crowdsourcing development software;development crowdsourcing applications;development crowdsourcedin;crowdsourcing development;crowdsourcing applications;development crowdsourcing;crowdsourcing applications discussed;online crowdsourcing development;crowdsourcing;crowdsourcedin;micro crowdsourcing;online crowdsourcing;industry development crowdsourcing;crowdsourcing process discussed;use online crowdsourcing;crowdsourcing process;community micro crowdsourcing;micro crowdsourcing process;design development crowdsourcedin;development crowdsourcedin paper;mechanical turk generate;services mechanical turk;crowdsourcedin paper;software contributions community;crowdsourcedin paper use;mechanical turk;software contributions;open source community;open source;contributions open source", "pdf_keywords": ""}, "222ae836430ad0c922b47a9345c17212f9584097": {"ta_keywords": "technique prevention oforaloraloraloral;prevention oforaloraloraloral;prevention oforaloraloraloral diseases;oforaloraloraloral diseases inoral;oforal gradients aoral;exhibiting gingival smile;oforaloraloraloral diseases;oforaloraloraloral;gingival smile;lip repositioning surgery;microwave assisted lip;lip repositioning;assisted lip repositioning;gingival smile gggg;generation oforal;establishment oforal;inoral diseases;inoral diseases presented;diseases inoral;female exhibiting gingival;oforal gradients;establishment oforal gradients;assisted lip;based generation oforal;diseases presented technique;oforal;gradients aoral region;exhibiting gingival;diseases inoral diseases;gradients aoral", "pdf_keywords": ""}, "723770d9ac418e923db5e087ae18c04702f5986e": {"ta_keywords": "rule building learner;rule learner generates;new rule learner;rule learner simple;learner generates rulesets;rule learner;rulesets repeatedly boosting;rule learner called;generates rulesets;generates rulesets repeatedly;rule builder;greedy rule builder;rule building;rule builder new;rulesets repeatedly;rulesets;builder new rule;repeatedly boosting simple;boosting simple greedy;boosting simple;repeatedly boosting;greedy rule;simple greedy rule;new rule;state art rule;learner generates;boosting;building learner xmath0;learner simple effective;problems new rule", "pdf_keywords": ""}, "b9ede62d1d586e1a3b1ef7ec046f09e4e35639bf": {"ta_keywords": "effective retrieval data;method effective retrieval;retrieval data;retrieval data based;effective retrieval;retrieval;data based quadratic;quadratic term approximation;based quadratic term;based quadratic;term approximation;quadratic term;data based;data;quadratic;approximation;method effective;new method effective;term;method;present new method;effective;new method;based;present;new;present new", "pdf_keywords": "nearest neighbor search;effective retrieval pipelines;nn search based;documents nn search;nn search approach;search term based;search using similarity;based similarity search;similarity search;generic nn search;nn search using;designing effective retrieval;search based;effective retrieval;search term;search approach;approximate nn search;search nn search;efficient nearest neighbor;nn search term;queries search engine;efficiently rank document;search approach based;search based questionwe;retrieval pipelines present;retrieval pipelines;queries search;nn search;quadratic search engine;similarity search algorithm"}, "6dafc41e9bbd3aa476a0a1c15ca2c459eaef6b98": {"ta_keywords": "inflection model effective;overlaps finding inflection;inflection model;finding inflection accurate;method inflection model;inflection accurate usual;inflection accurate;inflection;finding inflection;method inflection;new method inflection;lemma overlaps finding;usual lemma method;lemma method based;models demonstrate;lemma overlaps;problem lemma overlaps;models;train test split;model effective solving;model effective;lemma method;test split challenges;train test;based train test;model;split challenges generalized;accurate usual lemma;overlaps;method based train", "pdf_keywords": "inflection prediction split;predicting inflection linguistic;splitting morphological datasets;splitting morphological;morphological datasets lemma;splitting languages;languages split effective;splits challenge generalization;method splitting morphological;languages split;morphological inflection;pretrained language models;split inflection shared;method splitting languages;splitting languages families;method predicting inflection;inflection prediction;resourced languages split;inflection linguistic output;morphological datasets;split inflection;linguistic inflection;lemma characters pattern;linguistic inflection neural;language models;classify lemmas;inflection linguistic;morphological inflection fundamental;based lemma characters;lemma split"}, "21066ab388b386f3d3552a4a4c25322e0ca69632": {"ta_keywords": "extraction hypernyms based;hypernym extraction;extraction hypernyms;accuracy hypernym extraction;hypernym extraction word;extraction word embedding;approach extraction hypernyms;word embeddings projection;learning word embeddings;word embedding;hypernyms based projection;hypernyms based;word embeddings;projection learning word;hypernyms;improve accuracy hypernym;word embedding used;embeddings projection learning;learning word;hypernym;accuracy hypernym;extraction word;embeddings;embedding used improve;projection learning;projection learning used;based projection learning;regularization model significantly;embedding;embeddings projection", "pdf_keywords": "learning hypernymy extraction;extraction hypernymy relations;learning hypernym extraction;language hypernym extraction;hypernymy extraction morphologically;hypernymy extraction based;projection learning hypernymy;approach hypernymy extraction;hypernym extraction;hypernymy extraction;model extraction hypernymy;extraction hypernymy;hypernym extraction based;learning word embeddings;learning hypernym;hypernymy relations based;extract synonyms hypernyms;word embeddings;learning hypernymy;projection learning word;context hypernymy prediction;word embeddings approach;language hypernym;rich language hypernym;hypernymy relations;hypernymy prediction task;word vectors model;method learning hypernym;distributional word vectors;hypernymy prediction"}, "900ce63d71dce47059434cdf2d5e1d77bc716e8d": {"ta_keywords": "electron gas spin;spin polarized electron;polarized electron gas;gas spin polarized;dynamics spin polarized;gas spin dynamics;zeeman field spin;spin dynamics spin;spin polarized;spin dynamics;polarized electron;gas spin;dynamics spin;electron gas fascinating;field spin dynamics;electron gas;control spin dynamics;control spin;field spin;spin;electron gas used;effect zeeman field;effect zeeman;zeeman;electron;condensed matter physics;used control spin;zeeman field;study effect zeeman;gas fascinating subject", "pdf_keywords": ""}, "8ae392fc9acbada67a4288a6affc2a77f83befcd": {"ta_keywords": "variational autoencoder;variational autoencoder vq;predicting shape text;quantized variational autoencoder;autoencoder vq;autoencoder;machine translation model;autoencoder vq vae;neural machine translation;model autoregressive node;autoregressive vector quantized;node based neural;translation model;machine translation;autoregressive node based;shape text;autoregressive vector;autoregressive node;translation model experimental;neural machine;vector quantized variational;shape text given;non autoregressive vector;vae model autoregressive;text;predicting shape;method predicting shape;input text;model autoregressive;text given input", "pdf_keywords": "variational autoencoder vq;quantized variational autoencoder;autoregressive gan based;variational autoencoder;speech encoder;autoencoder vq;autoencoder vq vae;speech encoder models;autoregressive gan;neural vocoders proposed;speech using encoder;autoencoder;gan based neural;neural vocoders;non autoregressive gan;learns mapping speech;gan based vq;loss inspired gan;short term memory;based neural vocoders;encoder nmt models;speech e2e model;autoregressive transformer nmt;e2e speech processing;target speech encoder;adversarial networks speech;vae model autoregressive;text speech e2e;vector quantized variational;vocoders proposed model"}, "317d95f99ef62237f6c7d7834d1d19027166b392": {"ta_keywords": "language generation e2e;generation languages e2e;e2e language generation;new generation languages;generation languages;languages e2e language;languages e2e;language generation;e2e language;generation e2e;generation e2e proposed;e2e outperform existing;languages;systems e2e outperform;e2e proposed systems;proposed systems e2e;language;systems e2e;e2e proposed;e2e outperform;e2e;results new generation;terms naturalness performance;outperform existing systems;new generation;naturalness performance proposed;systems outperform existing;systems terms naturalness;proposed systems outperform;generation", "pdf_keywords": ""}, "db190db2567c334b772fd653dca10f300074e421": {"ta_keywords": "speech recognition asr;speaker adaptation proposed;speaker adaptation;automatic speech recognition;automatic speech;speech recognition;end automatic speech;like speaker adaptation;data target speaker;recognition asr systems;recognition asr;recognition asr proposed;tasks like speaker;target speaker;asr systems;speaker;method training end;speaker able perform;asr proposed method;asr systems proposed;training end end;target speaker able;training end;like speaker;adaptation proposed method;training data target;training data;speech;quality training data;speaker able", "pdf_keywords": ""}, "02cbb0db288af2c83b48a023f245812bd22a2408": {"ta_keywords": "tables text semistructured;models semistructured data;text models semistructured;text semistructured data;semistructured data;table text models;semistructured data high;text semistructured;semistructured data ones;models semistructured;tables text literature;generate accurate tables;semistructured;accurate tables text;text models;generating table text;tables text;accurate tables;generating table;tables;table text;literature high precision;method generating table;table;text literature;text literature high;models;novel method generating;data ones cambridge;able generate accurate", "pdf_keywords": "unstructured text evaluation;sentences generation unstructured;table text models;text models powerful;generation unstructured text;text evaluation;quality sentences generation;natural language prediction;automatic text generation;text models;automatic evaluation table;entailed generation text;evaluating quality sentences;unstructured natural language;performance natural language;evaluate quality sentences;text evaluation method;evaluating sentence quality;unstructured text use;sentences generation;text natural language;natural language generation;text generation;descriptions semi structured;generation text;evaluation table text;automatic evaluation;unstructured text;generated text underlying;language descriptions semi"}, "20937a0f03bcb845afbedda901a6d4e93a2b5c34": {"ta_keywords": "probability distribution based;given probability distribution;probability distribution function;probability distribution;distribution probability distribution;analysis distribution probability;distribution function method;applicable probability distribution;distribution probability;distribution based distribution;based distribution probability;distribution;probability density;distribution function;calculating probability density;distribution function obtained;method applicable probability;analysis distribution;probability density given;distribution based;density given probability;method calculating probability;based distribution;distribution based analysis;based analysis distribution;taking account distribution;given probability;calculating probability;account distribution probability;applicable probability", "pdf_keywords": ""}, "b8e2e764ac82f81a5bc645c818d0d5ad7806e806": {"ta_keywords": "entanglement;entanglement observed;entanglement particles;entanglement particles possible;entanglement related;entanglement entanglement particles;processing entanglement;entangled entanglement observed;entanglement entanglement;processing entanglement related;entangled entanglement;information processing entanglement;entanglement generated;highly entangled entanglement;classical world entanglement;new type entanglement;type entanglement entanglement;world entanglement;type entanglement;entanglement generated interaction;entanglement observed presence;world entanglement generated;entangled;highly entangled;medium highly entangled;quantum information;quantum information processing;quantum;state art quantum;art quantum information", "pdf_keywords": ""}, "7c976b0b54ace7d13b87e8feefe6f29c0599d78d": {"ta_keywords": "semantic word network;lexicical ontology datasets;ontology datasets russian;lexicical ontology;constructing semantic word;semantic word;semantics methods relations;known lexicical ontology;constructing semantic;word network;semantic;method constructing semantic;relation extraction;ontology datasets;relation extraction methods;distributional semantics methods;semantics methods;word network wfn;resources distributional semantics;outperforms relation extraction;hierarchical contexts using;ontology;distributional semantics;available dictionary resources;dictionary resources;dictionary resources distributional;available dictionary;datasets russian language;semantics;hierarchical contexts", "pdf_keywords": ""}, "a83bbc7bf70b1beedbfe0140d24d556e2dc5acc8": {"ta_keywords": "gaussian noise diffusion;noise diffusion particles;noise diffusion;diffusion particles random;disordered medium diffusion;gaussian noise;gaussian random phase;diffusion particles disordered;particles random phase;particles disordered medium;medium diffusion particles;effect gaussian noise;characterized gaussian random;diffusion particles;gaussian random;medium described gaussian;random phase random;described gaussian random;random phase characterized;medium diffusion;phase random;random phase;related diffusion particles;phase characterized gaussian;phase related diffusion;particles random;phase random phase;particles disordered;diffusion;random phase related", "pdf_keywords": ""}, "2aaf2ee779cd4ff0f26bb73958ea9fb0faa61907": {"ta_keywords": "binary classification fluorescently;classification fluorescently;classification fluorescently selected;features fluorescence selected;identify features fluorescence;grained classifier;microscope image fluorescently;coarse grained classifier;fluorescently selected images;image fluorescently selected;selected protein fluorescence;features fluorescence;fluorescently selected protein;fluorescence selected protein;fluorescence microscope image;grained classifier method;image fluorescently;fluorescence microscope;fluorescence selected;protein fluorescence;applied fluorescence microscope;protein fluorescence distribution;dataset 464 fluorescent;binary classification;fluorescently selected;method binary classification;fluorescence;classification;fluorescence distribution;applied fluorescence", "pdf_keywords": ""}, "08f199ebfd27a5f9ada79edd07ac41e46c7278d5": {"ta_keywords": "social interaction noise;delayed social interaction;communication delayed social;communication delayed noise;interaction noise early;time social interaction;noise early time;interaction noise;social interaction;noise early;delayed noise;evaluates noise early;verbal communication delayed;delayed noise early;early time social;social interaction quantified;communication;delayed social;noise;social interaction dominant;communication delayed;verbal communication;evaluates noise;equation evaluates noise;time social;fact verbal communication;interaction quantified;interaction;social;interaction quantified fact", "pdf_keywords": ""}, "34d5d2f75934caff89311ef20d18a275da5abb47": {"ta_keywords": "spin orbit coupling;orbit coupling spin;coupling spin polarized;spin polarized xmath0;xmath0 pair spin;zeeman field spin;coupling spin;coupling strength spin;pair spin orbit;spin polarized;field spin orbit;polarized xmath0 pair;spin orbit;controlled zeeman field;control spin orbit;orbit coupling;strength spin orbit;orbit coupling used;pair spin;orbit coupling strength;polarized xmath0;effect zeeman field;field spin;strength controlled zeeman;control spin;controlled zeeman;effect zeeman;spin;coupling;xmath0 pair", "pdf_keywords": ""}, "fbf2a6a887ea92311cf207d522c535daf867a6ba": {"ta_keywords": "text speech models;large corpus discrete;speech models;corpus discrete representations;large corpus;data large corpus;text speech;translation data;corpus discrete;translation data large;speech models enable;corpus;corpus conduct qualitative;data literature corpus;models enable translation;end text speech;literature corpus;enable translation data;corpus conduct;speech;literature corpus conduct;text;discrete representations;enable translation;discrete representations type;models;data literature;data characterize;proposed models;proposed models compare", "pdf_keywords": ""}, "da06caf4f340ebc81395f092f9dc3a3101827506": {"ta_keywords": "electrollary speech;electrollary speech keeping;naturalness electrollary speech;electrollarynx based statistical;prediction electrollarylarylary control;control method electrollarynx;electrollarynx based;excitation prediction electrollarylarylary;electrollarylarylary control method;method electrollarynx based;electrollary speech multiple;method electrollarynx;electrollarylarylary control;electrollarynx;prediction electrollarylarylary;speech keeping;speech keeping listenability;improve naturalness electrollary;electrollarylarylary;improvements naturalness electrollary;speech multiple;speech multiple speakers;statistical excitation prediction;conversation proposed method;excitation prediction;naturalness electrollary;electrollary;face conversation;face face conversation;face conversation proposed", "pdf_keywords": ""}, "acc2ad56a9c68c799747e08d978f9803997c1527": {"ta_keywords": "predicts precursors perovskite;synthesizability screening perovskite;screening perovskite compounds;precursors perovskite materials;precursors perovskite;perovskite compounds;perovskite compounds proposed;perovskite materials;novel perovskite compound;perovskite compound perform;proposed novel perovskite;perovskite compound;perovskite materials using;predicting inorganic materials;screening perovskite;novel perovskite;method predicting inorganic;predicting inorganic;perovskite;model predicts precursors;syntheses model trained;reported syntheses model;inorganic materials based;inorganic materials;predicts precursors;materials based data;syntheses model;model predicts;novel method predicting;model trained predict", "pdf_keywords": "predict precursors materials;predict precursors perovskites;predicting precursors novel;model synthesis materials;predicting precursors;able predict precursors;model predict precursors;predict precursors;materials synthesis empirical;precursors novel materials;method predicting precursors;materials synthesis;precursors materials;discovery synthesis routes;discovery synthesis;materials synthesis planning;identifying elements precursors;materials discovery;precursors perovskites;synthesis materials;elements precursors;synthesis materials systems;insights materials synthesis;precursors synthesis;materials design discovery;perovskite material discovery;training predict precursors;precursors synthesis actions;precursors perovskites recently;material discovery"}, "2aea6cc6c42101b2615753c2933a33e57dd665f2": {"ta_keywords": "walks knowledge base;beliefs knowledge base;knowledge base extracted;knowledge base new;knowledge base;knowledge base graph;knowledge base approach;scale knowledge base;large scale knowledge;learning inference large;inference large scale;learning inference;approach learning inference;walks knowledge;new beliefs knowledge;inference large;beliefs knowledge;random walks knowledge;scale knowledge;inference tasks;knowledge;base extracted web;base new approach;applicable inference tasks;100 applicable inference;inference;infer new beliefs;novel approach learning;inference tasks apply;base graph reliably", "pdf_keywords": ""}, "1134ec4cdfc1c2161d157b0f4e03dec85d8c4c8a": {"ta_keywords": "convolutional networks reconstruct;training deep;training deep convolutional;networks reconstruct network;canals aerial images;networks reconstruct;aerial images loss;convolutional networks;connectivity oriented loss;deep convolutional networks;irrigation canals loss;deep convolutional;function training deep;canals loss;canals loss function;reconstruct network;recovers road connectivity;roads irrigation canals;loss function training;reconstruct network like;images loss designed;roads canals;roads canals penalizing;positive roads canals;images loss;irrigation canals aerial;networks;canals aerial;irrigation canals;set irrigation canals", "pdf_keywords": "convolutional networks reconstruct;deep networks ground;road delineation deep;deep network outputs;delineation deep network;network trained reconstruct;deep learning connectivity;training deep;trained reconstruct connected;deep deep road;convnet trained loss;training deep convolutional;road network reconstructions;models connectivity trained;deep network;train deep networks;connectivity trained;road image;networks reconstruct network;connectivity reconstructions;convnet trained;connectivity reconstructions network;networks reconstruct;deep road;network trained;map network trained;images roads;deep convolutional;convolutional networks;function training deep"}, "b9b83860bc0d79b3b629b3035c4b7b7f9f71b5af": {"ta_keywords": "deployment algorithms study;deployment algorithms;experiences deployment algorithms;propagation forest trail;propagation forest;model propagation forest;experimental experiences deployment;measurement based strategies;performance algorithms forest;deployment;experiences deployment;statistical model propagation;algorithms based markov;performance algorithms;propagation;measurements study statistical;based markov process;algorithms forest;measurement based;data measurements study;model propagation;trail extract statistical;forest trail;process ms measurement;markov process ms;compare performance algorithms;forest trail extract;data measurements;measurement data;ms measurement based", "pdf_keywords": "deployment relays network;deploying wireless relay;deployment wireless sensor;relay nodes deployment;deployment relays;method deployment wireless;sensor networks optimal;placement sequential wireless;networks optimal placement;method deployment relays;wireless networks distance;trail network nodes;location relay;wireless relay backtracking;optimal deployment strategy;sensor network nodes;wireless relay nodes;deployment agent walks;simple deployment algorithm;deployment wireless;conventional deployment algorithm;finding optimal deployment;source location relay;relay placement;wireless sensor trail;nodes network accurately;sensor network method;optimization wireless;deploying wireless;deployment algorithm robust"}, "386bfd0e411dee4f512a8737c55dd84846981182": {"ta_keywords": "learns tabular data;learning learns tabular;learns tabular;tabular representation learns;joint learning learns;table based prediction;joint learning;learns data;learns data performs;model joint learning;learns exclusively data;tabular data;tabular;representation learns data;table based;learning learns;tabular representation;simple tabular representation;simple tabular;providing simple tabular;prediction tasks;table;learns;tabular data performs;based prediction tasks;prediction tasks achieves;representation learns;pretraining objective learns;learns exclusively;learning", "pdf_keywords": "learns tabular representations;learning tabular representations;learns tabular;tables associated text;tabular representations tables;table based representations;table identification comprehension;learning tabular;tabular representations;tabular representations substructures;identify semantics tables;table based prediction;areunderstanding semantics tables;joint learning tabular;tabbie learns tabular;semantics tables;characterizing table substructures;characterizing table;semantics table based;describing semantics table;representations tables;text table;representations tables associated;tables encoders areunderstanding;text table framework;unstructured table based;table document embed;semantics downstream table;table substructures framework;tabular table document"}, "466865aaeb8902f6f8ed93ceeb5fbf9fc8b593b1": {"ta_keywords": "online speech enhancement;speech enhancement;speech enhancement achieved;speech enhancement uses;frame online speech;deep learning nonlinear;enhancement uses deep;virtual beamforming nldw;enhancement achieved nonlinear;learning nonlinear filtering;signal using deep;deep learning;beamforming nldw;neural network nlnl;achieved nonlinear filtering;virtual beamforming;learning perform deep;learning based frame;deep learning based;online speech;deep neural network;nonlinear filtering;deep neural;deep learning perform;perform deep learning;node virtual beamforming;nonlinear filtering input;uses deep learning;using deep neural;learning nonlinear", "pdf_keywords": "online speech enhancement;speech enhancement task;speech enhancement;enhanced speech enhancement;speech enhancement low;based speech enhancement;speech enhancement achieves;enhancement reverberant speech;latency enhancement sound;simulated speech enhancement;speech enhancement using;based enhanced speech;online enhancement deep;enhanced speech;deep learning rnft;enhancement deep neural;frame online speech;enhancement deep;nlnlnn network deep;microphone complex spectral;enhancement deep inelastic;time domain deep;deep nonlinear;microphone complex;network deep;low latency enhancement;use network deep;architecture deep;deep neural;enhancement sound"}, "4759aaacd71fbb2b5ca253aa13ccceac0bc7fe8a": {"ta_keywords": "computational argumentation;computational argumentation given;task computational argumentation;argumentation;argument pair strengths;arguments;arguments given argument;convincingness arguments;explanations convincingness arguments;argumentation given;explain argument;arguments certain controversial;convincingness arguments given;arguments certain;argumentation given pair;explain argument convincing;argument convincing approach;natural language explanations;argument pair;arguments given;language explanations convincingness;argument;argument convincing;arguments order explain;given argument pair;given argument;order explain argument;explanations convincingness;annotating 26k explanations;pair arguments certain", "pdf_keywords": ""}, "2cf21fc85af45512bf34d710f325872dca8a5331": {"ta_keywords": "predict traffic conditions;prediction traffic conditions;traffic conditions area;traffic conditions;density fluctuations city;predict traffic;method predict traffic;prediction traffic;extremal conditions city;spatio temporal distribution;applied prediction traffic;traffic;model spatio temporal;fluctuations city;city method based;density fluctuations model;traffic caused local;conditions city method;city method;fluctuations city uses;local density fluctuations;conditions area city;spatio temporal;temporal distribution local;based assumption traffic;assumption traffic;traffic caused;city based assumption;density fluctuations;city uses model", "pdf_keywords": ""}, "fb0a68981dae15f31cbcf5442509a3b8279b264c": {"ta_keywords": "robust speech recognition;method robust speech;robust speech;speech recognition proposed;speech recognition;speech recognition using;noise features proposed;multi dimensional feature;noise features;dimensional feature space;dimensional feature spaces;feature space lss;features proposed method;corrupted noise features;feature spaces proposed;recognition using low;dimensional feature;low dimensional multi;using low dimensional;feature space defined;recognition proposed method;dimensional multi dimensional;feature spaces;uses multi dimensional;feature space;recognition proposed;dimensional multi;recognition using;known low dimensional;vector corrupted noise", "pdf_keywords": ""}, "4d41c2fa74dd018e39ddb3cbbfead1b42615612c": {"ta_keywords": "parameters deep network;million parameters deep;deep network;network named deep;deep network benchmark;named deep network;deep network achieving;networks using deep;deep learning model;deep learning;parameters deep;million parameters network;deep network able;introduce deep learning;unseen diverse networks;using deep learning;using deep;24 million parameters;benchmark network;parameters network;network able predict;diverse networks;parameters unseen diverse;predict parameters unseen;predict parameters;named deep;benchmark benchmark network;benchmark network named;learning model predict;diverse networks using", "pdf_keywords": "neural architectures deepnets;parameters deep learning;parameter prediction deep;architectures deepnets;deep learning pipeline;learning architectures deep;predict parameters deep;architectures deep;architectures deep learning;deep learning pipelinewe;predict parameters networks;parameters predicted deep;deep network benchmark;parameters based deep;deep nets;deep learning architectures;network deep;graphs neural architectures;network deep learning;prediction deep nets;predicted deep learning;deep nets bywe;architectures deepnets 1m;predicting parameters network;training deep networks;networks deep learning;learning network deep;training network parameters;graph parameters networks;networks nnns generalized"}, "709f0a4229e40339b595072ae9fbd3a1ae1fd93e": {"ta_keywords": "batching computations dynamic;computations dynamic neural;dynamic neural networks;dynamic neural network;dynamic neural;batching computations;library dynamic neural;batching computations based;batch computations;batching layer;batching batching layer;networks method batching;batch computations present;layer uses batching;batching layer uses;generative neural network;method batching computations;neural network toolkits;dimensional generative neural;neural network toolkit;generative neural;batching;batch;use batching;neural network;computations dynamic;novel method batching;based use batching;proxy batch computations;batching batching", "pdf_keywords": "batching computation graphs;efficiently batching computations;efficiently batching computation;computations batches easily;computations batches efficiently;batching computations parallel;batching computations;organize computations batches;batched algorithms network;constructing batching computation;batching computations batches;framework batching computations;batching computation;automatically batching computation;computations batching;computations batches;batching computation used;instance computations batching;computations batching algorithm;batched algorithms;implements efficiently batching;batching efficiently processing;efficiently batching algorithm;batches efficiently;batching efficiently;efficiently batching;manually batched algorithms;batching algorithm known;compute batch;constructing batching"}, "d745ba895cf8dcba5670fb01feea931fc72f9c77": {"ta_keywords": "deep reinforcement learning;deep reinforcement;reinforcement learning agents;source deep reinforcement;reinforcement learning agent;learning agents complex;learning reinforcement;learning agents;reinforcement learning;learning reinforcement learning;reinforcement learning reinforcement;learning agent;state reinforcement;agents complex tasks;reinforcement;state reinforcement learning;reinforcement learning set;performance reinforcement learning;performance reinforcement;improve performance reinforcement;experiments transfer boost;structure state reinforcement;highly variable transfer;boost performance reinforcement;agents complex;complex tasks;transfer used improve;experiments transfer;transfer highly variable;learning", "pdf_keywords": ""}, "af6c6e66fe0a9ba19c304665e01db1c5a5fba1e4": {"ta_keywords": "robust reinforcement learning;making stochastic decision;decision making stochastic;stochastic decision;reinforcement learning framework;robust reinforcement;stochastic decision processes;reinforcement learning;kernel underlying decision;example reinforcement learning;extends robust reinforcement;decision process;learning framework decision;decision maker choose;decision maker;policies satisfying constraints;decision process known;underlying decision process;decision processes;reinforcement learning powerful;choose policies satisfying;decision making;decision processes situations;transition kernel;situations decision maker;reinforcement;simple example reinforcement;transition kernel underlying;assumption transition kernel;example reinforcement", "pdf_keywords": ""}, "3389b6b8ee5a1ef0395df9f383e771650087b828": {"ta_keywords": "providing domain expert;information domain expert;domain expert;domain experts able;domain systems process;users domain experts;domain experts;domain domain systems;domain systems;domain expert fashion;information domain domain;domain expert advice;process information domain;information domain;domain domain;capable providing domain;queries users domain;systems process information;domain;users domain;providing domain;seeking systems process;systems process;process information rigorous;experts able provide;process information;answer seeking systems;information rigorous manner;seeking systems;expert", "pdf_keywords": "domain expert systems;domain expert response;information retrieval;learning document ranking;expert systems;retrieval systems;ranking systems;information retrieval pre;classical information retrieval;retrieval approach synthesizes;domain expert quality;expert response quality;based information retrieval;information retrieval tasks;document retrieval systems;authoritativeness score;retrieval ranking;retrieval systems pre;information retrieval approach;question answering systems;expert quality responses;retrieval ranking components;ranking documents;knowledge unstructured web;domain expert;document ranking;information retrieval framework;ranking document;ranking systems followed;retrieving knowledge unstructured"}, "f432d10677897cf72f9594ba7bd4c9199b270fb3": {"ta_keywords": "performance measures classification;performance measures satisfying;measures classification applications;performance measures;analysis performance measures;measures classification;classification applications formally;classification applications;classification;measures satisfying desirable;analysis performance;measures satisfying;measures;performance;systematic analysis performance;list desirable properties;analyze properties satisfy;satisfying desirable properties;analyze properties;desirable properties;family performance measures;systematic analysis;analysis;theoretically analyze properties;properties theoretically analyze;applications formally;desirable properties theoretically;properties satisfy properties;applications formally define;define list desirable", "pdf_keywords": "classification performance measures;classification validation measures;classification measure;classification measures;classification measures consistent;binary classification measure;binary classification measures;classification performance;classification measure satisfies;accuracy classification evaluation;accuracy binary multiclass;used evaluating classification;weighted multiclass averagings;score accuracy classification;evaluating classification;evaluating classification results;analysis classification performance;weighted multiclass;measures classification powerful;classification evaluation;classification definitions based;measures classification;distinguishing binary classification;macro weighted multiclass;binary classification evaluation;classification powerful tool;accuracy classification;classification evaluation indices;classification results aspects;accuracy binary classification"}, "da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71": {"ta_keywords": "setting endangered languages;endangered languages;endangered languages develop;reducing recognition error;robust data scarce;languages;training data scarce;ease training data;algorithms robust data;algorithms robust;based algorithms robust;training data;reducing recognition;34 languages;data scarce;machine data based;rate 34 languages;scarce setting endangered;recognition error rate;robust data;data scarce setting;data based algorithms;machine data;languages develop post;general machine data;algorithms;setting endangered;based algorithms;endangered;setting reducing recognition", "pdf_keywords": "endangered language correction;endangered languages scanned;endangered language documents;correction endangered language;endangered language document;endangered language text;data endangered languages;images endangered languages;endangered language texts;text endangeredthe preservation;textual data endangered;images endangered language;transcribed images endangered;text endangered language;language document correction;endangered language books;specifically correction endangered;endangered language transcription;protecting endangered languages;errors endangered language;setting endangered languages;preservation endangered languages;languages scanned;scanned images endangered;model endangered language;document correction;text endangered;languages scanned images;endangered language;transcription endangered language"}, "f66c82ca087b435463ef4fa0de49825c4eb55885": {"ta_keywords": "probabilistic parser;novel probabilistic parser;semantic parsing;probabilistic parser extends;semantic parsing based;context free parser;parser using context;method semantic parsing;parser;parsing;parsing based;free parser using;free parser;parser using;parser extends general;parsing based novel;domain atis corpus;parser extends;atis corpus;probabilistic context free;semantic;context free probability;probabilistic context;general probabilistic context;corpus;based novel probabilistic;novel method semantic;data formalization;novel probabilistic;method semantic", "pdf_keywords": ""}, "b37d073109cfcf913cf53aded3872e6158e828a0": {"ta_keywords": "vision language navigation;language navigation;approach vision language;vision language;grounding language multiple;ability grounding language;grounding language;expert models;language multiple modalities;set expert models;modalities ensemble prediction;language navigation problem;navigation;expert models access;modalities ensemble;navigation problem based;based ability grounding;approach vision;language multiple;different modalities ensemble;ensemble prediction;ensemble;ability grounding;grounding procedure set;novel approach vision;set expert;multiple modalities;models improved;models;access different modalities", "pdf_keywords": "vision language navigation;vision agents grounding;vision language;models agents visual;train visual agent;performance vision language;agent supervised viewpoint;agents grounding visual;visual experts trained;language navigation tasks;representation navigation;language instructions vision;vision agents;agents visual inputs;provide representation navigation;instructions vision language;ability vision agents;agents visual;pair visual experts;model train visual;visual experts;train visual;modality strategy trained;grounding visual;grounding language;language navigation;strategies based deep;vision based counterparts;grounding visual instructions;navigation tasks"}, "e2ef0dc26a669ed764e2d70257b162298b8b608e": {"ta_keywords": "secrecy rate multicast;radar dfrc communication;multicast multi antenna;optimizing rate multicast;maximize secrecy rate;diffraction radar dfrc;rate multicast multicast;antenna diffraction radar;multicast multi ant;approach maximize secrecy;radar dfrc;multicast multicast;maximize secrecy;rate multicast;secrecy rate;multicast multicast multi;rate multicast multi;diffraction radar;multicast;multicast multi;multi antenna diffraction;dfrc communication;dfrc communication analyze;dfrc communication using;multi antenna;physical layer design;antenna diffraction;secrecy;layer design physical;radar", "pdf_keywords": "layer optimal secrecy;broadcast radar multiple;radar communication propose;secrecy rate multicast;function radar communication;distributed radar;radar communication;distributed radar network;multi radar propose;users distributed radar;multi radar;optimal secrecy;broadcast radar;multiple radar;optimal secrecy rate;approach maximize secrecy;maximizing secrecy rate;radar multiple;radar channels;radar multiple users;multiple radar receivers;radar network;targets optimal secrecy;dual function radar;radar network method;algorithm maximizing secrecy;maximize secrecy rate;radar radar multiple;maximizing secrecy;radar multiple radar"}, "243880fde63abfc287bd1356c2e1dbf68a1a0aac": {"ta_keywords": "rapid development languages;development languages spoken;languages spoken country;languages spoken;development languages;languages;spoken country;development technology enable;rapid development;enable rapid development;development technology;technology enable;technology enable rapid;opportunities development technology;technology;challenges opportunities development;discuss challenges opportunities;development;discuss challenges;challenges opportunities;opportunities development;enable rapid;enable;challenges;rapid;opportunities;spoken;country;discuss", "pdf_keywords": "language technologies available;languages spell checking;language technologies;linguistic technology languages;dialects technology writing;linguistic technology;translation technologies;spell checking machine;effective linguistic technology;languages spell;machine translation technologies;corpora computer language;dictionaries languages use;text using dialects;dictionaries languages;computer language;dictionaries corpora computer;using dialects technology;technology languages document;languages using;dialects technology;list language technologies;machine translationwe speech;machine translationwe;create dictionaries languages;possible spelling arrangements;dictionaries corpora;languages document;range languages spell;technologies languages"}, "9364eff879a9dcb34fe3dfdd0843e69c14dd333b": {"ta_keywords": "optimizing interpretability model;optimizing interpretability;method optimizing interpretability;model interpretability model;model interpretability;iterations model interpretability;interpretability model;interpretability model based;interpretability features model;interpretability model use;interpretability features;interpretability;features interpretability features;incorporates features interpretability;features interpretability;model based loop;loop iterations model;iterations model;model optimize;model based;features model optimize;humans optimization loop;demonstrate approach datasets;based loop iterations;model optimize number;approach datasets;model;features model;based loop;including humans optimization", "pdf_keywords": "interpretable machine learning;optimizing models interpretability;learn interpretable models;interpretability training;interpretability alongside prediction;incorporating interpretability training;optimizes models interpretable;models interpretability;interpretable models;interpretable models predict;interpretability artificial intelligence;models interpretable optimizes;models human interpretability;identify interpretable models;models interpretable;interpretability training objectives;artificial intelligence interpretability;models human interpretable;interpretable models useful;interpretable optimizes models;identifythe interpretability models;human interpretable models;identifies models interpretable;interpretability models;learn interpretable;optimizing algorithms interpretability;generate interpretable models;interpretability optimizes;interpretability optimizes best;interpretable models use"}, "64c575bb8b3e11097605028de5c289b0b2d839a4": {"ta_keywords": "native speech synthesis;speaker individuality synthetic;individuality synthetic speech;naturalness preserving speaker;preserving speaker individuality;synthetic speech;speech synthesis;speech synthesis technique;non native speech;non native speaker;native speech;preserving speaker;native speaker explicitly;speaker individuality;native speaker;native speech spoken;individuality non native;spoken target speaker;individuality synthetic;using non native;speaker explicitly using;target speaker experimental;speaker experimental;new non native;speech spoken target;speaker explicitly;synthesis technique preserves;preserves individuality non;non native;speaker experimental results", "pdf_keywords": ""}, "5af9ab65d186e4e1e0b1cef1962ca15336f37931": {"ta_keywords": "dependent semantic parsing;semantic parsing;semantic parser;resulting semantic parser;context dependent semantic;semantic parser process;semantic parsing resulting;parsing resulting corpus;corpus context dependent;semantic utterances;dependent semantic;influential semantic utterances;parser process semantic;semantic utterances possible;new corpus context;corpus context;corpus;parsing;new corpus;semantic;present new corpus;resulting semantic;parser;selecting influential semantic;resulting corpus;influential semantic;use resulting semantic;corpus distinguished existing;semantic data;context dependent", "pdf_keywords": ""}, "4f8e1a4247ce06a15760fc2692c6849601d41b6f": {"ta_keywords": "semantic based entailment;entailment models;based entailment models;underlying knowledge graph;textual datasets;entailment models using;contextual subgraphs;knowledge graph;capture structural semantic;generate contextual subgraphs;multiple textual datasets;text semantic;challenging breakingnli dataset;based entailment;structural semantic information;knowledge generate contextual;textual datasets use;entailment;structural semantic;contextual subgraphs reduced;breakingnli dataset;complements text semantic;text semantic based;graph convolutional networks;semantic information underlying;semantic information;semantic;encoding subgraphs;encoding subgraphs using;personalized knowledge generate", "pdf_keywords": "entailment graph based;textual entailment datasets;entailment graph;natural language inference;joint entailment graph;textual entailment;entailment datasets;text based entailment;entailment models information;existing entailment models;underlying knowledge graph;entailment datasets use;entailment models;based entailment models;existing entailment;relational graph convolutional;multiple textual entailment;conventional entailment model;entailment models work;entailment model;context knowledge graphs;based entailment;conventional entailment;knowledge graph;graph knowledge underlying;contextual subgraph relevant;natural language data;extension conventional entailment;knowledge graphs;combined existing entailment"}, "9cf75483deee77b3c0ee4f996d808437ab4a7435": {"ta_keywords": "thermal gelation process;thermal gelation thermal;gelation thermal;thermal gelation;thermal gel process;gelation thermal gel;small thermal gelation;thermal gel;fluctuations resulting gelation;gelation gelation process;gel process gelation;gelation process;energy gelation gelation;gelation process process;process gelation;gelation gelation;process gelation induced;gel process;kinetic energy gelation;energy gelation;resulting gelation;gelation induced;gelation;gelation consequence;resulting gelation consequence;thermal fluctuations kinetic;gelation induced small;thermal fluctuations;gelation consequence competition;thermal fluctuations resulting", "pdf_keywords": ""}, "0ce184bd55a4736ec64e5d82a85421298e0373ea": {"ta_keywords": "identifying phase text;automatically identifying phase;phase text;phase text precisely;text precisely sequence;precisely sequence symbols;sequence symbols using;sequence symbols;symbols using sequence;identifying phase;automatically identifying;text precisely;text;phase;sequence model called;sequence sequence model;precisely sequence;sequence model;problem automatically identifying;symbols using;symbols;sequence sequence;sequence;using sequence sequence;using sequence;identifying;novel approach;automatically;novel approach problem;model", "pdf_keywords": "attention encoder;attention encoder encoder;new attention encoder;decoder attention;model attention based;model decoder attention;sequence encoder;recurrent neural network;decoder attention propose;attention based intelligent;end speech recognition;generated sequence encoder;automatic speech;conventional recurrent neural;attention based;model attention;neural network rnn;recurrent neural;sequence train;sequence encoded features;encoder decoder networks;sequence encoder output;describes model attention;encoders class speech;source sequence train;sequence unstructured filterbank;model conventional recurrent;sequence train new;encode source sequence;speech improved models"}, "448e15e267b20bee1644034e18630da2e68cf36e": {"ta_keywords": "strong magnetic field;field strong magnetic;response strong magnetic;magnetic field strong;dispersive response strong;strong magnetic;dispersive response;magnetic field;magnetic field analyzed;field strong;dispersive;uniform time field;magnetic;field assumed uniform;uniform space field;field analyzed;field analyzed field;time field assumed;time field;analyzed field assumed;space field;analyzed field;space field assumed;response strong;field;assumed uniform time;field assumed;strong;uniform time;uniform space", "pdf_keywords": ""}, "4c94dc1b2391d78c9cfdd69955d20b56d7a16982": {"ta_keywords": "mds convertible codes;convertible codes optimal;convertible codes characterized;convertible codes;convertible codes valid;class convertible codes;linear mds convertible;codes optimal;conversion linear mds;codes optimal tuning;code redundancy access;codes characterized;tuning code redundancy;optimal tuning code;code redundancy;optimal linear mds;codes characterized valid;codes valid parameters;linear mds;mds convertible;tuning code;redundancy access cost;cost conversion linear;access cost conversion;cost conversion tight;code;cost conversion;codes valid;parameters access cost;redundancy access", "pdf_keywords": "optimal convertible codes;convertible codes optimize;linear mds code;mds convertible codes;distributed storage codes;linear code convertible;convertible codes theorems;codes distributed storage;linear mds convertible;distributed matrix multiplication;distributed storage code;storage code minimum;access optimal conversion;consider convertible codes;conversion linear mds;storage codes based;known convertible codes;convertible code achieves;convertible codes based;access optimal convertible;convertible code conversion;convertible codes consider;matrix multiplication md;freedom convertible codes;optimal distributed storage;multiplication md variant;convertible code;convertible codes problem;storage code;conversion linear code"}, "c55d5805a6eb8b1482f21581fe893484eaf9ffb5": {"ta_keywords": "perceived age singing;age singing voice;age singing;perceived age singer;controlling perceived age;singing voice based;singing voice conversion;age singer based;based perceived age;age singer;regression mixture model;singing voice;desired perceived age;regression mixture;perceived age;multiple regression mixture;voice based perceived;singer based modified;voice conversion apply;voice conversion;singing;voice based;voice;mixture model;model controlling perceived;mixture model problem;singer based;regression;multiple regression;singing voice propose", "pdf_keywords": ""}, "4275d4c4bd10742b321467f175f16198ed7d17d7": {"ta_keywords": "music generation models;music generation;generate coherent music;track music generation;midi files generate;learn noisy midi;midi;multi track music;midi files;adversarial networks symbolic;noisy midi;jamming model composer;noisy midi files;track music;generative adversarial networks;coherent music bars;coherent music;music;generative adversarial;study generative adversarial;composer model;composer model hybrid;model composer;generative;music bars;multi track;composer;domain multi track;model composer model;adversarial networks", "pdf_keywords": ""}, "802ddaf5bd731b91e64d8cee43f7fb614b42c1df": {"ta_keywords": "quantum emitter nonlinear;optical oscillator nonlinear;nonlinear optical oscillator;generated nonlinear optical;emitter nonlinear interaction;optical oscillator quantum;electric charge nonlinear;emitter nonlinear;dynamics nonlinear optical;quantum emitter generate;charge nonlinear;nonlinear optical field;nonlinear optical;charge nonlinear interaction;oscillator nonlinear dynamics;oscillator nonlinear;oscillator quantum emitter;interaction nonlinear optical;oscillator quantum;dynamics generated nonlinear;device based nonlinear;quantum emitter;generated nonlinear;generated nonlinear interaction;coupled quantum emitter;optical oscillator;applied quantum emitter;interaction generated nonlinear;emitter generate electric;nonlinear dynamics generated", "pdf_keywords": ""}, "75f90cbbf3c27a8b27567d6a9c8c4538743c8fff": {"ta_keywords": "text generation tasks;tool tensor based;tensor based network;generation tasks toolkit;text generation;powerful tool tensor;text toolkit;extensible toolkit texar;variety text generation;tool tensor;tensor based;text toolkit used;toolkit texar;process text toolkit;extensible toolkit;introduce extensible toolkit;generation tasks;toolkit texar supports;tensor;toolkit;versatility extensible toolkit;texar;variety tensor based;texar supports;toolkit used extract;process text;tasks toolkit;text;toolkit used;extract common patterns", "pdf_keywords": ""}, "7e386158f474a395618c5e065ac55844b507007c": {"ta_keywords": "performance shared models;task lightweight prediction;shared models phase;shared model pretrained;stage speech processing;lightweight prediction heads;speech processing stage;lightweight prediction;shared models;speech processing;benchmark performance shared;shared model;stage shared model;universal stage speech;shared model results;performance shared;frozen shared model;novel framework benchmark;prediction heads frozen;prediction heads;model pretrained;processing stage shared;framework benchmark;learn task lightweight;model pretrained novel;framework learn task;benchmark performance;framework benchmark performance;models phase space;models phase", "pdf_keywords": "learn speech representation;speech processing superb;learning speech representation;train speech representation;tasks speech processing;speech based tasks;learning speech;speech processing tasks;speech dataset;speech representation;available speech dataset;framework learning speech;speech processing universal;speech representation variety;supervision tasks speech;speech representation encoders;benchmark self supervision;discriminative modeling multitask;learning pretrained models;wide range speech;tasks speech;speech representation noisy;speech processing;speech dataset resulting;approach learning speech;multitask learning pretrained;learn speech;train speech;pretrained model supersupervised;introduce speech processing"}, "9976ed0d88a4156ecdd3ebe39714c5fb4a5a0246": {"ta_keywords": "speech recognition slacius;spontaneous speech recognition;matrix adaptation evolution;covariance matrix adaptation;matrix adaptation;speech recognition systems;vocabulary speech recognition;speech recognition;large vocabulary speech;spontaneous speech;spontaneous spontaneous speech;adaptation evolution strategy;multi objective optimization;objective optimization;recognition slacius;covariance matrix;adaptation evolution;vocabulary speech;objective optimization approach;adapt cloud;recognition systems;adapt cloud computing;using covariance matrix;slacius;easy adapt cloud;systems using covariance;multi objective;computing services optimized;adaptation;optimized high accuracy", "pdf_keywords": ""}, "814421bb20ba1fba88928fc168db1b7175cca6ac": {"ta_keywords": "particles harmonic trap;harmonic trap particle;particle harmonic trap;harmonic trap;motion particles harmonic;motion particle harmonic;particles harmonic;harmonic trap called;particle harmonic;collective motion particle;trap particle;collective motion particles;trap particle initially;dynamics collective motion;collective motion;trap called collective;motion collective motion;motion particle described;motion particles;called collective motion;dynamics collective;collective motion collective;motion particle;harmonic;particle described;particle described time;stopped motion particle;motion collective;particle initially excited;particles", "pdf_keywords": ""}, "e8bd03ff376ab3c863f72f931c91e90eeb9b2be9": {"ta_keywords": "spin orbit interaction;xmath1he spin chain;chain spin orbit;orbit interaction spin;spin polarized xmath0he;control spin orbit;spin chain magnetic;interaction spin chain;xmath1he spin;spin chain spin;field spin orbit;spin orbit;interaction spin polarized;spin chain;interaction spin;xmath0he xmath1he spin;magnetic field spin;chain spin;control spin;spin polarized;field spin;spin;chain magnetic;chain magnetic field;used control spin;polarized xmath0he xmath1he;polarized xmath0he;orbit interaction used;orbit interaction;magnetic field", "pdf_keywords": ""}, "36c95e3ef362742a5c1844257e8b79d3251a781e": {"ta_keywords": "language 3d objects;modeling vision language;visual language 3d;vision language;vision language useful;useful robotic language;robotic language understanding;language 3d;jointly modeling vision;visual language;robotic language;language useful robotic;3d objects;models distinguishing objects;non visual language;distinguishing objects;3d objects new;distinguishing objects demonstrate;modeling vision;objects referenced natural;objects referenced;description introduce rnare;novel reasoning task;referenced natural language;objects;novel reasoning;rnare based models;novel rnare;rnare requires model;objects new benchmark", "pdf_keywords": "modeling vision language;vision language models;vision language model;language 3d objects;vision language useful;benchmark shapenet referringexpression;vision language;objects object representation;object representation;objects natural language;robotic language understanding;useful robotic language;language representations objects;visual language 3d;meta vision language;objects prediction neural;pretrained vision language;language useful robotic;vision language alignment;objects prediction;trained predict semantic;robotic language;visual objects;describing understanding tasks;representations physical objects;jointly modeling vision;modeling vision;predict semantic;shapenet referringexpression snare;representations objects"}, "807e421679d4a9d629d2fad1f60f28787dca60e7": {"ta_keywords": "generative questions train;questions train generative;generated questions training;generate questions;generative questions;model generate questions;generate questions based;generated questions human;generated questions;human generated questions;questions train;training question answering;question answering models;questions human generated;data generated generative;model generated questions;generative model generate;questions training;training framework generative;generated generative model;framework generative questions;generated generative;question answering;generative model propose;answering models develop;generative model;generative;answering models;questions based unlabeled;train generative", "pdf_keywords": "supervised question answering;question answering;question answering models;answering models;learning method answering;answering based extracting;domain adaptation generative;answering models propose;question answering based;answering combines reinforcement;questions context generative;generative model learns;learning domain adaptation;performance question answering;quantitative questions reinforcement;generative domain nets;questions reinforcement;learning adaptive generative;answer generated questions;question answering combines;answering questions context;generative model trained;questions reinforcement learning;reinforcement learning domain;training framework generative;question answering utilizing;gated recurrent network;generative models semi;answering utilizing unlabeled;discriminative model reinforcement"}, "d06493373421c86ba33dbb8834ccb725105a665f": {"ta_keywords": "grained lexical distinctions;fine grained lexical;lexical distinctions vocabulary;grained lexical;distinctions vocabulary items;lexical distinctions;vocabulary items;lexical;vocabulary items use;distinctions vocabulary;vocabulary;given word language;language means distinctions;extracting fine grained;automatically identifying extracting;word language learn;word language;meaning given word;distinctions explain meaning;items use distinctions;means distinctions;identifying extracting fine;distinctions explain;distinctions;automatically identifying;identifying extracting;given word;use distinctions;use distinctions explain;method automatically identifying", "pdf_keywords": "lexical distinctions extracting;grained lexical distinctions;identifying lexical distinctions;learning lexical;approach identifying lexical;extracting lexical;extracting lexical selection;learn lexical selection;language comparing lexical;grained lexical;method extracting lexical;lexical selection l2;method identifying lexical;fine grained lexical;lingual lexical selection;identifying lexical choices;lexical selection;learn lexical;lexical distinctions;lexical distinctions arising;comparing lexical;characterizing lexical;lexical choice learners;learning lexical selection;using lexical selection;descriptions language learning;automated language learning;characterizing lexical choices;identifying lexical;lingual lexical"}, "ece62ada00cef99d9fc7a60e7d4b773f6d87c8f9": {"ta_keywords": "generate entanglement level;entanglement multi level;entanglement level environment;efficient generation entanglement;generate entanglement;generation entanglement multi;entanglement level;generation entanglement;used generate entanglement;entanglement multi;entanglement;systems level coupled;coupled single level;level level coupled;set level systems;level coupled single;level coupled;level systems;multi level method;level systems level;multi level;method efficient generation;efficient generation;single level;level environment method;level used generate;level method based;level method;level environment;coupled single", "pdf_keywords": "machine translation finds;machine translation entity;aware machine translation;language aware machine;machine translation tasks;multilingual speech data;entities eng corpus;machine translation enabled;_machine translation;translation entity discovery;machine translation predict;machine translation;dataset multilingual speech;language processing;language synthesis machine;best translation model;machine translation able;eng corpus;parallel machine translation;multilingual speech;mt machine translation;tasks machine translation;speech models languages;translation based entity;knowledge predict translation;machine translation mt;_machine translation mt;linking _machine translation;text translation model;translation tasks"}, "73635c9dc0ffb61c2eac79234108c6eee1362c1b": {"ta_keywords": "bandit problem;bandit problem rewards;armed bandit problem;multi armed bandit;bandit;armed bandit;bounds underlying stochastic;underlying stochastic;stochastic;rewards evolve correlated;according stochastic;problem rewards;problem rewards evolve;underlying stochastic model;rewards;rewards evolve;mixing time bounds;stochastic model;effect underlying stochastic;according stochastic model;formulation multi armed;algorithms based mixing;fashion according stochastic;stochastic model resulting;multi armed;stochastic model analyze;time bounds underlying;model resulting algorithms;resulting algorithms;time bounds", "pdf_keywords": "policy stochastic bandit;dynamic bandit problem;interacting correlated bandit;rewards stochastic bandit;bandit problem future;multiarmed bandit algorithm;stochastic bandit problems;bandit policy interacting;bandit algorithm prediction;models stochastic bandit;problem bandit algorithms;bandit problems observed;correlated bandit environment;bandit problems leverages;bandit algorithms;stochastic bandit algorithms;stochastic bandit model;bandit problems environments;bandit algorithms stochastic;bandit problems partial;bandit policy method;prediction payoffs bandit;consider stochastic bandit;dynamic bandit;stochastic bandit algorithm;performance bandit algorithms;algorithms stochastic bandit;state underlying bandit;bandit problem rewards;bandit algorithms correlated"}, "18289b2b04fc8a7a86f474236e55a3b1070a98ad": {"ta_keywords": "gaussian noise electron;noise electron gas;noise electron;electron gas dynamics;gaussian noise dynamics;gaussian noise;effect gaussian noise;field electron gas;terms gaussian noise;noise dynamics;electron gas presence;dynamics dimensional electron;electron gas;noise dynamics dimensional;dimensional electron gas;gas dynamics surprisingly;field electron;strong magnetic field;gaussian;electron;effect gaussian;dimensional electron;presence strong magnetic;gas dynamics;noise;strong magnetic;terms gaussian;magnetic field effect;effect field electron;gas dynamics governed", "pdf_keywords": ""}, "db500c4e746897e5d5adafbf222b959c512445ad": {"ta_keywords": "data poisoning attack;novel data poisoning;modeling propose defenses;attack popular data;data poisoning;adversary control model;adversary control;allows adversary control;mitigate attack;propose defenses mitigate;attack allows adversary;poisoning attack allows;input apply attack;propose defenses;poisoning attack;defenses mitigate attack;adversary;models including sentiment;allows adversary;mitigate attack cost;sentiment models language;predictions desired trigger;trigger phrase;model predictions;defenses mitigate;model predictions desired;desired trigger phrase;sentiment models;control model predictions;popular data models", "pdf_keywords": "adversarial attack poisoning;adversarial attacks poisoning;poisoning machine translation;text adversarial;models nlps attack;text adversarial adversarial;machine translation adversary;attacks adversarial;adversarial attack;attacks adversarial model;attack language modeling;adversarial attacks;adversarial;adversarial model;processing nlp attack;filtering text adversarial;display adversarial behavior;adversarial model proposed;text adversarial setting;nlp attack based;class adversarial attacks;language adversary attack;adversarial behavior trained;type adversarial attack;adversarial behavior;robust attacks adversarial;process adversarial;attacks deep learning;type adversarial;poisoning nlp"}, "00936aa7c8f64fc919dd4dcee6192ccc83e0d26e": {"ta_keywords": "lesions radiography using;extracted teeth imaged;interproximal lesions radiography;teeth imaged micro;lesions radiography;imaging interproximal lesions;infrared transillumination reflectance;infrared transillumination;reflectance devices imaging;radiography using extracted;transform pseudo goldstone;pseudo goldstone transform;imaging interproximal;transillumination reflectance;transillumination reflectance devices;goldstone transform pseudo;computed tomography microct;tomography microct study;micro computed tomography;devices imaging interproximal;wavelength infrared transillumination;radiography;tomography microct;detect characterize lesion;teeth imaged;reflectance devices;radiography using;pseudo pseudo goldstone;devices imaging;extracted teeth", "pdf_keywords": ""}, "58b0800ef48da2678e15e5e8bc1d786e24190742": {"ta_keywords": "field speech enhancement;speech enhancement recognition;far field speech;speech enhancement;recognition using microphones;microphones placed distances;speech recognition;speech recognition asr;automatic speech recognition;using microphones placed;microphones placed;microphones;using microphones;field deep learning;field speech;distances speakers;signal enhancement extraction;advances field deep;deep learning;enhancement recognition;enhancement recognition using;placed distances speakers;distances speakers focus;field far field;far field;end signal enhancement;recognition asr;automatic speech;signal enhancement;field deep", "pdf_keywords": ""}, "901fbb51d6fb9078e572c83a446b408da4de9b2b": {"ta_keywords": "", "pdf_keywords": ""}, "d0a58b6da9f7788534aa9963a78c24a87038e4fc": {"ta_keywords": "review characterize social;social choice paper;best paper review;mapping social choice;paper review;paper review characterize;paper collection reviews;aggregate mapping social;paper choice satisfies;community aggregate mapping;learning community aggregate;choice paper choice;choice paper;characterize social choice;choosing best paper;social choice;review characterize;collection reviews;community aggregate;paper choice;mapping social;review;best paper collection;social choice apply;best paper;choosing best;collection reviews ijc;choosing;social;aggregate mapping", "pdf_keywords": ""}, "490c31b460316b7f68e9b8f5ff0d26aef2f7f45f": {"ta_keywords": "equilibria magnitude stochastic;conjecture define stochastic;sensitivity game equilibria;stochastic version optimal;game equilibria;strategy sba conjecture;conjecture related stochastic;optimal strategy;optimal strategy demonstrate;optimal strategy based;stochastic dynamics sensitivity;stochastic dynamics;stochastic;dynamics sensitivity game;optimal strategy sba;stochastic version;game equilibria magnitude;existence stochastic;define stochastic;define stochastic version;existence stochastic version;related stochastic;sensitivity game;related stochastic version;version optimal strategy;optimal;stochastic version stochastic;demonstrate existence stochastic;version stochastic;strategy demonstrate existence", "pdf_keywords": "stochastic congestion games;stochastic optimal games;game costs stochastic;stochasticity games multiple;stochasticity games;game known stochastic;optimal congestion function;study stochasticity games;dynamics stochastic congestion;cost stochastic congestion;stochastic congestion;congestion game problem;stochastic optimality dynamics;optimal congestion;game dynamics optimal;terms stochastic optimality;theory optimal games;optimality dynamics stochastic;optimal games;cost optimal congestion;congestion game;dynamics stochastic optimal;congestion games;optimal dynamics stochastic;mdp congestion state;stochastic optimality;framework congestion game;game known stochasticwe;stochastic optimality linked;optimality stochastic optimality"}, "2e0b1484740047d6d6fb6bd2c9d8816b54b33811": {"ta_keywords": "neural networks evaluation;performance neural networks;neural networks;neural networks increasingly;performance neural;neural networks provide;review neural networks;evaluation performance neural;development neural networks;networks evaluation;networks evaluation performance;neural;review neural;unbiased review neural;training tools;tools accelerate learning;training tools accelerate;networks;development neural;accelerate learning;accelerate learning process;set training tools;biological sciences recent;learning process;biological sciences;tasks biological sciences;networks increasingly;set training;learning process collected;networks provide set", "pdf_keywords": "rankings reviewers;rankings reviewers significant;ordinal rankings reviewers;participating reviewers;participating reviewers paper;reviewers nips 2016;reviewers scores;reviewers participate;review 2016 conference;distribution reviewers scores;reviewers significant;host reviewers methodology;reviewers papers;reviewers scores nips;structure reviewers papers;higher volunteer reviewers;reviewers nips;reviewers methodology results;reviewers papers connected;decisions participating reviewers;reviewers papers peer;distribution reviewers;structure reviewers;reviewers proposal;host reviewers;community reviewers;discrepancy reviewers nips;reviewers study;reviewers specifically observe;community reviewers papers"}, "2444be7584d1f5a7e2aa9f65078de09154f14ea1": {"ta_keywords": "born neural networks;networks perform distillation;neural networks perform;distillation tasks ability;ability born neural;born neural;distillation tasks;perform distillation tasks;neural networks;neural;distillation;perform distillation;ability networks;clusters ability networks;tasks ability networks;ability networks perform;vision language modeling;vision language;networks;tasks vision language;language modeling significantly;networks perform tasks;networks perform;clusters ability;tasks vision;language modeling;perform tasks vision;small clusters ability;ability born;modeling significantly enhanced", "pdf_keywords": "compress information deep;memory networks stochastically;memory networks;network trained;model compression;information deep;network representations training;network architecture learning;deep network;training network;deep neural;neural networks depth;neural network representation;network training network;learning memory networks;compressing information;training neural;training neural networks;network training;architecture learning;network representations;networks stochastically generated;information deep neural;predict neural networks;node compressing information;essence neural networks;deep neural networks;networks ability train;neural networks single;neural networks density"}, "f17ee5b9d3120960eddd2bdb9af2f4f689cebb3a": {"ta_keywords": "embeds relation mentions;relation extraction;supervision relation extraction;relation extraction using;relation mentions types;embeds relation;relation mentions;test relation mentions;answer pairs model;model embeds relation;relation types semantically;relation mentions achieve;question answer pairs;answer datasets;public answer datasets;relation types;pairs model embeds;answer pairs similar;use learned embeddings;qa objects relation;learned embeddings;objects relation types;objects relation;relation;mentions types text;mentions types;embeddings;learned embeddings estimate;supervision relation;pairs similar representations", "pdf_keywords": "supervised relation extraction;supervision relation extraction;relation extraction corpora;embeddings relation mentions;relation extraction sentences;extracting relation mentions;embeds relation mentions;relation extraction distant;relation extraction;embeddings relation mention;extract relation mentions;relation extraction task;relation mentions text;supervision extract relation;relation extraction sle;relation extraction testing;relation extraction model;learn embeddings relation;relation extraction design;extraction distant supervision;extract entity mentions;relation extraction pipeline;indirect supervision extract;relation mentions entity;framework relation extraction;relationship entities text;task relation extraction;mentions text corpus;extract typed relations;text relation"}, "90af87c1e4fba127d6db8f5e1f9e1ef3472507e8": {"ta_keywords": "spin orbit coupling;spin polarized fermions;zeeman field spin;orbit coupling spin;coupling spin polarized;coupling spin;qm spin orbit;coupling strength spin;polarized fermions framework;mechanics qm spin;qm spin;field spin orbit;polarized fermions;spin orbit;spin polarized;control spin orbit;effect zeeman field;fermions framework;strength spin orbit;relativistic quantum mechanics;fermions framework non;field spin;non relativistic quantum;relativistic quantum;orbit coupling;effect zeeman;orbit coupling used;control spin;orbit coupling strength;quantum mechanics qm", "pdf_keywords": ""}, "39365d95992c8294ba32d85c69d337040ddb8e54": {"ta_keywords": "translation neural network;translation neural;representation translation neural;binary tree structure;binary tree;seq2seq models;balanced binary tree;target tree structures;standard seq2seq models;arbitrary tree structure;tree structure target;tree structures model;seq2seq models bleu;tree structure;tree structures;seq2seq;arbitrary tree;tree structure resulting;neural network;standard seq2seq;generate arbitrary tree;representation translation;neural;neural network using;new representation translation;tree;outperforms standard seq2seq;syntax bleu points;various target tree;structure resulting model", "pdf_keywords": "parsers utilize tree;linearized parse trees;neural machine translation;parse trees sentences;trees linearized parse;translation tree based;parse tree model;translation tree;linguistically inspired parse;parse trees flexible;nonlinear thirdparty parsers;constituency parse trees;parse tree;generates parse trees;inspired parse trees;target parse tree;parse trees;rnn decoder trdec;parse trees linearized;given translation tree;rnn decoder;translations target tree;parse trees synthetic;thirdparty parsers;novel rnn decoder;machine translation model;parse trees nmt;thirdparty parsers utilize;parse tree using;sparse linearized parse"}, "162515d87256f13888d9d7ba95275ac4b6c35396": {"ta_keywords": "word recognition model;word recognition;novel word recognition;semicharacter model outperforming;rare unseen words;recognition model trained;words literature pipeline;standard semicharacter model;semicharacter model;semicharacter;standard semicharacter;recognition model;reduction standard semicharacter;unseen words literature;unseen words;recognition;model outperforming;words literature;model trained;words;model outperforming approaches;word;literature pipeline;novel word;literature pipeline achieves;absolute error reduction;rare unseen;error reduction;present novel word;outperforming approaches", "pdf_keywords": ""}, "615358de8e9a7cf318c172afafc2a303eab93d98": {"ta_keywords": "recommender clothing coordinates;recommender clothing;propose recommender clothing;fashion style sharing;photographs fashion item;photographs fashion;photographs fashion magazines;body photographs fashion;recommend items style;fashion item tops;given photographs fashion;photographs fashion magazine;real photographs fashion;fashion magazines task;tops recommend items;fashion item;recommend items;clothing coordinates;clothing coordinates using;making recommendations;fashion magazine fashion;magazine fashion style;fashion style;fashion magazines;making recommendations given;magazine fashion;fashion magazine;style sharing;items style sharing;fashion", "pdf_keywords": ""}, "58c04126a5196deb57ae31d6174cd4aae154f138": {"ta_keywords": "active learning partial;learning partially labeled;active learning;partially labeled examples;learning partial feedback;partially labeled;learning partially;labeled examples;labeled examples method;fully annotates;fully annotates annotater;learning partial;labeled;relative accuracy classification;annotates annotater data;baselines fully annotates;method learning partially;annotates;alpf method learning;annotates annotater;classification;partial feedback alpf;partial feedback;annotater;feedback alpf;annotater data;accuracy classification;annotater data 42;alpf improve relative;method learning", "pdf_keywords": "learning partially labeled;labeled examples learning;learning partial labels;active learning partial;classifier partial labels;annotation label;partially labeled;learning active learning;using labeled examples;active learning approaches;question learning;partially labeled data;effective using labeled;active learning methods;active learning;active learning models;active learning method;existing active learning;labeled examples;propose active learning;labeled data effective;using labeled;suggests partial labels;annotation label form;labeled;label form annotation;approach active learning;partial labels improves;learning partial feedback;partially labeled sets"}, "bdfb9f1c79ad726049a3563c741311391e18532a": {"ta_keywords": "generate coherent states;generation coherent states;coherent state generation;generation coherent state;coherent state generator;state generator coherent;generator coherent state;coherent states achieved;coherent states used;coherent states;state generation coherent;coherent state coherent;combination coherent state;coherent state;state coherent state;generate coherent;coherent states method;state coherent;collection coherent states;generator coherent;used generate coherent;generation coherent;achieved combination coherent;based generation coherent;combination coherent;coherent;states used generate;state generator;collection coherent;state generator method", "pdf_keywords": ""}, "eadf5023c90a6af8a0f8e8605bd8050cc13c23a3": {"ta_keywords": "speech recognition separation;chime speech recognition;speech recognition asr;recognition separation challenge;robust automatic speech;multi microphone conversational;speech recognition;microphone conversational;recognition separation;automatic speech recognition;chime speech;automatic speech;recognition asr;microphone conversational systems;fifth chime speech;distant multi microphone;microphone;multi microphone;context fifth chime;speech language processing;separation challenge;interface speech;introduce fifth chime;recognition asr promoting;chime;conversational systems challenge;fifth chime;research interface speech;separation challenge seeks;speech language", "pdf_keywords": "acoustic recordings utterances;recordings utterances;acoustic utterances;events acoustic utterances;distant multimicrophone conversational;arrest events acoustic;multimicrophone conversational;recordings utterances essential;acoustic utterances essential;speech recognition presented;binaural speech recognition;events acoustic recordings;speech separation recognition;speech recognition;speech recognition challenge;task distant multimicrophone;form acoustic utterances;multimodality noise speech;recognition home speech;multimicrophone conversational real;microphone capture;microphone;speech recognition combining;acoustic recordings;challenges identifying utterances;sound challenge;time delay speech;reverberation assisted speech;advanced sound challenge;events acoustic"}, "11465566a1f5ec7d4176bb7ab8edd26a154a1b60": {"ta_keywords": "smart meter;collected smart meter;smart meter proposed;smart meters;data collected smart;smart meters rapidly;number smart meters;devices data collected;collected devices data;data collected devices;devices data;meter proposed approach;meter proposed;meter;collected devices used;data collected;collected devices;analysis data collected;devices used;information users devices;meters;collected smart;users devices;devices paper proposes;devices provide;devices used improve;analysis data;data;meters rapidly increasing;number smart", "pdf_keywords": "privacy service contract;privacy protected demand;privacy contract;contract respect privacy;contract assigns privacy;consumer privacy;privacy aware data;approach privacy protection;privacy consumers;resulting privacy consumers;privacy management demand;respect privacy consumers;privacy protection data;privacy contract users;problem privacy management;problem estimating privacy;increases privacy contract;consider problem privacy;problem privacy;privacy management;estimating privacy;privacy protection;asymmetry consumer privacy;privacy aware;novel privacy protected;loss risk privacy;privacy consumers new;consumer event privacy;approach privacy;model privacy"}, "ba00cbd314dc52b299a8b0c34f1887bcd43cdc12": {"ta_keywords": "synsets using dictionaries;synsets meta clustering;graph synsets meta;graph synsets;word sense induction;input graph synsets;dictionaries word embeddings;word embeddings;word embeddings approach;empirical observation synsets;meta clustering;synsets meta;graph use word;sense induction;synsets using;use word sense;use meta clustering;induces synsets using;synsets;dictionaries;meta clustering approach;dictionaries word;word sense;sense induction deal;synsets rare;approach induces synsets;synsets rare rare;induces synsets;organized graph use;meta clustering algorithm", "pdf_keywords": "clean synonyms clustering;synonyms clustering;synonyms clustering method;clustering texts semantics;texts semantics clustering;semantics clustering;induce synsets synonyms;semantically clean synonyms;synonymy dictionaries;using synonymy dictionaries;synonymy dictionaries word;clustering words;semantics clustering approach;synonyms edges extracted;fuzzy clustering words;graph synonyms;synsets synonyms method;clean synonyms method;similarity texts semantics;concept synonymy graph;synonyms method improves;synonyms method;applied graph synonyms;synonymy graph;word similarity;synonyms;clustering texts;clustering words unweighted;synsets using synonymy;disambiguation algorithm"}, "48220433a2fb07761b26b2d6aa59b615289a3d4c": {"ta_keywords": "node attack graph;single node attack;attack graph network;specific node attack;node attack;node attack effective;attack graph;single nodes graphs;nodes graphs arbitrary;graph types targeted;targeting single nodes;nodes graphs;arbitrary graph types;classifying nodes graph;graph network;graph network based;nodes graph simply;nodes graph;targeted classifying nodes;graphs arbitrary graph;attacker pick specific;single nodes;simply perturbing nodes;attack effective variety;graphs arbitrary;graph types;arbitrary graph;single node;nodes;attacker pick", "pdf_keywords": "graph learning attacks;attack graph neural;attacks graph nodes;graph attack effective;graph attack;attack graph network;graph nodes attack;graphs resistant attacks;attacks graph;direct attacks graph;attack implemented graph;graph node attacker;attack node graph;graph topology attack;chosen graph attack;attack graph;novel attack graph;graph network gnn;attack graph perturbing;based attack graph;graph neural networks;graph datasets gnn;learning graphs resistant;adversarial model graph;learning attacks network;graph network;graph neural network;graph graph network;graph neural;graphs resistant"}, "25c50ef5a902586a06099ceb29e7f34e2172020a": {"ta_keywords": "generation entangled particles;entangled particles generation;particles generation entangled;entangled particles generated;entangled particle generated;generation single entangled;generating network entangled;generation entangled;generation pair entangled;entangled particles achieved;generated pair entangled;network entangled particles;entangled particle proposed;entangled particles;entangled particles based;single entangled particle;entangled particle;pair entangled particles;particles generated generation;particles based generation;single entangled;particles generation;network entangled;particles achieved generation;entangled;pair entangled;particles generated;generating network;particle generated pair;particle generated", "pdf_keywords": ""}, "e79bd5d5ad084009233c8524b02ac887029c5fe2": {"ta_keywords": "plasmon highly flexible;planar plasmon tunable;tunable laser plasma;plasmon tunable dispersion;flexible tunable laser;surface plasmon planar;flat surface plasmon;laser plasma interface;plasmon tunable;plasmon planar plasmon;surface plasmon highly;plasmon surface;flexible dielectric dispersion;surface plasmon;plasmon surface plasmon;highly flexible dielectric;surface plasmon surface;planar plasmon;flexible dielectric;plasmon planar;highly flexible tunable;tunable laser;laser plasma;plasma interface design;flexible tunable;plasmon highly;planar highly flexible;plasma interface;plasmon;tunable dispersion", "pdf_keywords": ""}, "f1b52bf723d7f5c4b68c8551c4d168ed1224f016": {"ta_keywords": "mitochondrial genome ilucycloches;complete mitochondrial genome;sequencing complete mitochondrial;mitochondrial genome complete;mitochondrial genome;genome species ilucycloches;complete mitochondrial;genome ilucycloches proc;genome ilucycloches;mitochondrial;complete genome species;species ilucycloches proc;complete genome;genome complete genome;species ilucycloches;genome complete;genome species;generation sequencing complete;sequencing complete;ilucycloches proc natl;ilucycloches proc;characterized generation sequencing;genome;ilucycloches;generation sequencing;sequencing;proc natl acad;natl acad sci;proc natl;species", "pdf_keywords": ""}, "c14fb834ac6ede13f94f71cfaf5649b55e70a2c2": {"ta_keywords": "equilibria data markets;generalized nash;equilibria data;data markets;nash gn equilibria;data markets characterize;generalized nash gn;equilibria socially efficient;class generalized nash;gn equilibria data;single aggregator case;single aggregator;aggregator case;markets;markets characterize;equilibria provide;equilibria provide necessary;gn equilibria;case infinitely equilibria;equilibria;infinitely equilibria provide;characterize resulting equilibria;markets characterize resulting;aggregator;conditions equilibria socially;equilibria socially;infinitely equilibria;aggregator case infinitely;conditions equilibria;resulting equilibria", "pdf_keywords": "data source equilibrium;data aggregators nonrivalrous;aggregating data competition;competition data aggregators;aggregators data market;data aggregator incentivizes;data aggregators share;data competition;equilibria data market;equilibrium games aggregators;equilibria data sources;aggregator incentivizes data;aggregators data sources;data aggregators highly;data sources incentivized;data aggregators;incentivizes data source;data sources aggregators;data buyers aggregators;nash equilibria data;data aggregators consider;aggregators nonrivalrous nature;data data aggregators;aggregators nonrivalrous;data market exogenously;aggregators highly nontrivial;data competition different;aggregators competing;aggregators bound effort;crowdsourcing extend results"}, "7c085d7f50a76cf1a09a114986206256e0ee1931": {"ta_keywords": "spin polarized atom;spin polarized photons;polarized photons magnetic;atom coupling spin;polarized atom coupling;coupling spin polarized;field spin polarized;spin polarized;polarized atom;polarized photons;coupling spin;polarized atom effect;photons magnetic;photons magnetic field;effect coupling spin;polarized atom used;magnetic field spin;atom coupling;atom effect coupling;field spin;spin;polarized;magnetic field order;magnetic field;photons;atom effect;produce magnetic field;atom;magnetic;produce magnetic", "pdf_keywords": ""}, "293ed3367027c99a81ead6ff3f31be7de43bce9c": {"ta_keywords": "generation random walks;generated random walk;random walk generated;walk generated random;random walks method;random walks;partitioning random walk;walk generated;walk random walk;random walk parts;random walk;walk random;random walk random;walks method;walks method based;parts random walk;efficient generation random;walk parts random;walks;generation random;walk;generated random;method efficient generation;based partitioning random;efficient generation;partitioning random;walk parts;new method efficient;method efficient;random", "pdf_keywords": "peer review method;reviews assignments strategyproof;subset reviewers based;subset reviewers;peer review;reviewers based expertise;reviewers based;agents based ratings;suitable subset reviewers;agents sequentially reviewed;peer selection mechanism;novel peer selection;peer selection;mechanism sharing reviews;peer selection process;agent reviews;reviewers;selection agents;literature peer selection;selection agents keywords;strategyproof select winners;performs agent reviews;sequence peer review;classify review;based ratings neighbors;selection mechanism strategyproof;strategyproof agents gain;arbitrary selection agents;review method able;strategyproof agents"}, "adeed0816a2cab763e3bee769957ff1849985759": {"ta_keywords": "automatically normalization text;automatically normalize text;normalization text historical;normalize text;normalization text;texts interactive normalization;automatic normalization automatic;normalization automatic;normalize text variety;automatic normalization;automatic normalization non;automatically normalization;normalization automatic normalization;automatically normalize;automatically automatically normalization;normalization non automatic;automatically automatically normalize;normalization tool;normalization tool introduced;lemmatization automatic normalization;normalization;interactive normalization tool;normalize;texts based string;normalization non;text historical texts;text historical;interactive normalization;historical texts interactive;based string distance", "pdf_keywords": ""}, "bd8334c1246adbd47f80eea60249c30a74925d7a": {"ta_keywords": "deployment wireless relay;networks line relay;cost stochastic decision;relay nodes deployed;cost stochastic;wireless relay networks;deployment wireless;relay networks;relay networks line;relay nodes;link outage probabilities;probabilities number relay;average cost stochastic;wireless relay;outage probabilities;deployed network;nodes deployed network;stochastic decision;line relay;stochastic decision process;line relay node;relay;outage probabilities number;relay node;number relay nodes;deployment problem average;problem deployment wireless;link outage;relay node moves;decision process mdp", "pdf_keywords": "optimal deployment relays;deployment relay networks;deployment sensor network;relays compute optimal;trails relay network;trails relay;forest trails relay;mobile robots relay;robots relay network;network optimal policy;efficient deployment sensor;deployment network relays;compute optimal deployment;deployment relays wireless;cost deployment network;deployment wireless relay;network optimal optimal;sensor network minimum;deployment optimal policy;network optimal;relay network deployment;sensor networks;deployment relays;robots relay;optimal deployment policy;deployment policy wireless;cost markov decision;wireless relay networks;relay networks;relay networks long"}, "7e0eb21f4903c2fe860d1c4f213879e99d7cd23c": {"ta_keywords": "naturalness electrolarynx based;improving naturalness electrolarynx;electrolarynx based spectral;electrolarynx keeping listenability;naturalness electrolarynx;electrolarynx based;naturalness electrolarynx keeping;electrolarynx;electrolarynx keeping;parameters voice conversion;voice conversion method;parameters voice;voice conversion;spectral parameters voice;voice;listenability high;keeping listenability high;listenability;keeping listenability;based spectral parameters;based spectral;method improving naturalness;spectral parameters;conversion method proposed;improving naturalness;hybrid method improving;results improving naturalness;spectral;method improving;propose hybrid method", "pdf_keywords": ""}, "1d56a0b8fb560a79ca28b44bfd6f1e645a36549a": {"ta_keywords": "thermocouple thermal control;concept thermocouple junction;thermocouple junction thermocouple;polarized material thermocouple;idea thermocouple junction;junction thermocouple;thermocouple junction;thermocouple thermal;concept thermocouple;thermocouple single layer;based concept thermocouple;junction thermocouple composed;thermocouple junction single;new thermocouple thermal;thermocouple data design;based idea thermocouple;propose new thermocouple;section determined thermocouple;material thermocouple;material thermocouple single;thermocouple single;thermocouple composed single;determined thermocouple;idea thermocouple;thermocouple data;thermocouple composed;new thermocouple;thermocouple;determined thermocouple data;thermal control based", "pdf_keywords": ""}, "ead3182dd47bdd8da98476cca1cfe0373dfc2edc": {"ta_keywords": "clustering vbec speech;model speech recognition;acoustic model speech;vbec speech recognition;variational estimation clustering;model speech;estimation clustering vbec;speech recognition;decision tree clustering;speech recognition propose;probabilistic mixture model;acoustic model topology;vbec speech;complicated acoustic model;based probabilistic mixture;finding appropriate acoustic;acoustic model;speech recognition using;variational estimation;recognition using variational;clustering based probabilistic;using variational estimation;appropriate acoustic model;probabilistic mixture;estimation clustering;clustering vbec;mixture model;tree clustering based;tree clustering;mixture model gmm", "pdf_keywords": ""}, "b593be8ff3c09c6994657678fcde0c5adf43328e": {"ta_keywords": "unsupervised constituency parsing;unsupervised parsing;performance unsupervised parsing;constituency parsing craft;constituency parsing experiments;improve constituency parsing;constituency parsing;unsupervised parsing selecting;span constraints corpus;parsing;constraints phrase bracketing;phrase bracketing improve;constraints corpus;phrase bracketing;constraints corpus text;parsing craft;parsing selecting span;parsing craft dataset;span constraints phrase;parsing experiments;parsing experiments span;corpus text method;corpus text;parsing selecting;corpus;unsupervised constituency;bracketing improve performance;bracketing;bracketing improve;span constraints based", "pdf_keywords": "supervised constituency parsing;training improve parsing;supervised syntactic parsing;unsupervised parsing performance;learning unsupervised parsing;unsupervised parsing unstructured;performance unsupervised parsing;unsupervised parsing;effective parsing;improve unsupervised parsing;effective parsing text;parsing unstructured;improve parsing;unsupervised parsing sentences;improve parsing performance;supervision supervised syntactic;unsupervised parsing approach;constituency parsing;unsupervised parsing context;parsing sentences;parsing biomedical text;method effective parsing;syntactic parsing;parsing;supervised syntactic;parsing biomedical;phrase bracketing span;novel method parsing;parsing performance introduce;syntactic parsing work"}, "07a5536c0570804f816fdb5a0a5ae890630e61bd": {"ta_keywords": "el speech enhancement;speech enhancement based;speech enhancement;electrollary el speech;enhancement based noise;voice conversion proposed;statistical voice conversion;voice conversion;reduction statistical voice;noise reduction;based noise reduction;noise reduction statistical;approach electrollary el;enhancement based;statistical voice;electrollary el;el speech causing;speech causing degradation;hybrid approach electrollary;naturalness el speech;el speech;approach electrollary;voice;enhancement;electrollary;based noise;speech;speech causing;improves naturalness el;conversion proposed method", "pdf_keywords": ""}, "d14afc470cd90521147130e153c0d3e1324cd104": {"ta_keywords": "predicting semantics languages;predicting semantics;languages able predict;method predicting semantics;predict semantic;predict semantic features;able predict semantic;semantics languages data;learn syntax semantics;semantics languages;features languages data;semantic features languages;syntax semantics languages;languages data;semantics languages method;100 million languages;syntactic phonetic features;100000 languages;infer syntactic phonetic;million languages;million languages proposed;100000 languages able;features languages;languages data set;syntactic phonetic;able infer syntactic;syntax semantics;phonetic features languages;neural machine translation;set 100000 languages", "pdf_keywords": "typology multilingual neural;multilingual neural machine;predicting typology languages;language typology database;inferring typology multilingual;predict typology languages;train multilingual neural;multilingual neural;multilingual neural network;multilingual machine translation;databases train multilingual;typology multilingual;neural machine translation;collected multilingual machine;typology languages network;multilingual machine;useful typology prediction;typological databases train;language typology;data collected multilingual;typology new language;language encoder learns;characterizing typology languages;predicting typology;detect typology languages;language learned vector;typology languages;collected multilingual;train multilingual;machine translation systems"}, "6aac35ec3bfaf7e835ac633414419c9623838007": {"ta_keywords": "features cochleogram spectrogram;cochleogram spectrogram features;spectrogram features deep;combine cochleogram spectrogram;single features spectrogram;features spectrogram features;combine auditory features;features spectrogram;spectrogram features;spectrogram features propose;auditory features cochleogram;spectrogram features log;cochleogram spectrogram;spectrogram;combine auditory;auditory features;banks spectrogram features;features cochleogram;method combine auditory;features deep learning;filter banks spectrogram;banks spectrogram;combining cnn;combine cochleogram;cnn single features;auditory;possibilities combine cochleogram;deep learning;use convolutional neural;combining cnn single", "pdf_keywords": ""}, "9895531c6dc3854f082de1a1ec651a9e179bbd07": {"ta_keywords": "predicting tonal language;connectionist temporal classification;predicting tonal;method predicting tonal;tonal language based;based connectionist temporal;temporal classification loss;tonal language;tonal languages;tonal languages described;language based connectionist;connectionist temporal;use tonal languages;description phonemic tonal;phonemic tonal;architecture connectionist temporal;phonemic tonal signals;tonal signals;temporal classification;tonal signals demonstrate;neural network;neural;neural network architecture;based connectionist;combines neural;classification loss function;tonal;classification loss;use tonal;combines neural network", "pdf_keywords": ""}, "8d1fd086a76d30343d2224b61cb7ddab2125d0b2": {"ta_keywords": "symbollevel learning programs;analyzing symbollevel learning;symbollevel learning;analyzing symbollevel;framework analyzing symbollevel;symbollevel;representation solution paths;learning programs presents;learning programs;solutionpath caching mechanisms;solutionpath caching;optimizations improve performance;solution paths;solutionpath;optimizations improve;series solutionpath caching;solution paths use;programs;novel optimizations improve;use novel optimizations;representation solution;optimizations;novel representation solution;novel optimizations;programs presents;presents series solutionpath;improve performance analysis;performance analysis;improve performance model;caching mechanisms", "pdf_keywords": ""}, "3256198d819f23f82640490b9160e85139627d6c": {"ta_keywords": "crossing representation wavelet;wavelet transform;wavelet transform domain;representation wavelet transform;representation wavelet;wavelet;zero crossing representation;stabilized zero crossing;edge intensity stabilized;zero crossing;based edge intensity;operation based edge;image processing;edge intensity;intensity stabilized zero;image processing introduce;threshold operation based;application image processing;demonstrate threshold operation;threshold operation;crossing representation experimentally;threshold operation works;processing introduce threshold;stabilized zero;crossing representation;introduce threshold operation;demonstrate threshold;edge;intensity stabilized;new stabilized zero", "pdf_keywords": ""}, "f9a86c2df17f408105c2d3e9429410cdc376c6f0": {"ta_keywords": "distributional semantics;distributional semantic;problems distributional semantics;distributional semantic modeling;distributional semantics second;distributional methods workshop;field distributional semantic;scaling distributional methods;methods scaling distributional;distributional;distributional methods;scaling distributional;solving problems distributional;problems distributional;semantic modeling;semantic modeling workshop;semantic;semantics;semantics second;field distributional;semantics second devoted;participants field distributional;new methods scaling;methods workshop;methods scaling;methods workshop devoted;discovery new methods;modeling workshop devoted;novel methods solving;novel methods", "pdf_keywords": ""}, "66713fbcb8d5e48a9eb6425bd7fdbb53751e60b1": {"ta_keywords": "pseudo_ weighting arrays;novel vectorized scheme;weighting arrays integers;scheme pseudo_ weighting;weighting arrays;vectorized scheme pseudo_;arrays integers scheme;faster scheme proposed;vectorized scheme;scheme based superscalar;significantly faster scheme;pseudo_ weighting;fastest schemes varintg8iu;novel vectorized;previously fastest schemes;faster scheme;processors singleinstruction multipledata;fastest schemes;scheme significantly faster;present novel vectorized;integers scheme based;modern processors singleinstruction;vectorized;scheme pseudo_;integers scheme;processors singleinstruction;arrays integers;modern processors;arrays;singleinstruction multipledata simd", "pdf_keywords": "techniques integer compression;integer compression scheme;new integer compression;integer compression bit;vectorized integer coding;scheme integer compression;integer compression;integer compression nearly;byte compression scheme;implementation vectorized bit;coding scheme vectorized;encoding scheme fastest;integer encoding scheme;compression performance algorithms;compression scheme based;technique encoding integers;based vectorized bit;bit byte compressing;compressing integers binary;vectorized bit;bit compress integers;byte compressing;compression bit;fast encoding;variable byte compression;compression scheme;vectorized bit packed;encoding speed achieved;byte compression;fast encoding 32"}, "3a2446c47000c3d0681b2cdf6d8b87a11ff630e2": {"ta_keywords": "wall insulation board;automatic mounting device;wall insulation boards;automatic mounting;mounting device building;assembly wall insulation;exterior wall insulation;novel automatic mounting;insulation board;mounting device;wall insulation;insulation board data;insulation boards presented;insulation boards;mounting;building exterior wall;device building exterior;configuration assembly wall;exterior wall;assembly wall;insulation;building exterior;wall;simulation configuration assembly;library used simulation;simulation design;board data generated;simulation output device;device building;simulation design verified", "pdf_keywords": ""}, "5ce3148ed36a1ea034da2c05b8cde9efbaf43e6a": {"ta_keywords": "field speech recognition;far field speech;speech recognition network;speech recognition;field speech;beamforming network;beamforming network extensive;recognition network;performance far field;spatial covariance features;recognition network propose;covariance features improve;weight beamforming network;beamforming;far field;covariance features;use spatial covariance;covariance features significantly;spatial covariance;xmath0 training performance;network use spatial;recognition;advanced xmath0 training;weight beamforming;xmath0 training;reduce weight beamforming;training performance evaluation;frequency bands;features improve performance;use spatial", "pdf_keywords": ""}, "981dbdf6f87f13f3f3047a925c519fc39a35202b": {"ta_keywords": "predicting word language;predicting word;model predicting word;word language model;language model;word level model;language model combines;neural model predicting;novel neural model;word language;short input contexts;neural;novel neural;word level;present novel neural;neural model;input contexts;language;local concatenation layer;concatenation layer;short input;original word level;model short input;input contexts struggles;predicting;model predicting;local concatenation;concatenation;flexibility local concatenation;concatenation layer resulting", "pdf_keywords": "neural probabilistic language;gram language modeling;word embeddings;probabilistic language model;language modeling;language models;language models computationally;deep learning graph;network predict word;language modeling datasets;language modeling model;level language modeling;biological language models;attention layerwe;concatenates word embeddings;language model;deep nonlinear generative;text model trained;language model transformers;classic language model;probabilistic language;self attention layerwe;word embeddings fixed;word level language;learning graph;given language model;graph compression;language model nplm;predict word analysis;models word level"}, "d92e0443768ec3715205cb232ef1a1917372b0af": {"ta_keywords": "notation novel nlu;nlu models;models same_ notation;novel nlu models;nlu models extend;same_ notation novel;open source models;source models same_;models same_;same_ notation same_;same_ notation;notation same_;notation same_ notation;source models;toolkit espnet slu;novel nlu;extend same_ notation;models extend same_;espnet slu generation;nlu;present open source;source software toolkit;notation novel;models;software toolkit espnet;software toolkit;open source software;open source;models extend;slu generation", "pdf_keywords": "speech processing toolkit;open source speech;source speech processing;source speech recognition;speech recognition translation;benchmark spoken language;representation speech trained;speech processing need;language understanding toolkit;implementations various speech;speech processing tasks;simple speech processing;speech trained;benchmark spoken;data desired spoken;scenarios speech processing;speech processing;including speech recognition;translation systems toolkit;experiments benchmark spoken;discovery spoken language;variety speech processing;speech recognition;speech recognition central;spoken language representation;speech trained novel;various speech processing;speech recognition optimization;desired spoken language;language understanding datasets"}, "04e3a3ee41c1ee977e023052435bbb5f4c680f66": {"ta_keywords": "stores quantum xmath0;quantum stores;class quantum stores;quantum stores quantum;stores quantum;quantum xmath0;xmath4 store designed;quantum xmath0 xmath1;quantum xmath3 xmath4;xmath3 xmath4 store;quantum xmath3;xmath4 store;xmath0 xmath2 stores;xmath0 xmath1 stores;stores xmath0 xmath2;stores xmath0;xmath2 stores;xmath1 stores;xmath1 stores xmath0;xmath2 stores located;quantum information processing;boundaries quantum xmath3;xmath3 xmath4;xmath0 xmath2;quantum;operation quantum;information processing qip;processing qip applications;xmath0 xmath1;xmath3", "pdf_keywords": ""}, "eba4c6b0b860a34461ffb8544111c89a3ef0d8b7": {"ta_keywords": "noisy pairwise comparisons;pairwise comparisons;pairwise comparisons pair;comparisons pair items;algorithms ranking problem;given pairwise comparisons;algorithms ranking;comparison noise constrained;pairwise comparisons non;ranking problem;ranking problem particular;strong stochastic transitivity;stochastic transitivity;comparisons pair;stochastic transitivity sst;ranking;items comparison noise;items noisy pairwise;pair items comparison;noisy pairwise;comparison noise;ratio algorithms ranking;algorithm competitive;items comparison;time algorithm competitive;comparisons non actively;competitive ratio algorithms;pairwise;transitivity;comparisons", "pdf_keywords": "ranking problem computationally;algorithm optimal ranking;optimal ranking;ranking order pairwise;symmetric ranking problem;ranking order;identification optimal ranking;ranking problem;consider ranking problem;symmetric ranking;optimal ranking stochastic;global ranking order;algorithm ranking symmetric;ranking problem given;ranking problem surprisingly;domination instances algorithm;ranking;ranking symmetric;decision problem domination;ranking problem active;algorithm ranking;inferring global ranking;consider ranking;global ranking;new algorithm ranking;algorithm set domination;algorithm domination;algorithm task domination;present algorithm domination;algorithms domination"}, "9d628e420922cc23a8944de1511ca5d3309f5d58": {"ta_keywords": "paired clinical summaries;extraction conversation points;improves extraction conversation;extraction conversation;patient conversation model;patient patient conversation;clinical summaries patient;summaries patient patient;patient conversation;corpus clinical data;clinical summaries;quality utterances;summaries patient;conditional quality utterances;quality utterances evaluate;extracted corpus clinical;utterances;extracted corpus;records improves extraction;paired paired clinical;paired clinical;extract paired paired;conversation model;conversation points evaluate;extract paired;approach extract paired;utterances evaluate;corpus;conversation points;conversation model builds", "pdf_keywords": ""}, "40ba59c9945e7c19d06dadfa8f496da5810ee30d": {"ta_keywords": "parallel beam search;traverse multiple utterances;utterances achieved speedup;batch multiple speech;speech recognition accelerates;decoder based speech;speech recognition;multiple speech utterances;utterances line recognition;multiple speech;multiple utterances achieved;beam search algorithm;beam search;multiple utterances;speech utterances;search algorithm encoder;vectorizing multiple hypotheses;speech utterances line;based speech recognition;algorithm encoder decoder;algorithm vectoring hypotheses;original beam search;utterances achieved;search algorithm vectoring;utterances line;encoder decoder;algorithm encoder;utterances;encoder decoder based;inference step vectorizing", "pdf_keywords": "traverse multiple utterances;batch multiple speech;vectorizing search hypotheses;speech corpus;search hypotheses queue;algorithm hypotheses queue;utterance using vectorization;utterances line recognition;utterances achieved speedup;algorithm vectorizing hypotheses;speech corpus csj;algorithm search hypotheses;recognizing information parallel;multiple speech utterances;recognizing duration utterance;multiple utterances experiments;speech corpus speed;speed csj corpus;vectorizing multiple hypotheses;libri speech corpus;speech utterances;speech recognition;multiple utterances;multiple speech;search hypotheses multiple;input utterances achieved;speech utterances line;hypotheses expansion pruning;search algorithm hypotheses;multiple input utterances"}, "ccd33442fef058c7c0eafc57d2c6e6a4cde10a3b": {"ta_keywords": "3d protein models;assessment 3d protein;3d protein;graph structure protein;protein models;protein models method;structure prediction benchmarks;equivariant convolutions graph;convolutions graph;convolutions graph nodes;structure prediction;structure protein;structure protein molecules;quality assessment 3d;quality model assessment;protein molecules;graph structure;model assessment;model assessment compared;improves quality model;prediction benchmarks;molecules based rotation;protein molecules based;assessment structure prediction;protein;models;assessment 3d;benchmarks;graph;rotation equivariant convolutions", "pdf_keywords": "3d protein graph;construct protein graph;structure protein graph;protein models network;protein graph;protein graph arbitrary;protein models represented;quality protein models;protein graph use;characterize protein structure;protein model improved;protein models;protein structure accurately;accuracy protein model;spherical graph convolutional;prediction protein structure;protein models proposed;predicting protein structure;represented molecular graphs;protein model;protein graph apply;assessment protein models;protein structure prediction;predictions protein structure;predict protein structure;protein model method;method protein structure;protein structure;molecular graphs use;graph amino"}, "5dd34d2781ca702d0e3cd1224517ff60d6c3e2ee": {"ta_keywords": "languages ubiquitous informal;informal digital communication;model informal decipherment;informal digital;ubiquitous informal informal;informal decipherment;latin script languages;informal text;ubiquitous informal;phonetic visual priors;non latin script;informal decipherment non;channel model informal;informal informal;languages ubiquitous;informal informal especially;form informal text;latin script;informal;bias phonetic visual;character mappings substantially;inductive bias phonetic;phonetic visual;especially form informal;informal especially;decipherment non latin;communication range languages;phonetic;script languages;priors character mappings", "pdf_keywords": "unsupervised model transliteration;source language transliteration;behavior transliteration;informal romanization algorithm;systems decoding romanized;model transliteration process;nonstandardized informal romanization;informal romanization;language transliteration;model transliteration;text transliteration;language transliteration process;informal romanization pattern;informal romanization idiosyncratic;transliteration fully;transliteration;romanization pattern users;decoding romanized;transliteration process;zerofrequency behavior transliteration;account informal romanization;romanization algorithm based;behavior transliteration high;romanization idiosyncratic process;romanization;conversations romanized text;latin script languages;perform transliteration fully;languages latin character;perform transliteration"}, "054ba27fe5cc6085d20ea2707de886db6865dbed": {"ta_keywords": "learning proximity measures;supervised learning proximity;learning proximity;proximity measures method;proximity measures;graph learning;graph learning method;walks graph structure;known graph learning;random walks graph;walks graph;proximity;random walks;known random walk;random walk model;graph structure;walk model;graph structure associated;supervised learning;supervised;random walk;model random walk;known graph;walk model random;novel method supervised;walks;combination random walks;method random walk;learning method;random walk restart", "pdf_keywords": ""}, "7621bfe36cc649a5876cea587366201e158a8b38": {"ta_keywords": "domain adaptation biological;domain adaptation;adapting annotations linked;adaptation biological domain;approaches domain adaptation;adapt domain biological;adapting annotations;linked source representation;training novel domain;sequence labeled neural;biological domain based;annotations linked source;novel domain adapted;processes adapting annotations;domain adapted;labeled neural;domain biological;labeled neural network;biological domain;annotations linked;domain biological processes;source representation;biological processes adapting;source representation sequence;adaptation biological;domain adapted version;neural;linked source;annotations;adapting", "pdf_keywords": ""}, "c1a4c5380d90dc77064de6003cfb9611ad218600": {"ta_keywords": "dialogue response generation;response generating dialogue;neural dialogue response;sentiment controlled dialogue;generating dialogue;dialogue generator;dialogue generator assisted;generating dialogue history;controlled dialogue generator;neural dialogue;dialogue response;method neural dialogue;responses according dialogue;controlled dialogue;response generation;dialogue history explicitly;training sentiment controlled;dialogue;response generation allows;dialogue history;training sentiment;controlling sentiment response;learning training sentiment;dialogue history given;generator assisted adversarial;conditional adversarial;response sentiment labels;response sentiment;according dialogue history;conditional adversarial learning", "pdf_keywords": "dialogue generator trained;neural dialogue generation;dialogue generator;controlled dialogue generation;dialogue generation;dialogue generation predicts;dialogue generation based;response dialogue generator;generate dialogue responses;generate dialogue;sentiment controlled dialogue;dialogue responses machine;responses conditional adversarial;automatically generate dialogue;method dialogue generation;neural dialogue;adversarial learning sentiment;dialogue response entropy;generated response dialogue;quality dialogue responses;conditional generative adversarial;controlled dialogue responses;conditional adversarial;algorithms generate dialogue;approach neural dialogue;generator dialogue;dialogue responses controlling;dialogue responses;based conditional adversarial;dialogue responses dialogue"}, "9d06638df32f8feefb95ef5a4769adbb1ae6297d": {"ta_keywords": "new rule learner;rule learner highly;rulesets repeatedly boosting;rule learner;rule learner slipper;generates rulesets;greedy rule builder;rule builder;generates rulesets repeatedly;slipper generates rulesets;rule builder new;rulesets repeatedly;rulesets;builder new rule;repeatedly boosting simple;boosting simple greedy;boosting simple;repeatedly boosting;greedy rule;boosting;simple greedy rule;new rule;learner highly scalable;learner slipper generates;learner slipper;rule;benchmark;simple greedy;learner highly;benchmark problems", "pdf_keywords": ""}, "4c78943e11195fb72a3c878a03b248bc317180e0": {"ta_keywords": "polynomial learnability restricted;logics subset learnable;learnable syntactic restrictions;learnable syntactic;learnability restricted;polynomial learnability;subset learnable syntactic;learnability restricted subset;study polynomial learnability;learningability;subset learnable learningability;learnable learningability;learnable learningability incomparable;learningability incomparable;learnable subset learnable;learnability;learnable subset;learningability incomparable subset;subset learnable;known learnable subset;learnable;description logics subset;previously known learnable;description logics;known learnable;enable tractable learning;tractable learning;syntactic restrictions exist;tractable learning positive;order description logics", "pdf_keywords": ""}, "f0baf134f0a2ee6e99f6f2287791109cf93305e7": {"ta_keywords": "subcellular patterns image;protein subcellular patterns;fluorescence microscopy images;images protein subcellular;subcellular patterns approach;subcellular patterns;microscopy images protein;analysis fluorescence microscopy;fluorescence microscopy;architecture fluorescence microscope;network architecture fluorescence;fluorescence microscope;protein subcellular;architecture fluorescence;biological network;microscopy images;biological network architecture;fluorescence microscope allows;subcellular;identification protein subcellular;use biological network;analysis fluorescence;microscopy;images protein;approach analysis fluorescence;fluorescence;patterns image;patterns image approach;microscope allows identification;microscope", "pdf_keywords": ""}, "db253b17043b6a86e02173b6aa597664b0c7f256": {"ta_keywords": "embedding rare words;words embeds;rare words semantic;content rare words;compositionality words embeds;words embeds characters;rare words method;embedding rare;sparsity rare words;semantic content rare;words semantic content;visual space embeddings;capture semantic;embeddings;method embedding rare;rare words;embeddings generated;text classification;embedding;semantic content;words semantic;embeds characters visual;capture semantic content;compositionality words;text classification task;embeds characters;space embeddings generated;space embeddings;model compositionality words;semantic", "pdf_keywords": "learn embeddings compositional;modeling compositionality characters;learns embeddings;compositionality characters language;character embeddings;characters convolutional neural;learned embeddings;learn embeddings;embed representations text;compositionality characters way;model learns embeddings;compositionality characters;learns embeddings visual;character embeddings instances;learn embed characters;learned embeddings model;networks learn embeddings;embeddings compositional;model compositionality words;visual embeddings characters;learns visual embeddings;embeddings characters;learn representation encoding;characteristics learned embeddings;classifying characters context;characters convolutional;embed representations;satisfies compositionality characters;embeddings compositional component;embeddings characters using"}, "f516c98c3d2dde5b31931715fbc48bbbc0580e27": {"ta_keywords": "workgroup leadership inferred;workgroup information;characterize workgroup structure;workgroup structure;workgroup leadership;workgroup information storage;workgroup structure dynamics;workgroups;characterize workgroup;demonstrate workgroup leadership;present workgroup information;workgroup;dynamics demonstrate workgroup;data characterize workgroup;groups work communicate;demonstrate workgroup;enables characterize workgroup;communicate present workgroup;present workgroup;work groups;understanding work groups;workgroups benefit;work groups work;workgroups benefit good;groups work;team members;leadership inferred collection;members inferred leadership;team members inferred;messages exchanged team", "pdf_keywords": ""}, "553b74de8cb7ebca42a686e2a3a2d6aae170946e": {"ta_keywords": "speech enhancement duuse;speech enhancement dp;based speech enhancement;speech enhancement;clean speech signals;model based speech;speech signals noisy;enhancement duuse model;diffusion probabilistic model;novel diffusion probabilistic;diffusion probabilistic;recover clean speech;aforementioned diffusion probabilistic;enhancement dp wave;speech signals;enhancement duuse;clean speech;based diffusion reverse;enhancement dp;based speech;probabilistic model;based diffusion;generative models designed;probabilistic model based;propose novel diffusion;noisy signals proposed;models designed enhance;noisy signals;novel diffusion;generative models", "pdf_keywords": "speech enhancement diffuse;enhanced speech noisy;based speech enhancement;noisy speech networks;approach speech enhancement;speech enhancement based;removing noisy speech;models diffusive filtering;speech enhancement;model denoising noisy;denoising noisy;trained diffusion process;diffusive filtering sde;denoising noisy signals;combines noisy speech;model trained diffusion;model based speech;noisy speech time;noisy speech signal;network model denoising;trained diffusion;non speech noisy;noisy speech;speech noisy signal;speech networks;existing network denoising;eliminating noise;speech separation based;eliminating noise mixed;diffusive filtering"}, "616cc6826066184a8c77c3f2562e4e891ce42911": {"ta_keywords": "approach reinforcement learning;reinforcement learning;based intrinsic fear;reinforcement;novel approach reinforcement;reinforcement learning accelerates;approach reinforcement;intrinsic fear;intrinsic fear new;fear new method;accelerates learning guards;avoiding dangerous states;repeated catastrophes approach;learning guards repeated;guards repeated catastrophes;problems avoiding dangerous;learning guards;avoiding dangerous;repeated catastrophes;catastrophes approach;problems avoiding;avoiding;mitigates problems avoiding;fear;catastrophes approach based;approach toy problems;learning accelerates;learning accelerates learning;accelerates learning;learning", "pdf_keywords": "intrinsic fear learning;learns avoid catastrophe;intrinsic fear learned;intrinsic fear model;fear learning;supervised intrinsic fear;fear model trained;catastrophic forgetting reinforcement;fear learning based;fear learned reward;forgetting reinforcement learning;learning avoid catastrophic;learns predict behavior;reinforcement learning model;natural fear model;approach intrinsic fear;fear model predicts;reinforcement learning;reinforcement learning drl;fear model;fear learned;patterns reinforcement learning;forgetting reinforcement;deep reinforcement learning;introduce intrinsic fear;catastrophes agents;intrinsic fear;model learns;reinforcement learning drlwe;environments model learns"}, "7e82015c386726f4b8f6f686b6e6bb7d1e7564bb": {"ta_keywords": "model topic topic;topic topic related;topic related;topic topic;model topic;topic;present model topic;related;model;present model;present", "pdf_keywords": "detection controversial posts;controversy detection models;controversy detection social;identifying controversial posts;comments detection controversial;controversy detection;identify controversial posts;level controversy detection;models controversy detection;detecting controversy social;comment graph convolutional;controversy detection gcn;detection controversial;convolutional network topicexisting;detecting controversy;controversial posts combining;topic comment graph;controversial posts social;predicting controversy social;predicting controversy;controversial posts;controversy social media;methods detecting controversy;detect controversy social;approach predicting controversy;inter topic detection;mining public sentiment;detect controversy;detect controversy weibo;detect controversy dissimilar"}, "b8cabd2f7fbf816d667701c5d756b5fcb982e6fe": {"ta_keywords": "player non convex;tau gradient descent;sum games learning;learning rate player;gamma_1 learning rate;gamma_1 learning;games learning rate;finite timescale separation;gradient descent ascent;zero sum games;denoted gamma_1 learning;convex nonconcave;convex nonconcave zero;tau gradient;non convex nonconcave;descent ascent player;ascent player non;games learning;parameter tau gradient;sum games;nonconcave zero sum;learning rate;non convex;separation parameter tau;descent ascent;role finite timescale;timescale separation;player denoted gamma_1;finite timescale;gradient descent", "pdf_keywords": "ascent non convex;ascent game nonconcave;non convex strategy;ascent converges critical;descent ascent theorem;ascent finite timescale;ascent theorem finite;ascent theorem;descent ascent converges;learning dynamics guaranteed;learning rate bounded;gradient descent ascent;learning dynamics finite;ascent converges locally;ascent differential equilibrium;descent ascent finite;learning rates stability;ascent gradient descent;descent ascent games;game nonconcave zero;descent ascent non;nonconvex concave optimization;learning rates convergence;descent ascent generalization;convergence learning dynamics;concave optimization guaranteed;convex strategy;dynamics finite learning;descent ascent gradient;ascent converges"}, "c5a323f8744838093ee36bee3739dea599ce62f0": {"ta_keywords": "spin heisenberg antiferromagnet;dynamics spin heisenberg;liquid spin dynamics;zeeman field spin;heisenberg antiferromagnet;spin liquid spin;heisenberg antiferromagnet field;spin dynamics studied;spin dynamics spin;spin dynamics;repulsion spin dynamics;dynamics spin;field spin dynamics;spin liquid;antiferromagnet field initially;antiferromagnet field;spin dynamics sensitive;spin heisenberg;liquid spin;forming spin liquid;antiferromagnet;field repulsion spin;repulsion spin;zeeman field repulsion;effect zeeman field;field spin;forming spin;spin;effect zeeman;applying zeeman field", "pdf_keywords": ""}, "e82eff0f3e3d150617f9a721f83046940a963c03": {"ta_keywords": "based generalization abstraction;explanation based generalization;generalization abstraction;learning using explanation;generalization abstraction mechanism;learning algorithms abstract;model pac learnability;abstraction;learnability 1984 implementation;abstraction functions concept;concept learners presented;abstraction functions;learnability;pac learnability;abstraction mechanism;functions concept learners;analysis abstraction;based generalization;concept learners;pac learnability 1984;learning described;analysis abstraction functions;using explanation based;learnability 1984;abstraction mechanism framework;learning using;algorithms abstract spaces;implementation applications learning;concept;generalization", "pdf_keywords": ""}, "c8171eaa3a3aac78c3b37351412101bc06e5f359": {"ta_keywords": "monolingual annotators;monolingual annotators accomplished;languages monolingual annotators;captions languages;images captions languages;monolingual annotators method;captions languages independently;low resource languages;quality data translation;resource languages monolingual;data translation;data translation accuracy;translation accuracy;comparable training data;resource languages;source target languages;languages monolingual;target languages;annotators;annotators accomplished;monolingual;quality comparable training;target languages use;captions;images captions;languages;annotators method curating;comparable training;quality comparable data;training data low", "pdf_keywords": "captions images languages;captures caption complexity;low caption diversity;multilingual image descriptions;caption diversity computational;monolingual annotators;caption diversity;captures linguistic similarity;captions captures;significantly improves translation;text captures linguistic;languages monolingual annotators;monolingual annotators method;machine translation;large corpus text;evaluation machine translation;multilingual machine translation;caption complexity;captures caption;getting captions;large multilingual dataset;machine translation performance;translation tasks;languages getting captions;getting captions images;large vocabulary translation;captures linguistic;vocabulary translation tasks;translation quality evaluation;large corpus"}, "51c33a79e05425b6335c8676a166a0f4e178c0a2": {"ta_keywords": "coding test items;automatic coding test;test items automatic;test items approach;test items;features test items;coding test;test items indicative;set test items;automatic coding;semi automatic coding;essential features test;features test;test performance;overall test performance;test performance useful;test;set test;overall test;approach set test;specific knowledge gaps;items automatic;knowledge gaps struggling;items automatic semi;knowledge gaps;indicative overall test;gaps struggling evaluate;line automatic;coding;items approach", "pdf_keywords": ""}, "2c1cb736df7bf526fc3facecd078980e007abceb": {"ta_keywords": "yeast calmodulin studied;yeast vertebrate calmodulin;yeast calmodulin exhibited;yeast calmodulin yeast;baker yeast calmodulin;concentration yeast calmodulin;yeast calmodulin;calmodulin yeast calmodulin;calmodulin yeast;calmodulin binding energy;binding energy yeast;vertebrate calmodulin binding;energy vertebrate calmodulin;calmodulin binding;energy yeast vertebrate;energy yeast;calmodulin exhibited similar;calmodulin studied analyzing;calmodulin studied;calmodulin exhibited;vertebrate calmodulin;calmodulin larger alpha;vertebrate calmodulin larger;yeast vertebrate;baker yeast;rod baker yeast;concentration yeast;yeast;binding energy vertebrate;calmodulin larger", "pdf_keywords": ""}, "62d1a3137b01a69443bebf4d92c1990ec512a6a1": {"ta_keywords": "language models attack;attack language model;attack language;adversary extract training;models attack;demonstrate attack language;large language models;safeguards large language;models attack exploits;language model trained;models querying language;language models querying;adversary extract;data large language;model demonstrate attack;training large language;language models;querying language model;attack training large;large language;language model;attack training;querying language;novel attack training;ability adversary extract;language model demonstrate;model trained public;attack exploits;adversary;sequences model training", "pdf_keywords": "memorization attack highly;novel memorization attack;memorization attack;extraction attack memorization;attack memorization;attacks extract memorized;memorization attack utilizes;attack memorization problem;memorization problem attack;eidetic memorization attack;trained private datasets;language models adversarial;secret training data;trained private data;encoded memorization;memorization data large;private datasets attack;datasets attack;privacy deep learning;datasets attack based;learning models memorized;data memorization powerful;encoded memorization demonstrate;information encoded memorization;attack predict strings;language models trained;smaller models memorization;attack extract training;training data vulnerable;characterizing privacy deep"}, "bc4cb14af1023123b3122a5f0b6f3bb76334ffb4": {"ta_keywords": "neutron rich barrier;neutron rich erenkov;rich erenkov barrier;erenkov barrier barrier;erenkov barrier;nuclear matter barrier;extract neutron rich;isotopes neutron rich;rich isotopes neutron;neutron rich isotopes;neutron rich nuclei;rich nuclei neutron;nuclei neutron rich;characterization neutron rich;extract neutron;neutron rich phase;neutron rich;isotopes neutron;isotopes produced neutron;development neutron rich;produced neutron rich;rich phase nuclear;rich barrier barrier;nuclei neutron;used extract neutron;important development neutron;characterization neutron;barrier barrier important;development neutron;barrier used extract", "pdf_keywords": ""}, "073798fde720d5f08dccfbb0c1917a064828c399": {"ta_keywords": "optical lattice bose;lattice bose einstein;bose einstein condensate;lattice bose;condensate dimensional optical;dimensional optical lattice;optical lattice;einstein condensate represented;component bose einstein;dynamics component bose;condensate analog bose;condensate represented component;represented component bose;analog bose einstein;einstein condensate analog;condensate represented;einstein condensate dimensional;einstein condensate;bose einstein;condensate dimensional;lattice;dimensional optical;component bose;condensate analog;condensate;analog bose;bose;optical;represented component;dynamics component", "pdf_keywords": ""}, "81f5ef41dfa72679cb7cb38999a41a1c534c3871": {"ta_keywords": "generate stationary state;stationary state generated;stochastic process stationary;process stationary state;stationary state method;method generate stationary;generate stationary;stationary state stationary;state stationary state;applied stationary state;state stationary;stationary state;dependent stochastic process;time dependent stochastic;stochastic process method;stochastic process;process stationary;dependent stochastic;state generated time;method applied stationary;application stochastic process;stochastic;application stochastic;state method based;stationary;applied stationary;generated time dependent;state generated;state method;based application stochastic", "pdf_keywords": ""}, "8d69f466bdf56ce6663c2f809514577e79dd3bed": {"ta_keywords": "wearable device controlled;sixth sense technology;interface wearable device;interface wearable;wearable device;architecture interface wearable;sense technology proposed;sense technology;technology proposed interface;device controlled environment;device controlled;device proposed architecture;wearable;device proposed;proposed interface;responsiveness device proposed;proposed interface able;responsiveness device;using sixth sense;device;improve responsiveness device;technology proposed;interface;novel architecture interface;sixth sense;hardware;controlled environment using;able integrate technologies;interface able;hardware restrictions", "pdf_keywords": "gesture extraction lab;gesture recognition lab;gesture detect;use gesture detect;gesture extraction;sensor detects gesture;detect gesture;detects gesture;lab use gesture;gesture segmentation sensor;lab extract gesture;gesture segmentation proposed;gesture recognition;gesture controlled environment;gesture segmentation;extract gesture scene;extract gesture;detects gesture performs;method detect gesture;gesture recognition color;gesture scene;gesture detect presence;detect gesture signatures;like gesture recognition;use gesture;method gesture recognition;hand gesture segmentation;gesture scene extend;gesture signatures electromechanical;gesture user using"}, "79c6713c41b4fedf9c7454b7e2bb48d0aeb1ae0f": {"ta_keywords": "space filling designs;space filling sample;image reconstruction surrogate;reconstruction surrogate modeling;filling sample designs;dimensions image reconstruction;designs proposed metric;quality space filling;sample designs;sample designs proposed;surrogate modeling benchmark;high quality space;surrogate modeling;filling designs proposed;filling designs;image reconstruction;reconstruction surrogate;quality space;benchmark optimization functions;benchmark optimization;space filling;optimization framework generate;designs proposed approach;designs proposed;applications dimensions image;inertial confinement fusion;confinement fusion icf;designs;optimization functions inertial;fusion icf", "pdf_keywords": "quality sampling designs;space filling sampling;sampling designs;sample designs arbitrary;sampling designs specifically;quality sampling patterns;sampling pattern spatial;randomness sample designs;random sample designs;sampling patterns arbitrary;sampling high dimensions;filling sample designs;resulting sample designs;sample designs represented;filling sampling patterns;space filling sample;sample designs propose;filling spectral designs;sample design defined;constructed sampling;sampling patterns;spectral sampling functions;optimal sampling;sample designs;sample designs proposed;quality sampling;metric random coverage;approach spectral sampling;sampling area characterized;algorithm optimal sampling"}, "2821db8962fce43265215a9c4b8d66af02e16ae7": {"ta_keywords": "scheduling redundant requests;scheduling redundant;delays schedule redundant;requests cancellation overheads;schedule redundant requests;redundant requests cancellation;job latency cancellation;overheads distributed computing;cancellation overheads distributed;queueing;schedule redundant;scheduling policy cancellation;queueing model;optimal scheduling policy;efficacy scheduling redundant;redundant requests achieve;cancellation delays schedule;optimal scheduling;cancellation delay considered;scheduling policy;redundant requests;cancellation overhead;propose new queueing;actual optimal scheduling;cancellation overheads;latency cancellation;significantly optimal scheduling;policy cancellation overhead;captures cancellation delays;cancellation delays", "pdf_keywords": ""}, "94245856c88e3e08777c876fc038ed1adf8f3285": {"ta_keywords": "calculating quantum numbers;calculation quantum numbers;closed quantum calculated;quantum numbers particles;quantum quantum numbers;quantum numbers;particles closed quantum;calculating quantum;calculation quantum;possible quantum numbers;numbers particles closed;quantum calculated;closed quantum quantum;based calculation quantum;closed quantum;method calculating quantum;quantum state calculation;closed quantum method;quantum quantum;numbers particles;calculated integration quantum;quantum calculated integration;quantum;quantum method;quantum state;quantum method based;integration quantum;integration quantum state;particles closed;based integration quantum", "pdf_keywords": ""}, "9ef4f6a070c750b746fe98ef34083d6a08c9ba42": {"ta_keywords": "pricing controlled diffusion;pricing linear quadratic;pricing controlled;quadratic dynamic games;optimal control;pricing linear;cost players;optimal control strategies;player cost players;ensuring total cost;dependence player cost;dynamic games problem;games problem fundamental;cost methods minimizing;diffusion general network;cost incurred leader;pricing;cost players finally;player cost;strategies study pricing;design optimal control;cost methods;controlled diffusion general;controlled diffusion;optimal;linear quadratic dynamic;study pricing controlled;quadratic dynamic;explicit dependence player;dynamic games", "pdf_keywords": ""}, "ff86133b3b49974f06fc881548c6f3c7a8ceffee": {"ta_keywords": "controlling voice timbre;voice timbre control;singer perceived age;technique controlling voice;controlling voice;change singer perceived;voice timbre based;maintaining singer individuality;age maintaining singer;proposed voice timbre;voice timbre;timbre control method;timbre based perceived;singer individuality proposed;timbre control;individuality proposed voice;singer individuality;singer perceived;based perceived age;change singer;possible change singer;perceived age;voice;perceived age maintaining;maintaining singer;proposed voice;effect perceived individuality;results proposed voice;timbre based;perceived individuality experimental", "pdf_keywords": ""}, "43d82bc8203c09edc7eb6b2bedcf4ab500690852": {"ta_keywords": "parallel corpus adjustment;adjustment large multilingual;nlp tasks;multilingual language models;corpus adjustment improves;small parallel corpus;nlp tasks introduce;processing nlp tasks;language processing nlp;cross lingual adjustment;corpus adjustment;parallel corpus;fine lingual adjustment;language processing np;lingual adjustment machine;lingual adjustment;processing nlp;language processing;cross lingual;approach fine lingual;natural language processing;large multilingual language;impact cross lingual;language models zero;large multilingual;zero shot transfer;multilingual;fine lingual;multilingual language;shot transfer", "pdf_keywords": "models crosslingual translation;large multilingual models;cross lingual embeddings;multilingual language models;tuning multilingual models;lingual transfer learning;embeddings using multilingual;multilingual models;transfer large multilingual;models crosslingual;multilingual models understood;multilingual models contextualize;large sample multilingual;multilingual models zero;embeddings languages target;lingual embeddings;training machine translation;lingual embeddings using;linguistic model captures;fine tuning multilingual;different models crosslingual;large multilingual language;language inference ner;contextualize embeddings languages;large multilingual;crosslingual translation;multilingual bertarian model;translation pre trained;transferred models english;adjustment large multilingual"}, "b57da3ccf214e8dad49116c8db9590c2c89629f5": {"ta_keywords": "named entity recognition;entity recognition ner;entity recognition;named entity;dataset named entity;problem named entity;recognition ner languages;entity;recognition ner;recognition ner use;supervised transfer learning;supervised transfer;ner languages;methods supervised transfer;supervised;transfer learning;ner languages demonstrate;available dataset named;dataset named;methods supervised;ner use publicly;publicly available dataset;named;ner use;dataset;state art methods;available dataset;ner;use state art;languages", "pdf_keywords": "named entity recognition;entity recognition;recognition named entity;entity recognition named;language annotators;information retrieval ner;corpora language annotators;news corpora language;language annotators curate;trained linguistic data;linguistic data resulting;language training data;information retrieval representation;named entities language;entities language;linguistic data;ner problem languages;partial knowledge language;annotators different languages;optimally train linguistic;retrieval ner fundamental;retrieval representation;corpora language;corpus partial knowledge;language equation ner;predict linguistic similarity;corpus extended knowledge;languages present ner;online news corpora;knowledge language"}, "05c8f15dbdd7c6661b9176638262bbc1e11de85f": {"ta_keywords": "interpretable sense embeddings;sense embeds word;sense embeddings;sense embeddings method;learning interpretable sense;sense method embeds;sense embeds;original sense embeds;learning interpretable;original sense method;interpretable sense;embeds word;embeddings;sense method;method learning interpretable;distinct original sense;embeds word form;method embeds word;embeddings method embeds;embeddings method;original sense;interpretable;embeds;method embeds;word form distinct;word form;learning;sense;word;novel method learning", "pdf_keywords": "word sense selection;interpretable sense representations;sense embeddings models;senses word model;sense embeddings;selecting sense word;selecting word senses;sense embeddings model;sense embedding model;sense similarity learned;existing sense embeddings;sense embedding;word senses based;sense representations;attention induction optimized;discovering interpretable sense;sense embeddings including;attention induction model;sense selection task;word selecting sense;sense representations coherent;sense selection;discovers interpretable sense;attention induction;contextual word sense;models word embeddings;sense word given;sense induction;sense embeddings minimal;easily interpretable representations"}, "58737fba500075136ee0f33f7801a5ac7f82ab68": {"ta_keywords": "rapid prototyping;rapid prototyping propose;prototyping;prototyping propose;prototyping propose novel;document style;known split document;field rapid prototyping;split document style;practice scoring features;split document;scoring features;document style version;validate reproducibility variants;methodology validate reproducibility;reproducibility variants;document;scoring features form;reproducibility variants known;novel methodology;methodology explores common;features;variants known split;databases validate effectiveness;validate reproducibility;features form;propose novel methodology;databases;style version known;heterogeneous data", "pdf_keywords": ""}, "14919b6a453a71f2a007d5fa57241887a982575f": {"ta_keywords": "independent learning logics;learning logics known;learning logics difficult;learning logics surprisingly;theorems representation independent;learning logics;representation independent learning;theorems theorems representation;logics known difficult;theorems representation;independent learning;logics surprisingly easy;logics difficult prove;logics difficult;logics known;difficult prove theorems;representation independent;prove theorems theorems;theorems theorems;theorems;logics surprisingly;logics;prove theorems;prove prove theorems;learning;representation;independent;known difficult prove;surprisingly easy prove;known difficult", "pdf_keywords": ""}, "fb089347919e8dada9335b4bac01f16eea758c56": {"ta_keywords": "topics computer ethics;computer ethics using;computer ethics;ethics using story;teaching topics computer;story machine stops;ethics using;machine stops teaching;using story machine;story machine;topics computer;ethics;stops teaching tool;tool discuss teach;teaching tool discuss;teaching topics;using story;suggestions teaching topics;teach issues address;teaching tool;discuss teach issues;machine stops real;teach issues;discuss teach;address story machine;machine stops;stops teaching;topics;suggestions teaching;tool discuss", "pdf_keywords": ""}, "e2c05b3abf77900ec82ffa8a95aa774308d2780f": {"ta_keywords": "analyzing code switching;switching patterns tweets;level language detection;code switching patterns;patterns tweets global;language detection;code switching;patterns tweets;analysis code switching;language detection technique;patterns tweets use;word level language;tweets global region;tweets global;unsupervised word level;level language;tweets use technique;languages dominated global;analyzing code;languages;technique analyzing code;word level labeling;languages dominated;language;tweets;switching patterns;unsupervised word;seven languages dominated;seven languages;tweets use", "pdf_keywords": ""}, "719916251f7e36d2e7a40e70f89f20ab97a8bc29": {"ta_keywords": "channel speech enhancement;speech enhancement student;speech enhancement;mask obtained multichannel;single channel speech;channel speech;cut soft mask;term soft mask;soft masks applying;soft mask target;soft mask;soft mask compared;resulting soft mask;mask resulting soft;soft mask resulting;mask target student;obtain soft masks;soft masks;multichannel input;enhancement student network;single channel;mask obtained;multichannel input proposed;multichannel;mask resulting;masks applying;obtained multichannel;mask compared;obtained multichannel input;mask", "pdf_keywords": "channel speech enhancement;multichannel speech enhancement;speech enhancement proposed;speech enhancement method;based speech enhancement;speech enhancement using;speech enhancement student;speech enhancement technique;speech enhancement model;speech enhancement;speech enhancement problem;masks multichannel speech;enhanced multichannel speech;mask based speech;based multichannel speech;noisy speech enhanced;noise mask;noise mask proposed;loss noise mask;multichannel speech;single channel speech;mask based beamformer;speech enhanced speech;channel speech;enhanced speech obtained;speech enhanced;soft masks multichannel;training single channel;enhancement using beamformer;enhanced speech"}, "0909fee90833e20913adb553bf6667c9a3b854b0": {"ta_keywords": "flexible learning wrapper;learning wrapper;wrapper learning widely;handle wrapper learning;learning wrapper learning;wrapper learning;learning problem wrapper;problem wrapper learning;tables learning modular;wrapper learning problems;tables learning;wrapper learning active;wrapper learning problem;types tables learning;flexible learning;present flexible learning;domains wrapper learning;tables able handle;able handle tables;tables;tables able;learning problems collection;learning widely used;handle tables;wrapper;learning active use;able handle wrapper;learning modular easily;literature problem wrapper;learning", "pdf_keywords": ""}, "4a45ace1f8c6a30ba00201b30acd93844b9797eb": {"ta_keywords": "carlo simulation proton;simulation proton;simulation proton proton;proton interaction;proton interaction ab;proton proton interaction;model proton proton;model proton;simple model proton;proton proton;carlo simulation;initio calculations ground;initio monte carlo;monte carlo simulation;proton;ab initio calculations;interaction ab initio;calculations ground state;initio calculations;interaction ab;monte carlo;simulation;ground state simple;ab initio monte;carlo;ab initio;study ab initio;interaction;state simple model;initio monte", "pdf_keywords": ""}, "dd3ba828dbbb17cf478f6840a37954f6ebc81770": {"ta_keywords": "dialog bidirectional attention;bidirectional attention based;short term memory;use bidirectional attention;bidirectional attention;attention based long;context dialog bidirectional;term memory;dialog bidirectional;level context utterance;context utterance left;term memory lds;context dialog;context utterance;level context dialog;attention based;word level context;dialog;utterance left;utterance left right;attention;utterance;word level;memory lds;long short term;memory;attention based lds;local word level;right level context;short term", "pdf_keywords": ""}, "81e57827bebb04305cc9e6c85e96c83951244ec2": {"ta_keywords": "efficiently predict polypharmacy;able predict polypharmacy;predict polypharmacy effects;predict polypharmacy;knowledge graph embeddings;polypharmacy effects using;drug class snp;graph embeddings approach;polypharmacy effects;graph embeddings;polypharmacy effects 16;knowledge graph;decagon model tensor;polypharmacy;model tensor decomposition;embeddings approach;embeddings approach combines;using knowledge graph;snp demonstrate outperforms;tensor decomposition approach;idea decagon model;tensor decomposition;snp;model tensor;embeddings;combines idea decagon;snp demonstrate;trials new drug;effects using knowledge;class snp", "pdf_keywords": "knowledge graph embedding;embeddings knowledge entities;embeddings knowledge;rank embeddings knowledge;effects knowledge graph;knowledge graph;novel knowledge graph;knowledge graphs;underlying knowledge graph;new knowledge graph;data knowledge graph;knowledge graph model;knowledge graph formulates;resulting knowledge graph;knowledge graphs model;knowledge graph flexible;relations knowledge graphs;graph embedding models;use knowledge graph;present knowledge graph;graph embedding model;based knowledge graph;structured knowledge graph;factorisation based knowledge;using knowledge graph;embedding approach predict;knowledge graph resulting;graph embedding approach;task knowledge graph;embedding vectors predict"}, "e98621050e52e9d8c60829d8d861e81ac86a8617": {"ta_keywords": "estimation amplitude single;estimation amplitude;estimation single point;amplitude single point;method estimation amplitude;point source map;single point source;point source method;source single point;map point source;source map point;amplitude single;point source single;point source based;source based estimation;estimation single;based estimation single;example estimation single;amplitude;map single point;method based estimation;single point method;map point;point source;example estimation;point method demonstrated;simple example estimation;estimation;source map single;single point", "pdf_keywords": ""}, "e2b097bce656db9215505659357263c43190194b": {"ta_keywords": "assigning reviewers paper;assigning reviewers;assigning reviewers given;dividing reviewers phases;dividing reviewers;empirically dividing reviewers;available dividing reviewers;problem assigning reviewers;reviewers phases uniformly;oracle optimal assignment;reviewers given paper;optimal assignment;reviewers phases conditions;reviewers phases;reviewers paper;reviewers paper based;random allows assignment;reviewers;assigning;reviews available dividing;set reviews;allows assignment nearly;set reviews available;review process;paper based review;oracle optimal;allows assignment;reviewers given;assignment nearly;good oracle optimal", "pdf_keywords": "assignment optimal reviewers;assignment reviewers optimal;assigning reviewers papers;reviewers optimal assignment;assigning reviewers paper;reviewers papers assignment;assigning reviewers optimal;reviewers optimal papers;assigned paper reviewers;minimized reviewers papers;assignment reviewers papers;assign reviewers optimal;paper assignment optimal;algorithms assigning reviewers;papers assigned reviewers;reviewers papers algorithm;optimal reviewers assigned;reviewers proportion papers;reviewers assigned paper;assignment problem reviewers;paper reviewers assigned;method assigning reviewers;distribution assigned reviewers;assigning reviewers;reviewer assigned paper;assigning reviewers reviews;assignment reviewers;optimal reviewers;pairs reviewers papers;assign reviewers"}, "c0c1950fb0a129b71a218ffa8b9fbc6d088cba2d": {"ta_keywords": "stochastic process emergence;stochastic growth stochastic;stochastic growth;growth stochastic process;growth stochastic;stochastic process;stochastic;states related emergence;stochastic process consider;class discrete states;discrete states;discrete states context;process consider emergence;discrete states related;states context stochastic;context stochastic growth;process emergence;process emergence new;consider emergence;new class discrete;emergence new class;consider problem emergence;emergence;consider emergence new;class discrete;problem emergence;problem emergence new;related emergence;emergence new;related emergence new", "pdf_keywords": ""}, "27f9b91bd7c70a99f578c8a5cb52d37e4123da47": {"ta_keywords": "weights regularize networks;regularize networks;convolutional neural networks;representation data convolutional;regularize networks large;topological structure activations;data convolutional;data convolutional neural;convolutional neural;networks;neural networks;activations regression weights;rank constraints activations;structure activations regression;convolutional;neural;activations regression;constraints activations regression;weights regularize;neural networks approach;constraints activations;networks approach;novel approach representation;representation data;structure activations;incorporate topological structure;equation motion proton;topological structure;regression weights regularize;motion proton", "pdf_keywords": "data deep;deep network tensor;training deep;tensor factorization deep;tensor convolution network;data deep learning;training deep neural;deep neural;trained tensor regression;contraction deep learning;training deep convolutional;convolution network tensor;deep learning experiments;deep learning end;learning tensortensor network;deep learning;tensor regression network;analysis data deep;tensor contraction deep;layers tensor networks;tensor networks;layers trained tensor;tensor regression layers;deep convolutional;trained tensor;deep network;deep convolutional neural;regression network tensor;kernel training deep;novel tensor network"}, "b09d49c3eacd93782a32ad16ab52f98a21ecc206": {"ta_keywords": "nonlinear conductivity conductor;calculation nonlinear conductivity;conductivity nonlinear;nonlinear conductivity;trivial conductivity nonlinear;conductivity nonlinear analogue;nonlinear response conductor;nonlinear analogue conductivity;trivial conductivity method;trivial conductivity conductor;response conductor perturbation;conductivity conductor;conductivity method;conductor perturbation;conductivity conductor non;conductor perturbation method;trivial conductivity;conductivity method applied;non trivial conductivity;analogue conductivity conductor;conductivity conductor based;conductivity;analogue conductivity;conductor based analysis;method applied conductor;conductor non trivial;analysis nonlinear response;response conductor;method calculation nonlinear;nonlinear response", "pdf_keywords": ""}, "1bf49ef0b33bf8fcc3ebdd16326db419f3af65d8": {"ta_keywords": "class manifolds representation;representation class manifolds;class distance points;class manifolds;hidden class distance;monte carlo representation;mc representation class;class distance;manifolds representation space;manifolds representation;representation hidden class;carlo representation hidden;represented monte carlo;representation class;carlo mc representation;carlo representation;representation space introduce;representation space;distance points represented;points represented monte;mc representation;points given class;manifolds;hidden class;class apply monte;monte carlo mc;points represented;representation hidden;monte carlo xmath0;explore monte carlo", "pdf_keywords": "representations soft nearest;entanglement soft nearest;maximizing entanglement representations;deep nearest neighbors;improving representations soft;loss characterize similarity;nearest neighbor loss;neural networks entangled;loss measure learning;representations adversarial;learning sensitive distance;embedding deep;entangled representations;representations learning;representations learning provide;soft nearest neighbor;entanglement representations;representations adversarial data;similarity structures entangled;deep nearest;dropout entangled representation;minimized decrease entanglement;nonlinear embedding deep;entangled representations deal;embedding deep learning;accurate representations adversarial;representations soft;similarity representation spaces;structures entangled representations;generative loss"}, "3a4f39dbb5e06a5fc55622315797da7a97cc76f6": {"ta_keywords": "word context encoded;encode global context;encode local context;context encoded text;context encoded;networks encode global;encode local global;xmath0 topological group;recurrent neural networks;encode global;target word context;genus_ xmath0 topological;context target word;neural networks encode;local context information;global context information;xmath0 topological;networks encode;local global context;encode local;recurrent neural;topological group;word context;architecture encode local;neural;global context;feedforward recurrent neural;local context;neural network;encode", "pdf_keywords": "forward recurrent networks;recurrent neural networks;forward recurrent neural;recurrent networks;forward neural model;forward neural;feed forward recurrent;embedding target words;neural model forward;recurrent networks refined;word embeddings target;new forward neural;recurrent neural;word embeddings;sentence based embedding;softmax;target words sequence;forward recurrent;concatenated word embeddings;sequence words model;deep neural;neural architecture word;deep neural architecture;neural network;neural networks encode;context target word;forward error prediction;neural architecture;layer represent words;neural networks"}, "b9a701c90f3d3df27366f5b29a97f798eb940ac7": {"ta_keywords": "dynamics underlying language;sequential learning;discourse dynamics;dynamics language captured;long range language;structure discourse dynamics;dynamics language;approach sequential learning;discourse dynamics underlying;multidimensional learning;sequential learning approach;power multidimensional learning;leveraging structure discourse;underlying language;learning;language captured;learning leveraging structure;understand dynamics language;multidimensional learning leveraging;language approach;structure discourse;learning approach;range language approach;range language;language;discourse;language approach leverages;sequential;learning approach outperforms;learning leveraging", "pdf_keywords": "long segment narrative;range language models;language models;language model segment;language model novel;language models lrlms;segments sampled narrative;standard language models;language model models;language model;language model trained;segment narrative;range language model;long range language;language model discover;segment narrative set;level language model;language models evaluate;based language model;rv based language;language models compare;narrative use dataset;chapter break challenge;segment level language;narrative set;use language model;long document summarization;long structured;identify beginning long;chapter breaks"}, "2559417f8a3d6ab922cfa824b43f9f0c642a1dae": {"ta_keywords": "entity recognition ner;named entity recognition;entity recognition;formalism extracting entities;extracting entities;similarity named entity;extracting entities data;entity level similarity;named entity;entities data;sequential classifying words;formalism extracting;semi data integration;entities data combines;entities;classifying words;classifying words allows;useful features entity;new formalism extracting;combines semi data;data integration methods;features entity;data combines semi;data integration;semi data;recognition ner;entity;based sequential classifying;sequential classifying;extracting", "pdf_keywords": ""}, "3ad3ba8d7fc793a19dfe6a87e32453449195c074": {"ta_keywords": "speech text autoencoders;end speech recognition;decoders automatic speech;speech training datasets;text autoencoders;speech recognition asr;autoencoders;large speech training;text autoencoders tts;automatic speech recognition;automatic speech;autoencoders tts;speech recognition ssn;training semi supervised;semi supervised end;autoencoders tts share;speech training;speech recognition;encoders decoders automatic;ssn large speech;semi supervised;performance semi supervised;speech text;recognition asr;end end speech;recognition asr model;end speech;supervised end end;supervised end;encoders", "pdf_keywords": ""}, "765bdcf27ebc1eb03a14f1e47aefa4dda1e03073": {"ta_keywords": "word representations robust;representations robust training;learn representations robust;representations robust;noisy texts model;training noisy texts;invariant word representations;robust training noisy;robust training;character convolutional neural;representations robust multiple;based character convolutional;noisy texts;word representations;character convolutional;model robustness;model robustness structure;texts model;learn representations;increase model robustness;robust;simultaneously learn representations;model based character;robustness structure;robustness structure invariant;robustness;texts model based;structure invariant word;neural network able;representations", "pdf_keywords": "word representations robust;representations robust training;learn representations robust;robustness neural;robustness neural machine;invariant word representations;representations robust;increasing robustness neural;robustness machine translation;invariant word representation;increase robustness character;robust training noisy;models word representation;robustness character based;training noisy texts;noisy texts model;robustness character;word representation ensemble;robust training;character convolutional neural;based character convolutional;word representations;adversarial examples recurrent;models trained noisy;word representation;word representation based;character convolutional;training adversarial;increased robustness machine;character based cnn"}, "1263e36598dd95cc4becf0e18398f832bb5cf337": {"ta_keywords": "resource languages encoder;languages encoder decoder;languages encoder;encoder use systems;low resource languages;encoder decoder;encode behavior encoder;encoder decoder approach;new encoder decoder;behavior encoder;encoder;resource languages;encoders;behavior encoder use;encode behavior;encoders concept;used encode behavior;encoder use;decoder;new encoder;present new encoder;receivers used encode;encode;encoders concept receivers;concept encoders;known concept encoders;decoder approach;explore behavior encoder;decoder approach based;based concept encoders", "pdf_keywords": ""}, "978582ad754eab481856d62bdc7b0ee5bcf21811": {"ta_keywords": "unlabeled datasets federated;clustering unlabeled datasets;datasets federated;clustering unlabeled;datasets federated environment;clustering information individual;cluster information;clustering information;reconstruct cluster information;datapoints reconstruct cluster;obtain clustering;clustering;problem clustering unlabeled;obtain clustering information;federated algorithm;iterative federated algorithm;clusters proposed method;cluster;models obtain clustering;federated environment statistical;unlabeled datasets;iterative federated;used obtain clustering;clustering properties clusters;clustering properties;clusters;proposed iterative federated;reconstruct cluster;obtain clustering properties;federated algorithm ifca", "pdf_keywords": ""}, "675098c4611b13920d163a9a9b972da7751460cb": {"ta_keywords": "recurrent neural network;latent topic models;word embedding latent;spoken language understanding;improved spoken language;word embedding;topic models pre;large scale corpora;embedding latent topic;recurrent neural;topic models;based word embedding;neural network rnn;spoken language;novel recurrent neural;network rnn;language understanding;embedding latent;scale corpora based;dialog;corpora based word;word turn sequences;latent topic;corpora based;language understanding using;network rnn architecture;entire dialog;approach improved spoken;rnn architecture;pre train networks", "pdf_keywords": ""}, "dd64013273eb4398821bf2fc8f024735466e5a1d": {"ta_keywords": "learns procedural knowledge;simulation human learning;student learns procedural;algorithm student learns;learns procedural;procedural knowledge;procedural knowledge example;perceptual representation learning;student plausible simulation;student learns;human learning;learning algorithm student;engineering effort learning;integrating perceptual representation;knowledge engineering effort;learns;knowledge engineering;prior knowledge engineering;method integrating perceptual;problem solving experience;effort learning;perceptual representation;solving experience;plausible simulation human;sim student plausible;representation learning algorithm;simulation human;learning;integrating perceptual;knowledge example", "pdf_keywords": ""}, "1abbe9b6bf3f134ce86e618bba83bf5c94f60f03": {"ta_keywords": "parametric distributed crowdsourcing;prediction mechanism agents;crowdsourcing platform agents;agents parametric model;agents assumed experts;distributed crowdsourcing;agents characterized expertise;agents parametric;crowdsourcing;crowdsourcing platform;modeling behavior agents;distributed crowdsourcing platform;behavior agents parametric;agents optimally designing;experts parametric model;prediction mechanism;subset agents;agents;behavior subset agents;agents optimally;platform agents;subset agents optimally;agents assumed;assumed experts parametric;parametric model training;mechanism agents assumed;experts parametric;platform agents characterized;behavior agents;mechanism agents", "pdf_keywords": "incentive mechanism prediction;mechanism optimal prediction;optimal joint incentive;prediction mechanism estimation;utility obtained prediction;joint incentive mechanism;prediction agent;agent gains prediction;prediction mechanism;agent estimating;prediction agent outcome;incentive mechanism;parameter predicting winner;appropriate incentive mechanism;prediction utility;optimal prediction;joint incentive;incurred prediction utility;facilitate parametric prediction;prediction minimal cost;incentive mechanism design;agents using statistical;make informed prediction;agents private information;agent private information;agent able estimate;model prediction agent;agents optimal;new incentive mechanism;problem incentivize agents"}, "a2ea2261bd56ae2505750d7571b501d9836175f0": {"ta_keywords": "complementary acoustic systems;constructing complementary acoustic;complementary acoustic;acoustic systems automatic;noisy speech recognition;speech recognition proposed;systems automatic speech;automatic speech recognition;speech recognition;acoustic systems;automatic speech;approach constructing complementary;speech recognition experiments;complementary optimized lattice;constructing complementary;complementary optimized;regarded complementary optimized;noisy speech;criterion regarded complementary;highly noisy speech;algorithm proposed;recognition proposed;optimized lattice based;recognition proposed method;complementary;algorithm proposed method;update algorithm proposed;parameter update algorithm;acoustic;based mutual information", "pdf_keywords": ""}, "39456ca31a530d85ec182b2676dc94266dada597": {"ta_keywords": "compositional distributional semantics;tensors compositional distributional;distributional semantics low;distributional semantics;distributional semantics compare;tensors compositional;semantic similarity;semantic similarity tasks;similarity representations words;rank tensors compositional;models compositional distributional;standard semantic similarity;learning models compositional;compositional distributional;low rank tensors;semantics low rank;representations standard semantic;tensors;similarity representations;tasks matrices tensors;matrices tensors;rank tensors;semantic;distributional;capture similarity representations;tensors demonstrate;semantics low;models compositional;matrices tensors demonstrate;representations words", "pdf_keywords": ""}, "bc8e67d532693818eb33aa8e401260fe2b774a18": {"ta_keywords": "study asymptotic behaviour;asymptotic behaviour;study asymptotic;asymptotic;behaviour;study", "pdf_keywords": ""}, "29dc4b10e60ed1e2b84598cc6f2622c786841fdf": {"ta_keywords": "control mobile robots;mobile robots;mobile robots subject;control mobile;framework control mobile;robots subject;robots;propose framework control;framework control;control;framework;article propose framework;propose framework;mobile;article propose;subject;article;propose", "pdf_keywords": "temporal logic specifications;abstraction based control;temporal logic systems;control barrier functions;control barrier function;control mobile robots;quadratic control barrier;functions temporal logic;specification minimizing control;linear temporal logic;synthesis controllers constrained;barrier functions temporal;temporal logic tools;using control barrier;control mobile robotic;use control barrier;specifications using control;quadratic programming robot;barrier function based;program based controller;robotic systems control;robots proposed controller;based quadratic control;framework synthesis controllers;function based controller;control barrier;constraints robotics proposed;temporal logic tasks;temporal logic lht;control architecture"}, "3f0f6c19c6f5d4e4d5066984c5f3e922a2c2ff85": {"ta_keywords": "learned language abstraction;exploration learned language;reinforcement learning learn;language train agents;language abstraction novel;sparse reinforcement learning;learned language;language abstraction;approach learn agents;learn agents capable;using learned language;agents using learned;learn agents;sparse reinforcement;reinforcement learning;language train;combines reinforcement learning;capable understanding language;reinforcement learning ability;features sparse reinforcement;agents capable understanding;write language train;ella exploration learned;abstraction novel;ella combines reinforcement;language instructions ella;learned language high;abstraction novel approach;abstraction;learning learn", "pdf_keywords": "abstraction reinforcement learning;language abstraction reinforcement;language reinforcement learning;abstraction reinforcement;level reinforcement learning;learned language abstraction;language abstraction learns;introduce _reinforcement learning;encoding reinforcement learning;low level reinforcement;learning reinforcement;_reinforcement learning;exploration learned language;reinforcement learning reinforcement;language reinforcement;abstraction learns;learning reinforcement learning;reinforcement learning rl;encoding reinforcement;level reinforcement;_reinforcement learning powerful;operating sparse reward;reinforcement learning;context reinforcement learning;reinforcement learning task;sparse reward;novel encoding reinforcement;reinforcement learning pre;learning rl agents;natural language reinforcement"}, "81ea04f822a1d5317e5846783900ac424a8f7528": {"ta_keywords": "narratives children autism;utterances narratives children;children autism spectrum;children autism;differences children autism;autism spectrum;autism spectrum disorders;single utterances narratives;autism;features single utterances;utterances narratives;single utterances;utterances;narratives children;automatic classifiers;classifiers;characteristic differences children;used automatic classifiers;disorders characteristic;disorders characteristic differences;children used automatic;differences children;narratives;children;disorders differences;spectrum disorders characteristic;types children;disorders differences types;differences types children;types children used", "pdf_keywords": ""}, "98fdc7e1e167eb465cdb1c8ee0800db750101155": {"ta_keywords": "observed prosodic changes;prosodic changes observed;prosodic features speech;prosodic changes;investigate prosodic features;variation observed prosodic;variation prosodic changes;spectral variation prosodic;prosodic features;changes observed utterances;observed prosodic;speaker spectral variation;investigate prosodic;predicting intra speaker;variation prosodic;observed utterances;predicts intra speaker;prosodic;intra speaker spectral;features speech;speaker spectral;speech samples;evaluations using speech;utterances;features speech propose;using speech samples;observed utterances sentence;using speech;speech;speech samples sentence", "pdf_keywords": ""}, "ae25ca24eb6c2772ef88e2d0315fc428feb8553e": {"ta_keywords": "information extraction;information extraction process;tosupervised information extraction;extract meaningful sets;extracted data;extracted data present;consists clustering data;extract meaningful;performance extracted data;method tosupervised information;able extract meaningful;extraction process consists;knowledge base;consists clustering;existing knowledge base;meaningful sets entities;method able extract;clustering data;clusters meaningful sets;extraction;extraction process;entities high accuracy;clustering;sets entities;extracted;extract;process consists clustering;sets entities high;clustering data clusters;data clusters meaningful", "pdf_keywords": ""}, "689ab475e8a0f552bf6e39a2f774d9d20e50b9cb": {"ta_keywords": "generation highly entangled;highly entangled state;entangled state single;state single photon;entangled state;atom excited laser;entangled;single photon;highly entangled;single photon method;level atom excited;pulse level atom;interacting level atom;atom excited;level atom interacting;excited laser pulse;photon method;excited laser;photon;atom interacting level;photon method based;using level atom;level atom;atom interacting;laser pulse;use level atom;laser;laser pulse method;laser pulse level;atom", "pdf_keywords": ""}, "38a73e6f48d057cb58264f5148f8b05522d0d030": {"ta_keywords": "semantic parsing;semantic parsing task;parsing;semantic;natural language nl;parsing task;parsing task translating;language nl utterances;machineable meaning representation;nl utterances;nl utterances machineable;language nl;natural language;utterances machineable meaning;translating natural language;utterances;utterances machineable;task translating natural;representation mller task;meaning representation mller;mller task;language;mller task important;meaning representation;task translating;translating natural;nl;representation mller;meaning;translating", "pdf_keywords": ""}, "045f90129a8d7148eec4a58770bc4166b51330ca": {"ta_keywords": "spatiotemporal characteristics parking;characteristics parking demand;parking demand based;averaged demand parking;parking demand;demand parking;demand parking spot;quantify spatiotemporal characteristics;quantify spatiotemporal;method quantify spatiotemporal;characteristics parking;quantify spatial autocorrelation;parking spot based;spatial autocorrelation spatial;spatiotemporal characteristics;empirical data spatial;parking;data spatial auto;spatial autocorrelation;spot based bayesian;demand use bayesian;bayesian mixture empirical;variable spatial autocorrelation;autocorrelation spatial;spatiotemporal;parking spot;spatial auto;autocorrelation spatial variable;spatial autocorrelation consistent;based bayesian mixture", "pdf_keywords": ""}, "fd0aa185be4e1f1fe3975779aec179348ec19ea8": {"ta_keywords": "review paper visibility;paper visibility affiliations;gradient synchrotron model;research papers alternating;visibility affiliations higher;study visibility affiliations;visibility affiliations;review paper;searching review paper;alternating gradient synchrotron;quantitative study visibility;majority reviewers self;synchrotron model;papers alternating gradient;reviewers self;gradient synchrotron;synchrotron model arxiv;arxiv majority reviewers;papers alternating;majority reviewers;synchrotron;research papers;reviewers self report;affiliations;paper visibility;post papers arxiv;reviewers;review process;affiliations higher;visibility affiliations degree", "pdf_keywords": "published online reviewers;online peer review;blind peer review;online reviewers;online preprints research;submissions internet review;preprints online authors;peer review empirical;blind reviewing authors;preprints published online;popularity online preprints;online reviewers review;posting preprints online;publish research online;accurate peer review;visibility reviewers searching;results online peer;measuring visibility reviewers;preprints online conduct;reviewing authors aware;double blind reviewing;reviewers likely preprints;online preprints;online reviewwe conducted;online authors;internet rank authors;visibility preprints published;reviewing publishing research;blind reviewing;research published online"}, "f837bf72e5b864e1c162e924fed59b778e946e23": {"ta_keywords": "embeddings classic guessing;guessing context game;accuracy mystery game;encoders learn context;category aware embeddings;mystery game improves;auto encoders learn;aware embeddings classic;guessing game recently;guessing game;aware embeddings;improves accuracy guesswhat;context game;classic guessing game;context game based;mystery game;game improves accuracy;auto encoders;game based regularized;encoders learn;gameplay accuracy mystery;guessing guessing context;gameplay accuracy;regularized auto encoders;embeddings classic;guess downshot scenario;accuracy guesswhat;guessing context;embeddings;released guess downshot", "pdf_keywords": "learning conceptual representations;aware embeddings perceptual;learns context;learns context category;category aware embeddings;learning learns context;embeddings perceptual information;learn embeddings;representations context aware;learning learn embeddings;embeddings perceptual;aware embeddings;embeddings informative objects;recognizing objects context;encoders learn context;learning embeddings;object representation imagination;able learn context;aware embeddings able;conceptual representations context;conceptual representations;learn embeddings informative;grounded conceptual representations;object representation;learn context aware;category aware objects;object guess proc;learning conceptual;learn context;perceptual module guess"}, "df4e3aa275b8f81e22a5332ab550805083094dae": {"ta_keywords": "translation data neural;parallel neurons;set parallel neurons;parallel neurons use;parallel images model;parallel images;computational model translation;translation data;data neural;neural network nm;set parallel images;model translation data;model translation;neural network;neural;generate set parallel;data neural network;generation set parallel;neurons use generate;neurons;images model;unsupervised generation;parallel;unsupervised generation set;set parallel;network nm model;rich unsupervised generation;neurons use;images;network nm", "pdf_keywords": "document level neural;neural machine translation;translation generation;underlying document train;machine translation generation;neural machine nle;document train;networks inform translation;models efficient translation;translation generation encourage;efficient translation document;machine translation nnt;machine translation;workshop neural machine;workshop neural;translation document level;document level parallel;document level models;translation generation wngt;timeneural machine translation;knowledge neural;machine nle model;generation summary document;knowledge neural machine;machine nle;translation document document;tasks generation;discovery new documents;efficient translation;knowledge underlying document"}, "e5efd7e2087e58c5a8860398dfcf143aa9dc865e": {"ta_keywords": "event boundary detection;beled event detection;acoustic driven event;boundary detection supervised;detection supervised label;event detection;deep neural;event detection proposed;detection supervised;inference using deep;supervised label inference;label inference;supervised label;deep neural network;using deep neural;label inference using;event boundary;driven event boundary;unsupervised supervised;supervised;boundary detection;using deep;benefits unsupervised supervised;event;combines acoustic driven;beled event;acoustic driven;detection;unlabeled data making;large amounts unlabeled", "pdf_keywords": "acoustic event classification;sound event detection;acoustic event detection;supervised acoustic event;detect sound events;annotate sound events;learning acoustic event;detection supervised acoustic;events acoustic scene;sound events acoustic;characterizing acoustic events;sound event classes;events acoustic;classification annotate sound;sound events;acoustic events;sound event;acoustic driven event;acoustic event;detection combines acoustic;sound events complex;supervised acoustic;method sound event;offset sound events;classify sound;events complex acoustic;approach detect sound;acoustic events presence;acoustic scenes;event classes sound"}, "0ae93646ad058853eb6424c1dc0ec1559414e5af": {"ta_keywords": "models automatic verification;estimates automatic language;automatic language;automatic verification estimates;automatic language processing;language processing models;automatic verification;processing models automatic;uncertainty estimates automatic;language processing;models automatic;instance rejection supervised;rejection supervised;model data uncertainty;rejection supervised unsupervised;verification estimates;supervised;filter model predictions;supervised unsupervised;estimates automatic;data uncertainty estimates;processing models;verification estimates used;data uncertainty;model predictions;difficult instances prioritised;uncertainty estimates;models;incorporating model data;language", "pdf_keywords": "rumours deep learning;uncertainty deep conversation;rumour verification uncertainty;conversation based deep;input rumour verification;rumours estimated neural;deep conversation based;verify veracity rumour;uncover rumours deep;predictions deep learning;use predictions deep;rumour verification use;rumour verification demonstrate;rumour verification difficult;predictions deep;veracity rumour;deep conversation;rumour verification;process rumour verification;rumour model;rumour model subject;veracity rumour propose;uncertainty deep;message predicted epistemic;learning predictions;deep conversation investigate;predict spread rumour;epistemic uncertainty supervised;process rumour;instances predicted epistemic"}, "0c12e4c611b32997f8be5811021ead80395a7e5c": {"ta_keywords": "lorentz violation xmath0;violation xmath0 xmath1;xmath9 xmath10 xmath11;xmath8 xmath9 xmath10;violation xmath0;xmath10 xmath11 xmath12;xmath9 xmath10;xmath7 xmath8 xmath9;xmath10 xmath11;xmath0 xmath1 xmath2;lorentz violation;xmath0 xmath1;xmath11 xmath12 xmath13;xmath8 xmath9;xmath1 xmath2;xmath2 xmath3 xmath4;xmath11 xmath12;xmath5 xmath6 xmath7;xmath1 xmath2 xmath3;xmath3 xmath4 xmath5;xmath6 xmath7 xmath8;xmath3 xmath4;xmath2 xmath3;xmath4 xmath5 xmath6;xmath0;xmath12 xmath13;xmath9;xmath5 xmath6;xmath10;xmath1", "pdf_keywords": ""}, "9af2264799bdc3490e4650e2f5d126762caf420f": {"ta_keywords": "joint attention eev;end speech recognition;attention eev eev;attention based encoder;attention eev;eev chan attention;attention eev chan;attention based end;chan attention based;encoder decoder baselines;speech recognition;methods joint attention;recognition using eev;method joint attention;eev chan framework;eev eev chan;joint attention;decoder baselines;attention based;using eev;end end speech;eev chan;using eev eev;speech recognition using;end speech;chan attention;encoder decoder;based encoder;eev eev;decoder baselines shown", "pdf_keywords": "end speech recognition;attention based encoder;attention encoder;connectionist temporal classification;attention encoder decoder;encoder decoder attention;sequences attention model;encoder trained attention;based attention encoder;generalization attention encoder;speech recognition learns;decoder attention;attention model speech;attention model trained;speech label sequences;encoder decoder recurrent;sequences attention;encoder modified attention;trained attention ctc;decoder recurrent neural;deep encoder;speech recognition;recurrent neural network;predictions attention based;encoder decoder trained;temporal classification ctc;character sequences attention;encoder trained;attention model;decoder recurrent"}, "9d555ed29496850c4ef8a3facd7dce734c86aae7": {"ta_keywords": "code open source;codecompletion tool;code editor;output codecompletion tool;codecompletion;codecompletion tool results;code editor use;output codecompletion;standard code editor;evaluated output codecompletion;language input evaluation;open source project;open source;input evaluation code;code;input standard code;natural language model;model language input;evaluation code;total code;evaluation code open;complete code;natural language;language model;use natural language;model language;language model language;language input;code open;source project model", "pdf_keywords": ""}, "7d9863258ef44ca8a6b87b68be738f7a83ac849a": {"ta_keywords": "speech recognition asr;architecture speech enhancement;automatic speech recognition;speech enhancement neural;speech recognition;end automatic speech;unified architecture speech;speech enhancement;automatic speech;neural beamformer framework;integrating acoustic phonetic;neural beamformer;enhancement neural beamformer;acoustic phonetic language;recognition asr unified;architecture speech;acoustic phonetic;recognition asr;systems integrating acoustic;phonetic language models;integrating acoustic;asr unified architecture;phonetic language;asr unified;phonetic;acoustic;beamformer;beamformer framework;asr;single neural network", "pdf_keywords": ""}, "c6c18ad62f39060e2547a0b683525e83312d0700": {"ta_keywords": "incentives reviewers bidding;reviewers bidding;incentives reviewers;reviewers bidding phase;reviewers prices paper;assigning budgets reviewers;proportional demand reviewers;assignment bidding;demand reviewers;reviewers prices;assignment bidding scheme;budgets reviewers prices;analysis incentives reviewers;assignment paper reviews;budgets reviewers;bidding scheme competitive;resulting assignment bidding;distribution analysis incentives;demand reviewers resulting;novel bidding;bidding;propose novel bidding;novel bidding scheme;analysis incentives;bidding scheme assignment;bidding scheme;paper reviews;prices paper roughly;prices paper;reviewers resulting assignment", "pdf_keywords": "incentives reviewers bidding;simple paper bidding;papers bidding scheme;paper bidding mechanism;paper bidding;reviewers based bidding;papers bidding;papers based bid;bidding assignments based;reviewers bidding;probability papers bidding;proportional bidding;proportional bidding scheme;based bidding;bidding mechanism;algorithm outcome bidding;assign papers participating;bidding assignments;bidding algorithm allows;reviewers choose bidding;incentives reviewers;bidding scheme paper;assigning papers reviewers;bids paper deals;bids paper;reviewers price paper;inspired bidding scheme;called proportional bidding;based bidding behavior;assigning papers participating"}, "be4d47a61fee83d332ca2f3fe097f19f63863d6c": {"ta_keywords": "graphs node clustering;node clustering performances;node clustering;algorithms probabilistic spectral;clustering performances;clustering performances normalized;clustering performances state;compares node clustering;impact node clustering;probabilistic spectral;probabilistic spectral families;clustering;graphs node;algorithms propose stochastic;undirected graphs node;normalized cut method;normalized cut;propose stochastic block;stochastic block;families undirected graphs;performances normalized cut;stochastic block model;algorithms probabilistic;undirected graphs;state art algorithms;algorithms propose;graphs;node;algorithms;art algorithms probabilistic", "pdf_keywords": ""}, "554eade16fb6040bbd21a72bacf903245d7458f1": {"ta_keywords": "human reasoning proposed;reasoning proposed vision;vision artificial intelligence;human reasoning;ability artificial intelligence;artificial intelligence able;intelligence able process;intelligence make decisions;artificial intelligence make;reason human reasoning;artificial intelligence;make decisions metrics;vision artificial;decisions metrics;reasoning proposed;decisions metrics evaluated;metrics measure ability;intelligence;proposed vision expressed;intelligence make;vision expressed;intelligence able;measure ability artificial;process reason human;reasoning;ability artificial;advance vision artificial;decisions evaluated using;proposed vision;make decisions evaluated", "pdf_keywords": "capabilities lacking ai;machine learning thinking;ai;lacking ai;capabilities artificial intelligence;lacking ai example;ai example;artificial intelligence;ai example adaptability;machine learning determined;future artificial intelligence;artificial intelligence systemwe;intelligence;capabilities artificial;learning determined ability;intelligence systemwe;artificial intelligence multi;ability recognize;similar capabilities artificial;learning consciousness;learning consciousness recently;human capabilities;human capabilities lacking;quality machine learning;determined ability recognize;machine learning;learning determined;intelligence multi;consciousness learning;machine learning connections"}, "b8f5f3c8816ab389c2f366fd8a45603550ea9667": {"ta_keywords": "biomedical text mining;text mining deepdirectional;text mining training;stm text mining;text mining;biomedical text;reliability biomedical text;constructing genetic association;genetic association database;mining deepdirectional resistance;mining deepdirectional;mining training;mining training directly;deepdirectional resistance organizing;resistance organizing;simulate behavior researcher;genetic association;example constructing genetic;resistance organizing behaviors;biomedical;constructing genetic;stm text;organizing behaviors;reliability biomedical;improve reliability biomedical;deepdirectional resistance;organizing behaviors demonstrate;association database;text;training directly simulate", "pdf_keywords": ""}, "5e327c2285ddf2a76d08e5c00d16c7358bc5412c": {"ta_keywords": "assigning evaluator submission;submission maximizes expertise;evaluator submission maximizes;given submission evaluator;submission evaluator;assignment quality guarantees;submission evaluator analyze;assign given submission;evaluator submission;assignment assignment quality;assignment quality;algorithms assignment assignment;algorithms assignment;conference peer review;expertise required assign;assigning evaluator;conference peer;submission maximizes;strategyproofness submission;given submission;maximizes expertise required;submission;analyze strategyproofness submission;guarantees evaluate methods;guarantees evaluate;evaluator;required assign given;peer review;assignment assignment;strategyproofness submission establish", "pdf_keywords": "peer evaluations;process peer assessment;peer assessment;peer assessment powerful;possible peer evaluations;form peer assessment;peer assessment peer;peer assessment competitive;peer assessment frequently;peer assessment involves;partition assignment agents;arbitrary assignment submissions;assignment partitioning studied;strategyproof assignment partitioning;peer assessment employees;peer evaluations use;assessment peer;similarity peer assessment;evaluations data peer;peer grading;assessment peer grading;papers peer assessment;submissions agents partitioning;optimality mechanism peer;peer grading homeworks;agents partitioning submissions;process peer;mechanism peer;describing optimal assignment;organizations process peer"}, "f878a7c756b90c0ed612838492fbbc02ecaaab70": {"ta_keywords": "skill problem solving;problem solving experience;learning using skill;study effectiveness learning;effectiveness learning;interleaved problem order;effective learning;using skill problem;effectiveness learning using;solving experience;learning blocked;learning;learning using;skill problem;learning blocked order;yields effective learning;problem solving;performance learning blocked;using skill;solving experience compare;study;problem type interleaved;study effectiveness;problem order;compare performance learning;interleaved order problems;student switched problem;interleaved problem;problem type;performance learning", "pdf_keywords": ""}, "1ee276db29ba9127e81d9a7d9cb08f5138339412": {"ta_keywords": "distributed coding scheme;novel distributed coding;distributed coding;optimal computation decoding;coding scheme;coding scheme based;computation decoding costs;dimensional product codes;codes scheme;product codes scheme;computation decoding;codes scheme allows;decoding costs;decoding costs achieving;decoding;product codes;coding;near optimal compute;order optimal computation;optimal compute time;optimal compute;optimal computation;novel distributed;codes;distributed;scheme based dimensional;present novel distributed;based dimensional product;computation;scheme allows order", "pdf_keywords": ""}, "34cb1f081c1d1d6b3dc16a9278940a9ee85fb2e0": {"ta_keywords": "performance machine translation;machine translation interpreter;translation interpreter;simultaneous performance interpreter;translation interpreter approach;simultaneous performance languages;translation output quantitative;performance languages equivalence;machine translation output;machine translation;quality machine translation;performance interpreter;performance languages;performance interpreter use;interpreter approach combines;languages equivalence equivalence;languages equivalence;simultaneous performance estimation;interpreter approach;translation output;model simultaneous performance;simultaneous performance;interpreter;simultaneous performance machine;predict simultaneous performance;predicting simultaneous performance;interpreter use;interpreter use model;performance estimation;translation", "pdf_keywords": "estimate interpreter performance;interpreter output quality;predicting simultaneous interpreter;output machine translation;machine translation output;translation output experiments;interpreter performance;interpreter performance building;interpreter features;interpreter support sentence;simultaneous interpreter performance;delivered interpreter compared;information delivered interpreter;interpreter compared;machine translation addressed;ablated interpreter features;machine translation improved;machine translation;interpreter features extracted;qe machine translation;evaluate interpreter output;accuracy machine translation;interpreter performance approximated;interpreter output;translation systems results;language data evaluate;interpreter;interpreter based;estimate interpreter;evaluate interpreter"}, "839cbcf5c13d5875e952e40ec2da14b19eee2202": {"ta_keywords": "smooth convex optimization;unconditional optimization;unconditional optimization problems;convex optimization;convex optimization problems;solving unconditional optimization;propose accelerated descent;accelerated descent;accelerated descent method;conditional optimization;conditional optimization problems;approach conditional optimization;gradient available propose;optimization problems gradient;descent method random;smooth convex;optimization problems obtain;optimization;optimization problems;descent method;gradient available;convex;consider smooth convex;estimates rate convergence;direction non prox;rate convergence method;available propose accelerated;propose accelerated;rate convergence;structure solving unconditional", "pdf_keywords": "nonlinear transition condensate;dynamics condensate atoms;condensate atoms harmonic;quasithe dynamics condensate;dynamics condensate;atomsthe dynamics nonlinear;atoms transition condensate;condensate interaction transition;atoms harmonic potential;nonlinear dynamics transition;considered nonlinear transition;nonlinear transition;transition nonlinear;treated nonlinear transition;phase transition nonlinear;condensate interaction;phase condensate atoms;potential nonlinear;condensate atoms considered;transition condensate;dynamics harmonic;interactionthe dynamics harmonic;transition quasithe dynamics;nonlinear potential model;nonlinear potential;condensate atoms;condensate condensate interaction;considered potential harmonic;nonlinear potential nonlinear;transition condensate ground"}, "ebcb425ed1a51e8f1a6eca422882abd454fe04f2": {"ta_keywords": "use library learners;learners named library;library learners provides;named library learners;library learners;library learners identifies;second language learners;learners use classroom;learners use;class learners use;generates class learners;learners provides;improving second language;class learners;learners;use classroom;learners named;language learners;language learners named;use library;classroom based use;learners identifies;use classroom based;use classroom designed;learners identifies generates;learners provides set;classroom;based use library;useful information students;provides set learners", "pdf_keywords": "lexical information target;providing rich lexical;extract vocabulary information;extract vocabulary;word sense disambiguation;knowledge target vocabulary;vocabulary information assist;rich lexical information;learning extract vocabulary;users learning vocabulary;target vocabulary approach;vocabulary information;lexical information;target vocabulary;vocabulary learning linggle;predicting semantics;learning vocabulary;vocabulary learning;vocabulary approach grammar;sense disambiguation;predicting semantics natural;present prototype vocabulary;natural language processing;prototype vocabulary learning;vocabulary method;vocabulary approach;adjectives extract collocations;disambiguation;vocabulary acquisition comparing;rich lexical"}, "4bff8862ba7956fdc2288e8399fb187b9595982b": {"ta_keywords": "gene mention recognition;gene text evaluated;gene expression annotated;mention recognition scientific;gene text;annotated corpus performance;expression annotated corpus;measure gene expression;corpus performance;annotated corpus;corpus performance measure;highlight humanly quantified;recognition scientific text;performance measure gene;read gene text;gene mention;mention recognition;gene expression;corpus;scientific text present;expression annotated;measure gene;humanly quantified performance;scientific text;workshop gene mention;scientific text organized;recognition scientific;ability read gene;gene;text present humanly", "pdf_keywords": ""}, "d8aeb318f68f4635b34c72aa1a0369fadcd79450": {"ta_keywords": "user communities microblogging;communities microblogging;communities microblogging networks;mining user community;user topic distribution;derive user communities;user community behaviors;topic distribution;user communities;distribution experiments twitter;topic distribution experiments;microblogging networks;microblogging networks based;microblogging;networks based sentiments;experiments twitter datasets;topics community;emotional topics community;twitter datasets model;topics derive user;experiments twitter;community behaviors;derive user topic;hidden topics derive;hidden topics;probabilistic graphical model;user community;uncover hidden topics;topics community demonstrate;propose probabilistic graphical", "pdf_keywords": ""}, "8442f9fd620ea34e1de3128b9388bddd1263f29b": {"ta_keywords": "method conic optimization;conic optimization based;conic optimization named;conic optimization;integral projected gradient;optimization based extrapolated;optimization named extrapolated;novel method conic;method conic;projected gradient method;order method conic;projected gradient;optimization based;optimization;optimization named;gradient method exex;proportional integral projected;conic;integral projected;extrapolated proportional integral;based extrapolated proportional;gradient method;extrapolated proportional;named extrapolated proportional;gradient;infeasibility exex pipg;automatically detects infeasibility;detects infeasibility exex;infeasibility exex;infeasibility demonstrate application", "pdf_keywords": "dual conic optimization;method convex optimization;method convex programming;convex optimization method;constrained optimization;optimization conic optimization;constrained optimal control;convex cone optimization;solving convex optimization;method conic optimization;convex optimization;conic optimization named;constrained optimization eq;optimization conic;constrained optimal;primal dual method;convex programming;integral projected gradient;optimization problems primal;optimization method;convex programming popular;problem optimization conic;convex optimization problems;convex optimization applications;optimization problem conic;cone optimization;control nonlinear optimization;method multipliers convex;multipliers convex constrained;constraints proposed optimization"}, "ee24fb876e6f1b345d492101c499bc5dd6b8196b": {"ta_keywords": "probabilistic signal enhancement;eeg signal empirical;electroencephalogram eeg signal;electroencephalogram eeg;signal enhancement method;signal enhancement experimental;eeg signal;electroencephalogram;signal enhancement;artifacts probabilistic signal;p300 electroencephalogram eeg;p300 electroencephalogram;novel probabilistic signal;signal empirical evaluation;potentials p300 electroencephalogram;wiener filter spatial;probabilistic signal;multichannel wiener filter;using multichannel wiener;signal empirical;multichannel wiener;spatial correlation prior;correlation prior proposed;wiener filter;eeg;enhancement method;enhancement experimental evaluation;enhancement experimental;enhancement method using;filter spatial correlation", "pdf_keywords": ""}, "ce0fce520c639af010c71cc6adf57cdeb2790322": {"ta_keywords": "optimization automatic speech;automatic speech recognition;speech recognition;input speech recognition;speech recognition function;black box optimization;automatic speech;probabilistic black box;speech recognition performance;speech recognition consider;consider automatic speech;box optimization automatic;input speech;parameters input speech;recognition function tuning;optimization automatic;use black box;effectiveness black box;box optimization;box optimization techniques;black box;conditional distribution optimization;optimization techniques covariance;recognition performance word;recognition performance;recognition function;neural network;mean adaptation strategy;recognition consider automatic;distribution optimization", "pdf_keywords": ""}, "516a0faeab9ec3a68bc6e7ec13a2df235a27ab52": {"ta_keywords": "noisy unstructured clinical;diagnosis problems network;unstructured clinical data;accurate diagnosis decision;support accurate diagnosis;clinical data;diagnosis decision;accurate diagnosis;unstructured clinical;diagnosis decision making;diagnosis presence noisy;diagnosis problems;propagates data clinical;neural network;data clinical;clinical data method;novel method diagnosis;convolutional neural network;diagnosis;data clinical notes;list possible diagnosis;method diagnosis presence;diagnosis presence;proposed method diagnosis;method diagnosis;noisy unstructured;convolutional neural;possible diagnosis problems;based convolutional neural;clinical data demonstrate", "pdf_keywords": "cnn based text;cnn model predicting;cnn model classify;network cnn;disease supervised learning;cnn model;novel cnn model;present novel cnn;novel cnn;neural network cnn;notes use cnn;cnn based;use cnn;disease supervised;use cnn model;cnn;classification model medical;convolutional based text;characterizing disease coreference;performance cnn model;network cnn method;based cnn;disease coreference resolution;text classification model;performance cnn;model medical diagnosis;cnn method;evaluate performance cnn;diseases corpus;neural networks predict"}, "d3793ae5b3b31f72605978b749e41811e6dcacd4": {"ta_keywords": "autonomous reinforcement learning;agents actions constrained;constrained environment agents;autonomous reinforcement;maximizing constrained policy;approach autonomous reinforcement;reward maximizing constrained;reinforcement learning;learning allows agents;reinforcement learning allows;actions reward maximizing;reinforcement;constrained policy;reward maximizing;agents actions;agents assumed distributed;allows agents actions;actions constrained environment;environment agents;agents use approach;actions constrained;society agents;agents;environment agents assumed;actions reward;approach autonomous;policies novel ways;mix policies novel;mix policies;approach mix policies", "pdf_keywords": "learning inverse reinforcement;inverse reinforcement learning;reinforcement learning inverse;inverse reinforcement;apply inverse reinforcement;uses inverse reinforcement;learn maximize environment;agents learning policies;learning learn maximize;learning policies constrained;behavioral constraints learnt;reinforcement learning learn;reinforcement learning agent;learning reward;learning policies;demonstrations task reinforcement;reward learning games;reward learning;approach reward learning;reward function learner;learning inverse;learn underlying rewards;simple reinforcement learning;learn maximize;maximize environment rewards;reinforcement learning;learning agent;activities reinforcement learning;simple reinforcement;behavior reinforcement learning"}, "405c0d9b7cf5482d2e1197167f690e7b7801b9bd": {"ta_keywords": "unsupervised multi hop;hop based unsupervised;multi hop based;multi hop qg;learning performance hybrid;competent multi hop;hop qg achieves;hop qg framework;multi hop;hotpot qg datasets;supervised learning performance;hop qg;pretraining generated data;hop based;supervised;generated training data;datasets respectively pretraining;training data;supervised learning context;generated training;qg datasets;qg datasets respectively;supervised learning;based unsupervised multi;pretraining generated;context multi hop;unsupervised multi;training data used;respectively pretraining generated;learning performance", "pdf_keywords": "hop question answering;hop question generation;question generation hybrid;question generation;hop questions generated;question answering;hop supervised unsupervised;question answering qa;hop qa unsupervised;questions human annotated;unsupervised learning generated;generated questions quality;generate pairs questions;learning generated data;generated questions;questions help train;learning data generated;questions quality generated;filter ungrammatical questions;quality generated questions;learning generated;human annotated training;multi hop supervised;human annotated data;questions generated;generate set questions;hop supervised;hop training data;hop questions set;generation natural language"}, "7bf2620188c0a66e1d0e779083cf61960a2f3e2f": {"ta_keywords": "constrained multilevel synthesis;timing constraints based;multilevel synthesis optimization;generates combinational logic;multilevel synthesis;combination delay constrained;defined timing constraints;timing constraints;synthesis generates combinational;synthesis optimization;synthesis optimization used;generate logic;delay constrained multilevel;based combination delay;user defined timing;used generate logic;combination delay;generate logic standard;logic given technology;combinational logic;defined timing;generates combinational;delay constrained;synthesis generates;combinational logic given;constraints based combination;logic standard cell;synthesis;constrained multilevel;present synthesis generates", "pdf_keywords": ""}, "d04c91bbb043666ebd6dae51995ee5bbc4291ddf": {"ta_keywords": "spin orbit interaction;interaction spin orbit;orbit interaction spin;coupled spin orbit;orbit coupled spin;spin orbit coupled;dynamics spin orbit;enhance spin orbit;interaction dynamics spin;effect spin orbit;enhancement spin orbit;spin orbit;suppression spin orbit;coupled spin;interaction spin;dynamics spin;orbit interaction dynamics;orbit interaction;orbit interaction responsible;orbit interaction used;effect spin;enhance spin;enhancement spin;suppression spin;orbit coupled;responsible enhancement spin;spin;responsible suppression spin;orbit interaction number;study effect spin", "pdf_keywords": ""}, "b209f2acbf0fbc62a3fd19ad6c13abbd46547736": {"ta_keywords": "speaker diarization monaural;speaker diarization;new speaker diarization;slac speaker challenge;talker recordings lebedev;multi talker recordings;talker recordings;speaker challenge;speaker challenge 2020;recordings lebedev project;monaural multi talker;track slac speaker;recordings lebedev;slac speaker;multi talker;present new speaker;speaker;new speaker;diarization accuracy;recordings;evaluated diarization track;diarization track slac;diarization error rate;diarization track;levels diarization accuracy;diarization monaural multi;talker;achieves levels diarization;diarization;diarization monaural", "pdf_keywords": "speaker extraction segmentation;speaker recognition;challenge speaker recognition;neural networks speaker;speaker diarization;speaker recognition consists;speaker recognition proposed;speaker embedding extractor;based speaker extraction;speaker extraction;joint speaker recognition;speaker diarization monaural;speaker embedding extractors;propose speaker diarization;speaker recognition able;speaker verification;speaker verification sv;diarization challenge speaker;novel speaker recognition;describes speaker diarization;networks speaker embedding;evaluated speaker verification;speaker diarization integrates;diarization joint speaker;speaker diarization joint;based speaker embedding;present speaker filtering;speaker embedding;networks speaker;speaker filtering proposed"}, "9e3e6ddf958c2005f7041cc9dd5fe050a0dbd02e": {"ta_keywords": "multiscale wavelet transform;multiscale wavelet;points multiscale wavelet;wavelet transform;wavelet transform mwt;crossing points multiscale;wavelet;points multiscale;multiscale;correspondence zero crossing;transform mwt correspondence;correspondence similar signals;scales correspondence similar;scales correspondence;correspondence scales correspondence;mwt correspondence scales;correspondence scales;zero crossing points;transform mwt;similar signals method;zero crossing;visually plausible correspondence;transform;similar signals;signals method presented;correspondence zero;signals method;signals;accuracy correspondence zero;discusses correspondence zero", "pdf_keywords": ""}, "4ec1d3407a5136c525b53f703c803571200902a4": {"ta_keywords": "spin dynamics electron;spin polarized electron;spin dynamics magnetic;dynamics spin polarized;spin dynamics spin;spin dynamics;magnetic field spin;modify spin dynamics;polarized electron gas;dynamics spin;field spin dynamics;spin dynamics modify;dynamics modify spin;dynamics electron gas;control spin dynamics;spin dynamics changing;spin polarized;modify spin;control spin;field spin;effect field spin;dynamics electron;polarized electron;electron gas effect;dynamics magnetic;electron gas;spin;perpendicular magnetic;dynamics magnetic field;dynamics changing magnetic", "pdf_keywords": ""}, "f2818da69bb72526fff9d601677db38f24a62ecc": {"ta_keywords": "dialogue modeling framework;dialogue modeling;based dialogue modeling;example based dialogue;user satisfaction prediction;response prediction using;responses user feedback;predicts user satisfaction;user feedback response;response prediction;feedback response prediction;satisfaction prediction prediction;multiple response candidates;satisfaction prediction;dialogue;based dialogue;response candidates;response pairs prediction;user satisfaction select;user query response;best response multiple;user satisfaction;feedback response;select best response;query response;responses user;prediction using user;methods user satisfaction;evaluation examples prediction;response multiple", "pdf_keywords": ""}, "febb305a854d02b138250a8a19af956ffa0ada4f": {"ta_keywords": "guarantees policy gradient;convergence guarantees policy;policy gradient algorithms;policy gradient;conditions policy gradient;nash equilibrium continuous;convergence guarantees;algorithms continuous action;gradient algorithms continuous;equilibrium continuous action;multi agent settings;existence convergence guarantees;guarantees policy;space multi agent;continuous action state;multi agent;algorithms avoid nash;avoid nash equilibrium;gradient algorithms avoid;action state space;nash equilibrium;equilibrium continuous;state space multi;sufficient conditions policy;algorithms continuous;gradient algorithms;counterexamples existence convergence;agent settings;agent settings provide;continuous action", "pdf_keywords": ""}, "f21a9d70319ca99227300349d7bcab5dee5869cd": {"ta_keywords": "translation multi source;missing source translations;incomplete multilingual corpora;multi source neural;source neural machines;source translations;use incomplete multilingual;incomplete multilingual;using incomplete multilingual;multilingual corpora improved;multilingual corpora;source translations replaced;multilingual corpora use;source neural;increase translation accuracy;translation multi;source translations increase;translation accuracy;translations;neural machines nmts;multilingual;translation accuracy use;translations replaced;increase translation;translations replaced special;translations increase translation;method translation multi;neural machines;corpora improved using;missing source", "pdf_keywords": "nmt experts multilingual;incomplete multilingual corpus;machine translation nmt;incomplete multilingual corpora;translation nmt machine;nmt situations multilingual;multilingual corpora train;multilingual corpus missing;multilingual corpus;incomplete multilingual;multilingual corpora multi;multilingual corpora;multilingual corpus mixture;incomplete corpora training;use incomplete multilingual;using incomplete multilingual;multilingual corpora available;translation nmt;real incomplete multilingual;multilingual corpus compare;missing source translations;multilingual corpus uses;translation nmt mixture;neural machine translation;using multilingual corpus;talks multilingual corpus;corpus mixture nmtwe;corpora multi source;incomplete corpora;use incomplete corpora"}, "b168fc72fa39e9669567bd099bab179549a15e14": {"ta_keywords": "a2gp1 iga antibodies;peptide a2gp1 iga;iga antibodies;iga antibodies data;xmath0 peptide a2gp1;iga 439 patients;peptide a2gp1;anti 2gp1 iga;2gp1 iga a2gp1;a2gp1 iga;iga a2gp1 iga;anti xmath0 peptide;iga a2gp1;2gp1 iga;incidence anti 2gp1;antibodies;a2gp1 iga 439;xmath0 peptide;antibodies data;bearing anti xmath0;antibodies data collected;anti 2gp1;iga;patients santi bearing;a2gp1;peptide;anti xmath0;bearing bearing anti;iga 439;study incidence anti", "pdf_keywords": ""}, "de43afd166a79c24b3a7dd16c5695059d9f0aa71": {"ta_keywords": "cognitive development;cognitive learning child;early stage cognitive;cognitive development child;cognitive learning late;late stage cognitive;stage cognitive learning;study cognitive development;learning child age;learning child;cognitive learning related;cognitive learning;stage cognitive;timescale cognitive learning;concept transitivity transition;development child;transitivity transition early;cognitive;child based concept;learning related emergence;learning late;propose timescale cognitive;development child based;new concept transitivity;child age;timescale cognitive;concept transitivity;transition early;years transition early;learning late stage", "pdf_keywords": ""}, "ab94fae3d49cd7016a47020469dc257d8090f5bb": {"ta_keywords": "multi speaker separation;speaker separation using;speaker separation;challenging speech separation;speech separation;speech separation incorporate;separation using deep;independent multi speaker;speaker independent multi;speaker independent;problem speaker independent;multi speaker;separation incorporate deeper;deep clustering;separation using;deep clustering approach;using deep clustering;separation incorporate;separation;performance challenging speech;problem speaker;speaker;challenging speech;context deeper signal;incorporate enhancement layer;incorporates enhancement layer;approach problem speaker;speech;enhancement layer;using deep", "pdf_keywords": "channel speech separation;speech separation based;independent speaker separation;speech separation approach;speaker separation;speech separation;speaker separation uses;problem speech separation;separation based deep;method separation speech;separation speech monaural;multi channel speech;separation speech;network speaker based;channel speech;signal quality speech;speaker training;speaker training speech;speaker independent speaker;network speaker;speaker independent;speakers uses clustering;source separation;speaker independent multi;independent speaker;method speaker independent;problem speaker independent;deep clustering enhanced;speaker based combination;training speech recognition"}, "6c477a65f0922d405c3665e31581eaa0f269116e": {"ta_keywords": "semantics word parallelization;word parallelization;word parallelization method;parallelization distributional relational;word representation;word representations;performance word representations;objective word representation;representation use parallelization;relational distributional semantics;distributional semantics word;word representation use;distributional semantics;parallelization distributional;word representations variety;method parallelization distributional;parallelization;use parallelization;parallelization method;method parallelization;situations parallelization;use parallelization method;parallelization method used;relational distributional;semantics word;situations parallelization method;distributional relational;parallelization method build;distributional relational objective;features relational distributional", "pdf_keywords": "word embeddings tensors;knowledge base embeddings;distributional relational semantics;word embeddings;word representations trained;relational objective wordnet;word representations;large scale semantic;word representations incorporate;parsing word representations;hypothesis word representations;low dimensional semantic;wordnet;objective wordnet;interaction word embeddings;objective wordnet preliminary;knowledge base completion;tensor network nnn;relational semantics end;relational semantics;distributional objective hypernymy;wordnet preliminary results;base embeddings approach;embeddings tensors use;dimensional semantic;embeddings approach;embeddings tensors;distributional relational;wordnet preliminary;embedding model"}, "10085f7fb0871329d34529cc54df0a8f75756fce": {"ta_keywords": "automatically generate spoken;automatic transcription;framework automatic transcription;generate spoken;speaking style transformation;language national congress;automatic transcription characterization;training texts labels;generate spoken lightly;transcription characterization languages;training texts;transcription;minutes reference texts;consists speaking style;speaking style;language national;style training texts;languages language national;texts labels documents;texts labels;consists speaking;languages;languages language;meeting minutes;texts;language;transcription characterization;like meeting minutes;labels documents like;uses minutes reference", "pdf_keywords": ""}, "248824ec5d9b4ddf0c36cdc51b6b57af6e881328": {"ta_keywords": "lingual transfer language;features cross lingual;lingual transfer;cross lingual transfer;transfer language;transfer language compares;cross lingual;language compares features;lingual;language processing task;language processing;language compares;language;natural language processing;typical natural language;natural language;considers features cross;features cross;feature;features;features typical;compares features typical;features problem better;considers features;feature based;transfer;intuition experimenter model;model considers features;captures essential features;experimenter model captures", "pdf_keywords": "language trained ranking;tasklanguage ranking based;train tasklanguage ranking;predicting transfer languages;language prediction task;tasklanguage ranking;language prediction;consider language prediction;languages use ranking;rank languages based;rank languages;languages transfer;prediction task ranking;promising languages transfer;candidate transfer languages;task transfer languages;transfer languages given;way rank languages;transfer languages;transfer languages model;languages given task;best transfer languages;task language trained;language task;transfer language;transfer languages machine;optimal transfer languages;potential transfer language;transfer languages propose;languages model trained"}, "70170035ef870df1c064cc52804178a52f6a69ef": {"ta_keywords": "zeeman field xmath0;effect zeeman field;xmath2 xmath3 xmath4;effect zeeman;xmath1 xmath2 xmath3;zeeman field;xmath15 xmath16 xmath17;xmath9 xmath10 xmath11;xmath17;xmath16 xmath17 xmath18;xmath17 xmath18 xmath19;xmath16 xmath17;xmath13 xmath14 xmath15;xmath2 xmath3;xmath17 xmath18;xmath14 xmath15 xmath16;xmath1 xmath2;xmath12 xmath13 xmath14;xmath10 xmath11 xmath12;xmath3 xmath4;xmath0 xmath1 xmath2;xmath11 xmath12 xmath13;xmath7 xmath8 xmath9;xmath10 xmath11;xmath14 xmath15;xmath8 xmath9 xmath10;xmath13 xmath14;xmath3 xmath4 xmath5;xmath2;xmath4 xmath5 xmath6", "pdf_keywords": ""}, "75c4aefc55bf0b345587740cad0a4e994f29962a": {"ta_keywords": "polyphonic sound detection;sound activity detection;segments sound activity;approach polyphonic sound;sound activity;polyphonic sound;approach polyphonic;shortterm memory recurrent;recurrent neural network;hybrid approach polyphonic;sound detection;polyphonic;identify segments sound;memory recurrent neural;sound detection based;recurrent neural;sound activity conduct;long shortterm memory;segments sound;shortterm memory;memory recurrent;frame detection method;frame detection;neural network rnn;long shortterm;dcase 2016 dataset;frame frame detection;activity detection network;activity detection;bidirectional long shortterm", "pdf_keywords": ""}, "cc19de8d0782917098029ed20261cbe0b0c62bf5": {"ta_keywords": "biases peer review;quantify biases peer;biases text peer;biases peer;quantify biases text;peer reviews;truth bias;text peer reviews;peer review;peer reviews formalize;quantify biases;framework quantify biases;truth bias inferred;biases text;bias inferred;bias visibility;estimated bias;bias visibility subgroup;link estimated bias;bias inferred evaluate;biases;bias;estimated bias visibility;ground truth bias;reviews formalize identification;subgroup membership indicators;identification strategy causally;reviews formalize;causally link estimated;peer", "pdf_keywords": "biases peer review;biases text peer;bias peer review;bias text peer;biases review text;review text bias;bias review text;quantifying biases text;quantify biases text;text bias review;peer reviews subgroups;peer review text;quantify bias text;biases expressed text;bias review textwe;identify bias review;detection biases peer;bias selection reviewers;text peer review;review text statistically;biases text;text bias;text peer reviews;detects bias peer;bias review ratings;bias respect authors;quantifying biases peer;review text subgroups;authors evidence bias;summary text bias"}, "8484fdb56e4690927dc0191ede11c2d24bc5e2ef": {"ta_keywords": "generated text distribution;text generation;distribution generated text;generated human text;text generation called;ended text generation;text distribution human;generated text;human text measures;text distribution;distribution human written;human written text;text measures;text measures differences;written text;human text;text using divergence;differences generated human;generated human;difference generated human;distribution generated;compares distribution generated;generated;differences generated;measures differences generated;open ended text;human written;generation;text demonstrate measure;text", "pdf_keywords": "text generation models;models text generation;text generation measure;text generation crucial;measures text generation;text generation fundamental;text generation;generating human text;text generation provide;ended text generation;text generation consider;generated human text;modern text generation;human generated text;text generation compares;mlm generate text;machine text generated;autoregressive language models;text generated model;neural language models;generate text model;generative models text;text generation method;web text generation;neural autoregressive language;text generate;fluency text generated;neural human text;text generation demonstrate;text generated"}, "aff5d7f43823e06bb68220db41de3bc82e2f3990": {"ta_keywords": "cell networks heterogeneous;small cell networks;cell networks;static mobile users;networks heterogeneous;networks heterogeneous structure;network model static;network model;propose network model;model static mobile;static mobile;micro base stations;network structure;network various;mobile users small;network various assumptions;users small cell;network structure including;mobile users;mobile users ss;network;base stations;stations bss mobile;propose network;assumptions network structure;bss mobile users;static users;networks;assumes static users;mobile", "pdf_keywords": ""}, "45cdf5e239a1f0057c350f6654ccd348fb4e2332": {"ta_keywords": "matching uncertain preferences;probability matching uncertain;stability probability matching;matching uncertain;uncertain preferences consider;uncertain preferences;choice indifference model;uncertain preferences significant;model choice indifference;probability matching;indifference model choice;uncertainty compact indifference;lottery model stability;indifference model;indifference model joint;choice indifference;order indifference model;compact indifference model;probability model lottery;consider models uncertainty;lottery model;models uncertainty;model lottery model;matching;uncertainty;model lottery;unknown order indifference;models uncertainty compact;model stability probability;preferences consider models", "pdf_keywords": "probability stable matching;matchings stability probability;stable matching preference;optimal matching stable;matchings indifference model;exist stable matchings;stable matching uncertain;stable optimal matching;matching uncertain preferences;stable matching possible;stable matching exists;matching matching uncertainty;stable matchings models;stable matchings restricted;matching uncertainty preferences;matchings indifference;stability matchings indifference;existence stable matching;stable matching problem;matching optimal stable;matching stable determined;stable matchings;stability probability matching;particular stable matchings;stable matchings particular;matching uncertain;preferences stable matchings;stable matching;matchings particular stable;matching uncertainty"}, "4731f89169604cd0d8b5352380baa1b4728bca0b": {"ta_keywords": "machine translation mt;machine translation;translation mt systems;discriminative language models;analysis machine translation;language models significantly;discriminative language;language models;use discriminative language;effective error analysis;error analysis machine;translation mt;error analysis;based use discriminative;discriminative;use discriminative;translation;frequency based analysis;mt systems method;effective error;analysis machine;mt systems;models significantly efficient;language;models significantly;efficient frequency based;models;method effective error;frequency based;comparing standard frequency", "pdf_keywords": ""}, "601408d6617bf72894c9f41ae54cf9c17905903a": {"ta_keywords": "t2s translation investigating;string t2s translation;accuracy translation;accuracy translation english;translation investigating;methods accuracy translation;accuracy translation basic;t2s translation;tree string t2s;phrasebased systems improved;par phrasebased systems;accuracy tree string;translation basic performs;translation investigating number;affect accuracy translation;phrasebased systems;string t2s;translation basic;performs par phrasebased;tree string;par phrasebased;accuracy tree;investigate accuracy tree;japanese english pairs;translation english;translation english japanese;translation;english pairs;english japanese english;t2s", "pdf_keywords": ""}, "3dc20be709818630e2249ab28b35b0666b4b544d": {"ta_keywords": "information used paralinguistic;paralinguistic information used;paralinguistic information;paralinguistic information suitable;translation systems paralinguistic;study paralinguistic information;systems paralinguistic information;paralinguistic process;used paralinguistic process;present study paralinguistic;paralinguistic process different;study paralinguistic;paralinguistic;focus sentence information;used paralinguistic;systems paralinguistic;speech translation later;processing conventional speech;convey focus sentence;speech translation;focus sentence;sentence information;speech speech translation;speech translation systems;sentence information used;conventional speech;conventional speech speech;used conventional speech;speech speech;speech", "pdf_keywords": ""}, "ead6323f137c2f99ef0ffcfa34fa6eb1c6eca3c6": {"ta_keywords": "chime speech recognition;speech recognition challenge;speaker diarization challenge;diarization speech recognition;unsegmented speech recognition;recognition speaker diarization;6th chime speech;speech recognition speaker;recognition speaker;speech recognition;speaker diarization;diarization speech;speaker diarization speech;diarization challenge organized;diarization challenge;involves recognition speaker;chime speech;speaker diarization second;present 6th chime;problem unsegmented speech;6th chime;unsegmented speech;recognition challenge;recognition challenge seeks;aspects speaker diarization;recognition challenge covers;diarization;recognition;chime;diarization second", "pdf_keywords": "speaker diarization chime;recognition speech separation;speech separation recognition;automatic speech separation;separation speaker diarization;diarization speech recognition;recognition speaker diarization;speaker diarization distant;speech recognition everyday;speaker speech recognition;conversational speech recognition;speech recognition speaker;microphone speech recognition;speech enhancement diarization;performs speaker diarization;speech signal separation;distant microphone speech;separation recognition chime;challenge automatic speech;enhancement speech separation;speech recognition;speaker diarization;speech recognition new;synchronization speech enhancement;speaker diarization dataset;recognition speaker;enhancement speaker diarization;synchronization speech;speaker diarization using;speech separation speaker"}, "af5c4b80fbf847f69a202ba5a780a3dd18c1a027": {"ta_keywords": "language inference commonsense;inference commonsense reasoning;commonsense inference;approach commonsense inference;commonsense inference unifies;inference commonsense;natural language inference;commonsense reasoning;commonsense reasoning use;language inference;approach commonsense;novel approach commonsense;counterfactuals use;commonsense;inference unifies natural;counterfactuals;counterfactuals use filter;potential counterfactuals use;inference unifies;inference problems high;language models oversample;unifies natural language;inference;potential counterfactuals;language models;natural language;inference problems;inference problems demonstrating;resulting inference problems;analysis resulting inference", "pdf_keywords": "grounded commonsense inference;inference commonsense;commonsense reasoning dataset;inference commonsense reasoning;commonsense inference unifying;language inference commonsense;commonsense inference;commonsense inference broadens;purpose commonsense inference;approach commonsense inference;inference nli commonsense;commonsense inference called;frames commonsense inference;commonsense reasoning;commonsense reasoning present;commonsense inference ss;nli commonsense reasoning;commonsense inference multiple;benchmark purpose commonsense;situated commonsense inference;grounded commonsense;natural language inference;inference unifying natural;reasoning dataset constructed;reasoning dataset;clip human reasoning;frames commonsense;commonsense;approach commonsense;nli commonsense"}, "d15eb5744474cec2d0634651bb30000b3873a309": {"ta_keywords": "time expression normalization;normalization methods twitter;normalized temporal;expression normalization;state art normalization;rules time expression;expression normalization method;normalized temporal value;construct normalized temporal;expression normalization sequence;twitter tm benchmarks;normalization sequence operations;normalization;generating rules time;normalized;normalization methods;time expression;methods twitter tm;automatically generating rules;model time expression;normalization sequence;twitter tm;operations construct normalized;construct normalized;normalization method;rules time;methods twitter;twitter;normalization method relies;tm benchmarks", "pdf_keywords": "normalizing time expressions;annotated time expressions;time expression normalization;recognizing normalizing time;normalization time expression;time expressions automatically;normalizing time;normalized temporal;normalization rules training;automatically annotated time;structure time expressions;semantic structure time;automatically recognizing normalizing;normalization time;construct normalized temporal;time expressions natural;annotated time;expressions natural language;recognizing normalizing;generate normalization rules;expression normalization;normalization sequence operations;rules time expression;normalized temporal value;rules expression time;semantics natural language;time term automatically;automatically generate normalization;natural language resulting;expression normalization operation"}, "3b563c16e9a918631d63a20027dad735b625625a": {"ta_keywords": "toolkit text generation;text generation distribution;text generation;text generation tasks;generation distribution toolkit;set text generation;versatile toolkit text;generation tasks features;toolkit text;toolkit based modular;distribution toolkit based;versatile toolkit;generation tasks;distribution toolkit;introduce versatile toolkit;toolkit based;toolkit include modular;toolkit freely;toolkit;generation;generation distribution;text;interoperability paradigm designed;structure popular interoperability;popular interoperability paradigm;flexibility toolkit freely;features toolkit;popular interoperability;interoperability paradigm;interoperability", "pdf_keywords": "text generation toolkit;toolkit text generation;extensible text generation;text generation texar;text generation tasks;unstructured text generation;models text generation;text generation;versatile toolkit text;text generation supports;model text generation;generation toolkit texar;generation toolkit;machine translation toolkit;extensible toolkit generation;text generation use;toolkit generation;text generation alsothis;translation toolkit powerful;generation texar;toolkit text;translation toolkit;text texar general;text texar;variety text generation;learning toolkit;versatile toolkit;toolkit generation analysis;unstructured text;powerful toolkit"}, "a8372f7cb2e482a455b06c3e47f65aec5c7a924b": {"ta_keywords": "electromagnetic em pump;circuits design pump;liquid lead bismuth;lead bismuth eutectic;future design pump;lbe circulation circuits;lead bismuth eute;circulation circuits design;design pump;design pump currently;em pump critical;design pump expected;bismuth eutectic lbe;lead bismuth;achieving pump;eutectic lbe circulation;achieving pump efficiency;simulation liquid lead;circulation circuits;em pump;potential achieving pump;design electromagnetic em;design electromagnetic;bismuth eutectic;pump expected;pump expected highly;pump efficiency;pump efficiency near;bismuth eute;pump currently pursued", "pdf_keywords": ""}, "0c07cc7ba1b862556f5cfee0d5d849866d21a693": {"ta_keywords": "oblivious updates distributed;oblivious updates storage;required oblivious updates;oblivious updates;problem oblivious updates;communication required oblivious;lower bound communication;distributed storage networks;mft codes optimal;updates distributed storage;storage networks present;storage networks;updates storage networks;bound communication;storage networks derive;bound communication required;xmath0 mft codes;distributed storage;consider problem oblivious;required oblivious;problem oblivious;codes optimal;mft codes;updates distributed;mft codes present;codes present algorithm;oblivious;codes optimal terms;updates storage;codes linear maximum", "pdf_keywords": "oblivious updates storage;oblivious update protocol;oblivious updates data;nodes oblivious update;oblivious update data;oblivious update arbitrary;perform oblivious updates;performing oblivious updates;bounds download oblivious;storage communication theorem;oblivious updates;generic oblivious update;oblivious updates deriving;oblivious updates general;oblivious updates setting;nodes oblivious;lower bound communication;linear update protocols;problem oblivious updates;oblivious update;protocols lower bound;number nodes oblivious;performance generic oblivious;protocol distributed memory;bound communication;case oblivious updates;nodes distributed storage;communication necessary storage;stale message bounded;bound communication necessary"}, "9650dbe79d34498113371770dcdb48f1bd7c9711": {"ta_keywords": "visualizing research papers;visualize research topics;visualizing research;visualize research;maps visualize research;new visualizing research;journal maps visualize;paper database heatmap;term similarity creation;term extraction using;information retrieval;term extraction;research papers using;term similarity;paper database;extensible term extraction;title paper database;heatmap;methods information retrieval;maps visualize;database heatmap;journal maps;natural language processing;similarity creation;research topics field;visualizing;new visualizing;group journal maps;research papers;present new visualizing", "pdf_keywords": "visualizing documents;visualizing topic space;visualizing map clusters;visualizing large relational;method visualizing documents;visualizing data bibliography;generate maps topics;research visualization;visualizing large collection;visualization underlying topic;visualizing map;maps word content;visualizing topic;word graph based;visualizing data sets;large scale bibliographic;visualization visualizing large;graph based topics;organize collection maps;approach visualizing data;research visualization underlying;maps topics;visualizing data;maps organize;modular visualizing topic;based word graph;visualizing hierarchical organization;space research visualization;documents resulting graph;visualization visualizing"}, "889c3b4394826639d483c039467cd9a05e68e73c": {"ta_keywords": "generative model;build generative model;neural network complete;neural network approximate;generative;approximate task composition;build generative;partial musical scores;complete partial musical;composition personalized;version neural;generative model initial;composition personalized way;version neural network;partial musical;blocked version neural;use build generative;train convolutional neural;convolutional neural;train convolutional;composition;neural network;neural;musical scores;convolutional neural network;musical scores use;task composition;musical;task composition personalized;like sampling", "pdf_keywords": "chorale model trained;corpus chorales model;music neural networks;model chorales;music neural;corpus chorales;features chorale model;popular corpus chorales;structure music neural;model tasks music;chorales model;essential features chorale;rewriting melodic matching;features chorale;unconditioned music generation;melodic matching unconditioned;melodic matching;chorale model;matching unconditioned music;reproduce chorales chorales;model musical scores;music including rewriting;standard model chorales;reproduce chorales;music model;chorales model yields;chorales;chorales harmonic transitions;stochastic gradient descent;stochastic amplitude matching"}, "68af273e04906e0450a5d01d5606c8313da01453": {"ta_keywords": "stochastic subset selection;selection sensor networks;approach stochastic subset;subset selection sensor;stochastic subset;learn stochastic subset;subset selection;subset selection problem;based stochastic approximation;sensor networks;selection sensor;sensor networks presented;novel approach stochastic;based stochastic;approximation learn stochastic;stochastic approximation;stochastic approximation learn;approach stochastic;stochastic approximation underlying;stochastic;exploits stochastic approximation;learn stochastic;strategy based stochastic;selection problem proposed;activated sensors;activated sensors mean;activated sensors proposed;selection problem;sensors mean;statistics exploits stochastic", "pdf_keywords": "stochastic sensor selection;algorithms stochastic sensor;stochastic sensor;sensors uses stochastic;sensor subset selection;sensor networks;gibbs sampling stochastic;sampling stochastic approximation;sampling stochastic;sensor subset;sensor network heterogeneous;algorithms stochastic approximation;problem sensor subset;based gibbs sampling;sensor selection context;efficient algorithms stochastic;exploiting stochastic approximation;sensor selection;gibbs sampling;algorithms stochastic;sensor network;sensor distributed;exploiting stochastic;sensors prove convergence;maximization scenario sensor;sensor networks deployed;stochastic approximations;sensors provide algorithm;stochastic approximations proposed;algorithm exploiting stochastic"}, "04f8f739924a19c01d196a48783b914554ac0fe5": {"ta_keywords": "composite convex optimization;algorithms composite convex;domain newton methods;second order algorithms;convex optimization;newton methods methods;newton methods;convex optimization called;contracting domain newton;composite convex;optimization called contracting;methods methods affine;convergence method xmath0;rate convergence method;domain newton;methods affine;convergence method;convex;methods affine invariant;order algorithms composite;optimization called;order lower approximation;optimization;algorithms composite;global second order;order algorithms;lower approximation smooth;global rate convergence;rate convergence;lower approximation", "pdf_keywords": "composite convex optimization;algorithms composite convex;optimization convex functions;stochastic optimization convex;minimizing composite convex;convex optimization;stochastic convex optimization;convex optimization realized;optimization convex;domain newton methods;convex optimization called;newton methods algorithms;convex optimization consider;xmath0 general convex;composite convex function;realized stochastic hessian;stochastic convex;convex functions stochastic;stochastic hessian;stochastic optimization;contracting domain newton;second order algorithms;composite convex;empirical risk minimization;convex functions;convex functions provide;general convex functions;general convex;theoretic method estimation;method estimation domain"}, "86ae1161026f23f9df691a867fd7453cee56fd28": {"ta_keywords": "lexical change;evolution meaning lexical;lexical change derived;lexical landscape;lexical;model lexical change;meaning lexical;lexical landscape approaches;meaning lexical landscape;model lexical;phylogenies;includes model lexical;phylogenies collection examples;phylogenies collection;phylogenies collection models;studying evolution meaning;evolution meaning;collection phylogenies;derived collection phylogenies;collection phylogenies collection;meaning;evolution;studying evolution;approaches studying evolution;landscape;change;change derived;approaches;approaches illustrated examples;landscape approaches", "pdf_keywords": "semantic change phylogenies;words semantic change;change meaning phylogenies;lexicon particularly change;meaning change lexical;conceptualized evolutionary linguistics;changes linguistic systems;changes linguistic;semantic change;semantic change spoken;studying semantic change;evolutionary linguistics;language evolutionary;semantic shift phylogenetic;studying change lexicon;model changes linguistic;evolutionary linguistics way;evolution linguistic;historical linguistics different;lexical replacement phylogenetics;change lexical;evolution linguistic systems;driving evolution linguistic;language complex evolutionary;language evolutionary model;semantic change rely;historical linguistics;change phylogenies;model evolution linguistic;meaning phylogenies"}, "5b8eaaf660b9e2d6a19886991350fffa1320b372": {"ta_keywords": "learn entities relations;learn entities;entities relations sentences;approaches learn entities;entities relations;relations sentences uses;inference based training;relations sentences;classifiers ilp inference;entities;sentences uses;sentences;classifiers ilp;classifiers;uses inference based;uses inference;inference based;inference uses;classifiers second;classifiers second uses;inference uses inference;local classifiers ilp;ilp inference;relations;ibt inference uses;uses local classifiers;inference;just local classifiers;ilp inference lastly;local classifiers", "pdf_keywords": ""}, "781e0e81834119c135091c8bdfcd1966c10b09ab": {"ta_keywords": "integer compression scheme;new integer compression;integer compression;compression scheme based;compression scheme;compression using simd;decoded 32bit integer;compression using;decoded 32bit;providing stateoftheart compression;stateoftheart compression;cycles decoded 32bit;compression;cpu cycles decoded;stateoftheart compression using;data simd instructions;processors scheme;simd instructions ubiquitous;common processors scheme;modern processors;cycles decoded;common processors;modern processors double;processors scheme uses;decoded;ubiquitous modern processors;scheme based singleinstruction;processors double speed;data simd;using simd instructions", "pdf_keywords": "accelerating integer compression;fastest integer compression;speed integer compression;algorithms integer compression;integer compression based;databases integer compression;integer compression using;integer compression;integer compression intersection;based integer compression;integer compression instructions;new integer compression;parallelized compression instruction;integer compression schemes;integer compression scheme;integer compression algorithm;method integer compression;efficient data compression;integer compression single;parallelized compression;parallelized compression technique;demonstration parallelized compression;algorithm index compression;integer encodings algorithmic;uses compression intersection;compression technique computing;integer compression key;deltas integer compression;compression intersection commodity;compression based bit"}, "65f632cbac465633a13b1e3f8c8c410c2f3aec3d": {"ta_keywords": "game theoretic reinforcement;theoretic reinforcement learning;theoretic reinforcement;novel game theoretic;reinforcement learning algorithm;game theoretic;reinforcement learning;novel game;reinforcement;propose novel game;learning algorithm;game;learning;algorithm;theoretic;novel;propose novel;propose", "pdf_keywords": ""}, "76862a851bd2c17dcf6bfc2cecbf4af186730123": {"ta_keywords": "segmenting nontext;segmenting nontext objects;method segmenting nontext;segmenting halftone images;grayscale document image;segmenting;grayscale document;directly grayscale document;solution aimed segmenting;unconventional method segmenting;segmenting halftone;document image;halftone images tables;aimed segmenting halftone;images tables graphs;aimed segmenting;nontext objects;halftone images;document image making;nontext objects directly;objects directly grayscale;images tables;method segmenting;nontext;directly grayscale;grayscale;document;images;tables graphs;tables graphs proposed", "pdf_keywords": ""}, "1a20d6c6891f3a0462515ff9560bc37e66eb422a": {"ta_keywords": "spectral density proton;density proton calculated;density proton presence;density proton;calculation spectral density;spectral density;proton calculated;proton presence strong;proton calculated using;proton presence;limit spectral density;calculation spectral;schrdinger equation continuum;approach calculation spectral;strong magnetic field;magnetic field approach;proton;continuum limit spectral;schrdinger equation;presence strong magnetic;approximate solution schrdinger;mean field approximation;field approximation;magnetic field;strong magnetic;solution schrdinger equation;limit spectral;spectral;exact solution schrdinger;density", "pdf_keywords": ""}, "68258e0541132027ef86f872b92406de1c6edab3": {"ta_keywords": "generator gaussian noise;symmetric generator gaussian;circularly symmetric generator;gaussian noise stability;noise stability simple;stability generator;generator gaussian;generator provided noise;noise stability;preserve stability generator;stable circularly symmetric;symmetric generator;stability generator pacs;stabilize generator;gaussian noise;gaussian noise used;stabilize generator provided;noise used stabilize;simple stable circularly;stability simple stable;used stabilize generator;stable circularly;large gaussian noise;effect gaussian noise;stability simple;noise sufficiently;provided noise sufficiently;simple stable;generator;stability", "pdf_keywords": ""}, "b1d309073623d46548e55269fb73485a3b7f11a8": {"ta_keywords": "embryology pretrained language;pretrained language model;pretrained language;pretraining linguistic;pretraining linguistic knowledge;language model learns;speeds pretraining linguistic;embryology pretrained;totipotent language model;refer embryology pretrained;results embryology pretrained;speech different learning;learning speeds pretraining;language model;model learns;totipotent language;learns;language model results;pretraining;pretrained;linguistic knowledge;improve pretraining;language model refer;developmental;model learns reconstruct;model refer embryology;generally improve pretraining;linguistic knowledge world;learns reconstruct predict;learns reconstruct", "pdf_keywords": "pretrained language model;language model pretrained;model pretrained language;predicting reconstructing tokens;discovery pretrained language;predict accuracy tokens;tokens accuracy pretraining;optimally train linguistic;train linguistic tasks;language model bert;learn reconstruct linguistic;predict semantic syntactic;train linguistic;predict semantic;learns predict;pretrained language;linguistic identities train;tasks token reconstruction;learns predict accuracy;able predict semantic;reconstruct linguistic;reconstructing tokens;token nodes learning;makes pretrained language;bert present pretraining;token reconstruction;model learns predict;reconstruct input tokens;reconstructing tokens different;language model lps"}, "0110abf15bf0ee1bdf28061ad05f85b1c9f6e1c3": {"ta_keywords": "structured information sources;based similarity documents;query based similarity;similarity documents;similarity documents proposed;similarity measures text;web based databases;integration structured information;information sources method;structured information;similarity measures;studied similarity measures;databases present information;information sources;based similarity;based databases;based databases present;use studied similarity;studied similarity;similarity;databases;query based;databases present;sources method based;observation web based;based observation web;text performs query;integration structured;web based;measures text", "pdf_keywords": ""}, "a3da7028a1b721e392c421c2f15096abb1a71afb": {"ta_keywords": "plaque vulnerability lipid;atherosclerotic plaque vulnerability;hba1c plaque vulnerability;analysis atherosclerotic plaque;visit variability lipid;lipid variability;a1c hba1c plaque;lipid variability remains;variability lipid profiles;atherosclerotic plaque;association glucose lipid;vulnerability lipid variability;hba1c plaque;plaque vulnerability;variability lipid;lipid profiles patients;lipid profiles;elevated glycated hemoglobin;plaque vulnerability visit;glycated hemoglobin a1c;glucose lipid;lipid metabolisms influence;regression analysis atherosclerotic;influence elevated glycated;elevated glycated hb;glucose lipid metabolisms;association glucose;glycated hemoglobin;analysis atherosclerotic;metabolisms influence elevated", "pdf_keywords": ""}, "3ed07f6643856b9ac4687b3bc667767f3ab4b563": {"ta_keywords": "controlling voice quality;voice quality control;voice quality expression;parameters voice quality;voice quality using;voice quality;appropriate voice quality;control parameters voice;selecting appropriate voice;controlling voice;intuitively controlling voice;parameters voice;multiple regression mixture;independency acoustic features;regression mixture model;regression mixture;quality using multiple;quality control improved;voice;scores independency acoustic;quality control;acoustic features corresponding;acoustic features;quality expression words;quality expression;mixture model choice;mixture model;appropriate voice;independency acoustic;quality using", "pdf_keywords": ""}, "ecde7c041e9ac48bccef7a8d078a3f80239b0479": {"ta_keywords": "object detection video;improving object detection;detection video;detection video captures;video captures temporal;video dataset;captures temporal context;youtube video dataset;object detection;video captures;domain adapted convolutional;labeled frames;labeled frames subsequently;subset labeled frames;captures temporal;convolutional neural;adapted convolutional;frames;adapted convolutional neural;baselines youtube video;frames subsequently;video;youtube video;convolutional;improving object;baseline baselines youtube;temporal context;convolutional neural network;detection;network trained", "pdf_keywords": "object detection video;predicting object detection;classification prediction video;improving object detection;detection video;detection video sequences;prediction video data;predict object classification;prediction video;detection video approach;object classification;object classification prediction;object detection;problem object detection;capable predicting object;model object classification;refining object detection;recognition real time;predicting object;video sequences discovery;video sequences;problem object classification;object classification localization;video data model;trained target frame;classifying classifying objects;approach predicting object;classifying objects;prediction objectness;networks recurrent"}, "085072963b33367b842369b9ce81394d32ac8843": {"ta_keywords": "separating speech noisy;channel speech separation;speech separation;perform separation noisy;speech separation able;separating speech;separation noisy environment;separation noisy;strategy separating speech;speech noisy environment;single channel speech;speech noisy;perform separation;achieve separation error;achieve separation;separation able achieve;channel speech;separation error;separation;noisy environment using;separating;used perform separation;separation complex environment;perform separation complex;separation able;able achieve separation;separation error xmath0;separation complex;noisy environment;training strategy separating", "pdf_keywords": "separation noisy speech;speech separation improved;separating speech noisy;channel speech separation;speech separation modeling;effective separating speech;deep learning separation;speech separation;separating speech;end speech separation;separation pure noise;separation noise effective;separating noisy signal;based separation noisy;learning separation;noisy speech network;source separation noise;separation noise;separation noisy;learning separation models;separation noisy mixtures;separating noisy;noisy mixtures speech;real speech noise;domain separation noisy;noisy speech signal;mixture real speech;speech signal synthetic;background noise conversational;speech network"}, "76fe5f80dd25078eefa522e59a7763bc5d5da826": {"ta_keywords": "gaussian field xmath0;component gaussian field;known gaussian field;field xmath2 particles;field xmath1 particles;gaussian field;gaussian field field;xmath1 particles model;xmath2 particles model;model component gaussian;component gaussian;field xmath0 component;xmath1 particles;generalization known gaussian;xmath2 particles;particles model generalization;known gaussian;field xmath2;particles model;field field xmath2;gaussian;xmath0 component model;field xmath1;field field xmath1;field xmath0;xmath0 component;component model generalization;particles;xmath2;simple model component", "pdf_keywords": ""}, "9165d5e99b2106825dd00b9f5daf60e454434399": {"ta_keywords": "interpretation systems translation;obtain translation data;translation data translation;translation data;simultaneous interpretation systems;interpretation systems simultaneous;systems translation data;translation data large;interpretation obtain translation;data translation;data translation data;systems translation;translation data use;interpretation systems present;obtain translation;interpretation systems;use simultaneous interpretation;interpretation systems compare;systems simultaneous interpretation;description collection simultaneous;simultaneous interpretation obtain;simultaneous interpretation;collection simultaneous interpretation;translation;performance simultaneous interpretation;languages compare performance;different languages;different languages compare;languages compare;number different languages", "pdf_keywords": ""}, "23d299b35366c18e397faeb2c8687c20f8e17688": {"ta_keywords": "detection deception attack;attack deep neural;deception attack deep;attack detection;detection deception;deception attack;attack detection algorithm;classification autonomous cyber;competing attack detection;neural network dnn;facilitates detection deception;attack deep;network dnn based;detection scheme outperforms;network dnn;based detection numerical;dnn based;dnn based image;detection scheme;detection numerical;neural network;deep neural network;proposed detection scheme;based detection;deep neural;detection algorithm achieving;perturbation based detection;detection algorithm;facilitates detection;component analysis pn", "pdf_keywords": ""}, "72302d8c5cdcf59b6df96290ffc874d3613fe6b1": {"ta_keywords": "cascade particles initially;particle cascade model;particles undergoing cascade;undergoing cascade particles;particle cascade;cascades particles based;cascade cascades particles;cascade particles;single particle cascade;cascades particles;onset cascade cascades;onset cascade method;unstable resulting cascades;onset cascade;cascade method;cascade model;cascade cascades;cascade method applied;resulting cascades used;needed onset cascade;resulting cascades;cascades used estimate;cascade model model;particles initially unstable;undergoing cascade;cascades used;particles initially;cascade;cascades;particles undergoing", "pdf_keywords": ""}, "12e9d005c77f76e344361f79c4b008034ae547eb": {"ta_keywords": "embeddings textual sequences;dimensional embeddings textual;embeddings textual;textual sequences;textual sequences approach;low dimensional embeddings;character gram count;embeddings;dimensional character gram;gram count vector;dimensional embeddings;textual;learning low dimensional;character gram;neural networks similarity;low dimensional character;gram count;similarity tasks;networks similarity tasks;convolutional neural networks;dimensional character;use low dimensional;low dimensional;similarity;gram;dimensional low dimensional;learning low;architectures based convolutional;networks similarity;low dimensional low", "pdf_keywords": "predicting sentence similarity;similarity text trained;predict sentence similaritywe;similarity sentence similarity;text word similarity;sentence similarity based;word similarity language;charagram embeddings textual;embeddings textual sequences;sentence similarity;word representations;similarity text similarity;similarity word similarity;word similarity text;sentence similarity text;text linguistic similarity;word similarity graph;embeddings textual;similarity text word;text similarity;word representations proposed;linguistic similarity feature;charagram embeddings outperform;word level similarity;sentence similaritywe;classification word similarity;word similarity;similarity language;language word similarity;predicting semantics sentences"}, "f053137323a88eb932d590bcdfc959ee805e2520": {"ta_keywords": "dependency parsing stack;dependency parsing;parsing stack;parsing stack point;architecture dependency parsing;treebanks;evaluation 29 treebanks;parsing;dependency tree;29 treebanks;sentence builds dependency;dependency tree root;builds dependency tree;encodes sentence builds;child stack;stack point networks;dependency;sentence builds;reads encodes sentence;child stack step;selects child stack;stack step evaluation;novel architecture dependency;tree;encodes sentence;stack step;stack;tree root;architecture dependency;networks stack", "pdf_keywords": "dependency parsing stack;dependency parsing;dependency parsing simple;stack pointer networks;parsing stack;sentence parsing implemented;pointer networks;architecture dependency parsing;sentences encoder based;based stackptr parsers;novel encoder parsing;pointer networks ii;attention networks;attention networks iii;joint parsing syntactic;simple parsing model;encoder parsing;parsing systems;stackptr parsers;sentences stack channel;parsing stack pointer;joint parsing;stack channel sentences;sentences encoder;graph based parsing;sentence parsing;parsing syntactic;simple parser based;syntactic semantic dependencies;encoder parsing important"}, "a4b1afd75bd2da0b21df58cd4ae1649fefabd8dd": {"ta_keywords": "utility learning multiplayer;learning multiplayer;learning multiplayer settings;agents utility maximizers;game theoretic framework;game theoretic;mobile fitness game;model agents utility;play correlated equilibrium;method simulated game;fitness game;utility learning;multiplayer settings model;simulated game;equilibrium strategy;utility maximizers;agents utility;correlated equilibrium strategy;simulated game chicken;multiplayer;game players balance;fitness game players;play correlated;game chicken;multiplayer settings;present game theoretic;equilibrium strategy test;individual agent given;privacy;agent given play", "pdf_keywords": ""}, "cbf9a2560eac548e7b3d5eb7074c40b7bb861909": {"ta_keywords": "speaker diarization conditioned;speaker diarization eend;end speaker diarization;model speaker diarization;speaker diarization;speaker diarization outperform;diarization conditioned speech;effectively model speaker;end speaker;conditioned speech activity;model speaker;speech activity overlap;end end speaker;speech activity;diarization conditioned;diarization eend use;conditioned speech;speaker;conditional learning;diarization eend;conditional learning method;diarization outperform conventional;diarization outperform;designed model speaker;novel conditional learning;diarization;overlap detection;learning method end;multitask eend;multitask eend designed", "pdf_keywords": "subtask speaker diarization;subtasks speaker diarization;detection subtasks speaker;subtask speaker activity;speaker diarization based;speaker diarization probabilistic;neural speaker diarization;speaker diarization conditioned;speaker diarization proposed;speaker diarization;speaker diarization method;performance speaker diarization;multivariate speaker diarization;end speaker diarization;independent speaker diarization;speaker recognition;speaker diarization eend;method speaker diarization;subtask speaker;concept subtask speaker;speaker activity;demonstrated speaker diarizationwe;subtasks speaker;speaker recognition proposed;novel subtask speaker;formulate speaker diarization;speaker activity distribution;speaker diarization formulate;speaker diarization self;speaker diarization demonstrate"}, "e9dfccd86b6116f7601d44590985de2df434a094": {"ta_keywords": "student explanations tutoring;adaptive help students;explanations tutoring;explanations tutoring results;tutoring;student responses feedback;student explanations;tutoring results;quality student explanations;contribute student learning;help students;student responses;student learning teaching;help students solve;student learning results;accuracy student responses;students solve problems;hints quality student;student learning;students solve;learning teaching;factors contribute student;learning teaching study;teaching study;successful learning teaching;tutoring results key;contribute student;teaching study investigates;correlated student learning;students", "pdf_keywords": ""}, "c933fed82e7b5cbf7230f0f970b69590b40f86a1": {"ta_keywords": "decentralized stochastic optimization;decentralized stochastic;framework decentralized stochastic;stochastic averaging;stochastic averaging local;stochastic optimization sdas;including stochastic averaging;stochastic optimization;optimization sdas unifies;pairwise gossip updates;averaging local updates;updates pairwise gossip;approaches including stochastic;optimization sdas;convergence rates smooth;gossip updates derive;stochastic;distributed data iid;including stochastic;pairwise gossip;universal convergence rates;distributed data;heterogeneous non distributed;distributed;gossip updates;averaging local;non distributed data;convergence rates;decentralized;framework decentralized", "pdf_keywords": "decentralizing stochastic gradient;decentralized stochastic gradient;distributed stochastic optimization;decentralized stochastic optimization;stochastic decentralized optimization;stochastically distributed optimization;stochasticity distributed optimization;randomized gossip algorithm;distributed stochastic;decentralizing stochastic;theory decentralized stochastic;decentralized gossip averaging;distributed stochastic subgradient;decentralized stochastic;known distributed stochastic;optimized stochastically distributed;high decentralized stochastic;problem decentralizing stochastic;consider decentralized stochastic;decentralized stochastic differential;random gossip algorithm;generalized decentralized gossip;pairwise randomized gossip;stochastic decentralized;distributed optimization;gossip algorithm;consider distributed stochastic;randomized gossip;noisy networks convergence;study decentralized stochastic"}, "91d98b0a175237b48122e7560010e87a968fb6e0": {"ta_keywords": "separation recognition speech;robust speech recognition;neural networks separation;noise robust speech;robust speech;recognition speech challenging;speech recognition;speech challenging environments;recognition speech;networks separation recognition;speech recognition compare;separation recognition;nonnegative matrix factorization;deep computational architectures;deep neural networks;enhancement noise robust;noise robust;deep computational;deep neural;use deep neural;performance deep computational;performance deep;enhancement noise;networks separation;speech challenging;matrix factorization;neural networks;level enhancement noise;nonnegative matrix;neural networks signal", "pdf_keywords": ""}, "cf8f2ca0c2d618104bc8724a6effc509088f16c4": {"ta_keywords": "machine learning neverending;learning neverending ending;ending language learner;learning neverending;ending learner;neverending ending language;learner nell neural;language learner nell;properties ending learner;language learner;neural model machine;ending language;learner;neverending ending;learner nell;machine learning achieves;machine learning;model machine learning;learning achieves desired;neural model;nell neural;nell neural model;neural;paradigm machine learning;learning;learning achieves;new paradigm machine;neverending;language;paradigm machine", "pdf_keywords": ""}, "cc7858e74a79edceb5a42c30fc5c2dc5117f365b": {"ta_keywords": "generative adversarial tree;generative tree search;adversarial tree search;generative tree;learned model planning;generative adversarial;adversarial tree;propose generative adversarial;follows generative adversarial;generative networks d_;normd generative networks;generative networks;generative;learned model;algorithm implements generative;implements generative tree;algorithm learned model;algorithm learned;propose generative;g_ algorithm learned;normd generative;gats follows generative;implements generative;follows generative;learn model;tree search gats;adversarial;d_ normd generative;tree search g_;model planning", "pdf_keywords": "gats reinforcement learning;gaussian matrix gan;deep reinforcement learning;reinforcement learning general;gdm learning;matrix gan gaussian;exploration deep;gdm learning unstructured;generative model trained;gan gaussian;reinforcement learning rl;reinforcement learning large;idea gdm learning;gdm trained unstructured;neural network gdm;training generative;advances deep reinforcement;gdm architecture learning;generalized neural network;deep reinforcement;learning reinforcement learning;generalized neural;gdm trained;learning general;generative models rl;exploration deep quantum;approach training generative;gan gaussian facto;linear reinforcement learning;gan model quickly"}, "82cb0c428f5edb1db6e733dc4b1b20023a2ce15f": {"ta_keywords": "voting rules existing;voting systems;voting systems using;voting rules;efficiency voting rules;million voting systems;voting rules continuous;compare efficiency voting;election data testing;efficiency voting;netflix prize dataset;evaluate million voting;classes voting rules;data netflix prize;election data;prize dataset;prize dataset main;voting;rules continuous preference;classes voting;generate election data;main classes voting;million voting;netflix prize;preference preferential preference;preferential preference;preference preferential;restriction statistical models;generate election;preference compare", "pdf_keywords": ""}, "f0bbc7b84c166e2258b6ba4f9d9835ecac04e842": {"ta_keywords": "spontaneous speech recognition;clustering inference speech;speech recognition framework;speech recognition;speech recognition problem;oscillators discrete model;variance clustering;coupled acoustic oscillators;nonlinear oscillators discrete;oscillator discrete model;inference speech recognition;acoustic oscillators model;spontaneous speech;variance clustering inference;discrete model coupled;coupled coupled acoustic;problem spontaneous speech;coupled acoustic;oscillators discrete;nonlinear oscillator discrete;clustering;coupled nonlinear oscillators;framework variance clustering;model coupled nonlinear;probabilistic framework;acoustic oscillators;probabilistic;oscillator discrete;oscillators model constructed;coupled nonlinear oscillator", "pdf_keywords": ""}, "19b6537012412bee0a36e3e271f84b95868fe859": {"ta_keywords": "arguments ad hominem;define semantics arguments;semantics arguments;arguments ad;ad hominem ibm;semantics arguments paper;explainable neural networks;using explainable neural;ad hominem;arguments based;arguments used define;explainable neural;explainable neural network;arguments;arguments used;arguments based argument;ad hominem triggers;compares arguments ad;experiments explainable neural;typology arguments based;argument validate hypotheses;provides typology arguments;semantics;argument argument;based argument argument;triggers ad hominem;arguments paper;typology arguments;based argument;neural networks provide", "pdf_keywords": "argumentation use neural;argumentation present neural;argumentation demonstrate neural;novel argumentation strategies;computational linguistics argumentative;predict fallacies argumentation;argumentation strategies identify;novel argumentation;argumentation strategies;arguments discourse predict;ad hominem arguments;argumentation online;argumentation online platform;describing argumentative;typology argumentations support;noisy argumentation;methodology describing argumentative;argumentation examine;linguistics argumentative;investigate argumentative dynamics;argumentation use model;argumentation online forum;argumentative dynamics;linguistics argumentative arguments;noisy argumentation present;argumentation simple manner;argumentations support;argumentations;argumentative arguments literature;typology argumentations"}, "36a5e0e0a8ce67e4cd9077d86e3b4d50fdcff15f": {"ta_keywords": "efficientcatalysts energy splitting;energy splitting ofcatalysts;energy splitting acatalyst;splitting ofcatalysts;new energy efficientcatalysts;energy efficientcatalysts energy;energy efficientcatalysts;efficientcatalysts energy;splitting ofcatalysts key;splitting acatalyst;efficientcatalysts;ofcatalysts;splitting acatalyst key;ofcatalysts key factor;ofcatalysts key;energy splitting;design new energy;acatalyst key factor;acatalyst;acatalyst key;new energy;splitting;energy;factor design new;factor design;key factor design;key factor;design new;factor;key", "pdf_keywords": ""}, "d3e13d2514edaf74b863bfbe45a739c32a7689e1": {"ta_keywords": "neural code generation;code examples neural;predicting code natural;code natural language;predicting code;code generation tasks;code generation;examples neural code;code generation model;method predicting code;neural code;existing code examples;code examples;performance code generation;reference existing code;code natural;code;existing code;subtree retrieval;subtree retrieval allows;based subtree retrieval;examples neural;natural language;improves performance code;generation tasks xmath0;generation tasks;subtree;natural language approach;approach based subtree;based subtree", "pdf_keywords": "generating neural syntactic;syntactic code generating;neural code generation;neural syntactic code;code examples neural;code natural language;neural machine translation;code generation tasks;examples neural code;generate code natural;language using tree;neural syntactic;syntactic code;code generating;code generation;code generation partial;generating neural;gram action subtree;neural model generate;neural code;code syntactically;code generation model;generate code;method generating neural;language generate;syntactically correct generated;convolution neural code;generating correct code;automatic generation trees;partial automatic generation"}, "ba3322280992d0425bc9e2b4c59de24857e5f4e7": {"ta_keywords": "performative risk minimization;risk minimization performative;minimization performative risk;risk minimization;risk minimization provide;repeated risk minimization;risk minimization introduce;minimization performative;process specified classifier;trajectories performative risk;assigning weights;minimization;minimization provide;classifier;minimization introduce;minimization introduce notion;minimization provide sufficient;assigning weights data;specified classifier;convergence repeated risk;process considering perturbed;performative risk;performative alignment;perturbed trajectories performative;performative alignment provides;specified classifier characterize;classifier characterize;weights data generated;weights data;problem assigning weights", "pdf_keywords": "performative risk minimizer;performative risk minimizers;strategic classification task;minimizers performative risk;minimizer risk minimization;driven classifier;risk minimizers class;risk minimizer;risk minimization;driven classifier consider;risk minimizers;performative risk convex;risk minimization powerful;strategic classification;data driven classifier;gradient performative risk;risk minimization parametrically;classification;repeated risk minimizer;classification task;repeated risk minimization;repeated risk minimizers;classifier;minimizer risk;risk minimizers demonstrate;decision dependent distributions;risk minimizers general;point strategic classification;behavior risk minimizers;risk minimizer theoremwe"}, "3c6670ecdfccd4633755c4b19d774453bfb77de3": {"ta_keywords": "matching organs;matching organs given;problem matching organs;different types fairness;organs given donors;types fairness consider;types fairness;fairness consider achieve;consider problem matching;matching;fairness;patient donors;organs given;given donors;fairness consider;donors patient donors;given donors patient;patient donors patient;donors patient;donors patient identify;problem matching;donors;organs;patient identify;patient;patient identify number;consider problem;given;problem;identify number different", "pdf_keywords": ""}, "d79b613a67cf79740e1c08037f7d054585a12284": {"ta_keywords": "speech translation e2e;end speech translation;conformer encoder architecture;enhance encoder performance;speech translation;parallelizing encoder incremental;encoder performance experimental;encoder performance;conformer encoder;parallelizing encoder;encoder incremental generation;novel conformer encoder;encoder;encoder architecture;encoder incremental;enables parallelizing encoder;translation e2e;translation e2e st;methods enhance encoder;efficient non autoregressive;autoregressive models propose;autoregressive models;autoregressive models adopt;end end speech;non autoregressive models;encoder architecture called;autoregressive end end;generation non autoregressive;end speech;enhance encoder", "pdf_keywords": "nar decoder auxiliary;masked language model;predicting translation long;connectionist temporal classification;nar decoder;nar decoder referred;accurately predicting translation;predicting translation;speech translation task;generated nar decoder;orthros nar decoder;temporal classification ctc;decoder auxiliary;speech translation e2e;translation prediction;conditional masked language;encoder auxiliary;translation audiobooks encoder;speech translation audiobooks;end speech translation;auxiliary shallow decoder;text translation prediction;language model;models rnms;encoder decoder;prediction translation;autoregressive nar models;improved encoder architecture;improved encoder;shallow decoder"}, "fd9e38e240b4372c49b9205d6f909d070ff3804c": {"ta_keywords": "classification unstructured text;text classification unstructured;unstructured text based;text classification;classification unstructured;unstructured text;improving text classification;information retrieval;developed information retrieval;relational database extension;relational database;database extension developed;information retrieval community;database;database extension;version relational database;classification;database extension parallel;text based;retrieval community;parallel version relational;unstructured;retrieval;retrieval community method;version relational;text based use;method improving text;improving text;relational;extension developed information", "pdf_keywords": ""}, "cd96cae0f8eabc7bb327c6f30151741bfdd62ee0": {"ta_keywords": "radiatively unstable decay;radiatively unstable modes;radiatively unstable;radiatively unstable mode;dimensional radiatively unstable;decay mode xmath0;unstable mode xmath1;generation radiatively unstable;pandemic dynamics;unstable decay mode;global pandemic dynamics;pandemic dynamics focusing;unstable decay;xmath0 briefly review;mode xmath0 briefly;mode xmath1;xmath0 briefly;mode xmath0;mode xmath1 conclude;xmath1;unstable modes;decay mode;status global pandemic;global pandemic;xmath0;radiatively;xmath1 conclude;decay;new generation radiatively;unstable mode", "pdf_keywords": ""}, "dec6bb3c7bb671c86296a2a089e0e38aa3f69279": {"ta_keywords": "autoregressive machine translation;models knowledge distillation;knowledge distillation performance;knowledge distillation;knowledge distillation reduce;machine translation;translation quality based;effect knowledge distillation;translation quality;machine translation theart;translation theart models;best translation quality;distillation reduce complexity;distillation performance non;model complexity distilled;distillation performance;complexity distilled data;theart models knowledge;models knowledge;non autoregressive machine;performance non autoregressive;autoregressive machine;distillation;provides best translation;distilled data;distilled data provides;complexity distilled;theart model complexity;translation theart;distillation reduce", "pdf_keywords": ""}, "6bfeb25ea4bb41ab0840bb1be09f9b2de7eea8e4": {"ta_keywords": "gpcmv coupled receptor;signaling viral;cellular signaling viral;gpcmv protein guinea;pig virus gpcmv;signaling viral growth;guinea pig virus;gpcmv protein coupled;virus gpcmv protein;gpcr coupled receptor;coupled receptor gpcr;protein coupled receptor;pathogenesis gpcmv protein;receptor gpcr coupled;virus gpcmv coupled;gpcmv protein;pig virus;receptor gpcr related;receptor gpcr;coupled receptor;pathogenesis gpcmv;virus gpcmv;growth pathogenesis gpcmv;viral growth pathogenesis;receptor;viral growth;gpcr related protein;gpcmv coupled;cellular signaling type;protein cellular signaling", "pdf_keywords": ""}, "609010cb866a19dd996281d00818c3fc7363ec94": {"ta_keywords": "unsupervised cross lingual;entity ner recognition;lingual np labeled;cross lingual neural;named entity ner;lingual neural based;cross lingual;labeled named entity;cross lingual np;task cross lingual;entity ner;lingual neural;lingual;lingual np;ner recognition model;ner recognition;word level adversarial;named entity;labeled named;languages;sharing feature augmentation;different languages;experiments different languages;entity;languages demonstrate effectiveness;unsupervised cross;feature augmentation;model achieves word;different languages demonstrate;ner", "pdf_keywords": "model unsupervised crosslingual;entity recognition ner;unsupervised cross lingual;model crosslingual nernst;unsupervised crosslingual;cross lingual model;lingual named entity;unsupervised crosslingual nerns;cross lingual annotated;ner knowledge language;model crosslingual;crosslingual nernst based;entity capture linguistic;named entity recognition;language acquisition sla;new model crosslingual;lingual model;crosslingual nerns knowledge;crosslingual nernst;entity recognition;word embeddings language;second language task;transfer ner knowledge;cross lingual neural;lingual model uses;target language augmented;lingual annotated;use cross lingual;lingual neural neruclid;lingual annotated source"}, "a1c4ce9de92338646c6ee93c7c2e5ee366784b1a": {"ta_keywords": "performance semantic parsing;parsing meaning representations;semantic parsing datasets;existing semantic parsing;semantic parsing meaning;semantic parsing;semantic parsing approaches;meaning representations datasets;parsing meaning;benchmark meaning representations;reveal semantic parsing;different meaning representations;parsing datasets completing;parsing approaches exhibit;parsing approaches;parsing datasets;parsing;existing semantic;performance semantic;integrating existing semantic;evaluation performance semantic;semantic;meaning representations;benchmark reveal semantic;meaning representations experimental;generate different meaning;representations datasets comprehensive;representations datasets;meaning representations integrating;completing missing logical", "pdf_keywords": ""}, "facefd2fc4b718c6a0d8096b4eb02866028a04c2": {"ta_keywords": "open retrieval conversation;retrieval conversation;retrieval conversation questions;supervision open retrieval;matching answer span;open retrieval;answer span passage;learning weak supervision;answer span;conversation questions approach;retrieval;answer span answer;span answer passage;conversation questions;answer passage;xcite nhqc;weak supervision open;nhqc xcite nhqc;nhqc xcite;nhqc xcite method;questions approach;xcite nhqc xcite;conversation;answer passage apply;dataset nhqc xcite;span answer;questions approach based;weak supervision;questions;dataset nhqc", "pdf_keywords": "open retrieval conversations;open retrieval conversation;retrieval question answering;retrieval conversations;retrieval conversation;retrieval questions conversational;supervision weak answers;retrieval conversation datasets;supervised answer pair;question answering;questions open retrieval;question encoder learned;open retrieval;weak supervision learns;open retrieval questions;retrieval retrieves evidence;span answers conversations;answer use learned;question answering method;solving open retrieval;approach open retrieval;retrieval conversations assume;learned weak supervision;open retrieval question;supervised answer;answer retrieved passage;retriever supervised learning;learned questions;learning model conversations;answers conversations freeform"}, "75d33c125eba966b50d4dccd359a2f6aa4e0e2e7": {"ta_keywords": "policy risk functionals;estimating policy risk;pardo risk functionals;risk functionals;risk functionals including;risk functionals subsumes;features pardo risk;policy risk;pardo risk;framework estimating policy;estimating policy;doubly robust estimators;robust estimators;conditional value risk;robust estimators generate;risk cvar variance;risk;framework importance sampling;value risk cvar;importance sampling doubly;risk cvar;estimates collection policy;value risk;plugin estimates;collection policy risk;importance sampling;distorted risks instantiate;variance distorted risks;generate plugin estimates;estimators generate plugin", "pdf_keywords": "estimates reward distribution;estimate reward distribution;reward distribution estimator;bandits empirical;empirical estimate reward;armed bandits empirical;bandits empirical data;empirical estimates reward;valid randomizing bandits;contextual bandits;randomizing bandits;randomizing bandits method;contextual bandits subsumes;bandit problem exploits;estimator condition reward;estimator condition rewardtheorem;context contextual bandits;bandits method;estimating parameters reward;estimate reward;estimating risk functionals;bandit problem;reward distribution bias;distribution arm bandit;based assumption bandits;estimates reward;estimation risk functionals;assumption bandits;variance condition reward;bandits random random"}, "cb0de2de79533d4faada3d745f43702eb89d1a60": {"ta_keywords": "reusable documentation templates;reusable documentation;develop reusable documentation;documentation templates;documentation templates following;documentation;templates;templates following;classes datasets;develop reusable;templates following classes;following classes datasets;datasets;reusable;classes;aim develop reusable;case studies efforts;present case studies;case studies;following classes;studies efforts;studies;studies efforts aim;efforts;efforts aim develop;present case;develop;efforts aim;present;case", "pdf_keywords": "machine learning templates;documentation datasets models;documentation datasets;detailed documentation datasets;documentation data;datasets models nlp;documentation purpose describing;fields proposed documentation;models nlp templates;descriptions datasets;proposed documentation;descriptions datasets models;nlp templates used;nlp templates;machine learning practitioners;documentation purpose;documentation;documentation present;proposed documentation schemata;detailed descriptions datasets;creation documentation;uses datasets models;standard documentation;documentation practices;creation documentation cases;machine learning related;documentation templates;machine learning research;field natural language;standard documentation practices"}, "13b6c8cce3b4557ad7a3188f2d54636e755e8145": {"ta_keywords": "multichannel source separation;unfolding multichannel mixture;unfolding multichannel;multichannel mixture model;source separation based;obtained unfolding multichannel;multichannel mixture;multichannel audio;domain multichannel audio;source separation;multichannel source;separation based generative;multichannel audio resulting;audio resulting network;process multichannel source;architecture multichannel source;process multichannel;architecture multichannel;multichannel;frequency domain multichannel;domain multichannel;network obtained unfolding;separation based;network architecture multichannel;able process multichannel;separation;mixture model;deep computational network;audio;audio resulting", "pdf_keywords": ""}, "77c63e8f102465e3fc4a46e0b07c32fa8d2f8a54": {"ta_keywords": "grammar induction algorithms;unlabeled grammar induction;unsupervised parsers gnus;accuracy unsupervised parsers;accuracy unlabeled grammar;grammar induction;unsupervised parsers;parsers gnus;unlabeled grammar;parsers labeled;parsers gnus library;parsers labeled dependencies;parsers;check error parsers;error parsers labeled;error parsers;labeled dependencies gnus;grammar;induction algorithms;evaluating accuracy unlabeled;gnus library method;induction algorithms method;gnus library;evaluation accuracy unsupervised;dependencies gnus;dependencies gnus library;accuracy unlabeled;labeled dependencies;gnus;unlabeled", "pdf_keywords": ""}, "c065f9997794b13565dd49a6e475fc5e8c9d54ce": {"ta_keywords": "stiffness lamina wave;wave lt joints;tensile stiffness lamina;stiffness lt joint;lamina wave wave;lamina wave;results tensile stiffness;stiffness lamina;tensile stiffness lt;tensile stiffness;spring constant joint;improve tensile stiffness;lt joints approach;wave wave lt;wave wave;material structure joint;analysis results tensile;wave lt;stiffness lt;based double laminated;joints approach based;lt joints;joint utilizes kinetostatic;wave;improve tensile;joints approach;approach improve tensile;stiffness;results tensile;laminated material structure", "pdf_keywords": ""}, "112eb8a8273ab725d47789efb87237edbc4f02db": {"ta_keywords": "learnability restricted;learnability restricted order;learnability shown tractable;learnability shown;learnability;learning concepts conjunctive;analyze learnability shown;study learnability restricted;monomials learning concepts;tractable learning exists;analyze learnability;logics define simple;tractable learning;restricted order logics;shown tractable learning;description logic;simple description logic;learning exists alphabets;monomials learning;study learnability;learning monomials;learning monomials learning;learning concepts;logics define;learning exists;description logic summarize;power analyze learnability;alphabets primitive classes;logics;concepts conjunctive", "pdf_keywords": ""}, "e6accbbb366387faf817126dc7b0260c450bd2e6": {"ta_keywords": "mathini inline formula;mathini notation inline;tex mathini inline;mathini inline;inline formula tex;tex mathini notation;formula tex mathini;notation inline formula;inline formula;mathini notation;tex mathini;introduce tex mathini;notation inline;formula tex;mathini;inline;formula;notation;tex;introduce tex;introduce", "pdf_keywords": ""}, "ca7a67aa29c67b006017f651601091145644f243": {"ta_keywords": "calibrated speaker localization;speaker localization statistical;speaker localization;speech detection domestic;712 speaker localization;localization statistical speech;speaker localization 743;speech detection development;calibrated speaker;speech detection;integration calibrated speaker;statistical speech detection;localization 743 speech;calibrates localization;method calibrates localization;calibrates localization errors;accuracy 712 speaker;743 speech detection;localization statistical;statistical speech;localization;localization errors;speaker;localization errors included;method calibrates;localization 743;based method calibrates;743 speech;calibrates;712 speaker", "pdf_keywords": ""}, "a61aebbfe029c4b8eafae4042e6242cdca8f54b7": {"ta_keywords": "questions relation structures;questions relation;relation structures database;relation structures generate;generate relational;structures generate relational;database relation structures;answer entity;relation structures;target answer entity;domain questions relation;relational;training open domain;generate relational database;relational database relation;database relation;open domain questions;artificial questions;natural artificial questions;answer entity demonstrate;questions;relational database;relation;database conduct extractive;structures database;extractive training;entity;extractive training target;training target answer;domain questions", "pdf_keywords": "knowledge open domain;grounded relational wikidata;relation dense retrieval;open domain relational;relational model train;relational open domain;relational wikidata based;relational open;relations supervised;open domain supervised;relational extractive;relational wikidata;generate relational;relational model grounded;domain relational open;supervised open domain;interrelated relational extractive;based generate relational;construct grounded relational;relational qaq dataset;generate relational qaq;domain relational;relations supervised develop;constructed relational model;use trained relation;model answer questions;model train relational;relational extractive quality;generative knowledge open;trained relation"}, "2d1f442578feb7034aa2b68bbf95f608f2342256": {"ta_keywords": "group fairness;group fairness context;multiarmed bandit;context multiarmed bandit;formulation group fairness;multiarmed bandit cmab;fairness context multiarmed;fairness arbitrary;bandit;bandit cmab;fairness arbitrary number;bandit cmab setting;fairness context;notions fairness;notions fairness arbitrary;accommodate notions fairness;fairly protected groups;fairness;bounds regret algorithm;provide bounds regret;groups provide bounds;regret algorithm;groups;bounds regret;protected groups;regret algorithm validate;groups provide;datasets intervention;group;world datasets intervention", "pdf_keywords": "agents biased fairness;fairness biased feedback;contextual multiarmed bandit;group fairness contextual;context multiarmed bandit;group agents biased;group fairness biased;fairness contextual multi;biased feedback contextual;individual fairness simulated;bias operations agents;fairness contextual;bias mechanism optimal;agents biased;biased fairness constraint;biased fairness;multiple agent decision;group fairness;fairness framework describing;fairness framework;group fairness good;bias empirically explore;fairness simulated;fairness simulated case;fairness definitions capture;response group fairness;multiarmed bandit;individual fairness;bias context;biased feedback"}, "147ba336fcba32fadca470e14a858ce069375475": {"ta_keywords": "contour generation xmath3;based synthesis xmath0;xmath4 plane contour;synthesis xmath0;based synthesis xmath;synthesis xmath0 xmath1;synthesis xmath;states xmath2 plane;generation xmath3 states;xmath3 states;generation xmath3;xmath3 states xmath4;xmath2 plane model;states xmath4 plane;xmath1 states xmath2;xmath2 plane;xmath1 states;xmath0 xmath1 states;states xmath2;xmath5 states generated;contour based synthesis;xmath5 states;xmath4 plane;states xmath4;xmath2;xmath0 xmath1;xmath3;method xmath5 states;states generated contour;xmath1", "pdf_keywords": ""}, "00c8d88abef116d8d3d673a28ff4098115cf8da3": {"ta_keywords": "cooperative persuasive dialogue;automatic cooperative text;persuasive dialogue proposed;persuasive dialogue;based cooperative persuasive;cooperative text image;cooperative text;cooperative persuasive;dialogue proposed based;dialogue proposed;based natural language;automatic cooperative;dialogue;generation natural language;fully automatic cooperative;natural language;natural language understanding;natural language generation;image based cooperative;text image based;language generation natural;persuasive;text image;text;propose fully automatic;able persuade users;based cooperative;language generation;cooperative;reward learned", "pdf_keywords": ""}, "04a7d9f0388ded93c1ec16e36a6df3cd44cb95b0": {"ta_keywords": "multilingual entity linking;entity linking language;crosslingual linking;crosslingual linking problem;linking language specific;multilingual entity;linking language;entity linking;scale crosslingual linking;language specific mentions;formulation multilingual entity;large scale crosslingual;linking;new formulation multilingual;multilingual;formulation multilingual;mentions resolve language;knowledge base;knowledge base train;entity;crosslingual;linking problem;linking problem provide;language specific;specific mentions resolve;train dual encoder;dual encoder;resolve language;specific mentions;resolve language agnostic", "pdf_keywords": "multilingual entity linking;crosslingual entity linking;lingual entity linking;entity linking language;linking multilingual entity;multilingual entity reference;linking entities languages;entities language link;entity linking;entity linking based;multilingual entity;entity linking better;retrieving entities multilingual;single entity retrieval;linking multilingual;mentioned entity retrieval;entities multilingual;entity retrieval;entity linking seeks;generate crosslingual entity;language knowledge base;entity linking arbitrary;linking language;linking language specific;linking entities;knowledge entities language;cross lingual entity;crosslingual entity;entities languages crosslingual;entity retrieval model"}, "9768d7ba9d09ac3bf3d52ec674bde1a6e615daad": {"ta_keywords": "adaptive probabilities pairwise;probabilities pairwise comparisons;bern adaptive estimators;estimators bern adaptive;estimation bern adaptive;bern adaptive probabilities;adaptive estimators bern;bern adaptive;pairwise comparisons introduce;pairwise comparisons;aggregating pairwise comparison;adaptive probabilities;estimation bern;risk estimation bern;probabilities future comparisons;estimators bern;comparisons introduce adaptivity;pairwise comparison;pairwise comparison data;comparisons study minimax;bounds adaptivity index;adaptive estimators;minimax risk estimation;estimate outcome probabilities;introduce adaptivity index;comparisons;probabilities pairwise;order estimate outcome;adaptivity index;adaptive", "pdf_keywords": "estimating matrices indifference;estimating pairwise comparison;pairwise comparison probabilities;comparison probability matrix;matrix indifference;matrix indifference sets;pairwise comparison matrix;probability matrix indifference;aggregating pairwise comparison;matrices indifference;stochastic optimal ranking;matrices indifference sets;risk estimating pairwise;strong stochastic transitivity;adaptivity index indifference;indifference set matrices;matrices defined indifference;adaptivity index optimal;optimal adaptivity index;adaptivity index benchmarks;pairwise comparison probability;indifference sets matrix;indifference set matrix;stochastic transitivity;comparison matrix depends;transitivity design estimators;estimating pairwise;index indifference;stochastic transitivity sst;pairwise comparison data"}, "b7ffc8f44f7dafd7f51e4e7500842ec406b8e239": {"ta_keywords": "describing interaction sentences;interaction sentences;comprehension problem gating;interaction sentences sentences;reading comprehension tasks;interaction sentences approach;comprehension tasks;comprehension tasks including;sentences sentences approach;sentences approach combines;sentences;reading comprehension;sentences sentences;mechanism interaction sentences;fine grained gating;sentences approach;reading comprehension problem;gating;including reading comprehension;characters sentences;grained gating;sentences approach outperform;sentences uses;gating mechanism interaction;sentences uses fine;characters sentences uses;describing interaction;properties characters sentences;gating mechanism;grained gating mechanism", "pdf_keywords": "read comprehension tasks;reading comprehension tasks;attention reader;task natural language;natural language processing;performance natural language;gated attention reader;language processing tasks;question answering;question answering benchmark;comprehension tasks compute;comprehension tasks achieve;comprehension tasks approach;document query tokens;comprehension tasks;tasks like reading;combines natural language;token document generalization;attention architectures;semantics large scale;language processing;like reading comprehension;generalization gated attention;reading comprehension propose;approach read comprehension;reading comprehension combination;document generalization gated;query tokens introducing;tasks finding semantically;reading comprehension work"}, "90357a6dc817e2f7cec477a51156675fbf545cf1": {"ta_keywords": "learns multimodal script;learns multimodal;representations temporal commonsense;model learns multimodal;multimodal script knowledge;temporal commonsense;videos speech;videos speech transcribed;youtube videos speech;temporal commonsense achieve;multimodal script;script knowledge watching;multimodal;learns match images;contextualizes;box representations temporal;watching millions youtube;millions youtube videos;contextualizes happening globally;learns;contextualizes happening;model learns;representations temporal;youtube videos;images temporally;words contextualizes;images temporally corresponding;millions youtube;commonsense;corresponding words contextualizes", "pdf_keywords": "predicting multimodal representations;learning multimodal representations;multimodal temporal representations;learns multimodal;learns multimodal script;learning multimodal;multimodal world representations;multimodal representations;multimodal representations video;learning multimodal script;learning multimodal world;learning strong multimodal;predicting multimodal;increase learning multimodal;language representation videos;learning powerful multimodal;video language learning;model learns multimodal;captions learning powerful;strong multimodal temporal;multimodal world representation;multimodal temporal;video learn representations;captions learning;learning sequences videos;videos transcribed speech;model learning multimodal;multimodal event representation;match captions learning;multimodal representations apply"}, "ba3f39606cfd4150ea80fec1b2e1137933c6d143": {"ta_keywords": "generate stationary state;identically generated charges;generated charges method;generated charges;method generate stationary;generate stationary;charges method based;dependent probability density;stationary state;time dependent probability;charges method;stationary state consisting;charges method applied;dependent probability;probability density function;probability density;density function application;simple method generate;density function method;method generate;stationary;time dependent;density function;charges;density function consisting;identically generated;consisting identically generated;generate;collection identically generated;application time dependent", "pdf_keywords": ""}, "8ae4a584539a8f30d654e2678dde64a8334461b7": {"ta_keywords": "user personalized recommendation;denotated recommendation systems;personalized recommendation approach;personalized recommendation;recommendation systems used;recommendation systems;behavior user personalized;denotated recommendation;recommendation approach;recommendation approach based;underlying content recommendation;content recommendation build;learning behavior user;user reading product;behavior user reading;user personalized;content recommendation;family denotated recommendation;personalized;user reading;based generative model;recommendation build;model underlying content;approach based generative;generative model underlying;generative model;model behavior user;based generative;recommendation build family;build generative model", "pdf_keywords": "generates personalized reviews;personalized reviews conditioned;personalized reviews;personalized reviews product;provide personalized reviews;generated reviews;rating review model;personalized reviews large;generated reviews achieve;conditionally generated reviews;reviews using item;contextually likely reviews;generate highly recommendable;item rating network;sentiment rating review;classify sentiment;classify sentiment star;demonstrate classify sentiment;user ratings model;ratings model generates;rating review;rating user item;review product category;review model;product reviews conditioned;reviews product approach;ratings model;reviews using;rating network;random matrix representations"}, "b01ecfd2322437fcc9c7ce6605d6f5a50f67ec50": {"ta_keywords": "active learning generally;approaches active learning;active learning;active learning al;benefits active learning;active learning worth;actively acquired dataset;model actively acquired;approaches active;existing approaches active;model actively;outperform training sampled;training sampled data;learning;training sampled;learning generally;lack training data;training data;actively;outperform training;acquired dataset does;training data large;acquired dataset;learning generally generalize;learning worth;consistently outperform training;actively acquired;sampled data raises;learning al popular;dataset does", "pdf_keywords": "active learning text;use active learning;active learning;active learning extended;active learning al;active learning novel;benefits active learning;bayesian active learning;trained corpus active;efficacy active learning;models text classification;learning models investigate;learning tasks;deep bayesian active;text classification tasks;corpus active data;learning text;learning text classification;text classification use;benchmark text classification;acquisition classify sentences;classification general learned;learning models;classification tasks;model trained corpus;classification tasks conduct;text classification general;text classification support;learning model;learning ml"}, "2177bf060aaf2c0c2b551d3e805779cb35c19bb1": {"ta_keywords": "cold polar molecules;polar molecules based;ultra cold polar;polar molecules;cold polar;optical fiber thermal;generation ultra cold;fiber thermal;fiber thermal bath;polar;high temperature xmath0;ultra cold;temperature xmath0 low;temperature xmath0;optical fiber high;thermal bath high;density high temperature;high purity thermal;optical fiber;temperature;fiber high density;thermal bath method;high temperature;molecules based;thermal bath;molecules;purity thermal bath;thermal;molecules based combination;cold", "pdf_keywords": ""}, "2068825cabd94c951a0282ed731a8b8f2da1721c": {"ta_keywords": "supervised semantic parsing;semantic parsing learns;semantic parsing atis;semi supervised semantic;experiments semantic parsing;semantic parsing;parsing learns limited;parsing learns;supervised semantic;parsing atis;parsing;parsing atis domain;model semi supervised;semi supervised;data experiments semantic;semantic;outperforms strong supervised;supervised;experiments semantic;python code generation;strong supervised;atis domain python;strong supervised models;unlabeled data model;available unlabeled data;extra unlabeled data;unlabeled data;generation extra unlabeled;domain python code;supervised models", "pdf_keywords": "learned parser;data learned parser;predicting semantic parsing;predict semantic parsing;parser data learned;semantic parsing structvae;supervised semantic parser;parser learn;parser supervised;parser supervised learning;supervised semantic parsing;supervised parser;learned parser used;supervised parser self;supervised parser predict;domain learned parser;semantic parsing tree;parser self training;outperforms supervised parser;semantic parser;supervised parser data;parser learn lexical;uses supervised parser;novel supervised parser;parser proposed inference;semantic parsing combines;latent semantically structured;supervised parser supervised;semantic parsing;capable generating syntactically"}, "8ca58f3f6e59a6d243f3da6c196e9f730e6e9993": {"ta_keywords": "neural speaker diarization;speaker diarization eend;speaker diarization;end neural speaker;extends speaker tracing;speaker tracing;speaker tracing buffer;overlapping speech flexible;neural speaker;overlapping speech;handle overlapping speech;variable numbers speakers;method extends speaker;speakers proposed method;flexible numbers speakers;extends speaker;numbers speakers;numbers speakers proposed;numbers speakers experiments;speaker;speech flexible numbers;diarization eend;speakers experiments;speech flexible;speakers proposed;speakers experiments real;offline eend method;speakers;diarization;offline eend", "pdf_keywords": "online speaker diarization;novel speaker diarization;speaker diarization method;speaker diarization;conventional speaker diarization;speaker diarization framework;online speaker;offline model speakerin;leading speaker diarization;speaker models proposed;speaker based models;speaker permutation;speaker tracing;speaker permutation sbo;speaker tracing buffer;novel speaker tracing;propose speaker tracing;standard speaker permutation;speaker models;using speaker tracing;diarization overlapping speech;number speakers offline;online diarization;speakers offline model;speakers training offline;online diarization problem;overlapping speech flexible;class speaker based;model speakerin;number speakers chunks"}, "ca6d5c7829a76d10069fa3aa6776c35cc044b7ba": {"ta_keywords": "teaching learning undergraduates;learning undergraduates;learning undergraduates use;teach undergraduate;teach undergraduate courses;tools teach undergraduate;learning tools teaching;american association teaching;undergraduate courses;tools teaching learning;teaching learning;undergraduate courses discuss;undergraduates use;use tools teach;teaching learning large;university use tools;undergraduates use data;undergraduates;learning tools;courses discuss learning;association teaching learning;tools teach;tools teaching;undergraduate;teaching;association teaching;courses;courses discuss;teach;discuss learning", "pdf_keywords": ""}, "6e3f8187f8fef3e11578a73f32da07d33dbf8235": {"ta_keywords": "domain structured data;domain tables semantic;structured data release;structured data;open domain structured;data domain tables;semantic triples data;structured data darts;data domain data;domain data framework;domain structured;extracting semantic triples;tables semantic editing;triples data domain;tables semantic;domain tables;data domain;domain data;extracting semantic;semantic editing;semantic triples;method extracting semantic;darts data domain;data release dta;data framework;construction open domain;data darts data;data release;structured;data framework based", "pdf_keywords": "tree structured semantic;corpus structured data;structured semantic;semantic parsing datasets;ontology annotation tables;domain corpus structured;data structured text;domain semantic parsing;annotated tree ontologies;generating structured data;semantic parsing dialogue;structured semantic frame;ontologies structured data;rich semantic relationships;semantic parsing;corpus structured;generate structured data;harvest rich semantic;open domain corpus;annotation tables;domain corpus;exploiting semantic;text generation annotated;structured data structured;resulting table annotations;tree ontology annotation;structured text propose;semantic datasets;structured text;text generation data"}, "bdbf635476477eec5be5a292b494e20b8902cc35": {"ta_keywords": "accuracy machine translation;machine translation mt;machine translation systems;translation systems;translation mt systems;machine translation;translation systems including;variety machine translation;noise clean data;translation mt;typos proposed method;improve accuracy machine;accuracy machine;clean data;underlying text;represent underlying text;improve accuracy;typos proposed;clean data method;number typos proposed;accurately represent underlying;mt systems emulating;method improve accuracy;noise clean;translation;accurately represent;text;small number typos;number typos;occurring noise clean", "pdf_keywords": "robust noise translation;translate noisy text;accurately translate noisy;robustness machine translation;words robust noise;noise translation;translate noisy;noise use translation;noisy text guaranteed;dependent noise messageswe;noisy text;noise messageswe;exploiting natural noise;noise translation rare;network translation;noise messages;dependent noise messages;noise data;robust noise;noise messages contain;structure network translation;natural noise improved;noise clean data;noise data structure;words robust;noise messageswe present;noised data;systems natural noise;improved robustness vanilla;robustness machine"}, "b3979990dc2080138021cb3d767c7ec6d3e96194": {"ta_keywords": "novel unsupervised;relationships digital humanities;relationship descriptors;interesting correlations annotations;descriptors relationship dataset;text novels model;present novel unsupervised;topic model;outperforms topic model;global relationship descriptors;correlations annotations;digital humanities;topic model baselines;correlations annotations existing;humanities social sciences;humanities;novel unsupervised neural;network characterizing relationships;digital humanities social;relationship dataset;characterizing relationships digital;humanities social;raw text novels;relationship descriptors trajectory;characterizing relationships;unsupervised neural;relationship dataset raw;model baselines crowdsourced;descriptors relationship;baselines crowdsourced", "pdf_keywords": ""}, "e12c52fb542f76b3f0d29178842428d6a4edfe1e": {"ta_keywords": "defer arbitrary decision;defer given decision;arbitrary decision maker;defer process learning;learning defer process;learning defer;process learning defer;learning defer given;problem learning defer;arbitrary decision;deferring accurate biased;captures decision decision;decision maker;learning defer improve;demonstrate learning defer;defer improve accuracy;learn defer arbitrary;given decision maker;fairness used deferring;improve accuracy fairness;learn defer;accuracy fairness;decision decision;decision maker demonstrate;deferring accurate;decision decision making;decision making processes;decision maker propose;defer arbitrary;accuracy fairness used", "pdf_keywords": "learning defer decision;learning defer learning;adaptively learning defer;defer learning;learning defer adaptive;generalizes rejection learning;generalization rejection learning;learning defer models;defer equivalent learning;learning defer;extend rejection learning;method learning defer;rejection learning generalizes;rejection learning models;rejection learning framework;defer learning defer;defer decision making;learning generalizes rejection;learning defer equivalent;learning defer generalization;adaptive rejection learning;learning defer given;provided rejection learning;compare learning defer;problem learning defer;dm learning defer;outperforms learning defer;learning defer model;trained defer adaptive;equivalent learning defer"}, "4ce47dd7a8674f8ffd53f1883bc57e62460a83f0": {"ta_keywords": "privacy aware service;privacy consumers select;privacy consumers;differentiated according privacy;according privacy consumers;internet things interoperability;privacy aware;aware service contracts;security purchase insurance;tools privacy aware;power company incentive;privacy;vulnerability tradeoff energy;efficiency vulnerability tradeoff;tools privacy;service contracts;insurance inefficiencies arise;statistics tools privacy;efficiency vulnerability;inherent efficiency vulnerability;incentive invest security;insurance inefficiencies;security purchase;things interoperability propose;purchase insurance inefficiencies;service contracts service;contracts service differentiated;contracts service;according privacy;security", "pdf_keywords": ""}, "fa9b043ae8da3cc60c975762ae9066d2fb010f41": {"ta_keywords": "clustering natural language;graph clustering natural;use graph clustering;graph clustering;graph clustering algorithms;clustering natural;natural language processing;variety graph clustering;processing nlp;language processing nlp;graph representation;use graph representation;introduced nlp method;graph representation graph;recently introduced nlp;nlp method nlp;nlp method;method nlp;processing nlp tasks;clustering;representation graph method;nlp tasks;clustering algorithms including;nlp method based;clustering algorithms;representation graph;method nlp method;use graph;introduced nlp;tutorial use graph", "pdf_keywords": ""}, "52540497682c4209b8e20125c8255358b22d0fa7": {"ta_keywords": "cognitive learning students;cognitive learning tasks;perform cognitive learning;learning tasks student;approach cognitive learning;cognitive learning;future learning tasks;students using extended;student behavior learn;using extended student;learning students;extended student student;behavior learn student;student future learning;extended student;learning students using;extended student successfully;tasks student;novel approach cognitive;learning tasks;approach cognitive;problem student behavior;learn student;perform cognitive;student behavior;future learning;tasks student relatively;successfully perform cognitive;learn student future;problem student", "pdf_keywords": ""}, "2ed6f376e9e7eee6d833ad7b6aba63d7ad40c0f8": {"ta_keywords": "thermal instability fiber;instability fiber cooled;fiber formed thermal;fiber based thermal;thermal fiber based;instability fiber formed;plasma thermal instability;thermal fiber;type thermal fiber;instability fiber;instability fiber transferred;fiber transferred thermalized;thermalized plasma thermal;thermalized plasma;based thermal instability;plasma thermal;thermal instability generated;thermal instability;fiber cooled;thermalized plasma cooled;instability generated thermal;generated thermal instability;fiber cooled ground;layer thermalized plasma;state thermal instability;plasma cooled ground;transferred thermalized plasma;plasma cooled;fiber formed;fiber based", "pdf_keywords": ""}, "bd0db679d595399b91c5acca1db33a2803697d53": {"ta_keywords": "cues online political;evaluation social cues;effect social cues;influence candidate evaluation;social cues online;political information search;social cues;social cues function;voters reach judgments;online political information;influence candidate;search evaluation social;political information;cues adversely influence;candidate evaluation making;candidate evaluation;negative cues;counterparts negative cues;online political;adversely influence candidate;negative cues adversely;information search evaluation;heuristic allowing voters;informed counterparts negative;evaluation social;cues adversely;judgments similar informed;candidate;effect social;candidate absence signals", "pdf_keywords": ""}, "79f47ebf896b848e7c981c8aa6862ca1a7e5e7e5": {"ta_keywords": "kernel clustering bias;kernel clustering useful;kernel clustering;bias kernel clustering;locally adaptive kernels;adaptive kernels;adaptive kernels directly;kernels prove bias;heuristic kernel;clustering bias;clustering bias reduces;fundamental bias kernel;case heuristic kernel;common class kernels;bias kernel;class kernels;kernels;kernels directly;clustering useful small;kernels prove;locally adaptive;prove bias kernel;clustering useful;class kernels prove;clusters provide conditions;useful small clusters;clustering;class locally adaptive;small clusters;clusters", "pdf_keywords": ""}, "0222a48657d554b2a5a3d7ec3bb0b6833b8970a1": {"ta_keywords": "flow lfia thrombocytopenia;thrombocytopenia large prospective;thrombocytopenia;thrombocytopenia large;lateral flow assay;lfia thrombocytopenia;parasitized blood analysis;lfia thrombocytopenia large;diagnosis rapid nanoparticle;flow assay based;flow assay;suspected parasitized blood;parasitized blood pretest;parasitized parasitized blood;parasitized blood;blood analysis performed;blood analysis;patients suspected parasitized;nanoparticle based lateral;assay based use;assay based;4t9s diagnosis rapid;blood pretest probability;blood pretest;rapid nanoparticle;diagnosis rapid;rapid nanoparticle based;patterning 4t9s diagnosis;serptolpic patterning 4t9s;based use serptolpic", "pdf_keywords": ""}, "8c5ba1c914eab16b705da03352fe69d5bcfc72ea": {"ta_keywords": "structure induction summarization;abstractive summaries generating;generates abstractive summaries;induction summarization based;summaries generating;summarization based document;induction summarization;abstractive summaries;sentences source document;dependencies sentences source;summarization based;explicit dependencies sentences;summarization;summaries generating novel;dependencies sentences;single decoder models;decoder models;structure induction framework;summaries;documents generates abstractive;decoder models framework;document level structure;structure induction;end single decoder;trained cnn dm;incorporates interpretable sentence;cnn dm;single decoder;sentences source;latent explicit dependencies", "pdf_keywords": ""}, "7fc0097f6a51282dc1e9020d7c28e12cecaef519": {"ta_keywords": "xmath0 spin orbit;collider rhic spin;xmath0 spin;ion collider rhic;relativistic heavy ion;case xmath0 spin;rhic spin orbit;spin orbit coupling;ray lattice relativistic;heavy ion collider;lattice relativistic heavy;ion collider;coupling dynamics ray;lattice relativistic;orbit coupling responsible;effect spin orbit;orbit coupling enhanced;ray lattice rhic;orbit coupling dynamics;spin orbit;orbit coupling;dynamics ray lattice;collider rhic;rhic spin;xmath1;relativistic heavy;lattice rhic effect;investigate effect spin;ray lattice;lattice rhic", "pdf_keywords": ""}, "3cfb319689f06bf04c2e28399361f414ca32c4b3": {"ta_keywords": "transfer learning based;transfer learning;approach transfer learning;transfer learning unifies;framework transfer learning;mining problem transfer;training models;transfer model;transfer model model;pre training models;automated natural language;problem transfer model;training models large;automated natural;natural language processing;learning;natural language;transfer;novel approach transfer;learning unifies pre;fully automated natural;language processing np;models;learning based novel;new framework transfer;unlabeled fully automated;learning unifies;models large scale;approach transfer;framework transfer", "pdf_keywords": "problem transfer learning;transfer learning natural;transfer learning pipeline;transfer machine learning;transfer learning techniques;natural language tasks;transfer learning;transfer learning understanding;transfer learning various;approach transfer learning;use transfer learning;transfer learning achieves;framework transfer learning;model transfer learning;transfer learning machine;transfer learning context;transfer learning generative;text based prediction;natural language models;transfer learning objectives;predict semantics natural;training machine translation;prediction text based;method transfer learning;transfer learning combines;text transfer tasks;tuning natural language;learning natural language;transfer learning problem;performance transfer learning"}, "d19e097f388ca12ff111989b2bac7d3cc3cf15ca": {"ta_keywords": "uncover coordinated messaging;coordinated messaging analysis;riots detect networks;user text graph;clusters posting similar;text similarity graph;user clusters posting;parleys social media;text graph;messaging analysis;coordinated messaging;similar textual content;clusters posting;messaging analysis user;user coordination network;text graph text;posting similar textual;social media construct;coordinated user clusters;detect networks coordinated;coordination network graph;graph text text;text similarity;similar textual;networks coordinated user;graph text;text text similarity;user parleys social;networks coordinated;similarity graph", "pdf_keywords": "dynamics social media;social media disrupt;social media activity;rumor spreadthe dynamics;underlying social media;social network parler;patterns social media;riot users social;social media posts;parler political unrest;capitol riots generated;influence social media;message content social;dissemination disinformation;social media content;riots generated;capitol riots;free speech social;uncover coordinated messaging;2017 capitol riots;content social media;event patterns rumor;political unrest use;riots;users social media;propagation social information;dissemination disinformation responsible;tweets;political unrest;riots generated surge"}, "2c9158e20f58df04a6c5cd54dd3ee7d8df656421": {"ta_keywords": "optimization search;optimization search problems;framework optimization search;introduced optimization framework;optimization framework optimization;optimization optimization framework;optimization optimization;optimization framework;recently introduced optimization;optimization;optimization problems suitable;search problems based;optimization problems including;optimization problems;library optimization problems;framework optimization optimization;optimization framework framework;problems including optimization;including optimization;introduced optimization;library optimization;extensible library optimization;new framework optimization;framework optimization;search problems;search;variety optimization problems;variety optimization;use variety optimization;flexible extensible library", "pdf_keywords": "similarity search;similarity search methods;nn search library;new nn search;nn search;based nn search;similar search data;distance based nn;evaluation similarity search;n00010 based distance;similar search;sparse vectors neuneur;pattern recognition neuneuart;retrieval reranking plugable;text retrieval reranking;retrieval reranking;performing similar search;search data neuneumann;toolkit evaluation similarity;text retrieval;metric space library;accurate text retrieval;search library based;sparse vectors;similarity;distance agnostic algorithms;sparse vectors recent;vectors neuneur;search library;n00010 based nn"}, "d864944df8e765d597484ace12dbc3ac99e950a9": {"ta_keywords": "gradients proximal policy;proximal policy optimization;surrogate reward function;policy optimization;policy optimization pop;surrogate reward;pop surrogate reward;optimization pop surrogate;gradients proximal;tailed nature gradients;nature gradients proximal;reward function;gradients motivated;gradients motivated propose;reward function demonstrate;tailedness increases agent;robust estimators;proximal policy;optimization pop;dimensional robust estimators;gradients exhibit;demonstrate gradients exhibit;nature gradients motivated;nature gradients;gradients;high dimensional robust;demonstrate gradients;policy highlight;function demonstrate gradients;optimization", "pdf_keywords": "tailedness policy gradients;tailed policy gradients;policy gradients empirical;tails policy gradients;learning tail policy;theproximal policy gradient;distribution policy gradients;policy gradient methods;policy gradients observation;gradient aggregation estimator;learning policy noisy;trained policy gradients;underlying policy gradient;proximal policy gradients;policy gradient optimization;policy gradient algorithms;policy gradient learning;policy gradients performance;tailedness actor gradients;characterize policy gradients;estimator gradients policy;agents policy gradient;performance policy gradient;policy gradients;gradient underlying policy;proximal policy optimization;gradients proximal policy;robust estimator gradients;policy gradient heavy;tailedness gradients proposed"}, "e6239cc789da289929d49ffed2c0a562213d4703": {"ta_keywords": "welding residual stress;structural stability titanium;alloy ring stiffened;simulate welding residual;welding residual;stability titanium alloy;titanium alloy ring;effect welding residual;stiffened cylindrical shell;influence welding residual;fillet welding influence;ring stiffened cylindrical;stability titanium;fillet welding;welding position constraint;residual stress deformation;ring stiffened;cylindrical shell analyzed;titanium alloy;simulate welding;stress deformation structural;stress deformation analyzed;welding influence welding;caused fillet welding;deformation structural stability;welding position;welding influence;stiffened cylindrical;alloy ring;used simulate welding", "pdf_keywords": ""}, "60e339d25d43c026cf96395aa8accf34eae744a5": {"ta_keywords": "pairwise comparisons pictures;comparisons pictures based;crowdsourcing dataset;crowdsourcing dataset evaluating;comparisons pictures;scale crowdsourcing dataset;pairwise comparisons;filtering smoothed imagecaptions;smoothed imagecaptions;dataset evaluating pairwise;evaluating pairwise comparisons;smoothed imagecaptions image;large scale crowdsourcing;image iscaption;imagecaptions;scale crowdsourcing;crowdsourcing;pictures based known;large scale dataset;imagecaptions image;pictures based;image iscaption tempboxa;imagecaptions image iscaption;evaluating pairwise;pairwise;comparisons;iscaption;dataset evaluating;dataset;gaussian filtering smoothed", "pdf_keywords": "evaluating pairwise comparisons;graph pairwise comparisons;scalable crowdsourcing algorithm;scalable crowdsourcing;pairwise comparisons index;pairwise comparisons;pairwise comparisons based;propose scalable crowdsourcing;dataset evaluating pairwise;list pairwise comparisons;pairwise comparisons evaluate;crowdsourcing algorithm extracting;individuals toloka crowdsourcing;crowdsourcing algorithm;evaluating pairwise;pairwise comparisons ofwe;comparisons index favored;toloka crowdsourcing platform;crowdsourcing platform;crowdsourcing platform present;crowdsourcing;toloka crowdsourcing;comparisons index based;comparisons index;index list pairwise;comparisons based known;comparisons index use;pairwise;pairwise data;graph pairwise"}, "4ef77ef9b8ca8c8a96f1e62fa86a988feb582bd1": {"ta_keywords": "learning domain adaptation;domain adaptation;domain adaptation achieved;domain adaptation powerful;concept domain adaptation;machine learning domain;learning domain;machine learning;adaptation powerful concept;based concept domain;training model;concept machine learning;adaptation;concept domain;model generalized;achieved training model;learning;underlying distributions framework;concept machine;adaptation achieved training;adaptation powerful;larger set framework;adaptation achieved;powerful concept machine;training model small;underlying distributions;resulting model generalized;distributions framework based;distributions framework;model generalized larger", "pdf_keywords": "learning conditional regular;language models domains;domain training;training set generalization;language model adaptation;domain adaptation;learning including conditional;considers domain training;learning domain;accuracy domain adaptation;domains limited training;trained large domain;domain training strategies;model learning;machine learning domain;learning domain limiting;set generalization loss;learning conditional;generalization loss specialized;loss small learning;approach learning conditional;language models tuning;combines domain training;generalization loss;language models;specialized language model;domain adaptation depends;adaptation reduce learning;model training;weights language modeling"}, "09ec8d8e2251e079abb0e109979f33ee120211fa": {"ta_keywords": "planck constant method;proximal extragradient method;computing order planck;tensor method order;extragradient method;method order xmath0;tensor method;2013 tensor method;accelerated proximal extragradient;planck constant order;order planck constant;numerical accuracy method;computing numerical order;extragradient method 2013;method 2013 tensor;order order planck;numerical order;order planck;constant order planck;numerical order order;proximal extragradient;computing numerical;numerical;planck constant;accelerated proximal;author numerical;planck;hybrid accelerated proximal;numerical accuracy;method order", "pdf_keywords": ""}, "a8a863e85a95919773868204d672f1260e0058ce": {"ta_keywords": "neural machine translation;translation nmt models;machine translation nmt;machine translation;monolingual data training;universal model translate;translation nmt;model translate;shot translation able;zero shot translation;translate multiple languages;model translate multiple;domain adaptation simple;domain adaptation;used domain adaptation;translation able surpass;translation able;monolingual data;existing neural machine;use monolingual data;shot translation;translate multiple;language specific parameterization;enables use monolingual;use monolingual;modification existing neural;nmt models;translation;neural machine;nmt models enables", "pdf_keywords": "neural translation machine;neural machine translation;scalable machine translation;machine neural translation;translation machine model;machine translation nmt;framework machine translation;translation machine;machine translation tasks;neural translation;neural translation able;various machine translation;machine translation;human machine translation;translation learning;universal language encoder;nt machine translation;translation learning proposed;machine translation nt;machine translation enables;language encoder;language specific parameterization;description multilingual nmt;multilingual nmt systems;machine translation based;translation based modular;machine translation difference;signal neural translation;multilingual translation approach;translation source language"}, "d82592f3a110308366dfc7c42565d437b5bf59af": {"ta_keywords": "automate process social;social skills training;training method automated;automatically automate;automated automatically;method automatically automate;method automated automated;method automated automatically;automated automated;process social skills;human social skills;automate;automated;automatically automate process;automated tasks;set automated tasks;method automated;skills training method;tasks method automated;automate process;automated tasks task;automated automatically modifies;social skills;social skills learned;set automated;method automatically;automated automated validated;automatically;automated validated;skills learned participants", "pdf_keywords": ""}, "5dab371fecc43904c0b785a50136d20cee43a99a": {"ta_keywords": "translation trained parallel;translation transcribed speech;recognizer machine translation;cascaded speech recognizer;characterizing translation transcribed;transcribed speech combines;trained parallel texts;machine translation trained;transcribed speech;speech source text;speech recognizer;text alignment models;machine translation;cascaded speech;translation transcribed;translation trained;speech recognizer machine;source text alignment;transcribed;characterizing translation;speech source;source speech source;parallel texts;text alignment source;alignment models naturally;source speech;alignment models;trained parallel;target text alignment;establishing source speech", "pdf_keywords": "cascaded model speech;model speech translation;speech recognition cascade;speech translation models;model speech recognition;model speech;speech prediction;attention model speech;speech translation able;coding model speech;tasks speech translation;direct speech translation;speech translation stage;speech high resource;cascaded decoder attentional;speech prediction prediction;predict spoken language;applications speech prediction;translation models;able predict spoken;trained cascaded architecture;speech recognition task;translation models require;cascaded model trained;cascaded decoder;speech recognition;speech recognition ars;translated speech experiments;speech translation;predict spoken"}, "461188735d46dc1062f5d1d382d940a24c355fad": {"ta_keywords": "similarity extraction;similarity extraction_;use similarity extraction;extract similarity physical;extract similarity;corpus sentences similarity;called similarity extraction_;extract similarity context;sentences similarity;similarity used extract;similarity extraction_ use;sentences similarity used;sentences use similarity;used extract similarity;similarity physical objects;method extract similarity;similarity physical;objects called similarity;similarity;use similarity;similarity used;similarity context;called similarity;similarity context large;large corpus sentences;corpus;corpus sentences;context large corpus;large corpus;statistical method extract", "pdf_keywords": ""}, "f6be5d90199d1644b85e6b41a7a7f42fb29dbc9a": {"ta_keywords": "children object structure;evaluation children object;object structure perspective;children object;tool evaluation children;structure perspective taking;objects training significantly;object structure;objects training;children ospt ability;evaluation children;space dependent assessment;multiple objects training;investigate ability young;perspective taking;ability young elementary;perspective taking ospt;objects;assessment;assessment tool evaluation;structure perspective;dependent assessment;multiple objects;assessment tool;ability young;elementary students transfer;children ospt;dependent assessment tool;12 curriculum;involving multiple objects", "pdf_keywords": ""}, "a278c07c8bd2921e59dd862cd91a0540dd340030": {"ta_keywords": "estimation treatment effects;estimation treatment;external field methods;accurate estimation treatment;effect field methods;predict effect field;treatment effects;methods use neural;neural networks;treatment effects presence;neural networks predict;use neural networks;effect field;networks predict effect;models proposed methods;use neural;predict effect;external field;neural;external field cols;field methods;methods presence external;novel methods accurate;propose novel methods;novel methods;proposed methods outperform;networks predict;treatment;field methods use;analytical models", "pdf_keywords": "neural network treatment;prediction quality propensity;predicting outcome treatment;inference neural;treatment prediction objective;regularization improves estimation;estimation treatment;neural networks estimation;treatment prediction;estimation quality treatment;novel outcome prediction;networks estimation treatment;estimating effect treatment;inference neural networks;propensity score estimation;estimation treatment effects;prediction objective nednet;outcome treatment noisy;treatment effect estimation;prediction method dragonnet;estimation exploits propensity;model targeted regularization;training neural;regularization induces bias;targeted regularization improves;predicting outcome;parametric treatment prediction;approach training neural;approach targeted regularization;based targeted regularization"}, "9c5c794094fbf5da8c48df5c3242615dc0b1d245": {"ta_keywords": "unsupervised learning disentangled;learning disentangled representations;disentanglement learned representations;learning disentangled;disentangled representations fundamentally;disentangled representations;disentanglement learned;disentangled representations presence;enforce disentanglement learned;biases unsupervised learning;inductive biases unsupervised;disentangled;robustness unsupervised learning;unsupervised learning;impossible inductive biases;biases unsupervised;learned representations;successfully enforce disentanglement;inductive biases models;disentanglement;enforce disentanglement;inductive biases;presence inductive biases;representations fundamentally impossible;study robustness unsupervised;robustness unsupervised;representations presence inductive;representations fundamentally;biases models;unsupervised", "pdf_keywords": "unsupervised learning disentangled;learning disentangled representations;unsupervised disentanglement learning;disentangled representations unsupervised;learning disentangled representation;representation learning disentangled;disentanglement representations supervised;disentanglement learning;disentangled representations experimentally;trained disentangled representation;learning disentangled;disentanglement learning methods;learn disentangled representation;disentanglement learning impossible;learning methods disentanglement;disentangled representations fundamentally;disentangled representations;learned trained disentangled;disentangled representations robust;based disentangled representations;able learn disentangled;recent unsupervised disentanglement;disentangledtheorem establishes unsupervised;disentanglement representations;learn disentangled;trained disentangled;establishes unsupervised disentanglement"}, "2a2d03a1534b365c5b048c824c0886e16ccf7dfa": {"ta_keywords": "learns syntactic semantic;syntactic semantic inference;semantic inference rules;learns syntactic;model learns syntactic;parsed text model;semantic inference;syntactic semantic;knowledge base freebase;rules binary relations;large knowledge base;inference rules binary;knowledge base;graph representation parsed;representation parsed text;binary relations graph;binary relations;relations graph representation;walk model learns;inference rules;freebase reading;text model;base freebase reading;semantic;parsed text;predict resulting relations;text able predict;text model able;relations high accuracy;freebase reading entire", "pdf_keywords": ""}, "535c58ec8020782d41ed3ca72cf94aff7fd65120": {"ta_keywords": "recognition speech single;recognition speech;model recognition speech;speech single mode;neural network;model recognition;neural network network;speech single;model applied recognition;recognition;concept neural network;present model recognition;mode model based;speech;applied recognition;information individual nodes;based concept neural;concept neural;neural;single mode model;network coupled nodes;mode model;network network coupled;network coupled;individual nodes;nodes controlled way;individual nodes controlled;able capture information;coupled nodes;coupled nodes used", "pdf_keywords": ""}, "4c93bcc05d6b41cb3df451d703f34bab9c9e9201": {"ta_keywords": "differentially private text;private text mechanisms;privacy utility differentially;differentially private;private text;privacy utility;tradeoff privacy utility;privacy amplification;utility differentially private;balancing tradeoff privacy;laplace differentially private;noise privacy amplification;tradeoff privacy;privacy;privacy amplification step;defer noise privacy;noise privacy;private;text mechanisms;text mechanisms provide;text mechanisms defer;text mechanisms calibrated;differentially;utility differentially;text;mechanisms defer noise;called laplace differentially;framework called laplace;mechanisms defer;utility", "pdf_keywords": "privacy preserving er;assigning privacy noisy;privacy noisy data;preserves privacy users;algorithm preserving privacy;algorithm preserves privacy;privacy construct robust;method characterize privacy;preserving privacy;privacy preserving;preserve privacy users;algorithm construct privacy;characterize privacy user;construct privacy preserving;designing privacy preserving;characterize privacy;privacy preserving algorithm;defers noise privacy;preserves privacy;protecting privacy;preserving privacy data;privacy users analysis;characterizing privacy user;privacy data mining;able preserve privacy;assigning privacy;preserve privacy;preserves privacy use;characterizing privacy;approach characterizing privacy"}, "96abcdded2985bd44b9514e28f5b8da4fa1e4371": {"ta_keywords": "concept slippery;use word slippery;discussed concept slippery;concept slippery particle;word slippery;slippery particle;way concept slippery;word slippery widespread;slippery;slippery particle introduced;slippery widespread;slippery widespread tool;interpretability introduced;interpretability;interpretability introduced relevance;concept interpretability introduced;discussed concept interpretability;concept interpretability;tool analysis;analysis data;analysis data used;tool analysis data;objects non destructive;analysis;data;used model behavior;widespread tool analysis;particle introduced relevance;model behavior;objects non", "pdf_keywords": ""}, "18e8646001fc53465fdc8f8eb01523e24c134493": {"ta_keywords": "scores ordinal rankings;superiority cardinal scores;cardinal scores arbitrary;consider cardinal scores;ordinal rankings;cardinal scores ordinal;induced ranking;cardinal scores;ordinal rankings variety;estimators rely ranking;ranking estimators;induced ranking design;rely ranking estimators;ranking estimators used;consistent induced ranking;ranking;rely ranking;rankings;rankings variety;rating based estimators;ranking design;scores arbitrary miscalibrations;scores arbitrary;scores ordinal;rankings variety applications;ranking design rating;superiority cardinal;ratings strictly uniformly;ratings strictly;rating based", "pdf_keywords": ""}, "f78e5aaf34cc1e4874490e9155c640b73c630021": {"ta_keywords": "gradient conjectures;opponents optimize surrogate;implicit gradient conjectures;dynamic behavior players;opponents optimize;behavior opponents optimize;conjectures agents heterogeneous;agents heterogeneous conjectures;game theoretic;conjectures agents;embeds conjectures agents;game theoretic outcomes;gradient based learning;learning incorporates dynamic;gradient based;concept implicit gradient;model behavior opponents;behavior players games;cost function embeds;players games;gradient;players games study;surrogate cost function;number game theoretic;games study agents;variations equilibrium;behavior players;general framework gradient;heterogeneous conjectures;games study", "pdf_keywords": ""}, "f132f6534ec326a1ba61870b701015cd3d1560a2": {"ta_keywords": "domain adaptation using;domain adaptation;performance domain adaptation;machine translation selection;translation selection;translation selection data;domain generalization selection;target domain generalization;selection language modeling;tuning domain data;domain generalization;modeling machine translation;improving target domain;machine translation;tuning domain;fine tuning domain;adaptation using data;data selection language;selection language;generalization selection data;adaptation using;language modeling;target domain;language modeling machine;generalization selection;selection data complementary;domain data;using data selection;selection data effective;adaptation", "pdf_keywords": "selection language modeling;language models selection;pretrained language models;translation data selection;machine translation select;tuning pretrained language;optimize translation accuracy;modeling machine translation;pretraining domain classifiers;optimize translation;machine translation classifier;improves accuracy translation;pretrained language;translation pipeline trained;domain generalization training;translation classifier effective;machine translation data;translation accuracy deep;selection language;machine translation pipeline;machine translationwe propose;approach optimize translation;selection adapt language;classifier machine translation;new machine translation;machine translation;translation classifier;dataset machine translationwe;translation select;translation select domain"}, "835587dbe94b70adeb0b16384e10bb6e0e29de84": {"ta_keywords": "xmath1 symmetric xmath2;symmetric xmath2 xmath3;xmath0 symmetric xmath1;symmetric xmath1;xmath1 symmetric;xmath0 symmetric;symmetric xmath2;behaviour xmath0 symmetric;xmath2 xmath3 xmath4;symmetric xmath1 symmetric;xmath10 xmath11 xmath12;xmath9 xmath10 xmath11;xmath12 xmath13 xmath14;xmath3 xmath4 xmath5;xmath16 xmath17 xmath18;xmath4 xmath5 xmath6;xmath11 xmath12 xmath13;xmath2 xmath3;xmath3 xmath4;xmath8 xmath9 xmath10;xmath15 xmath16 xmath17;xmath14 xmath15 xmath16;xmath17 xmath18 xmath19;xmath7 xmath8 xmath9;xmath13 xmath14 xmath15;xmath6 xmath7 xmath8;xmath5 xmath6 xmath7;xmath17 xmath18;xmath11 xmath12;xmath16 xmath17", "pdf_keywords": ""}, "97bcea32979ed602fd404448a4e4cedad4171d79": {"ta_keywords": "prediction nonverbal behavior;predict nonverbal behaviors;models predict nonverbal;nonverbal behaviors given;nonverbal communication construct;predict nonverbal;nonverbal communication;nonverbal behaviors;prediction nonverbal;nonverbal communication process;nonverbal behavior participants;sentences nonverbal behavior;improve prediction nonverbal;nonverbal behavior;framework nonverbal communication;spoken sentences nonverbal;study nonverbal communication;sentences nonverbal;nonverbal;study nonverbal;framework nonverbal;process framework nonverbal;participants spoken sentences;participants spoken;language representation participants;representation participants spoken;language representation;input language representation;spoken sentences;communication", "pdf_keywords": ""}, "89c148d3d4edcb7b13c35da36b97ffb881c38058": {"ta_keywords": "control electrolarynx based;effective control electrolarynx;electrolarynx based statistical;control electrolarynx;electrolarynx based;electrolarynx;electrolaryl el speech;el speech experimental;speech experimental results;speech experimental;statistical excitation prediction;electrolaryl el;excitation prediction proposed;excitation prediction;natural electrolaryl el;electrolaryl;natural electrolaryl;based statistical excitation;produce natural electrolaryl;statistical excitation;excitation;el speech;prediction proposed experimental;enables ngectomees;method enables ngectomees;speech;control;enables ngectomees produce;effective control;prediction proposed", "pdf_keywords": ""}, "b8dcc2ae3346e41a421232169c2ca07957c654c4": {"ta_keywords": "dynamics agents games;coevolving network game;zero sum game;agents interact zero;game theoretic;game theoretic setting;introduce game theoretic;behavior coevolving network;sum game study;agents games;network game;game study dynamics;sum game;agents interact;laws information theoretic;dynamics agents;coevolving network;agents games setting;information theoretic;average behavior coevolving;interact zero sum;information theoretic flavor;polynomial time;theoretic setting agents;behavior coevolving;time algorithm predicts;algorithm predicts time;dynamics;time average behavior;exhibits polynomial time", "pdf_keywords": ""}, "135ace829b6ad2ec9db040d8e5fd137034e83665": {"ta_keywords": "conditional random fields;protein rich information;predict features protein;random fields semi;model protein rich;random fields;rich information synthesis;features protein conditions;features protein;information processing protein;model protein;processing protein rich;crfs able predict;information synthesis;contexts including protein;fields semi crfs;protein;rich information processing;protein rich;protein conditions easily;including protein rich;used model protein;able predict features;information synthesis propose;predict features;conditional random;including protein;processing protein;protein conditions;fields semi", "pdf_keywords": ""}, "635932ee917d71e01f07211c0359abf3e0e65e47": {"ta_keywords": "model automatic speech;speech recognition asr;automatic speech;automatic speech recognition;speech recognition;recognition asr;insertion based token;recognition asr problem;token sequence model;insertion based models;insertion based;insertion based model;model based insertion;new insertion based;dependent insertion based;based insertion based;asr problem model;output token sequence;token sequence;insertion;asr problem;token generation;based insertion;dependent insertion;sequence model;sequence model formulated;token generation non;generation non automatic;new insertion;based token generation", "pdf_keywords": "speech recognition asr;automatic speech;speech recognition;automatic speech recognition;model leading speech;end automatic speech;insertion based decoding;leading speech recognition;speech recognition ntm;connectionist temporal classification;speech recognition tasks;speech recognition mask;classification ctc insertion;temporal classification ctc;recognition asr;speech recognition ngs;modeling connectionist temporal;variety speech recognition;insertionion models;training insertion based;models insertionion models;insertion based models;based decoding generalized;recognition asr known;insertion based sequence;modeling insertion based;ctc insertion based;based sequence generation;modeling insertion;insertionion models ss"}, "b30195763eb103e2e5564228119f3810ab423b2e": {"ta_keywords": "text classification;perform text classification;topic sentiment classification;sentiment classification;data supervised words;supervised words;classification using labeled;labeled documents learning;text classification using;sentiment classification using;documents learning unlabeled;classification using label;related words label;words label;learning unlabeled data;words label names;learning unlabeled;classification;words describing categories;categories self training;unlabeled data supervised;including topic sentiment;topic sentiment;classification using;semantically related words;labeled documents;using labeled documents;categories associating semantically;data supervised;label class", "pdf_keywords": "weakly supervised text;model text classification;training labels words;category words supervised;model document classification;words supervised;benchmark text classification;text classification model;vocabulary classify;supervised text classification;document classification;word classificationwe propose;corpora learning labeled;predict category words;supervised text;categorize words classes;learning labeled;words class label;word classificationwe;classifier predict vocabulary;text classification;learning labeled data;document classification use;predict vocabulary;related word classificationwe;predicting analyzing labeling;categorize words;neural language model;model predict vocabulary;label names words"}, "002c256d30d6be4b23d365a8de8ae0e67e4c9641": {"ta_keywords": "language model using;language model;performance language model;improving performance language;performance language;language;method improving performance;model method effective;improving performance;method improving;parameters model;model using;model method;parameters model method;model accurate;tasks particularly effective;large variety tasks;model method able;particularly effective context;present method improving;model;tasks;model using small;tasks particularly;recent model accurate;number parameters model;method effective large;context recent model;effective context recent;original model method", "pdf_keywords": "improves language models;language models retrieval;large language models;improving language models;retrieval enhanced language;improve language models;tokens retrieval enhanced;large scale retrieval;improved language models;language model improves;trained language models;enhanced language models;language information retrieval;language models retrieving;efficient retrieval;improved language model;tokens retrieval;retrieval performance deep;retrieval tokens;language model large;information retrieval tokens;large corpus based;efficient retrieval documents;language models learning;retrieved large corpus;retrieval database english;preceding tokens retrieval;representation text training;models retrieval alleviate;large corpus"}, "933396f5b9111f6acdd76710ee6ab4d24e8673dd": {"ta_keywords": "speech text data;end automatic speech;automatic speech;automatic speech recognition;unpaired speech text;novel semi supervised;encoded speech text;semi supervised;network obtain speech;obtain speech text;speech recognition;advantages semi supervised;encoded speech;speech text;semi supervised ability;semi supervised method;large unpaired speech;dissimilarity encoded speech;unpaired speech;speech recognition method;obtain speech;supervised ability encode;encode large unpaired;encoder;shared encoder;encoder network;encode;encoder network obtain;shared encoder network;use shared encoder", "pdf_keywords": ""}, "4c49e9b57c8fc58b0df29a27ecca8cc2e376f02b": {"ta_keywords": "collaborative filtering data;effective collaborative filtering;useful collaborative filtering;collaborative filtering;collaborative filtering cf;data useful collaborative;data collection spider;filtering data collected;useful collaborative;filtering data;spider collect data;spiders data;spiders data collected;filtering;programmed spiders data;nearly effective collaborative;autonomous spider collect;collection spider autonomously;data collected spider;spider collect;collection spider;collaborative;users effective data;effective collaborative;collect data useful;data collection plausible;collected spider;uses autonomous spider;filtering cf algorithm;programmed spiders", "pdf_keywords": ""}, "7d5f83cb234a5640487bd258ace06d9dc967d222": {"ta_keywords": "statistical relational learning;relational learning combines;relational learning;learning latent context;latent context learning;structure learning latent;learning combines structure;combines structure learning;joint learning inference;context learning;structure learning;statistical relational;joint learning;domains joint learning;learning inference sl;learning inference;context learning approach;learning latent;approach statistical relational;relational;learning combines;latent context;latent context invention;tasks latent context;learning approach;learning;state art baselines;learning approach outperforms;inference sl;baselines real world", "pdf_keywords": ""}, "f1c7419b87cbf853e691e500643f71720b68fb86": {"ta_keywords": "online stacked learning;stacked graphical learning;learning stacked learning;stacked learning stacked;stacked learning;learning stacked;stacked learning save;online stacked;new stacked graphical;stacked graphical;pass online stacked;large streaming datasets;stacked;graphical learning scheme;propose new stacked;new stacked;streaming datasets;graphical learning;streaming datasets minimal;large streaming;learning scheme;handle large streaming;learning save training;streaming;learning save;datasets minimal memory;save training time;learning;learning scheme integrate;datasets", "pdf_keywords": ""}, "9b3fd2525a2d1abc44145308e013f117d3d7bdee": {"ta_keywords": "el speech enhancement;control electrolarynx based;electrolaryl el speech;speech enhancement;electrolarynx based;control electrolarynx;speech enhancement technique;f0 control electrolarynx;electrolarynx based statistical;electrolarynx;speech keeping intelligibility;develop electrolaryl el;el speech keeping;face conversation experimental;electrolaryl el;prediction develop electrolaryl;develop electrolaryl;speech keeping;electrolaryl;statistical excitation prediction;el speech;conversation experimental;excitation prediction;excitation prediction develop;conversation experimental results;based statistical excitation;face conversation;speech;face face conversation;enhancement technique effective", "pdf_keywords": ""}, "1afa3ab80abda57920b8d456a6513e6f01cc82e7": {"ta_keywords": "predicting length conversations;time event conversations;conversations datasets;length conversations;event conversations;methods conversations datasets;event conversations method;conversations datasets demonstrating;conversations method based;conversations method;regression methods conversations;length conversations demonstrate;survival regression methods;conversations;predicting time event;survival regression;based survival regression;conversations demonstrate method;conversations demonstrate;survival regression family;methods conversations;predicting time;method predicting time;method based survival;novel method predicting;art survival regression;effective predicting length;state art survival;predicting;survival", "pdf_keywords": ""}, "5cfdb162ffa4dce18f7c576d352bd459b6a11292": {"ta_keywords": "rush driven coupons;category future purchases;driven coupons;future purchases based;driven coupons currently;coupons currently delivered;future purchases;coupons;coupons currently;purchases based anonymized;forecasting time category;card providers rush;time category future;time category information;forecasting time;framework forecasting time;purchases based;value rush driven;providers rush provides;category future;time category;forecasting;credit card;providers rush;large scale transaction;transaction records;model time category;value rush;novel framework forecasting;purchases", "pdf_keywords": ""}, "1ddcc9671dd6486e34cefadfe71bbbc1bc55035a": {"ta_keywords": "embeddings word similarity;subword information embeddings;word embeddings;word embeddings trained;source word embeddings;word embeddings low;standard word embeddings;word embeddings word;embeddings word;embeddings trained morphologically;morphologically rich subword;class word embeddings;information embeddings outperform;rich subword information;word similarity;word similarity task;embeddings low resource;information embeddings;embeddings outperform;embeddings trained;embeddings low;subword information;trained morphologically rich;embeddings outperform standard;embeddings;rich subword;trained morphologically;subword;morphologically rich;outperform standard word", "pdf_keywords": ""}, "33d2ebe41477811296abc4077bf9ce09b927ef98": {"ta_keywords": "voice conversion;voice conversion based;model source speaker;framework voice conversion;speaker model target;conversion based noisy;integrates speaker model;source speaker;source speaker experiments;speaker model;framework integrates speaker;speaker experiments;integrates speaker;noisy channel model;voice;speaker;based noisy channel;conversion based;speaker experiments demonstrate;framework voice;channel model proposed;channel model;noisy channel;novel framework voice;conversion;based noisy;density model source;model source;model target;noisy", "pdf_keywords": ""}, "c0e8846eb5ce574a6dca3f3a600e82b184339254": {"ta_keywords": "rewards language context;rewards language;infers rewards language;interactive flight booking;booking natural language;interactive flight;new interactive flight;environments inferred rewards;inferred rewards;rewards accurately inferred;inferred rewards accurately;language new interactive;language predict optimal;infers rewards;flight booking natural;natural language predict;model infers rewards;rewards accurately;flight booking;predict optimal actions;language predict;simple reinforcement;reinforcement learning;simple reinforcement learning;use language;language;reinforcement;rewards;language context;ones simple reinforcement", "pdf_keywords": "language utility reinforcement;behavior natural language;natural language agent;rewards conditioned utterance;flight booking game;utterance reward function;decisions utterance reward;natural language utility;language improve reward;describing natural language;flight language;utterance reward;reward inference able;interactive flight booking;reward inference;natural language representations;model infer reward;reward inference context;natural language;evaluating natural language;reward function model;preferences natural language;task reward inference;goal game communicate;flight language like;language agent;interpret natural language;language agent tasked;booking game;reward learning model"}, "a3cc75975a5998d5a7dd494e70a479ba0a550013": {"ta_keywords": "authoring expert model;expert model student;authoring expert;required authoring expert;expert model;expert model generated;authoring student problem;quality expert model;authoring student;author authoring student;model student problem;authoring problem;problem study;student problem study;authoring process;authoring problem greater;authoring process strategy;problem study examines;required authoring problem;expert;time required authoring;model generated authoring;required authoring;student problem;authoring;model student;quality expert;author authoring;examines quality expert;generated authoring process", "pdf_keywords": ""}, "04f4e55e14150b7c48b0287ba77c7443df76ed45": {"ta_keywords": "physical commonsense reasoning;answering physical commonsense;task physical commonsense;physical commonsense;physical commonsense provide;commonsense reasoning corresponding;commonsense reasoning;physical commonsense process;objects natural language;commonsense provide analysis;question answering physical;commonsense;commonsense process modeling;dataset physical interaction;commonsense process;interaction question answering;commonsense provide;behavior objects natural;behavior objects;knowledge existing models;physical interaction;modeling behavior objects;natural language;question answering;objects natural;answering physical;modeling behavior;introduce task physical;dataset physical;task physical", "pdf_keywords": "physical commonsense reasoning;physical commonsense understanding;physical commonsense knowledge;task physical commonsense;understanding physical commonsense;physical commonsense;studying physical commonsense;require physical commonsense;commonsense reasoning;source physical commonsense;commonsense reasoning easy;commonsense reasoning corresponding;physical interaction answering;physical commonsense world;physical commonsense common;answering physical interaction;interaction answering physical;descriptions everyday objects;humans physical commonsense;commonsense knowledge;commonsense concepts able;commonsense understanding;extent physical knowledge;commonsense knowledge introduce;commonsense understanding natural;key commonsense concepts;interaction answering;physical knowledge;commonsense concepts;knowledge introduce physical"}, "5e0f2f82b4a28d59f1aa8b8ffe497790de1cdf9d": {"ta_keywords": "deep reinforcement learning;mistakes deep reinforcement;deep reinforcement;reinforcement learning toy;learning reinforcement;learning reinforcement learning;avoids catastrophic states;reinforcement learning;reinforcement learning reinforcement;catastrophic states avoiding;mitigate catastrophic mistakes;catastrophic mistakes deep;avoids catastrophic;mitigate catastrophic;reinforcement;approach mitigate catastrophic;unacceptable performance reinforcement;reinforcement learning approach;performance reinforcement learning;accelerates learning guards;catastrophic states;method avoids catastrophic;catastrophic mistakes;relies intrinsic fear;learning toy problems;avoiding dangerous states;policies repeated catastrophes;learning guards alternating;performance reinforcement;learning guards", "pdf_keywords": ""}, "7a9f4a8a99f9a38e9213da890f9eab6150ae928e": {"ta_keywords": "learning similarity measures;similarity graph associated;similarity graph;learning similarity;similarity measures;similarity measures based;approach learning similarity;degree similarity graph;similarity;walks use labeled;label associated graph;estimate degree similarity;labeled edge sequence;based random walks;large graph associated;random walks;degree similarity;random walks use;associated graph used;labeled edge;graph associated;query large graph;edge sequence label;associated graph;labeled labeled edge;graph associated graph;associated graph associated;graph used query;large graph;use labeled", "pdf_keywords": ""}, "ebaae38a09c5a4909049e16af759c71db9cc87dc": {"ta_keywords": "desireable level neural;model level neural;noisy examples model;level neural model;level neural;neural model input;neural model;noisy examples;continuous noisy examples;simple desireable model;neural model variety;neural;performs noisy examples;noise model;inputs words;desireable model secure;noise model level;desireable model;continuous noisy;model performs noisy;noisy;simple desireable;resistant noise model;words outputs desired;model secure;inputs words outputs;noise;desireable level;input inputs words;variety continuous noisy", "pdf_keywords": ""}, "b17fa6625681d99370122145ba9911f701dd92cb": {"ta_keywords": "decoding semantic parser;semantic parser adapt;decoding semantic;semantic parser;13 context modeling;grammar based decoding;typical context modeling;based decoding semantic;parser;context modeling;parser adapt;context modeling methods;parser adapt typical;13 context;evaluate 13 context;grammar based;based decoding;decoding;adapt typical context;domain datasets;grammar;domain datasets best;cross domain datasets;context;semantic;present grammar based;typical context;datasets best;datasets significant improvements;datasets", "pdf_keywords": "semantic parsing context;parsing context challenging;parsing context;parser used context;parsing context present;existing context modeling;typical context modeling;13 context modeling;effective context modeling;parsing context compare;decoding semantic parser;world semantic parsing;semantic parser;semantic parser adapt;semantic parsing;modeling contextual;different context modeling;context modeling explore;modeling contextual clues;context modeling relationship;context modeling;generalize frequent contextual;schemas context cross;context modeling methods;frequent contextual;problem semantic parsing;problem context modeling;decoding semantic;inference modeling contextual;schemas context"}, "bf4da952df7a6ef9c0b2be8b4b4b69ad63848b8f": {"ta_keywords": "transfer learning;transfer learning framework;propose transfer learning;predict traffic;spatiotemporal data source;used predict traffic;spatiotemporal data;predict traffic speed;spatiotemporal features;spatiotemporal features proposed;effective spatiotemporal features;traffic;structure spatiotemporal data;various effective spatiotemporal;traffic speed target;effective spatiotemporal;spatiotemporal;traffic speed;transfer;empirical structure spatiotemporal;data source areas;structure spatiotemporal;learning;source areas;learning framework;speed target areas;historical data;source areas various;features proposed approach;data available", "pdf_keywords": ""}, "879cd78b0d4413aef614bc6b6cce075e8e6ad4be": {"ta_keywords": "streaming codes designed;streaming codes;streaming code;existing streaming codes;streaming codes incorporates;existing streaming code;streaming codes variable;model streaming codes;streaming code constructions;construction streaming codes;code arrival sizes;arrivals proposed code;achieved existing streaming;proposed code arrival;existing streaming;streaming;variable size arrivals;fixed size arrivals;codes variable size;code arrival;size arrivals theoretically;size arrivals proposed;builds existing streaming;model streaming;generalized model streaming;proposed code construction;size arrivals;simple construction streaming;proposed code;size arrivals present", "pdf_keywords": ""}, "bd318e959236b0d33a7567b6d3afc8d5e92b8ea3": {"ta_keywords": "ethical boundary form;clauser horne domain;notion ethical boundary;ethical boundary;horne domain;horne domain form;domain form clauser;bounded domain form;form bounded domain;form clauser horne;clauser horne;boundary;bounded domain;domain;form domain;domain form domain;notion ethical;domain form;ethical;introduce notion ethical;form clauser;form domain form;boundary form;clauser;horne;boundary form bounded;form bounded;notion;bounded;introduce notion", "pdf_keywords": "bounded artificial intelligence;artificial intelligence preferences;intelligence artificial intelligence;decision swarm agents;notion bounded artificial;existence artificial intelligence;intelligence artificial;artificial intelligence able;agent able play;game playing agent;artificial intelligence axion;intelligence robust flexible;artificial intelligence clarified;games distinguishable play;boundeded machine useful;playing agent able;artificial intelligence robust;artificial intelligence artificial;bounded artificial;artificial intelligence;games distinguishable;game played agent;properties artificial intelligence;artificial intelligence including;agent representation;behaviors artificial intelligence;introduction artificial intelligence;play games distinguishable;agent able;intelligence robust"}, "4a93f7654f795871ed99dece2e1805e4950fd194": {"ta_keywords": "learns understand spatial;referent spatial utterances;spatial utterances;referent spatial;tabletop scene seeking;spatial utterances level;representations la semantic;understand spatial;objects tabletop scene;la semantic relations;comprehension data human;spatial;spatial configurations objects;recovers referent spatial;utterances;la semantic;semantic relations;semantic relations referents;learns understand;tabletop scene;scene seeking;objects tabletop;utterances level approaching;comprehension data;semantic;understand spatial configurations;present learns understand;referents landmarks evaluate;referents landmarks;eliciting production comprehension", "pdf_keywords": ""}, "9e172f35b2b0ebcff090f01d40e61fa5aecefa68": {"ta_keywords": "different adversarial losses;adversarial losses;adversarial losses based;adversarial losses propose;valid adversarial losses;adversarial losses combining;set adversarial losses;discriminative adversarial networks;discriminative adversarial;adversarial networks;different adversarial;adversarial networks sdms;adversarial;comparing different adversarial;adversarial networks dans;generative adversarial networks;generative adversarial;based discriminative adversarial;losses based discriminative;valid adversarial;functions valid adversarial;set adversarial;based generative adversarial;extensive set adversarial;generative;losses based;discriminative;component functions regularization;based generative;losses", "pdf_keywords": ""}, "ebe04f06580abab035408c4c2e65245b3950934e": {"ta_keywords": "oscillator nonlinear optical;nonlinearity parametric oscillator;parametric oscillator nonlinearity;optical parametric oscillator;parametric oscillator nonlinear;nonlinear optical parametric;nonlinear optical;type nonlinear optical;oscillator nonlinearity;oscillator nonlinear;oscillator based nonlinearity;nonlinearity nonlinearity parametric;oscillator nonlinearity good;parametric oscillator;nonlinearity parametric;parametric oscillator based;optical parametric;nonlinearity nonlinearity nonlinearity;nonlinearity nonlinearity;based nonlinearity nonlinearity;nonlinearity;nonlinearity nonlinearity good;nonlinear;oscillator based;type nonlinear;based nonlinearity;new type nonlinear;nonlinearity good;oscillator;agreement nonlinearity nonlinearity", "pdf_keywords": ""}, "dd36aca034312a266d6f10b37414d3342c3b9c79": {"ta_keywords": "energy particle quantum;energy particle field;particle quantum field;calculation energy particle;calculating energy particle;quantum field method;quantum field;particle assumed field;field basis energy;particle field;energy particle;transverse momentum particle;basis energy particle;particle field basis;particle quantum;method calculating energy;calculation energy;calculating energy;field fixed transverse;based calculation energy;momentum particle;momentum particle assumed;particle assumed;transverse momentum;fixed transverse momentum;particle;quantum;basis energy;field method;field method applied", "pdf_keywords": ""}, "17351cfeac949c266f4d1ff86c515250b931bdc2": {"ta_keywords": "structure learning logic;learning logic programs;learning logic;logic programs graphs;logic networks;learning method logic;logic programming;probabilistic logic;logic programming framework;learn structures;logic programs;new probabilistic logic;including logic networks;logic networks foil;probabilistic logic called;learn structures tractable;structure learning;structure learning method;efficient structure learning;shown learn structures;method logic programming;novel structure learning;meta interpretive learning;programs graphs;logic called proppr;logic;including logic;programs graphs proposed;domains including logic;logic called", "pdf_keywords": ""}, "394e17f5ee5e8a734b2714795b7da3cd704716da": {"ta_keywords": "class online courses;learning platform moocs;online courses;precision cardinal scores;online courses delivered;class online;cardinal scores significantly;platform moocs;platform moocs proposed;cardinal scores;moocs proposed approach;improved using ordinal;content class online;courses delivered massive;moocs;scores significantly improved;moocs proposed;massive learning platform;scores significantly;courses;ordinal approach;ordinal approach evaluate;ordinal;using ordinal approach;using ordinal;courses delivered;precision cardinal;cardinal approaches;conventional cardinal approaches;learning platform", "pdf_keywords": ""}, "57e4074c588c0e27e4c0bc89f12512ccdb900d79": {"ta_keywords": "unsupervised style transfer;text style transfer;including sentiment transfer;style transfer tasks;sentiment transfer;sentiment transfer formality;style transfer unifies;style transfer;unsupervised text style;deep generative;corpus learns transform;non generative techniques;deep generative model;transfer wordment related;transfer wordment;parallel corpus learns;generative model unsupervised;unsupervised style;generative techniques;present deep generative;model unsupervised text;non generative;unsupervised text;proposed non generative;corpus learns;formality transfer wordment;learns transform sequences;generative techniques model;generative;generative model", "pdf_keywords": "deep generative;deep generative model;unsupervised machine translation;generative model inference;machine translation unsupervised;latent parallel corpus;novel variational inference;generative model unsupervised;data generative;generative techniques hypothesizing;generative techniques;present deep generative;parallel corpus latent;non generative techniques;amortized variational inference;translation unsupervised machine;sequence model learns;variational inference natural;unsupervised style transfer;generative;variational inference;generative model;parallel corpus model;challenging parallel corpus;sharing generative;parallel corpus partially;inference natural unsupervised;transfer machine translation;data generative framework;learns transform sequences"}, "8a99e1eb3285f127eed7169441679d47be7f1633": {"ta_keywords": "data text generation;generation data text;text generation directly;text generation;splice neighbor text;data generation;neighbor text method;generation data;data generation tasks;neighbor text;useful generation data;data text;variety data generation;generation directly splice;useful generation;graph theoretic;simple graph theoretic;directly splice neighbor;generation directly;generation tasks;particularly useful generation;method generation data;splice neighbor;graph theoretic framework;text method based;generation tasks method;text;powerful method generation;text method;based simple graph", "pdf_keywords": "generating text;generate text splicing;text generation directly;text generation;generation text;data text generation;text generation simple;generate text;generation text np;splicing neighbor text;predict generation text;generation unstructured text;text splicing neighbor;generate unstructured text;informative text generation;generating text data;generation text use;text generation models;generation splicing approach;predicting generation text;generation text sender;novel generation splicing;text generation set;text splicing;generation text model;generation splicing;problem text generation;sequence generation text;framework generating text;text generation fixed"}, "3c40fc36217a56aafb0abc735ff7d132b17e83a0": {"ta_keywords": "melody harmonization;melody harmonization including;task melody harmonization;melody chord pairs;harmonization;chords result evaluation;harmonization including;melody chord;harmonization including template;dataset 9226 melody;chords;9226 melody chord;melody;9226 melody;chords result;chord pairs considering;chord pairs;task melody;approaches task melody;triad chords;different triad chords;chord;triad chords result;template matching;model deep;deep learning evaluation;model deep learning;hidden model evaluation;deep learning;hidden model deep", "pdf_keywords": "predicting chords melody;melody harmonization model;automatic melody harmonization;model melody generation;melody generation model;melody based deep;predict chords melody;music harmonization models;melody harmonization;models harmonization music;harmonization melody evaluation;matching model melody;melody generation based;automatic melody;melody generation;predicting chords;melody harmonization including;melody harmonization implement;music harmonization model;task melody harmonization;framework predicting chords;novel music harmonization;models task melody;based model melody;task automatic melody;harmonization melody;melody note model;harmonization music;music harmonization;model melody"}, "77efa3102456c9f921b05b95eefe845d2ce6bc4b": {"ta_keywords": "relative entropy discrimination;continuous phoneme recognition;phoneme recognition;phoneme recognition tasks;regularized discriminative training;conventional discriminative training;discriminative training methods;discriminative training method;entropy discrimination;discriminative training;minimum relative entropy;propose regularized discriminative;entropy discrimination mred;regularized discriminative;training method acoustic;relative entropy;conventional discriminative;discriminative;acoustic models;acoustic models derived;method acoustic models;recognition tasks;evaluated continuous phoneme;recognition;interpretation conventional discriminative;continuous phoneme;training methods propose;method acoustic;training methods;regularized", "pdf_keywords": ""}, "803a0d2677a7d6b20c3964533595775fa5c7c750": {"ta_keywords": "ranked preference data;preference data test;test ranked preference;ranked preference;preference data;sample test ranked;pairwise comparison models;pairwise comparisons;comparison models;preference;pairwise comparisons finally;pairwise comparison;test ranked;comparison models parameter;comparisons;range pairwise comparison;consisting pairwise comparisons;comparisons finally;empirically evaluate results;ranked;datasets consisting pairwise;comparison;comparisons finally apply;data test;distinguish sets samples;empirically evaluate;data test designed;pairwise;test designed distinguish;real world datasets", "pdf_keywords": "testing partial ranking;pairwise comparison ranking;comparison ranking data;hypothesis partial ranking;optimal tests pairwise;comparison ranking;tests pairwise comparison;testing pairwise comparison;algorithm partial ranking;stochastic pairwise comparisons;pairwise comparisons items;partial ranking;pairwise comparisons;testing problem ranking;partial ranking subset;partial ranking data;pairwise comparison models;observe pairwise comparisons;sample testing pairwise;testing given rank;data partial ranking;case partial ranking;partial total ranking;pairwise comparisons partial;pairwise comparison data;ranking data distinguish;partial rankings;algorithm pairwise comparison;ranking data partial;testing algorithm pairwise"}, "c859416a8e5682bee3c35df29bc02e02a22de072": {"ta_keywords": "darwin brown tao;tao li darwin;language processing darwin;natural language processing;natural language resources;distribution natural language;li darwin brown;brown tao li;natural language;literature darwin brown;processing darwin brown;field natural language;li darwin;tao li;brown tao;darwin brown;literature darwin;processing darwin;resources literature darwin;tao li leading;language processing associated;language processing;darwin;tao;language resources literature;language resources;literature;li;language;brown", "pdf_keywords": ""}, "90705ece92a71efcf256cd047da53cbc1d4e5295": {"ta_keywords": "gear meshing frequency;frequency gear meshing;meshing frequency rotating;transmission frequency gear;meshing frequency;gear vibration signal;gear vibration;meshing frequency investigated;machines gear meshing;understand gear meshing;meshing frequency transmission;vibration signal gear;gear meshing;signal gear meshing;meshing frequency key;frequency gear;component gear vibration;frequency rotating machines;rotating machines gear;frequency rotating;frequency transmission frequency;transmission frequency;frequency investigated transient;understand gear;frequency investigated;frequency;machines gear;transmission frequency transmission;frequency transmission;link transmission frequency", "pdf_keywords": ""}, "30fa01df767339a6c8bd37c32160992fcb19ed18": {"ta_keywords": "bandit problem algorithm;bandit problem;armed bandit problem;multi armed bandit;bandit;large scale games;algorithm solving large;armed bandit;solving large scale;novel algorithm approximate;scale games;solving large;games based multidimensional;large set baselines;algorithm approximate;novel algorithm;algorithm based super;scale games based;accurate large set;games;algorithm approximate equi;super arm guaranteed;guaranteed accurate large;algorithm improvement performance;performance algorithm;large scale;algorithm compared;new algorithm solving;algorithm solving;experiments accuracy algorithm", "pdf_keywords": ""}, "196be0bdec3b7bcb3ee35cd126fb2730a9d742d6": {"ta_keywords": "student able solve;student projects;set student projects;student projects designed;study student problem;finding student able;student problem;projects designed solve;students;student problem problem;study student;problem finding student;flexible allowing students;solve simple problems;approach study student;finding student;student;projects learn approach;projects learn;allowing students choose;students choose;allowing students;student able;set student;projects;different projects learn;problems approach designed;students choose different;designed solve simple;use set student", "pdf_keywords": ""}, "46619f0547b1a9c2e7649d0e5c931e9aa857a938": {"ta_keywords": "spin polarized light;generation spin polarized;light induced spin;polarized light xmath0he;spin polarized;observation spin polarized;polarized light method;spin orbit coupling;polarized light;polarized light induced;polarized light based;induced spin orbit;spin orbit;applied generation spin;subsequent generation spin;induced spin;idea spin orbit;generation spin;method generation spin;observation spin;based observation spin;spin;light xmath0he xmath1he;light xmath0he;polarized;light method;light method applied;orbit coupling method;orbit coupling;idea spin", "pdf_keywords": ""}, "8963602d4b9c3b1054a5ed6fb2a2088dec774824": {"ta_keywords": "personal information management;searches personal information;improve personal information;performance searches personal;searches personal;personal information;improve performance searches;machine learning used;information management tools;machine learning;information management;performance searches;evidence machine learning;improve personal;searches;used improve personal;learning used improve;circumstances machine learning;management tools;learning used;used improve performance;improve performance;management tools use;evidence machine;information;performance;personal;experimental evidence machine;learning;machine", "pdf_keywords": ""}, "daedf33077099f7c808e9f4022469e15bf224ad7": {"ta_keywords": "organisms depends critically;organisms choice immune;crucial deciding survival;immune crucial deciding;deciding survival life;healthy organisms depends;deciding survival;fitness choice immune;lived organisms choice;choice immune determined;deciding survival long;immune determined;organisms choice;lived organisms depends;choice immune crucial;organisms depends;factor deciding survival;depends critically fitness;lived healthy organisms;survival life survival;critically fitness choice;long lived organisms;life survival;immune crucial factor;healthy organisms;choice immune;survival life;immune crucial;life survival long;immune determined number", "pdf_keywords": ""}, "59d487d6ef839c82ae128550e35fa44058b03d37": {"ta_keywords": "einstein condensate magnetic;evolution condensate magnetic;condensate magnetic field;bose einstein condensate;condensate magnetic;interaction condensate magnetic;evolution bose einstein;einstein condensate;model evolution bose;magnetic field model;evolution bose;bose einstein;study evolution condensate;evolution condensate;magnetic field;magnetic;interaction condensate;condensate;based interaction condensate;field model applied;bose;model evolution;field model solved;field model;gaussian;gaussian xmath0;field model based;present model evolution;gaussian xmath0 approximation;einstein", "pdf_keywords": ""}, "60ce57713261b41fe2e3d222f1d4530c4fc69241": {"ta_keywords": "probabilistic serial ps;probabilistic serial;computational complexity agent;manipulating probabilistic serial;complexity agent manipulating;agent manipulating probabilistic;complexity agent;computational complexity;polynomial time algorithm;probabilistic;polynomial time;expected utility best;complexity;manipulating probabilistic;study computational complexity;ps rule computing;serial ps rule;computing expected utility;sequential allocation discrete;sequential allocation;expected utility better;response expected utility;allocation discrete objects;utility better response;rule computing expected;expected utility;utility best response;rule computing;allocation discrete;computed polynomial time", "pdf_keywords": "agent manipulating probabilistic;agents expected utility;random games allocation;allocations expected utility;agent allocation;mechanism random assignment;random allocations;relations random allocations;response agent allocation;expected utility best;agents based indifferences;stochastic dominance;agents utility rule;agent make optimal;random assignment objects;rule maximized agents;games allocation;expected utility better;complexity agent manipulating;computing expected utility;utility polynomial time;mechanism assignment agents;random allocations expected;expected utility;games allocation possible;sequence random games;agents utility;maximized agents;manipulating probabilistic;assignment agents objects"}, "74d8a998269bcdd087a21840b0e28d86c256c121": {"ta_keywords": "preference models;preference models obtain;class preference models;partial learning algorithm;known learning bounds;partial learning;learning bounds provide;learning bounds;learning algorithm;learning algorithm able;preference;novel class preference;study partial learning;known learning;class preference;previously known learning;exponential improvement;obtain exponential improvement;learning;exponential improvement previously;algorithm;performance algorithm;models obtain exponential;algorithm able significantly;algorithm able;improve performance algorithm;partial;case study partial;models;models obtain", "pdf_keywords": "learning preferences agent;choice discounted utility;intertemporal choice discounted;models intertemporal choice;learning utility;learning preferences;models choice time;discounted utility model;learning preference;general preference models;learning utility functions;detect optimal choice;preference models;preference models defined;preferences consistent learning;preference models provide;learning economic parameters;learnable polynomial time;problem learning indifference;preference models particular;problem learning preference;learning indifference;problem learning preferences;problem learning utility;choice discounted;learning distribution preference;discounting theorems;utility model;preference model;hyperbolic discounting theorems"}, "c994372b3c33bbc1ad6b504c5efb5afd515a5009": {"ta_keywords": "target speech extraction;speech extraction;approach target speech;target speech;speech recordings;based mixture speaker;speech extraction recognition;available speech recordings;speaker identity loss;recognition weak supervision;speech recordings proposed;supervision based mixture;mixture speaker characteristics;mixture speaker;extraction recognition weak;weak supervision based;available speech;publicly available speech;loss mixture consistency;speaker identity;speaker characteristics proposed;identity loss mixture;parts speaker identity;consists parts speaker;speech;recognition weak;recordings proposed loss;loss mixture;parts speaker;mixture consistency loss", "pdf_keywords": ""}, "09d88e0bb8863fd402030aeb625c52c0492c4fef": {"ta_keywords": "blind reverberation deep;recurrent noising auto;reverberation deep recurrent;auto encoder dae;robust automatic speech;deep recurrent noising;noising auto encoder;speech recognition;automatic speech recognition;speech recognition systems;encoder dae;recurrent noising;automatic speech;encoder dae early;auto encoder;reverberation deep;blind reverberation;systems blind reverberation;noising auto;recognition systems blind;dae early fusion;encoder;deep recurrent;implemented challenge 2014;reverberation;systems blind;challenge 2014 version;fusion;early fusion;noising", "pdf_keywords": ""}, "320278b24a3c53a44f95e8ef5465bebe56f24225": {"ta_keywords": "dependency parsing pause;parsing pause prediction;parsing pause;annotated pauses;explicitly annotated pauses;pauses main information;joint dependency parsing;pause prediction;dependency parsing;annotated pauses resulting;pause prediction proposed;considers pauses;annotated syntactic information;considers pauses main;annotated syntactic;pause proposed;pauses;pause;explicitly annotated syntactic;syntactic information;parsing;model considers pauses;pause proposed model;pauses main;improvement pause;methods pause proposed;pauses resulting;pause measure;improvement pause measure;methods pause", "pdf_keywords": ""}, "568462ab0a0a59a2575b70db2cd9022572526f3f": {"ta_keywords": "learning dynamics;zero sum games;stackelberg game;dynamics learning optimization;games using deterministic;general sum games;learning dynamics emulating;stackelberg game using;structure stackelberg game;games zero sum;sum games zero;games using theorem;dynamics learning;learning optimization landscape;sum games;learning optimization;sum games using;based learning dynamics;study dynamics learning;optimization landscape zero;deterministic stochastic updates;deterministic stochastic;analysis deterministic stochastic;stochastic updates zero;game using implicit;gradient based learning;updates zero sum;novel gradient;develop novel gradient;analysis deterministic", "pdf_keywords": ""}, "92622a58377a4671b2ba59e8e59b19b0ab5119bb": {"ta_keywords": "partitioning knowledge graphs;ontology aware partitioning;knowledge graph identification;partitioning knowledge;aware partitioning knowledge;scaling knowledge graphs;knowledge graph;knowledge graphs built;knowledge graphs;approach knowledge graph;knowledge graphs powerful;distributional information partitioning;ontology aware;ontology distributional information;information partitioning;information partitioning approach;based ontology aware;ontology distributional;scaling knowledge;leverages ontology distributional;aware partitioning;identification leverages ontology;ontology;graph identification leverages;leverages ontology;millions extractions approach;partitioning approach;based ontology;approach based ontology;partitioning approach based", "pdf_keywords": ""}, "2ab481028dda04197283c03115bb5f46f5998cc3": {"ta_keywords": "adjoint dirac equation;inverse beta functions;dirac equation;dirac equation solutions;potentials inverse beta;dirac equation considered;inverse beta function;self adjoint dirac;potentials inverse;beta functions self;beta function self;problem inverse beta;adjoint dirac;beta functions;values inverse beta;functions self adjoint;inverse beta;function self adjoint;beta function method;class potentials inverse;beta function;beta function problem;function problem inverse;problem inverse;framework inverse beta;dirac;solved framework inverse;method solution obtained;self adjoint;obtained class potentials", "pdf_keywords": ""}, "582089a00a6c9fb534f16d1dbbafc50cc4e3912a": {"ta_keywords": "providing semantic query;semantic query queries;using semantic query;queryd using semantic;semantic query query;semantic query;queries query domain;query domain specific;domain specific information;query domain;using semantic;providing semantic;query queries;semantic;query queries query;domain specific;specific domain specific;problem providing semantic;queryd using;queries;domain specific domain;information query queryd;queries query;specific information query;query queryd using;information query;query queryd;specific domain;query query queryd;query query", "pdf_keywords": ""}, "1adadbfa95e43a70fcd17e6ce947a0652b86bfc3": {"ta_keywords": "", "pdf_keywords": "crawled corpora;crawled corpora large;web scale corpora;clean crawled corpora;similarity internet archive;corpora scrape internet;creating large corpora;search corpora reasonable;creation large corpora;crawled;unstructured text corpora;search corpora;utilizes similarity internet;corpora search;report large corpus;large corpora;similarity internet;large corpora language;internet dataset;corpora search corpora;large corpus;search corpora language;unstructured data significantly;internet archive common;corpora large unlabeled;internet archive;corpora scrape;unstructured text ofwe;tothe search corpora;corpora used machine"}, "90129b0733ac48ead26b7c86e8b4df917568e208": {"ta_keywords": "particles probability density;probability density states;density states background;states background field;particles probability;density states presence;background field method;density states computed;background field;background field finite;number particles probability;calculation probability density;density states;probability density;case background field;distribution probability density;presence background field;states presence background;states background;calculation probability;applied case background;computed set particles;distribution probability;background;particles;number particles;set particles;analysis distribution probability;case background;method calculation probability", "pdf_keywords": ""}, "02d98ca8f4ecd1a2b885d6867f4c1407ae8d1007": {"ta_keywords": "supervision semantic parsers;semantic parsers trained;semantic parsers learnt;semantic parsers;semantic parsers experiments;performance semantic parsers;extra supervision semantic;supervision semantic;parsers trained;parsers trained weak;parsers learnt;query extra supervision;parsers experiments wiki;parsers;weakly supervised;weakly supervised baseline;parsers experiments;parsers learnt cold;wiki database method;improves performance semantic;wiki database;experiments wiki database;trained weak supervision;improve performance semantic;selecting examples query;weak supervision;examples query extra;art weakly supervised;semantic;selecting examples", "pdf_keywords": "semantic parsers trained;supervision semantic parsing;finallyweakly supervised parsing;parsers trained weakly;semantic parser weak;weak supervision semantic;parser weak supervision;train semantic parser;supervised semantic parser;supervised parsing promising;learning semantic parsing;weakly supervised semantic;learning semantic parsers;supervised semantic parsers;semantic parsing annotating;supervised parsing;neural semantic parser;candidate semantic parsers;semantic parsing small;semantic parser perform;semantic parsing;performance semantic parsers;semantic parsing study;performance semantic parsing;neural semantic parsing;semantic parser small;create semantic parsing;semantic parser;semantic parsing datasets;semantic parsing model"}, "601398838250a4e69c69cc339d65f5c51e727ad1": {"ta_keywords": "questions semantic role;semantic roles produces;semantic roles;semantic role;meaningful questions semantic;ontology predicates roles;contextually meaningful questions;questions semantic;semantic role model;semantic role revise;class semantic roles;predicates roles;generation contextually meaningful;ontology predicates;contextually meaningful;semantic;generation contextually;broad coverage ontology;model generation contextually;ontology;coverage ontology predicates;set contextually meaningful;coverage ontology;contextually;meaningful questions;large class semantic;class semantic;set contextually;diverse formed questions;predicates", "pdf_keywords": "contextualize questions semantic;questions semantic role;questions semantic;semantics natural language;introduce contextualize questions;contextualize questions;coverage ontology relations;semantic relations;natural language questions;corpus semantic roles;contextually appropriate questions;semantic relations bringin;predicate semantic role;semantic role predicate;semantic roles predicate;semantic role ontology;broad coverage ontology;targeting predicate semantic;questions expressed context;semantic role context;questions annotators;ontology relations;structure natural language;possible semantic roles;semantic roles using;prototype questions semantic;generate additional questions;predicate semantic;additional questions annotators;corpus semantic"}, "c783bc02f5f901e4604eb3b0d504a036369afd91": {"ta_keywords": "shape perovskite layer;perovskite layer;perovskite layer long;effect shape perovskite;shape perovskite;perovskite;layer characterized xmath0;layer long range;layer characterized;layer long;layer;xmath3 symmetry;characterized xmath0 symmetry;symmetry xmath3 symmetry;symmetry xmath3;xmath3 symmetry xmath4;xmath2 symmetry xmath3;xmath2 symmetry;range behavior layer;symmetry xmath2;symmetry xmath2 symmetry;xmath7 symmetry;xmath4 symmetry;xmath1 symmetry;symmetry xmath4;symmetry xmath7 symmetry;symmetry xmath7;symmetry xmath1 symmetry;xmath6 symmetry xmath7;symmetry xmath4 symmetry", "pdf_keywords": ""}, "a3452276ada37727d0008dad8ca7c27bbbee6984": {"ta_keywords": "rumor detection;rumor detection performed;discovering rumor social;characterizing discovering rumor;discovering rumor;claim rumor detection;rumor social media;steps rumor detection;rumor social;claim rumor;target claim rumor;embedding conversation thread;embedding conversation;social media;social media approach;steps rumor;conversation thread aggregating;following steps rumor;performed embedding conversation;rumor;conversation thread waveguide;user responses infer;detection performed embedding;infer target claim;method characterizing discovering;characterizing discovering;representation conversation thread;gaussian representation conversation;responses infer target;infer target", "pdf_keywords": "networks rumor detection;rumor detection task;rumor detection;rumor detection networks;rumor propagation graph;rumor propagation;interactions rumor detection;rumor detection arumor;learns rumor representations;features rumor propagation;modeling rumor propagation;discover rumor detection;predict rumor detection;rumor detection model;rumor detection crucial;attention networks rumor;rumor detection based;hierarchically rumor detection;novel rumor detection;rumor detection proposed;rumor propagation graphwe;rumor representations neural;rumor detection robust;detect rumor;rumor detection results;rumor detection accuracy;rumor propagation explicit;deal rumor propagation;model rumor detection;rumor detection method"}, "19be8dd52d949fed1a3e5aca7630669da2575d73": {"ta_keywords": "stability probability matching;probability matching agents;probability matching;uncertainty lottery model;models uncertainty lottery;uncertainty lottery;probability model stability;indifference model stability;matching agents;probability matching significant;matching agents having;matching;lottery model compact;indifference model joint;indifference model;model stability probability;stability probability;agents having preferences;order indifference model;lottery model;compact indifference model;joint probability model;computing stability probability;consider models uncertainty;probability model;models uncertainty;model joint probability;model compact indifference;uncertainty;joint probability", "pdf_keywords": ""}, "35ee53492c7f32dbb3b4ed7ba4d1395218b13ee9": {"ta_keywords": "everyday algorithm auditing;everyday algorithm users;algorithm auditing;auditing behaviors;facilitate auditing behaviors;auditing behaviors finally;algorithm users;problematic machine behaviors;tools facilitate auditing;facilitate auditing;algorithm users surfacing;algorithm auditing drawing;auditing;auditing regardless users;everyday algorithm;machine behaviors elude;cases everyday algorithm;underlying algorithms analyze;users surfacing problematic;everyday users;auditing regardless;everyday users powerful;machine behaviors argue;machine behaviors;problematic machine;users powerful surfacing;users surfacing;organized forms auditing;underlying algorithms;auditing drawing", "pdf_keywords": "everyday algorithm auditing;algorithm auditing everyday;everyday algorithm audits;algorithm audits everyday;everyday algorithm audit;algorithm auditing users;everyday auditing behaviors;everyday algorithm users;algorithmic behaviors everyday;algorithm audit users;harmful algorithmic behaviors;audits algorithmic systems;facilitating everyday algorithm;algorithm audits algorithmic;algorithm auditing;algorithm auditing popular;algorithm auditing efforts;algorithm auditing practices;algorithm auditing discuss;algorithmic behaviors day;audits algorithmic;algorithm audits;formal algorithm auditing;existing algorithm auditing;everyday algorithmic;algorithm audits focusing;algorithmic systems everyday;algorithm auditing approaches;algorithm auditing characterizing;algorithm auditing viewed"}, "1f38ba33063f118f574cf57ff9f1a0e7de2857ff": {"ta_keywords": "semantic similarity shared;semantic similarity evaluation;semantic similarity ssep;semantic similarity;known semantic similarity;similarity shared task;approach semantic similarity;similarity representations shared;evaluate similarity representations;framework semantic similarity;similarity evaluation sssep;similarity shared;similarity representations;similarity evaluation;similarity set representations;challenge evaluate similarity;similarity ssep;similarity ssep challenge;evaluate similarity;similarity;evaluate similarity set;representations shared task;similarity set;language known semantic;known semantic;task framework semantic;semantic;novel approach semantic;representations language;representations shared", "pdf_keywords": "russian semantic similarity;evaluating semantic similarity;evaluation semantic similarity;semantic similarity evaluation;semantic similarity measures;benchmark semantic similarity;semantic similarity;semantic similarity similarity;word relation similarity;similarity semantic similarity;similarity measures language;semantic similarity present;overview russian semantic;similarity semantic;semantic similarity set;semantic similarity held;russian semantic;dialogue semantic similarity;determined similarity semantic;semantically similar words;semantically similar;judgements semantic similarity;semantic similarity scalable;compute semantic relatedness;assess semantic relation;semantic similarity extraction;relation semantically similar;semantic relation classification;semantic similarity context;classification relation similarity"}, "857036a25401c19e484cc32d974c90cd9a46cd66": {"ta_keywords": "local nash equilibria;nash equilibria noncooperative;nash equilibria games;computing local nash;continuous games conditions;equilibria games played;equilibria games;noncooperative continuous games;nash equilibria;constitute local nash;local nash;equilibria noncooperative continuous;continuous games;played finite dimensional;games played finite;equilibria noncooperative;games conditions;dimensional space equilibria;player strategies constitute;games conditions based;player strategies;ensuring player strategies;number players game;conditions ensuring player;played finite;space equilibria obtained;space equilibria;strategies constitute local;game number players;players game", "pdf_keywords": ""}, "d473ff103565d8c76e0cbfa33bdd4b0db1cbb23f": {"ta_keywords": "equilibrium computation games;symmetric shot games;evolution strategies nes;evolution strategies;equilibria finite strategy;natural evolution strategies;strategies mixed equilibrium;games approximate;strategy generation;finite strategy;games using natural;incremental strategy generation;shot games approximate;computation games;strategies function;finite strategy set;games approximate best;computation games using;strategy generation framework;strategies function space;strategies nes;search strategies function;strategies nes formulate;incremental strategy;equilibrium computation;strategy set;strategies mixed;mixed equilibrium computation;strategy;approximation compute equilibria", "pdf_keywords": "generalization symmetric game;deep strategies games;learning equilibrium game;shot probabilistic games;symmetric games method;symmetric games;symmetric game;symmetric symmetric games;information symmetric games;symmetric game theory;deep game model;probabilistic games;game symmetric;learning deep game;games multiple symmetric;game symmetric symmetric;learning symmetric;learning payoffs games;symmetric games bb84;symmetric sbg games;learning deep strategies;optimal strategies learning;symmetric version game;probabilistic games sbgs;semiclassical approximation game;symmetric shot probabilistic;learning equilibrium;stochastic evolution neural;symmetric players;evolution strategies"}, "ecd2b355f250abfd4eb9d6c7c598c33c7cd6bcb0": {"ta_keywords": "noisy crowdsourced labels;crowdsourced labels;crowdsourced labels principle;truth noisy crowdsourced;probabilistic labeling;probabilistic labeling model;noisy crowdsourced;crowdsourcing datasets binary;crowdsourcing datasets;novel probabilistic labeling;real crowdsourcing datasets;crowdsourcing;real crowdsourcing;variety real crowdsourcing;crowdsourced;labeling model jointly;labeling;labeling model;labels principle jointly;gather noisy labels;noisy labels provides;conditional entropy;probabilistic;novel conditional entropy;labels principle;conditional entropy principle;provides novel probabilistic;labels;noisy labels;novel probabilistic", "pdf_keywords": ""}, "0fc01a915cc7bf7025f80d44f805bd54b6425a33": {"ta_keywords": "nonintrusive load monitoring;general nonintrusive load;load monitoring algorithm;power consumption signal;nonintrusive load;load monitoring;aggregate power consumption;power consumption;consumption signal;consumption signal function;monitoring algorithm;noise device usage;monitoring algorithm analyze;general nonintrusive;monitoring;behavior general nonintrusive;measurement noise;measurement noise device;nonintrusive;device usage;signal function measurement;function measurement noise;isintrusive;bounds probability distinguishing;error isintrusive;aggregate power;given aggregate power;noise device;power;case error isintrusive", "pdf_keywords": "nonintrusive load monitoring;analyzing power consumption;characterizing power consumption;load monitoring;power consumption building;power consumption signal;load monitoring nilm;aggregate power consumption;power consumption;consumption signal aggregate;load monitoring np;consumption signal formulate;consumption signal;household appliances analyze;analyzing power;consumption signal present;general nonintrusive load;appliances analyze;algorithm characterizing power;appliances analyze tradeoff;monitoring nilm algorithms;monitoring;nonintrusive load;modeling measurement;monitoring nilm;limits nonintrusive load;framework analyzing power;detect noise;measurement error sampling;monitoring np algorithm"}, "16457c13a40aa589fa06d8533a47b3f96aede474": {"ta_keywords": "moving agents medium;agents medium moving;agents moving medium;determined moving agents;action moving agents;moving moving agents;moving agents moving;medium moving agents;moving agents acting;agents acting moving;agents moving moving;acting moving medium;agents medium metastability;moving agents;moving medium action;medium action moving;agents moving;action moving moving;determined action moving;medium determined moving;moving moving medium;consider action moving;action moving;medium determined action;acting moving;medium moving;moving medium determined;determined moving;agents acting;moving moving", "pdf_keywords": ""}, "639cc01afcc1c78063f7a6bbdae998cd147911c4": {"ta_keywords": "robust utility learning;correlated utility learning;utility learning framework;novel utility learning;utility learning;utility learning scheme;approximated correlations players;robust parametric inference;robust utility;novel correlated utility;correlated utility;bagging gradient boosting;correlations players;game noncooperative players;boosting estimate noise;extends robust utility;gradient boosting estimate;correlations players leverage;gradient boosting;employs robust parametric;novel utility;noncooperative players;learning scheme;user interaction game;learning framework;bootstrapping bagging gradient;propose novel utility;noise covariance provides;boosting estimate;interaction game noncooperative", "pdf_keywords": "robust utility learning;correlated utility learning;formulate utility learning;parametric utility learning;utility learning;utility learning scheme;utility learning methods;utility learning framework;learning using heteroskedastic;game utility learning;prediction game utility;learning framework robust;learning utility;utility learning method;correlated game forecasting;problem utility learning;utility learning noisy;utility learning allows;utility utility learning;utility learning problem;utility learning learn;utility learning approach;propose utility learning;learning utility empirical;utility learning based;utility learning incentive;estimation heteroskedastic inference;utility learning performance;utility learning using;based utility learning"}, "03d2af05e41aac58ebae4ab37f09155e53d4b35b": {"ta_keywords": "recovering matrix completion;matrix matrix completion;matrix completion;matrix completion data;matrix completion simply;theory matrix completion;matrix completion extreme;noisy bit observations;recovering matrix;accurate estimate matrix;estimate matrix matrix;instead recovering matrix;estimate matrix;bit observations;completion data;bit observations suitable;completion data instead;bit measurements eliminate;generating bit measurements;matrix;matrix matrix;observations suitable constraint;observations;completion extreme case;completion simply;completion;case noisy bit;theory matrix;noisy bit;bit measurements", "pdf_keywords": "bit matrix completion;matrix recovery;estimation matrix completion;matrix completion;bounds matrix completion;recovered observed matrix;matrix completion algorithm;matrix completion powerful;matrix completion accurately;matrix completion problem;recovered matrix binary;matrix completion task;matrix completion matrix;matrix recovered convex;reconstructing random matrix;recovering noisy matrix;completion matrix completion;regression matrix completion;recovered matrix bound;matrix completion rate;recovery noisy matrix;completion matrix;original matrix recovery;rank matrix recovered;recover matrix distribution;matrix recovery noisy;recovered convex programming;matrix completion incomplete;size recovered matrix;observations probabilistically matrix"}, "3bfa1fe0a8d031d59dfc0cfa4975c296951ee56c": {"ta_keywords": "recognition complex signals;recognize complex signals;complex signals living;recognize recognize complex;recognition complex;complex signals;signals living systems;recognize complex;recognition speech;recognition speech recognition;speech recognition;present recognition complex;speech recognition speech;speech recognition able;combination speech recognition;living systems based;living systems;complex signals variety;recognition able recognize;signals living;recognition;approach recognition complex;resulting recognition able;recognize recognize;able recognize recognize;domains resulting recognition;resulting recognition;recognition able;signals;present recognition", "pdf_keywords": ""}, "8873d1369590249113e1f0491ce49d1502395b9c": {"ta_keywords": "video text compliance;text compliance;video compliance;video text;trainable compliance network;given video compliance;video compliance associated;trainable compliance;introduce video text;compliance associated text;text compliance vtc;videos atomic activities;text compliance problem;activities natural language;compliance network improves;videos atomic;compliance vtc dataset;end trainable compliance;new video text;compliance network;compliance;videos;activity given video;dataset collection videos;text;instruction introduce video;associated text instruction;compliance associated;collection videos atomic;text instruction", "pdf_keywords": ""}, "7488429131b8970425a66f3410920d98ff6e9c36": {"ta_keywords": "debiasing method adaptive;adaptive regularization;method adaptive regularization;regularization given evaluation;adaptive regularization required;debiasing method;propose debiasing method;regularization;choose regularization;regularization required choose;choose regularization given;regularization required;debiasing;required choose regularization;propose debiasing;regularization given;algorithm experimental evaluations;algorithm experimental;guarantees performance algorithm;method adaptive;algorithm;performance algorithm;performance algorithm experimental;adaptive;evaluations;guarantees performance;theoretical guarantees performance;experimental evaluations;evaluation;given evaluation", "pdf_keywords": "students bias evaluations;bias semiparametric;estimating bias semiparametric;assigning student bias;bias evaluations;estimating bias training;biased evaluations;bias cross validation;bias semiparametric model;assigning bias course;bias evaluations biased;biased evaluations work;estimators graded data;grading data estimator;bias training;bias optimal;bias course given;bias algorithm training;bias induced grading;bias course bound;estimators graded;bias validation;bias jointly;student review algorithm;minimizing bias;evaluations biased evaluations;minimizing bias cross;objective minimizing bias;bias student author;semiparametric model training"}, "579e01c3c864cc98e57c728f84fcf553c5b1bcba": {"ta_keywords": "visibility xmath0 parity;xmath0 parity talk;visibility xmath0;enhancement visibility xmath0;xmath0 parity;enhance visibility talk;visibility talk proposed;parity talk;parity talk given;visibility talk;fluid dynamics;fluid dynamics aps;xmath0;fluid;division fluid;division fluid dynamics;talk proposed framework;visibility;society division fluid;enhancement visibility;parity;enhance visibility;talk proposed;used enhance visibility;talk given;meeting american physical;american physical;framework enhancement visibility;talk used enhance;dynamics", "pdf_keywords": ""}, "4f1eef4acaf0164593b9e654dba4b8cd3e72421d": {"ta_keywords": "stacked graphical learning;graphical learning meta;datasets stacked graphical;datasets stacked;stacked graphical;meta learning scheme;propose stacked graphical;graphical learning;meta learning;experiments datasets stacked;analysis stacked graphical;learning meta learning;analysis stacked;graphical learning average;graphical learning able;learning meta;stacked;learning scheme;base learner;learning scheme base;learning;theoretical analysis stacked;predictions related instances;instance features predictions;features predictions related;propose stacked;features predictions;datasets;learning average time;base learner augmented", "pdf_keywords": ""}, "31884a623af77136413d997049b5787b394db461": {"ta_keywords": "quantum state estimation;estimation parameters quantum;parameters quantum state;quantum state wave;evolution quantum state;state wave function;quantum state based;quantum information processing;quantum state;parameters quantum;quantum information;analysis evolution quantum;information processing quantum;processing quantum state;state estimation;evolution quantum;quantum state key;quantity quantum information;processing quantum;state estimation problem;state wave;wave function amplitude;quantum;evolution wave function;wave function;estimation parameters;analysis evolution wave;function amplitude phase;wave function method;quantity quantum", "pdf_keywords": ""}, "2818bd090206ef33f9d7e1be03bc4f742c6762d1": {"ta_keywords": "word agglutinative language;agglutinative language word;word sequence agglutinative;language word agglutinative;sequence agglutinative language;agglutinative language;agglutinative language agglutinative;agglutinative language stem;language agglutinative language;language agglutinative;word agglutinative;structure word sequence;word suffix structure;suffix structure word;suffixes used language;stem word suffix;language stem word;language structure word;word suffix;sequence agglutinative;word sequence determined;agglutinative;suffix structure;word sequence;suffixes;suffixes used;language stem;suffix;language word;language structure", "pdf_keywords": ""}, "8f99f9409f254134aa32fbf072475100f688d613": {"ta_keywords": "codes distributed storage;secrecy capacity codes;storage network code;distributed storage network;distributed storage networks;codes distributed;distributed storage;storage networks achieve;storage networks;theoretic secrecy capacity;capacity codes based;generated distributed storage;class codes distributed;secrecy capacity;storage network;information theoretic secrecy;capacity codes;able distribute data;codes based lv;distributed;network code;network code capable;theoretic secrecy;transmitting data random;code capable transmitting;distribute data;storage;codes based;secrecy;generated distributed", "pdf_keywords": "secure distributed storage;secure codes distributed;codes distributed storage;constructions secure codes;theoretic secrecy capacity;secure codes;information theoretic secrecy;storage systems codes;theoretically secure codes;codes storage;nodes distributed storage;storage codes storage;codes storage data;storage networks;distributed storage data;storage nodes distributed;data storage nodes;existence secure distributed;secure distributed;distributed storage;considers distributed storage;distributed storage networks;storage codes;ofdistributed distributed storage;storage nodes;storage presented distributed;storage network nodes;secrecy capacity;optimal message storage;message storage codes"}, "b130b6387b105ecd9b4718b179b1e128157f9516": {"ta_keywords": "mean field model;mean mean field;mean field mean;mean field;field mean field;estimation mean field;field mean mean;parametric mean field;field mean;field model estimation;mean mean mean;model estimation mean;new parametric mean;mean mean;parametric mean;estimation mean;field model;mean;field;model estimation;estimation;introduce new parametric;new parametric;parametric;model;introduce;new;introduce new", "pdf_keywords": ""}, "191543c7cb084d3af6a48ae771ca3dfd0588ab22": {"ta_keywords": "recommendation text model;review text recommendation;text recommendation text;recommendation text;recommendation tasks;transform review text;review text;learns transform review;text recommendation;transnets learns transform;transnets learns;learning model deep;called transnets learns;deep learning introducing;extends deep learning;model deep learning;deep learning;deep learning model;model deep;transnets;text model;art recommendation tasks;model called transnets;called transnets;review;learns transform;latent layer;additional latent layer;text;latent layer representing", "pdf_keywords": "predicting ratings fiber;rating prediction deep;predicting rating recommender;recommender using neural;predicting rating;predicting ratings;rating prediction;method predicting rating;predicting quality recommendation;method predicting ratings;model predicting ratings;method rating prediction;predict ratings;rating recommender;rating recommender using;predicting ratings popular;predict ratings popular;ratings fiber;predicting rating book;user reviews predicted;predicted ratings;reviews predicted ratings;ratings fiber optic;predicted ratings likely;network predict;network predict user;quality recommendation item;recommendation item uses;recommendation item;neural net"}, "c72cdb5ce7e0911c7f442ab503652d6fdeef35e0": {"ta_keywords": "efficacy different models;comparative study efficacy;efficacy;efficacy different;study efficacy different;study efficacy;different models;models;present comparative study;comparative study;study;comparative;present comparative;different;present", "pdf_keywords": "grounded semantic parsing;semantic parsing task;parsers downstream semantic;parser sentence entities;supervised parsers downstream;semantic parsing;supervised parser sentence;supervised parsers unsupervised;unsupervised parsers trained;weakly supervised parsers;supervised parsers;fully supervised parser;languages supervised parsers;parsing sentences;supervised parser;task parsing sentences;unsupervised parsers;supervised parsers available;comparison supervised parsers;parsers unsupervised weakly;parsing sentences present;parsers trained;parsing sentences based;natural language processing;semi unsupervised parsers;parsers downstream;filling task parsing;parsers trained simple;parsers unsupervised;syntactic semantic performance"}, "61d2dda8d96a10a714636475c7589bd149bda053": {"ta_keywords": "captioning source code;source code captioning;code captioning;image captioning source;captioning source;image captioning;encoder decoders tasks;encoder decoder;novel encoder decoder;encoder decoders;tasks image captioning;encoder decoder framework;captioning;encoder;propose novel encoder;novel encoder;improves conventional encoder;decoders tasks image;conventional encoder decoders;decoder;decoder framework;decoders tasks;review network;conventional encoder;decoders;source code;called review network;decoder framework called;review network framework;review", "pdf_keywords": ""}, "12239e761e8c7cd05e12e18f43dba7b46dfd8ac1": {"ta_keywords": "neural machine translation;translation single source;multi source neural;accuracy translation single;source neural machine;optimization translation accuracy;machine translation nmt;accuracy translation;machine translation;translation accuracy;translation accuracy proposed;parallel optimization translation;translation accuracy xmath0;obtain translation accuracy;augmentation data multi;source neural;comparing accuracy translation;optimization translation;translation single;nmt obtain translation;translation nmt obtain;approach augmentation data;translation nmt use;augmentation data;data multi source;single source language;translation nmt;neural machine;obtain translation;multi source", "pdf_keywords": "nmt machine translation;augmentation incomplete multilingual;machine translation nmt;neural machine translation;translation nmt multi;nmt model translation;machine translation model;translation machine;machine translation;translation translation machine;multiple sources translate;methods multilingual translation;sources translate multiple;method multilingual translation;translation model;translation model monolingual;discovery minimum translation;set translation machine;target language augment;translations source language;model translation;translation machine based;multilingual translation;machine translation uses;incomplete multilingual;translation nmt;translation approach multilingual;extension translation approach;multi source neural;source neural machine"}, "d2a2be6ce932a0f1939f31cfff4d64ea3d76723d": {"ta_keywords": "humanlike uncertainty dataset;humanlike uncertainty improvement;uncertainty dataset robust;non humanlike uncertainty;robust suitable training;benchmark dataset deep;humanlike uncertainty;uncertainty improvement classification;uncertainty dataset;dataset deep neural;deep neural;exhibits humanlike uncertainty;deep neural networks;training non humanlike;dataset deep;dataset robust;neural networks largely;neural networks exhibits;dataset robust suitable;generation deep neural;new benchmark dataset;uncertainty improvement;neural networks;benchmark dataset;explicit training model;new benchmark;uncertainty;neural;explicit training;benchmark", "pdf_keywords": "cnn classifiers;crowdsourced human categorization;classification human judgments;trained soft labels;cnn classifiers trained;accurate categorization images;cnns trained;labels image classification;image classification training;deep learning label;accurate categorization;empirical classification human;soft labels training;cnns trained soft;learning label noise;learning label;classification training;method accurate categorization;categorization images;classifiers trained;human categorization;state art cnn;distribution human labels;human categorization judgments;classification human;testbed evaluation classification;classification benchmark;image classification;categorization images using;cnns"}, "1668b0b9cc631cdfc0dfaf77b71627f5524a866c": {"ta_keywords": "effective mass particle;mass particle quantum;mass particle field;particle quantum field;particle field theory;quantum field theory;particle classical field;calculation effective mass;spectral function particle;calculating effective mass;particle field;quantum field;mass particle;classical field theory;field theory method;calculation spectral function;calculation spectral;applied calculation spectral;used calculation spectral;effective mass;particle quantum;particle classical;function particle classical;field theory used;function particle quantum;field theory;theory used calculation;classical field;spectral function;method calculating effective", "pdf_keywords": "stochastic convex optimization;optimization stochastic convex;stochastic optimization directional;smooth stochastic optimization;stochastic optimization optimization;equivalent stochastic optimization;consider stochastic optimization;optimization smooth stochastic;stochastic optimization;stochastic stochastic optimization;stochastic optimization powerful;linear stochastic optimization;stochastic optimization problems;consider stochastic gradient;stochastic approximation directional;optimization stochastic;stochastic gradient;smooth stochastic convex;stochastic gradient methods;noisy stochastic approximation;methods stochastic optimization;computation stochastic gradient;stochastic convex functions;stochastic gradient method;stochastic convex;optimization problems stochastic;case stochastic convex;stochastic optimization problem;minimization function stochastically;class stochastic optimization"}, "3e398bad2d8636491a1034cc938a5e024c7aa881": {"ta_keywords": "convolution free pyramids;pyramid free convolution;pyramids based vision;vision transformer;big bang vision;proposed vision transformer;free pyramids based;vision transformer vit;based vision transformer;based pyramids free;vision transformer pvt;free pyramids;pyramids free;introduce pyramid free;pyramid free;pyramids based;pyramids based pyramids;free convolution free;introduce pyramid;pyramids;versatile pyramids based;pyramid;based pyramids;convolution free;based pyramids based;specifically introduce pyramid;free convolution;versatile pyramids;image classification specifically;bang vision", "pdf_keywords": "cnn pyramids;alternative cnn backbones;conventional cnn backbone;cnn backbones;cnn backbone;cnn backbone dense;using cnn pyramids;based cnn backbone;cnn backbones resnet;cnn pyramids called;conventional cnn;alternative cnn;pvt alternative cnn;cnn backbone model;object detection deep;pvt based cnn;resnet dense prediction;cnn;cnn novel approach;using cnn;object detection pipeline;detection deep;detection deep convolutional;pyramid vision;based cnn;prediction tasks pyramid;dense object detection;gap conventional cnn;cnn novel;dense prediction tasks"}, "115f1366318e7622f89f3a870e5863282670b1ad": {"ta_keywords": "kidney injury iqi;operative statin therapy;statin therapy;statins leading drug;kidney injury aki;acute kidney injury;pre operative statin;statin therapy independent;statins;statin;treatment acute kidney;operative statin;kidney injury;acute kidney;protective factor iqi;injury iqi limited;statins leading;injury iqi;factor iqi affected;injury aki use;cholesterol levels report;high cholesterol levels;high cholesterol;cholesterol levels;cholesterol;iqi affected;use treatment acute;iqi affected type;injury aki;treatment acute", "pdf_keywords": ""}, "d516daff247f7157fccde6649ace91d969cd1973": {"ta_keywords": "prediction machine shape;predict shape machine;able predict shape;predicting shape given;predicting shape;predict shape;shape given machine;prediction machine;machine shape model;machine shape;algorithm allows prediction;shape machine;problem predicting shape;machine learning algorithm;model machine learning;allows prediction machine;supervised learning model;shape model able;machine based supervised;shape model;learning model machine;model able predict;shape machine output;machine learning;hoc_ supervised learning;learning algorithm allows;learning algorithm;supervised;supervised learning;predicting", "pdf_keywords": "machine learning interpretable;learning interpretable;models interpretable;underlying interpretability;interpretable models;property interpretable models;interpretable models propose;underlying interpretability finding;interpretability identifying;interpretability;motivations underlying interpretability;linear models interpretable;intrinsically interpretable suggesting;interpretability research;confer interpretability;interpretability finding;interpretability axiomatically;model explain decisions;confer interpretability identifying;interpretable suggesting;interpretability research propose;models strictly interpretable;algorithms intrinsically interpretable;thought confer interpretability;intrinsically interpretable;interpretable suggesting simple;interpretation models important;interpretability finding diverse;papers proclaim interpretability;interpretability identifying transparency"}, "5ba57ff3c3e6e319586b86a990b6e082f4ecf972": {"ta_keywords": "probabilistic models speech;models speech recognition;models speech;speech recognition;accuracy speech recognition;accuracy speech;speech recognition based;speech recognition factor;probabilistic models;improves accuracy speech;probabilistic model;recognition based information;variational version probabilistic;variational versions probabilistic;class probabilistic models;probabilistic models improves;probabilistic;information principle;based information principle;probabilistic model pm;information principle ic;new class probabilistic;class probabilistic;speech;models maximum variance;paper introduces variational;recognition based;model fwm variational;maximum variance model;likelihood approach theoretical", "pdf_keywords": ""}, "e5d143ae82ede67726aa1a9aeac3de4bf53d8920": {"ta_keywords": "knowledge graph embeddings;vision language pretraining;knowledgeaware representations;pretraining approach knowledge;knowledge based vision;aligned knowledgeaware representations;knowledgeaware representations approach;semantically aligned knowledgeaware;enhance learning semantically;knowledge graph;uses knowledge graph;learning semantically;language pretraining;embeddings extracted;learning semantically aligned;graph embeddings;graph embeddings extracted;language pretraining kk;embeddings;image object tags;knowledge;tags enhance learning;pretraining approach;novel pretraining approach;pretraining;aligned knowledgeaware;vision language;object tags enhance;introduce novel pretraining;embeddings extracted text", "pdf_keywords": ""}, "7f52e3914a61994f68583635e43bc1bb9203e3b3": {"ta_keywords": "human risk genetic;risk genetic modifications;risk genetic;study risk genetic;risk lung cancer;reduce risk lung;risk lung;genetic modifications;study human risk;genetic modifications working;genetic;human risk;study risk lung;present study risk;reduce risk;lung cancer study;high study risk;cancer study revealed;lung cancer;study risk;cancer study;risk;lung cancer promising;cancer;working environment high;working environment study;environment study conducted;modifications working environment;cancer promising;working environment", "pdf_keywords": ""}, "60a4ad8e8f4389f317d109550f5da2a571cbb515": {"ta_keywords": "answer large corpus;natural language query;large corpus text;comprehend natural language;large corpus;language query extract;natural language;corpus text;corpus;systems designed comprehend;extract answer large;language query;designed comprehend natural;datasets aimed evaluating;large scale datasets;query extract;query extract answer;designed comprehend;comprehend natural;comprehend;extract answer;datasets;query;datasets aimed;text;language;answer large;new large scale;large scale;scale datasets aimed", "pdf_keywords": "answers construction dataset;large corpus text;dataset reading comprehension;comprehension using unstructured;completeness text comprehension;text comprehension;used answer extraction;answer extraction;comprehension proposed data;machine reading comprehension;text comprehension using;answer extraction proposed;searching large corpus;large corpus;datasets quasar quasar;quasar datasets promoting;datasets quasar;reading comprehension proposed;questions large corpora;corpus text relevant;large corpora quasar;present quasar datasets;context machine reading;quasar opendomain research;corpus text;datasets measure lexical;quasar datasets;quasar new dataset;unstructured text collection;answers construction"}, "15513c732d6af975f312307be3b5e2bd674ac0ef": {"ta_keywords": "word error rate;speech recognition errors;weighted word error;error rate metric;impact speech recognition;recognition errors event;speech recognition;event related brain;error rate;relatedwwe wwer metric;wwer metric evaluated;investigate impact speech;recognition errors;metric based event;wwer metric;related brain potential;impact speech;errors event;errors event related;word error;brain potential;brain potential using;metric evaluated performance;event relatedwwe wwer;weighted word;rate metric;results standard wer;event relatedwwe;speech;rate metric based", "pdf_keywords": ""}, "7261b088c48be7eca10263e765739f7347665481": {"ta_keywords": "spin polarized xmath1;spin polarized xmath0;polarized xmath0 model;dynamics spin polarized;polarized xmath1;polarized xmath0;polarized xmath1 xmath2;spin polarized;single spin polarized;dynamics spin;study dynamics spin;xmath0 model;characterized single spin;magnetic field model;xmath0 model presence;single spin;model presence magnetic;spin;xmath2 xmath3 xmath4;xmath3 xmath4;xmath3 xmath4 xmath5;xmath2 xmath3;xmath8 xmath9 xmath10;xmath7 xmath8 xmath9;xmath1 xmath2 xmath3;xmath1 xmath2;xmath4 xmath5;xmath7 xmath8;xmath4 xmath5 xmath6;xmath6 xmath7 xmath8", "pdf_keywords": "equilibrium routing games;stochastic network equilibrium;stochastic user equilibrium;stochastic game;consider stochastic game;equilibrium routing;network game solve;demand routing games;routing games generalize;wardrop equilibrium routing;routing game;novel network equilibrium;network equilibrium models;network optimal time;network equilibrium;optimal traffic;network equilibrium model;problem optimal traffic;stochastic game players;problem network game;routing games;game routing;algorithms equilibrium problems;theorem equilibrium game;network optimal;multicommodity routing game;game routing games;algorithms equilibrium;equilibrium model players;optimize network optimal"}, "4fb8009422903f7cb6f9a929409264b7fbca55e3": {"ta_keywords": "feature enhancement method;feature enhancement;novel feature enhancement;dimensional signal processing;high dimensional signal;noise feature vectors;corresponding clean features;signal processing;splice algorithm clean;clean features;corrupted noise feature;noise feature;enhancement method;piecewise linear compensation;features enables efficient;dimensional signal;splice algorithm;environments splice algorithm;enhancement method high;learns piecewise linear;clean features enables;feature vectors;piecewise linear transformation;compensation environments splice;signal processing low;known piecewise linear;feature vectors corresponding;features;feature;linear compensation environments", "pdf_keywords": ""}, "359dfdfea38f645d5fa49efc846a3b5ebce317fe": {"ta_keywords": "interpretable machine learning;approach interpretable machine;interpretable machine;interpretable model;true model interpretable;model interpretable;interpretable model real;model interpretable model;novel approach interpretable;interpretable;approach interpretable;machine learning;real world data;model real world;machine learning approach;real world;model real;conjectures tested real;tested real world;real world present;model true;true model;conjectures;model true model;learning;conjectures tested;data;learning approach;assumptions;set conjectures tested", "pdf_keywords": "interpretationsable machine learning;interpretable machine learning;machine learning stakeholders;interpretable models;definitions interpretable machine;interpretable machine;machine learning accompanied;interpretable models met;models demand interpretable;interpretationsable machine;definitions interpretable;interpretable;interpretations rise interpretable;machine learning literature;interpretationsable;demand interpretable models;machine learning ask;supervised learning terms;large scale decision;policy experts aware;machine learning;algorithms form supervised;learning terms proposed;used machine learning;supervised learning;demand interpretable;policy experts;interpretations;interpretations terms proposed;proposed interpretations"}, "2c94bc68388517aa4a2d2dfc7d35df95ce24b1a8": {"ta_keywords": "adversarial minimax game;adversarial minimax;formulated adversarial minimax;learning invariant;learning invariant representations;adversarial;formulated adversarial;invariant representations;framework formulated adversarial;invariant representations given;framework learning invariant;minimax game;minimax;invariant;representations;minimax game leads;trait data;trait data proposed;representations given;generalization;better generalization evidenced;representations given factor;trait;generalization evidenced improved;better generalization;factor trait data;learning;generalization evidenced;factor trait;framework learning", "pdf_keywords": ""}, "5a0c9bbf0432dac8bd357a4aabf82b83a6c95524": {"ta_keywords": "document similarity measure;based document similarity;document similarity experiments;document similarity;document similarity document;similarity document specific;document similarity used;aspect based document;document classification;document classification task;terms document similarity;document mining;similarity document;pairwise document classification;document specific aspect;similarity measure research;context document mining;novel aspect based;aspect based;similarity measure;specific aspect based;similarity used recommend;measure research papers;resulting aspect based;aspect;similarity experiments;similarity measure newly;similarity;research papers context;similarity used", "pdf_keywords": "document classification aspect;document similarity measures;aspect based similarity;document similarity research;based document similarity;aspects documents similar;document classification task;document similarity;detect document similarity;articles aspect information;hypothesis document similarity;document similarity combine;document classification;captures similarity wikipedia;aspect based document;document similarity pairwise;document similarity large;wikipedia articles aspect;document similarity good;aspect information extracted;similarity wikipedia articles;traditional document similarity;classify research papers;documents consider aspects;pairwise document classification;consider aspects documents;aspects paper label;aspects documents;similarity research papers;similar dissimilar documents"}, "4f74be7e5dd4b8e9113e86132cf792da2c32ca3d": {"ta_keywords": "propagation charged particle;charged particle turbulent;zeeman field propagation;turbulent medium particle;particle turbulent medium;particle turbulent;effect zeeman field;field propagation charged;propagation charged;turbulent medium;stationary medium particle;medium particle assumed;medium particle;zeeman field;charged particle;field propagation;effect zeeman;particle assumed vicinity;turbulent;particle assumed;particle;zeeman;propagation;vicinity stationary medium;study effect zeeman;stationary medium;vicinity stationary;assumed vicinity stationary;stationary;field", "pdf_keywords": ""}, "90720bba46dd79bc340359617a7a07fcecc890c1": {"ta_keywords": "strategy spaces equilibria;equilibria games continuous;continuous strategy spaces;stability equilibria games;perturbations strategy spaces;strategy spaces structurally;strategy spaces;games continuous strategy;equilibria games;local optimality emerge;spaces equilibria local;naturally games continuous;equilibria local;games continuous;structural stability equilibria;continuous strategy;stability equilibria;having local optimality;local optimality;equilibria local sense;spaces equilibria;optimality emerge;optimality emerge naturally;spaces structurally stable;equilibria;perturbations strategy;stability;structurally stable;respect perturbations strategy;structural stability", "pdf_keywords": ""}, "834fb0d09e764b88ef76ee77e0befb8faeaad7fe": {"ta_keywords": "distribution speech parameters;speech parameters;quality synthetic speech;speech parameters using;synthetic speech;synthetic speech proposed;acoustic parameter segments;individual acoustic parameter;posterior distribution speech;acoustic parameter;rich context models;speech proposed methods;context models statistical;distribution speech;parameter generation;parameter generation high;context models;represent individual acoustic;methods parameter generation;generated posterior distribution;iteratively generated posterior;generated posterior;individual acoustic;use rich context;generation high quality;acoustic;parameters;novel methods parameter;speech;high quality synthetic", "pdf_keywords": ""}, "2507a6924007efbe0c3116048a85108398f23007": {"ta_keywords": "cross lingual glossing;lingual glossing model;automatic cross lingual;lingual glossing;lingual glossing linguistic;glossing linguistic information;glossing linguistic;accurate cross lingual;cross lingual;lingual;linguistic information model;linguistic information;glossing;glossing model;distinctive descriptive text;text distinctive descriptive;glossing model used;linguistic;sources translation;different sources translation;descriptive text;translation;terms accuracy robustness;convert encoded text;encoded text distinctive;text distinctive;approach automatic cross;automatic cross;encoded text;text", "pdf_keywords": ""}, "5ad44a9d6b850405da42f989711af431427425b5": {"ta_keywords": "mixed text bilingual;synthesize mixed text;synthesizing code mixed;text single language;text bilingual;language use synthesize;text bilingual speakers;synthesizing code;code mixed text;single language;different languages;bilingual;distinguish different languages;different languages able;single language use;different languages comparing;languages comparing;mixed text;bilingual speakers;present synthesizing code;bilingual speakers different;single language demonstrate;languages comparing spellings;languages;languages able;spellings able synthesize;mixed text single;speakers different languages;languages able distinguish;code mixed mixed", "pdf_keywords": ""}, "d95b66901d72a13d0c96c7e9bfd4a999ed7fb19c": {"ta_keywords": "language single text;structure language single;characterizing structure language;structure language;language represented single;global representation language;representation language approach;language represented;single text based;representation language;representation language represented;language approach;characterizing structure;language single;language approach based;approach characterizing structure;text based;language;single text;use global representation;text based use;global representation;represented single expression;text;structure;represented single;representation;single expression approach;data sets;data set", "pdf_keywords": ""}, "ad21f0709a3e5d7db91606e2f67926f93e01838d": {"ta_keywords": "players choose values;payoffs based contextual;games players choose;information contextual information;payoffs based;values contextual information;based contextual information;contextual information formulate;choose values payoffs;contextual information;information contextual;players choose;choose values contextual;games;values payoffs based;information contained contextual;novel class games;contextual information contextual;contextual information contained;values payoffs;class games;values contextual;information formulate;based contextual;class games players;contained contextual information;games players;information;new class games;payoffs", "pdf_keywords": "successive contextual games;repeated contextual games;contextual games characterized;rewards contextual games;contextual games;contextual games propose;contextual game;players contextual regret;minimize contextual regret;contextual games information;contextual games type;games described contextual;players minimize contextual;regret repeated games;minimize players contextual;regret optimal context;class contextual games;contextual bandits;games driven contextual;game optimal policy;study contextual game;equilibria optimal contextual;contexts game outcomes;linear contextual bandits;repeated games;repeated games described;game optimal;contextual regret;minimize contextual regrets;contextual regret define"}, "f2d5861a24b7aa33036208ba81e11bb9b2090e7c": {"ta_keywords": "trained multilingual paraphrasing;baselines forsupervised paraphrasing;multilingual paraphrasing corpora;multilingual paraphrasing;paraphrasing corpora model;forsupervised paraphrasing model;paraphrasing corpora;paraphrasing model based;paraphrasing model;paraphrasing;forsupervised paraphrasing;translation based softmax;shot baselines forsupervised;corpora model trained;trained using translation;trained multilingual;baselines forsupervised;procedure trained multilingual;zero shot baselines;softmax;corpora;shot baselines;corpora model;softmax training;autoencoding;baselines;multilingual;softmax training scheme;based softmax training;autoencoding objective", "pdf_keywords": "shot paraphrasing multilingual;paraphrasing multilingual;paraphrasing multilingual text;paraphrasing languages parallel;paraphrasing words softmax;large scale paraphrasing;paraphrasing sentences languages;paraphrasing languages;zero shot paraphrasing;paraphrasing memory efficient;corpora generate paraphrasing;objective paraphrasing languages;generate paraphrasing;parallel paraphrasing;documents parallel paraphrasing;paraphrasing sentences;parallel paraphrasing approach;model paraphrasing words;shot paraphrasing;paraphrasing memory;parallel resulting paraphrases;paraphrasing;neural machine translation;paraphrasing words;paraphrasing large parallel;generate paraphrasing output;scale paraphrasing memory;model paraphrasing;learn sentence embeddings;paraphrasing approach"}, "0e02765103001a792b20242b4dee6dc81917b850": {"ta_keywords": "bootstrapping gene recognizer;gene mentions annotation;gene recognizer;distinguish gene names;gene recognizer biomedical;recognizer biomedical text;method bootstrapping gene;bootstrapping gene;gene names;distinguish gene;gene names gene;names gene mentions;biomedical text approach;recognizer biomedical;gene mentions;names gene;biomedical text;mentions annotation scheme;mentions annotation;annotation scheme;annotation;ability distinguish gene;gene;annotation scheme evaluate;novel method bootstrapping;recognizer;text approach;text approach relies;biomedical;bootstrapping", "pdf_keywords": ""}, "5b94512a17483595c5dffc503a16ba0b46c347e5": {"ta_keywords": "extracting disambiguated hypernymy;hypernyms sets synonyms;disambiguated hypernymy relationships;synonyms synsets;synonyms synsets establishes;sets synonyms synsets;disambiguated hypernymy;hypernyms sets;hypernymy relationships propagates;relationships propagates hypernyms;propagates hypernyms sets;propagates hypernyms;extracting disambiguated;sets synonyms;synonyms;hypernyms;hypernymy relationships;aware relationships synsets;method extracting disambiguated;relationships synsets;relationships synsets method;hypernymy;sense aware relationships;synsets establishes sense;datasets english russian;disambiguated;synsets establishes;synsets;datasets english;synsets method", "pdf_keywords": ""}, "0e52ce6cfd1385e1e9304dcf71d66b53fdc2d4bd": {"ta_keywords": "trends information retrieval;information retrieval;information retrieval ir;trends news centre;recent trends information;trends information;trends news held;retrieval;trends news;recent trends news;retrieval ir;ir recent trends;retrieval ir held;recent trends;workshop recent trends;ir recent;workshop ir recent;trends;news held budapest;news held;european ir workshop;news centre;news centre large;news;european ir;second workshop recent;workshop recent;recent;international workshop ir;ir workshop ir", "pdf_keywords": ""}, "e705255814756178dba75638c29b602095c3cdf4": {"ta_keywords": "transfer learning;learning representation policy;benefits transfer learning;transfer learning representation;training transferred layers;representation policy game;outperforms training transferred;policy game;training transferred;game benefits transfer;learning;learning useful;policy game benefits;learning useful representation;learning representation;transfer highly;outperforms training;transfer;representation policy;transferred layers;transfer highly variable;generally outperforms training;training;benefits transfer highly;transferred;variable requirements learning;useful representation;benefits transfer;complexity fine tuning;requirements learning useful", "pdf_keywords": "quantum state game;quantum learning;deep reinforcement learning;learn quantum state;learns policy transferring;agent learns policy;develop quantum learning;reinforcement learning transferability;learns policy;implementation learn quantum;deep reinforcement;learn quantum;quantum learning algorithm;agent learns;rainbow learn quantum;source deep reinforcement;reinforcement learning drl;learn policy game;layers trained gamein;use deep reinforcement;reinforcement learning;child agent learns;complexity learning policy;learns;transfer learning experiments;learning experiments agents;learning transferability;ability learn policy;reinforcement learning model;learning policy"}, "51203e9d5620abdcdf6c9be93b1e221e79cda67d": {"ta_keywords": "language independent fusion;low dimensional languages;sequence independent language;sequence transfer;dimensional languages;dimensional languages perform;transfer low dimensional;independent language models;perform sequence transfer;language modeled sequence;target language modeled;language models target;languages perform hybrid;models target language;language models;independent fusion transfer;sequence transfer presence;fusion transfer;independent language;language modeled;perform language independent;fusion transfer able;target language;language independent;modeled sequence words;target language improves;languages;sequence words perform;sequence words;transfer", "pdf_keywords": "language independent deep;external language model;attention based sequence;multilingual adaptation;language model joint;language independent model;multilingual adaptation empirically;sequence model deep;rapid adaptation language;acoustic multilingual adaptation;linguistic context training;vocabulary languages adaptation;s2s semantic linguistic;languages adaptation;external language;training language independent;context external language;joint information linguistic;adaptation language independent;adaptation language;novel language model;linguistic context external;language model decoder;text data adaptation;transfer external language;language independent unified;multilingual transfer external;language model;language model referred;models language independent"}, "8319786ed7b9cb13e29130b5617bf0aef586cd6f": {"ta_keywords": "expert model cognitive;expert model generated;accuracy expert models;expert models created;expert model;expert models;results expert model;author expert model;model cognitive student;authoring tutoring better;generated authoring tutoring;demonstrated tutoring;authoring tutoring;tutoring better;study accuracy expert;tutoring;expert;tutoring better higher;model cognitive;demonstrated tutoring decreases;need demonstrated tutoring;created author expert;accuracy expert;cognitive student;cognitive student conducted;tutoring decreases;author expert;results expert;study accuracy;key results expert", "pdf_keywords": ""}, "c37c40db51ccfd6f93004e788102ede72578e5d8": {"ta_keywords": "filtering information social;social information networks;data tweets users;tweets users earthquake;data tweets;using data tweets;social information;information social information;tweets users;filtering information;information social;information networks;information networks proposed;tweets;users earthquake;extracting filtering information;information massive;users earthquake warning;filtering;machine learning;extracting filtering;networks proposed framework;based machine learning;machine learning able;networks proposed;information massive number;networks;information;framework extracting filtering;situations information massive", "pdf_keywords": ""}, "92259193a9d7377368790bf8517cd9798f30caae": {"ta_keywords": "information pollution approach;information pollution;approach explainable questions;context information pollution;explainable questions;explainable questions context;approach explainable;explainable answer uses;information approach illustrated;information approach;novel approach explainable;principles explainable answer;answer explain provenance;pollution approach;pollution approach based;uses principles explainable;explain provenance;pollution;explain provenance validity;principles explainable;provenance validity;information;questions context information;provenance;validate correctness answer;explainable answer explain;answer uses principles;context information;principles principles explainable;approach illustrated examples", "pdf_keywords": ""}, "0e1a665334b1ec35d77ab1cd4f21bd0da9745548": {"ta_keywords": "knowledge text categorization;text categorization able;text categorization;approach text categorization;categorization able accurately;categorization;expert knowledge text;categorization able;expert knowledge outperforms;expert knowledge given;expert knowledge;knowledge given word;represent expert knowledge;accurately represent expert;knowledge text;algorithmic approach text;expert;knowledge outperforms;algorithmic;represent expert;utility expert knowledge;terms accuracy;knowledge outperforms existing;algorithmic approach;terms accuracy efficiency;method terms accuracy;algorithmic approach able;recent algorithmic approach;given word result;approach text", "pdf_keywords": ""}, "9ca95a09c8bf2d7d28234ff37ece182836dd8632": {"ta_keywords": "transition based parsing;parsing algorithm;imitation learning statistical;based parsing algorithm;describing behavior particle;parsing;parsing algorithm task;based parsing;imitation learning;algorithm based imitation;behavior particle random;based imitation learning;particle random world;behavior particle;particle algorithm;statistical model particle;action particle algorithm;particle random;based imitation;particle;random world algorithm;particle algorithm applied;particle determined action;imitation;algorithm task describing;learning statistical;model particle;learning statistical model;synthetic data;transition based", "pdf_keywords": ""}, "0aa0131253b832fdba27ac43f8fa78a322763191": {"ta_keywords": "transformation paralinguistic information;lingual transformation paralinguistic;paralinguistic information proposed;paralinguistic information;transformation paralinguistic;paralinguistic features;possible paralinguistic features;paralinguistic features handle;different possible paralinguistic;paralinguistic;possible paralinguistic;features input speech;cross lingual transformation;lingual transformation;output speech;method cross lingual;input speech;translate features input;speech output;speech output speech;cross lingual;input speech output;translate features;features target language;input acoustic features;reconstructing input acoustic;output speech continuous;speech continuous space;lingual;method translate features", "pdf_keywords": ""}, "a94ec1cd89839aa5132118916849d46dff861914": {"ta_keywords": "interacting systems;mind modeling;dynamics interacting systems;interacting systems context;mind modeling able;interacting;study dynamics interacting;situated interactions interactions;dynamics interacting;theory mind focus;interactions;theory mind;interactions interactions;interacting particles;task mind modeling;situated interactions;interacting particles point;case situated interactions;pair interacting;capture dynamics interacting;context theory mind;pair interacting particles;interactions interactions form;interactions form;models task mind;dynamics;mind focus case;systems context theory;computational models;mind focus", "pdf_keywords": "interacting players game;modeling situated dialogue;interacting players;multimodal situated dialogue;situated collaborative tasks;situated dialogue collaborative;game interactions;interplay interactions shared;interplay interactions;situated human dialogue;representation game interactions;players communicate;behaviors situated language;dialogue collaborative tasks;study interplay interactions;dynamics interacting players;dialogue utterances game;collaborative behaviors situated;collaborative tasks 3d;observe players communicate;utterances game players;situated language communication;modeling situated collaborative;mind modeling situated;dialogue task situated;human collaborative behaviors;situated dialogue;collaborative dialogue task;collaborative behaviors;minecraft game"}, "8278e5c2a894793e2c93c6c9f0e7535109e7858f": {"ta_keywords": "2d electron gas;electron gas dynamics;dynamics 2d electron;electron gas confined;interaction electron gas;electron gas box;electron gas trapped;box electron gas;electron gas;gas dynamics 2d;trapped box electron;2d electron;repulsive interaction electron;dimensional 2d electron;box electron;box repulsive interaction;attractive interaction electron;interaction electron;gas box repulsive;gas dynamics;gas confined box;gas confined;confined box repulsive;gas trapped box;electron;repulsive interaction;dynamics 2d;box attractive interaction;gas box;box repulsive", "pdf_keywords": ""}, "c637636a2afd7968bdb893af8d2fd220fd39df8f": {"ta_keywords": "monitoring meetings;meeting analyzer monitoring;monitoring meetings obtain;meeting analyzer;real time meeting;time meeting analyzer;group meeting based;latency monitoring meetings;monitoring conversations;meetings obtain;camera center meeting;meeting transcription;monitoring conversations ongoing;group meeting;ongoing group meeting;meeting table;analyzer monitoring conversations;meeting based;conversations ongoing group;meeting table aim;meetings;context meeting transcription;meetings obtain best;meeting transcription techniques;meeting;center meeting table;distant microphones;center meeting;uses distant microphones;context meeting", "pdf_keywords": ""}, "ffac42087ee4ad50df9203762db715dedd209c0b": {"ta_keywords": "semantic tags propagation;semantic tags method;semantic tags instructions;semantic tags;tags propagation;grammars enriched semantic;enriched semantic tags;semantic analysis;instruction semantic tags;tags propagation mechanism;context free grammars;useful semantic analysis;propagation instruction semantic;semantic;semantic analysis offer;method semantic analysis;enriched semantic;free grammars enriched;semantic analysis based;free grammars;grammars enriched;useful semantic;instruction semantic;new method semantic;method semantic;method useful semantic;tags method;grammars;context free;tags instructions", "pdf_keywords": ""}, "919c929dfa665cb0595a835b4380f96da4cd0143": {"ta_keywords": "bandlimited sensors sampling;sensing matrices sampling;dimensional bandlimited sensors;sensors sampling;sensors sampling strategies;bandlimited sensors;matrices sampling;matrices sampling strategies;bandlimited fields random;sampling strategies reconstruction;reconstruction dimensional bandlimited;dimensional bandlimited fields;sensing matrices;field reconstruction;sampling strategies performance;derived matrices sensing;field reconstruction results;matrices sensing;performance existing sampling;bandlimited fields;sampling strategies;sampling;existing sampling strategies;matrices sensing matrices;performance sampling strategies;performance sampling;existing sampling;sampling strategies evaluated;paths dimensional bandlimited;dimensional bandlimited", "pdf_keywords": "bandlimited field sampling;location unawareness sampling;sensor applied sampling;field mobile sensor;reconstruct sampling points;field sampling strategies;mobile sensor method;field sampling;field sampling trajectories;points sampling;sampling points sampling;location sensor mobile;sampling points;reconstruction noisy field;mobile sensor analyze;reconstruct sampling;estimating bandlimited field;location sensor proposed;sensing problems proposed;sampling geometry sampling;mobile sensor;geometry sampling;sampling geometry;location sensor;field based measurements;used reconstruct sampling;points sampling powerful;trajectories optimal sampling;position mobile sensor;optimal sampling trajectories"}, "59bdf61a81e46a6c9a96c0c5f96f2f77b82ab09f": {"ta_keywords": "subsurface detonations studied;subsurface detonations;buried subsurface detonations;multipole free fusion;multiphase multicomponent multipole;electron gas fusion;detonations studied;fusion composed layers;multiphase multicomponent;generation multiphase multicomponent;multicomponent multipole;multicomponent multipole free;gas fusion composed;composed layers degenerated;behavior buried subsurface;new generation multiphase;detonations studied context;free fusion;layers degenerated unconfined;gas fusion;detonations;systems fusion composed;buried subsurface;generation multiphase;multiphase;fusion composed;layers degenerated;fusion msp systems;multipole free;dimensional electron gas", "pdf_keywords": ""}, "c340b89e7b7fa84fac85cdcf38ba7007e2e71930": {"ta_keywords": "based clustering speaker;clustering speaker embeddings;end speaker diarization;speaker diarization;clustering speaker;speaker diarization problem;problem speaker diarization;speaker embeddings;speaker embeddings improves;diarization problem speaker;end speaker;end end speaker;problem speaker;dialogue recordings;real dialogue recordings;diarization problem addressed;calls real dialogue;speaker;mixtures real telephone;diarization;diarization problem;dialogue recordings reveal;self attention blocks;clustering;attention blocks proposed;attention blocks;dialogue;introducing self attention;state art clustering;real dialogue", "pdf_keywords": "speaker diarization based;speaker diarization model;clustering based speaker;automatic speaker diarization;speaker diarization systems;speaker diarization approach;learn speaker diarization;based clustering speaker;speaker diarization technique;speaker diarization method;based speaker diarization;speaker diarization;clustering speaker;speaker diarization process;speaker recognition;speaker embeddings clustering;clustering speaker embeddings;present speaker diarization;typical speaker diarization;nonlinear speaker diarization;extension speaker diarization;speaker recognition using;approach automatic speaker;nonlinear speaker representation;nonlinear speaker recognition;estimating speaker label;training speaker dimensional;speaker representation;speaker label sequence;model training speaker"}, "0fcdf20477f907aa50578876226f5fabf5e074ea": {"ta_keywords": "link bias recommender;bias recommender systems;citation networks;citation networks methods;bias recommender;link bias;correcting link bias;world citation networks;real citations;real citations literature;citations;citations literature;real world citation;citations literature propose;recommender systems;fraction real citations;recommender systems approach;learning exposure probabilities;exposure probabilities learn;probabilities learn exposure;recommender;citation;learn exposure probabilities;world citation;known exposure probabilities;probabilities learn;bias;networks methods;networks methods reliably;methodology correcting link", "pdf_keywords": "citations social networks;links citation networks;citation networks social;bias link recommendation;empirical link prediction;observed citation network;citations citation networks;citation networks;bias empirical link;citation networks finally;citation network;citation network improve;link prediction models;citation datasets propose;world citation networks;citations social;learning link probabilities;links citation;link exposure bias;link prediction;networks missing citations;bias recommender;bias link;citation datasets;intrinsic bias recommender;recommendation correcting bias;link probabilities propensity;citations;predict paper cite;citation networks missing"}, "da1296f071bb2b65ae9e0b016d914d24b4edb2d2": {"ta_keywords": "stability frontier capital;frontier capital markets;measure stability frontier;stability frontier;capital markets measure;frontier capital;capital markets;quantitative measure stability;measure stability;markets measure based;markets measure;stability;frontier;capital;sectional distribution weighted;markets;weighted weighted average;sectional distribution;weighted average;weighted average weighted;average weighted weighted;distribution weighted average;cross sectional distribution;average weighted;average weighted average;new quantitative measure;based cross sectional;distribution weighted;weighted weighted;weighted", "pdf_keywords": ""}, "916c8553beb3ba4e0d20ba6d7eb2bca365d820c8": {"ta_keywords": "protein folding induced;protein folding presence;protein protein folding;dynamics protein folding;folding induced protein;protein folding process;protein folding;process protein folding;folding caused protein;folding process protein;protein folding caused;folding presence protein;folding induced;folding process;study dynamics protein;dynamics protein;process protein;folding caused;induced protein protein;folding;caused protein protein;protein protein;folding presence;protein;induced protein;caused protein;presence protein protein;presence protein;dynamics;process", "pdf_keywords": ""}, "7f613ab03d776f996eb582f04d258a51868dca03": {"ta_keywords": "alkyl benzohydrazides lipase;hydrazine lipase catalyzed;benzohydrazides lipase;benzohydrazides lipase investigated;hydrazine lipase;lipase catalyzed alkyl;catalyzed alkyl benzohydrazide;insertion hydrazine lipase;compound alkyl benzohydrazides;alkyl benzohydrazides method;alkyl benzohydrazides;lipase investigated method;alkyl benzohydrazide;lipase catalyzed;benzohydrazides method demonstrated;alkyl benzohydrazide yield;benzohydrazides method;benzohydrazides;lipase investigated;benzohydrazide yield;benzohydrazide yield method;benzohydrazide;lipase;based insertion hydrazine;catalyzed alkyl;organic compound alkyl;compound alkyl;synthesis classic organic;hydrazine;insertion hydrazine", "pdf_keywords": ""}, "2aa85084315a4107e2b9b935506b4e9f11428601": {"ta_keywords": "model caries mechanism;caries mechanism model;domain caries mechanism;caries incipient lesions;caries mechanism;caries mechanism smooth;model caries;smooth model caries;results study caries;smooth domain caries;study caries;caries;study caries incipient;caries incipient;domain caries;remineralization model bovine;model bovine surface;bovine surface model;bovine surface;lesions produced remineralization;smooth domain remineral;remineralization model;domain remineral;remineralization;produced remineralization model;lesions;model bovine;lesions produced;remineral;produced remineralization", "pdf_keywords": ""}, "542ad79bb96fc10c46086778aaafc8f3509c5c18": {"ta_keywords": "nonconvex nonconcave minimax;nonconcave minimax;alternating gradient descent;nonconcave minimax problems;gradient descent ascent;minimax problems alternating;ascent agda algorithm;gradient descent;descent ascent agda;ascent sg method;minimax problems;minimax;based alternating gradient;descent ascent;descent ascent sg;alternating gradient;nonconvex nonconcave;class nonconvex nonconcave;problems alternating gradient;ascent agda;agda algorithm;nonconcave;ascent;agda algorithm algorithm;ascent sg;algorithm based alternating;sublinear rate algorithm;solving class nonconvex;gradient;novel algorithm solving", "pdf_keywords": "nonconcave minimax optimization;sequential gradient descent;minimax optimization;nonconcave minimax;nonconvex nonconcave minimax;minimax optimization number;algorithms minimax;min max optimality;algorithms minimax problems;strategy nonconvex minimax;alternating gradient descent;nonconvex minimax;gradient descent ascent;alternating gradient ascent;minimax problems finite;nonconcave objectives finite;nonconvex nonconcave objectives;max optimality;nonconvex minimax problem;algorithm solving minimax;minimax problems alternating;stochastic sequential gradient;minimax problem stochastic;convex optimization;generalization convex optimization;method converges optimality;stochastic nonconvex optimization;convex optimization method;pdd algorithms minimax;nonconvex optimization methods"}, "69ee9b3a915951cc84b74599a3a2699a66d4004f": {"ta_keywords": "archi9 tecture semantic;semantic spatial pathways;tecture semantic spatial;ma10 nipulation framework;end networks;spatial pathways ma10;end end networks;unseen objects 14;nipulation framework;objects 14 explicit;semantic spatial;pathways ma10;nipulation framework capable;end networks combines;networks;objects 14;pathways ma10 nipulation;tecture semantic;ma10 nipulation;instance 15 segmentations;networks combines;15 segmentations history;unseen objects;15 segmentations;folding unseen objects;worlds stream archi9;stream archi9;stream archi9 tecture;poses instance 15;objects 14 folding", "pdf_keywords": "generalize task grasping;manipulation tasks combines;manipulation tasks;task grasping;semantic manipulation stream;manipulation train language;conditioned manipulation tasks;grained manipulation objects;manipulation tasks simulate;vision language grounding;imitationlearning agent combines;language conditioned imitationlearning;semantic manipulation single;trained tasks;manipulation objects;manipulation multi task;imitationlearning agent;conditioned imitationlearning;fine grained manipulation;imitationlearning;better grasping state;task trained agents;level semantic manipulation;grasping state art;representation task trained;conditioned imitationlearning agent;manipulation train;task trained;approach grasping objects;task agents trained"}, "fa3826770207f7bf8bd85a8e97c9ac437f46b061": {"ta_keywords": "effect multiplicity small;multiplicity small scale;study effect multiplicity;observing false positive;threshold adjustment multiplicity;positives adjustment multiplicity;probability observing false;multiplicity small;effect multiplicity;small scale experiments;number false positives;scale experiments probability;observing false;false positives adjustment;false positives;experiments probability observing;experiments probability;adjustment multiplicity;scale experiments;multiplicity fixed;multiplicity;multiplicity fixed value;experiments;adjustment multiplicity fixed;false positive;positives adjustment;false positive given;significant reduction;threshold;significant reduction number", "pdf_keywords": ""}, "ce89ee7aaeeea2c9d474707690f3ea9d948776a3": {"ta_keywords": "noisy machine translation;machine translation systems;translation systems;machine translation;translation systems consisting;sourced translations;reddit sourced translations;translations;sourced translations english;japanese comments english;translations english;comments english;comments english quantitatively;english japanese comments;dataset performance noisy;noisy comments;consisting noisy comments;noisy comments reddit;translations english japanese;japanese comments;translation;benchmark dataset;performance noisy;benchmark dataset performance;propose benchmark dataset;performance noisy machine;benchmark;english quantitatively examine;english japanese;noisy", "pdf_keywords": "machine translation noisy;translation noisy text;noise machine translation;neural machine translation;appropriate translation noisy;translation noisy;extract comments noisy;dataset machine translation;classify noisy text;translation domain adaptation;noisy text languages;machine translation models;comments noisy text;machine translation;existing machine translation;identify noisy comments;translation models;machine translation method;machine translation domain;noisy text based;characterizing noisy text;noisy comments;consisting noisy comments;patterns noisy text;best translation strategy;machine translation mt;noisy comments comments;comments noisy;translation resulting patterns;english japanese comments"}, "7f85b7ee0fc6cdb5b92417035a7049247729545a": {"ta_keywords": "imbalance performance classifiers;increasing class imbalance;class imbalance performance;classifiers given dataset;class imbalance;optimal classifiers;class imbalance method;finding optimal classifiers;classifiers;optimal classifiers given;dataset increasing class;imbalance performance;classifiers present;classifiers given;size class imbalance;performance classifiers;performance classifiers present;instances dataset increasing;imbalance method based;classifiers present method;increasing class;imbalance method;dataset increasing;training data size;dataset results increasing;training data;imbalance;instances dataset;increasing number instances;number instances dataset", "pdf_keywords": ""}, "ce5a57c0ccc8993f4a8e3a07101140a757024d9f": {"ta_keywords": "annotated spoken quotes;popularity spoken quotes;spoken quotes influential;spoken quotes spoken;spoken quotes;annotated spoken;quotes spoken quotes;features public talks;spoken quotes highly;quotes spoken;linguistic acoustic features;form annotated spoken;talks form annotated;memorableness popularity spoken;public talks;linguistic acoustic;quotes influential;quotes highly informative;study linguistic acoustic;public talks form;quotes influential factors;popularity spoken;affect original quotes;talks;memorableness popularity;affect memorableness popularity;talks form;quotes;quotes highly;linguistic", "pdf_keywords": ""}, "61ad8a0778598022e71c0ee3ba9bc53ddd616517": {"ta_keywords": "questions posed conversational;simple interrelated questions;interrelated questions posed;represent questions answer;questions posed;represent questions;language represent questions;interrelated questions;simple expressive language;conversational;expressive language;posed conversational;simple expressive;questions;expressive language represent;use simple expressive;conversational setting;posed conversational setting;questions answer fully;simple interrelated;questions answer;conversational setting use;expressive;interrelated;language represent;answer fully implicit;language;present collection simple;collection simple interrelated;use simple", "pdf_keywords": "parsing question answering;sequential semantic parsing;parsing complex intents;sequential question answering;semantic parsing;semantic parsing floating;context dependent parsing;semantic parsing question;semantic parsing simple;semantic parsers;semantic parsing dataset;question answering;total semantic parsing;new semantic parser;performance semantic parsers;semantic parser;semantic parsers advanced;semantic parser addresses;parser neural;question answering focused;parsing;dependent parsing;floating parser neural;question answering natural;dependent parsing complex;used semantic parser;parser neural network;question answer task;parsing simple related;sqa sequential semantic"}, "9837207d3f4ee8c493375a97077c6f8b22cadac9": {"ta_keywords": "residency test xmath0sn;durational residency test;validity durational residency;durational residency;test xmath0sn theorem;residency test;xmath0sn theorem examined;validity xmath3sn test;case validity xmath3sn;xmath3sn test consequence;xmath0sn theorem;xmath2sn case validity;tests valid xmath1sn;valid xmath1sn case;validity xmath3sn;validity durational;xmath3sn test;xmath1sn case consistent;test xmath0sn;residency;xmath1sn case;tests incompatible xmath4sn;valid xmath1sn;consistent xmath2sn case;case consistent xmath2sn;xmath4sn case;xmath2sn case;incompatible xmath4sn case;theorem fact tests;case validity", "pdf_keywords": ""}, "de9d3a28f9e112a248d097d72ba6ad41a71c8a78": {"ta_keywords": "learnable polynomials depth;learning polynomials depth;learning polynomial depth;logarithmic learnable polynomials;polynomials depth based;polynomials depth logarithmic;learnable polynomials;polynomials depth;polynomial depth;type polynomials depth;learning polynomials;polynomials depth particular;learning polynomial;depth logarithmic learnable;polynomial depth particular;logarithmic learnable;language polynomials depth;method learning polynomials;clauses language polynomials;equivalent learning polynomial;type polynomials;depth logarithmic fact;particular type learnable;depth logarithmic;language polynomials;determinate clauses language;particular type polynomials;polynomials;depth particular type;type learnable", "pdf_keywords": ""}, "bcde1ba141078cf37a69a691fd329d8fd7e70b9b": {"ta_keywords": "generation active galaxies;set active galaxies;active galaxies method;active galaxies single;active galaxies;active galaxies based;active galaxies necessary;active galaxy method;single active galaxy;galaxies single active;galaxies based simultaneous;active galaxy set;active galaxy;galaxies method;galaxies method based;galaxies single;galaxy set active;galaxies;generation set active;galaxies necessary;galaxies necessary step;galaxy method;galaxies based;galaxy method applied;generation active;applied generation active;simultaneous generation set;simultaneous generation;based simultaneous generation;single active", "pdf_keywords": ""}, "83d6b4bfa8701578c291e55f5f1e5e6508aff313": {"ta_keywords": "medical ethics technology;medical ethics;ethics technology ethics;future medical ethics;ethics technology design;technology ethics;ethics technology;technology ethics intricately;technology medical settings;use technology medical;technology ethics offer;technology medical;patient autonomy;areas medical ethics;context patient autonomy;patient autonomy areas;autonomy areas medical;ethics;ethics intricately intertwined;ethics intricately;medical settings;future medical;medical settings particular;ethics offer practical;medical;use technology;autonomy;perspectives use technology;technology design;technological perspectives use", "pdf_keywords": ""}, "1bc87dba9838b3028b636f456084252f2beac108": {"ta_keywords": "game building energy;allow occupants energy;energy efficiency incentive;incentive design game;occupants energy;building energy;occupants energy individual;building energy efficiency;social game building;encourage interaction occupants;incentive design;efficiency incentive design;efficiency incentive;game building;game designed encourage;energy;energy efficiency;energy individual;present social game;social game;incentive;encourage interaction building;design game;designed encourage interaction;interaction occupants;game game designed;noncooperative game game;noncooperative game;building manager occupants;interaction occupants occupants", "pdf_keywords": ""}, "f016ac107259d6d222c9f52b37208fca4fa1d6bc": {"ta_keywords": "use case modeling;case modeling methodology;case modeling;domain specific;novel use case;specific domain specific;use case;domain specific domain;quality given domain;modeling methodology specifying;given domain specific;methodology specifying quality;specific domain;domain;modeling methodology;specifying quality;case;methodology specifying;specifying quality given;given domain;specifying;propose novel use;methodology;modeling;novel use;quality;specific;quality given;propose novel;use", "pdf_keywords": ""}, "36c770b79937db2e3416204b8cf177d0c9881f54": {"ta_keywords": "control magnetization plasma;magnetization plasma magnetic;magnetized plasma magnetic;magnetization plasma;magnetized plasma;properties magnetized plasma;plasma magnetic;plasma magnetic field;magnetic properties magnetized;control magnetization;magnetization;properties magnetized;used control magnetization;magnetic field magnetic;field magnetic properties;field magnetic;magnetic field;magnetic field used;effect magnetic field;magnetic properties;plasma;magnetized;effect magnetic;magnetic;study effect magnetic;field used control;field used;field;properties;used control", "pdf_keywords": ""}, "0a485fd94b2cb554e281d0f8d7e9f71db4891ce0": {"ta_keywords": "downsampling method vision;novel token downsampling;token downsampling method;token downsampling;method vision transformers;downsampling;downsampling method;vision transformers improves;vision transformers;exploiting redundancies images;transformers improves accuracy;images intermediate token;token representations;token representations proposed;predicting shape;accuracy resulting reconstruction;reconstruction error exploiting;intermediate token representations;redundancies images;redundancies images intermediate;transformers improves;predicting shape given;method vision;transformers;transformers achieve better;output transformers;pruned output transformers;transformers achieve;improves accuracy;representations proposed method", "pdf_keywords": "transformers deep learning;downsampling softmax attention;aware transformers deep;tokens deep learning;token downsampling constrained;novel token downsampling;novel token downsamplingtransformers;tokens deep neural;transformers deep;token representations improve;downsampling softmax;bottleneck vision transformers;token downsamplingtransformers;downsamplingtransformers machine learning;downsampling input tokens;token downsampling;based downsampling softmax;token downsampling pretrained;token downsampling principle;present token downsampling;using token downsampling;softmax attention;image deep learning;token downsampling method;deep transformerswe;recent deep learning;attention layer transformer;image deep;token downsamplingtransformers machine;softmax attention sub"}, "d1f32060e921b6e06badd7fdb2b750638b2d131c": {"ta_keywords": "deep beamforming acoustic;training deep beamforming;multi channel speech;acoustic modeling networks;deep beamforming;channel speech recognition;beamforming acoustic modeling;speech recognition;channel speech;beamforming acoustic;np2012 meeting corpus;training deep;speech recognition proposed;joint training deep;acoustic modeling;networks multi channel;meeting corpus;beamforming;networks multi;multi channel;corpus;modeling networks multi;speech;unified computational network;acoustic;single network;method joint training;np2012 meeting;networks;modeling networks", "pdf_keywords": ""}, "98290fb02a844108df202e9a6dc3461e3f14ee32": {"ta_keywords": "stationary points optimization;convex compact surrogate;compact surrogate problem;surrogate problem efficiently;points optimization problems;optimization problems form;points optimization;optimization problems;optimization;approximate order stationary;order stationary points;compact surrogate;maxyy sets convex;convex compact;stationary points;surrogate problem;problem efficiently optimized;sets convex compact;optimized approach;efficiently optimized approach;order taylor approximation;efficiently optimized;convex;optimized approach relies;finding approximate order;minxx maxyy sets;optimized;order stationary;taylor approximation finding;taylor approximation", "pdf_keywords": "nonconcave minimax optimization;nonconvex nonconcave minimax;nonconcave minimax;stronglyconcave minimax optimization;convex nonconcave minimax;nonconvex stronglyconcave minimax;nonconcave maximizers;quadratically nonconcave maximizers;nonconcave problems primal;stronglyconcave minimax;nonconcave maximizer quadratically;nonconcave maximizer;quadratically nonconcave maximizer;minimax optimization;stationary points optimization;nonconvex minimization;concave approach relies;minimax optimization bound;maximizer quadratically nonconvex;approximating local equilibrium;nonconvex nonconcave problems;nonconvex minimization problems;constructing optimal point;search quadratically nonconcave;convex concave approach;nonconcave maximizers proof;nested maximization;solving nonconvex minimization;maximizing global maximizer;concave approach"}, "ad26e5105b6019ff68404962e39ea3a1dfb5931d": {"ta_keywords": "algorithm synthesize incentive;synthesize incentive sequence;incentive sequence minimizes;incentive sequence;agent finite decision;finite decision horizon;design incentives principal;incentives principal;method design incentives;finite decision;behavior stochastic decision;synthesize incentive;stochastic decision;stochastic decision process;incentives;decision process mdp;incentive;propose polynomial time;incentives principal offer;design incentives;temporal logic formula;temporal logic;mdp propose polynomial;expressed temporal logic;decision horizon;minimizes cost principal;underlying mdp deterministic;mdp deterministic;decision horizon model;polynomial time", "pdf_keywords": "algorithm synthesize incentive;optimal incentive design;optimal behavior incentives;incentive design optimal;policy optimal incentive;agent incentive design;optimality incentive design;finding optimal incentive;problem optimal incentive;synthesize incentive design;designing incentives maximizes;optimal incentive;optimality incentive;synthesize incentive sequence;preserve optimality incentive;problem incentive design;simple incentive design;incentive design synthesize;incentive design polynomial;modeled incentive design;agent propose incentive;incentive design;incentive design minimizes;incentive sequence minimizes;incentives agent computes;method synthesize incentive;incentive design agents;incentive design method;propose incentive design;incentive design implemented"}, "89a8edbc0fe2ea8b9ee703ca37e5d5d6d34c571a": {"ta_keywords": "speech translation e2e;translation performance end;end speech translation;translation performance;improve translation performance;speech translation;translation e2e st;translation e2e;improve translation;end end speech;approach improve translation;end speech;e2e st model;e2e;e2e st;translation;performance end end;st model eq;speech;model eq;model eq eq;performance end;eq eq;eq;st model;end end;eq eq eq;performance;model;improve", "pdf_keywords": "neural machine translation;deep bidirectional language;predict paraphrased transcriptions;neural network translation;speech text translation;paraphrased transcriptions auxiliary;machine translation;end speech translation;machine translation corpus;machine translation nmt;end translation speech;automatic transcription task;paraphrased transcriptions;deep bidirectional;speech translation e2e;automatic transcription;translation corpus;speech translation;translation knowledge distillation;pretraining deep bidirectional;language training forward;automatic speech text;translation speech documents;translation speech;forward speech source;fragments machine translation;translation nmt models;performance automatic transcription;network translation knowledge;paraphrasing large corpus"}, "daa7e6af585d03e9cb05487413a6495f23400398": {"ta_keywords": "wireless networks assignment;assignment problems wireless;networks assignment problem;variables wireless networks;networks assignment;binary variables wireless;assignment binary variables;assignment problem formulated;assignment binary;set permutation matrices;wireless networks;assignment problems;variables wireless;assignment problem;permutation matrices numerical;permutation matrices;problems wireless networks;unsupervised learning algorithm;learns nonconvex projection;algorithm proposed learns;permutation matrices new;learning algorithm;task set permutation;matrices new unsupervised;problem set permutation;learning algorithm proposed;wireless networks notoriously;unsupervised learning;binary variables;proposed learns nonconvex", "pdf_keywords": "learning arbitrary assignment;convex assignment problems;assignment tasks learning;convex assignment problem;network assignment problems;nonlinear deep learning;deep learning approximation;nonlinear network snn;assignment problems wireless;network assignment;network nonlinear neural;assignment problems proposed;assignment problems training;convex assignment;solving assignment problems;deep learning network;non convex assignment;nonlinear programming;called nonlinear programming;assignment problems numerical;network deep learning;arbitrary assignment tasks;learning network;propose nonlinear network;assignment tasks training;nonlinear neural network;network deep;nonlinear network;neural network nonlinear;assignment problem arises"}, "f11ed27f4640dd8785ea7c4aff9705ddaad2b24d": {"ta_keywords": "detection multimedia fake;multimedia fake news;visual content fake;multimedia fake;content fake;content fake news;detection multimedia;methods detection multimedia;visual features detection;features detection methods;features representative detection;features detection;fake news chapter;representative detection methods;fake news;visual features;visual content;effective visual features;representative detection;visual features representative;detection methods;detection methods detection;detection;fake;visual content including;field visual content;detection methods chapter;multimedia;role visual content;content", "pdf_keywords": ""}, "ad4b09832454a821e925e45e96e769f0c01bd3d6": {"ta_keywords": "arcsecond sparse word;arcsecond word graph;sparse word graph;word graph based;word graph algorithm;sub arcsecond word;word graph;word correla tions;sub arcsecond sparse;arcsecond word correla;sparse word;arcsecond sparse;arcsecond word;characterization sub arcsecond;based sub arcsecond;tions large collections;sub arcsecond;correla tions;correla tions large;documents algorithm;word correla;arcsecond;documents algorithm based;collections documents algorithm;sparse;graph based sub;version sub arcsecond;tions;large collections documents;novel algorithm characterization", "pdf_keywords": ""}, "1f133158a8973fb33fea188f20517cd7e69bfe7f": {"ta_keywords": "models text classification;accurate text classification;text classification tasks;text classification;text classification variety;classification tasks significantly;classification tasks;class models text;models text;classification;accurate text;outperform existing models;perform accurate text;new class models;classification variety benchmark;class models;significantly outperform;tasks significantly outperform;significantly outperform existing;accurate counterparts models;benchmark datasets;outperform;models;known simple models;text;simple models gennes;simple models;existing models;outperform existing;hooft significantly accurate", "pdf_keywords": "fnet encoder outperforms;fnet models;models fnet;attention sublayers random;models fnet models;fnet encoder;architecture attention models;attention models;sublayer dft attention;performance fnet;attention sublayers;attention sublayer;dft self attention;self attention sublayers;self attention sublayer;attention sublayer simple;discrete models fnet;fnet simple highly;trained fast fourier;bert large benchmarks;attention models based;fnet architecture outperforms;outperforms random encoder;random memory neural;dft attention;dft attention mechanism;attention large class;convolution model powerful;exponential convolutional neural;random sublayer fnet"}, "c9ce3889c03fee2990b2277423bbc0fb4366df53": {"ta_keywords": "discriminative language modeling;approach discriminative language;discriminative language;learn embeddings data;language modeling;learn embeddings;new approach discriminative;model feature representation;language modeling problem;network learn embeddings;approach discriminative;discriminative;embeddings data;feature representation;linear model feature;log linear model;embeddings;neural network learn;feature representation used;use neural network;parameterized neural network;function parameterized neural;parameterized neural;log linear;embeddings data form;neural network;model feature;use neural;based use neural;form log linear", "pdf_keywords": ""}, "2de8019fd7d04e3d1305d5efaeeb591f0d966550": {"ta_keywords": "transformer speech recognition;autoregressive transformer speech;transformer speech;speech recognition;speech recognition propose;deep transformers;autoregressive transformer;non autoregressive transformer;deep transformers factor;propose new decoding;new decoding;new decoding strategy;art deep transformers;decoding;training non autoregressive;transformer;transformers;state art deep;decoding strategy;transformers factor;transformers factor case;speech;decoding strategy starts;recognition propose new;turing;turing machine;recognition;autoregressive;turing machine proposed;outperform state art", "pdf_keywords": ""}, "ca2144b895cf6812eec535261df9294896417425": {"ta_keywords": "relation extraction classification;relation classification task;relation extraction;relation classification;semantic relation extraction;relation extraction model;results relation extraction;end relation extraction;related extraction classification;present relation classification;related extraction;semantic relation;extraction classification information;relation;theory related extraction;extraction classification related;embeddings present relation;extraction classification task;extraction classification;concept candidate embeddings;end relation;classification information;semantic;classification related;classification information respectively;classification task extend;time semantic relation;end end relation;candidate embeddings present;candidate embeddings", "pdf_keywords": "relation extraction classification;semantic relation extraction;relation classification neural;relation extraction;relation extraction subtask;relation classification task;relations relation extraction;relation classification;relation extraction model;called relation extraction;accuracy relation extraction;relation extraction variety;input relation extraction;semantic relations;semantic relations relation;second relation classification;predict semantic relation;semantic relation;word embedding training;semantic relation type;involves semantic relation;extract information relation;information extraction;instances semantic relations;concepts called relation;semantic scholar corpus;relation concepts;types relation classification;task semantic relation;relation concepts called"}, "3638e5dfc79ba3fb757900f46ac0c7e7f6dadb05": {"ta_keywords": "cameraphone use social;relate use cameraphone;context aware cameraphone;cameraphone application mobile;cameraphone use;use cameraphone use;aware cameraphone application;mobile media sharing;cameraphone application;social network photos;use cameraphone;cameraphone prior photographic;aware cameraphone;cameraphone;mobile media;media sharing;network photos users;use cameraphone prior;cameraphone prior;media sharing relate;photos users;photographic activity new;social uses;use social network;imaging communicative uses;photographic activity;application mobile media;photos users quickly;uses imaging communicative;social uses identified", "pdf_keywords": ""}, "86eb740bbc54a6d734242be28fccf76fd4d2c1ba": {"ta_keywords": "parallelization games algorithms;algorithms parallelization games;parallelization games;learning algorithms parallelization;finite time learning;stochastic game;stochastic game theory;theory stochastic game;games algorithms;games algorithms based;algorithms parallelization;parallelization;time learning algorithms;time learning;learning algorithms;finite time;gradient based learning;game theory;stochastic;theory stochastic;algorithms based gradient;computer science convergence;time learning powerful;learning shown convergent;convergence algorithms;class finite time;science convergence algorithms;examples finite time;convergent long time;games", "pdf_keywords": ""}, "0fdc3efc11526995d192f18e19f07fba062a76f7": {"ta_keywords": "programmatic weak supervision;advances weak supervision;weak supervision paradigm;weak supervision context;weak supervision;conjunction weak supervision;weak supervision gws;supervision gws paradigm;supervision context programmatic;supervision context;supervision paradigm;supervision paradigm sph;supervision gws;supervision;programmatic weak;limited labeled data;limited labeled;labeled data;conjunction weak;labeled data scenarios;labeled;machine learning discuss;context programmatic weak;recent advances weak;weak;tackling limited labeled;advances weak;machine learning;sph conjunction weak;sph machine learning", "pdf_keywords": "labeling supervision proposed;labeling supervision;programmatic weak supervision;supervision label models;weak supervision label;synthesizing weak supervision;weak supervision models;heuristic labeling supervision;supervision models data;weak supervision method;approach weak supervision;sets weak supervision;predict labeling;weak supervision generative;supervision label;models labeling;supervision models;end models labeling;process predict labeling;weak supervision;labeling process predict;labeling data sets;weak supervision area;labeling tasks;labeled data;labeling data;labeling tasks suitable;limited labeled data;approach labeling;supervised"}, "66340a93813d8f816a8c82354a8f39fa985de27f": {"ta_keywords": "knowledge conceptnet;knowledge conceptnet tandem;background knowledge conceptnet;conceptnet;textual entailment trained;text large corpus;generic textual entailment;textual entailment;retrieve supporting text;conceptnet tandem;entailment trained task;large corpus science;generic textual;related text;related text rewriter;large corpus;textual;corpus science;supporting text;corpus;science related text;entailment trained;corpus science related;background knowledge;entailment;incorporate background knowledge;tandem generic textual;support retrieved results;question queries;question queries used", "pdf_keywords": ""}, "2ac6b8ade2a5e1ac89b99012ca6548eca4f8323f": {"ta_keywords": "topological layer robust;layer robust deep;robust deep learning;novel topological layer;topological layer;robust deep;topological features data;topological features;represent topological features;based persistent landscapes;persistent landscapes;layer robust;persistent landscapes proposed;layer based robust;represent topological;layer robust noise;novel topological;robust generative;learning based persistent;robust generative function;propose novel topological;based robust generative;proposed layer robust;landscapes proposed layer;topological;deep learning;layer classification;used represent topological;proposed layer classification;deep learning based", "pdf_keywords": "topological persistent;topological layer robust;topological layer crucial;networks persistent homology;topological layer based;novel topological layer;neural networks persistent;layer topological information;topological persistent homology;classify data topological;topological layer;persistent homology network;layer global persistence;efficiently represent topological;topological layer characterization;present topological persistent;layer topological;topological data;proposed topological layer;layer global topological;data topological;topological layer global;topological layer finally;topological features deep;layer characterization topological;global topological layer;characterization topological layer;topological layer general;set persistence landscapes;topological layer study"}, "1fa02e5a5adffe82a41225f61f5f8ce86cf229d0": {"ta_keywords": "new segmentation model;segmentation model;segmentation model combines;segmentation;new segmentation;propose new segmentation;random field model;field model normalized;field model statistical;random field;normalized model;model normalized;field model;properties random field;normalized model model;model normalized model;statistical properties normalized;combines random field;normalized;properties normalized model;field;model based statistical;model statistical;properties normalized;model combines random;model statistical properties;statistical properties random;statistical;based statistical properties;statistical properties", "pdf_keywords": ""}, "1cfd9b1db68fc320698da05fc6876dd0ea96fc9b": {"ta_keywords": "connectionist temporal classification;speech recognition;speech recognition model;recognition model mobile;based connectionist temporal;automatic speech recognition;temporal classification gc;temporal classification;pruning layer model;devices based connectionist;layer pruning;connectionist temporal;pruning layer;automatic speech;end automatic speech;layer pruning process;connectionist;method pruning layer;based connectionist;size layer pruning;pruning method end;recognition model;proposed method pruning;pruning;classification;present pruning;pruning method;recognition;model mobile devices;classification gc", "pdf_keywords": "deep connectionist;pruning neural networks;pruning based connectionist;speech recognition;connectionist temporal classification;pruning neural;structure deep connectionist;end speech recognition;speech recognition asr;approach tolayer pruning;automatic speech recognition;pruning retaining regularization;method pruning neural;present deep learning;deep learning;recognition asr model;ctc layer pruning;layer pruning based;recognition asr;layer pruning;speech recognition approach;pruning based layer;approaches pruning layer;regularization training;pruning layer;deep learning models;temporal classification ctc;deep connectionist spike;structure deep learning;automatic speech"}, "acbb4495dd698b3190db6899d7d35b0817e0a85e": {"ta_keywords": "adversarial networks convex;generalization properties generative;gradient descent proximal;trained minimax;generative adversarial networks;properties generative adversarial;generative adversarial;generalization performance trained;adversarial networks;performance trained minimax;standard gradient descent;adversarial;trained minimax model;networks convex concave;networks convex;gradient descent;generative;generalization performance;minimax;descent proximal;descent proximal point;generalization;minimax model;properties generative;generalization properties;concave non convex;convex non concave;concave settings optimization;non convex;convex concave", "pdf_keywords": "generalization minimax;generalized minimax learning;convex minimax;convex minimax objective;analyzing generalization minimax;generalized minimax;minimax optimization algorithms;optimization algorithms minimax;propose generalized minimax;optimal minimax optimization;minimax learning;minimax optimization;minimax optimization method;generalization properties minimax;non concave minimax;optimality convex minimax;performance learned minimax;minimax learning stable;algorithms minimax;stochastic minimax objective;analyze minimax learning;stochastic minimax;minimax problems generalization;gradient descent ascent;new optimal minimax;optimal minimax;minimax optimization analyze;risk minimax learning;learned minimax;standard gradient descent"}, "0053f75b7053f43b9787a9955426281e672b147b": {"ta_keywords": "predicting shape sentence;predict shape sentence;deep inside supervised;shape given message;generative model predict;inside supervised;generative model;given message approach;predicting shape;predicting shape given;message approach based;message approach;predict shape;inside supervised methods;supervised;approaches predicting shape;generative;predicting;use generative;supervised methods;shape sentence;model predict shape;approaches predicting;existing approaches predicting;use generative model;problem predicting shape;given message;predict;based combination deep;trees approach", "pdf_keywords": "unsupervised parsing;learning trees raw;unsupervised parsing simultaneously;parsing simultaneously learns;experiments unsupervised parsing;parser trained autoencoder;unsupervised parsing method;supervised parsing;unsupervised parsing segment;parser trained external;chart parser trained;parser trained;outside recursive autoencoders;parser learn;based unsupervised parsing;learns syntactic structure;parsing able predict;chart parser learn;parser learn syntactic;tree chart parser;learns syntactic;recursive autoencoders;learning trees;unsupervised learning trees;recursive autoencoders diora;learning predict semantics;parsing segment recall;method unsupervised parsing;trees raw text;learning trees binary"}, "95e8edd26744ecc2bc23996cfaa68fe6252442a9": {"ta_keywords": "fake news detection;claim aware fake;semantic sneture mining;aware fake news;graph based semantic;evidences contextual semantic;context claims;based semantic sneture;claim aware;problem claim aware;semantic sneture;evidences contextual;claims;empirical evidences contextual;sneture mining approach;fake news;detection based graph;methods context claims;contextual semantic information;contextual semantic;news detection;sneture mining;based semantic;aware fake;claim;graph based;semantic information;semantic information reduce;semantic;based graph based", "pdf_keywords": "claim evidence graphs;semantics evidences graphs;claim evidence graph;evidence graph structured;graph based evidences;evidence graphs graph;evidence graphs;evidence graph;graph based semantic;graph based semantics;fake news graph;evidence aware semantic;semantic relationship graphs;evidences graphs;evidences graphs verify;evidences modeled graph;embedding claim evidence;extract semantics evidences;semantics evidences;semantic sneture mining;graph based fake;claim veracity learned;uncover veracity claim;graph knowledge underlying;evidence level representations;claims evidences modeled;belief propagation semantic;semantic structures claim;verify veracity news;graph structured data"}, "452059171226626718eb677358836328f884298e": {"ta_keywords": "questions model trained;question answering;dynamic memory network;question answering sentiment;dynamic memory;introduce dynamic memory;natural language input;questions generates;questions model;tasks natural language;speech tagging;answering sentiment analysis;natural language processing;including question answering;sequences questions model;language input sequences;sequences questions generates;memory network;analysis speech tagging;questions generates relevant;answering sentiment;natural language;memory;input sequences model;sentiment analysis speech;memory network ddem;generates relevant answers;language processing;number natural language;input sequences", "pdf_keywords": "memory natural language;neural network recurrent;recurrent network;recurrent neural network;recurrent network model;network recurrent;network recurrent network;memory neural network;memory neural;recurrent neural;question answering;simple recurrent neural;question answering tasks;tasks natural language;accomplished simple recurrent;answer natural language;network novel recurrent;recurrent neural networkwe;episodic memory module;general question answering;question answering based;nlp tasks;language question answering;generating episodic memory;natural language representation;generalization memory neural;encoding natural language;processing nlp tasks;novel recurrent neural;question answering problems"}, "c39ac49e2d3feec992e84868256cb0a0ff028346": {"ta_keywords": "distributed convex optimization;optimal distributed convex;distributed optimization;use distributed optimization;distributed optimization framework;method optimal distributed;distributed convex;optimal distributed;convex optimization;convex optimization dimensional;optimization framework;optimization framework method;optimization;use distributed;convex;new method optimal;method shown optimal;distributed;bound oracle rate;optimization dimensional method;shown optimal;method optimal;optimal;optimization framework use;framework use distributed;combination use distributed;bound oracle;novel method oracle;optimization dimensional;oracle rate method", "pdf_keywords": "decentralized convex optimization;decentralized distributed convex;distributed convex optimization;gradient decentralized convex;convex optimization distributed;advances decentralized convex;decentralized convex;applied decentralized convex;decentralized algorithm convex;consider decentralized convex;convex consider decentralized;decentralized optimization stochastic;consider distributed optimization;consider decentralized optimization;decentralized optimization smooth;decentralized optimization following;convergent decentralized convex;decentralized optimization;decentralized optimization effective;problem decentralized optimization;novel distributed convex;decentralized optimization introduced;distributed optimization;method decentralized optimization;conjecture decentralized optimization;decentralized optimization method;distributed optimization problems;distributed convex;rate distributed optimization"}, "affb8d759af00540458c19696532220dd1c1373a": {"ta_keywords": "word text recognition;recognize vocabulary words;text recognition;open vocabulary sub;recognize vocabulary;sub word text;vocabulary sub word;novel open vocabulary;able recognize vocabulary;vocabulary sub;deep neural network;vocabulary words range;hidden model dnn;word text;text recognition using;model dnn hmm;text datasets;open vocabulary;vocabulary words;words range 6k;recognition using deep;used text datasets;deep neural;using deep neural;words range;model dnn;vocabulary;sub word;neural network;languages able recognize", "pdf_keywords": ""}, "5c5bedaf66cadebbcd9116f38acd3df9ed43d816": {"ta_keywords": "paul wavelet transform;optimized paul wavelet;wavelet transform;used wavelet functions;wavelet functions;wavelet transform eopwt;wavelet functions demonstrate;wavelet;continuous wavelet;use continuous wavelet;continuous wavelet flexible;wavelet flexible;commonly used wavelet;used wavelet;paul wavelet;wavelet flexible commonly;spectral decomposition data;time frequency analysis;transform eopwt based;implementing spectral decomposition;entropy optimized paul;entropy optimized;spectral decomposition;analysis called entropy;transform eopwt;frequency analysis;carbonate gas reservoir;frequency analysis called;gas reservoir southwestern;called entropy optimized", "pdf_keywords": ""}, "bc247abf8180f583a42de392e4f7d2b2a41ad72d": {"ta_keywords": "parser independent interactive;questions novel parser;text database;enhancing text database;novel parser independent;users using multichoice;independent interactive approach;text database performance;parser independent;using multichoice questions;interactive approach capable;parser;interactive;novel parser;multichoice questions novel;independent interactive;interactive approach;interactive approach piia;present novel parser;interacts users using;using multichoice;cross domain databases;interacts users;database;multichoice questions;domain databases demonstrate;databases;multichoice;piia interacts users;domain databases", "pdf_keywords": "parser independent interactive;parser integrated;interacting users parsers;proposed parser independent;interaction parsers;presents parser integrated;uses parsers independent;parser independent interaction;proposed parser;called interaction parsers;parsers using;propose parser independent;interaction parser;use interaction parser;users parsers using;users parsers;systems proposed parserwe;approach named parser;uses parsers;parsers;interaction semantic parsing;semantic parsing process;parsers independent user;choices parser training;propose parser;interaction parser user;parser training;named parser independent;parsers independent;parser"}, "c5bcb690b0aa85ad0a5fd7e7aa4b8c468cd8c69a": {"ta_keywords": "discriminative training framework;discriminative training;train discriminative training;margin estimation provide;margin estimation;discriminative training new;novel discriminative training;train discriminative;field margin estimation;discriminative;novel discriminative;presents novel discriminative;way train discriminative;energy dmmi minimum;minimum energy dmmi;dmmi minimum energy;training framework;minimum energy minimum;problems field margin;cost minimum energy;energy minimum;minimum energy xmath0;energy minimum energy;framework based minimum;minimum energy;dmmi minimum;training framework based;margin;estimation provide flexible;error statistics framework", "pdf_keywords": ""}, "f300a62d0522d9a623b62f1305052928d8d7170c": {"ta_keywords": "detection emojis using;detection emojis;emojis using subset;method detection emojis;emojis using;emojis;extract interesting features;dataset interesting features;interesting features data;dataset interesting;datasets area social;create dataset interesting;interesting features;area social media;social media use;datasets;largest datasets;features data;social media;dataset;subset largest datasets;extract interesting;balanced dataset;largest datasets area;features;detection;dataset use;datasets area;create balanced dataset;create dataset", "pdf_keywords": ""}, "50851e9e16b52e14c422b6e937cfd3ed063b6998": {"ta_keywords": "cross lingual encoder;amber multilingual cross;lingual transfer learning;amber multilingual;lingual encoder trained;lingual encoder;multilingual cross lingual;multilingual cross;align multilingual representations;cross lingual;cross lingual transfer;multilingual representations;shot cross lingual;encoder trained parallel;lingual transfer;multilingual;align multilingual;lingual;multilingual representations different;objectives align multilingual;trained parallel data;encoder trained;transfer learning;sequence tagging;trained parallel;transfer learning different;encoder;tasks amber achieves;score sequence tagging;accuracy retrieval large", "pdf_keywords": "training multilingual encoders;multilingual bidirectional encoder;multilingual encoders;multilingual encoders amber;encoders amber multilingual;learning cross lingual;lingual transfer learning;multilingual machine translation;predicting multilingualwe present;amber multilingual bidirectional;training multilingual;multilingual machine;predicting multilingualwe;multilingual representations;multilingual learning;multilingual bidirectional;lingual representation learning;encoded parallel corpus;cross lingual task;predict correct translation;align multilingual representations;multilingual learning present;crucial multilingual learning;linguistic cross lingual;masked language modeling;translation strategy predicting;cross lingual tasks;method training multilingual;language modeling masked;including cross lingual"}, "37e06f3622c17dc6194b547c944462b2a513b878": {"ta_keywords": "consistency generated summaries;corrections generated summaries;factual correction models;generated summaries span;generated summaries;summaries span selection;factual consistency generated;question answering models;summaries span;generated summaries sacrificing;learned question answering;answering models;question answering;answering models make;suite factual correction;boost factual consistency;summaries sacrificing summary;sacrificing summary quality;summary quality;make corrections generated;correction models;summaries sacrificing;summaries;factual consistency;corrections generated;consistency generated;factual correction;suite factual;sacrificing summary;significantly boost factual", "pdf_keywords": "correctness summarization models;consistency generated summaries;improves summarization quality;factuality summaries generated;improve summarization quality;quality generated summaries;improve factuality summaries;accuracyabstractive sentence summarization;summarization quality;summarization capable generating;summarization quality sacrificing;standard summarization quality;summarization leveraging fact;correctors improve summarization;learning summaries;abstractive summarization model;summarization quality multiple;significantly improves summarization;learning summaries compared;abstractive summarization systems;facts generated summaries;summarization correct factuality;sentence summarization capable;summarization datasets demonstrate;enhance abstractive summarization;summarization models;factual correctness summarization;correctness summarization;abstractive summarization leveraging;generated summaries"}, "e487f2508e5f62b2745a2e56ceb3c601c286d2e3": {"ta_keywords": "xmath1 xmath2 xmath3;xmath2 xmath3 xmath4;xmath16 xmath17 xmath18;xmath15 xmath16 xmath17;xmath17 xmath18 xmath19;xmath0 xmath1 xmath2;xmath3 xmath4 xmath5;xmath14 xmath15 xmath16;xmath12 xmath13 xmath14;xmath13 xmath14 xmath15;xmath4 xmath5 xmath6;xmath1 xmath2;xmath11 xmath12 xmath13;xmath10 xmath11 xmath12;xmath6 xmath7 xmath8;xmath17 xmath18;xmath9 xmath10 xmath11;xmath18 xmath19 xmath20;xmath3 xmath4;xmath5 xmath6 xmath7;xmath16 xmath17;xmath7 xmath8 xmath9;xmath2 xmath3;xmath8 xmath9 xmath10;xmath19 xmath20 xmath21;xmath15 xmath16;xmath18 xmath19;xmath13 xmath14;xmath14 xmath15;xmath11 xmath12", "pdf_keywords": ""}, "705e6b53f88ec733e3c186c6232c41b268248c01": {"ta_keywords": "netflix prize dataset;movie ratings netflix;ratings netflix prize;ratings netflix;netflix prize;movie ratings;sparse data;analysis sparse data;social choice;real movie ratings;play analysis sparse;prize dataset;sparse data illustrate;ratings;social choice highlighting;inference behavioral modeling;behavioral perspective social;netflix;perspective social choice;inference behavioral;sparse;analysis sparse;behavioral modeling;behavioral modeling play;social;roles inference behavioral;behavioral perspective;behavioral;data;modeling play analysis", "pdf_keywords": ""}, "740182c3aa9a3045fcd9370269d446455ae9f623": {"ta_keywords": "finite state transducers;state transducers;neural finite state;state transducers fnts;fast inference;detect sense motion;training inference;transducers;motion moving parts;accurate fast inference;present training inference;neural finite;training inference algorithms;introduce neural finite;strings present training;sense motion;fast inference variety;neural;sense motion moving;transducers fnts;transducers fnts define;detect sense;inference variety tasks;inference algorithms locally;inference algorithms;moving parts;motion moving;motion;distributions pairs strings;moving parts demonstrate", "pdf_keywords": ""}, "ba159dbf205193d0cb7c9c18dd01f830d2f56eb8": {"ta_keywords": "structure natural language;structure text semantic;characterizing semantics natural;semantics natural language;characterize semantics text;text characterize semantics;text semantic;text semantic content;semantic content text;semantics natural;intrinsic structure text;natural language;semantics text;semantics text using;method characterizing semantics;semantic content;characterizing semantics;characterize semantics;structure text;semantic;semantics;text provide statistical;natural language use;intrinsic structure natural;characterizing intrinsic structure;structure natural;characterize intrinsic structure;structure text provide;comparison intrinsic structure;text using information", "pdf_keywords": ""}, "b694472c13420acb599a5b1d25d5f2bd42eb8c1b": {"ta_keywords": "novo assembly algorithm;assembly algorithm;assembly algorithm requires;novel novo assembly;novo assembly;assembly;propose novel novo;minimum data efficient;novel novo;data efficient respect;requires minimum data;algorithm inspired information;data efficient;algorithm requires;computation algorithm inspired;minimum data;algorithm;space computation algorithm;novo;algorithm inspired;computation algorithm;algorithm requires minimum;using minimum data;efficient respect space;space computation;data;respect space computation;inspired information theoretic;information theoretic;information theoretic perspective", "pdf_keywords": ""}, "71d649dcb3dee2ca57d0775a9679cb68f82f22d5": {"ta_keywords": "snn training data;predictive neural network;feature extraction;feature extraction data;neural network snn;performance snn demonstrated;predictive neural;data set snn;extract acoustic summary;integrate feature extraction;acoustic summary data;ability extract acoustic;snn method;network snn method;network snn;set predictive neural;improve performance snn;snn demonstrated snn;snn method relies;snn training;demonstrated snn training;extract acoustic;demonstrated snn;performance snn;snn demonstrated;neural network;data set predictive;training data;extraction data set;set snn", "pdf_keywords": ""}, "0be998fffc5f44496042f7757fb2ffa8924e54cd": {"ta_keywords": "stochastic data entry;trained stochastic data;trained stochastic;differentiable data selection;data selection;data selection ddd;data entry;data entry model;classification tasks ddd;translation image classification;reinforcement learning;stochastic data;data scoring network;content machine learning;stochastic stochastic data;score stochastic stochastic;data scoring;score stochastic;learning model;network trained stochastic;machine learning;scoring network trained;reinforcement learning approach;present reinforcement learning;learning model improve;optimization data scoring;quality score stochastic;machine learning model;selection ddd;learning approach", "pdf_keywords": "shot learning multilingual;risk shot learning;reward function training;performance shot learning;shot learning variety;training scorer network;learning optimized task;shot learning;learning optimized;meta learning;optimizing training data;optimizing learning rate;optimize training data;multilingual neural networks;training network scoring;optimizing learning;function training network;learning rate optimized;field meta learning;training domain adaptation;learning adaptation;optimizing training;scorer adaptive;image classification multilingual;learning variety multilingual;learning multilingual;function training;learning framework optimizing;reinforcement learning slk;learning reinforcement"}, "c5bb38b8e3ce21063670dfd81ac64dcb2ecf10b2": {"ta_keywords": "frequencies spectral notches;pinna spectral notches;spectral notches proposed;spectral notches;compute notches using;compute notches;notches observed images;notches using linear;notches proposed method;used compute notches;finer contours notches;contours notches;notches observed pin;notches observed;overlaying notches observed;notches using;contours notches overlaying;modeling pinna spectral;notches proposed proposed;notches overlaying notches;comparison notches observed;overlaying notches;notches proposed;notches overlaying;computation frequencies spectral;comparison notches;notches;frequencies spectral;pinna spectral;validated comparison notches", "pdf_keywords": ""}, "8abd724b770348bd21b16b9aaf2ba0a77596b2ed": {"ta_keywords": "speaker wise attractors;sequentially generates speaker;speakers introduce encoder;decoder based attractor;end neural diarization;generates speaker wise;neural diarization;generates speaker;attractor calculation module;attractor calculation;attractors basis sequence;neural diarization eend;based attractor calculation;number speakers introduce;attractors basis;speaker wise;based attractor;attractor;diarization eend;number attractors;number speakers;attractors;speakers introduce;introduce encoder;end end neural;wise attractors basis;unknown number speakers;end neural;wise attractors;diarization eend method", "pdf_keywords": ""}, "af787fda38ce6fa1d14ad2fb8568088faf973a21": {"ta_keywords": "mode harmonic oscillator;calculation spectral density;harmonic oscillator frequency;spectral density single;spectral density;oscillator frequency comb;applicable harmonic oscillator;single mode harmonic;harmonic oscillator;applied harmonic oscillator;calculation spectral;generated harmonic oscillator;comb generated harmonic;harmonic oscillator method;oscillator frequency;method calculation spectral;bose einstein condensate;mode harmonic;condensate applicable harmonic;frequency comb generated;density single mode;oscillator method;frequency comb;oscillator method based;oscillator method applied;generated harmonic;method applied harmonic;spectral;applicable harmonic;applied harmonic", "pdf_keywords": ""}, "be312e930f6739a709e60547aa0dfb9c3dc44497": {"ta_keywords": "multilingual machine translation;multilingual lexicon encoding;multilingual lexicon;multilingual machine;purpose multilingual machine;novel multilingual lexicon;translation nmt encoding;multilingual;state art multilingual;machine translation nmt;encoding lexicon;novel multilingual;lexicon encoding;decoupled encoding lexicon;propose novel multilingual;purpose multilingual;encoding lexicon designed;lexicon encoding framework;translation nmt;machine translation;framework purpose multilingual;nmt encoding;nmt encoding based;low resource languages;art multilingual;soft decoupled encoding;lexicon designed share;shared languages;word latent space;encoding based soft", "pdf_keywords": "multilingual neural networks;multilingual neural machine;multilingual neural;predicting words multilingual;accuracy multilingual neural;multilingual lexicons;methods multilingual neural;encoding words multilingual;multilingual translation capacity;words multilingual lexicons;translation nneural networks;method multilingual neural;multilingual translation;large monolingual data;word embedding learn;improve accuracy multilingual;multilingual nmt achieves;embedding pre translation;neural machine translation;small monolingual data;words multilingual languages;word embedding performance;multilingual languages;multilingual nmt;words multilingual;machine translation nneural;multilingual lexicons obviates;multilingual;robust small monolingual;sentences multilingual translation"}, "e961c8de1df75f70254656e98ca82f9d9fbd640c": {"ta_keywords": "compressive phase retrieval;algorithms compressive phase;known phase retrieval;unconstrained phase sparser;classical phase retrieval;phase retrieval based;phase sparser signal;compressive phase;sparse graph coding;phase retrieval problem;sparse graph codes;phase retrieval;efficient algorithms compressive;phase sparser;algorithms compressive;retrieval based sparse;algorithms based sparse;sparse graph;sparser signal;based sparse graph;compressive;graph codes algorithms;vectors unconstrained phase;unconstrained phase;graph coding;sparser signal classical;graph codes;based sparse;graph coding framework;codes algorithms", "pdf_keywords": "compressive phase retrieval;phase retrieval sparse;using compressive phase;recovering sparse signal;compressive phase;phase retrieval robustified;recovery sparse signals;general compressive phase;reconstruct sparse signal;friendly compressive phase;novel compressive phase;consider phase retrieval;problem compressive phase;phase retrieval based;phase retrieval algorithm;decades phase retrieval;phase retrieval signals;phase retrieval;retrieval sparse signals;fidelity phase retrieval;set phase retrieval;complexity phase retrieval;fourier friendly compressive;phase retrieval problem;recovering sparse;sparse signal;sparse signals;propose phase coding;measurement phase retrieval;phase retrieval regime"}, "e2a4e1a9f8e66baf12a49a3e5d8e33291f9347e7": {"ta_keywords": "text entity linking;neural semantic matching;aggregated semantic matching;entity linking;short text entity;entity linking proposed;semantic matching models;semantic matching;aggregated semantic;semantic matching ess;neural semantic;jointly disambiguation rank;linking;jointly disambiguation;tweet datasets;text entity;disambiguation rank aggregation;based neural semantic;framework aggregated semantic;semantic;public tweet datasets;disambiguation rank;linking proposed;captures semantic;semantic information local;short text;captures semantic information;disambiguation;model captures semantic;semantic information", "pdf_keywords": ""}, "f951aad88e244182b37e4918c3d570560108c68c": {"ta_keywords": "robust classifiers perceptually;robust classifiers;adversarially robust classifiers;constructing robust classifiers;robust classifiers based;adversarially robust;gradients property robust;perceptually aligned gradients;classifiers perceptually aligned;constructing adversarially robust;aligned gradients perceptually;property robust classifiers;gradients perceptually aligned;classifiers perceptually;classifiers based perceptually;classifiers;gradients perceptually;constructing robust;constructing adversarially;adversarially;based perceptually aligned;robust;classifiers based;method constructing robust;aligned gradients;means constructing adversarially;gradients occur randomized;gradients;perceptually aligned;aligned gradients property", "pdf_keywords": "adversarially robust;adversarially use smoothed;adversarially compute gradient;constructing adversarially robust;adversarial attacks convex;adversarially trained;robust classifiers adversarially;robustness deep learning;adversarially robust classifiers;trained adversarially;learning classifier adversarial;classifier adversarial;adversarially trained neural;classifiers adversarially trained;trained adversarially compute;classifiers adversarially;robustness deep;demonstrate robustness deep;adversarial;adversarially compute;image gradient ascent;adversarially;constructing adversarially;large adversarial;perceptually aligned gradients;adversarial examples randomized;trained adversarially new;network trained adversarially;networks trained adversarially;robust neural networks"}, "5eaa425af39339e0ae30202b348cc6e253813993": {"ta_keywords": "scientific information retrieval;retrieval designed russian;information retrieval designed;information retrieval;retrieval designed;citation indexes facilitating;citation indexes;scientific information;retrieval;language uses patents;contracts citation indexes;indexes facilitating expertise;present scientific information;russian language;expertise process comparison;russian language uses;expertise process;designed russian language;facilitating expertise process;russian;expertise;patents;indexes;facilitating expertise;scientific;indexes facilitating;uses patents;government contracts citation;patents government;designed russian", "pdf_keywords": ""}, "0761a69310f7b8f4ab01495f31a30c6fe53d83b8": {"ta_keywords": "adaptive adaptation acoustic;adaptation acoustic model;adaptation acoustic;posterior distributions adaptation;integrate multiscale adaptation;multiscale adaptation;multiscale adaptation scheme;robust speech recognition;robust speech;distributions adaptation achieved;distributions adaptation;realize robust speech;integrate adaptive adaptation;acoustic model language;adaptive adaptation;speech recognition;speech recognition performance;adaptation achieved updating;acoustic model;time dependent stochastic;updating posterior distributions;dependent stochastic evolution;adaptive;adaptation scheme;stochastic evolution posterior;integrate adaptive;method integrate adaptive;adaptation;adaptation achieved;dependent stochastic", "pdf_keywords": ""}, "225767ce707781d0114815068c355622869ee642": {"ta_keywords": "emergence cognitive process;cognitive signal model;cognitive signal;cognitive process;cognitive process set;emergence cognitive;model emergence cognitive;cognitive task resulting;cognitive task;specific cognitive task;cognitive;specific cognitive;stochastic process generates;constructed stochastic process;stochastic process;model constructed stochastic;constructed stochastic;task resulting model;stochastic;associated specific cognitive;model emergence;presence cognitive signal;present model emergence;data model based;signal model constructed;process;detect presence cognitive;process set data;data model;hypothesis able detect", "pdf_keywords": ""}, "91184a2d40be8a0171b5c926b336666ed717ec6e": {"ta_keywords": "scientific digital libraries;digital libraries;digital libraries expected;digital libraries focus;digital libraries resulting;digital libraries tutorial;structure scientific digital;selection scientific digital;scientific digital;libraries resulting scientific;libraries;resulting scientific digital;biases tutorial written;biases tutorial;libraries tutorial intended;free biases tutorial;libraries tutorial;libraries focus;tutorial structure scientific;libraries expected free;structure scientific;digital;bias selection scientific;libraries focus paper;paper structure scientific;libraries expected;bias selection;introduction problem bias;biases;libraries resulting", "pdf_keywords": ""}, "7d148b46f45e935765e56887d720492b2b716e55": {"ta_keywords": "reconstruction theory networked;reconstruction networks biology;networks biology;intrinsic dynamics collective;reconstruction networks;dynamics collective dynamics;collective dynamics proposed;dynamics collective;collective dynamics;theory networked systems;networks biology social;theory networked;networked systems intrinsic;dynamics proposed reconstruction;model independent reconstruction;independent reconstruction theory;reconstruction theory scalable;reconstruction theory;proposed reconstruction theory;systems intrinsic dynamics;role reconstruction networks;independent reconstruction;scalable larger networks;networks;larger networks;intrinsic dynamics;networked systems;dynamics;systems intrinsic;networks play", "pdf_keywords": "neural dynamics;neuron network represented;dynamics neuron;neuron network;dynamics neuron determined;neuron network stochastic;neurons spiking patterns;neurons random network;neurons network present;collective dynamics network;spiking patterns networks;dynamics generate neural;neurons spiking;reconstruct neurons spiking;neurons network;approach neural dynamics;inputs dynamics neuron;synaptic inputs dynamics;stochastic neuron network;connectivity neuron;connected synaptic connectivity;neurons network introduce;collective dynamics generate;synaptic connectivity neuron;connections neurons random;synaptic connectivity;networks neurons;represent neuron state;dynamics network dynamical;intrinsic collective dynamics"}, "d3dd80269f2542cc173afb3a1df24b582a1e4af2": {"ta_keywords": "accuracy transformers;examine accuracy transformers;accuracy transformers used;transformer determined accuracy;transformer determined;transformers used distinguish;choice transformer determined;input choice transformer;transformer;choice transformer;transformers used;transformers;determined accuracy cross;accuracy cross section;accuracy cross;determined accuracy;right determined accuracy;normalization cross;simple normalization cross;accuracy;position input improved;models input choice;different models input;position input;examine accuracy;distinguish different models;models input;input moving right;choice position input;normalization", "pdf_keywords": "transformer representations strings;language bit strings;languages accuracy transformers;strings transformer constructed;strings transformer;strings construct transformers;transformers recognize languages;language longer strings;machine translation transformers;input strings transformer;encodings convolutional;encoding string length;bit strings;transformer encoder word;representations strings arbitrary;constructing transformer encoder;encodings;languages ability transformers;efficiently xmath0 encoding;encodings encoder;representations strings;transformer encoder;generalize longer strings;subnetworks encoder;encoding string;string length alphabets;encoding;strings trained recognize;encoder;encodings convolutional neural"}, "920257774e2caee8a8c74968c64c10bcb79a136c": {"ta_keywords": "propagation function pb;propagation function transmission;function transmission lines;genetic algorithm vector;hybrid genetic algorithm;transmission lines proposed;vector fitting based;transmission lines;algorithm vector fitting;propagation function;vector fitting;characterization propagation function;approximation propagation function;genetic algorithm;proposed approximation propagation;fitting based approach;estimate propagation function;propagation;estimate propagation;pb mixtures estimate;based hybrid genetic;approximation propagation;pbpb mixtures proposed;method characterization propagation;characterization propagation;function transmission;pb pbpb mixtures;pbpb mixtures;algorithm vector;used estimate propagation", "pdf_keywords": ""}, "147b954ba0881d643706c918e017f7d66a15b827": {"ta_keywords": "models students cognitive;learning student representation;student cognitive skills;student representation learning;instruction intelligent tutoring;intelligent tutoring;students cognitive;learning models students;student cognitive;representation student skills;intelligent tutoring systems;student representation;tutoring systems;tutoring;representation learning student;features student cognitive;combination student representation;detailed representation student;instruction cognitive science;student skills;instruction cognitive;students cognitive context;models students;learning student;cognitive skills;cognitive model best;representation student;cognitive model;cognitive models important;cognitive models", "pdf_keywords": ""}, "3426f000673aae995a55ade9273c842bb484ad18": {"ta_keywords": "multilingual phoneme recognizers;phoneme recognizers monolingual;crosslingual phoneme recognizers;detection phoneme boundaries;automatic detection phoneme;detection phoneme;phoneme recognizers;phoneme recognizers classify;phoneme boundaries audio;recordings unknown language;multilingual phoneme;detect boundaries multilingual;language detect boundaries;monoand multilingual phoneme;language detect;recognizers monolingual;multilingual crosslingual phoneme;crosslingual phoneme;detected segments phoneme;phoneme boundaries;unknown language detect;boundaries audio recordings;boundaries audio;segments phoneme;boundaries multilingual crosslingual;boundaries multilingual;recognizers monolingual gold;segments phoneme units;automatic detection;bantu language", "pdf_keywords": ""}, "89c2cbdf1a5049a4068ca9215aa8859a1a97b1a3": {"ta_keywords": "contextual bandit learning;algorithm contextual bandit;bandit learning;contextual bandit;bandit learning learner;bandit;reward action approach;learning learner repeatedly;reward action;observes reward action;cost sensitive classification;observes reward;actions observes reward;achieves statistically optimal;learner repeatedly takes;solving fully supervised;reward;learner repeatedly;supervised cost sensitive;learning learner;statistically optimal performance;classification problems achieves;fully supervised cost;learning;supervised cost;algorithm contextual;statistically optimal;new algorithm contextual;supervised;classification", "pdf_keywords": "contextual bandit learning;algorithm contextual bandit;actions contextual bandit;based contextual bandit;contextual bandit;contextual bandit problem;bandit learning;contextual bandit setting;bandit learning problem;bandit problem learner;version contextual bandit;bandit problem;bandit problem ucb;bandit setting;learner superior randomized;randomized coordinate descent;bandit setting uses;bandit;randomized learning;randomized learning thewe;strategy coordinate descent;context observes reward;learning use stochastically;statistically optimal regret;policies efficiently sampled;efficient randomized;superior randomized learning;distribution maximum reward;regret policy maximizes;given strategy contextual"}, "89f7db77a755d44d3aabdbcc7549b743d7debcc5": {"ta_keywords": "preference networks;preference networks expressed;preference relations dependence;predicting preferences;predicting preferences set;formalism predicting preferences;preference relations;preference relation;individual preference relations;structure preference networks;structure individual preference;choice preference relation;links preference networks;preferences features;preferences features particular;dependence structure preferences;preferences set objects;dependence structure preference;structure preferences features;dependence links preference;uncertainty choice preference;individual preference;choice preference;preferences;preference;structure preferences;preferences set;dependence structure;structure preference;relations dependence structure", "pdf_keywords": ""}, "18a82459d495fa3ad22a60bd7c9527df8bd55e1e": {"ta_keywords": "distributed gret learning;algorithm network games;network games;deterministic network;gret learning algorithm;distributed gret;gret learning;propose distributed gret;case deterministic network;network subjected teams;learning algorithm network;deterministic network subjected;available observations players;dual averaging;algorithm network;learning algorithm;network games uses;observations players;distributed;teams distinct objectives;dual averaging locally;propose distributed;observations players provide;averaging locally available;players provide analysis;uses dual averaging;case network;averaging locally;averaging;network", "pdf_keywords": ""}, "341f6353547f4a58fdf11fbcc9de3a31083a619b": {"ta_keywords": "harmonics einstein field;einstein field method;einstein equations method;calculation energy einstein;harmonics einstein;energy einstein field;dimensional harmonics einstein;exact solution einstein;solution einstein equations;einstein equations;einstein field;solution einstein;energy einstein;calculation energy;applied calculation energy;einstein;dimensional harmonics;obtaining exact solution;combination dimensional harmonics;method obtaining exact;equations method;equations method based;field method;field method applied;harmonics;method applied calculation;exact solution;obtaining exact;equations;energy", "pdf_keywords": ""}, "7a79099447bef9a3ea13b1dc409d04b3dff57320": {"ta_keywords": "generative structure music;underlying music model;structure music based;music model;properties underlying music;structure music;music model sequence;underlying music relative;underlying music;music relative velocity;music relative;underlying track model;amplitude underlying track;music based novel;music;underlying track;music based;constructing generative structure;generative structure;underlying track relative;model constructing generative;track relative amplitude;track model;events model structural;track model available;constructing generative;track;generative;model constructing;velocity underlying track", "pdf_keywords": "music sequence model;automatic music composition;music information retrieval;music information processing;music modeling generation;generate music;music sequence;music modeling;generation music sequence;generate music arbitrary;music compositions method;neural network music;generated music;use music information;automatic music;approach music information;based music modeling;generate new music;stochastically generated music;music composition;framework automatic music;music composition knowledge;musical score sequence;resulting music using;music compositions;music information;events construct music;music discovery modeling;generated music style;transforming musical score"}, "e9d8db4f5b5c106c43a268f635788c0a94b2916a": {"ta_keywords": "stochastic gradient descent;variants stochastic gradient;gradient descent ascent;optimization variational inequalities;descent ascent methods;analysis stochastic gradient;stochastic gradient;optimization variational;gradient descent;ascent methods;ascent methods solving;solving optimization variational;new variants stochastic;variants stochastic;ascent methods including;convergence analysis stochastic;randomization distributed variants;stochastic estimates;descent ascent;distributed variants compression;variational inequalities problems;variety stochastic gradient;variational inequalities;stochastic estimates applicable;variants arbitrary sampling;variants compression;reduction coordinate randomization;sampling variance reduction;randomization;distributed variants", "pdf_keywords": "stochastic gradient descent;stochastic gradient methods;stochastic gradient method;stochastic extragradient method;variants stochastic gradient;known stochastic method;stochasticity reduction generalization;stochastic methods;descent ascent methods;solving stochastic gradient;extragradient method stochastic;stochastic gradient;stochasticity reduction;using stochastic gradient;stochasticity reduction scheme;gradient descent ascent;optimization stochasticity;stochastic method named;stochastic methods solving;derive stochasticity reduction;stochastic method;stochastic generalization;method stochastic;descent ascent method;ascent methods arbitrary;introduce stochastic extragradient;processing stochastic extragradient;consider stochastic gradient;batchsize stochastic;stochastic extragradient"}, "b9913ddf94245c864509f0b94847bdbe77899b46": {"ta_keywords": "tonal prediction based;tonal prediction;tonal transcription language;phonemic tonal transcription;method tonal prediction;tonal prediction promise;tonal transcription;connectionist temporal classification;linguistic workflow tonal;phonemic tonal;temporal classification loss;workflow tonal prediction;prediction based connectionist;function phonemic tonal;based connectionist temporal;transcription language;signal highlighting phonetic;phonetic;loss function phonemic;highlighting phonetic;tonal;transcription language documentation;novel method tonal;connectionist temporal;architecture connectionist temporal;method tonal;transcription;temporal classification;use neural;workflow tonal", "pdf_keywords": ""}, "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a": {"ta_keywords": "oscillator level convolution;parametric oscillator transform;high speed vision;level parametric oscillator;oscillator level transformer;speed vision using;oscillator transform;level convolution;oscillator transform input;speed vision;stage parametric oscillator;parametric oscillator level;parametric oscillator;vision using combined;level transformer;level transformer method;stage parametric;level parametric;convolution;oscillator level;stage stage parametric;vision using;transform input;transform;results level parametric;vision;oscillator;parametric;transformer method;action level parametric", "pdf_keywords": "convolutional models large;transformers based attention;attention weights models;describing deep images;deep images;transformer large datasets;recognition benchmarks;training deep bidirectional;state art convolutional;attention weights;training deep;convolutional models;recognition benchmarks propose;deep bidirectional transformers;representation learning;training vision transformer;embeddings improve performance;learned embeddings improve;image recognition benchmarks;transformation deep;image terms attention;tuning attention weights;embeddings class transformer;art convolutional models;representation transfer learned;learned embeddings;large scale dataset;learning embedding;embeddings improve;deep neural"}, "4533fd4cf13d2f4dd105edaf612934a1bd85ad5a": {"ta_keywords": "recorded multichannel electroencephalogram;multichannel electroencephalogram;multichannel electroencephalogram presented;electroencephalogram;electroencephalogram presented method;electroencephalogram presented;noise removal singletrial;potentials recorded multichannel;noise removal;method noise removal;related potentials recorded;event related potentials;potentials recorded;event related potential;amplitude separated signal;separated signal;models amplitude separated;removal singletrial event;separated signal proposed;amplitude separated;related potential data;coefficients probabilistic generative;singletrial event related;locally models amplitude;recorded multichannel;probabilistic generative model;singletrial event;potential data set;probabilistic generative;potential data", "pdf_keywords": ""}, "b62430b9f8810da4d9f28842ac0ca899aa66d422": {"ta_keywords": "coupling electron spin;spin polarized electron;polarized electron spin;zeeman field spin;spin orbit coupling;electron spin;electron spin orbit;coupling spin polarized;orbit coupling spin;coupling spin;orbit coupling electron;coupling electron;spin polarized;polarized electron;effect zeeman field;field spin orbit;spin orbit;strength spin orbit;effect zeeman;field spin;orbit coupling;orbit coupling used;orbit coupling study;zeeman field;zeeman;electron;spin;tune strength spin;study effect zeeman;strength spin", "pdf_keywords": ""}, "d1c41eb99824e8f4752190da1b815378be23b4b9": {"ta_keywords": "autoregressive models trained;exposure bias autoregressive;bias autoregressive models;autoregressive models;bias autoregressive;model dependent exposure;mean field model;models trained;field model;field model pmf;models trained improved;improved scheduled sampling;exposure bias;models;field model psf;dependent exposure bias;autoregressive;field model smf;scheduled sampling;model pmf;model psf quadratic;scheduled sampling approach;improves accuracy predicted;quadratic mean field;model pmf framework;model;trained improved scheduled;accuracy predicted;framework quadratic mean;pmf framework quadratic", "pdf_keywords": "autoregressive models trained;sequence tasks autoregressive;rnns autoregressive models;trained sequence sequences;sampling adaptation recurrent;networks rnns autoregressive;imitation learning il;trained scheduled sampling;train autoregressive models;imitation learning;scheduled sampling training;rnns autoregressive;broadly imitation learning;learning sequence neurons;learning sequence;sampling performance neural;trained sequence;models trained scheduled;models broadly imitation;scheduled sampling adaptation;train autoregressive;adaptation recurrent neural;prefix scheduled sampling;model trained scheduled;sampling training;algorithm trained sequence;tasks autoregressive models;prediction pretrained language;recurrent neural;sequence tasks"}, "175b58fe7e49bb5c0c771b73f8834bcff21b59c7": {"ta_keywords": "sentence encoder models;evaluation sentence encoder;sentence encoder;language inference evaluation;natural language inference;decisions sentence encoder;sentence complexity evaluation;language inference;encoder models evaluated;encoder models task;task natural language;sentence complexity;encoder models;natural language;inference evaluation;inference evaluation based;degree sentence complexity;encoder;automatically constructed stress;models task natural;evaluation sentence;inferential decisions sentence;constructed stress tests;evaluation based automatically;models evaluated datasets;stress tests;evaluation reveals;models evaluated;inference;stress tests allow", "pdf_keywords": "sentence stressen models;sentence encoder models;sentence encoder model;language inference nli;robustness sentence stressen;sentence encoding models;inference nli tests;sentence encoder;natural language inference;evaluates natural language;evaluation sentence stressen;sentence stressen;nli tests robustness;performing sentence encoder;performance sentence encoder;task natural language;models specific linguistic;nlp models;programming nlp models;inference nli;language inference;challenging linguistic;linguistic phenomena evaluation;tests robustness sentence;word learning multinli;structure natural language;textual entailment machine;nlp models variety;challenging linguistic phenomena;sentences natural language"}, "49edf7f0dbad8b8c101af9ef95c72f62f545591e": {"ta_keywords": "compact topic embeddings;topic embeddings;captures topic correlations;learns compact topic;topic embeddings captures;embeddings captures topic;closeness topic vectors;topic vectors;topic vectors method;topic correlations;topic correlations closeness;linear topic size;sparsity topic occurrence;topic occurrence achieved;topic occurrence;topic size speedup;variational inference fast;low dimensional embedding;captures topic;topic size;compact topic;speedup variational inference;inference low dimensional;embeddings;embedding;variational inference;dimensional embedding;inference fast sampler;embedding space reducing;exploit sparsity topic", "pdf_keywords": "correlated topic modeling;topic embedding model;topic vectors embedding;compact topic embeddings;learning topic correlations;topic inference;correlated topic model;learned topic embeddings;accuracy topic inference;topic embedding;compute topic embeddings;topics exploit embedding;captures topic correlations;accurate topic modeling;topic embeddings;novel topic embedding;latent topics correlations;topic modeling;topic embedding method;topic models;topic vectors cheap;topic model learns;novel topic model;topic modeling learn;topic document embeddings;topic embeddings captures;compact topic vectors;embeddings captures topic;learning topics large;topic embeddings capture"}, "db79a3e55690c5c86cfd0ec97712ed4ad1e47b3b": {"ta_keywords": "sequential ranking;sequential ranking algorithm;comparisons items ranked;sequential active ranking;prove sequential ranking;ranking algorithm;ranking algorithm counts;recovering ranking;ranking set items;recovering ranking using;items ranked according;ranked according probability;active ranking;ranking;items ranked;succeeds recovering ranking;item items ranked;ranking set;ranking using;ranking using number;active ranking set;noisy pairwise comparisons;ranked according;pairwise comparisons items;comparisons items;comparisons optimal;ranked;pairwise comparisons;number comparisons optimal;number comparisons won", "pdf_keywords": ""}, "3c37b9ec2ff1828877575acc600b73c3bcde138f": {"ta_keywords": "modeling behavior recommender;recommender systems approach;recommender systems;behavior recommender;bandit setup;multi armed bandit;behavior recommender systems;bandit;recommender;dependent horizons recommender;bandit setup captures;armed bandit setup;recommender address;recommender address case;armed bandit;horizons recommender;ucb based algorithm;novel approach modeling;based algorithm optimal;approach modeling behavior;recent ucb based;challenging case users;algorithm optimal;modeling behavior;recent ucb;approach modeling;based algorithm;horizons recommender address;captures policy dependent;optimal", "pdf_keywords": "stochastic bandit problems;bandits interaction learner;stochastic bandit;bandits interaction;bandit problems distributed;class stochastic bandit;departing bandits interaction;multi armed bandits;bandits exploits policydependent;dynamics departing bandits;bandit problems;bandits msw formalizes;bandits;bandits msw;optimal recommendation;bandits real world;propose departing bandits;bandits exploits;departing bandits exploits;problem optimal recommendation;users recommend learner;departing bandits;bandits real;bandit;optimal recommendation algorithm;armed bandits;recommendation algorithm users;optimal policy user;armed bandits msw;recommend learner"}, "f6f4d30e4740bd92b31acd297a15872d490e7f11": {"ta_keywords": "link based classification;relation extraction tasks;relation extraction;classification tasks;classification tasks novel;based classification tasks;domain specific heuristics;results relation extraction;supervised classification tasks;heuristics domain specific;based classification benchmarks;classification benchmarks present;traditional supervised classification;classification benchmarks;supervised classification;classification;specific heuristics domain;classification tasks modest;based classification;tasks realistic domains;heuristics domain;modeling traditional supervised;classes link based;tasks novel domain;studied link based;specific heuristics automatically;novel domain specific;supervised;language modeling;link based", "pdf_keywords": "modeling semi supervised;semi supervised learning;classification relation extraction;text classification relation;supervised learning ssl;link based classification;learned classifier constraints;learning constraints failure;semi supervised;constraints proposed learning;learning constraints;constraints learning process;constraints learning;language modeling semi;text classification;classifier constraints;approach learning constraints;framework semi supervised;supervised learning approach;supervised learning;relation learning;learning failure examples;classifier constraints combined;supervised learning method;classification propose link;process semi supervised;supervised learning framework;predicting relation objects;relation learning learning;existing supervised learning"}, "dab261b25ff8ccd2c9144a5cb3a46b39ac0ac4bd": {"ta_keywords": "predicting performance quantum;quantum information design;quantum information;performance quantum;quantum computers;information design quantum;concept quantum information;quantum information conclude;performance quantum begin;design quantum computers;relevance quantum information;design quantum;performance quantum extent;quantum;introduction concept quantum;concept quantum;relevance quantum;discuss relevance quantum;quantum begin;quantum extent capable;quantum begin brief;quantum extent;capable predicting performance;state problem predicting;predicting performance;capable predicting;problem predicting performance;predicting;information conclude;problem predicting", "pdf_keywords": "machine learning argue;machine learning scholarship;machine learning seen;machine learning discuss;machine learning powerful;machine learning explain;machine learning research;misuse machine learning;perturbations machine learning;prevent neural networks;adversarial;neural networks hinders;machine learning intrinsic;adversarial network alsothis;problem learning prediction;counterfactual reasoning;counterfactual reasoning based;problem learning;machine learning field;machine learning systems;novel approach counterfactual;machine learning use;advances machine learning;adversarial network;learning machine learning;approach counterfactual reasoning;deep learning high;machine learning;learning prediction;deep learning"}, "b79dcc5304e557ce200b161d2a884c0ff77f34ec": {"ta_keywords": "spell misspellings platform;misspellings platform;spelling correction models;platform spelling correction;spelling toolkit;spell spelling toolkit;misspellings platform built;spell spell misspellings;misspellings build platform;spell misspellings;training misspellings;additional spelling correction;testing training misspellings;spelling correction;spelling correction english;spelling toolkit publicly;training misspellings build;introduce platform spelling;misspellings build;training additional spelling;platform spelling;misspellings;spell spell;neu spell spelling;neu spell spell;spell;spell spelling;neu spell;spelling;service neu spell", "pdf_keywords": "predicting spelling correction;toolkit predicting spelling;trained predict spelling;predict spelling correction;building misspellings;models spelling correction;predict spelling;predicting spelling;spelling strategy toolkit;building misspellings detect;spelling correction deep;spelling correction systems;testing models misspellings;automatically correcting spelling;misspellings detect;misspellings evaluate performance;word building misspellings;models misspellings;models misspellings use;spelling correction toolkit;spell checkers toolkit;models word spelling;misspellings use model;testing spelling correction;systematically correct spelling;misspellings detect given;correcting spelling errors;misspelling words;datasets spelling testing;correction deep learning"}, "9b621c0bcd2029006b389bc51395fb6604f9a855": {"ta_keywords": "questions game synthetic;question examples model;humans ask questions;questions game;human answerers;model answer questions;synthetic human answerers;questions controlled manner;20 questions game;questions question examples;humans ask;questions controlled;questions accurately;question examples;questions accurately expected;ask questions controlled;questions;answer questions accurately;answer questions;ask questions;20 questions;ability humans ask;answerers;questions question;model goal oriented;oriented 20 questions;goal oriented;goal oriented 20;model open domain;demonstrate model goal", "pdf_keywords": "captioner requiring supervised;questioner caption model;captioner answerer model;image captioner answerer;captions model based;captioner answerer;natural language captioning_;trained image captioner;human answerers;captions model;answerers natural language;captioning_ answerer;captioner utterances;generating natural language;captioning_ answerer make;questioner caption;model answering questions;generate questions;human answerers natural;synthetic human answerers;pretrained image captioner;produce informative questions;paired captions model;captioner;known image captioner;natural language questions;caption model;questions visually;random questioner caption;language captioning_ answerer"}, "a84c319fef32b2514af9541576189a1735aac507": {"ta_keywords": "global network method;network global network;global network global;global network;global global network;introduction global network;network global;network method;network method based;method construction global;network method applied;construction global global;global global;global;network;construction global;introduction global;new method construction;method construction;based introduction global;propose new method;construction;new method;method based;method applied;method;method based introduction;introduction;new;propose new", "pdf_keywords": ""}, "f2de2c9d83b9058695e3272a4fbb7c30e04a1476": {"ta_keywords": "learning word second;ease learning word;english semantic density;word level predictors;language duolingo mobile;word second language;learning english second;learning word;users learning english;distributional models lexical;cross linguistic semantic;second language duolingo;language duolingo;second language;linguistic semantic alignment;models lexical semantics;english second language;alignment english semantic;semantic alignment english;models lexical;lexical semantics provide;effects cross linguistic;linguistic semantic;duolingo mobile;second language using;word second;duolingo mobile app;lexical semantics;semantic density;cross linguistic", "pdf_keywords": "learning word second;second language learning;learning second language;differences word learning;language duolingo mobile;word learning accuracy;word learning performance;english words predictors;word second language;words predictors predict;words predictors;ease learning word;accuracy words learned;learn language similarity;predict accuracy words;accurately predict word;learning word;language similarity significantly;studying second language;accuracy learning english;predictors duolingo dataset;word learning;predictors duolingo;second language duolingo;language duolingo;learning english second;word level predictors;words closely related;language learning;second language"}, "5b6c1e9dddc4b55036a5629227ae2cc7d49eb6d0": {"ta_keywords": "structured image finder;astronomical image processing;astronomical image;image finder slif;present astronomical image;structured images categorised;biomedical literature tool;image finder;structured images;collection structured images;literature collection structured;literature examining images;images individual papers;information biomedical literature;tool structured image;structured image;biomedical literature;image processing tool;descriptors tool;finder slif;finder slif used;present astronomical;examining images;images categorised;astronomical;images categorised according;subject descriptors tool;descriptors tool able;image processing;examining images individual", "pdf_keywords": ""}, "a1babdf55a6bff96d533fd0c9bc44864283ec107": {"ta_keywords": "depositors risk aversion;depositors risk;aversion consider banks;depositors opening account;bank president chairperson;bank respect corporate;promotion bank president;consider banks managed;factors depositors risk;particular bank respect;account particular bank;banks managed based;banks managed;bank president;bank president existence;decision factors depositors;corporate governance variables;consider banks;promotion bank;banks;particular bank;service bank president;rate external auditors;bank respect;depositors;simultaneous promotion bank;depositors opening;bank;corporate governance;external auditors", "pdf_keywords": ""}, "3af5e203368fa2c7959d035493571d181a8682af": {"ta_keywords": "performances audio dataset;music performances audio;musical score midi;analysis classical music;score midi format;tracks provide musical;classical music performances;score midi;performances individual tracks;recordings individual tracks;individual tracks audio;audio dataset;audio dataset comprises;performances audio;instrument classical music;music performances;music pieces;separately recorded performances;midi;music pieces assembled;recorded performances;midi format audio;classical music pieces;provide musical score;tracks audio;midi format;classical music;musical score;audio recordings individual;audio recordings", "pdf_keywords": "music performances dataset;music performance dataset;datasets music transcription;music performance analysis;multimodal music performance;analyzing music transcription;multimodal music;rochester multimodal music;track music performance;datasets music;music transcription research;music demonstrate datasets;music transcription;music transcription sequences;recordings midi score;multimodal musicalthough;music recordings midi;analyzing music;audio visual analysis;recordings midi;multi track recordings;analysis music scores;including multimodal musicalthough;music transcription audio;recordings individual tracks;performances individual tracks;recordings note annotations;detecting analyzing music;multimodal musicalthough dynamical;separately recorded performances"}, "8f43b63ca400a0ea1fdd272f8c83fd67f01d0182": {"ta_keywords": "encoded conditional cross;tags encoded conditional;conditional cross talk;cross talk features;encoded conditional;cross talk signals;talk signals encoding;conditional conditional cross;predict features tags;using cross talk;talk features sentences;respective tags encoded;conditional cross;talk features;talk features used;cross talk;tags encoded;talk signals;way cross talk;conditional conditional;signals encoding;sentences respective tags;features tags competitive;features sentences respective;encoding achieved;conditional;signals encoding achieved;predict features;approach encoding data;tags competitive way", "pdf_keywords": ""}, "14a09a04c5c295a93ff25492516112cd86fa0114": {"ta_keywords": "semi supervised;model semi supervised;semi supervised learning;variational encoder coderders;morphological inflection benchmark;variational encoder;languages morphological inflection;based variational encoder;morphological inflection;languages morphological;learning based variational;encoder;model semi;encoder coderders model;majority languages morphological;new model semi;encoder coderders;variational;supervised;based variational;morphological;supervised learning;inflection benchmark;semi;results majority languages;inflection;languages;supervised learning based;learning;coderders model outperforms", "pdf_keywords": "transduction semi supervised;learning lemma representations;encoder learning;encoder learning data;semi supervised;learns generative model;semi supervised model;semi supervised learning;labeled sequence transduction;learns generative;underlying encoder learning;learning labeled;propose semi supervised;supervised model learns;learn lemma embeddings;encoder decoders learn;learning model label;semi learning;variational auto encoders;autoencoder generative;variational encoder decoders;autoencoder generative model;variational autoencoder;variational encoder decoder;supervised learning labeled;variational autoencoder generative;variational encoder;lemma representations based;model label transduction;learning labeled version"}, "5e63e47cb3386b032ec43a92ce5980466228c761": {"ta_keywords": "erasure coded data;data center cluster;data data center;storage code;data center;erasure coded;cluster storing;data center network;cluster production recovery;new storage code;operations erasure coded;recovery operations erasure;production recovery data;day cluster storing;recovery data;storage code using;storage;measurements data center;coded data data;petabytes rs coded;recovery data results;coded data;cluster storing multiple;present new storage;center cluster production;new storage;operations erasure;coded data address;rs coded data;storing multiple petabytes", "pdf_keywords": "codes distributed storage;failures distributed storage;distributed storage;data distributed storage;distributed storage systems;erasure codes distributed;data storage propose;network infrastructure storage;proposed code hadoop;code hadoop distributedthis;data storage;erasure codes network;availability traditional erasurecodes;storage recovery data;hadoop distributedthis;storage recovery;storage systems consider;availability data erasure;storage efficiency network;erasure coded systems;infrastructure storage;storage propose;purpose data storage;data erasure coded;storage code;hadoop distributedthis paper;storage systems;code hadoop;infrastructure storage optimality;coded systems availability"}, "af44f5db5b4396e1670cda07eff5ad84145ba843": {"ta_keywords": "reason text model;neural network rnn;quiz model;recursive neural network;recursive neural;introduce recursive neural;rnn model;text model;neural network;neural networks;network rnn model;network rnn;quiz model outperforms;reason text;model reason text;neural;rnn;rnn model reason;text model applied;bowl quiz model;text;field neural networks;quiz;field neural;questions bowl quiz;applications field neural;bowl quiz;networks;questions bowl;questions", "pdf_keywords": ""}, "1bd43c91ecbf46098ef2b521c5367e849819960e": {"ta_keywords": "iterative translation based;weighting iterative translation;dynamic curriculum learning;iterative translation;translation models including;translation models;translation based dynamic;data translation model;translation model;curriculum learning;curriculum learning strategy;dynamic curriculum;translation use machine;translation model used;machine translation;data translation;generate translation;synthetic data translation;use machine translation;based dynamic curriculum;generate translation use;domain adaptation;including domain adaptation;domain adaptation low;machine translation demonstrate;translation based;variety translation models;curriculum;translation demonstrate approach;used generate translation", "pdf_keywords": "translation domain adaptation;improving iterative translation;iterative translation best;language prediction translation;competitive iterative translation;strategies iterative translation;iterative translation baseline;improve iterative translation;iterative translation based;machine translation model;translated data training;iterative translation;strategy iterative translation;iterative translation domain;neural machine translation;iterative translation investigate;data machine translation;prediction translation;machine translation;sentences domain adaptation;dynamic curriculum learning;sentences domains translation;model based translation;training language models;translation powerful tool;translation use neural;iterative translation applied;translated sentences domains;method improve translation;translation investigate performance"}, "16f0c508aa54e26aa18e3b0f3c91b0c143c6a605": {"ta_keywords": "distributionally robust optimization;distributionally robust;based distributionally robust;empirical risk minimization;develop distributionally robust;risk minimization;robust optimization dro;robust optimization;risk minimization erm;dro minimizes worst;models unfair mitigate;optimization dro minimizes;minimizes worst case;dro minimizes;amplifies representation disparity;minimizes worst;fair models unfair;empirical risk;minimization erm amplifies;representation disparity;robust;representation disparity time;models unfair;unfair mitigate;based distributionally;representation disparity common;fair models;optimization dro;initially fair models;minimization erm", "pdf_keywords": "risk representation disparity;minimizing risk representation;group risk minimization;representation disparity unfair;minimize group risk;minimizing group risk;subgroup fairness model;representation disparity worst;loss best representation;distributionally robust optimization;models unfair mitigate;fair subgroup fairness;unfair mitigate problem;representation group risk;fairness model proposed;turk loss minimization;distributionally robust;fairness model;develop distributionally robust;risk best representation;representations groups robustly;use distributionally robust;subgroup fairness;fair models unfair;representations robustly;representation disparity representation;distributionally robust approach;discrimination based parity;unfair mitigate;disparity unfair"}, "5c159745fce2b87e8b00307b76f0948b9fa8b1d7": {"ta_keywords": "translation systems;translation systems built;translation tasks forest;central translation systems;central translation tasks;translation tasks;architecture central translation;string forest;central translation;forest string forest;forest string;string forest string;present central translation;forest string achieves;translation;tasks forest;tasks forest string;unprecedented performance tasks;performance tasks;forest;string achieves unprecedented;performance tasks based;string achieves;string;tasks;systems built combination;systems;tasks based novel;tasks based;novel architecture central", "pdf_keywords": ""}, "7a1a202e268ccc910e8044be556e56aa9eb5a94f": {"ta_keywords": "automated dependence plots;extending dependence plots;dependence plots;automated dependence;method automated dependence;dependence plots showing;usefulness automated dependence;dependence plots adp;dependence;extending dependence;exploring latent;method extending dependence;exploring latent space;model selection bias;behavior exploring latent;selection bias detection;bias detection;generative model;sample behavior exploring;generative model demonstrate;including model selection;space generative model;latent space generative;plots;arising generative model;model demonstrate usefulness;generative model formalize;plots adp;demonstrate usefulness automated;feature space latent", "pdf_keywords": ""}, "c674ce6454d69a87f00797f3ec90d1b38b451063": {"ta_keywords": "distributed convex optimization;method distributed convex;dual stochastic gradient;distributed convex;primal dual stochastic;convex optimization;stochastic gradient oracle;convex optimization problems;dual stochastic;stochastic gradient;gradient oracle method;oracle method distributed;optimization problems networks;convergence terms duality;gradient oracle;method distributed;method rate convergence;introduce primal dual;convex;primal dual;rate convergence propose;method shown optimal;duality gap probability;optimal terms communication;distributed;method optimal;optimization;optimization problems;method optimal terms;proposed method optimal", "pdf_keywords": "distributed convex optimization;distributed optimization;problem distributed optimization;distributed optimization problem;algorithms stochastic dual;approach distributed optimization;distributed algorithms stochastic;stochastic dual oracles;distributed optimization network;method distributed convex;convergence distributed learning;problem distributed algorithms;dual gradient descent;dual method distributed;distributed algorithms;distributed accelerated gradient;optimally parallelized dual;optimization problem distributed;distributed convex;dual oracles optimal;problem distributed learning;distributed learning algorithm;distributed learning;stochastic dual;theorem optimality distributed;strongly convex optimization;convex optimization present;optimality distributed;convex optimization;convex optimization problems"}, "2aecdd190066a57db8fea1e1143dc5fc288050e0": {"ta_keywords": "privacy metric;privacy metric assumes;new privacy metric;internet things privacy;things privacy consumers;new privacy;privacy consumers;privacy;introduce new privacy;things privacy;privacy consumers illustrate;ability infer private;adversary ability infer;adversary model;infer private;infer private parameter;data collected internet;adversary model provides;adversary ability;performance realistic grid;adversary;bound adversary;bound adversary ability;collected internet things;private parameter;strong adversary model;assumes strong adversary;strong adversary;private parameter finally;upper bound adversary", "pdf_keywords": ""}, "242cf2e991f0eed4b1309a2a9dff548e8b95900f": {"ta_keywords": "robust speech recognition;methods robust speech;beamforming methods robust;robust speech;learning based beamforming;based beamforming methods;speech recognition;beamforming methods;predicts beamforming;predicts beamforming weights;estimation beamforming weights;speech recognition methods;estimation beamforming;based beamforming;beamforming weights generalized;beamforming;likelihood estimation beamforming;beamforming weights;distortionless response beamforming;response beamforming;beamforming weights maximum;response beamforming weights;network predicts beamforming;beamforming weights fit;predicts timefrequency mask;recognition methods neural;features neural network;estimate timefrequency distortionless;recognition methods;correlation features neural", "pdf_keywords": ""}, "b3bd90f630b2d19856ef031b3dddfcb9b041b243": {"ta_keywords": "learning solving problems;strategies learning solving;problem learning solving;learning strategies;learning solving;worked examples learning;generalized learning solving;learning strategies learning;strategies learning;learning generalizing worked;learning worked examples;problem learning;examples learning generalizing;examples learning;study problem learning;examples possible learning;possible learning generalizing;problems better learning;problems feedback learning;feedback learning generalizing;compare learning strategies;need generalized learning;learning generalizing;solving problems better;better learning worked;learning;learning worked;generalized learning;better learning;possible learning", "pdf_keywords": ""}, "dd3770b2dbc9668578fefdc078d37457ba9c0b9a": {"ta_keywords": "eel speech enhancement;speech enhancement;speech enhancement implement;excitation prediction electrolaryngeal;electrolaryngeal eel speech;statistical excitation prediction;removing micro prosody;prediction electrolaryngeal;improve statistical excitation;excitation prediction;prediction hybrid enhancement;prediction electrolaryngeal eel;micro prosody low;avoiding voice conversion;micro prosody;voice conversion;eel speech;voice conversion evaluate;avoiding voice;statistical excitation;filtering avoiding voice;electrolaryngeal eel;prosody low pass;hybrid enhancement;prosody low;enhancement;electrolaryngeal;voice;enhancement implement;enhancement implement removing", "pdf_keywords": ""}, "44cabe32482d4b622d9ca00bf23b3ee7950e2710": {"ta_keywords": "human machine decision;machine predictive decisions;machine decision making;human machine predictive;predictive decisions;machine decision;predictive decision making;ml predictive decision;human ml predictive;predictive decision;predictive decisions leads;decisions produced individually;criteria human machine;quality decisions produced;machine predictive;decision making systems;higher quality decisions;decisions produced;predictive decisions accounting;decision making differ;quality decisions;criteria human;decisions leads higher;decision making;judgments;ml predictive;variation judgments introduce;variation judgments;judgments introduce taxonomy;decisions", "pdf_keywords": "machine predictive decisions;human machine decisions;human machine decision;human machine predictions;human machine predictive;machine decision makers;complementarity predictive decision;machine decision making;machine decisions;machine predictions combine;machine decisions proposed;combine predictive decisions;machines make decisions;machine decision;machine decisions various;machine make decisions;machine predictions unifying;human ml decisions;predictive decisions static;accuracy human decisions;human ml predictive;predictive decision making;predictive decisions;decision makers machine;quality individual prediction;ml predictive decision;human decision makers;decision making predictive;predictive decision;predictions combine optimally"}, "3ac59132297f4e50d5e83852555392f9ff05d8b4": {"ta_keywords": "excitation quantum dot;quantum dot spectral;applied quantum dot;quantum dot;quantum dot method;quantum dot strong;particle excitation quantum;dot spectral density;excitation quantum;single particle excitation;spectral density excitation;particle excitation;density excitation calculated;density single particle;dot spectral;density excitation;density excitation method;dot strong spin;spectral density single;single particle;applied quantum;calculation spectral density;method applied quantum;calculate spectral density;quantum;excitation calculated;dot method;spectral density;calculation spectral;excitation method", "pdf_keywords": "distributed optimization convex;decentralized distributed optimization;optimal distributed optimization;distributed optimization stochastic;distributed optimization method;distributed optimization;accelerated distributed optimization;distributed optimization problem;problem distributed optimization;decentralized optimization problems;decentralized optimization;solving decentralized optimization;convex apply decentralized;stochastic convex optimization;algorithm convex optimization;convex optimization;solving convex optimization;convex optimization method;constraints decentralized distributed;convex optimization problems;optimal distributed;method stochastic convex;optimization stochastic oracle;composite optimization;optimization convex functions;general convex optimization;optimization convex;results convex optimization;sliding algorithm convex;convex optimization problem"}, "ff3b83ef0a153ed376556057269f3a61da3a103a": {"ta_keywords": "symbolic separation instruments;symbolic separation;models symbolic separation;separation instruments tracks;separation sequential multiclass;multitrack music;tracks multitrack music;instruments tracks;instruments tracks multitrack;separate parts symbolic;parts symbolic mixture;sequential multiclass classification;multitrack music frame;symbolic mixture;separation sequential;separation instruments;music frame task;classification;instruments;separate parts;tracks multitrack;multiclass classification;approach separate parts;task separation sequential;multiclass classification problem;classification problem;mixture played keyboard;parts symbolic;separation;sequences notes labels", "pdf_keywords": "music separate parts;separating music;separating music separate;organize instruments notes;organize instruments;separation multitrack music;analyzing datasets music21;instrumental instrumentation;music separate;task voice separation;multitrack music examine;problem separating music;music sequences;multiple instruments;instruments frame task;music examine;objects music model;music sequences hierarchical;perform multiple instruments;instruments;task separation sequential;instruments notes;objects music;instrumental instrumentation use;multiple instruments time;voice separation strategies;separation chorale objects;voice separation;separate original instrumental;datasets music21"}, "cb90d5ea3a95b4c6ec904f622f51d752f506636e": {"ta_keywords": "metric network nodes;characterizing metric network;metric network;network nodes;network nodes called;nodes;nodes called;network;new metric characterizing;metric characterizing;present new metric;new metric;metric;metric characterizing metric;characterizing metric;new;characterizing;present new;called;present", "pdf_keywords": ""}, "d4f5f1a196e203226e4a69d52a04d46823f32fb3": {"ta_keywords": "multilingual nmt models;trained multilingual nmt;indic language pairs;multilingual nmt;languages samanantar dataset;collection indic languages;indic languages samanantar;million sentence pairs;collection indic language;web trained multilingual;language pairs;languages samanantar collection;indic languages;trained multilingual;sentence pairs web;models spanning languages;indic language;pairs 11 languages;spanning languages samanantar;languages samanantar;language pairs public;parallel collection indic;samanantar collection indic;11 languages;multilingual;languages 37 million;sentence pairs;spanning languages;nmt models spanning;pairs web trained", "pdf_keywords": "large parallel corpus;parallel machine translation;large parallel corpora;parallel corpus;corpus extraction parallel;parallel corpus using;quality parallel corpus;available parallel corpora;parallel corpus indic;parallel corpora enhanced;largest parallel corpus;parallel sentences corpora;parallel corpora;englishcentric parallel corpus;quality parallel corpora;mining parallel sentences;parallel corpora collection;extracting parallel sentences;extraction parallel sentences;parallel corpora large;parallel corpus 49;extract parallel sentences;parallel corpora does;parallel sentences texts;parallel translation proposed;multilingual machine translation;parallel corpora nnt;sources parallel sentences;parallel sentences train;known parallel translation"}, "d301054c2819e1a21480800fdabbe5ae909abe09": {"ta_keywords": "abstract program search;program search;language abstract program;learning libraries search;program search technique;search models synthesis;effective finding program;libraries search models;natural language annotations;finds program solves;finding program;abstract program;libraries search;integrate library search;language abstract;library search model;laps language abstract;language annotations;finds program;library search;finding program able;learning libraries;natural language;program solves;model finds program;joint learning libraries;program;annotations guide joint;annotations;using natural language", "pdf_keywords": "program abstractions neural;abstraction program learning;program abstractions learn;program synthesis generalizes;program synthesis;programs synthesis;program synthesis incorporate;deep synthesis primitives;learning abstraction;learn abstractions;learn abstractions search;learning abstraction search;generative model programs;language learn abstraction;learn abstraction;program deep;deep synthesis;language abstraction learning;generative models programs;learn abstraction search;trained program deep;program abstractions leveraging;program neural;abstraction learning introduce;sample programs synthesis;programs synthesis model;abstract reasoning scenes;class program synthesis;learns abstract;model program learning"}, "27b7489bd54dfd585edd2ba0da3920a31e7fd8b5": {"ta_keywords": "sensorimotor games;sensorimotor game;consider sensorimotor game;sensorimotor game problem;sensorimotor learning;study sensorimotor games;games consider sensorimotor;learning using teleoperation;sensorimotor games context;sensorimotor learning using;games using gradient;observations sensorimotor learning;continuous learning derive;continuous learning;steady transient play;sensorimotor;consider sensorimotor;transient play games;games context continuous;context continuous learning;transient play;learning linear quadratic;teleoperation;empirical observations sensorimotor;study sensorimotor;learning linear;observations sensorimotor;using teleoperation;based learning linear;gradient based learning", "pdf_keywords": ""}, "af38829cdb55ee7b71d49399f71397d975e40a95": {"ta_keywords": "querying corpus answer;answer question data;querying corpus;corpus answer question;question data;hard questions data;questions data;question data consists;conditional answers set;corpus answer;answers set np;questions difficult answer;answer values data;conditional answers;questions data maximum;answer values;corpus;answer values use;answers set;process querying corpus;consists conditional answers;np hard questions;answer question;data rich questions;questions;answers;questions difficult;querying;hard questions;distribution answer values", "pdf_keywords": "documents conditional answers;conditional answer dataset;conditional answers particularly;conditional answers predicting;answers predicting conditional;questions conditional answers;conditional answers;learning task conditional;document conditionalqa predict;conditional knowledge document;conditional answer conditional;conditional conditional knowledge;conditional learning task;document predict conditional;documents present conditional;novel conditional answer;answer conditional;answer conditional answer;comprehension model conditional;documents conditional;conditional conditional learning;answer conditionalqa dataset;complex documents conditional;document conditionalqa;predict answer question;challenging dataset conditional;conditional answer conditionalqa;conditional questions conditional;conditional knowledge;called conditional answer"}, "2c2234548de4694b6455a19cd0d85a9d6c473456": {"ta_keywords": "library similarity search;similarity search methods;similarity search;metric space library;manual library similarity;library similarity;metric spaces library;describes non metric;non metric;tools measure search;similarity;non metric spaces;measure search quality;non metric space;methods non metric;metric;spaces library designed;spaces library;search quality;search quality provide;search methods non;space library;measure search;metric spaces;metric space;search methods;benchmarking evaluation;quality results library;purpose benchmarking evaluation;benchmarking", "pdf_keywords": "distances efficient memory;approximate similarity search;similarity search methods;similarity search structure;computing distances efficient;similarity search;similarity search goal;generic approximate similarity;approach similarity search;metric space library;approximate similarity;method computing distances;space algorithm scalable;algorithm computing distances;nearest neighbor search;platform similarity search;metric spaces algorithm;space easy compute;possible similarity measures;approximate nearest neighbor;spaces algorithm based;given space algorithm;computing distances;search nearest neighbor;distances vectors implementation;metric spaces library;distances efficient;finding nearest neighbor;similarity measures;computing distances vectors"}, "c00ba15810496669d47d2ed5b627e6c7d2b1f6aa": {"ta_keywords": "training document translation;document translation tasks;document translation summarization;translation tasks based;translation tasks;document translation;including document translation;unsupervised multi lingual;translation summarization information;multi lingual sequence;translation summarization;lingual sequence sequence;multi lingual;lingual sequence;sequence model learned;learned unsupervised parallel;summarization information retrieval;unsupervised parallel adaptive;sequence sequence model;lingual;translation;parallel adaptive learning;pre training document;sequence model;unsupervised parallel;summarization information;training document;tasks based unsupervised;tasks including document;summarization", "pdf_keywords": "document reconstruction paraphrasing;sentence retrieval train;sentence retrieval pre;translation document summarization;multi document retrieval;document retrieval model;document retrieval;summarization information retrieval;large multilingual text;large multilingual corpus;relevance documents;relevance documents targets;crosslingual sentence retrieval;information retrieval semi;machine translation training;information retrieval pre;relevance relevance documents;reconstruction paraphrasing approach;reconstruction paraphrasing;machine translation tasks;unsupervised multi lingual;training document reconstruction;multi document summarization;sentence retrieval;documents corpus;learn relevance relevance;document summarization;retrieval semi empirical;natural language reconstruction;lingual multi document"}, "4b890b6ded71f005414e55adb87c23efd437ef95": {"ta_keywords": "parametric speech synthesis;speech synthesis proposed;speech synthesis;speech synthesis including;statistical parametric speech;synthesizers based statistical;voice conversion;tts voice conversion;parametric speech;ssf generated speech;voice conversion vc;text speech tts;various synthesizers based;speech tts voice;applicable various synthesizers;postfilters modify modulation;various synthesizers;modify modulation spectrum;synthesizers based;generated speech;synthesizers;modify modulation;speech tts;synthesis proposed postfilters;synthesis including text;text speech;including text speech;modulation spectrum;tts voice;generated speech close", "pdf_keywords": ""}, "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7": {"ta_keywords": "pre trained byte;trained byte level;byte level models;trained byte;byte level;inference speed byte;byte;speed byte level;speed byte;models based t5;inference speed;based t5 architecture;t5 architecture;training flops inference;token level counterparts;pronunciation;t5 architecture competitive;sensitive spelling pronunciation;t5;spelling pronunciation;flops inference speed;level models significantly;tasks sensitive spelling;parameter count training;pre trained;token level;based t5;level models;set pre trained;level models based", "pdf_keywords": "training arbitrary text;text models trained;pre trained corpus;text preprocessing tokenization;text sequences unified;long text sequences;preprocessing tokenization model;trained language models;efficient machine translation;machine translation architecture;scalable machine translation;arbitrary text sequences;predicting semantics natural;neural language models;text models;neural machine translation;unstructured text tasks;representation word decoder;machine translation;trained corpus;simple tokenizer;building text preprocessing;optimally predicting semantics;text sequences;predicting semantics;language models;trained language model;pre trained language;natural language current;vocabulary free encoder"}, "041b2510e54b2504890cb9f58b9bbc5601f35e3e": {"ta_keywords": "course nonlinear dynamics;nonlinear dynamics assignments;course nonlinear;nonlinear dynamics;dynamics assignments;nonlinear dynamics present;level course nonlinear;dynamics assignments based;research field nonlinear;nonlinear;field nonlinear dynamics;dynamics;dynamics present;field nonlinear;assignments graduate level;assignments;dynamics present set;assignments based;assignments graduate;assignments based structure;set assignments graduate;data designed;graduate level course;graduate level research;data;students;students undertake;based structure data;structure data;structure data designed", "pdf_keywords": ""}, "399ab2a0eddf7a7abf776241d5c0a2c4cd5bf313": {"ta_keywords": "linear speech recognition;continuous speech recognition;speech recognition weighted;speech model linear;linear speech;speech recognition;results linear speech;speech recognition based;speech model;representation speech model;continuous speech;problem continuous speech;model linear graph;based representation speech;linear graph proposed;graph means perceptron;linear graph;recognition weighted weighted;perceptron algorithm;recognition weighted;parameters linear graph;means perceptron algorithm;linear graph means;averaged perceptron algorithm;recognition based representation;perceptron algorithm weight;perceptron algorithm experimental;representation speech;graph proposed approach;recognition based", "pdf_keywords": ""}, "4b9bc5b5985bbfbc860039b98f233f8a43ad9171": {"ta_keywords": "neural networks solve;pdes time dependent;equations pdes time;pdes time;partial differential equations;time dependent coefficients;pde captured features;artificial neural networks;artificial neural;neural networks;using artificial neural;equations pdes;pdes;differential equations pdes;pde;quantities derived pde;parametric partial differential;solve parametric partial;parametric partial;derived pde;partial differential;dependent coefficients proposed;pde captured;derived pde captured;networks solve parametric;time dependent;dependent coefficients;differential equations;neural;features space coefficient", "pdf_keywords": "stochastic partial differential;equations pde random;pde random;pde random coefficients;physical quantity pde;stochastic partial;pde provide theoretical;simulations partial differential;dimensional conductivity;energy nonlinear schrdinger;pde convolutional nn;neural network solving;solving physical properties;neural network solve;theoretical guarantees neural;dimensional conductivity inhomogeneous;neural network numerical;nonlinear pde modelwe;conductivity function symmetry;pde modelwe;conductivity dimensional;pde modelwe propose;sdes nn approximation;conductivity dimensional conductivity;nonlinear schrdinger equation;sdes using neural;elliptic stochastic partial;nonlinear schrdinger;quantifications based neural;solving physical"}, "302face5b5a0944cab13665a2d4e07ef3aaf5240": {"ta_keywords": "domain question answering;question answering task;question answering;np open questions;np open ambiguous;open questions;questions np open;answering task;answering task named;open ambiguous;answering;answer paired disambiguated;open questions half;question pairs plausible;open ambiguous exhibiting;question pairs;dataset np open;half questions np;paired disambiguated rewrite;task named ambigqa;disambiguated rewrite;np open;new open;question construct dataset;question construct;dataset np;named ambigqa predicting;introduce new open;set question pairs;questions", "pdf_keywords": "domain question answering;question answering open;question answering task;identifying ambiguous questions;annotated open ended;question answering;open ended annotated;answering ambiguous open;problem annotated annotators;disambiguated questions;annotated annotators approach;annotated annotators;answers potentially ambiguous;annotated open;annotators approach;discover plausible answerswe;question disambiguated questions;answering ambiguous;annotators approach relies;identifying plausible answers;task identifying ambiguity;answering open;called ambigqa answering;questions identifying;natural information search;domain questions discover;effective identifying ambiguous;answer question algorithm;ambigqa answering ambiguous;answering open domain"}, "d86227948b6000e5d7ed63cf2054ad600b7994a0": {"ta_keywords": "simple deep neural;deep learning simple;deep neural;natural language processing;deep learning;features natural language;deep neural network;combines deep learning;neural network;natural language;simple deep;learning simple network;language processing tasks;neural;learning simple;variety natural language;neural network captures;nonlinearly transforming inputs;model combines deep;network nonlinearly transforming;simple network nonlinearly;present simple deep;language processing;combines deep;nonlinearly transforming;transforming inputs;network nonlinearly;transforming inputs performs;learning;features natural", "pdf_keywords": ""}, "8c4b187bdaf91bf068adfe005a0463c4f9c36387": {"ta_keywords": "delineation curvilinear structures;curvilinear structures based;curvilinear structures;structures based pixel;features linear structures;topological features linear;microscopy aerial images;method delineation curvilinear;delineation curvilinear;linear structures;topological features;data microscopy aerial;microscopy aerial;linear structures approach;aerial images;data microscopy;features linear;order topological features;structures approach outperforms;structures based;structures;structures approach;range data microscopy;curvilinear;pixel wise loss;microscopy;based pixel wise;pixel wise;topological;higher order topological", "pdf_keywords": "convolutional image network;network image representation;deep network;deep architectures;image network;deep architectures continued;predicting structure image;image based deep;powerful deep architectures;using topological loss;single deep network;loss function trained;topology aware loss;pixel wise losses;deep learning;using single deep;loss underlying structure;network image;topological loss;entropy training;representation resulting network;trained fully convolutional;scale image representations;convolutional image;network trained;deep network moves;image representation;image representations;deep learning current;fully convolutional image"}, "8147a495b9a933742f06458244f7c5df00767c4e": {"ta_keywords": "assertions natural language;assertions generated supervised;open domain assertions;extracted assertions simple;extracted assertions;comparing extracted assertions;assertions generated;domain assertions natural;supervised open learning;assertions natural;assertions simple;domain assertions;assertions simple test;open learning;generated supervised open;assertions;rank aware learning;natural language sentences;supervised open;trained set assertions;sentences global accurate;sentences global;open learning model;language sentences global;set assertions generated;extracting open domain;method extracting open;extracting open;sentences;dataset 100 sentences", "pdf_keywords": "open information extraction;aware learning extractions;information extraction open;learning extractions generated;learning extractions;extract information natural;iteratively learns model;method iteratively learns;iteratively learns;extractions different sentences;information extraction;assertions natural language;improve open information;binary classification iterative;rank aware learning;dataset open information;problem information extraction;learning based binary;accuracy extraction;iteratively training accuracy;information natural language;information extraction context;classification iterative learning;accuracy extraction limited;learning terms quality;iterative learning terms;supervised learning large;method extracting higher;model iteratively training;predictions semantic representation"}, "43e8e371449aaef34c2f43ae90f2157fd5a617bd": {"ta_keywords": "convex equilibrium selfish;equilibrium selfish agents;pricing socially optimal;equilibrium selfish;planner establish convex;socially optimal cost;selfish agents game;pricing socially;socially optimal;selfish agents;control convex discuss;convex equilibrium;control convex;fairness weighted pricing;incurred pricing socially;establish convex equilibrium;feedback control convex;determining social planner;social planner;convex discuss;social planner establish;optimal cost;fairness weighted;weighted pricing;agents game problem;weighted pricing reducing;planner establish desired;convex;pricing reducing;establish convex", "pdf_keywords": ""}, "830a396a4a77567caad1c155dd3b22597314e9f3": {"ta_keywords": "fairness aware systems;social choice framework;outcomes fairness aware;outcomes fairness;perspectives outcomes fairness;fairness aware;computational social choice;choice framework;choice framework integration;propose computational social;social choice;computational social;fairness;uses social choice;personalized recommendation application;recommendation application;recommendation application non;social;aware systems provide;personalized recommendation;outcomes;example personalized recommendation;systems provide;aware systems;perspectives outcomes;multiple perspectives outcomes;profit uses social;systems provide example;systems;application non profit", "pdf_keywords": ""}, "5083d9e25113a09faeba7d56b7808e2f77b5c15e": {"ta_keywords": "constructing templates neural;templates neural;templates neural model;compositionally constructing templates;order templates neural;compositionally constructing;constructing templates;construct representations;templates differentiable representations;efficiently construct templates;construct representations method;bases construct representations;construct templates;compositionally construct;used compositionally construct;method compositionally constructing;knowledge bases construct;knowledge bases;second order templates;large knowledge bases;compositionally construct second;compositionally;neural model based;representations method;representations method used;templates;construct templates differentiable;differentiable representations;neural model;neural", "pdf_keywords": "method constructing relations;constructing relations;constructing relationship entities;encode relations sparse;relation matrices based;constructing relations relation;encode relations;encode relations entities;reasoning templates formed;entities encoding relations;constructing relationship movies;relations sparse;relations entities encoding;relations sparse matrices;step reasoning templates;encoding relations;method encode relations;reasoning templates;constructing relationship;operation encode relations;relations entities;templates neural;encoding relations form;relations relation matrices;relation matrices;relationship movies method;relation set nesting;templates neural model;relations different types;hop templates neural"}, "4b762c0344f14bb00d590f5666c27b3aac7b0a7d": {"ta_keywords": "describing sentences dependency;sentences dependency dependencies;sentences dependency;model describing sentences;test sentence completion;recurrent neural network;neural network language;language model planck;propose recurrent neural;language model describing;sentence completion proposed;sentence completion;recurrent neural;performance recurrent neural;describing sentences;language model;sentence completion evaluate;performance recurrent;improves performance recurrent;sentences;dependencies proposed model;network language model;dependency dependencies;dependencies;recurrent;dependency;planck test sentence;dependency dependencies proposed;neural;test sentence", "pdf_keywords": "dependency neural language;rnn syntactic dependency;neural language models;rnn syntactic;network rnn syntactic;based dependency neural;neural language model;dependency rnn improves;dependency neural;syntactic dependency parse;rnn adapt syntactic;dependency rnn;proposed dependency rnn;dependency parse trees;models sentence completion;neural word embeddings;syntactic dependency structure;recurrent neural network;language model learns;language models deprnns;dependency parse;learns recurrent neural;generation dependency parse;dependency parse sentence;neural language;simple neural language;learns recurrent;syntactic dependency;trained input words;neural network rnn"}, "bcd45c86e1bcf8d1411eb6704c4c58d0831b5b4f": {"ta_keywords": "multinomial distribution models;text based multinomial;based multinomial distribution;binomial distributions models;multinomial distribution;betanomial distributions introduced;betanomial distributions;based multinomial;distribution models;statistical models;models text based;distributions models;statistical models description;distribution models constructed;variables betanomial distributions;positive binomial distributions;multinomial;random variables betanomial;distributions introduced model;models text;binomial distributions;distributions models solved;class statistical models;text based;statistical;text based negative;new class statistical;classical models text;description text based;class statistical", "pdf_keywords": ""}, "0ad8284dbae11901a725cc71318a165c08852278": {"ta_keywords": "approach voice conversion;voice conversion using;voice conversion;density model speaker;speaker model proposed;speaker model using;utterances speaker model;speaker model nonparallel;model speaker model;model speaker;speaker model;model parallel utterances;parallel utterances speaker;utterances speaker;voice;speaker;conversion using joint;joint density model;conversion using;approach voice;conversion;parallel utterances;combines joint density;probabilistic formulation;probabilistic formulation proposed;model using probabilistic;using probabilistic formulation;density model;novel approach voice;utterances", "pdf_keywords": ""}, "b21b927c251c415b601b6d7f785a42cc5c292635": {"ta_keywords": "extraction knowledge scientific;knowledge scientific literature;scientific knowledge graph;task identification extraction;extraction knowledge;multi task identification;identification extraction knowledge;scientific literature framework;scientific literature;knowledge scientific;scientific knowledge;knowledge graph;identification tasks framework;scientific literature use;task identification;models extraction knowledge;construction scientific knowledge;multi task;identification tasks;identification extraction;framework multi task;existing models extraction;simultaneous identification tasks;literature framework supports;models extraction;tasks framework;tasks;literature framework;knowledge;task", "pdf_keywords": "predict relation extraction;predicting relation extraction;leveraging crossreference information;predicting scientific entities;relation extraction spanwe;scientific entities relations;relation extraction coreference;scientific information extraction;entity recognition;relations coreference clusters;relation extraction span;relation extraction knowledge;coreference clusters scientific;extract scientific entities;entity relation extraction;entity recognition relation;annotation entities relations;named entity recognition;relation extraction;datasets entity recognition;learning extract relation;leveraging crossreference;scientific knowledge graph;representations leveraging crossreference;advances annotation entities;scientific knowledge graphs;single entity recognition;extraction coreference;recognition relation extraction;extraction coreference resolution"}, "34fb3e21a63fb2987f7a87f88ecf49aea53cff36": {"ta_keywords": "entangled pairs photons;pairs photons mechanism;generation entangled pairs;entangled pair mechanism;mechanism generation entangled;pairs photons;coherent entangled pair;entangled pairs;entangled pair;generation entangled;single coherent entangled;photons generated single;coherent entangled;photons generated;photons mechanism;photons mechanism based;entangled;fact photons generated;photon process mechanism;powerful photon process;simple powerful photon;photon process;powerful photon;photons;photon;generated single coherent;pair mechanism;based fact photons;fact photons;pair mechanism illustrated", "pdf_keywords": ""}, "95cedaeb3178a4671703a05171a144e6b964a819": {"ta_keywords": "count based neural;neural count based;neural language model;count based languages;neural count;hybrid language model;based neural language;language model;neural language;language model combines;advantages neural count;languages hybrid model;mixture weights vocabulary;hybrid language;weights vocabulary words;count based;constructing count based;language model allows;languages hybrid;based languages hybrid;formulation count based;present hybrid language;constructing count;weights vocabulary;based neural;based languages;neural model;words experiments dataset;neural model terms;vocabulary words experiments", "pdf_keywords": "stochastic language model;neural language models;nonlinear mixture models;nonlinear mixture distributions;models nonlinear mixture;language models;nonlinear mixtures distributions;language modeling;distributions nonlinear mixture;mixture distributions nonlinear;ddms gram models;mixture models;mixture models dynamically;gram models;mixture models general;language models combine;gram models suited;language models incorporate;language modeling generalizes;gram models nonlinear;describing nonlinear mixtures;mixture models ddms;modeling rare words;learning mixture weights;firstneural language models;learning mixture;stochastic language;mixture weights distributions;language model;distributions vocabulary words"}, "a13d8400813743adb22ba0bd0570c49af2675a39": {"ta_keywords": "separating speech target;continuous speech separation;method separating speech;speech separation compared;speech separation;separating speech;speech target based;block level separation;speech target;separation model proposed;continuous speech;level separation model;novel method separating;separation model;problem continuous speech;separation compared baseline;model offline block;level separation;method separating;dual path random;offline block;random variable nonlinear;separation compared;separation;offline block online;baseline model offline;separating;path random variable;nonlinear nn architecture;speech", "pdf_keywords": ""}, "54316d2861eb3d575a8c7d071f4cf7c2fc30be01": {"ta_keywords": "robustification pre trained;efficient robust denoised;robust denoised smoothing;robust denoised;accuracy denoised smoothing;classifiers efficient robust;fidelity denoised smoothing;denoised smoothing efficient;accuracy denoised;trained classifier robustify;denoised smoothing able;denoised smoothing;classifier robustify;robustify classifier higher;higher fidelity denoised;robustify classifier;novel approach robustification;fidelity denoised;robustification;classifier robustify classifier;approach robustification;approach robustification pre;robustification pre;smoothing efficient inference;efficient robust;denoised smoothing apply;pre trained classifiers;pre trained classifier;degree accuracy denoised;smoothing efficient", "pdf_keywords": ""}, "73bbd0b53044e9f518a3596a3607521bbce12fc2": {"ta_keywords": "weakly supervised segmentation;weakly supervised;method weakly supervised;improvement robustness segmentation;robustness segmentation;supervised segmentation method;robustness segmentation standard;optimization method weakly;supervised segmentation;segmentation;segmentation standard approaches;segmentation method;weakly;segmentation standard;gradient descent;supervised;novel local optimization;segmentation method based;gradient descent method;simplicity gradient descent;local optimization;based gradient descent;method weakly;local optimization method;gradient descent achieves;simplicity gradient;gradient;method based gradient;based gradient;attractive simplicity gradient", "pdf_keywords": ""}, "a5b1d1cab073cb746a990b37d42dc7b67763f881": {"ta_keywords": "natural language models;dimensional natural language;language models nps;language models;natural language;np assisted model;training dimensional natural;model joint training;novel statistical model;statistical model joint;joint training dimensional;s__ trained model;trained model;models nps np;novel statistical;models nps;np assisted;nps np assisted;training dimensional;model s__ trained;assisted np assisted;assisted np;np assisted np;present novel statistical;assisted model s__;s__ trained;statistical model;assisted model;statistical;trained model demonstrate", "pdf_keywords": "semantic parsers trained;neural semantic parser;semantic parsers tables;parsers trained unstructured;predict semantic parsing;generating semantic parsers;semantic parsing supervised;semantic parsing database;supervised semantic parser;supervised semantic parsing;parsers trained;semantic parser novel;resulting semantic parsers;build neural semantic;semantic parsers natural;representations semantic parsers;like semantic parsing;semantic parsing structured;parsers tables;semantic parsers;semantic parser;trained natural language;semantic parsing;learn predict semantic;parsers tables approach;neural semantic;generation semantic parsers;parsing supervised;learn schema representations;semantic parsers decoding"}, "7e870eb8d580fb1b8b7a8f97d94d67555a225635": {"ta_keywords": "recipients message composition;intelligent message addressing;potential recipients message;specified recipients;message addressing problem;message addressing problems;previously specified recipients;recipients message;message addressing;recipient contact auto;recipients;recipients initial letters;letters intended recipient;subject message addressing;corpus frequently email;recipient contact;intended recipient;intended recipient contact;specified recipients initial;recipient;recipients initial;models intelligent message;message composition;frequently email users;contact auto completion;frequently email;email users subject;message composition given;users subject message;intelligent message", "pdf_keywords": ""}, "267b94325028e0e2e6da1ae2cbe7f7a93284722e": {"ta_keywords": "structural graph reranking;graph walk similarity;similarity metrics documents;graph reranking;walk similarity measures;graph reranking schemes;extended similarity metrics;similarity metrics;similarity measures;walk similarity;similarity measures outperform;lazy graph walk;extended similarity;based graph walk;graph walk provide;lazy graph;structural graph;graph walk;consider extended similarity;metrics documents objects;similarity;embedded graphs;reranking;integrated structural graph;reranking schemes based;graphs facilitated lazy;facilitated lazy graph;based graph;graphs;email data content", "pdf_keywords": ""}, "a309ad4c4088843d230be1a85806960e633e1e46": {"ta_keywords": "data curation happening;data curation;moot data curation;data curation used;curation happening data;data used improve;performance models argue;models argue;models argue point;improve performance models;curation used improve;models;performance models;happening data used;curation used;data used;moot data;curation;data;happening data;curation happening;point moot data;performance;used improve performance;improve performance;position paper argues;paper argues;paper argues point;argues point moot;point moot", "pdf_keywords": "models learn linguistic;nlu machine learning;nonlinear linguistic;theory nlp;nonlinear linguistic theory;theory nlp discuss;challenges model decoherence;data use training;field nonlinear linguistic;training task data;models learn;linguistic theory nlp;decoherence bias privacy;model decoherence prone;learn linguistic;nlu machine;data universe;issues decoherence bias;machine learning focus;nlp;nlp discuss;field nlu machine;model decoherence;natural data use;learn linguistic paradigms;nlp discuss power;data curation argues;handling linguistic;knowledge linguistic;natural data"}, "1be28ce9a1145c2cf4f78e6c494a4c15397fbac3": {"ta_keywords": "biomedical knowledge graph;summarization graph neural;attention based subgraph;summarization graph;subgraph extraction module;named summarization graph;subgraph extraction;based subgraph extraction;graph neural;knowledge graph;graph neural network;noisy biomedical knowledge;detection adverse drug;based subgraph;subgraph;biomedical knowledge;knowledge graph kg;channel knowledge data;knowledge data;graph;knowledge data integration;adverse drug drug;drug drug interactions;adverse drug;drug interactions ddi;drug interactions;named summarization;drug drug;method named summarization;multi channel knowledge", "pdf_keywords": "network summarization drug;predicting drug interactions;drug interaction prediction;summarization graph neural;summarization drug interaction;neural network summarization;network embeddings drug;approach predicting drug;predicting drug;drug interaction discovering;predict relation drug;predicting interaction drug;prediction adverse drug;diseases summarization graph;knowledge summarization subgraphs;subgraph deep learning;summarize subgraph information;summarization drug;summarization predict;summarization predict distribution;subgraph effective summarization;clinical prediction discovering;subgraph deep;knowledge summarization predict;predict outcome drug;predict pharmacological;summarization subgraphs;drug interactions based;summarize subgraph;summarization graph"}, "ce6143e24a455edc233f12933e9903426b963799": {"ta_keywords": "implementation variational latent;scalable implementation variational;implementation variational;variational latent;variational;present scalable implementation;scalable implementation;present scalable;latent;scalable;implementation;present", "pdf_keywords": ""}, "51bf7a3aee6b1f61b902625f6badffedf200d31a": {"ta_keywords": "rules encoded deep;deep generative;deep generative model;encoded deep generative;manipulating layer deep;linear associative memory;deep network;associative memory;deep network linear;layer deep;layer deep network;manipulating specific rules;network linear associative;encoded deep;associative memory demonstrate;generative model;generative;associative memory derive;rules encoded;specific rules encoded;entry associative memory;deep;generative model address;structural rules;interesting structural rules;linear associative;modifying entry associative;associative;rules located modified;manipulating layer", "pdf_keywords": "network deep;rules encoded deep;manipulating layer deep;deep generative;features generative networks;generative networks demonstrate;image based deep;deep generative model;deep network;generative networks;network generative feature;network generative;deep networks;deep learning generative;learning generative;state art generative;visual editing effects;deep convolutional;characterizing structure deep;representation network generative;alter learned rule;deep learning;large scale deep;representation network;layer deep;structure deep network;neural network deep;layer deep network;visual editing;network deep learning"}, "990c7726dd31723f97a364828d5191080fe7ec2d": {"ta_keywords": "topological quantum circuit;chiral topological superconductor;topological quantum computation;topological superconductor;topological superconductor method;universal topological quantum;topological quantum;gate topological quantum;correlated majorana edge;chiral topological;quantum circuit constructed;mode chiral topological;majorana edge modes;quantum computation based;quantum computation;quantum circuit;uses quantum circuit;majorana edge mode;correlated majorana;quantum circuit consisting;strongly correlated majorana;superconductor;matrix correlated majorana;superconductor method;gate topological;method universal topological;encode quantum;edge mode chiral;quantum;universal topological", "pdf_keywords": "topological quantum hall;fractional quantum hall;topological quantum computing;topological quantum computer;topological quantum gates;topological quantum computation;quantum hall;topological quantum gate;circuits topological quantum;quantum spin hall;quantum circuits topological;quantum hall fqh;based topological quantum;quantum computing tqc;topological superconductors based;quantum hall effect;quantum computation tqc;universal quantum circuits;universal quantum circuit;based topological superconducting;quantum computation tqqc;quantum circuit based;quantized spin hall;tqc topological quantum;quantum gates based;computer fractional quantum;quantum computer fractional;based quantum circuit;superconductor quantum;computation tqqc quantum"}, "6fae71765a5e86dfef2f93bbe03c4a2e20f827b5": {"ta_keywords": "automatic speech recognition;speech recognition;automatic speech;english automatic speech;speech recognition named;neural net;english automatic;feature neural net;neural net novel;approach english automatic;feature neural;word error rate;neural;recognition named combines;achieves word error;novel feature neural;speech;recognition;lattice rescoring combination;word error;lattice rescoring;derecoding lattice rescoring;recognition named;net;rescoring combination;net novel approach;rescoring combination achieves;automatic;combination achieves word;evaluation novel approach", "pdf_keywords": ""}, "b9e6c65aacfe8ecc1b7833b47803672273a918ec": {"ta_keywords": "spin orbit interaction;xmath0 model spin;orbit interaction xmath0;zeeman field spin;significant spin orbit;effect zeeman field;interaction xmath0 model;model spin orbit;spin orbit;field spin orbit;control spin orbit;interaction strength xmath;orbit interaction induces;effect zeeman;change spin orbit;orbit interaction strength;interaction xmath0;orbit interaction used;effect significant spin;xmath0 model;orbit interaction;model spin;significant spin;control spin;field spin;change spin;significant change spin;zeeman field;spin;zeeman", "pdf_keywords": ""}, "cf08bef866885edb8b001deb18e582eec94c51de": {"ta_keywords": "spontaneous speech summarization;automatic speech summarization;speech summarization;speech summarization experimental;speech summarization open;automatic speech;summarization open domain;summarization experimental;method automatic speech;summarization experimental results;meaningful messages talks;messages given talk;summarization open;summarization;extracting meaningful messages;spontaneous speech;extract meaningful messages;messages talks present;talks method combines;talks method;semantic acoustic features;speech;messages talks;acoustic features extract;combines semantic acoustic;results spontaneous speech;semantic acoustic;talks present;talks;open domain talks", "pdf_keywords": ""}, "b790c3e712c92065d596364af81a494adbc62c39": {"ta_keywords": "distributionally robust optimization;large scale generative;generative models;generative models proposed;generative model;generative models develop;scale generative models;relaxation generative model;novel distributionally robust;generative;generative model inner;distributionally robust;relaxation generative;based relaxation generative;scale generative;robust optimization dro;robust optimization;optimization large scale;model selection heuristics;novel distributionally;propose novel distributionally;model inner maximization;based optimization large;optimization large;gradient based optimization;distributionally;model selection;inner maximization objective;optimization dro;optimization dro framework", "pdf_keywords": ""}, "43d5c00938bd2acb1aca8e81a7d220025eddbc23": {"ta_keywords": "xmath0he atom magnetic;magnetic properties xmath0he;atom magnetic field;atom magnetic;properties xmath0he atom;state xmath0he atom;xmath0he atom;xmath0he atom determined;magnetic properties ground;ground state xmath0he;magnetic field spin;control magnetic properties;magnetic properties;field magnetic properties;control magnetic;properties xmath0he;state xmath0he;dependent magnetic;strongly dependent magnetic;properties ground state;magnetic field;magnetic field magnetic;interplay magnetic;atom determined interplay;field magnetic;interplay magnetic field;atom;magnetic;field spin;determined interplay magnetic", "pdf_keywords": ""}, "6695d3b92e7cd7f2359f698a09c7b3dc37996329": {"ta_keywords": "augmented multimodal pretraining;multimodal pretraining;multimodal pretraining model;language pretraining;language pretraining model;language language pretraining;label augmented multimodal;novel pretraining framework;learn representations visual;augmented multimodal;pretraining framework;pretraining model;pretraining model based;pretraining;visual language;visual language language;novel pretraining;model novel pretraining;novel visual language;learn representations;pretraining framework named;pretraining model purpose;method learn representations;multimodal;learning;reinforcement learning rtg;representations visual objects;learning rtg;visual;label augmented", "pdf_keywords": "augmented multimodal pretraining;label augmented pretraining;multimodal pretraining model;label augmented multimodal;label augmentation pretraining;multimodal pretraining;pretrained bert representations;recognition objects pretrained;pretraining label augmentation;description language pretrained;augmentation pretraining tasks;language pretrained bert;language pretrained;vision language field;learning vision language;augmented multimodal;pretraining simultaneous description;enrich vision language;objects pretrained models;visual object trained;augmented pretraining model;augmented pretraining;vision language;cross modal pretraining;masked language modeling;vision language pairs;pretrained models trained;modal pretraining;novel pretraining task;description vision language"}, "887d84c1310c6e71a0f89874ef9985b65a44c855": {"ta_keywords": "noisy speech recognition;speech recognition;mutual information dmmi;speech recognition experiment;information dmmi criterion;applied noisy speech;maximum mutual information;feature transform;parameters feature transform;recognition performance standard;recognition performance;noisy speech;mutual information;dmmi criterion method;feature transform using;differenced maximum mutual;estimating parameters feature;dmmi criterion;minimum phone error;superior recognition performance;method minimum phone;parameters feature;information dmmi;recognition;superior recognition;maximum mutual;minimum phone;achieve superior recognition;recognition experiment;method applied noisy", "pdf_keywords": ""}, "46bf4bece58764d22764acfd3d232b50fb7767f9": {"ta_keywords": "cnns classification mild;classification mild cognitive;mild cognitive impairment;cognitive impairment patients;neuroimaging data deep;cognitive impairment normal;deep net trained;patients mild cognitive;deep net;cnns classification;cognitive impairment;networks cnns classification;3d deep convolutional;deep convolutional neural;structural neuroimaging data;convolutional neural;networks cnns;distinguish mild cognitive;neuroimaging data;classify patients mild;cognitive impairment based;data deep net;3d deep;mild cognitive;convolutional neural networks;impairment patients mild;cognitive impairment neuropsychiatric;impairment normal subject;classification mild;neuropsychiatric impairment based", "pdf_keywords": ""}, "6f173939f6defe3ebae8fb12f19349ba96b7b5c4": {"ta_keywords": "generated attractors clustering;attractors clustering;attractors clustering process;identified attractors clustering;diarization attractor based;speakers generated attractors;speakers identified attractors;unsupervised clustering;unsupervised clustering process;end diarization attractor;present unsupervised clustering;diarization attractor;clustering;speakers recording;speakers generated;clustering process;diarization used generate;clustering process output;number speakers recording;recording output speakers;diarization;clustering process combines;number speakers generated;end diarization;speakers identified;speakers recording output;diarization used;end end diarization;attractor based;combines attractor end", "pdf_keywords": "clustering speaker embeddings;diarization speaker clustering;speaker diarization model;speaker clustering;based clustering speaker;clustering method speaker;clustering speaker;trainable diarization model;speaker clustering method;attractor based diarization;attractor speaker counting;speaker embeddings;based speaker diarization;trainable diarization;speaker diarization methods;speaker embeddings able;speaker diarization proposed;speaker attractor approach;speaker diarization;novel speaker diarization;eend vector clustering;diarization speaker;speech speaker diarization;speaker diarization speaker;end trainable diarization;end speaker diarization;diarization model captures;conventional speaker diarization;based diarization subsequence;speaker attractor"}, "1fb88c130bedcd2e75fd205b70af2999c6a8c49d": {"ta_keywords": "noisy environment network;neural network noisy;network noisy environment;random path network;propagation neural;network noisy;propagation neural network;noisy environment;network generated random;path generated network;study propagation neural;path network generated;generated random path;propagation;random path;environment generated network;generated network;connected environment random;random path generated;environment random path;path network;network generated network;network generated;environment network generated;generated network interaction;neural network;noisy;environment network;network connected environment;environment random", "pdf_keywords": ""}, "6ad56b1b776a2c448fc90c543b50756941e5a119": {"ta_keywords": "incentives energy efficiency;incentive programs energy;design incentives energy;incentives energy;utility interaction modeled;utility utility interaction;energy providers approach;efficiency design incentive;programs energy providers;utility interaction described;utility interaction;incentive programs;energy providers;design incentive programs;programs energy;energy efficiency design;consumer utility;utility interaction consumer;approach design incentives;matrix utility utility;consumer utility provider;design incentives;model utility utility;utility utility;interaction consumer utility;utility provider;design incentive;utility provider utility;matrix utility;random matrix utility", "pdf_keywords": ""}, "bb6317bbd2c4a81e94cf3d7eb1b73da246a022db": {"ta_keywords": "nearest neighbors text;domain adaptation;neighbors text approach;text based nearest;natural language text;learning natural language;neural language model;domain adaptation simply;model domain adaptation;nearest neighbor search;text sequences;neural language;natural language;neighbors text;language model approximate;language text based;nearest neighbor;use neural language;varying nearest neighbor;based nearest neighbor;set text sequences;text approach;learning natural;language text;language model;nearest neighbors;text sequences use;text based;sequences use neural;text approach effective", "pdf_keywords": "language model deep;neural language models;language models directly;natural language models;trained neural language;pre trained embedding;word distribution nearest;language models;learning structure corpora;neural language;trained embedding;effective language modeling;language modeling plentiful;trained embedding space;interpolating word distribution;language modeling;words aggregating random;neighbors knn models;standard language models;language model;learns predict;word distribution;model deep learning;model nearest neighbors;knn object generalization;corpora knowledge;deep learning generate;learning generate sequence;word aggregating random;model deep"}, "b31eb3428320342dfde042693ff2ca106dabed0d": {"ta_keywords": "abstractive summarization smm;abstractive summarization;framework abstractive summarization;sequence learning framework;sequence sequence learning;summarization smm;sequence learning;summarization;formulating text generation;contrastive learning;framework contrastive learning;text generation;contrastive learning used;summarization smm bridge;text generation reference;powerful framework contrastive;generation reference free;reference free evaluation;framework contrastive;systems formulating text;formulating text;contrastive;generation reference;text;sequence;learning framework;free evaluation;sequence sequence;learning used improve;learning framework robust", "pdf_keywords": "quality generated summaries;generate generated summaries;model abstractive summarization;generated summaries;summarization conditional text;abstractive summarization sim;generated summaries present;summarization adopt contrastive;conditional text generation;abstractive summarization;summarization conditional;generated summaries biased;abstractive summarization frameworkabstractive;generated summaries articles;framework abstractive summarization;text generation tasks;contexts generated summaries;formulating text generation;text summarization;summarization frameworkabstractive;summarization sim;frameworkabstractive summarization conditional;abstractive summarization eq;summarization adopt;text generation task;text generation model;text summarization adopt;text generation;generate candidate summary;summarization frameworkabstractive summarization"}, "5b1516c87818084dc5d195cc274e1ee8923210d2": {"ta_keywords": "robust cross lingual;lingual named entity;entity recognition languages;named entity recognition;cross lingual named;languages cross lingual;lingual named;cross lingual;translation lexical items;entity recognition;cross lingual setting;recognition languages minimal;lingual;recognition languages;translation lexical;lingual setting;languages cross;tested languages cross;languages minimal;languages minimal resource;robustness word order;named entity;commonly tested languages;principles translation lexical;lexical items;lexical items respective;tested languages;languages;attention robustness word;attention robustness", "pdf_keywords": "bilingual word embeddings;multilingual word embeddings;word embeddings unsupervised;word embeddings shared;unsupervised transfer lexical;lexicon common embedding;word embeddings method;learn embeddings words;word embeddings;embeddings words;approach unsupervised crosslingual;embeddings unsupervised transfer;learned embeddings translated;embeddings translated;embeddings words different;embedding based dictionary;linguistic structure embeddings;unsupervised crosslingual nervation;unsupervised crosslingual;multilingual named entity;word embeddings exploits;transfer lexical items;embeddings translated target;languages cross lingual;unconstrained natural language;crosslingual nervation combines;transfer lexical;languages learned embeddings;embeddings unsupervised;embeddings low resource"}, "1ce0664989e0b28ceea223cab68f885ed18c39c4": {"ta_keywords": "modelling speaker clustering;speaker clustering based;speaker clustering;speaker cluster discrete;dynamics speaker cluster;speaker cluster;approach modelling speaker;modelling speaker;mixture models proposed;mixture models;scale mixture models;cluster discrete time;dynamics speaker;clustering based multi;capture dynamics speaker;cluster discrete;clustering;multi scale mixture;clustering based;entire cluster discrete;cluster;speaker;dynamics entire cluster;scale mixture;discrete time scale;entire cluster;mixture;information theoretic approach;based multi scale;models proposed", "pdf_keywords": ""}, "2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6": {"ta_keywords": "distributed optimization protocol;accurate distributed optimization;distributed optimization;neural networks protocol;optimization protocol efficient;optimization protocol;averaging protocol efficient;based averaging protocol;averaging protocol;gossip based averaging;efficient accurate distributed;iteratively generated gossip;protocol based iteratively;average guarantees algorithm;networks protocol;accurate distributed;networks;protocol efficient accurate;protocol efficient;guarantees algorithm converges;guarantees algorithm;guarantees algorithm efficient;networks protocol based;evaluation neural networks;distributed;accurate evaluation neural;generated gossip based;run algorithm;neural networks;algorithm zero guarantees", "pdf_keywords": "novel distributed optimization;distributed optimization;distributed training;gossip based optimization;new distributed optimization;distributed training protocol;algorithm training gossip;approach distributed optimization;nodes distributed optimization;distributed optimization algorithm;problem distributed optimization;distributed optimization approach;distributed machine learning;distributed optimization problem;distributed optimization timevarying;popular distributed training;distributed learning;algorithm based gossip;optimization communication efficient;distributed optimization unstable;algorithm gossip;cloud training;simpler algorithm gossip;cloud training method;method distributed learning;distributed computing;algorithm based distributed;algorithm gossip consider;distributed learning approach;distributed computation"}, "d9212b207e49a3aa6806fb2ddadb303b7b1d47a8": {"ta_keywords": "structure natural language;natural language np;natural language;np command systems;command systems;formalism procedures programs;captures underlying hierarchical;outperforms reactive baselines;procedures programs captures;reactive baselines;language np command;hierarchical structure natural;reactive baselines large;formalism procedures;propose formalism procedures;underlying hierarchical;command systems instantiate;framework iqa alfred;programs captures underlying;procedures programs;command;hierarchical;alfred datasets;alfred datasets demonstrate;iqa alfred datasets;underlying hierarchical structure;language np;np command;hierarchical structure;structure natural", "pdf_keywords": "programs natural language;procedural programs;procedures variety programming;natural language commands;holistic procedural programs;generate useful procedures;procedural knowledge agent;generate programs;procedural knowledge framework;procedural programs better;predictions executable programs;language commands representation;generate programs complete;hierarchical procedural knowledge;programs represents procedures;procedures corresponding executable;commands representation;programming language;executable procedures corresponding;formalism procedures programs;procedure controlled hierarchy;use generate programs;representing hierarchical procedural;predefined procedure agent;procedures programs;level procedural knowledge;procedures programs powerful;procedures generate executable;task hierarchical modular;procedures generate"}, "8fcd012e8ed2ea8190163369c9f222178e70a19d": {"ta_keywords": "speech recognition;ctc attention based;automatic speech recognition;automatic speech;hybrid attention ctc;end automatic speech;speech recognition ar;attention based learning;crumpled special case;attention ctc attention;crumpled special;attention based;ctc attention;crumpled crumpled special;case crumpled special;special case crumpled;approach hybrid crumpled;hybrid attention;consists hybrid attention;hybrid crumpled;case crumpled;attention ctc;hybrid crumpled crumpled;crumpled;recognition;crumpled crumpled;recognition ar;recognition ar proposed;speech;attention", "pdf_keywords": ""}, "49418122bba375fa02907d38b0be80689f750b39": {"ta_keywords": "learned codes accurately;approach learned codes;learned codes;codes accurately reconstruct;coding theoretic;coding theoretic approach;unavailable predictions neural;designing codes;codes accurately;non linear computations;encoding decoding functions;coding;reconstructions unavailable computation;decoding functions;decoding;decoding functions produce;present coding theoretic;encoding decoding;approach designing codes;designing codes perform;present coding;codes perform;image classifiers;predictions neural;training methodology encoding;methodology encoding decoding;use neural network;codes perform non;neural network architecture;linear computations proposed", "pdf_keywords": "learning coded computation;designing erasure codes;erasure codes approximate;erasure codes;erasure codes impart;learning coded;erasure codes handcrafting;approach learned codes;propose coded computation;learned codes;handcrafting erasure codes;codes handcrafting erasure;functions propose coded;decoding functions neural;learned code;predictions learned code;coded computation;learning unavailability computation;learned codes canwe;erasure codes handle;coded computation approach;function encoding decoding;decoding functions code;encoding decoding functions;decoding functions robust;approach learning coded;learning encoding;learning decoding function;decoding functions making;code accurately reconstruct"}, "49f657d704a1b80ce3dba0d8a9e5479ec1d703d4": {"ta_keywords": "autoregressive network speech;transformers structure speech;speech recognition;network speech recognition;speech recognition proposed;structure speech recognition;deep non autoregressive;speech recognition used;non autoregressive transformers;autoregressive transformers;network speech;autoregressive transformers structure;different decoding strategies;non autoregressive network;decoding;decoding strategies;different decoding;structure speech;novel non autoregressive;support different decoding;autoregressive network;non autoregressive;decoding strategies including;train deep non;speech;kolmogorov index benchmark;autoregressive;recognition used train;recognition;recognition proposed framework", "pdf_keywords": "transformerbased automatic speech;speech recognition decoding;speech recognition asr;model speech recognition;autoregressive model speech;speech recognition icps;speech recognition;transformer trained sequence;automatic speech;deep transformer;automatic speech recognition;deep transformer transformer;output transformer training;end speech recognition;predict new decoding;model speech;speech recognition language;training transformer network;recognition language modeling;recognition decoding;transformer trained;speech recognition assume;transformer transformer trained;transformer training;transformer training transformer;recognition decoding strategies;predictions entire speech;2019 speech recognition;deep transformers;recognition asr"}, "a1321f4527559836509c27008329afaf11f8ea89": {"ta_keywords": "effectiveness learning solving;correction learning agent;learning agent;learning solving different;learning solving;interleaved problem order;effectiveness learning;learning;compares effectiveness learning;correction learning;effective learning;yields effective learning;problem order interleaved;learning domains interleaved;effective learning domains;solving different problems;problem order;interleaved problem;simulation study;order interleaved problem;detection correction learning;simulation study compares;results interleaved problem;problem order results;learning domains;problem order provides;problem order yields;blocked problem order;simulation;present simulation study", "pdf_keywords": ""}, "da46a0b5ddf0f4bf4caad9d29d6b4a93dd2eb2d2": {"ta_keywords": "game modeled agent;game modeled interacting;player game modeled;modeled interacting agents;interacting agents agent;interacting agents;games game modeled;game modeled;describing games game;problem describing games;modeled agent agent;describing games;agent assumed player;modeled agent;agents agent assumed;agent agent assumed;agent agent;agents agent;agent assumed;agents;assumed player game;modeled interacting;agent;player game;interacting;games game;games;game;assumed player;describing", "pdf_keywords": ""}, "8ff54aa8045b1e30c348cf2ca42259c946cd7a9e": {"ta_keywords": "sequential question answering;neural semantic parsing;dynamic neural semantic;question answering;parsing framework trained;semantic parsing;semantic parsing framework;neural semantic;weakly supervised reward;supervised reward guided;dynamic neural;parsing framework;parsing;reward guided search;challenging question sequential;novel dynamic neural;reward guided;answering;supervised reward;neural;trained using weakly;sequential context;sequential context outperform;weakly supervised;questions present;leverages sequential context;questions;using weakly supervised;question sequential;semantic", "pdf_keywords": ""}, "aaaff6b99684cb5b5e0a68e214bd8bbd4bf2e231": {"ta_keywords": "named entity recognition;entity recognition systems;entity recognition;syntactic parsing systems;parsing systems;model syntactic parsing;parsing systems able;syntactic parsing;parsing;named entity;study named entity;hidden model syntactic;model syntactic;syntactic;entity;data able predict;predictions quality data;structure data;data make predictions;based hidden model;recognition systems;quality data;structure data make;quality data able;systems based hidden;data use predictions;make predictions quality;recognition systems based;predictions quality;data", "pdf_keywords": ""}, "956e096b1e8422c91989938b9508272b956d3070": {"ta_keywords": "graph reranking approach;probabilities graph reranking;rank random graph;random graph walks;graph reranking;walks gradient descent;learning rank random;graph walks;graph walks gradient;reranking approach;approaches learning rank;reranking approach uses;learning rank;reranking;walks gradient;rank random;random graph;walks;descent algorithm tunes;transition probabilities graph;gradient descent;rank;gradient descent algorithm;algorithm tunes;approaches learning;probabilities graph;comparison approaches learning;traversed paths;learning;personal information management", "pdf_keywords": ""}, "a4a8e91995ae8c8b203dd857bdc0915facddeebe": {"ta_keywords": "crowd labeled data;data minimize noisy;minimize noisy data;noisy data minimize;labels worker quality;quality noisy data;noisy crowd labeled;modeling labels worker;noisy data optimizing;noisy data;estimate quality noisy;jointly modeling labels;labeled data;labeled data alternating;quality noisy crowd;estimate quality data;worker quality noisy;crowd labeled;data minimize;modeling labels;labels worker;simulated noisy workers;minimize noisy;quality data;noisy workers real;data alternating minimization;quality data experiments;noisy workers;data optimizing loss;estimate quality", "pdf_keywords": "classification noisy labels;learning noisy labels;classification noisy;binary classification noisy;noisy labels multiclass;aggregating noisy labels;generalization error classification;improves accuracy generalization;classification;classification outperforming;classes classification;jointly modeling labels;binary classification;model binary classification;classification problem learned;supervised learning;learning noisy;learning model crowdsourced;supervised;better label examples;classifier;noisy annotations collected;multiclass classification;noisy labels propose;propose supervised learning;accuracy generalization;modeling labels worker;object classification outperforming;learning noisy crowd;supervised learning model"}, "ca3535dcdda9849350ad7c991a60660b22844f2f": {"ta_keywords": "learning hidden intermediates;intermediates speech recognition;recognition sub task;hidden intermediates speech;speech recognition sub;hidden intermediates sequence;speech translation model;searchable hidden intermediates;searchable hidden representations;hidden representations intermediate;intermediates sequence model;learning hidden;decomposed sub tasks;speech recognition;learn searchable hidden;unsupervised learning hidden;hidden representations;sub tasks;sub tasks proposed;hidden intermediates;representations intermediate stages;sub tasks demonstrate;recognition sub;sub task;intermediates speech;translation model extracts;framework speech translation;sequence model;translation model;systems learn searchable", "pdf_keywords": "sequence task predicting;speech translation encoder;sequence decomposable utterance;task speech translation;translation encoder trained;sequence sequence learning;sequence learning framework;sequence learning;speech translation intermediate;models automated speech;task translating speech;decoder models improve;speech translation decomposes;encoder decoder training;task predicting input;decomposable sequence tasks;sequence utterances;decoder training;sequence tasks;decoder trained;sequences decomposable tasks;translation speech recognition;desired sequence utterances;tasks including speech;end speech translation;decomposable speech;speech recognition tasks;automated speech;decoder trained output;predicting linguistic representation"}, "d56244c6abf3141900386d6911dd9097697a346b": {"ta_keywords": "web site classifier;site classifier based;classifier new web;bag words classifier;site classifier;text classifier;words classifier;text classifier improves;simple text classifier;page classifier;classifier based wrappers;words classifier new;resulting page classifier;page classifier able;accuracy bag words;text traffic world;words classifier resulting;classifier based;text traffic;classifier;classifier improves;classifier improves accuracy;bag words;classifier able handle;classifier new;new web;half text traffic;pages single web;new web site;classifier able", "pdf_keywords": ""}, "42605c1ee030721cb38a3c225992d63297a6ace0": {"ta_keywords": "language documentation conservation;language documentation revitalization;documentation revitalization;documentation revitalization context;documentation conservation;language revitalization technologies;practical language revitalization;language documentation;documentation conservation proposed;language revitalization;workshop language documentation;future natural language;natural language processing;field natural language;natural language;language processing;assist language documentation;documentation;language processing goal;workshop language;language processing aim;practical language;linguistic;workshop bring language;results workshop language;novel practical language;language community;bring language community;language;language community members", "pdf_keywords": "allosaurus speech recognition;recognize phones language;allomorphous speech recognition;phonetic representation language;phones language;new speech synthesis;combines phonetic models;speech recognition;convert phonetic;speech synthesis;speech recognition intended;synthesis combines phonetic;convert phonetic representation;language based allosaurus;speech recognition recognizes;language revitalization technologies;phonetic representation;documentary linguists technologists;speakers language documented;phonetic models;recognize sounds language;speech synthesis combines;combines phonetic;transcription efforts;building automatic language;language orthography development;phones language customize;inventory convert phonetic;documentary linguists;present allosaurus speech"}, "c8d0e13de2eaa09a928eff36b99d63f494c2f5ec": {"ta_keywords": "code natural language;language descriptions architecture;neural architecture generation;architecture based grammar;generation code natural;architecture generation code;generate code;programs natural language;able generate code;generation code;descriptions architecture able;natural language descriptions;language descriptions parallel;generate code set;grammar model captures;descriptions architecture;language descriptions proposed;set natural language;syntax natural;architecture able generate;neural architecture;novel neural architecture;natural language;architecture generation;propose novel neural;language descriptions;generation complex programs;captures target syntax;syntax natural way;code set natural", "pdf_keywords": "deep syntax driven;generating underlying syntax;deep syntax;deep semantic parser;parsing uses neural;generation semantic parsing;generative semantic parsers;generative semantic parser;generate deep semantic;semantic parsing tasks;parsing using deep;parsing natural language;generating abstract syntax;syntax driven;syntax limited training;semantic parsers;syntax target programming;semantic parsing;architecture powered grammar;parsing semantic;examples semantic parsing;semantic parser;parsing tasks demonstrate;syntax prior knowledge;semantic parsers source;generation code linguistic;approach semantic parsing;semantic parser present;syntax driven encoding;recover underlying syntax"}, "9c03d14520c897ca8536e165507f568d1980dabd": {"ta_keywords": "lexical matching approach;lexical matching method;lexical matching;using lexical matching;coreference resolution;coreference resolution method;strong lexical matching;types coreference resolution;question types coreference;text using lexical;coreference;document matching;example document matching;characterizing text approach;types coreference;characterizing text using;using lexical;characterizing text;method characterizing text;lexical;document matching problem;process characterizing text;text approach;based strong lexical;matching approach based;matching approach;strong lexical;matching;text using;matching method", "pdf_keywords": ""}, "2ea226a7fadde6a45f537c714e0832e83136f861": {"ta_keywords": "biomedical event extraction;event extraction;event extraction based;structured prediction;structured prediction search;approach biomedical event;biomedical event;combination structured prediction;use structured prediction;joint inference improves;prediction search;search new biomedical;prediction search based;prediction search new;biomedical signals;new biomedical signals;joint inference;biomedical signals use;framework joint inference;extraction based;biomedical;new biomedical;approach biomedical;extraction based use;prediction;extraction;inference improves;event;novel approach biomedical;use structured", "pdf_keywords": ""}, "708f8c0eb5032edd6f31663a27febbb0529cbcf3": {"ta_keywords": "visually grounded neural;learning syntactic representations;neuraltax;grounded neural neural;neural neuraltax;grounded neural;neural neural neuraltax;learning syntactic;neuraltax svn;neural neuraltax svn;syntactic representations;syntactic representations structures;structure text svn;neuraltax svn method;linguistic structure text;predictions linguistic structure;method learning syntactic;linguistic structure;neural neural;neural;predictions structure text;text svn;text svn able;syntactic;structure text;make predictions linguistic;predictions linguistic;explicit supervision svn;visually grounded;structures explicit supervision", "pdf_keywords": "grounded neural syntax;neural syntax learner;learns syntactic representations;grounded textual representations;neural syntax;learns syntactic;visually grounded language;trees grounded textual;visually grounded neural;neural syntax vg;syntax natural language;syntactic representations;textual representations;syntax learner;visual representations constituents;syntactic representations structures;parse trees captions;nsl learns syntactic;embeddings improves;parsing model multimodal;textual representations learning;syntax learner vg;neural learner visually;grounded textual;grounded language learning;word embeddings improves;syntactic supervision;learn parse trees;parse trees supervised;explicit syntactic supervision"}, "06e36261b21af2943e464a562c92c09dac292a82": {"ta_keywords": "program decompiler;program decompiler free;program decompiler powerful;tool reversecompiler analysis;reversecompiler analysis;reversecompiler analysis easy;reversecompiler analysis difficult;powerful tool reversecompiler;reversecompiler;decompiler free software;decompiler known tool;known tool reversecompiler;tool reversecompiler;content program decompiler;decompiler powerful tool;decompiler;decompiler free;decompiler known;used reverse execution;reverse execution;decompiler powerful;analysis easy recompilerize;easy recompilerize;tool used reverse;recompilerize;software tool used;easy recompilerize constrain;characterize content program;software tool;free software tool", "pdf_keywords": "dirty implemented decompiler;implemented decompiler programmatic;compilation decompilers;decompiled program based;architecture decompiler powerful;decompiled code;decompiler programmatic dirty;architecture decompiler;decompiled variable recompiler;names decompiled code;decompiler easily computable;reverse engineering binaries;decompiler programmatic;useful decompilerizing executing;decompiler decompilation based;implemented decompiler;decompiled code dirty;decompiled code tokens;million decompiled code;compilation decompilers able;dirty decompiled variable;decompiled program;decompilerizing executing code;decompiler powerful tool;new decompiler decompilation;improvements decompilers;use decompiled program;decompiled code used;decompiled functions;reverse compilation binary"}, "54e7de06a97b4b6c41e185c0bee60c838a15265a": {"ta_keywords": "articulatory speech modification;modify speech waveforms;modified speech waveforms;generation articulatory speech;generating modified speech;speech waveforms manipulating;speech waveforms manipulated;automated generation articulatory;enables modify speech;articulatory speech;speech modification framework;articulatory parameters production;modify speech;speech modification;manipulated articulatory parameters;articulatory parameters estimated;manipulating unobserved articulatory;unobserved articulatory parameters;speech waveforms;waveforms manipulated articulatory;articulatory parameters proposed;virtual articulatory parameters;articulatory parameters;generation articulatory;manipulated articulatory;virtual articulatory;unobserved virtual articulatory;modified speech;unobserved articulatory;statistical inversion production", "pdf_keywords": ""}, "7771aa7badc3375a31bfac8dc47755ff5d5c7780": {"ta_keywords": "subword level distributions;word subword level;word subword levels;subword levels tend;distributions word subword;morphological complexity;subword level;subword levels;types morphological complexity;word subword;bpe probability distributions;process bpe probability;morphological;levels bpe merges;bpe probability;bpe merges;different types morphological;types morphological;subword;incremental process bpe;bpe merges interaction;probability distributions word;process bpe;interaction word subword;distributions interpreted terms;levels bpe;bpe;distributions incremental process;relationship word subword;distributions word", "pdf_keywords": "tokenization text entropy;subword tokenization;subword tokenization text;subword representations;subword tokenization use;entropy orthographic words;entropy word level;subword regularization significantly;subword level predictability;suitable subword representations;text entropy;subword units predictable;entropy subword;subword regularization computational;subword regularization accuracy;subword representations multilingual;entropy subword units;predicting accuracy subword;subword word complexity;text relies entropy;subword regularization generally;levels subword tokenization;subword regularization effective;word complexity merges;subword regularization;linguistics accuracy subword;subword entropies;accuracy subword regularization;languages subword regularization;terms subword entropies"}, "79c93274429d6355959f1e4374c2147bb81ea649": {"ta_keywords": "cross modality encoders;cross modality encoder;cross modality representations;learning cross modality;modality encoders;modality encoder;modality encoder encoders;modality representations elementary;objects encoders elementary;modality encoders goal;elementary objects encoders;encoders elementary objects;objects cross modality;modality representations;objects encoders;encoders elementary;cross modality;encoders framework learning;propose cross modality;representations elementary objects;encoders;encoders framework;encoders goal learn;encoder encoders;encoder;encoder encoders framework;learning cross;elementary objects cross;encoders goal;elementary objects", "pdf_keywords": "attention crossmodality representationswe;attention crossmodality representations;visual question answering;visual reasoning nlvr2;attention cross modality;language encoder attention;visual reasoning dataset;learning cross modality;encoder attention graph;encoder attention;cross modality encoder;cross modality encoders;challenging visual reasoning;joint attention crossmodality;visual reasoning generalizability;attention graph language;visual reasoning task;language visual reasoning;attention crossmodality;vision language task;encoder cross modality;learning models attention;task natural language;cross modality encoderwe;object crossmodality encoder;models attention objects;crossmodality representationswe;crossmodality representationswe propose;encoder respectively attention;crossmodality representations"}, "03e4f33c0ccc4cb8c7e1589158a5377cdf5241d2": {"ta_keywords": "qualitative preferences agent;conditional qualitative preferences;predicting preferences human;preferences human agent;agent subjective preferences;qualitative preferences corresponding;preferences corresponding ethical;preferences corresponding human;subjective preferences corresponding;preferences agent conditional;qualitative preferences;agent conditional qualitative;predicting preferences;qualitative preferences used;community conditional qualitative;preferences human;subjective preferences;human community conditional;model predicting preferences;principles community conditional;agent subjective;agent conditional;community conditional;preferences agent;conditional qualitative;conditional conditional qualitative;preferences corresponding;relations agent subjective;outcomes given decision;ethical principles community", "pdf_keywords": ""}, "d5f22dbc8f4b9e99f62e6ecf886bc4b9a0372e4d": {"ta_keywords": "large classes web;classes web algorithm;classes web;exploration large classes;large classes;web algorithm based;web algorithm;hierarchical algorithm exploration;classes;hierarchical;novel hierarchical algorithm;algorithm exploration large;hierarchical algorithm;novel hierarchical;algorithm exploration;present novel hierarchical;web;algorithm based;algorithm;exploration large;large;exploration;based;novel;present;present novel", "pdf_keywords": ""}, "680e61a17e27a1e8e121276c7ec53fc4fd40babb": {"ta_keywords": "information density;information density uid;reading time acceptability;uniform information density;time acceptability data;acceptability data strongest;acceptability data;hypothesis uniform information;uniform information;document strongest predictor;sentence document strongest;data strongest;time acceptability;reading time;acceptability;regression phrase document;information;data strongest predictor;context reading time;strongest predictor uid;regression phrase;document strongest;uid regression mean;data;density uid;sentence document;uid regression;phrase document;strongest predictor;density", "pdf_keywords": "linguistic information distribution;language underlying distribution;thethe linguistic information;linguistic information;distribution information linguistic;information linguistic signal;linguistic process hypothesis;language comprehension investigate;information linguistic signals;linguistic signal surprisal;information linguistic;measure linguistic;language users utterances;linguistic signals level;linguistic signal;linguistic unit optimally;language user lexical;measure linguistic unit;linguistic signal specifically;variance linguistic signal;information exchange linguistic;measure variance linguistic;studies language comprehension;linguistic signals;exchange linguistic words;linguistic words;explains linguistic;language production explored;linguistic process;variance linguistic"}, "34f8214cbaa0655794c2c9570898abf15649b079": {"ta_keywords": "recognition reverberant speech;reverberant speech based;reverberant speech reverberation;using reverberant speech;speech reverberation;speech based dereverberation;speech reverberation time;reverberant speech;recognition reverberant;method recognition reverberant;reverberation;using reverberant;reverberation time;reverberant;experiment using reverberant;reverberation time revealed;compared recognition dereverberated;recognition dereverberated;dereverberation proposed method;speech based;adaptive training implemented;based dereverberation proposed;using adaptive training;rate compared recognition;adaptive training;using adaptive;dereverberation;based dereverberation;optimized using adaptive;dereverberation proposed", "pdf_keywords": ""}, "3aba582b62d1abfcd95264e6c7b32aab4c9db4b8": {"ta_keywords": "text classifier interpretable;self explainability model;novel self explainability;self explainability;explainability model;text classifier;classifier interpretable;predictions text classifier;classifier interpretable robust;explainability model explains;text classifier using;explainability;illustrated text classifier;continuation interpretable robust;interpretable robust;predictions text;interpretable robust model;classifier;explains predictions text;continuation interpretable;interpretable;algebraic continuation interpretable;classifier using simple;classifier using;model explains predictions;text;model illustrated text;model explains;explains predictions;illustrated text", "pdf_keywords": "explaining neural text;self explaining neural;predictions neural text;neural text classification;neural text classifier;language explanations ai;explanations ai;natural language explanations;concept aware encoder;explanations model self;predictions explanations model;self explaining architectures;neural text;explaining neural;predicting semantics natural;self explaining model;natural language representations;novel self explaining;learning annotator;explainable ai significantly;encoder trained interpret;called self explaining;text classification tasks;predicting semantics;predictions neural;explains predictions neural;novel concept aware;predict semantics;self explaining framework;explainable natural language"}, "e5a5888966be6b5f9c0e8a82facd604086a1ee4c": {"ta_keywords": "automatically tagging genes;tagging genes;tagging genes help;term detector chimera;term detector;automatically identify gene;automatically tagging;improvement accuracy tagging;reference accuracy tagging;accuracy tagging demonstrated;hmm based bionerner;parser called chimera;tagging demonstrated identification;accuracy tagging;identify gene names;method automatically tagging;gene names articles;accuracy tagging tested;tagging;bionerner crf syntax;gene names;parser;syntactic parser;automatically identify;tagging demonstrated;parser called;near term detector;syntactic parser called;tagging tested;project hmm based", "pdf_keywords": ""}, "73271677da83a3f55523148d1b43a0501f0a35dd": {"ta_keywords": "robustness online learning;equilibrium robustness online;online learning periodic;convergence online learning;online learning behavior;learning behavior robust;invariant equilibrium robustness;online learning;equilibrium robustness;exogenous variations game;zero sum games;robust variations game;learning periodic zero;robust exogenous variations;robustness online;behavior robust exogenous;time invariant equilibrium;game context convergence;games time invariant;poincar\u00e9 robustness online;learning periodic;robust exogenous;invariant equilibrium;study robustness online;formally poincar\u00e9 robustness;behavior robust variations;learning behavior;behavior robust;poincar\u00e9 robustness;sum games", "pdf_keywords": "periodic game theorems;periodic game time;games learning dynamics;learning dynamics periodic;periodic game;payoffs periodic game;sum game periodic;game periodic payoffs;leader learning periodic;game periodic;learning periodic zero;game basis periodic;online learning dynamics;zero sum games;learning dynamics games;polymatrix games learning;learning dynamics;sum games learning;sum games cyclic;time invariant nash;period periodic game;learning dynamics characterized;zerosum polymatrix games;games payoffs periodic;game payoffs periodic;dynamics player periodic;learning periodic;zero sum game;dynamics game theorem;constant sum game"}, "74fb2834c820d2297b08201cb72de1c1d3d27f54": {"ta_keywords": "gender accent masking;protecting privacy users;protecting privacy;privacy users;cloud based speech;models protecting privacy;accent masking;privacy users process;privacy;client privacy;speech processing designed;accent masking conclude;based speech processing;spanning gender accent;speech processing;gender accent;research client privacy;based speech;class models protecting;masking;users process reading;accent;cloud based;models protecting;masking conclude;cloud;protecting;tasks spanning gender;reading transmitting data;transmitting data models", "pdf_keywords": "privacy speech recognition;privacy speech data;developments privacy speech;privacy speaker identity;research privacy speaker;privacy approaches speech;ensuring privacy speech;privacy speaker;privacy speech;privacy ability transform;privacy preserving;approach privacy speech;privacy downstream tasks;preserving privacy;obtain privacy preserving;extensible preserve privacy;privacy computation;privacy guarantees formalized;performance privacy computation;recent developments privacy;privacy preserving data;preserve privacy data;preserve privacy;encode speaker information;protecting gender accent;privacy downstream;privacy computation performed;privacy guarantees;ensuring privacy downstream;privacy ability"}, "030d7d7ae48a9f81700b2c1f7cf835235777b8e7": {"ta_keywords": "retriever search effective;retriever search engine;retriever search;define retriever search;model retriever search;retriever search use;search effective handling;scalable model retriever;search effective;search use model;model search depinning;search engine effective;search depinning candidates;model search;use model search;search engine;search use;search depinning;depinning model retriever;search;model retriever;model define retriever;define retriever;retriever;scalable model;large volumes data;propose scalable model;data generated depinning;scalable;propose scalable", "pdf_keywords": "question passage retriever;retrieval natural questions;extraction improved retrieval;progressively improve retrieval;improve retrieval;recent retrieval model;employs recent retrieval;relevance guided supervision;improved retrieval;answered text corpora;improved retrieval results;questions answered text;neural retrieval model;passage retriever generalization;recent retrieval;relevance guided;search natural language;learns retrieve passages;retrieval model train;passage retriever learns;improve retrieval quality;recent neural retrieval;unsupervised retrieval;retrieve passages;extract challenging questions;propose relevance guided;passage retriever;approach unsupervised retrieval;retrieval natural;retrieval"}, "7c655ef6f0de8c1a219cdb796c77f4ae3c389b82": {"ta_keywords": "species dna landscape;dna landscape method;model dna landscape;dna landscape based;dna landscape self;dna landscape;able distinguish species;species dna;species species dna;distinguish species;species spatially homogeneous;assumption species spatially;distinguish species species;species spatially;discrimination species species;interaction based dna;based dna field;model dna;discrimination species;applied model dna;based assumption species;method discrimination species;based dna;dna field;spatially homogeneous interact;landscape based assumption;assumption species;species;landscape method;species species", "pdf_keywords": ""}, "b2b0fbf9033f1c36bea8bb11c173f14378c60db9": {"ta_keywords": "speech s2s translation;s2s translation based;translation based speech;speech speech s2s;speech s2s;speech s2s combination;s2s speech speech;speech s2s speech;s2s combination speech;s2s speech;s2s translation;translation based;based speech speech;speech speech;combination speech speech;based speech;speech;s2s;s2s combination;combination speech;translation;present speech speech;present speech;based;combination;present", "pdf_keywords": ""}, "262c0e54370dfc03a7ad53d79930568d18dd448c": {"ta_keywords": "distributed machine learning;italic distributed machine;coded italic distributed;algorithms using italic;italic distributed;italic coded;coded italic;coded italic codes;codes italic coded;italic codes;italic codes italic;italic coded italic;distributed machine;codes italic;framework accelerating distributed;algorithm uses inline;accelerating distributed machine;using italic coded;data shuffling italic;machine learning algorithms;distributed;math notation inline;italic data shuffling;accelerating distributed;notation inline;machine learning algorithm;notation inline formula;algorithms using;italic;italic data", "pdf_keywords": "distributed algorithms matrix;distributed algorithm matrix;coded distributed algorithms;distributed machine learning;distributed learning algorithms;uncoded distributed algorithm;distributed algorithms;learning data distributed;coded distributed algorithm;distributed algorithms finally;problem distributed algorithms;data distributed learning;existing distributed algorithms;distributed computation;distributed algorithms introducing;distributed algorithms running;problem distributed computing;distributed algorithm;uncoded distributed;distributed learning;distributed algorithm possible;distributed algorithms optimal;distributed algorithms runtime;distributed computing;uncoded coded distributed;distributed learning successfully;distributed algorithm optimal;distributed algorithm linear;distributed algorithm achieve;distributed learning used"}, "14a058a1e41459a30327bb5fb480d51430b6a096": {"ta_keywords": "discovery geneid ranking;geneid ranking based;ranking gene associations;geneid ranking;method ranking gene;rank gene associations;ranking gene;gene discovery geneid;discovery geneid;gene associations based;method gene discovery;approach gene discovery;gene discovery;rank gene;gene discovery named;gene discovery novel;used rank gene;gene associations;gene associations use;novel method ranking;discovery named entity;geneid;discovery novel method;ranking based;method ranking;method gene;novel method gene;approach gene;novel approach gene;ranking based use", "pdf_keywords": ""}, "a6d505a6e46c15ef0d213b9a4349ce2f852be894": {"ta_keywords": "partial mixture learning;mixture learning algorithms;mixture estimation partial;mixture learning classifier;algorithms mixture estimation;mixture learning;mixture estimation;estimator partial mixture;partial mixture;partial partial mixture;novel algorithms mixture;algorithms mixture;mixture proportion estimator;using mixture;mixtures;mixtures computed;mixture;mixtures computed using;mixture proportion;examples small mixtures;small mixtures computed;using mixture proportion;computed using mixture;small mixtures;estimation partial partial;proportion estimator partial;estimation partial;supervised learning;supervised learning setting;learning classifier", "pdf_keywords": "learning mixture classifier;learning mixture;classifier mixture;methods learning mixture;mixture classifier;classifier empirically;mixture classifier accurate;unlabeled data learning;data learning positive;distribution classifier empirically;classifier mixture pvu;classifier empirically valid;optimal classifier;negative classifier;pvu classifier mixture;mixture proportion training;optimal classifier trained;classifiers based empirical;classifier robust loss;classifier propose;classifiers;learn classifier;classifier learning partial;estimating mixture;learning classifier;classifier;versus negative classifier;underlying distribution classifier;classification;loss function classifier"}, "b08545e1281c1eb748e4474687eb61fd3b25d1a6": {"ta_keywords": "novel language discovery;language discovery;language discovery platform;novel words;novel language;500 novel words;provide novel language;novel words published;present novel dataset;novel dataset;words published;words published ny;novel dataset 500;dataset 500 novel;paper wit;words;discovery platform;discovery;language;wit;sci;provide novel;dataset;natl acad sci;acad sci;sci usa provide;500 novel;novel;usa provide novel;ny paper wit", "pdf_keywords": "word innovation corpus;novelty lexical;context linguistic novelty;linguistic novelty;class novelty lexical;novel word nodes;innovation corpus;word innovation;word innovation type;innovation corpus contains;linguistic novelty class;novel word innovation;contextual prediction novelty;novelty lexical derivation;word lexical;contextuality word pieces;words context new;annotated class novelty;word lexical normalization;characterizing lexical;word nodes;method characterizing lexical;times word innovation;characterizing lexical contribution;lexical contribution;lexical;novelty class annotations;new class linguistic;novel linguistic structures;create novel linguistic"}, "43fae0a7af211d91557d115d2f82e3c46d8bf022": {"ta_keywords": "language generation nlg;natural language generation;generation nlg including;generation nlg;language generation;nlg including compression;nlg including;creation metrics;transduction creation metrics;nlg;interpretable metrics;creation metrics evaluated;interpretable metrics suitable;natural language;family interpretable metrics;generation;output text evaluated;metrics achieve stronger;models intrinsic alignment;metrics achieve;output text;interpretable;metrics evaluated using;relies intuitions metrics;aspects natural language;metrics evaluated;compression transduction creation;designed family metrics;creation;metrics suitable evaluating", "pdf_keywords": "natural language generation;generation text summarization;language generation nlg;language generation tasks;dialog story generation;approach generation text;language generation benchmark;language generation evaluation;generation nlg;general language generation;generation text;summarization natural language;generation nlg broad;summarization tasks;captioning summarization dialog;captioning summarization;generation tasks;generation tasks quality;summarization natural;summarization dialog;programming tasks text;summarization tasks style;language generation;story generation;summarization experiments;text summarization natural;summarization dialog story;image captioning summarization;evaluation general language;summarization process significantly"}, "b0894f5c914cd90cc3b3e16b15bec11efe317b14": {"ta_keywords": "test detecting strategic;behaviour peer assessments;peer assessments test;detecting strategic behaviour;peer assessments;detecting strategic;strategic behaviour peer;behaviour peer;behaviour collection assessments;strategic behaviour collection;test detecting;assessments test designed;assessments evaluate test;strategic behaviour;principled test detecting;assessments test;patterns strategic behaviour;evaluate test practical;assessments evaluate;evaluations demonstrate strong;test practical;strategic;assessments;evaluate test;evaluations demonstrate;test designed detect;test designed;principled test;test;patterns strategic", "pdf_keywords": "manipulations peer assessment;reviewers evaluate peers;peer assessment powerful;peer evaluation;peer assessments ubiquitous;peer assessment tasks;process peer evaluation;peer assessment;peer submissions evaluation;peer evaluation performed;peer assessments;non peer assessment;manipulation reviewers ranking;peer assessment allows;evaluate peers;behaviour peer assessment;peer review;detect manipulation reviewers;output rankings reviewers;rankings reviewers;rankings reviewers incentivized;given peer review;peer assessment setup;peer review easy;reviewers incentivized evaluate;rankings return reviewers;assess quality reviewers;evaluate quality peer;hypothesis reviewers;evaluate counterparts strategically"}, "f481d6dea08e348cecd5eb23a813d47373e62a94": {"ta_keywords": "programs natural language;programming natural language;natural language code;automatic explanation programs;code modeling natural;natural language specifications;modeling natural language;language code modeling;natural language;explanation programs natural;code language;code modeling;explanation programs;code language automatic;natural language methods;programming natural;learning models programs;natural language elements;language automatic generation;natural language discuss;language code language;language code;automatic explanation;automatic generation programs;tutorial machine learning;differences programming natural;programs natural;language automatic;programming;models programs natural", "pdf_keywords": ""}, "c7424d651d60ef9f052e91bff18efd88782225a3": {"ta_keywords": "election breaking ties;ties stage voting;controlling result election;ties strategically np;election breaking;breaking ties strategically;result election breaking;compute break ties;ties strategically;break ties;voting rule;complexity controlling result;breaking ties;strategically np hard;stage voting rule;election;stage voting;voting;break ties stage;computational complexity controlling;complexity controlling;result election;computational complexity;strategically np;complexity;ties;study computational complexity;ties stage;np hard compute;computational", "pdf_keywords": "complexity election tiebreaking;complexity controlling election;controlling complexity election;election tiebreaking;election tiebreaking assume;complexity election;candidate tie breaking;controlling election breaking;consider complexity voting;ties stage voting;candidates tied votes;control election breaking;single election method;theorems electionwe consider;election breaking ties;election method;complexity voting;election theorems election;breaking problem election;theorems electionwe;election theorems;underlying voting rules;election polynomial tie;election method based;controlling election;theorems election;given candidate tie;candidate tie;theorems election theorems;variations underlying voting"}, "19a6e362840d3a2d27d0fa5509eaa4d4597a2859": {"ta_keywords": "particles fluid energy;charged particles fluid;fluid energy momentum;momentum charged particles;fluid energy;charged particles;energy momentum charged;charged particles related;particles fluid;universal scaling law;mass energy momentum;momentum charged;zero mass energy;particles related universal;scaling law;energy momentum;scaling law scaling;mass energy;particles;fluid non zero;non zero mass;calculation energy momentum;law scaling law;zero mass;law scaling;particles related;scaling law valid;related universal scaling;law valid fluid;fluid non", "pdf_keywords": ""}, "b145a46718f293429054f0a9a4cdd2de94813b37": {"ta_keywords": "search hyperlinked pages;effective search hyperlinked;search hyperlinked;web search method;method search hyperlinked;hyperlinked pages web;web search;hyperlinked pages;gaussian distributed search;wide web search;distributed search algorithms;distributed search;use hyperlinked pages;hyperlinked pages link;links results;available links results;use hyperlinked;hyperlinked;search algorithms;links results compared;based use hyperlinked;search method;effective search;search algorithms algorithm;search;publicly available links;shown effective search;search method shown;pages link;new method search", "pdf_keywords": ""}, "5ea3c08614e9673a109f581cf114af488f3aa601": {"ta_keywords": "automatic embryo staging;embryo staging;method automatic embryo;automatic embryo;classifier isolate embryo;residual networks best;isolate embryo background;residual networks;embryo background develop;downstream classifier;vanilla residual networks;embryo background;embryo staging exploits;embryo;lapse data train;sequence developmental stages;region proposal network;network downstream classifier;downstream classifier isolate;data train region;sequence developmental;isolate embryo;data train;decoder postprocesses predictions;developmental stages;structure time lapse;dynamic programming;train region proposal;stages;proposal network downstream", "pdf_keywords": "embryo detection video;embryo deep learning;predicting embryo stages;embryo stages train;automated embryo staging;predicts stage embryo;datasets demonstrate embryo;images region embryo;stages using deep;potential embryo dataset;predicting embryo;embryo based dataset;embryo detection;predict embryo;automated embryo;embryo dataset;method predicting embryo;stages embryo;able predict embryo;stage embryo;embryo stages;detect region embryo;predicting potential embryo;predict embryo initiation;embryo predictions;problem embryo detection;embryo deep;task demonstrate embryo;embryo staging;predict potential embryo"}, "6ab36d2577f7c9487b28b2bcdf236191ba901aad": {"ta_keywords": "task oriented dialogs;dialog agent helps;dialogs user learn;dialog agent;dialogs;dialog;dialogs way;interaction dialogs;dialogs tod;deal interaction dialogs;tod dialog agent;oriented dialogs;oriented dialogs tod;resulting dialogs;dialogs way able;process resulting dialogs;resulting dialogs way;dialogs tod dialog;learning task oriented;dialogs user;interaction dialogs user;end learning task;tod dialog;consisting 738 dialogs;learning task;end end learning;738 dialogs;task oriented;end learning;dialogs grounded", "pdf_keywords": "end learning dialogs;learning dialogs;task oriented dialogs;learning dialogs grounded;task oriented dialog;task oriented dialogue;dialogs;dialog flo dataset;systematically builds dialog;dialog;dialogue useful representation;dialogue modeling combines;dialogs tod;dialog tod;oriented dialogs;collect corpus dialogs;oriented dialogue systems;learning yes dialog;dialogue systems;conversation task exposes;starting dialogs;oriented dialogs tod;conversation task;using dialog;oriented dialog tod;conversational agents task;dialog flo;dialogue modeling;dialog referred;corpus dialogs"}, "1b114486d67252ff83fc90d4a8607636045c54ce": {"ta_keywords": "quality aligned data;high quality aligned;aligned data;quality aligned;aligned;method high quality;data;high quality;propose method high;quality;method high;propose method;method;high;propose", "pdf_keywords": ""}, "518cb6d4247bdebf21e2811f296b0c7372602a0a": {"ta_keywords": "multilayer masking;principled masking strategy;multilayer masking technique;random uniform masking;masking strategy based;masking strategy;principled masking;propose principled masking;uniform masking proposed;masking technique;uniform masking;masking proposed strategy;masking;masking proposed;masking technique tokens;mask uniformly random;version multilayer masking;tokens mask uniformly;mask uniformly;technique tokens mask;mask;pointwise mutual information;tokens mask;mutual information;mutual information pmi;multilayer;concept pointwise mutual;drawback random uniform;pointwise mutual;unifies improves prior", "pdf_keywords": "masking word spans;random token masking;collocations large corpora;masking vocabulary;masking vocabulary entries;high collocation corpus;method masking vocabulary;collocation corpus;subword tokens;mask located words;masks token gram;collocation corpus introduce;large corpora method;highly collocating gram;multiple subword tokens;method large corpus;masking word;large corpus;collocations effective learning;masked subset text;subword tokens uniform;token masking;large corpus grams;token gram;method masking word;correlated grams text;predict linguistic structure;token gram exhibits;predicting linguistic structure;token masking constitutes"}, "aa9e0bf1e22563fca053578315b857688a0817cb": {"ta_keywords": "dialogue systems simulator;user simulator task;dialogue systems;task oriented dialogue;user simulator;user simulator movie;simulator movie booking;oriented dialogue systems;novel user simulator;simulator task oriented;simulator task;dialogue;simulator demonstrated agents;simulator;oriented dialogue;serve user simulator;simulator movie;simulator designed;systems simulator;booking seeking interactions;performance simulator;described performance simulator;systems simulator designed;simulator designed serve;movie booking;agent;performance simulator demonstrated;simulator demonstrated;movie booking seeking;agent described", "pdf_keywords": "learns dialogue policies;dialogue systems learns;systems learns dialogue;dialogue trained dialog;learns dialogue;model dialogue agent;automatically learn dialogue;dialogue systems;dialogue systems proposed;dialogue task;task oriented dialogue;sequence model dialogue;dialogue systems increasingly;human machine dialogues;trained dialog;oriented dialogue systems;dialogue state tracking;unstructured dialogue systems;dialogue agent sequence;spoken dialogue systems;machine dialogues;completion dialogue trained;dialogue systems propose;dialogue activity;trained dialog act;user dialogue;dialogue task goal;dialogue agent;dialogue strategies sequence;completion dialogue task"}, "6b4ca249b3b28d3fee65f69714440c08d42cee64": {"ta_keywords": "unregularized generative generative;continuous unregularized generative;unregularized generative;generative image models;general generative image;generative generative image;results general generative;generative image;generative generative;general generative;stabilize generative generative;generative;image models convergent;stabilize generative;proposed stabilize generative;convergent discuss regularization;regularization strategies;image models;regularization;gradient penalties generator;regularization strategies recently;distributions absolutely;discuss regularization strategies;discuss regularization;image models prove;gradient penalties;convergence simplified gradient;distributions absolutely continuous;absolutely continuous unregularized;continuous unregularized", "pdf_keywords": "convergence regularized gan;gan training converges;regularized gan training;gan training convergent;gan training convergence;regularized gan;convergence gan training;gan converges locally;gans able converge;convergence gans theorems;convergence guaranteed gan;gan regularized;convergence gans;theorems convergence gans;gans theorems convergence;gan converges;continuous unregularized gan;theorems convergence gan;gans converge;gans linearly convergent;unregularized gan training;convergence gan;gan optimization locally;generator gan regularized;convergence generative adversarial;gans converge stationary;stabilizes training gans;networks gans converge;dynamics gan converges;gan regularized vector"}, "993c184553c41ca9134f149a3eb71b5bfab298b5": {"ta_keywords": "disclosed accounts twitter;accounts twitter;narrative network maneuvers;sponsored information operations;broader digital campaign;accounts twitter platform;groups accounts engaged;view modularity clustering;twitter;accounts engaged distinct;distinct narrative network;accounts engaged;disclosed accounts;digital campaign;modularity clustering;information operations;methodology disclosed accounts;twitter platform discover;twitter platform;coordinated;narrative network;holistically determine coordinated;information operations distinct;modularity clustering mvmc;identify groups accounts;groups accounts;operations distinct strategic;accounts;view modularity;multi view modularity", "pdf_keywords": ""}, "6916118de98cb5293425c8f74919395a003e6076": {"ta_keywords": "methods text categorization;text categorization;text categorization compare;categorization compare propositional;ilp methods text;categorization compare;categorization;ilp methods;performance ilp methods;ilp;propositional counterparts eeectiveness;propositional counterparts;compare propositional counterparts;order representation propositional;propositional rule flipping;representation propositional rule;propose new ilp;performance ilp;new ilp;order representation advantageous;methods text;compare propositional;counterparts eeectiveness methods;study performance ilp;rst order representation;representation propositional;propositional;order representation;propositional rule;eeectiveness methods", "pdf_keywords": ""}, "affdfafb0293b44412ec99ff39b114de5e83eb98": {"ta_keywords": "model dynamics xmath0;xmath1 model;xmath0 xmath1 model;dynamics xmath0 xmath1;xmath2 model;xmath2 model xmath3;xmath3 model;xmath1 model presence;xmath3 model known;dynamics xmath0;study dynamics xmath0;model xmath3;model xmath3 model;predictions xmath2 model;xmath1;xmath0 xmath1;predictions xmath2;xmath2;xmath0;xmath3;model solved quenched;compared predictions xmath2;numerical study dynamics;quenched approximation;quenched approximation using;solved quenched approximation;known model dynamics;model dynamics;attractive field model;present numerical study", "pdf_keywords": ""}, "88347f9f12b50590f50aefce4cf71b3a3f0bd138": {"ta_keywords": "language grounding 3d;oriented language grounding;language grounding;grounding 3d environments;reinforcement imitation;grounding 3d;novel reinforcement imitation;3d game engine;reinforcement imitation learning;task oriented language;3d game;imitation learning;game engine simulate;imitation learning approach;instructions environment;novel reinforcement;3d environments;based 3d game;language grounding rich;combines novel reinforcement;reinforcement;environment based 3d;simulate challenges task;instructions environment states;3d environments proposed;simulate challenges;oriented language;instructions unseen maps;game engine;task oriented", "pdf_keywords": "imitation learning architecture;language grounding 3d;learning approaches imitation;learning imitation;visual language grounding;approaches imitation learning;imitation learning train;learning imitation learning;imitation learning;imitation learning policy;language grounding reinforcement;grounding reinforcement learning;learning module imitation;module imitation learning;imitation learning bagnell;reinforcement learning imitation;imitation learning whichwe;perceptual knowledge trainable;oriented language grounding;language grounding;approaches imitation;language grounding challenging;3d environment reinforcement;describing behavior robotic;learning agent;language grounding raw;predicting optimal action;language grounding framework;optimal action machine;learning train agent"}, "5e10a61b34867c6e5b32ed7a1359bd47bbfb5e2d": {"ta_keywords": "explanation based learning;learning calledabductive explanation;explanations training example;explanations training;calledabductive explanation based;possible explanations training;based learning calledabductive;knowledge level explanation;calledabductive explanation;explanation based;learning calledabductive;choose possible explanations;worse explanation based;inconsistent explanation problem;training example;explanations;multiple inconsistent explanation;learning ebl solves;based learning;based learning ebl;level explanation based;worse explanation;inconsistent explanation;training example ebl;explanation problem using;learning;certain type knowledge;learning ebl;knowledge level;set covering techniques", "pdf_keywords": ""}, "e3862b1ff18dbb6a421b9efd1c0db22e09644b6d": {"ta_keywords": "harmonic trap particle;dimensional harmonic trap;harmonic trap;particle confined trap;trap particle confined;dynamics particle trap;trap particle;region trap particle;confined trap trap;particle trap;particle dimensional harmonic;confined trap;trap placed defined;trap trap placed;particle trap key;dynamics single particle;trap placed;defined region trap;particle confined;trap key understanding;trap trap;dynamics particle;dimensional harmonic;region trap placed;study dynamics particle;particle confined defined;trap;harmonic;single particle;region trap", "pdf_keywords": ""}, "6f69fcacdf53a811ef18c5e9ac8ec58035dc43fc": {"ta_keywords": "connectionist temporal classification;transducer generalizes recurrent;temporal classification ctc;network rnn loss;network rnn;recurrent neural;recurrent neural network;generalizes recurrent neural;neural network rnn;rnn loss;temporal classification;transducer based connectionist;based connectionist temporal;rnn;connectionist temporal;graph representation labels;transducer losses;standard rnn;transducer losses restricting;rnn loss accept;generalizes recurrent;transducer generalizes;novel transducer generalizes;different transducer losses;training lattices;classification ctc like;manipulate training lattices;classification ctc;neural;loss accept graph", "pdf_keywords": "network transducer rnn;neural network transducer;transducer rnn loss;transducer rnn;connectionist temporal classification;temporal classification ctc;networks classifying label;networks trained;recurrent neural network;networks trained training;stateless prediction networks;transducer based connectionist;label states neural;networks classifying;recurrent neural;temporal classification;rnn loss;prediction networks vertex;transducer losses implementing;network transducer;states neural network;based connectionist temporal;prediction networks;transducer losses;generalizes recurrent neural;network model neural;model speech recognition;graph representation labels;networks able predict;transducer losses restricting"}, "31dc1e65d61a431964c75bf2eec167bcd9dca0fa": {"ta_keywords": "cavity field arbitrarily;particle cavity field;motion particle cavity;force field arbitrarily;field motion particle;magnetic field motion;field arbitrarily small;cavity field;particle cavity;field arbitrarily large;field motion;force field;suitable force field;magnetic field;effect magnetic field;field arbitrarily;motion particle;effect magnetic;particle;applying suitable force;magnetic;arbitrarily small applying;study effect magnetic;suitable force;arbitrarily large applying;arbitrarily small;field;force;small applying suitable;arbitrarily large", "pdf_keywords": ""}, "86471bf927401bf88af83626797228c2bf10a282": {"ta_keywords": "social attribution;process social attribution;social science resulting;social attribution powerful;behavior propose causal;social science;social behavior model;science propose causal;behavior process social;social behavior;social science propose;propose causal model;version social science;causal model;attribution;propose causal;process social;causal;social science rich;social;human behavior;features social behavior;causal model captures;human behavior process;attribution powerful tool;attribution powerful;human behavior propose;study human behavior;understand human behavior;behavior model", "pdf_keywords": "attribution termed predict;social attribution model;formalization social attribution;social attribution based;attribution model faithful;attribution model;attribution intent humans;robust social attribution;process social attribution;social attribution intent;faithful social attribution;social attribution;social attribution termed;attribution based;attribution intent formalize;attribution faithfulness;attribution intent causal;aligned social attribution;attribution intent;attribution termed;attribution faithfulness natural;causal attribution faithfulness;interpretations model prediction;methodology social attribution;attribution;causal attribution;predictions natural language;model faithful social;explanations artificial models;attribution based analysis"}, "aa30949af5b59624224980e7d741ad8c084271ec": {"ta_keywords": "tweets discussedvaccinationvaccination messages;majority ofvaccination messages;ofvaccination messages consistent;ofvaccination messages;unique tweets discussedvaccinationvaccination;discussedvaccinationvaccination messages;tweets discussedvaccinationvaccination;discussedvaccinationvaccination messages analysis;ofvaccination messages shared;structure ofvaccination messages;discussedvaccinationvaccination;showed majority ofvaccination;ofvaccination;majority ofvaccination;ongoing pandemic 2019;ongoing pandemic;anti symmetricvaccination;pandemic 2019;cause ongoing pandemic;pandemic 2019 identified;current coronavirus;covid 19 cause;structure ofvaccination;symmetricvaccination;pandemic;cause current coronavirus;covid 19;current coronavirus disease;coronavirus disease covid;coronavirus", "pdf_keywords": ""}, "d35534f3f59631951011539da2fe83f2844ca245": {"ta_keywords": "photographs generative adversarial;training generative adversarial;generative adversarial networks;generative adversarial network;generative adversarial;training generative networks;generative networks;training generative;photographs generative;adversarial networks learns;distinct photographs generative;generative networks uses;adversarial network;adversarial networks;approach training generative;adversarial;face verification;generative;identity matched photograph;adversarial network use;face verification demonstrate;learns latent codes;matched photograph;latent codes identities;networks learns;networks learns latent;matched photograph provide;learns latent;distinct photographs;evoke identity matched", "pdf_keywords": "generative models face;gans dataset;models train gans;features train gans;train gans dataset;gans dataset product;training generative adversarial;trained generative;gans leverage discriminative;training generative;conditional generative adversarial;generative adversarial networks;conditional gans;generative models train;generative adversarial;gans dataset productwe;trained generative model;combines gans encoder;gans encoder;joint generative discriminative;generative adversarial network;generative view;learn generative models;generative discriminative;train gans;generative models;dataset face photographs;using conditional gans;combines gans;approach combines gans"}, "61a07d1e4eaa831152e253b96b91808ef3a184b4": {"ta_keywords": "crowdsourcing marketplaces ensembling;sourcing natural language;data annotation;crowdsourcing;natural language data;language data annotation;data annotation discuss;crowdsourcing marketplaces;use crowd sourcing;labeling data;labeling data provide;accurately labeling data;crowd sourcing natural;crowd sourcing;largest crowdsourcing;run largest crowdsourcing;annotation;largest crowdsourcing marketplaces;annotation discuss challenges;challenges accurately labeling;annotation discuss;natural language;labeling;language data;tutorial use crowd;accurately labeling;data collection projects;use crowd;practical solution labeling;data collection", "pdf_keywords": ""}, "aead4418733b998792deb9cbf198a834449e00d2": {"ta_keywords": "symbolic distribution phylogenetic;distribution phylogenetic trees;symbolic distribution;symbolic distribution develop;symbolic distribution variety;performance symbolic distribution;distribution phylogenetic;phylogenetic trees;phylogenetic trees demonstrate;problem symbolic distribution;trees problem symbolic;phylogenetic trees problem;symbolic mathematical;symbolic models;domain symbolic distribution;symbolic models variety;symbolic;symbolic mathematical integration;phylogenetic;symbolic problems;symbolic problems including;distribution variety symbolic;problem symbolic mathematical;variety symbolic models;variety symbolic problems;consider problem symbolic;including problem symbolic;analyzing performance symbolic;problem symbolic;trees demonstrate utility", "pdf_keywords": "systematic generalization symbolic;generalization symbolic;systematic generalization;generalization symbolic integration;evaluating generalization takes;neural sequence integrator;generalization carefully constructed;evaluating generalization;prediction neural sequence;generalizing systematically test;generalization;generalization takes;sequenceto sequence models;functions neural sequence;sequence models;outof distribution generalization;generalizing;generalizing systematically;distribution generalization;integrator neural sequence;methodology evaluating generalization;generalization required task;generates sequence iterations;generalize;requires generalizing systematically;sequence models domain;distribution generalization carefully;learning integrate;generalization takes advantage;integration requires generalizing"}, "9f7e317c6ef0bb15aacc9b19f0f0d00fee6c9a36": {"ta_keywords": "memorization based;memorization;new approach memorization;good results memorization;memorization based theory;approach memorization based;memorization problem;results memorization;approach memorization;theory memorization;results memorization problem;memorization problem subtle;memorization key;data large computational;dimensional data large;memorization key problem;learning algorithms;high dimensional data;based theory memorization;learning algorithms fit;data large;known learning algorithms;known learning;training example algorithm;large computational;algorithms fit data;dimensional data;learning;example algorithm easily;algorithms", "pdf_keywords": "example memorization known;memorization estimation;known memorization;empirical evidence memorization;new memorization estimator;memorization highly;memorization known;net memorization;memorization estimator;memorization highly predictive;example memorization;memorization known influence;examples memorization;memorization examples accuracy;memorization given examplememorization;best example memorization;memorization context deep;memorized examples high;tail theory memorization;estimator based memorization;result memorization highly;memorization estimator based;net memorization model;theory memorization;memorization data;memorized examples;optimal memorization;learning important memorization;memorization modern deep;approach memorization estimation"}, "a1a8eeb64c0846070b10531061c18fed6d566f8c": {"ta_keywords": "complexity manipulating vote;complexity tie breaking;complexity tie;consider complexity tie;tie breaking voting;game consider complexity;manipulating vote tie;hard computational complexity;computational complexity manipulating;vote tie breaking;complexity manipulating;computational complexity;tie breaking vote;complexity computing manipulation;breaking voting game;complexity;vote popular tie;consider complexity;breaking voting;cowinners computational complexity;vote tie;manipulating vote;winner cowinners computational;complexity computing;computational complexity computing;popular tie breaking;manipulate vote;breaking vote;manipulate vote popular;vote increase polynomial", "pdf_keywords": ""}, "c4919feb50c514e32eb0f4131399180c6f9a0d7d": {"ta_keywords": "procurement cost function;allocation multiple customers;polynomial procurement;online resource allocation;cost total allocation;surrogate function procurement;procurement cost;procurement cost total;function procurement cost;focuses polynomial procurement;facing procurement cost;allocation analyze competitive;procurement;resource allocation multiple;customer facing procurement;total allocation;allocation multiple;allocation;function procurement;sequentially seller;resource allocation;sequentially seller irrevocably;total allocation analyze;facing procurement;allocation analyze;competitive ratio primal;dual algorithm design;multiple customers;optimization;cost function order", "pdf_keywords": "dual algorithm maximizing;optimal primal dual;primal dual objective;dual algorithm optimal;greedy primal dual;optimal procurement cost;defining optimal procurement;dual algorithm optimizing;optimal procurement;procurement cost function;primal dual algorithms;maximizes function revenue;primal dual algorithm;consider optimal primal;market primal dual;procurement cost functions;optimization problems primal;cost function objective;greedy algorithm maximizes;optimal primal;optimal cost;cost function efficiently;ratio primal dual;primal dual analysis;greedy primal;maximizing revenue maximization;proposed cost function;increasing cost function;dual algorithms problem;ratio greedy primal"}, "df56ccda14b5bc255a07fc061c50839e75563c5a": {"ta_keywords": "parking related traffic;parking takes account;parking takes;traffic demonstrate parking;parking related;parking;street parking takes;demonstrate parking related;takes account parking;street parking;demonstrate parking;area demonstrate parking;routing game model;model street parking;new routing game;routing game;traffic;traffic traffic;related traffic traffic;related traffic;objective demonstrate parking;traffic traffic demonstrate;traffic adjusted satisfy;account parking;seattle downtown area;traffic adjusted;account parking related;seattle downtown;related traffic adjusted;subsections seattle downtown", "pdf_keywords": ""}, "4218563e1fe927440e00bf0abe5cb1e037deaf71": {"ta_keywords": "machine learning deployments;predicting accuracy real;confidence predicting accuracy;predicting accuracy;method learns threshold;learns threshold model;method predicting accuracy;learns threshold;accuracy real world;model confidence predicting;threshold model confidence;thresholded confidence;confidence predicting;predicting accuracy fraction;thresholded confidence atc;real world machine;machine learning;novel method predicting;accuracy real;average thresholded confidence;model confidence;confidence exceeds threshold;method learns;method predicting;world machine learning;accuracy;learning deployments method;examples model confidence;learning deployments;novel method learns", "pdf_keywords": "predicting accuracy distribution;method learns threshold;domain accurately predicted;predict accuracy predictions;predicting accuracy;predicting accuracy classifier;learns threshold score;accuracy predictions significantly;confidence predicting accuracy;methods predicting accuracy;predict accuracy classifier;method predicting accuracy;learns threshold;predicting accuracy novel;accurately predict accuracy;predict accuracy;predicts accuracy classifier;learns threshold model;predict accuracy wide;accurate prediction shift;predicts accuracy;accuracy predictions;predicting target domain;method predict accuracy;confidence threshold classifier;domain accuracy based;able predict accuracy;accuracy based unlabeled;demonstrate accuracy predictions;target domain accuracy"}, "2d6d26c118f43f3ab314d07f58c20df6e89a13af": {"ta_keywords": "identically prepared dna;prepared dna single;single dna based;dna based dna;prepared dna;based dna based;based dna;dna based;dna single dna;dna single;single dna;controlled identically prepared;dna;prepared identically controlled;identically prepared identically;prepared identically;identically prepared;number identically prepared;generate;identically controlled identically;generate large number;controlled identically;identically controlled;generate large;prepared;method generate;method generate large;large number identically;identically;number identically", "pdf_keywords": ""}, "92a8f7f09f3705cb5a6009a42220a6f01ea084e8": {"ta_keywords": "extract actionable knowledge;language extract actionable;actionable knowledge model;actionable knowledge high;actionable knowledge;large language models;language models;language models llms;extract actionable;natural language extract;llms extract actionable;knowledge high level;high level tasks;level tasks expressed;language extract;tasks expressed natural;level tasks;level tasks propose;tasks expressed;using large language;knowledge model;expressed natural language;natural language;large language;models llms extract;tasks;knowledge high;tasks propose method;knowledge model evaluate;knowledge", "pdf_keywords": "language models generate;large language models;learned language models;generation large language;knowledge large language;actions large language;demonstrations semanticallylarge language;generated language model;semanticallylarge language models;language models semantically;language models;large scale language;simple language models;language models mlps;induction large language;actionable knowledge large;trained language model;level language generate;language generation tools;language models llms;language generation;high level language;language generate;language models household;tasks predicting semantics;conventional language generation;language model;extract actionable knowledge;train large language;actions natural language"}, "59653e5cfa854a17c2ffcb86f2a454f27e12c716": {"ta_keywords": "bias translation gender;bias pronouns nmt;characterizing bias pronouns;bias pronouns;pronouns nmt distributions;pronouns nmt;bias translation;known bias translation;translation gender salient;translation gender;translations purpose characterizing;generated real translations;real translations differ;real translations purpose;real translations;pronouns;translations differ;translations;translations purpose;distributions real translations;translation;translations differ factor;bias;source known bias;characterizing bias;gender salient;gender salient source;purpose characterizing bias;known bias;suggesting known bias", "pdf_keywords": "machine translation powerful;machine translation;translationneural machine translation;search translation optimal;characterizing translation accuracy;good translation accuracy;search pronouns;translation accuracy neural;power machine translation;characterizing translation;translation accuracy;translation optimal bias;beam search pronouns;translation models;accuracy translation strategy;translation models investigate;human translation use;translation optimal;translation accuracy increases;search pronouns common;modelsneural machine translation;human translation;search translation;train translation models;method characterizing translation;translation strategy;accuracy translation;machine translation ntm;translation strategy good;search translation _we"}, "ac41e0ef30b6f9ee4930ac85dc46a9b50a1963d2": {"ta_keywords": "mechanism crowdsourced training;crowdsourced training data;novel mechanism crowdsourced;mechanism crowdsourced;crowdsourcing data;crowdsourced training;problem crowdsourcing data;crowdsourcing;crowdsourced;problem crowdsourcing;studies problem crowdsourcing;training data theoretically;suitable incentive mechanism;suitable incentive;incentive mechanism proposed;training data;incentive mechanism;data theoretically empirically;choice training data;incentive;training data set;ability workers vote;coupling suitable incentive;data theoretically;data;workers vote;empirically rigorous;workers vote right;preliminary empirical;provide simple axiomatic", "pdf_keywords": ""}, "76f02d20e02c6baf39fee8f115cd94e4ceacf32b": {"ta_keywords": "quality reviewers submission;reviewers submission;evaluate quality reviewers;quality reviewers;reviewers submission identify;reviewers;peer submission pipeline;reviewers master;peer submission;reviewers master junior;evaluation new submission;133 reviewers;133 reviewers master;submission pipeline;submission called alpha;algorithms evaluate quality;components peer submission;pipeline 133 reviewers;submission;novel algorithm evaluation;evaluation new;alpha stream supervised;evaluate quality;evaluation;bias recommendations design;stream algorithms evaluate;submission identify;peer;submission called;algorithms evaluate", "pdf_keywords": ""}, "bf0beed35ea09aab56027d64c744098cc963fbde": {"ta_keywords": "change detection transient;detection transient dynamics;detect change quickly;detection transient;quickest change detection;change detection;detect change;transient dynamics change;alarm durations transient;transient dynamics;instantaneously series transient;durations transient;durations transient phases;transient;objective detect change;considered dynamic cumulative;transient phases objective;dynamic cumulative;transient phases;series transient;dynamic cumulative cumulative;series transient phases;persistent distribution;dynamics change initial;transient phases completely;happen instantaneously;final persistent distribution;false alarm durations;happen instantaneously series;does happen instantaneously", "pdf_keywords": "detection abrupt changes;detect change reliably;change detection qcd;detect change quickly;detecting transient changes;detecting abrupt changes;generalization dynamic cusum;transient changes stochastic;detect change;detection qcd transient;detection change point;change detection;quickest change detection;algorithms detection transient;cusum test based;detection transient;detection transient events;dynamic cusum;dynamic cusum cusum;detecting transient;detection change;modification cusum algorithm;changes stochastic algorithm;cusum algorithm generalization;simultaneously detecting transient;detection abrupt;detecting abrupt;changes stochastic;cusum algorithm;based hypothesis transient"}, "f1005edfa1fbc4ea0d9a90345388bda8a01e69ed": {"ta_keywords": "tubular graphs;discrete tubular graphs;tubular graphs introduce;learning discrete tubular;curves forming vessel;vasculature using supervised;forming vessel trees;reconstructing accurate bifurcations;vessel trees;accurate bifurcations complex;discrete tubular;capillary vasculature using;accurate bifurcations;enforce discrete tubular;continuous oriented curves;capillary vasculature;complex near capillary;near capillary vasculature;graphs;vessel trees enforce;bifurcations complex;bifurcations complex near;oriented curves;curves;oriented curves forming;bifurcations;forming vessel;curves forming;graphs introduce;near capillary", "pdf_keywords": "trees vessel graphs;vessel graphs;vessel graphs volumetric;reconstruction vessel trees;vessel tree reconstruction;confluent vessel trees;vessel tree estimation;tubular graph reconstructions;vessel trees using;vessel trees significantly;unsupervised vessel tree;vessel trees;vessel trees efficient;methods vessel tree;vessel tree;trees vessel flow;trees vessel;confluent trees vessel;estimation vessel branches;vessel trees construction;vessel branches accurate;graphs volumetric data;tubular graphs minimizing;tubular graph method;graphs tubular flow;reconstructing confluent vessel;tubular graphs generalize;vessel trees paper;graphs volumetric;undirected tubular graphs"}, "794b0a1e9719d809ebdf2ef87ff84c2039bfdd52": {"ta_keywords": "wireless protocol stack;distributed protocol stack;protocol stack based;implemented wireless protocol;protocol stack;event driven architecture;objects distributed protocol;distributed protocol;stack based event;protocol stack active;propose wireless protocol;wireless protocol;data networks stack;implemented wireless;event driven;wireless data proposed;protocol;reuse wireless data;based event driven;machine implemented wireless;distributed data networks;stack implemented state;wireless data;proposed stack implemented;networks stack;networks stack composed;stack implemented;distributed data;data proposed stack;stack based", "pdf_keywords": ""}, "ae30f8fc5a969d2d14ae066db4cd07d86fadbf42": {"ta_keywords": "absorption proton plasma;steric absorption proton;absorption proton;proton plasma investigated;proton plasma results;proton plasma;anti steric absorption;section proton plasma;steric absorption;calculation absorption cross;cross section proton;absorption cross section;likelihood absorption cross;calculation absorption;applied calculation absorption;absorption cross;plasma investigated;likelihood absorption;plasma investigated using;maximum likelihood absorption;section proton;proton;absorption;plasma results used;plasma results;mechanism anti steric;plasma results compared;plasma;anti steric;mechanism anti", "pdf_keywords": ""}, "ffe1416bcfde82f567dd280975bebcfeb4892298": {"ta_keywords": "speech recognition asr;automatic speech recognition;speech recognition;connectionist temporal classification;automatic speech;end automatic speech;recognition asr;neural network architecture;recognition asr problem;architecture connectionist temporal;temporal classification;temporal classification approach;neural network;state art neural;connectionist temporal;art neural network;network architecture connectionist;architecture connectionist;advances language model;connectionist;language model;language model goal;neural;asr problem;asr problem using;end end automatic;asr;end automatic;recognition;accuracy training novel", "pdf_keywords": ""}, "c50f98961c951fe3fbdb6f375beb28e40a6b0581": {"ta_keywords": "peer review auction;quality peer review;peer review;participate peer review;peer review proposed;review auction;auction review slots;review auction review;auction review;high quality peer;review process;quality peer;high quality submissions;submission review process;paper submission review;review process simultaneously;authors raise bids;authors participate peer;review slots;submission review;submissions proposed mechanism;quality reviews high;quality submissions proposed;high quality reviews;quality submissions;auction;incentivizing high quality;review proposed mechanism;review slots second;reviews high", "pdf_keywords": "peer review mechanism;mechanism peer review;peer review systems;mechanisms peer review;peer review powerful;participate peer review;peer review;peer review induce;reviewers participate peer;scientific peer review;motivations peer review;community peer review;peer review process;review mechanism reviewers;peer review fundamental;process peer review;incentives reviewers participate;publication peer review;results peer review;peer review optimized;proposal peer review;mechanism incentivizes reviewers;peer review argue;content peer review;mechanism predicting review;peer review experiment;able incentivize reviewers;generation peer review;peer review content;incentivizes reviewers participate"}, "3febb2bed8865945e7fddc99efd791887bb7e14f": {"ta_keywords": "contextualized word representation;deep contextualized word;word representation models;bidirectional language model;linguistic contexts polysemy;word representation;text corpus model;deep contextualized;contexts polysemy use;corpus model easily;type deep contextualized;language model;contexts polysemy;bidirectional language;polysemy use bidirectional;use bidirectional language;contextualized word;word use syntax;language model bilm;corpus model;large text corpus;linguistic contexts;text corpus;corpus;word use;vary linguistic contexts;polysemy use;linguistic;contextualized;semantics ii uses", "pdf_keywords": "deep bidirectional language;deep contextualized word;short term memory;learning word representations;contextualized word representation;long term memory;word vectors learned;word representations nonlinear;term memory bilm;word representations;word representation models;natural language tasks;deep bidirectional;deep context dependent;term memory;deep contextualized;bidirectional attention;bidirectional attention layer;combines bidirectional attention;learning word;polysemy word vectors;deep context;type deep contextualized;context dependent representations;bilm large corpus;character convolutions bidirectional;bidirectional language model;word representation;nlp tasks;quality deep context"}, "aa2bd932a2ecb6e07c768bcf0dc119f0cd20f6e0": {"ta_keywords": "similarity measures database;identify similarity measures;identification similarity measures;similarity measures;identify similarity;used identify similarity;able identify similarity;identification similarity;based identification similarity;similarity;database able identify;method identification similarity;measures database difficult;measures database method;measures database;measures database able;database;database difficult;database method based;database method used;database method;database able;method based identification;database difficult access;measures;method used identify;based identification;used identify;method identification;identification", "pdf_keywords": ""}, "85a18aafcffdcc4eafcb9e5eda0abb8aa5cb8c3b": {"ta_keywords": "sdn big data;defined networking sdn;software defined networking;networking sdn flexibility;sdn flexibility big;future sdn big;sdn big;networking sdn;sdn flexibility;future sdn;sdn;big data sdss;defined networking;argue future sdn;big data platforms;network architecture;approach network architecture;network architecture combines;data sdss;optimize network;optimize network behavior;sdss context big;flexibility big data;big data used;networking;jointly optimize network;data platforms argue;data platforms;sdss;big data", "pdf_keywords": ""}, "d89f4534d1a87005cdf470ec5d8154998d5abdc7": {"ta_keywords": "prediction serving systems;slowdowns failures prediction;efficient resilience slowdowns;failures prediction serving;networks learn parity;prediction serving;resilience slowdowns failures;coding machine learning;learn parity models;ideas erasure coding;failures prediction;ideas machine learning;resource efficient resilience;resilience slowdowns;efficient resilience;systems build parm;parity models;learn parity;parity;erasure coding machine;neural networks learn;erasure coding;neural networks;encoders decoders reconstruct;use neural networks;parity models enable;encoders;ideas machine;encoders decoders;use neural", "pdf_keywords": "models parity computations;parity computations;coded computation prediction;predictions design parity;parm scalable computing;parity computations new;computation prediction serving;computation parities enabling;parity models 2019;coded computation models;computation parities;computation prediction;parity incremental processing;perform computation parities;parity models new;parity models parity;based parity models;systems parity models;parity models;parity models parm;predictions deployed parity;using parity models;using parity;parity model task;systems parity;parm scalable;parm new coding;parity model enables;parity models applicable;parities enabling decoder"}, "e95a96dec775cc792b763f4eec13343c22e850e1": {"ta_keywords": "information theory machine;advances information theory;machine learning held;information theory;theory machine learning;systems informatics;systems informatics 23;informatics;systems systems informatics;informatics 23;information systems;informatics 23 27;machine learning;advances information;information systems systems;theory machine;activity report workshop;conference information systems;activity report;report workshop advances;workshop advances information;information;conference information;advances;report workshop;2017 report covers;2017 report;international conference information;learning;june 2017 report", "pdf_keywords": ""}, "49984bc327ef6952118c4b871eeef2a907f7a4ed": {"ta_keywords": "regularized learning algorithms;regularized learning;learning algorithms games;classes regularized learning;class regularized learning;algorithms games;games form recency;algorithms games form;learning algorithms form;learning algorithms;equilibria multiplayer;regularized;algorithms form recency;bias achieve faster;correlated equilibria multiplayer;normal form games;recency bias achieve;equilibria multiplayer normal;classes regularized;natural classes regularized;class regularized;recency bias natural;bias natural classes;learning;recency bias;bias natural;introduce class regularized;faster convergence rates;games;bias achieve", "pdf_keywords": "games regret algorithms;regret algorithms converge;theorem regret games;regret algorithms;maximizing regret game;consider regret minimization;classes regret algorithms;algorithm tradeoff regret;regret algorithm tradeoff;vanishing regret algorithm;regret minimization;cost game convergence;regret games partial;bounded different regret;guarantee convergence games;convergence smooth games;regret algorithm;regret player bounded;smooth games regret;convergence games;maximizing regret;regret minimization problem;regret algorithms lead;regret strategy game;game convergence;convergence games theorems;regret games;game good approximation;regret bound game;convergence players class"}, "c460fd4a0dc86bc518f9a8e982bc48faf1efb942": {"ta_keywords": "time broadcasting content;broadcasting content social;time broadcasting;optimal time broadcasting;social network;content social network;social network quantify;real data twitter;twitter;timeline information exchange;data twitter;twitter discover counterintuitive;social platforms users;social platforms;scheduling;content social;twitter discover;data twitter discover;broadcasting content;optimal time;overload bursty circadian;information overload bursty;parametrise social platforms;determining optimal time;bursty circadian rhythms;scheduling strategy;discover counterintuitive scheduling;bursty circadian;counterintuitive scheduling;timeline information", "pdf_keywords": "scheduling content social;scheduling tweets;scheduling tweets simple;broadcast scheduling tweets;content social timelines;aversion timelines subscribers;monotony twitter;social media broadcast;follower consumption timeline;behaviour microblogs weibo;timelines subscribers;microblogs weibo platform;optimum social media;microblogs weibo;microblogs exhibit bursty;timelines subscribers affect;followers social media;volume social media;social network monotony;like twitter instagram;broadcast tweets timescale;twitter study;behaviour microblogs;time consecutive tweets;performance social media;online communities temporal;platforms like twitter;social media content;monotony twitter friends;tweets timescale"}, "1a9c89cb2e57e06dadd4c2fab5fae1bfdbb3b6d5": {"ta_keywords": "aggregation preference networks;preference networks nets;preference networks;nets based aggregation;approach aggregation preference;aggregation preference;collective reasoning tasks;perform collective reasoning;collective reasoning;preferences elementary nets;based aggregation;new approach aggregation;aggregation;approach aggregation;agents expresses preferences;based aggregation collection;expresses preferences elementary;elementary nets;collection elementary nets;aggregation collection elementary;nets elementary nets;aggregation collection;nets based;nets elementary;expresses preferences;networks nets;determining preferred;networks nets based;elementary nets elementary;reasoning tasks", "pdf_keywords": ""}, "f5a0c6593ba95d23c025608ce9280848da8b929f": {"ta_keywords": "gene mention recognition;mention recognition task;mention recognition;gene mention;results gene mention;results gene;analysis results gene;gene;recognition task;recognition;statistical;present statistical analysis;statistical analysis results;present statistical;results;statistical analysis;task;analysis results;mention;analysis;present", "pdf_keywords": ""}, "92891a984b45df5fc764d81bf9bcd42e7e7ed1c7": {"ta_keywords": "differential game nonlinear;game nonlinear dynamics;equilibrium multi network;inducing global equilibrium;network model epidemic;game nonlinear;desired local equilibrium;equilibrium open loop;model epidemic;model epidemic spread;local equilibrium;global equilibrium;loop differential game;global equilibrium multi;loop differential games;local equilibrium open;equilibrium open;epidemic spread proposed;equilibrium;differential games proposed;differential game;epidemic spread;equilibrium multi;differential games;epidemic;nonlinear dynamics;multi network model;based quadratic pricing;network model;quadratic pricing", "pdf_keywords": ""}, "3cc790174d138d7904189df997d5763f1793dedf": {"ta_keywords": "annotator agreement normalization;normalized text agreement;agreement normalization text;accuracy normalized text;agreement normalization;normalized text;normalized text accuracy;normalization text method;agreement given normalization;normalization text;checking normalized text;inter annotator agreement;annotator agreement;normalized;agreement mapping;text agreement;text agreement given;normalization task;normalization task proposed;inter annotator;agreement mapping certain;analyzing accuracy normalized;normalization;accuracy normalized;text accuracy proposed;given normalization task;evaluating accuracy normalized;checking normalized;text accuracy;annotator", "pdf_keywords": ""}, "1d255aeabcb87929742280251007fd8c01bbe914": {"ta_keywords": "oxidized tin synthesized;oxidized tin based;oxidized oxidized tin;oxidized tin characterized;oxidized tin oxidized;type oxidized tin;demonstrated oxidized tin;form oxidized tin;tin oxidized oxidized;oxidized tin;oxidized tin demonstrated;tin demonstrated oxidized;tin oxidized;tin synthesized;tin synthesized synthesis;tin based materials;class oxidized tin;tin based;based polyoxometalate po11fe;iron based polyoxometalate;tin;tin characterized;synthesized iron based;polyoxometalate po11fe;tin demonstrated;synthesized iron;polyoxometalate po11fe subsequently;synthesis synthesized iron;functionalized form oxidized;new type oxidized", "pdf_keywords": ""}, "737f9a32d7f4007aa9526556c256ed4a182aec69": {"ta_keywords": "price quadratic game;optimal price quadratic;optimal price convex;players optimal price;quadratic game;quadratic game players;socially optimal price;optimal price designed;design optimal price;optimal price;price convex;price quadratic;price convex function;players optimal;number players optimal;optimal;price designed robust;socially optimal;perturbations game parameters;design optimal;small perturbations game;notion socially optimal;fixed cost design;game parameters design;perturbations game;propose design optimal;game parameters;cost design;function number players;convex", "pdf_keywords": ""}, "4aa72e4232ae809ea1a9fe142275da25ba930655": {"ta_keywords": "algorithms quadratic games;linear quadratic games;convergence guarantees policy;quadratic games satisfy;guarantees policy gradient;underlying quadratic game;quadratic games provide;quadratic games;policy gradient algorithms;convergence guarantees;quadratic game;quadratic game generate;games satisfy conditions;equilibria underlying quadratic;policy gradient avoid;gradient algorithms quadratic;policy gradient;conditions policy gradient;existence convergence guarantees;gradient avoid equilibria;guarantees policy;algorithms quadratic;gradient algorithms;games satisfy;counterexamples existence convergence;games provide sufficient;underlying quadratic;existence convergence;guarantees;avoid equilibria underlying", "pdf_keywords": "quadratic games policy;policy gradient game;players linear policies;converges nash equilibrium;linear quadratic games;guarantees convergence nash;policy gradient players;lq games theorems;convergence nash equilibria;stable equilibria nash;quadratic games satisfy;convergence nash equilibrium;games policy gradient;gradient dynamics games;equilibrium games;theorems state game;player quantum games;players guarantees convergence;game lq theorem;equilibrium games local;gradient players guarantees;generalized game theory;linear feedback games;games local quantum;quantum games;nash equilibria linear;quadratic games;generalized game;existence nash equilibria;nash equilibria continuous"}, "b62ce3135ed6065863c0dec26037fd07c081abba": {"ta_keywords": "counterfactually augmented data;counterfactually augmented;language causality;language causality constructed;representation language causality;difference counterfactually augmented;natural language inference;language inference;language inference tasks;counterfactually;causality constructed;causality;difference counterfactually;coherence underlying data;causality constructed intrinsic;makes difference counterfactually;natural language;coherence underlying;augmented data;inference;documents classification relative;inference tasks;applying natural language;inference tasks classification;coherence;augmented data method;augmented;intrinsic coherence underlying;intrinsic coherence;documents classification", "pdf_keywords": "counterfactually annotated datasets;counterfactually annotated;classifiers trained counterfactually;trained counterfactually manipulated;annotated datasets sentiment;trained counterfactually;manipulating automated sentiment;automated sentiment;learning sentiment;counterfactually manipulating automated;backgrounding counterfactually annotated;targeted counterfactually;machine learning sentiment;targeted counterfactually class;make targeted counterfactually;counterfactually manipulated data;datasets sentiment;learning sentiment analysis;natural language models;counterfactually revise documents;learningwe present counterfactually;natural language crucial;sentiment analysis natural;automated sentiment analysis;counterfactually revised data;processing humans counterfactually;counterfactually manipulated;counterfactually class;natural language critical;counterfactually class applicable"}, "ca86a63362e51eea2e213ae2d3faed668ec1ad74": {"ta_keywords": "commonsense concept extraction;texts commonsense concepts;commonsense concepts;concept extraction semantic;extraction semantic similarity;commonsense concepts proposed;commonsense concept;similarity detection natural;approach commonsense concept;texts commonsense;concept extraction detection;language texts commonsense;approach commonsense;based approach commonsense;detection natural language;semantic similarity detection;novel approach commonsense;concept extraction;commonsense;semantic similarity;extraction semantic;similarity detection;natural language texts;deconstructs natural language;natural language;natural language approach;extraction detection deconstructs;detection deconstructs natural;graph based;similarity", "pdf_keywords": ""}, "a1da1d600acd506b80c8870d293a756c70791683": {"ta_keywords": "bilingual lexicon induction;bilingual lexicons distribution;aligned bilingual lexicons;bilingual lexicons larger;bilingual lexicons;bilingual lexicon;work bilingual lexicon;depended aligned bilingual;lexicons distribution matching;limited aligned bilingual;aligned bilingual;lexicon induction bli;lexicon induction;unaligned word embeddings;lexicons distribution;word embeddings;recent work bilingual;bilingual;embedding spaces empirically;work bilingual;estimate isometry embedding;isometry embedding;isometry embedding spaces;lexicons larger;propose semi supervised;spaces empirically;semi supervised;embeddings;assumption weakens languages;isometric assumption leveraging", "pdf_keywords": ""}, "f184908270fc934ab74438a0aaac7a43a5eae6d2": {"ta_keywords": "single document summarization;document summarization incorporates;summarization incorporates latent;document summarization;summarization incorporates;document summarization process;sentence dependencies single;summarization;explicit sentence dependencies;summarization process model;summarization process;sentence dependencies;generation sentence structure;sentence structure analysis;model single document;document model;sentence structure;dependencies single document;analysis document model;latent explicit sentence;incorporates latent explicit;incorporates latent;single document;spontaneous generation sentence;latent explicit;structure aware encoders;document model outperforms;encoders integrates spontaneous;generation sentence;dependencies single", "pdf_keywords": "neural abstractive summarization;neural networks summarization;abstractive summarization models;new summarization model;summarization models suffer;attention sentences encoder;summarization model;abstractive text summarization;summarization models;summarization model news;simple abstractive summarization;networks summarization efficient;abstractive summarization;abstractive summarization introduce;summarization efficient;summarization models leverage;abstractive document summarization;summarization aims compressing;generated summaries helps;abstractive summarization approach;generated summaries;summarization models interpretable;abstractive summaries source;learned sentence structure;summarization introduce novel;evaluating summarization models;sentence structure generate;abstractive summaries;summarization related intrinsic;structured attention sentences"}, "99ac83b990af1fc591db5b676300a7c002905dae": {"ta_keywords": "semi supervised learning;semi supervised;tackle semi supervised;multi view datasets;view datasets proposed;multiple views techniques;multiple views;multi view;views;supervised learning;views techniques;view datasets;data view;number multi view;supervised;views techniques make;view;scores data view;use linear programming;linear programming em;linear programming mixed;linear programming;presence multiple views;datasets proposed;supervised learning presence;view compare proposed;data view compare;integer linear programming;view compare;semi", "pdf_keywords": ""}, "3f90668994d6e5949a530dfc84a10b492ff35cfa": {"ta_keywords": "shallow semantic parsing;task shallow semantic;shallow semantic;parsing semantic;semantic parsing;semantic parsing semantic;parsing semantic labels;relation sub tasks;predicting relation sub;relation sub task;sub task shallow;parsing;task shallow;predicted relation sub;semantic labels;distinct semantic labels;semantic labels structurally;structurally similar sentences;semantic;method predicting relation;sentences copied test;relation sub;semantic labels corpus;predicting relation;test sentences;distinct semantic;similar sentences copied;sentences proposed method;sentences copied;test sentences proposed", "pdf_keywords": ""}, "fce19dd512a82693ab9070049ed426179eca8856": {"ta_keywords": "nlp limited argumentative;web content argumentative;argumentative structure content;content argumentative;analyze argumentative structure;content argumentative spectrum;argumentative structure;argumentative spectrum web;argumentative argumentative spectrum;analyze argumentative;processing nlp;natural language processing;technique applied argumentative;argumentative argumentative;processing nlp nlp;nlp technique;applied argumentative argumentative;argumentative;nlp technique applied;applied argumentative;nlp nlp successful;nlp successful nlp;language processing nlp;discover analyze argumentative;successful nlp technique;limited argumentative;nlp nlp;nlp successful;nlp;argumentative spectrum", "pdf_keywords": ""}, "0d22ce72a62419086fd4860a4671991846cd492b": {"ta_keywords": "lightweight block ciphers;block ciphers;block ciphers called;lightweight block;ciphers;ciphers called p__;generation lightweight block;ciphers called;attacks data p__;random attacks data;global random attacks;random attacks;new generation lightweight;attacks data;generation lightweight;attacks;high throughput random;block;lightweight;design implementation new;throughput random;implementation new;throughput random random;implementation new generation;data p__ ps;speed high throughput;based p__ ps;p__ ps family;high throughput;design implementation", "pdf_keywords": ""}, "fb6ef2d6fbd1ea4905070077ab6c5b0108f2c38a": {"ta_keywords": "detect sarcasm tweets;detection method sarcasm;sarcasm detection method;method detect sarcasm;detection sarcasm;method sarcasm detection;detection sarcasm sarcasm;sarcasm tweets approach;strategies detection sarcasm;sarcasm detection;detect sarcasm;sarcasm tweets;sarcasm sarcasm detection;method sarcasm;measure method sarcasm;manuallylabeled tweets;000 manuallylabeled tweets;manuallylabeled tweets evaluate;tweets approach;tweets approach based;tweets evaluate;sarcasm sarcasm;sarcasm;tweets evaluate different;tweets;large social network;social network;social network consisting;analysis large social;social", "pdf_keywords": ""}, "b8e49216e5b4a017342b0be5f6fbbd79e690a1c7": {"ta_keywords": "auction combination neural;auctions complex;auction combination;auctions near optimal;auction;design auctions complex;mechanism given auction;auctions complex single;design auctions based;design auctions;given auction combination;auctions based;approach design auctions;auctions based use;auctions;given auction;design auctions near;auctions near;used design auctions;allows design auctions;constrained learning;constrained learning approach;deep learning approach;deep learning;based constrained learning;use deep learning;neural network;neural network approach;neural;combination neural", "pdf_keywords": "auction maximizes learning;auction resulting neural;discover optimal auctions;designing optimal auctions;optimal auctions essentially;trained suitable auction;design optimal auction;designing optimal auction;auction network algorithms;optimal auctions mechanisms;designing auctions maximizes;items optimal auctions;bidder learning;optimal auctions;auctions algorithms;given bidder learning;near optimal auctions;optimal auction;designing auction maximizes;reproduces optimal auctions;method constructing auction;control auctions algorithms;designing auctions;auctions allows optimal;designing auction;auctions mechanisms;optimal auctions multiple;optimal auctions thesoupnet;optimal design auctions;constructing auction"}, "f297e939212780637705eba8798c9a386befd771": {"ta_keywords": "pivot phrases data;extract pivot phrases;pivot phrases;pivot language;uses pivot language;pivot language models;extract pivot;used extract pivot;method extract pivot;applying extract pivot;phrases data;uses pivot;phrases data method;phrases data used;pivot;method uses pivot;phrases data demonstrate;data used extract;models input translation;input translation;language models input;language models;translation process;phrases;input translation process;translation process method;novel method extract;method used extract;extract;method applying extract", "pdf_keywords": ""}, "8a902a848c3710290f04f2d59030f5670d3433f8": {"ta_keywords": "morphological complexity language;increases morphological complexity;morphological complexity;languages importance morphology;analysis use morphological;use morphological features;morphology increases morphological;use morphological;morphological;morphological features;morphology;prediction certain languages;morphological features improves;importance morphology;increases morphological;morphology increases;error prediction;importance morphology increases;accuracy error prediction;error prediction certain;error analysis;models error analysis;certain languages;complexity language;certain languages importance;languages;languages importance;error analysis use;language;classical models error", "pdf_keywords": ""}, "ab48fb72541653f40523caa9fcaac9cb84bf3373": {"ta_keywords": "multi channel speech;jointly neural source;neural source model;source separation;recognition multi channel;channel speech;joint source separation;channel speech proposed;neural source;source separation dereverberation;jointly neural;uses neural source;frontend recognition multi;source model conditional;optimized jointly neural;novel frontend recognition;multi channel;conditional vector analysis;frontend recognition;recognition multi;model conditional vector;speech;source model;achieve joint source;joint source;conditional vector;vector analysis;noisy data;proposed noisy data;separation dereverberation parameters", "pdf_keywords": "beamforming recognition speech;separation speech networks;recognition speech sources;source separation speech;speech sources multiple;beamforming recognition sources;neural beamforming recognition;using neural beamforming;power neural beamforming;approach neural beamforming;neural beamforming;beamforming recognition;speaker automatic speech;speech networks;speech recognition;speech networks based;speech sources;separation multiple sources;separation neural networks;channel multi speaker;multi speaker;automatic speech recognition;multi speaker automatic;neural beamforming frontends;dereverberation beamforming techniques;recognition speech;source separation;joint source separation;dereverberation beamforming;automatic speech"}, "3c5d3bbb73aa0e3e969a25487a81b5b1f0c14044": {"ta_keywords": "knowledge graphs large;facts large datasets;graphs large datasets;infer facts large;knowledge graph;knowledge graphs;large datasets;large scale knowledge;scale knowledge graphs;large datasets method;large datasets apply;knowledge graph proposed;large scale dataset;dataset 80k facts;infer facts;structure knowledge graph;inference facts noisy;graphs large;facts large;large number facts;inference facts;joint inference facts;infer structure knowledge;infer large scale;facts noisy incomplete;ability infer facts;datasets;infer large number;80k facts;scale knowledge", "pdf_keywords": ""}, "5ede529879d162d2779d410a5775d3f6cd6be3f4": {"ta_keywords": "distributionally robust optimization;novel distributionally robust;distributionally robust;robust optimization dro;generative models;large scale generative;robust optimization;generative models develop;learning models able;model selection heuristics;gradient based optimization;propose novel distributionally;novel distributionally;learning models;optimization large scale;phenomenological inner maximization;scale generative models;generative;machine learning models;training machine learning;based optimization large;distributionally;inner maximization objective;data distributions;optimization large;optimization dro;model selection;optimization dro framework;data distributions proposed;machine learning", "pdf_keywords": "learning rate adversary;empirical risk training;adversarial;adversarial network model;model adversary propose;model adversary;adversary model;solution generative adversarial;update model adversary;adversary model used;np robust learning;adversarial network;adversary performance parametrized;model generative;adversary simultaneous gradient;generative adversarial;adversary performance;adversary approach;risk training model;learning based empirical;models trained;learning models;adversary approach based;machine learning models;generative model;predicting distribution;robust learning;robust heuristic predicting;trained model;adversary strongly dependent"}, "c6b462aaca52d0325db3118d2779865915b266c3": {"ta_keywords": "rule induction large;complexity rule induction;induction large training;rule induction methods;runtime rule induction;rule induction;large training sets;induction methods loss;new pruning techniques;pruning techniques;pruning techniques dramatically;induction methods;improve runtime rule;complexity rule;induction large;asymptotic complexity rule;training sets;pruning;new pruning;study asymptotic complexity;complexity;induction;large training;asymptotic complexity;form new pruning;methods scale noisy;noisy data;methods loss accuracy;training sets presence;noisy data present", "pdf_keywords": ""}, "dfb35ebe4fd754f59053d27c78f555bb5e7ccbff": {"ta_keywords": "minimizing curvature;minimizing curvature center;prior minimizing curvature;block coordinate descent;lines surfaces estimate;surfaces estimate location;curvature center lines;surfaces estimate;curvature center;coordinate descent;absolute curvature;estimation structures;lower bound optimization;curvature;quadratic absolute curvature;center lines surfaces;bound optimization algorithm;descent prior minimizing;optimization;optimization algorithm;approach estimation structures;coordinate descent prior;estimation structures combine;orientation structures;absolute curvature illustrate;bound optimization;curvature illustrate;prior minimizing;curvature illustrate advantage;estimate location orientation", "pdf_keywords": "edge vessel detection;minimizing curvature;based curvature regularization;prior minimizing curvature;absolute curvature regularization;optimization absolute curvature;curvature regularization;edge detection;minimizing curvature centerlines;curve edge detection;structures based curvature;regularize curvature;curvature regularization present;curvature penalization approach;vessel detection 2d;variational inference curved;curvature penalization;vision require estimation;line regularize curvature;detection 2d 3d;curvature approximations;edge detection computational;curvature regularization unlike;valued edge localization;edge detection real;absolute curvature approximations;curvature approximations propose;regularize curvature center;structures sub pixel;vessel detection"}, "cd5a9a0061de6a6841c63e60281133207b2d6763": {"ta_keywords": "description phrase dataset;description decoders;phrase natural language;phrase dataset;context encoders description;describing phrase;method describing phrase;encoders description decoders;phrase dataset created;description decoders demonstrate;natural language;describing phrase natural;wikipedia term dataset;comparing description phrase;description phrase;context encoders;natural language based;global contexts model;contexts model;encoders description;contexts;global contexts;describing;dataset created unstructured;wikipedia term;model comparing description;consists context encoders;contexts model consists;unstructured data;dataset created wikipedia", "pdf_keywords": "describing unknown phrase;phrases emerging entities;unfamiliar words phrases;global contexts decoder;contexts decoder;words phrases model;generating natural language;description unfamiliar words;unknown words phrases;description decoder;generate description unfamiliar;polysemous words phrases;given unfamiliar phrase;natural language description;phrase context task;natural language model;phrase global context;model describing words;describing words;description polysemous words;phrases model;unfamiliar phrase text;context generation description;text emerging entities;phrase global contextthis;entities text unfamiliar;phrase embedding;context generation;phrase local context;able generate description"}, "89b8153a86708b411bd21357c5b6006142104fc9": {"ta_keywords": "corpus spoken quotes;ted talk corpora;corpora public speaking;talk corpora;talk corpora construct;analyze corpus spoken;sample annotated corpora;public speaking analyze;annotated corpora;corpus spoken;annotated corpora public;spoken quotes ted;quotes ted talk;corpus;cons annotated corpora;analyze corpus;spoken quotes;corpora;corpora public;public speaking;speaking analyze;quotes ted;ted talk;corpora construct;corpora construct sample;sample annotated;quotes;construct sample annotated;ted;annotated", "pdf_keywords": ""}, "91ef95907dc637ad3c29ac3cc0e682b9c1985a37": {"ta_keywords": "simultaneous speech translation;speech translation methods;speech translation;optimal segmentation;optimal segmentation strategy;segmentation strategy simultaneous;learning optimal segmentation;segmentation strategy;simultaneous speech;strategy simultaneous speech;segmentation;translation methods;translation methods based;speech;learning optimal;algorithms;dynamic programming;search dynamic programming;performance algorithms;greedy search;dynamic programming respectively;optimal;based greedy search;greedy search dynamic;methods learning optimal;algorithms experimental;performance algorithms experimental;algorithms experimental evaluation;translation;learning", "pdf_keywords": ""}, "1f3c381eedfe8914b81e93070bfdb00cf86ac943": {"ta_keywords": "learning structure graphs;structure graphs representations;structural views graphs;graphs representations;graphs representations contrasting;structure graphs;views graphs;learning structure;self supervised learning;graphs;self supervised;views graphs increasing;neighbors graph diffusion;neighbors graph;results self supervised;graph diffusion;graph;supervised learning benchmarks;supervised learning;order neighbors graph;learning benchmarks linear;supervised;graph diffusion achieve;representations contrasting structural;structural views;approach learning structure;graphs increasing;views;learning;learning benchmarks", "pdf_keywords": "learns graph representations;graph representation learning;learning graph representations;representation learning graph;graph learned representations;views learning graph;learn graph representations;representations graph learned;graph representations views;graph representations empirically;node graph representations;learning graph encodings;graph level representations;graph trained;underlying graph representation;graph learned;learns graph;graph representations;graph learning;view representation learning;graph representations contrasting;describing graph representations;graph representations describing;representations underlying graph;graph encodings contrast;representation learning node;approaches graph learning;learning graph;graph learning demonstrate;graph representations able"}, "4abdea830316d80ab0b29fb94ee0786216f6a1cd": {"ta_keywords": "phrase alignment extraction;joint phrase alignment;phrase alignment;itgs phrase alignment;machine translation;memorize phrase pairs;machine translation tasks;alignment extraction;alignment extraction using;alignment extraction achieved;inversion transduction grammars;based machine translation;model joint phrase;phrase based machine;selecting phrase pairs;phrase pairs;transduction grammars itgs;pairs phrase dependent;memorize phrase;able memorize phrase;phrase pairs phrase;selecting phrases phrase;transduction grammars;dependent selecting phrases;translation tasks;selecting phrases;phrase dependent selecting;phrase dependent;methods inversion transduction;phrases phrase independent", "pdf_keywords": ""}, "d6b3effdeb3d38ac9ee43c3b8292b0937a295c30": {"ta_keywords": "multitask learning;hierarchical multitask learning;connectionist temporal classification;multitask learning improves;multitask learning context;standard multitask training;multitask training;temporal classification ctc;observe multitask learning;performance speech recognition;ctc based speech;hierarchical multitask;multitask;context connectionist temporal;speech recognition;standard multitask;training observe multitask;multitask training observe;connectionist temporal;learning context connectionist;effect hierarchical multitask;speech recognition observe;temporal classification;improves performance speech;observe hierarchical multitask;speech recognition factor;classification ctc;observe multitask;classification ctc based;based speech recognition", "pdf_keywords": "recognition multitask learning;multitask learning neural;recognition multitask;recognition model multitask;free recognition multitask;multitask learning;cnn uses multitask;combines multitask learning;model multitask learning;multitask learning approach;uses multitask learning;hierarchical multitask learning;multitask learning standard;pretraining multitask learning;multitask learning achieve;performance multitask learning;multitask learning common;multitask learning context;neural speech recognition;improves multitask learning;multitask learning baseline;task phonetic classification;method multitask learning;training phonetic recognition;multitask learning consistently;ctc based speech;end neural speech;connectionist temporal classification;multitask model auxiliary;combine pretraining multitask"}, "3c78c6df5eb1695b6a399e346dde880af27d1016": {"ta_keywords": "level sentence models;generating sentence level;sentence models document;sentence models;sentence models outperforming;sentences generated using;sentences generated;sentence level;models document reading;generating sentence;document reading approach;method generating sentence;neural process sentences;document reading;sentence level sentence;document reading method;form sentence level;process sentences generated;sentences;problem generating sentence;reading approach;reading approach based;normalization training;approach based neural;level sentence;questions form sentence;process sentences;reading method;shared normalization training;normalization training objective", "pdf_keywords": "question answering models;adapting neural paragraph;paragraph level neural;question answering;reading comprehension model;neural paragraph level;level question answering;answering models;question answering systems;neural reading comprehension;paragraph models;paragraph reading comprehension;reading comprehension systems;softmax operator paragraph;paragraph predictions;quality paragraph predictions;neural paragraph;comprehension model;answer question paragraph;train paragraph level;open question answering;reading comprehension squad;paragraph models method;sentences apply softmax;including comprehension questions;question answering investigate;tasks including comprehension;model paragraph sentence;paragraph predictions recently;comprehension squad model"}, "7c3a2e953d2c07ff4f150865112e4ceec14090ea": {"ta_keywords": "eel speech enhancement;speech enhancement;speech enhancement proposed;excitation prediction electrolaryngeal;electrolaryngeal eel speech;statistical excitation prediction;noise reduction method;noise reduction;improving statistical excitation;based noise reduction;excitation prediction;prediction hybrid enhancement;method predicting excitation;voice conversion method;parameters voice conversion;voice conversion;prediction electrolaryngeal eel;removing micro prosody;predicting excitation parameters;prediction electrolaryngeal;predicting excitation;spectral parameters voice;method based noise;micro prosody low;eel speech;statistical excitation;method enhancing spectral;parameters voice;micro prosody;enhancement proposed hybrid", "pdf_keywords": ""}, "73fe797b4f4f2d18784246bb74626426a8fe108e": {"ta_keywords": "pointer graph neural;pointer graph;graph neural networks;graph neural;pgns trained pointer;trained pointer based;neural networks pgns;graph connectivity;graph connectivity tasks;dynamic graph connectivity;trained pointer;augment sets graphs;networks pgns novel;deep sets;pointer based data;sets graphs;networks pgns;data structures disjoint;sets graphs additional;sets dynamic graph;networks;data structures;structures introduce pgns;deep sets dynamic;expressivity pgns trained;pointer based;graphs;data structures introduce;structures disjoint set;pgns augment sets", "pdf_keywords": "pointer graph networks;execute graph learning;learning networks pgns;graph networks pgns;learning execute graph;graph connectivity querying;latent pointerbased graph;graph neural;graph inference;graph networks;latent graph inference;represented graph neural;graph learning;graph connectivity tasks;efficient latent graph;graph neural networks;polynomial learning networks;graph network;queries connectivity;graph connectivity query;network graph neural;learning latent pointerbased;graph network capable;graph connectivity;graph neural network;dynamic graph connectivity;pointerbased graph;networks pgns gradient;gnns query data;pointer graph"}, "8b652c4d7a8d5836925ce0fe28a91dc661778524": {"ta_keywords": "language models berts;neural language models;comparative questions text;large neural language;answer comparative questions;berts answer comparative;semantically comparable;models berts answer;semantically comparable similar;entities semantically comparable;completing comparative questions;language models;neural language;models berts;comparative questions different;questions text oriented;comparative questions;large neural;berts answer;questions text;comparative;answer comparative;models achieve nearly;ability large neural;text oriented;completing comparative;comparable similar;neural;questions different;performance completing comparative", "pdf_keywords": "place ai conference;ai conference;ai conference vancouver;place natural language;neural language models;language models entities;language models learn;knowledge neural language;place nference conference;neural language;nference conference;humans use sentences;using neural language;best place ai;natural language;language models;natural language world;predicting semantics;language models text;language model;language model finetuning;large language models;large language model;intelligent use sentences;predict semantics;predicting semantics biological;place ai;human annotated questionswe;pretrained language model;predict semantics biological"}, "f53aa1d2676689c94429944f6a69431f96e05ae1": {"ta_keywords": "learning mixed membership;mixed membership models;supervised learning mixed;membership models;entity clustering;topic modification gibbs;mixed membership model;entity clustering task;supervised;supervised learning;supervision models;membership model experimental;introduce supervised;supervision models present;methods introduce supervised;membership models use;membership model;introduce supervised learning;results entity clustering;forms supervision models;underlying topic modification;gibbs sampler;models use biasing;modification gibbs sampler;gibbs sampler used;clustering;learning mixed;entity;range supervised learning;indicators underlying topic", "pdf_keywords": ""}, "57676e07d66b102f3335a5c538735ebff9076623": {"ta_keywords": "zeeman field xmath0;effect zeeman field;xmath2 xmath3 xmath4;effect zeeman;xmath1 xmath2 xmath3;zeeman field;xmath15 xmath16 xmath17;xmath9 xmath10 xmath11;xmath17;xmath16 xmath17 xmath18;xmath17 xmath18 xmath19;xmath16 xmath17;xmath13 xmath14 xmath15;xmath2 xmath3;xmath17 xmath18;xmath14 xmath15 xmath16;xmath1 xmath2;xmath12 xmath13 xmath14;xmath10 xmath11 xmath12;xmath3 xmath4;xmath0 xmath1 xmath2;xmath11 xmath12 xmath13;xmath7 xmath8 xmath9;xmath10 xmath11;xmath14 xmath15;xmath8 xmath9 xmath10;xmath13 xmath14;xmath3 xmath4 xmath5;xmath2;xmath4 xmath5 xmath6", "pdf_keywords": ""}, "ba1823889a80c231966a0f24e57c6cf4a569ff8c": {"ta_keywords": "multimodal fake news;diverse multimodal fake;multimodal entity;multimodal entity inconsistency;entity enhanced multimodal;model multimodal entity;detect diverse multimodal;multimodal fake;diverse multimodal;multimodal fusion framework;visual entities celebrities;multimodal fusion;enhanced multimodal fusion;modal correlations entity;enhanced multimodal;multimodal;images model multimodal;model multimodal;cross modal correlations;modal correlations;entities celebrities;semantics images model;entities celebrities landmarks;semantics images;extract visual entities;visual entities;mutual enhancement text;fake news model;entity inconsistency mutual;detect diverse", "pdf_keywords": "multimodal fake news;news exploiting multimodal;detect multimodal entities;fusing multimodal information;news characterized multimodal;multimodal correlations text;multimodal feature fusion;characterized multimodal entity;detection named multimodal;based multimodal entities;multimodal feature extraction;multimodal entities extract;multimodal news;correlations multimodal fake;multimodal news important;exploiting multimodal correlations;multimodal entities;multimodal entity;textual visual multimodal;distinguish multimodal fake;multimodal feature;inin multimodal news;multimodal entities attention;multimodal information;multimodal entity inconsistency;visual multimodal feature;obtain multimodal entity;fusing multimodal;multimodal correlations;multimodal clues simultaneously"}, "499ada382b7ce8f1cbd890e8c21500d95e20f2fe": {"ta_keywords": "audio representation provides;audio representation;ear challenge evaluate;ear challenge;purpose audio representation;audio;hear 2021 challenge;human ear challenge;purpose audio;holistically human ear;general purpose audio;hear 2021;human ear;ear;challenge evaluate models;2021 challenge develop;models;2021 challenge;models able;models able perform;able perform holistically;challenge develop;challenge evaluate;hear;evaluate models able;perform holistically human;representation provides;learning;domains challenge evaluate;evaluate models", "pdf_keywords": "evaluation audio representations;np representation audio;evaluate audio representations;audio evaluation challenge;audio representations competition;audio representations provide;audio representations;challenge evaluate audio;audio representations variety;audio classification task;novel audio representations;flexible audio representations;audio representation;new audio representation;representation audio;holistic evaluation audio;audio representations sound;audio representation class;challenges audio;2021 challenges audio;challenges audio community;audio classification;hear challenge discovery;generic audio evaluation;audio evaluation;language representation np;audio domains challenges;evaluation audio;tasks natural language;discovery new audio"}, "aa2428e1c4ea6d6bb347cfa59beead8736e19c46": {"ta_keywords": "hyperfine states magnetic;hyperfine state magnetic;xmath5 hyperfine states;xmath6 hyperfine state;xmath1 xmath2 states;states xmath3 xmath4;xmath2 states;ground state xmath0;xmath2 states xmath3;perpendicular xmath6 hyperfine;states xmath3;xmath4 xmath5 hyperfine;states magnetic field;xmath5 hyperfine;states magnetic;field perpendicular xmath6;state magnetic field;state xmath0 xmath1;xmath6 hyperfine;hyperfine state;hyperfine states;state magnetic;state xmath0;xmath3 xmath4;magnetic properties ground;xmath1 xmath2;xmath3 xmath4 xmath5;xmath0 xmath1 xmath2;perpendicular xmath6;field magnetic properties", "pdf_keywords": ""}, "95ee674a03ad23eaaf4837121fc8aea30d885088": {"ta_keywords": "siamese networks learn;siamese networks;deep siamese networks;novel metric learning;use deep siamese;metric learning;deep siamese;metric learning approach;compact preference representations;preference representations;learn distance partial;use popular siamese;siamese;novel metric;siamese formalism;siamese formalism represent;distance partial orders;learn distance function;popular siamese formalism;preference representations use;preferences leverage deep;metric function computationally;directly proposed metric;networks learn distance;popular siamese;present novel metric;recently proposed metric;metric;proposed metric;deep neural", "pdf_keywords": "learning metric network;network learns metric;metric learning;learning distance metric;novel metric learning;learning metric data;network learning distance;metric network;learning distance conditional;learns metric function;learns metric;learning metric;metric learning using;problem metric learning;metric learning approach;encode metric network;structured preference representation;preferences using neural;preference distance lexicographic;learn metric distance;features preference distance;valued network classifiers;encodes metric network;predicting preferences unstructured;preference distance computed;learn metric;network learning;network supervised learning;network supervised;network regression classification"}, "683bbb665bdaea8688834e97559d63842242ee1f": {"ta_keywords": "learning reinforcement agent;reinforcement learning game;learning reinforcement;reinforcement agent trained;improves reinforcement learning;reinforcement agent;reinforcement learning reinforcement;reinforcement learning;reinforcement learning approach;learning game approach;based reinforcement learning;present reinforcement learning;learning game;approach based reinforcement;based reinforcement;based intrinsic fear;reinforcement;agent trained predict;predict probability catastrophe;improves reinforcement;intrinsic fear;game approach;agent trained;intrinsic fear new;fear new method;agent based intrinsic;rank agent based;rank agent;learning approach;trained predict probability", "pdf_keywords": ""}, "13d9d24ff2ba69de4cedcebd8f59371a5c1de7ed": {"ta_keywords": "context words knowledge;words knowledge based;knowledge based learning;words knowledge;useful context words;context words;identify useful context;knowledge based;based learning approach;learning proposed framework;learning approach based;based learning proposed;based learning;identification useful context;useful context;mining data mining;data mining;knowledge;data mining proposed;data mining data;mining proposed framework;mining data;simple word distance;learning proposed;learning approach;word distance;use simple word;context;combination data mining;identify useful", "pdf_keywords": ""}, "788aa828a194a6d6c4e5ab1d4b46fc5f987159b0": {"ta_keywords": "spin glass interaction;interact spin glass;generate spin glass;spin glass spin;application spin glass;applying spin glass;spin glass;glass spin glass;glass spin;spin glass like;glass interaction classical;generate spin;spins interact;spins interact spin;interact spin;interacting spins;interacting spins interact;applied interacting spins;state applying spin;glass interaction;method generate spin;classical application spin;applying spin;application spin;based application spin;glass like state;spins;spin;glass;glass like", "pdf_keywords": ""}, "3df97e8237c7d98c7343fc025eacbbc2b96a10ae": {"ta_keywords": "dynamics exosomes;model dynamics exosomes;dynamics exosomes presence;dynamics exosomes based;description dynamics exosomes;exosomes presence strong;exosomes based;exosomes presence;exosomes based self;exosomes;extracellular field model;particle binding energy;energy particle binding;binding energy model;binding energy particle;strong extracellular field;binding energy;energy binding;binding energy binding;binding energies;energy binding energy;extracellular field;strong extracellular;description binding energy;particle binding;single particle binding;binding energy number;presence strong extracellular;energy number binding;extracellular", "pdf_keywords": ""}, "a43d6fa0e96d56d0200e8d5e4407be8befc4e063": {"ta_keywords": "metamaterials classical;physical resulting metamaterials;metamaterials metamaterials classical;metamaterials classical quantum;examining resulting metamaterials;metamaterials;resulting metamaterials;resulting metamaterials compared;metamaterials compared predictions;resulting metamaterials metamaterials;metamaterials metamaterials;metamaterials compared;given physical;corresponding physical resulting;happening given physical;corresponding physical;choice corresponding physical;behavior given physical;classical models;classical quantum nature;classical models resulting;quantum nature;myopia;given physical paper;quantum;classical quantum;quantum nature corresponding;predictions classical models;physical resulting;properties determined", "pdf_keywords": ""}, "b2fac3812885e3c8101cc729b6846f9108ac4d70": {"ta_keywords": "stretch estimator mle;likelihood estimator stretch;stretch maximum likelihood;estimators stretch maximum;popular stretch estimator;estimator stretch popular;estimators stretch;estimator stretch;estimator mle suboptimal;stretch estimator;maximum likelihood estimator;estimator mle;mle based stretch;mle suboptimal;class estimators stretch;likelihood estimator;maximum likelihood mle;mle suboptimal terms;maximum likelihood;estimators;likelihood mle;likelihood mle based;mle;estimator;box maximum likelihood;class estimators;new class estimators;mle based;stretch maximum;likelihood", "pdf_keywords": "richardson equation mle;estimation presented optimalwe;maxwell lematre richardson;estimation optimal terms;estimation optimal;optimal estimation;lematre richardson equation;error optimal estimation;estimator optimal terms;optimal terms bias;theorems bias reduction;theorems optimal performance;optimal estimation optimal;reduction optimal estimation;optimal estimation parameter;bias reduction optimal;presented optimalwe propose;mle bias;thatwe study optimality;mle theorem guarantees;optimal estimation achieved;estimator theorems bias;richardson equation;jones function mle;error optimal terms;asymptotic bias;mle theorem;presented optimalwe;estimating bias parameter;error estimator optimal"}, "8e56db786a685b4b9c7f1b750f60a81baebff0b5": {"ta_keywords": "soft whispered voice;talk speaker quiet;whispered voice recorded;whispered voice;speaker quiet environment;soft whispered;quiet environment improved;speaker quiet;nonaudible;speech owing acoustic;talk speaker;conduction talk speaker;compared natural speech;microphone;quiet environment;talk certain intensity;natural speech;voice recorded microphone;whispered;nam soft whispered;speech;microphone body conduction;microphone body;nonaudible nam;nonaudible nam soft;voice;signal form talk;body conduction talk;voice recorded;acoustic changes", "pdf_keywords": ""}, "418349df9bf28e2b1290b758a4ebcf0d812c7288": {"ta_keywords": "blog predicting similarity;predicting shape blog;blog predicting;shape blog network;predict shape blog;nodes given blog;given blog predicting;blog network method;blog network;shape blog;predicting similarity supervised;predicting similarity;automatically classify nodes;similarity supervised;given blog;similarity supervised learning;blog;classify nodes;automatically classify;classify nodes given;method predicting shape;supervised learning approach;predict shape;supervised learning;predicting shape;similarity;method based supervised;able automatically classify;classify;used predict shape", "pdf_keywords": ""}, "e00f0a9e184a9d2afd8bb344908ca25d8bdc9e04": {"ta_keywords": "natural language np;language np matrix;natural language computer;design natural language;inputs natural language;language language natural;matrix problems based;language natural language;natural language;natural language language;based natural language;np matrix problems;language computer nl;np matrix;language natural;language np;nl designed task;matrix problems;matrix;language language computer;problems based natural;language computer;respond voice inputs;computer nl designed;computer nl;task oriented respond;oriented respond voice;user inputs natural;language language;language language language", "pdf_keywords": ""}, "692320cf5ae6980bc6b2b2d7bc48df961b545c22": {"ta_keywords": "channel speech enhancement;speech enhancement multiple;speech enhancement single;speech enhancement;speech enhancement context;multi channel speech;enhancement single microphone;speech enhancement presence;single microphone array;microphone array task;microphone array;single microphone;channel speech;microphone;coherent multi channel;enhancement multiple distributed;enhancement multiple;multi channel;single acoustic reference;acoustic reference track;single acoustic;task multi channel;enhancement context 2021;challenge coherent multi;enhancement single;coherent multi;acoustic reference;development coherent multi;enhancement;enhancement context", "pdf_keywords": "multi microphone speech;channel speech enhancement;microphone arrays video;multi channel speech;speech enhancement tasks;speech enhancement multiple;microphone speech recordings;speech recorded microphone;microphone speech;speech enhancement proposed;distant multi microphone;multi microphone;speech enhancement single;distributed microphone arrays;distributed microphone;single microphone array;microphone arrays athis;microphone array;recorded microphone arrays;speech recordings;microphone arrays;speech enhancement;video conferencing rooms;multiple distributed microphone;field speech recorded;chil audiovisual presentations;single microphone;microphone;speech recognition multi;speech enhancement singlethis"}, "87d50fc84c71ed9860ed02b0149266b74c446c9c": {"ta_keywords": "hidden model parameters;regression hidden model;continuous speech recognition;parameters using variational;linear regression hidden;hidden model;speech recognition;parameters nonparametric bayes;model parameters nonparametric;regression hidden;results continuous speech;using variational techniques;using variational;nonparametric bayes;nonparametric bayes manner;parameters nonparametric;continuous speech;techniques derive variational;speech recognition confirm;variational techniques;derive variational;model parameters;variational techniques derive;nonparametric;realizes linear regression;model parameters using;derive variational lower;probabilistic treatment linear;variational;recognition confirm generalizability", "pdf_keywords": ""}, "dc3adb99f682a11fe0507dcbc5dc2958199c5af1": {"ta_keywords": "atom interaction laser;generation xmath0 laser;xmath0 laser pulse;laser atom interaction;interaction laser atom;comb generated laser;concept laser atom;laser atom;generated laser pulse;xmath0 laser;generated laser;laser pulse model;laser pulse;laser pulse frequency;interaction laser;interaction laser field;result interaction laser;laser field laser;pulse frequency comb;laser field model;laser field;field laser;concept laser;laser;frequency comb generated;field laser field;based concept laser;frequency comb;generation xmath0;atom interaction analog", "pdf_keywords": ""}, "48685f26b32d199e6a4d80f6c61e62cc9738e403": {"ta_keywords": "domainadapted parsing;domainadapted parsing form;problem domainadapted parsing;parsing form npnpnp;parsing;parsing form;npnpnp task approach;npnpnp task;structural representation task;deep structural representation;form npnpnp task;support vector machine;npnpnp;form npnpnp;based deep structural;structural representation;deep structural;machine classifiers;vector machine classifiers;approach problem domainadapted;classifiers;rule support vector;machine classifiers achieve;approach based deep;domainadapted;classifiers achieve;vector machine;problem domainadapted;rule support;representation task allows", "pdf_keywords": ""}, "e107beee5e84cd11d6460f7040676687a51a378b": {"ta_keywords": "reconstruction operators based;reconstruction operators;end reconstruction operators;reconstruction network;posed inverse problems;reconstruction network solved;distributions reconstruction;output reconstruction network;posed inverse;ill posed inverse;initialized output reconstruction;output reconstruction;distributions reconstruction ground;variational framework iterative;distance distributions reconstruction;data ill posed;reconstruction ground truth;variational problem initialized;variational framework;ground truth variational;operators based unpaired;unpaired training data;inverse problems proposed;classical variational framework;truth variational problem;variational problem;inverse problems;end reconstruction;end end reconstruction;reconstruction", "pdf_keywords": "learned regularizer adversarial;adaptive regularization adversarial;learned regularizer iteratively;unrolled adversarial regularization;regularization inverse;regularizer adversarial learning;method regularization inverse;regularization inverse problems;trained reconstruction operator;adaptive regularization;regularization adversarial;regularizer adversarial;adversarial regularization;regularization accuracy learned;regularization adversarial setting;variational loss adversarial;data adaptive regularization;reconstruction network iteratively;driven regularization;learned regularizer;driven regularization approach;iteratively unrolled reconstruction;accuracy learned regularizer;unrolled reconstruction network;reconstruction network minimizes;driven regularization accuracy;reconstruction problem variational;reconstruction unrolled operator;end reconstruction operators;regularization"}, "9f1d9dfb0b30d9fc5881d07b8e7f508815296c93": {"ta_keywords": "train new parser;statistical parsers;new parser;parsers;statistical parsers used;parser;parsers used parsing;strategy statistical parsers;parsing;parser new;parsers used;new parser new;parser new domain;parsing process;used parsing process;parse;used parsing;parse datasets;parsing process strategy;model able parse;able parse datasets;able parse;domains language strategy;domains language;multiple domains language;language strategy;training strategy statistical;language strategy make;datasets;language", "pdf_keywords": ""}, "51ec4e93d8ae4c62453fdb34c6866696da0527b1": {"ta_keywords": "detection multimedia fake;multimedia fake news;visual content fake;multimedia fake;content fake;content fake news;detection multimedia;methods detection multimedia;visual features detection;features detection methods;features representative detection;features detection;fake news chapter;representative detection methods;fake news;visual features;visual content;effective visual features;representative detection;visual features representative;detection methods;detection methods detection;detection;fake;visual content including;field visual content;detection methods chapter;multimedia;role visual content;content", "pdf_keywords": "news detection fake;detection fake news;fake news detection;detect fake news;detect fake newsin;multimedia fake news;false news detection;news detection image;fake news visually;false news detecting;characterizing fake news;fake images videos;news detection including;visual content fake;fake news analyze;news detection;news detecting false;news detecting;content fake news;news detection effectively;news detection role;detect false news;detection fake events;fake news detected;fake content;news detection social;content fake content;detection fake;news detection revealed;news detection public"}, "a16cecbaf87d965e396e610f251f710a807b70ad": {"ta_keywords": "hearing impairment simulation;proposed hearing impairment;adjusting hearing impairment;individual hearing impaired;hearing impaired persons;hearing impairment;hearing impairment level;hearing impaired;propose hearing impairment;based approximation auditory;audiogram based approximation;impairment simulation method;audiograms individual implement;manually adjusting hearing;adjusting hearing;audiogram based;easily measurable audiograms;impairment simulation;measurable audiograms individual;accuracy proposed hearing;method uses audiogram;impairment simulation experimental;uses audiogram based;audiograms individual;approximation auditory;individual hearing;measurable audiograms;applied individual hearing;audiogram;audiograms", "pdf_keywords": ""}, "f49065750931c1c3c9edaf7d2f4bc8ea1342450a": {"ta_keywords": "oversmoothing neural autoregressive;neural autoregressive sequence;autoregressive sequence models;neural machine translation;oversmoothing neural machine;neural autoregressive;sequence models;autoregressive sequence;sequence models tackle;oversmoothing neural;issue oversmoothing neural;unreasonably short sequences;model distribution decoding;regularization model distribution;machine translation propose;regularization model;machine translation;short sequences;oversmoothing rate training;autoregressive;explicitly minimize oversmoothing;proposed regularization model;minimize oversmoothing rate;regularization;sequences confirming high;short sequences confirming;degree oversmoothing neural;distribution decoding;proposed regularization;distribution decoding performance", "pdf_keywords": "neural machine translation;neural machine translationwe;neural sequence modeling;translation learning;autoregressive neural sequence;machine translation;machine translation learning;machine translation goal;sequence learning;sequences neural autoregressive;neural autoregressive sequence;short sequences neural;oversmoothing translation low;machine translation conditional;oversmoothing translation;context machine translation;premature translation;machine translation presented;translation context machine;field machine translation;sequence learning process;premature translation context;linear sequence learning;translation quality premature;problem premature translation;translation quality experiments;autoregressive neural;principle translation quality;neural autoregressive;issue oversmoothing translation"}, "b0b1112b06898733faefc32f54940aa4e84bc383": {"ta_keywords": "emphasized speech corpora;speech corpora;speech corpora used;emphasis expressed languages;languages corpora;languages corpora contain;expressed languages corpora;corpus;sentences conversation corpus;corpora;corpus 2030 parallel;conversation corpus collected;conversation corpus;digit corpus;strings digit corpus;corpus collected;corpora used;emphasized speech;parallel sentences conversation;corpus 2030;speakers speak japanese;corpora contain;digit strings conversational;digit corpus 2030;strings conversational;corpora used study;strings conversational setting;collection emphasized speech;speech;corpus collected collection", "pdf_keywords": ""}, "c55bc339122ad8cdba1ae74d1336be3d2f089699": {"ta_keywords": "stochastic convex optimization;convex optimization;convex smooth objective;convex optimization problems;stochastic convex;optimization problems affine;consider stochastic convex;convex smooth;smooth strongly convex;proximal step convex;problems affine constraints;affine constraints;strongly convex smooth;convex;step convex smooth;affine constraints develop;convex smooth strongly;strongly convex;smooth objective functions;optimization methods;step convex;primal dual approach;smooth objective;optimization methods propose;optimization;optimization problems;using optimization methods;convenient using optimization;using optimization;special penalization technique", "pdf_keywords": "stochastic optimization convex;stochastic convex optimization;convex convex optimization;methods stochastic convex;convexity objective theorems;convex optimization;convex optimization following;convex optimization problems;optimization convex objectives;handle stochastic convex;optimization stochastic gradients;convexity stochastic;solution convex optimization;convexity stochastic function;convergence distributed optimization;stochastic convex;optimization convex;method convex optimization;consider stochastic optimization;convexity stochastic differential;convex optimization method;solving convex optimization;problem convex optimization;stochastic optimization;convex objectives;convex optimization problem;given stochastic optimization;optimization stochastic gradient;solve convex optimization;consider stochastic convex"}, "6cf3bdcdee6236f9f04e7773e3601dbbb8fbc61e": {"ta_keywords": "named entity recognition;entity recognition ner;entity recognition;questions natural language;answered natural language;natural language questions;questions entity;entity questions answered;named entity;simple natural language;questions entity questions;generated natural language;entity questions;natural language;language questions natural;ask questions entity;efficiently answered models;questions answered natural;natural language setting;language questions generate;task named entity;answered models;entity;recognition ner;questions answered;ner datasets;questions generate set;ner datasets generated;questions natural;use ner datasets", "pdf_keywords": "entity recognition ner;entity recognition nerner;named entity recognition;learning named entity;entity recognition;disease entities text;new ner datasets;natural language ner;extract disease entities;ner datasets gener;entity recognition based;tuning ner sentences;dataset disease entities;entity level annotations;resource ner datasets;generates ner datasets;human annotated datasets;ner datasets;generates ner dataset;ner dataset disease;approaches named entity;ner dataset;ner training sentences;fine grained entities;ner datasets asking;questions generates ner;entities natural data;named entity;rely human annotated;natural language nern"}, "a0f00d5ea3727151b1c2fc8c407404f0c6641051": {"ta_keywords": "parser based lalarkov;shalzov style parser;parsers;parser;novel parser;parser ability easily;parser ability;simple parser;parsers novel;perform parsing;novel parser based;parsing;perform parsing parallel;parsing parallel;present novel parser;parallel perform parsing;parser based;parsers novel feature;parser used;results parsers novel;cost simple parser;accurate results parsers;style parser;simple parser produces;parser produces accurate;parser produces;parser used analyze;novel feature parser;style parser used;millions sentences parallel", "pdf_keywords": ""}, "3b7321832ba109cf47bfd13595c3b58acd4cb080": {"ta_keywords": "spin polarized electron;polarized electron spin;electron spin orbit;spin orbit coupling;orbit coupling spin;spin orbit interaction;coupling spin polarized;electron spin;orbit interaction spin;interaction spin orbit;coupling spin;effect spin orbit;polarized electron;control spin orbit;interaction spin;spin orbit;spin polarized;orbit coupling;polarized electron used;control spin;effect spin;orbit interaction;orbit interaction used;spin;electron used control;electron;coupling;used control spin;study effect spin;orbit", "pdf_keywords": ""}, "3400b8bf1ffde3ef3d35dfcea893e6506427aa21": {"ta_keywords": "separation speech recognition;recognition multiple speakers;speech sequence unifying;single speech sequence;sequences single speech;speech recognition;mapping speech;speech sequence;source separation speech;learn mapping speech;speech recognition functions;multiple label sequences;sequence unifying source;single speech;framework joint recognition;label sequences single;decodes multiple label;joint recognition;joint recognition multiple;separation speech;label sequences;sequence framework joint;recognition multiple;directly decodes multiple;multiple speakers end;unifying source separation;decodes multiple;multiple speakers;sequence unifying;recognition functions end", "pdf_keywords": "separation speech recognition;end speech recognition;speech mixtures based;speech separation encoder;input speech mixture;speech mixtures;speech mixture;speech recognition task;speech sequence unifying;class speech mixtures;recognition dimensional speech;multiple separate speech;recognition spontaneous speech;speech mixture multiple;separate speech sources;speech recognition encoder;separate speech;decoder networks speech;speech recognition;single speech sequence;speech separations;speech sources encoder;speech separation;source separation speech;experiment speech mixture;independent speech separation;speech based combination;standard speech recognition;sequences single speech;single utterance generate"}, "3ce0f00d6c949192107f1bd6a167c03e1fb7393a": {"ta_keywords": "deterministic dependency parsing;dependency parsing algorithm;dependency parsing;transition based parsers;optimized parsing models;globally optimized parsing;optimized parsing;parsing models;based parsers;parsers;parsing algorithm;parsing;based parsers nears;parsing algorithm attempts;novel deterministic dependency;parsers nears performance;parsers nears;deterministic dependency;arcs dependency structure;dependency structure;easiest arcs dependency;dependency structure nondirectional;dependency;arcs dependency;best transition based;create easiest arcs;easiest arcs;transition based;novel deterministic;accurate best transition", "pdf_keywords": "deterministic dependency parsing;dependency parsing algorithm;deterministic parsing algorithm;parsing sentences based;greedy deterministic parsing;parsing sentences;deterministic parsing;dependency parsing;dependency parsers;dependency parsers optimized;parsing linguistic;transition based parsers;graph based parsers;parsers algorithm;based dependency parsers;based parsers algorithm;parsers algorithm based;best nlogn parser;novel parser based;standard parsing algorithm;nlogn parser;parsing algorithm allows;parsing strategy;parsing linguistic constructs;parsing algorithm able;parsing algorithm significantly;parsing algorithm;standard parsing;novel parser;parsers"}, "b9f5115b0353c268999fcc2f49c4b8e03a223994": {"ta_keywords": "simulating outcomes interventions;experiments class structuredsimulation;class structuredsimulation module;evolutionary causal matrices;class structuredsimulation;based evolutionary causal;structuredsimulation module based;structuredsimulation module suitable;efficient simulating outcomes;present class structuredsimulation;structuredsimulation;structuredsimulation module;simulating outcomes;evolutionary causal;interventions based data;psychology proposed module;outcomes interventions;interventions based;interventions;module based evolutionary;outcomes interventions based;causal matrices markov;simulation model;experiments social psychology;proposed simulation model;simulation model proposed;experiments social;causal matrices;causal;data experiments social", "pdf_keywords": "population dynamics stochastic;populations biological systems;biological systems model;model evolution populations;stochastic nature populations;dynamics stochastic model;stochastic model evolution;systems captures stochastic;patterns biological systems;dynamics stochastic processes;analyzing dynamics stochastic;stochastic processes model;stochastic model based;simulate evolution interventions;model based stochastic;dynamics stochastic;stochastic nature population;propose stochastic model;describing stochastic;stochastic model;stochastic models;dynamics terms stochastic;captures stochastic nature;machine simulate evolution;describing stochastic nature;stochastic processes;populations biological;population dynamics terms;simulate evolution;population dynamics"}, "cc4cc594c7dd38482c46a2db440135b8f26ff54f": {"ta_keywords": "pt skin catalytic;pt xmath0 nanocubes;skin catalytic;fuel cells catalyst;skin catalytic oxidation;skin pt78zn22 proposed;catalyst constructed;structure pt skin;cells catalyst constructed;pt skin pt78zn22;exchange membrane fuel;cells catalyst;catalysis proton exchange;pt skin;faceted pt skin;membrane fuel;concave pt xmath0;membrane fuel cells;skin pt78zn22;catalytic;catalyst;xmath0 nanocubes;catalytic oxidation metal;proton exchange membrane;catalyst constructed combining;highefficiency catalysis proton;pt xmath0;pt skin endows;xmath0 nanocubes highindexted;catalysis proton", "pdf_keywords": ""}, "e2f015bbddd7bade7caca693e37f84c4cf70a7f5": {"ta_keywords": "matrix factorization mnmf;negative matrix factorization;matrix factorization;matrix factorization nsf;factorization mnmf;factorization mnmf multi;factorization nsf proposed;speech binary masking;factorization;factorization nsf;non negative matrix;negative matrix;mnmf multi channel;method enhanced speech;easily accessed speechindicator;speechindicator;enhanced speech binary;matrix;speech binary;accessed speechindicator;speechindicator experiments;accessed speechindicator experiments;spectrum method enhanced;clusters latent variables;binary masking proposed;mnmf multi;cross spectrum method;clusters latent;binary masking;masking proposed initialization", "pdf_keywords": ""}, "f8d7b263e8d663583cd22d5988c8ea4a49ed2840": {"ta_keywords": "entity relation extraction;relation extraction based;relation extraction;extract new relations;relations existing entities;novel relations entities;extract novel relations;relations entities;relations entities used;joint entity relation;entity relation;approach joint entity;joint entity;new relations existing;relations existing;entities used efficiently;entities;existing entities accuracy;new relations;existing entities;entities accuracy better;entity;simple entity;based simple entity;simple entity model;entity model;relations;entity model significantly;entities accuracy;entities used", "pdf_keywords": "relation extraction learns;entity relation extraction;extraction entities relations;extracting entities relations;extracting relations entities;relation extraction independently;relations extraction;extract entities relations;relation extraction model;relation extraction improve;relation extraction;recognition relation extraction;relation extraction natural;entity recognition relation;relation extraction use;relation extraction datasets;extracting relations;relations extraction consider;entity recognition;relation extraction scientific;end relation extraction;extraction entities;relations unstructured text;university relations extraction;extracting entities;information extraction;relation extraction sentence;evaluation relation extraction;approach extracting entities;entities relations unstructured"}, "249b7517a746b1389991e10fd618cad62e66c4df": {"ta_keywords": "extracting word synonyms;similarity extraction;synonyms corpus text;extracting similarity;extracting similarity measures;word synonyms corpus;similarity measures parsed;extract similarity measures;extract similarity;similarity extraction method;method extracting similarity;synonyms corpus;used extract similarity;similarity measures;context similarity extraction;word synonyms;measures parsed text;synonyms;corpus text method;similarity;corpus text;similarity measures different;extracting word;corpus;context similarity;approaches context similarity;problem extracting word;word types;different word types;parsed text", "pdf_keywords": ""}, "0718237a30408609554a0e2b90d35e37d54b1959": {"ta_keywords": "subword mining;subwords fasttext embedding;based subword mining;entropy based subword;subword mining algorithm;semantically meaningful subwords;subwords fasttext;mined subwords fasttext;utilizing mined subwords;meaningful subwords investigate;meaningful subwords;mined subwords;subwords;subwords investigate;subword;based subword;subwords investigate utilizing;downstream language modeling;fasttext embedding model;representations downstream language;fasttext embedding;language modeling task;language modeling;mining algorithm fast;identifying semantically;learned representations downstream;algorithm fast unsupervised;identifying semantically meaningful;fast unsupervised;embedding model compare", "pdf_keywords": ""}, "771c1cb5fb161231e9aaa0a189caba672256a880": {"ta_keywords": "unsupervised morphological disambiguation;unsupervised morphological;morphological disambiguation;context unsupervised morphological;generating words;morphological disambiguation context;processes generating words;directly generating word;generating words sequence;generating word;written morphological analyzer;written morphological;words sequence morphs;morphological;open vocabulary language;word forms generating;generating word forms;morphological analyzer;forms generating words;hand written morphological;predicts words using;language model predicts;predicts words;vocabulary language model;morphological analyzer experiments;words using mixture;model predicts words;introduce open vocabulary;open vocabulary;language model", "pdf_keywords": ""}, "7374494ee88608ef76f74b58a8f8c26ab06adfb9": {"ta_keywords": "processing overlapping speech;overlapping speech generated;overlapping speech;clustering based diarization;speech generated end;diarization frames clusters;end diarization model;end diarization;diarization model proposed;based diarization;based diarization method;diarization frames;diarization model;end end diarization;diarization;clusters number speakers;diarization method;partitions diarization frames;diarization method iteratively;speech generated;diarization model post;processing overlapping;speakers uses end;iteratively select speakers;overlapping;improve overlapped;overlapped;partitions diarization;improve overlapped region;select speakers results", "pdf_keywords": "unsupervised speaker clustering;speaker clustering challenge;speech separation recognition;overlapping speech challenge;overlapping speech detect;speech detect overlapping;detect overlapping speech;overlapping speech frames;speaker clustering;speaker clustering based;based overlapping speech;embeddings overlapping speech;submitted speaker clustering;overlapping speech frame;speech frames dataset;unsupervised speaker;handle overlapping speech;overlapping speech;overlapping speech treating;method unsupervised speaker;speaker embeddings overlapping;speech separation;speaker diarization;proposed speaker diarization;speaker diarization generically;speech frames;tool speech separation;speaker diarization powerful;speaker diarization given;speech detect"}, "6276bbe6cc56234d430725a31a27939eeec88149": {"ta_keywords": "quotability identification;quotability document;quotability identification problem;quotability document quantifies;models quotability identification;model quotability identification;model quotability document;quotability;quoted document model;accuracy quoted document;models quotability;model quotability;quoted document;based model quotability;quantifies accuracy quoted;document quantifies accuracy;superiority models quotability;document quantifies;accuracy quoted;document model;feature based;document;document model outperforms;feature based model;novel feature based;outperforms feature based;feature;novel feature;quoted;identification problem outperforms", "pdf_keywords": "document predicted quotability;texts measure quotability;quotability text documents;quotability source texts;quotability identification;quotability identification aim;quotability text;passage ranking;passage ranking validate;quotability source;passage quotability text;examine passage quotability;predicting text documents;quote counts compare;perform quotability identification;passage ranking problem;similarity derived text;differences quotability source;examine passage ranking;document quotable likely;detect quote ing;quotability identification form;document quotable;passage ranking sentence;predicted quotability;passage quotability;problem quotability identification;problem passage ranking;predicted quotability evaluate;influence passage quotability"}, "96d5e1f691397dfb51e8b818a21a2d11eee46a59": {"ta_keywords": "coding scheme multicore;multicore processing;scheme multicore processing;multicore processing based;sparse linear coding;scheme multicore;multicore;distributed computing;computing systems coding;linear coding scheme;propose sparse;sparse linear;design distributed computing;propose sparse linear;faster uncoded schemes;distributed computing systems;sparse;optimal runtime;linear coding;coding scheme;systems coding;faster uncoded;order optimal runtime;optimal runtime nlog;systems coding solution;times faster uncoded;coding;coding solution achieves;coding solution;advances design distributed", "pdf_keywords": ""}, "75fe6c3ffdea2608794b4f21119c5a4dec07663a": {"ta_keywords": "autoregressive sequence generation;generative flow;using generative flow;density sequential latent;generation using generative;distributions using neural;non autoregressive sequence;sequential latent variables;generative flow technique;conditional density sequential;autoregressive sequence;sequential latent;sequence generation;non autoregressive models;generative;autoregressive models;using generative;conditional density decoding;sequence generation using;density sequential;art non autoregressive;non autoregressive;autoregressive;approach non autoregressive;model complex distributions;modeling conditional density;flow tailored modeling;autoregressive models terms;sequential;flow", "pdf_keywords": "generative model floweq;floweq novel generative;flow based generative;generative flow;neural machine translation;using generative flow;nonautoregressive neural machine;autoregressive sequence generation;nonautoregressive neural;generative flow method;generative process encoding;using generative flows;generative flows;generative flows introduce;machine translation models;model generative;generating translation outputs;novel autoregressive flow;sequence using generative;generative model;translation samples floweq;generation using generative;generating translation;translation models;autoregressive sequence noisy;generative process;model generative model;novel generative;translation nmt models;novel generative method"}, "aa0b93501f79d57fe8542e72ccc8843ea50443c9": {"ta_keywords": "multilingual optimization effectiveness;multilingual optimization;strategies seq2seq models;context multilingual optimization;optimization effectiveness seq2seq;seq2seq models based;seq2seq models stack;seq2seq models;effectiveness seq2seq models;various strategies seq2seq;seq2seq models low;strategies seq2seq;seq2seq;effectiveness seq2seq;model hmm systems;investigate effectiveness seq2seq;model hmm;target language;features context multilingual;models based stacked;hmm systems;scenario target language;multilingual;hidden model hmm;models stack;context multilingual;models low resource;hmm systems investigate;language;optimization effectiveness", "pdf_keywords": "multilingual acoustic models;multilingual acoustic feature;speech recognition large;end speech recognition;seq2seq multilingual features;multilingual features;existing multilingual acoustic;multilingual features effective;approach multilingual acoustic;multilingual acoustic;speech recognition asr;speech recognition;seq2seq automatic speech;monolingual features;monolingual features work;acoustic feature training;multilingual features different;ctc attention networks;multi lingual training;multilingual fine tuning;models multilingual;seq2seq models multilingual;automatic speech recognition;lingual seq models;attention networks including;acoustic models training;lingual training stacked;automatic speech;obtained monolingual features;speech recognition able"}, "de971e50d70bc4d66f7debfab242942b0d1cae34": {"ta_keywords": "cascade speech summarization;speech summarization module;bert based fusion;speech recognition asr;automatic speech;context automatic speech;automatic speech recognition;speech recognition;cascade speech;speech summarization;recognition asr;recognition asr approach;power cascade speech;summarization module;power bert based;noise depth depth;summarization module experiments;asr approach combines;combines power bert;noise depth;speech;signal noise depth;based fusion module;fusion module;bert based;depth information context;information context automatic;depth depth information;how2 dataset;summation signal noise", "pdf_keywords": "cascade speech summarization;bert summarization model;speech summarization model;summarization speech recognition;speech summarization combines;speech summarization exploits;new speech summarization;bert based summarization;speech summarization;summarization speech;bert summarization;approach summarization speech;summarization performance neural;problem speech summarization;proposes speech summarization;summary speech data;summarization model exploit;speech summarization used;translation bert summarization;improving summarization performance;propose speech summarization;summarization task performance;summarization model robust;text summaries spoken;compact summary speech;summarization model proposed;summarization model;summarization taskwe propose;improving summarization;summarization performance"}, "e0ab89821af308f51647bfe872f114d775fd8818": {"ta_keywords": "speech translation medical;multilingual speech recognition;translation multilingual medical;speech translation s2st;speech speech translation;speech translation;multilingual medical data;multilingual speech;translate spoken utterances;translation medical domain;multilingual medical;translation multilingual;speech recognition network;translation medical;network based speech;translation s2st designed;transducers gram language;designed translate spoken;facilitate translation multilingual;development multilingual speech;translation s2st;s2st designed translate;translate spoken;speech recognition;finitestate transducers gram;multilingual;weighted finitestate transducers;gram language models;transducers gram;language utilizes weighted", "pdf_keywords": ""}, "fba7ad8f63a42111b3618e51e3493ed70aafdcd0": {"ta_keywords": "influences people conversation;people conversation data;estimating influences people;conversation data proposed;conversation data;speaker using expectation;word distributions depending;people conversation;word use speaker;word distributions;estimating influences;efficiently estimates word;model estimating influences;different word distributions;probabilistic model estimating;propose probabilistic model;propose probabilistic;conversation;speaker using;influences people;probabilistic model;estimates word use;estimates word;estimating;assumes people use;word use;probabilistic;using expectation maximization;speaker;use speaker using", "pdf_keywords": ""}, "1ce96d8dbf69199ebd043de6cfa25d7e48b8ab03": {"ta_keywords": "causal effects linguistic;effects linguistic properties;effects linguistic;estimating causal effects;estimating causal;linguistic properties;algorithm estimating causal;problem estimating causal;linguistic;causal effects;formalize causal quantity;text algorithm estimating;causal quantity effect;formalize causal;causal quantity;causal;linguistic properties formalize;properties formalize causal;effect writer intent;writer intent establish;writer intent;adjustment text based;results introduce text;introduce text;introduce text algorithm;estimating;text based results;text based;text algorithm;setting bias", "pdf_keywords": "text based causal;causal effects linguistic;causal effects text;effects text observationally;causal model text;linguistic property reader;effect linguistic;effects linguistic;text observationally based;effect linguistic property;text observationally;causal inference proxy;effects linguistic properties;estimating causal effects;effects arbitrary language;model text outcomes;formalize causal effect;text outcomes based;linguistic;estimating causal;text outcomes;outcomes based linguistic;linguistic meaning text;causal inference based;causal inferences;causal inference;control effect linguistic;predict linguistic;linguistic property;proxy text estimate"}, "c37ecbccecab1774b545a5a5804b575718218f7d": {"ta_keywords": "emotional speech recognition;features emotional speech;cnn bottleneck features;features cnn bottleneck;bottleneck features cnn;bottleneck features emotional;improve emotional speech;features cnn;features bottleneck features;cnn bottleneck;features phonemes combine;features bottleneck;bottleneck features;features phonemes;emotional speech;essential features phonemes;features emotional;extract features bottleneck;speech recognition;bottleneck features represent;speech recognition hypothesize;sturucture bottleneck features;deep neural;recognition hypothesize bottleneck;deep neural network;using deep neural;bottleneck;features;phonemes combine;bottleneck sturucture", "pdf_keywords": ""}, "ae5a34c20fee705ad7094c93a711d5f724d535f0": {"ta_keywords": "fairness aware tensor;recommendation fairness preserving;recommendations preserving fairness;enhances recommendation fairness;recommendation fairness;tensor recommendation framework;aware tensor recommendation;fairness preserving quality;fairness proposed framework;novel fairness aware;fairness aware;preserving fairness proposed;fairness preserving;preserving fairness;tensor recommendation;sensitive latent factor;recommendation framework designed;quality recommendations preserving;recommendation framework;aware tensor;fairness proposed;propose novel fairness;sensitive information regularizer;fairness;framework enhances recommendation;recommendations preserving;improve quality recommendations;sensitive information latent;novel fairness;sensitive latent", "pdf_keywords": "enhanced recommendation tensor;tensor recommendation framework;tensor based recommendation;fairness aware tensor;recommendation tensor algorithm;recommendation algorithm tensor;recommendation tensor;aware tensor recommendation;sparsity recommendation fairness;tensor completion fairness;fairness enhanced recommendation;recommender systems tensors;comprehensive tensor factorization;tensor factorization;recommendation fairness;tensor factorization approach;fairness implicit recommender;recommendation quality matrix;sensitive attributes tensor;tensor recommendation;sensitive information tensor;recommendation fairness social;worsening fairness recommendations;recommendation framework;factor matrix sensitive;recommendation framework designed;enhance recommendation quality;traditional matrix factorization;improving recommendation quality;improved recommendation quality"}, "ff187722c5b5462ac2066a737ae97650ffa177ed": {"ta_keywords": "pronunciation assessment noisy;automatic pronunciation assessment;automatic pronunciation;pronunciation assessment;noise reduction gprl_;assessment noisy classroom;novel automatic pronunciation;_gprl_ based noise;gprl_ based sound;noisy classroom;based noise reduction;noisy classroom uses;assessment noisy;noise reduction;prl_ based noise;sound processing;sound processing assisted;noise reduction technique;based sound processing;pronunciation;sound processing performance;based noise;noisy;noise;reduction gprl_ based;based sound;_gprl_ based;gprl_ based;reduction gprl_;sound", "pdf_keywords": ""}, "0f61621206e363367db85b39e8e4325e425afcb4": {"ta_keywords": "singing voice conversion;voice conversion;voice conversion process;converted singing voice;statistical singing voice;voice preserving conversion;conversion accuracy singer;improve statistical singing;accuracy converted singing;singing voice preserving;statistical singing;converted singing;singer using vocoder;singing voice;voice preserving;characteristics source singer;target singer using;singer using;preserving conversion accuracy;converts statistical characteristics;converts statistical;accuracy singer;conversion process proposed;voice;preserving conversion;accuracy singer identity;vocoder based waveform;method converts statistical;conversion process;improve conversion accuracy", "pdf_keywords": ""}, "94c3fd8eea08008cecd98f4aace024cf63954ead": {"ta_keywords": "malicious sensor proposed;malicious sensor;secure estimation linear;approach secure estimation;secure estimation;presence malicious sensor;attack detection;attack detection probability;higher attack detection;sensor proposed estimation;estimation linear process;novel approach secure;estimation linear;filtering learning algorithm;alarm rate constraint;approach secure;using sensor;filtering learning;estimation scheme;proposed estimation scheme;sensor observations;using sensor observations;learns optimal filter;sensor proposed;false alarm rate;sensor observations numerical;detection probability;sensor;detection probability small;estimation scheme uses", "pdf_keywords": "estimation dynamic attack;attack remote estimation;noisy observation attack;detecting stationary attacks;secure estimation algorithm;secure estimation linear;detection secure estimation;secure estimation estimation;secure estimation noisy;attack estimate noisy;detect attack;secure remote estimation;algorithm secure estimation;propose secure estimation;attack detected detection;attacks explicitly detecting;attack proposed method;observation attack;secure estimation;linear attacks;method detect attack;secure estimation challenging;linear attacks attack;attack detection;attack detection secure;estimate presence attacks;attack unknown sensor;considered attack detection;attack attack detection;class linear attacks"}, "473021db54cbae9c4546597cd7e4b5d687a51c7f": {"ta_keywords": "empirical studies crowdsourcing;crowdsourcing problem labeled;crowdsourcing platform amazon;crowdsourcing problem;crowdsourcing;approach crowdsourcing;crowdsourcing platform;studies crowdsourcing;new approach crowdsourcing;studies crowdsourcing platform;approach crowdsourcing problem;labeled training data;labeled training;labeled;rule labeled training;training data attractive;training data propose;problem labeled training;training data;rule labeled;scoring rule labeled;data attractive unique;data attractive;problem labeled;data propose;platform amazon results;data;amazon results;amazon results experiments;platform amazon", "pdf_keywords": "incentive mechanisms crowdsourcing;mechanisms crowdsourcing;crowdsourcing labeled;mechanisms crowdsourcing consider;crowdsourcing labeled training;labeling interface crowdsourcing;labeled data crowdsourcing;crowdsourcing data mechanism;interface crowdsourcing labeled;crowdsourcing items;crowdsourcing design incentive;crowdsourcing design;crowdsourcing goal;amazon mechanical turk;interface crowdsourcing;crowdsourcing data;mechanical turk consider;crowd crowdsourcing items;data crowdsourcing design;data crowdsourcing;crowdsourcing items threshold;crowdsourcing;crowdsourcing consider;crowdsourcing framework;assigning approval crowdsourcing;item crowd crowdsourcing;problem crowdsourcing;mechanical turk;crowdsourcing framework design;new crowdsourcing framework"}, "042959b54176ad2c4f9d0966490ec407b6057527": {"ta_keywords": "xmath1 expansion large;expansion large xmath2;xmath0 expansion large;asymptotic behaviour xmath0;xmath2 limit expansion;expansion xmath1 expansion;xmath1 expansion;xmath0 expansion;xmath0 expansion xmath1;expansion xmath1;large xmath2 limit;behaviour xmath0 expansion;xmath2 limit;large xmath2;expansion restricted small;expansion large;asymptotic behaviour;space study asymptotic;asymptotic;xmath1;limit expansion restricted;expansion restricted;xmath2;limit expansion;parameter space expansion;space expansion restricted;behaviour xmath0;expansion;xmath0;study asymptotic behaviour", "pdf_keywords": ""}, "90dd676184a796e3e5835c8e1f6a632985ce3e89": {"ta_keywords": "loss particle fluid;loss distribution fluid;energy loss particle;spectrum obtained fluid;energy loss distribution;fitting particle spectrum;loss particle;calculate energy loss;fit particle spectrum;analysis energy loss;particle spectrum obtained;spectrum obtained particle;obtained particle spectrum;distribution fluid obtained;distribution fluid;particle fluid;fluid obtained fitting;particle spectrum;obtained fitting particle;fitting particle;particle fluid based;spectrum obtained fit;obtained fit particle;fluid based analysis;loss distribution;fit particle;fluid method based;energy loss;fluid method;spectrum obtained", "pdf_keywords": ""}, "80cb8981af401d9e4df0096626553c514d9e6600": {"ta_keywords": "optical properties pyrochlore;properties pyrochlore lattice;pyrochlore lattice field;pyrochlore lattice;pyrochlore lattice strongly;lattice optical properties;optical properties lattice;properties pyrochlore;properties lattice optical;magnetic field optical;lattice optical;field optical properties;optical properties;change optical properties;effect magnetic field;effect magnetic;investigated effect magnetic;magnetic field effect;pyrochlore;lattice field;lattice field strongly;field optical;properties lattice;lattice strongly affected;induce change optical;presence magnetic;effect field optical;magnetic field;magnetic;lattice", "pdf_keywords": ""}, "12b12ea73652da56023e0e4776211e4f4301f339": {"ta_keywords": "argumentation mining method;argumentation mining;purpose argumentation mining;document purpose argumentation;structure used argumentation;annotation text document;annotation text;argumentation;method annotation text;purpose argumentation;annotation;argument component;argumentation choice;used argumentation;used argumentation choice;method annotation;argumentation choice argument;argument component strongly;novel method annotation;argument component depends;document structure;argument;text document purpose;document structure used;text document;length document structure;document;document purpose;mining method based;length document", "pdf_keywords": ""}, "77b919c4f4f37415d8f1019b1b04191d46de426c": {"ta_keywords": "random walk learning;query execution supervised;fast query;walk learning;constrained random walks;fast query execution;random walks approach;approach fast query;unsupervised random walk;retrieval;random walks;retrieval problems;learning based path;retrieval problems using;walks approach combines;approach experiments retrieval;walks approach;improve query performance;experiments retrieval;filtering approach truncation;query performance;experiments retrieval problems;random walk;walk learning ability;filtering;supervised;path constrained random;supervised learning based;execution supervised learning;execution supervised", "pdf_keywords": ""}, "e862e5f9a17938f1817017b2730e10463d94fb54": {"ta_keywords": "quantum fluid harmonic;fluid harmonic trap;harmonic trap initially;harmonic trap;dimensional quantum fluid;quantum fluid;dynamics dimensional quantum;quantum;dimensional quantum;fluid harmonic;steady state dynamics;cooled steady state;harmonic;state dynamics initially;state dynamics;steady state;trap initially prepared;dynamics dimensional;trap initially;trap;dynamics initially uniform;ground state dynamics;dynamics;dynamics initially;uniform cooled steady;study dynamics dimensional;cooled ground state;cooled steady;form square cooled;steady", "pdf_keywords": ""}, "2826ac3621fdd599303c97cb9e32f165521967b2": {"ta_keywords": "predicting uncertainty scores;uncertainty scores based;uncertainty scores;uncertainty score;uncertainty score method;data uncertainty evaluation;predicting uncertainty;uncertainty evaluation;function uncertainty score;method predicting uncertainty;representation uncertainty score;uncertainty evaluation method;uncertainty score function;score function uncertainty;data uncertainty;based data uncertainty;uncertainty;function uncertainty;representation uncertainty;dimensional representation uncertainty;scores based data;scores based;score method;method predicting;score function;predicting;identify patient cases;based data;score method works;new method predicting", "pdf_keywords": "uncertainty prediction medical;models trained uncertainty;expert disagreement estimated;trained uncertainty;uncertainty classification bias;expert disagreement instances;direct uncertainty prediction;uncertainty prediction;models uncertainty prediction;trained uncertainty scores;high expert disagreement;uncertainty classification;expert disagreement;expert disagreement arising;high expert disagreements;networks estimate uncertainty;expert disagreements;uncertainty imaging task;estimating true uncertainty;prediction medical imaging;disagreement estimated estimating;predicting label disagreement;estimating uncertainty;assume uncertainty imaging;associated uncertainty estimation;uncertainty diagnosis outcome;human experts challenge;uncertainty imaging;uncertainty estimation;predicting outcome noisy"}, "a7f30bae9303825adbc333a8df8a03398dea5151": {"ta_keywords": "automated sentiment classification;automated sentiment;data automated sentiment;sentiment classification;sentiment classification models;sentiment classification elmo;different sentiment classification;classification models syntactically;inputs like sentences;logic rules classification;syntactically complex inputs;study different sentiment;models syntactically complex;classification elmo project;sentiment;classification elmo;syntactically complex;rules classification;like sentences;rules classification provide;models syntactically;different sentiment;classification models;classification provide visualizations;sentences;classification;like sentences use;syntactically;sentences use empirical;classification provide", "pdf_keywords": "accuracy sentences network;sentences neural networks;sentences network;accuracy sentences neural;improved sentence classification;sentence classification;sentences network use;sentiment classification methods;sentiment classification;sentence classification rule;classifying complex sentences;improves accuracy sentences;improving accuracy sentences;approach sentiment classification;sentences neural;different sentiment classification;improve accuracy sentences;sentiment classification based;embeddings logic rules;effectiveness different sentiment;natural language representations;annotated word representations;word embeddings entropic;word embeddings;accuracy sentences;natural language processing;sentences network factor;sentence comparison performance;use neural network;techniques natural language"}, "203da29a37a983c487ce75a894b0d70698077bf5": {"ta_keywords": "links sharing behavior;links sharing;patterns links sharing;pattern links sharing;media social networks;joint information content;joint patterns links;social networks;social networks method;sharing behavior media;patterns links;sharing behavior;potentially problematic content;users news sources;media social;behavior media social;content users news;problematic content;problematic content real;pattern links;users news;analysis joint information;information content;sharing;sharing behavior method;content real world;content;news sources uses;joint information;information content users", "pdf_keywords": ""}, "7f588b1d2a5b199a19a4c3bad6bd5154c7355817": {"ta_keywords": "protein based particulate;application protein corona;protein corona;protein corona blood;protein corona form;availability protein corona;camouflage domains active;composites specific biological;based particulate media;application protein;camouflage domains;widespread application protein;particulate media;biological activity domains;protein based;particulate media work;based particulate;corona blood samples;known domains camouflage;domains active biological;biological properties domains;protein;domains camouflage;corona blood;availability protein;utilization protein based;camouflage;widespread availability protein;specific biological properties;biological properties", "pdf_keywords": ""}, "44aa9a79cfc9eef9ac3f861cfa58a172cb863bd2": {"ta_keywords": "spin orbit interaction;electron gas excitations;magnetic field electron;effect spin orbit;electron gas presence;gas presence magnetic;field electron gas;electron gas;spin orbit;dynamics dimensional electron;field spin orbit;dimensional electron gas;magnetic field spin;case electron gas;suppression electron gas;effect spin;gas excitations effect;gas excitations;orbit interaction dynamics;orbit interaction;electron;dimensional electron;field spin;field electron;presence magnetic;case electron;suppression electron;study effect spin;presence magnetic field;orbit interaction responsible", "pdf_keywords": ""}, "aeffb61024e5ccac5021ca0bf9d199d9196a0521": {"ta_keywords": "tolls population players;constraints population players;enforce tolls population;population players stochastic;repeatedly enforce tolls;enforce tolls;enforcing population distribution;players stochastic dynamics;coupled congestion costs;congestion costs assume;dynamics coupled congestion;population players converge;tolls population;players converge optimal;congestion costs;compute minimum toll;population distribution constraints;distribution constraints population;distribution given toll;minimum toll value;players stochastic;toll value ensuring;converge optimal population;minimum toll;coupled congestion;constraints population;toll construct;optimal population distribution;congestion;given toll construct", "pdf_keywords": "game tolling algorithm;congestion game population;congestion game;player congestion game;class congestion games;modeling congestion;modeling congestion costs;mdp congestion game;congestion games;theorem tolling games;tolling games;constraints games tolling;equilibria player congestion;population players stochastic;global tolled game;congestion costs construct;tolled game operator;equilibria game tolling;tolled game;congestion game mdp;tolling constraint;congestion games players;tolled game induces;tolling algorithm;congestion costs transition;dynamics coupled congestion;game tolling;compute minimum toll;optimal solution game;players stochastic dynamics"}, "dc1d1f64503578d9c5d906da4556f631d4178b04": {"ta_keywords": "prediction vehicle collision;vehicle collision events;vehicle collision;collision events based;collision data;collision events;collision data extensive;distribution collision data;rule based cnn;collision data approach;combining collision data;prediction vehicle;joint distribution collision;collision;cnn based;idea combining collision;combining collision;cnn based approaches;based cnn;distribution collision;approach prediction vehicle;cnn;based cnn based;events based joint;prediction;prediction accuracy proposed;prediction accuracy rule;prediction accuracy;data set prediction;events based", "pdf_keywords": ""}, "de5057c1da9391269e926d4661d4558072db9f18": {"ta_keywords": "novel parallel classifier;parallel cascade attention;speech recognition encoders;speech recognition;automatic speech recognition;parallel classifier;parallel classifier experiments;parallel encoders end;parallel encoders;training strategy parallel;automatic speech;strategy parallel encoders;recognition encoders trained;parallel cascade features;encoders trained;end automatic speech;attention mechanisms encoder;encoders trained using;recognition encoders;cascade attention;stream model trained;cascade attention mechanisms;model trained;trained data output;train parallel cascade;train parallel;novel parallel cascade;trained data;parallel;novel parallel", "pdf_keywords": "data streams training;training parallel encoders;fusion based attention;parallel encoders trained;speech recognition encoders;distributed speech recognition;trained single stream;single stream encoder;speech recognition;training parallel;stream encoder;stream encoder separate;parallel cascade encoders;end speech recognition;streams training;ctc attention framework;stream model trained;parallel encoders;encoders described parallel;automatic speech recognition;attention framework;data streams;attention framework amplitude;automatic speech;distributed speech;cascade encoders;parallel encoders purpose;recognition encoders described;collection parallel encoders;problem training parallel"}, "281605579936538ee92bc4b0baad1b83c683c076": {"ta_keywords": "trees dependency graphs;graph dependency tree;dependency graphs;dependency trees;dependency graphs approach;dimensional representations languages;dependency trees dependency;dependency tree;representations languages using;transition dependency trees;dependency tree demonstrate;graph dependency;resulting graph dependency;representations languages;high dimensional representations;trees dependency;generation high dimensional;dimensional representations;languages using transition;languages using;dependency;languages;representations;based separation graph;languages using approach;graphs;high dimensional;transition dependency;separation graph;graph set linearized", "pdf_keywords": ""}, "4ae0c4a511697e960c477ea3e37b3e11bf3e0e02": {"ta_keywords": "accuracy domain adaptation;dimensional domain adaptation;domain adaptation;domain adaptation range;domain adaptation tasks;training robust convolutional;robust convolutional networks;robust convolutional neural;training robust;neural networks penalizes;robust convolutional;generation robust convolutional;convolutional networks capture;convolutional networks;representations learned;local representations learned;convolutional neural networks;method training robust;improve accuracy domain;learned earlier layers;convolutional neural;networks penalizes predictive;penalized model train;adaptation;accuracy domain;adaptation tasks;adaptation range;high dimensional domain;penalizes predictive;adaptation tasks demonstrate", "pdf_keywords": "training robust convolutional;convolutional networks penalizing;deep representations robust;training robust;robust convolutional networks;networks learn deep;benchmark resembles imagenet;adversarial regularization;patterns deep learning;resembles imagenet outperforms;wise adversarial regularization;imagenet outperforms;robust image classifiers;representations robust;imagenet;learn deep representations;learning deep;robust convolutional;deep learning context;deep learning;regularization par convolutional;adversarial regularization prd;learning deep learning;resulting representations robust;networks penalizing predictive;image classifiers predict;representations robust general;patch wise convolutional;deep representations;patch wise adversarial"}, "ce4db7a32724e0abc8afe27f74d33e32e099b8e6": {"ta_keywords": "fat induced transcript;thefit1 gene;thefit1 gene identified;misexpression thefit1 gene;fit1 fi1 gene;droplet accumulation fat;fi1 gene;fat induced;fi1 gene member;fi1 gene displays;transcript fit1 fi1;induced transcript fit1;accumulation fat induced;lipid droplet;rich lipid droplet;lipid droplet accumulation;triglyceride rich lipid;family important triglyceride;important triglyceride;lipid;accumulation fat;misexpression thefit1;triglyceride;fit1 fi1;resulting misexpression thefit1;fat;rich lipid;important triglyceride rich;triglyceride rich;thefit1", "pdf_keywords": ""}, "7506626f776f211afac2c2d1138aca0e0479e5c3": {"ta_keywords": "gan approach robust;images generated gan;stochastic clipping images;carlo gan approach;monte carlo gan;gan gan approach;generated gan gan;generated gan;based stochastic clipping;gan approach;gan approach method;stochastic clipping;clipping images generated;encodings unseen images;gan gan;images generated monte;clipping images;method recovery images;carlo gan;generated monte carlo;recovery images generated;gan;recovery images;dimensional images generated;experiments dimensional images;unseen images;images generated;approach robust noise;monte carlo;robust noise", "pdf_keywords": "networks gans;networks gans generative;optimization gans;gans generative;generative capacity gans;capacity gans improved;gans generative capacity;neural networks gans;gans optimized;optimization gans optimized;gans improved;demonstrate optimization gans;gans optimized recover;generative adversarial network;images generative adversarial;gans improved combination;generative adversarial;capacity gans;embedding images generative;generative adversarial nature;use generative adversarial;gans measured maximum;gans;combination generative adversarial;robust inversions generative;images generative;capacity gans measured;gans measured;generators deep convolutional;adversarial network extend"}, "712cd873d7370db280f4ceaaf000dc49f76b59fe": {"ta_keywords": "robustness seq2seq models;assessing robustness seq2seq;robustness seq2seq;models untargeted attacks;allows adversarial perturbations;adversarial perturbations;attacks allows adversarial;untargeted attacks demonstrate;untargeted attacks;seq2seq models;additional constraints attacks;allows adversarial;adversarial perturbations meaning;seq2seq models untargeted;constraints attacks;adversarial;constraints attacks allows;seq2seq;attacks demonstrate existing;attacks allows;attacks;robustness;framework assessing robustness;attacks demonstrate;assessing robustness;assumption source perturbations;models untargeted;source perturbations;evaluation methods preserve;source perturbations result", "pdf_keywords": "sentence adversarial loss;adversarial attacks seq2seq;word sentence adversarial;sentence adversarial;adversarial source word;attacks translation models;systematic attacks translation;adversarially correct word;evaluating adversarial perturbations;make adversarial attacks;make adversarial;attacks translation;source adversarial;evaluate adversarial attacks;evaluating adversarial;evaluate adversarial;adversarial perturbations meaningwe;adversarial source;unconstrained source adversarial;adversarial training;adversarial source unconstrained;source adversarial source;finding adversarially correct;adversarially;evaluating adversarial examples;allows adversarial perturbations;adversarial;adversarial examples chrf;adversarial loss;finding adversarially"}, "1c8d9d5558dc43f3505fa37fc50247e3ce0d2f54": {"ta_keywords": "deconfounded observational dataset;deconfounded data;estimating average treatment;combines deconfounded data;data small deconfounded;deconfounded observational;deconfounded data small;selecting samples deconfounded;small deconfounded observational;deconfounded repeating trial;samples deconfounded;samples deconfounded repeating;combines deconfounded;deconfounded;average treatment effect;average treatment;setting combines deconfounded;deconfounded repeating;observational dataset demonstrate;mutation cancer;small deconfounded;estimating average;genetic mutation cancer;treatment effect;observational dataset;dataset demonstrate;dataset demonstrate ate;dataset;approach estimating average;dataset related genetic", "pdf_keywords": "unobserved confounder;deconfounded confounder;confounder anfounded graph;confounder based discovery;deconfounded confounder derive;assumption unobserved confounder;unobserved confounder categorical;confounded observational data;confounder based;deconfounded data analysis;binary confounder based;confounders assumed;deconfounded data estimation;confounder ate estimated;confounder anfounded;quantification confounders;assume confounders assumed;discovery binary confounder;confounders noisy data;quantification confounders noisy;deconfounded observational data;estimating causal effects;confounded data approach;confounders assumed cases;assume confounders;confounder associated threshold;statistics confounders;deconfounded data demonstrate;confounder model;model quantification confounders"}, "4d86b32ea80e2d9df2283fac39892d6dbd87ea87": {"ta_keywords": "support vector machine;pattern recognition;error pattern recognition;pattern recognition method;range pattern recognition;classifiers;support support vector;support vector;prototype based classifiers;pattern recognition problems;recognition method;class discriminant functions;based classifiers new;discriminant functions geometric;based classifiers;vector machine novel;classifiers new method;classifiers new;vector machine;general class discriminant;margin maximization support;discriminant functions;class discriminant;geometric margin maximization;discriminant;recognition method increases;recognition;functions geometric margin;recognition problems;margin maximization", "pdf_keywords": ""}, "8786ddc38ae0763e772337bf9331436252452918": {"ta_keywords": "mitigating entity bias;fake news detectors;fake news detection;entity bias extensive;entity debiasing;entity bias;mitigate entity bias;entity debiasing framework;entity bias cause;propose entity debiasing;generalizes fake news;news detection models;news detectors;news detectors online;debiasing framework;debiasing framework generalizes;news detection;base fake news;entities contents training;cause entities contents;debiasing;bias cause;bias extensive offline;bias cause effect;chinese datasets demonstrate;cause entities;bias extensive;entities mitigate entity;bias;entities mitigate", "pdf_keywords": "novel entity debiasing;entity bias existing;entity debiasing;entity debiasing framework;detect fake news;mitigating entity bias;entity bias framework;entity bias real;detection fake news;entity bias;propose entity debiasing;news detect fake;entities news veracity;unintended entity bias;entity bias enhances;mitigates entity bias;fake news detection;overlooked entity bias;entity bias paper;fake news detectors;fake news detector;entity bias cause;generalizes fake news;novel debiasing framework;newscop propose debiasing;news detectors framework;propose debiasing framework;debiasing framework fake;false news detection;entities news"}, "9d0e4e9c9343b85311b1adff145fdbdfb69486ff": {"ta_keywords": "relativistic astroparticle emission;cosmic ray emission;galactic center observations;astroparticle emission model;observation cosmic ray;framework relativistic astroparticle;emission galactic center;relativistic astroparticle;emission galactic;ray emission galactic;astroparticle emission;observation cosmic;resolution observation cosmic;galactic;cosmic ray;galactic center;emission model observations;cosmic;astroparticle;emission model results;ray emission;relativistic;emission model;framework relativistic;center observations;high resolution observation;center observations carried;emission;carried framework relativistic;theoretical models observations", "pdf_keywords": ""}, "cc74ef901219dfd26efbbb8b7b87d1b7b7d38634": {"ta_keywords": "document represented random;define given document;documents brain particular;structure documents brain;define document;define document document;definition document document;documents brain;definition document;given document document;document represented;document document based;document document;given document;hypothesis document represented;structure documents;document;document based hypothesis;document based;used define document;documents;document document problem;information processing;represented random variable;represented random;document problem relevant;information processing present;study structure documents;document problem;brain particular", "pdf_keywords": ""}, "cd06dfa789bfe491130ac7440e55d9d407396a43": {"ta_keywords": "algorithm accelerated methods;accelerated methods;accelerated methods solving;accelerated methods applicable;coordinate descent methods;randomized coordinate descent;class accelerated methods;coordinate descent;descent methods proposed;algorithm accelerated;original algorithm accelerated;descent methods;methods solving optimization;approximate conjugate directions;optimization problems big;methods proposed acceleration;optimization;variant original algorithm;solving optimization;popular randomized coordinate;acceleration strategy robust;solving optimization problems;coordinate directions spectral;randomized coordinate;spectral conjugate directions;accelerated;proposed acceleration strategy;optimization problems;dimensions based augmentation;class accelerated", "pdf_keywords": "stochastic descent theorems;descent methods theorems;descent theorems optimal;randomized coordinate descent;stochastic coordinate descent;stochastic descent methods;consider stochastic descent;descent theorems;stochastic descent;stochastic descent known;stochastic descent method;descent stochastic;theorem stochastic;stochastic descent sp;convergence stochastic descent;vectors stochastic descent;given theorem stochastic;theorems optimal;given stochastic descent;sd stochastic descent;stochastic descent stochastic;stochastic descent algorithm;information theorems optimal;stochastic spectral descent;descent methods;class stochastic descent;stochastic descent problem;stochastic descent rate;acceleration strategy randomized;methods theorems theorems"}, "aacaad6ab396e085799052b1a667c965d6465e32": {"ta_keywords": "protein interaction emulsions;protein emulsions emulsifying;interaction protein emulsions;protein emulsions;protein emulsifying activity;protein emulsifying;protein emulsions fully;mechanism protein emulsifying;mobility protein emulsions;emulsifying activity protein;emulsions emulsifying weakening;emulsions emulsifying;interaction emulsions;emulsions fully understood;interaction emulsions known;protein protein interaction;protein interaction;emulsions;emulsifying activity resulting;emulsifying activity;emulsions fully;affect emulsifying;protein mobility;protein interaction protein;emulsions known;emulsifying;decrease protein mobility;affect emulsifying activity;emulsions known affect;emulsifying weakening", "pdf_keywords": ""}, "d5810f15cfdd59da549ffa648c5a05d806d94eb7": {"ta_keywords": "fact checking platform;automated fact checking;fact checking;automated fact;present automated fact;evidence supports refutes;platform given claim;predicts piece evidence;claim predicts;given claim predicts;claim predicts piece;evidence supports;supports refutes claim;evidence;piece evidence supports;verdict architecture;final verdict architecture;refutes claim;user study platform;refutes claim returns;checking platform;piece evidence;user friendliness transparency;fact;present automated;study platform predictions;automated;verdict architecture choices;checking platform given;returns final verdict", "pdf_keywords": "fact checking platform;automated fact checking;fact checking dataset;novel fact checking;fact checking present;fact checking;journalistic corpus testing;journalistic corpus;claims identifying sentences;checking platform journalists;using journalistic corpus;propose automated fact;checking claims news;new automated fact;evidence large document;scale fact checking;automated fact;enormous data journalists;supporting refuting evidence;corpus testing journalists;evidence regarding factual;automated checking claims;sentences providing evidence;claims field russian;searching supporting refuting;factual claims evaluated;refuting evidence;evaluated using journalistic;data journalists lacking;claims news"}, "a5148776955ef523de318a2fb45f8256e966b98e": {"ta_keywords": "labels labeling scheme;labeling scheme accurate;labeling scheme;standard labeling scheme;labeling labels accurate;labeling scheme labels;labeling scheme labeling;use labeling scheme;accurate standard labeling;scheme labeling labels;labeling scheme illustrate;labeling accurate;labels accurate standard;labeling scheme advantage;labels accurate;labels labeling;labeling accurate standard;labeling scheme collection;labeling labels;scheme labels labeled;labels labels labeling;labels labels;standard labeling;labels;labeling;labels labeled linear;scheme labels;labels labeled;scheme labeling;use labeling", "pdf_keywords": ""}, "05c2bb89a5c42ad7932420bb39df2e566df6e1ec": {"ta_keywords": "friendly editor parallel;editor parallel version;editor parallel;parallel version java;java library;user friendly editor;version java library;parallel version;java;friendly editor;version java;editor;efficient user friendly;parallel;library;development efficient user;efficient user;article development efficient;development efficient;new article development;user friendly;efficient;development;friendly;article development;present new article;version;new article;user;new", "pdf_keywords": ""}, "2127bea25859ba9c5997e2d15e17899a75ef6cb3": {"ta_keywords": "programming big code;programming big;big code tools;debugging big code;topic big code;big code;big code process;learning debugging big;big code talk;debugging big;seminar programming big;code tools;code tools developed;tools data mining;programming;learning debugging;tools learning debugging;data mining talk;mining data analysis;code process;code talk;data mining;data mining data;development tools data;development tools learning;short seminar programming;code;seminar programming;mining data;code talk devoted", "pdf_keywords": ""}, "3e2bac2abfb5b33a43fe56db5a868e17e38c616a": {"ta_keywords": "project based learning;teaching machine learning;teaching machine;performance computing classes;learning high performance;high performance computing;approach teaching machine;machine learning high;project based;performance computing;use project based;based learning approach;based learning;classes approach based;classes;computing classes;use project;computing classes approach;classes approach;computing;machine learning;project workshops;project workshops discussed;learning approach;based use project;learning approach illustrated;learning;teaching;results project workshops;learning high", "pdf_keywords": ""}, "baf47cd0b471a9bb7b2230fec0b680fc9b3c4783": {"ta_keywords": "pragmatic speaker model;natural language instructions;generate natural language;generation natural language;pragmatic speaker;listener speaker models;language instructions variety;present pragmatic speaker;speaker models simulate;speaker models;natural language;instructions variety tasks;traditional speaker model;listener speaker;speaker model model;language instructions;speaker model;base listener speaker;pragmatic;instructions variety;present pragmatic;speaker;speaker model able;learned base listener;simulate interpretation candidate;models simulate interpretation;traditional speaker;alternative descriptions preferred;simulate interpretation;alternative descriptions", "pdf_keywords": "pragmatic models instruction;instructions structured representations;pragmatic inference models;pragmatic reasoning generate;explicit pragmatic inference;instruction generation tasks;pragmatic models;descriptions pragmatic listener;pragmatic inference;instruction generation;pragmatic listener capable;generating instructions actions;pragmatic inference procedure;pragmatic speaker model;tasks reasoning speaker;reasoning speaker capable;use pragmatic inference;generation tasks reasoning;models instruction following;complex sequential instruction;sequential instruction;descriptions pragmatic;instructions structured;pragmatic reasoning;models instruction;following instruction generation;environments pragmatic speaker;generating instructions;instructions actions natural;rational model instruction"}, "00cc6deb3cf2c9281ddcf4875aad3ee14c92e52f": {"ta_keywords": "lingual named entity;named entity recognition;entity recognition leverages;entity recognition;entity recognition variety;cross lingual named;machine translation systems;approach cross lingual;translation systems;machine translation;lingual named;leverages machine translation;cross lingual;translation systems able;lingual;named entity;matches based distributional;recognition variety languages;languages average;entity;languages average points;translation;based distributional statistics;languages;statistics derived dataset;identify matches based;recognition leverages machine;distributional statistics;methods named entity;distributional statistics derived", "pdf_keywords": "predicting entity recognition;named entity recognition;entities corpus;entity recognition;crosslingual named entity;entity recognition large;important entities corpus;lingual named entity;entity recognition diverse;entities corpus perform;translating entities;entities corpus method;corpus target language;entity recognition remain;nerns corpus target;entities corpus documents;entity recognition owing;cross lingual corpora;entity recognition problem;language nerns corpus;machine translation systems;translating entities matches;entities target sentence;mt translating entities;large corpus languages;approach crosslingual named;machine translation;machine translation improve;natural language nerns;nerns cross lingual"}, "be360de73689dc4af56f7adcee7e38d7acfed1e1": {"ta_keywords": "aggregate orderings;orderings rankings;orderings rankings collection;set orderings rankings;aggregate orderings maintains;rankings collection;rankings collection aggregate;game match race;collection aggregate orderings;rankings;orderings;match race available;match race;fairness derive lower;certain properties fairness;set orderings;aggregating given set;orderings present;aggregating;given set orderings;orderings maintains certain;fairness derive;fairness;aggregating given;number orderings;bounds size lists;logarithmic number orderings;aggregate;www match org;properties fairness", "pdf_keywords": ""}, "0f5bb9ae0c060b349597c0b2582bf271a5a2156a": {"ta_keywords": "bidirectional neural models;fully bidirectional neural;bidirectional neural;long range tags;bidirectional long;text outperforms approaches;model bidirectional long;bidirectional long range;fully bidirectional;range tags;range tags basis;bidirectional;single text outperforms;capture entire sentence;sentence single text;model bidirectional;text outperforms;text;neural models;single text;developed fully bidirectional;tags;tags basis;neural;neural models model;art model bidirectional;long range;sentence single;entire sentence single;tags basis recently", "pdf_keywords": ""}, "ce268e0942ce0d1f6942e4d7e7e2aa6464f1b577": {"ta_keywords": "native pronunciation learning;pronunciation learning knowledge;evidence pronunciation learning;pronunciation learning;pronunciation learning using;non native pronunciation;non native pronunciations;native pronunciations experiments;native pronunciation able;acoustic evidence pronunciation;native pronunciations;native pronunciation;pronunciations experiments;learning using acoustic;evidence pronunciation;proficiency speakers;pronunciation able achieve;pronunciation able;pronunciations experiments degrees;pronunciation;pronunciations;using acoustic evidence;high proficiency speakers;knowledge non native;acoustic evidence;proficiency proposed method;non native;english proficiency;learning knowledge non;english proficiency proposed", "pdf_keywords": ""}, "cb53f9558bd13c853026f97dce3bbe3d989ca97d": {"ta_keywords": "fallacies language game;game learning fallacies;language game demonstrate;code case game;learning fallacies language;language game;case game demonstrate;popular game learning;game demonstrate;learning fallacies;game demonstrate method;fallacies language;game learning;case game;porting popular game;adapting code case;popular game;code case;fallacies;code code case;adapting code;works adapting code;adapting code code;game;code;demonstrate;code code;language;adapting;demonstrate method works", "pdf_keywords": ""}, "57dd2bd5fb6677191f9b36b589c91bb171e217ff": {"ta_keywords": "xmath1 xmath2 xmath3;xmath2 xmath3 xmath4;xmath16 xmath17 xmath18;xmath0 xmath1 xmath2;xmath15 xmath16 xmath17;xmath9 xmath10 xmath11;xmath10 xmath11 xmath12;xmath17 xmath18 xmath19;xmath3 xmath4 xmath5;xmath1 xmath2;xmath14 xmath15 xmath16;xmath4 xmath5 xmath6;xmath8 xmath9 xmath10;xmath12 xmath13 xmath14;xmath16 xmath17;xmath13 xmath14 xmath15;xmath11 xmath12 xmath13;xmath7 xmath8 xmath9;xmath17 xmath18;xmath3 xmath4;xmath5 xmath6 xmath7;xmath6 xmath7 xmath8;behaviour xmath0 xmath1;xmath0 xmath1;xmath2 xmath3;xmath19 xmath20 xmath21;xmath10 xmath11;xmath18 xmath19 xmath20;xmath15 xmath16;xmath9 xmath10", "pdf_keywords": ""}, "a4dd375c18709b1554249cc5cb88d8ba6acfea10": {"ta_keywords": "machine translation discuss;machine translation;field machine translation;state machine translation;translation;translation discuss factors;translation discuss;field machine;machine;state machine;growing field machine;brief review;rapidly;present brief;current state machine;present brief review;brief;field;review;contribute rapidly;brief review current;review current state;growing field;rapidly growing field;contribute rapidly growing;growing;rapidly growing;factors contribute rapidly;contribute;discuss", "pdf_keywords": ""}, "a95400c70c4beb609c77cc500677b2f1ed852e8e": {"ta_keywords": "automatic question generation;question generation;question generation able;questions using semantically;context automatic question;phrase level inference;automatic question;motivated phrase level;predict answer question;questions accurately expressed;semantically motivated phrase;phrase level;infer context question;answer infer context;questions accurately;accurately predict answer;multiple choice questions;semi semantically motivated;choice questions using;answer question uses;semantically motivated;semantically semi semantically;generates multiple choice;questions using;level inference;context automatic;question uses information;answer questions accurately;inference;inference approach utilizes", "pdf_keywords": ""}, "63d99a61e798d7cb714f336a8d581ae2b75672ee": {"ta_keywords": "resource challenge predicting;challenge predicting;machine learning novel;machine learning;challenge predicting behavior;zero resource challenge;behavior machine learning;learning novel autoregressive;predicting behavior machine;predict behavior machine;machine learning apply;predicting;resource challenge;predict behavior;learning novel probabilistic;predicting behavior;novel autoregressive model;used predict behavior;autoregressive;autoregressive model;novel autoregressive;model zero resource;predict;novel probabilistic;novel probabilistic model;autoregressive model used;zero resource;behavior machine;learning;present zero resource", "pdf_keywords": "contrastive language prediction;deep cluster goal;clustering deep cluster;resource speech challenge;deep cluster network;deep cluster;deep cluster proposed;clustering deep;language prediction;speech challenge;deep cluster method;speech representations framework;speech challenge 2021;applying deep cluster;speech representations;discrete speech representations;outputs deep cluster;zero resource speech;clustering outputs deep;learning discrete speech;contrastive predictive coding;means clustering deep;language prediction based;generate linguistically discriminative;combine deep cluster;contrastive learning machine;linguistically discriminative representations;cpc deep cluster;predictive coding tasks;contrastive predictive"}, "d4305b3bf233e5f192a5d17dde114b771b621d92": {"ta_keywords": "estimating power spectrum;estimate power spectrum;power spectrum particle;estimation power spectrum;spectrum particle background;spectrum free energy;energy particle background;power spectrum;spectrum particle;power spectrum free;resulting power spectrum;particle background field;spectrum used estimate;free energy particle;power spectrum used;energy particle;method estimating power;particle background;spectrum free;estimate power;spectrum;estimation power;background field obtained;estimating power;background field method;spectrum used;field resulting power;used estimate power;case particle background;based estimation power", "pdf_keywords": ""}, "de3c3eb590065a6d78ec8566161f8236ab2a7435": {"ta_keywords": "mpi dt architecture;parallel mpi dt;hybrid mpi dt;mpi dt cores;composed parallel mpi;parallel mpi;dt architecture hybrid;hybrid mpi;study hybrid mpi;frequency mpi dt;parallel translation modes;ghz frequency mpi;dt architecture designed;frequency mpi;parallel translation study;parallel translation;mpi dt;dt architecture;number parallel translation;hybrid composed parallel;results parallel translation;dt cores single;architecture hybrid;cores single ghz;architecture hybrid composed;translation study hybrid;single ghz;dt cores;mpi;translation modes provide", "pdf_keywords": ""}, "ffb562d3ac7d86b5c527863f5a3e72e1aa22a809": {"ta_keywords": "rational agents method;heterogeneous rational agents;predicting behavior physical;rational agents;agents act micro;predicting behavior;behavior predicting;method predicting behavior;behavior demonstrate prediction;behavior predicting behavior;predicting behavior behavior;agents method based;act micro agents;behavior physical based;agents method;agents influence behavior;demonstrate prediction process;agents agents influence;prediction process;demonstrate prediction;micro agents agents;agents influence;predicting;act micro;micro agents;method predicting;behavior agents;heterogeneous rational;prediction;use heterogeneous rational", "pdf_keywords": ""}, "076b2ba158c35bd2941769864ce7455cf76ecd8e": {"ta_keywords": "herding peer review;controlled trial herding;trial herding peer;discussions strong causal;trial herding;herding peer;effect discussion initiator;causal effect opinion;discussion opinion reviewers;peer review discussions;discussion initiator opinion;opinion outcome discussion;herding;causal effect discussion;review discussions strong;outcome discussion;discussion initiator;review discussions;effect discussion;discussions large scale;initiator opinion outcome;peer review;effect opinion presented;melix discussions large;discussions;effect opinion;causal effect;discussions large;discussions strong;reviewers", "pdf_keywords": "discussion peer review;peer review effect;discussion strong influence;herding behaviour peer;peer review discussions;observed peer review;peer review discussion;herding effect conference;peer review bias;peer group discussions;biases investigate herding;behaviour peer review;peer review manifestation;discussion initiated reviewers;discussion effect caused;discussion effect;outcome discussion peer;discussion peer;initiator discussion effect;effect conference peer;investigate herding behaviour;findings suggest herding;herding effect;distribution participating reviewers;effect herding behaviour;conference peer review;influenced opinion discussion;manage discussion peer;discussion disproportionately influenced;peer review process"}, "80b747af8d86541cf53198519c8fa51109eed4f9": {"ta_keywords": "supervised data augmentation;unlabeled data augmentation;supervised augmentation uda;unlabeled supervised augmentation;data augmentation;supervised augmentation;data augmentation popular;data augmentation ojta;data augmentation does;complex data augmentation;augmentation ojta;augmentation popular effective;augmentation uda using;augmentation;augmentation uda;data classification examine;augmentation ojta simplest;unlabeled supervised;efficacy unlabeled supervised;augmentation popular;data classification;augmentation does require;augmentation does;approach data classification;supervised data;supervised;classification examine;classification;unlabeled data;popular implementation ojta", "pdf_keywords": "unlabeled data augmentation;augmentation strategies text;sequence labeling tasks;sequence tagging tasks;extend sequence tagging;novel data augmentation;classification sequence tagging;data augmentation strategies;data augmentation methods;data augmentation;random word replacement;naive data augmentation;sequence tagging;augmentation perform classification;word replacement;perform sequence tagging;replace words;benchmark text classification;sequence labeling;data augmentation perform;natural language tasks;text model trained;augmentation perform sequence;text classification sequence;classification tasks extend;unlabeled data supervised;semi supervised learning;data augmentation method;incorporating unlabeled data;language tasks robust"}, "b033400e9a80915a928f4603582e5e8bf7656a85": {"ta_keywords": "multimodal navigation;multimodal navigation based;performance multimodal navigation;multimodal navigation systems;method multimodal navigation;multimodal data mining;multimodal data;performance multimodal;multimodal;navigation;navigation based;method multimodal data;assessment performance multimodal;navigation systems;navigation based use;navigation systems present;use unimodal baselines;new method multimodal;unimodal baselines;improve performance navigation;method multimodal;navigation provide simple;unimodal baselines substantially;performance navigation;unimodal baselines demonstrate;performance navigation provide;based use unimodal;navigation provide;use unimodal;demonstrate use unimodal", "pdf_keywords": "navigation models multimodal;navigation question answering;performance unimodal navigation;unimodal navigation models;evaluation unimodal navigation;egocentric question answering;navigation agent trained;assessing performance multimodal;performance multimodal;models multimodal;multimodal;navigation egocentric;unimodal navigation;visual navigation egocentric;navigation tasks;multimodal datasets provide;room question answering;navigation subtask benchmark;bias multimodal datasets;multimodal datasets;question answering component;bias multimodal;performance multimodal techniques;natural language navigation;answering use neural;present navigation agent;sensing navigation tasks;navigates scene;model navigation tasks;agent navigates scene"}, "c0099a15bd3251083c62ebd47c9705a16309b974": {"ta_keywords": "zero crossing image;crossing image point;crossing image;point source algorithm;scale zero crossing;image algorithm;phase shift algorithm;image algorithm based;zero crossing;image point source;image point;representation image algorithm;shift algorithm;dynamic phase shift;time representation image;static phase shift;dimensional discrete time;crossing;phase shift;simple algorithm generation;shift static phase;algorithm;algorithm generation;discrete time representation;phase shift static;simple algorithm;source algorithm;dynamic phase;combination dynamic phase;representation image", "pdf_keywords": ""}, "a16ae67070de155789a871cb27ecbf9eaa98b379": {"ta_keywords": "text annotation separation;textual sources training;annotation separation crucial;sources text annotation;annotation separation;quickly identifying textual;text annotation;identifying textual sources;identify textual sources;identifying textual;machine annotation introduce;quality text annotation;identify textual textual;textual sources text;identify textual;textual textual sources;correctly identify textual;machine annotation;textual sources;annotation separation state;textual textual;annotation;textual sources provide;annotation introduce method;annotation introduce;ability identify textual;textual;sources text;human machine annotation;art text annotation", "pdf_keywords": "generated text evaluations;text generation evaluators;generated text training;generated text annotated;human generated text;text evaluations;generated text compared;generated content human;text generated;human evaluations evaluator;generated text neural;generated text achieved;machine generated text;natural language generation;generated text simply;text generated machine;untrained evaluations results;generate human text;detect generated text;machines generate text;uses generated text;untrained evaluations;quality text generated;generated text;evaluation task untrained;text generation;generate text human;rely human evaluations;text sufficient generate;generated content annotator"}, "ece56ab633f11d1592a3d4f9386412d3f48fcf95": {"ta_keywords": "implicit warrants crowdsourcing;warrants crowdsourcing approach;warrants crowdsourcing;crowdsourcing process warrants;crowdsourcing process;scalable crowdsourcing;crowdsourcing approach relies;scalable crowdsourcing process;crowdsourcing;linguistic accuracy warrants;relies scalable crowdsourcing;crowdsourcing approach;characterizing characterizing warrants;characterizing warrants;characterizing implicit warrants;linguistic accuracy claims;characterizing linguistic accuracy;warrants characterizing;accuracy warrants characterizing;characterizing warrants general;identifying plausible warrants;warrants general context;accuracy warrants;linguistic accuracy;authentic arguments news;implicit warrants;characterizing linguistic;warrants characterizing variance;accuracy claims approach;arguments news comments", "pdf_keywords": "natural language argumentation;argument comprehension tackles;arguments annotated crowd;natural language arguments;called argument comprehension;language argumentation human;language argumentation task;argument reasoning comprehension;argumentation task called;reasoning natural language;argument comprehension;argumentation task;argument generated annotators;annotated arguments;language argumentation;large corpus arguments;reasoning comprehension tackles;arguments annotated;dataset 200 debates;large dataset debates;argumentation human;task scalable crowdsourcing;human reasoning tasks;analysis annotated arguments;argument reconstruct warrants;argument reasoning;large crowdsourcing study;measures crowdsourcing experiments;reasoning tasks;large crowdsourcing"}, "2232808cf3161ca4c434126e35f47ee33c0c8219": {"ta_keywords": "explanations given student;training explanations accurate;student training explanations;training explanations;based intuition explanations;explanations explanations worth;explanations explanations;simulate teacher model;quantify value explanations;trained simulate teacher;intuition explanations accurate;student model trained;teacher model;explanations worth;simulate teacher;given student model;explanations worth mere;student model;given explanations explanations;teacher model approach;explanations accurate simply;explanations;intuition explanations;model trained simulate;explanations accurate available;explanations accurate;value explanations;trained simulate;model trained;worth given explanations", "pdf_keywords": "learning explanations;explanations training;explanations student model;incorporate explanations training;explanations training student;learning explanations used;explanations necessary learning;learns explain explanations;learn explanations;necessary learning explanations;learning strategies explanations;prediction explanation generation;learn explanations benefit;explanations text classification;methods learn explanations;incorporate explanations text;explanations dataset effective;explanations student;explanations communicate information;explanations utility;explanations communicate;candidate explanations dataset;explanations utility student;student learns explain;learns explain;generated explanationswe propose;incorporating explanations human;incorporating explanations;explanations techniques;incorporate explanations"}, "228f2efe7b06b6db3b2c6c0a61d7b33daee1d641": {"ta_keywords": "sense disambiguation unsupervised;disambiguation based unsupervised;sense disambiguation based;disambiguation unsupervised way;unsupervised word sense;word sense disambiguation;disambiguation unsupervised;sense disambiguation;disambiguation based;unsupervised word;sentence synsetconstituting sense;new unsupervised word;synsetconstituting sense;disambiguation;unsupervised induced synsets;perform word sense;semantic similarity;synsetconstituting sense target;sense target word;sense input word;semantic similarity given;word sense;sentence synsetconstituting;synsets;synsetconstituting;unsupervised way able;unsupervised way;unsupervised;synsets able perform;datasets outperforms sparse", "pdf_keywords": ""}, "301352755a94d7524312b7c7f2fab7d3fd3d334d": {"ta_keywords": "preferences probabilistic uncertainty;conditional preferences probabilistic;preferences probabilistic;qualitative conditional preferences;agent aggregation propose;aggregation individual agents;agent aggregation;multiagent aggregation;conditional preferences;agent agent aggregation;individual agents aggregate;multiagent aggregation individual;agents aggregate;aggregate single agent;agents aggregate single;probabilistic uncertainty;context multiagent aggregation;aggregation propose tractable;probabilistic;probabilistic uncertainty study;aggregation propose;pcp nets formalism;multiagent;tractable approximation dominance;individual agents;concept pcp nets;aggregation;aggregation individual;qualitative conditional;single agent", "pdf_keywords": ""}, "035595ebf6821031a543ee1c30386a6230fc7a41": {"ta_keywords": "speaker diarization algorithm;speaker diarization;novel speaker diarization;speaker notation algorithm;speaker tracing;experiments speaker tracing;speaker tracing buffer;buffer speaker notation;outputs experiments speaker;tracing buffer speaker;buffer speaker;speaker notation;diarization algorithm based;diarization outputs;consistent diarization outputs;diarization algorithm;speaker;diarization outputs buffer;novel speaker;supervised self attention;diarization;experiments speaker;consistent diarization;fully supervised self;ensures consistent diarization;attention mechanism eend;fully supervised;self attention mechanism;present novel speaker;supervised self", "pdf_keywords": "online speaker diarization;speaker diarization based;speaker diarization algorithm;speaker diarization directly;speaker diarization eend;speaker diarization;blind speaker diarization;end speaker diarization;based speaker tracing;online speaker tracing;compute speaker diarization;speaker tracing chunk;speaker diarization problem;based speaker chunk;proposed speaker tracing;speaker tracing method;speaker tracing;learning based speaker;speaker detection;speaker tracing technique;way compute speaker;speaker diarization error;using speaker tracing;propose speaker tracing;performance speaker tracing;novel speaker detection;speaker tracing buffer;speaker detection decoding;compute speaker;speaker chunk buffer"}, "8328508dc12c295165f997e02d74d00a42971c01": {"ta_keywords": "decoding semantic parser;semantic parser adapt;decoding semantic;semantic parser;13 context modeling;grammar based decoding;typical context modeling;based decoding semantic;parser;context modeling;parser adapt;context modeling methods;parser adapt typical;13 context;evaluate 13 context;grammar based;based decoding;decoding;adapt typical context;domain datasets;grammar;domain datasets best;cross domain datasets;context;semantic;present grammar based;typical context;datasets best;datasets significant improvements;datasets", "pdf_keywords": ""}, "d36e39aedd802aea4be1ea303c70dc56e97dbc3c": {"ta_keywords": "consistent summarization summary;factually consistent summarization;consistent summarization;summarization summary methodology;summarization summary;summarization;statistically sound summarization;summary consistent;summary consistent source;summary methodology based;summary methodology;answers summary consistent;summary methodology relies;sound summarization;summary receive similar;sound summarization summary;automatically evaluating factually;ask questions summary;questions summary receive;evaluating factually consistent;questions summary;answers summary;similar answers summary;evaluating factually;summary;summary receive;objectively understandable statistically;factually consistent;methodology automatically evaluating;automatically evaluating", "pdf_keywords": "generated summaries frequently;quality sentences generated;generated summaries;quality summaries summarization;summary generated text;model generated summaries;automatic generation factual;summaries model generated;summarization evaluate accuracy;high quality summaries;sentences based summarization;conditional text generation;quantifying quality sentences;summarization model;summarization model summarizationwe;generated text answers;text summarization evaluate;fluent topical summaries;extractive summarization model;quality summaries;topical summaries model;sentences generated text;summaries summarization dataset;generation factual information;summary generated;sentences generated;evaluating generated text;text summarization examining;summarization stable output;summarization dataset"}, "6c6975750207f787c318627ff7cb63a649165a8d": {"ta_keywords": "learn display representation;learn display;displays proposed learning;information learn display;learner integrates spatial;dimensional visual displays;intelligent tutoring agent;display representation experimental;visual displays;tutoring agent;intelligent tutoring;learning perceive dimensional;algorithm intelligent tutoring;learning perceive;visual displays proposed;display representation;tutoring;perceive dimensional visual;temporal information learn;grammar learner integrates;algorithm learning perceive;learner integrates;dimensional visual;user interface;tutoring agent integrating;statistical learning algorithm;statistical learning;user interface layout;novel statistical learning;proposed learning", "pdf_keywords": ""}, "1a671afdac8e7b759cf3b5ec7d03d485c76a989c": {"ta_keywords": "automatic speech recognition;speech recognition framework;automatic speech;end automatic speech;speech recognition;autoregressive end end;autoregressive end;recognition framework novel;autoregressive model novel;non autoregressive end;autoregressive model;recognition framework combines;non autoregressive model;recognition framework;autoregressive;novel encoder decoder;novel encoder;novel feature mask;novel non autoregressive;non autoregressive;encoder;feature mask novel;encoder decoder;decoder;decoder encoder;novel model trained;encoder used;end automatic;feature mask;decoder encoder used", "pdf_keywords": "autoregressive speech recognition;autoregressive model speech;model speech recognition;speech recognition asr;end speech recognition;speech models goal;predicting mask encoder;speech models;autoregressive speech;automatic speech;model speech;speech recognition framework;non autoregressive speech;automatic speech recognition;autoregressive encoder decoder;speech recognition;speech recognition objective;recognition asr encoder;end automatic speech;class speech models;speech recognition help;model mask predicting;mask predicting end;autoregressive encoder;decoder model conditional;problem speech recognition;token length speech;non autoregressive encoder;predicting masked tokens;autoregressive model masking"}, "38ff6cf441050a1db10df85ac0771ccc88dea748": {"ta_keywords": "peer review;peer review process;framework peer review;reviews guarantees strategyproofness;authorship conflict graph;review process guarantees;reviews guarantees;framework reviews guarantees;review process;theoretical framework peer;peer;authorship conflict;conflict graph satisfied;strategyproofness natural efficiency;conflict graph;guarantees strategyproofness natural;review;process guarantees strategyproofness;strategyproofness natural;flexible framework reviews;guarantees strategyproofness;framework peer;property authorship conflict;strategyproofness;authorship;reviews;unanimity empirically;framework reviews;unanimity empirically requisite;submissions data demonstrate", "pdf_keywords": "efficient peer review;peer review algorithm;efficiency peer review;strategyproof peer review;peer review organized;reviewer assignment aggregation;competitive peer review;peer review effective;peer review;conference peer review;assigning peer reviewed;reviewer assignments aggregation;reviewers papers algorithm;peer review reviewer;peer review data;peer review framework;review algorithm;peer review process;peer review validate;algorithm assigns reviewers;peer review present;peer review states;evaluation strategyproof peer;review algorithm assigns;score reviewers papers;review algorithm guaranteed;peer review faces;algorithm reviewer assignment;ranking papers algorithm;score reviewers"}, "97ca917f66d60f5277651a74f233804b03cb5e3d": {"ta_keywords": "estimate morphological segmentation;morphological segmentation;segmentation given morphological;morphological segment;morphological segmentation given;morphological segment using;given morphological segment;estimate morphological;morphologically distinct segments;method estimate morphological;morphological;able estimate morphological;given morphological;segmentation types morphologies;segment using deep;morphologically;segmentation;segmentation types;examples segmentation;morphologically distinct;method examples segmentation;estimate number morphologically;segmentation given;examples segmentation types;segmentation given segment;convolutional neural networks;segments;segment short time;morphologies;convolutional neural", "pdf_keywords": ""}, "49989dc4d77b9df775b284ab7682ba76c080be12": {"ta_keywords": "classifying spatiotemporally structured;classification spatiotemporally structured;classifying spatiotemporally;spatiotemporal classification spatiotemporally;classification spatiotemporally;spatiotemporal classification;spatiotemporally structured objects;approach spatiotemporal classification;spatiotemporally structured;problem classifying spatiotemporally;spatiotemporal entropy proposed;spatiotemporal entropy determined;spatiotemporal entropy;distribution spatiotemporal entropy;spatiotemporal;approach spatiotemporal;spatiotemporally;novel approach spatiotemporal;distribution spatiotemporal;underlying distribution spatiotemporal;classifying;classification;structured objects;object underlying distribution;structured objects using;hidden model approach;objects using hidden;hidden model;intrinsic structure object;structure object underlying", "pdf_keywords": ""}, "51d735419392dbe961c60bff7eee95388b8d6d3d": {"ta_keywords": "dependencies supervised grammar;labeled dependencies supervised;induction labeled dependencies;dependencies supervised;unsupervised labeled dependencies;supervised grammar;supervised grammar method;labeled dependencies;significant labeled dependencies;induction unsupervised labeled;unsupervised labeled;method induction labeled;supervised;grammar method based;based induction unsupervised;induction labeled;grammar method;statistically significant labeled;grammar method applied;labeled;grammar;dependencies;novel method induction;induction unsupervised;method based induction;based induction;significant labeled;method induction;selecting statistically;induction", "pdf_keywords": ""}, "b26ca2bb882c2d3526fb4ac7f544fb87c39ded62": {"ta_keywords": "gradient matching pursuit;matching pursuit method;kernel gradient matching;matching pursuit;pursuit method introduces;image datasets speech;gradient matching;pursuit method;improved kernel gradient;handwritten image datasets;orthogonality constraints;introduces orthogonality constraints;recognition experiments based;orthogonality constraints obtained;recognition experiments;kernel gradient;method introduces orthogonality;propose improved kernel;basis vector set;constraints obtained basis;recognition;method conducting recognition;improved kernel;speech datasets;conducting recognition;conducting recognition experiments;datasets speech;handwritten image;datasets speech datasets;based handwritten image", "pdf_keywords": ""}, "2cd7c3ed5a06c461b259694376820dcfcfbe94a9": {"ta_keywords": "effective inference generative;inference generative neural;generative neural models;generative neural model;generative neural;inference generative;neural models improves;generative;effective inference;neural models;accuracy search space;neural model choniak;problem effective inference;improves accuracy search;models improves accuracy;approach effective inference;significantly search space;models improves;neural model;neural;accuracy search;search space;pruning function effective;inference;pruning;models;using pruning;space using pruning;effective improving accuracy;model choniak charniak", "pdf_keywords": "search generative parsers;generative parser trained;neural generative parsers;parser trained generative;generative parsers;generative parsers achieves;trained treebank generative;generative parser;generative parsers based;parsers based generative;treebank generative;treebank generative model;parser trained treebank;beam search generative;parser trained;constituent parser trained;method generative parser;search neural generative;trained treebank;beam search neural;search generative;model constituent parser;treebank surpassing prior;model parsers;search fixed parser;single model parsers;parsers achieves;parsers;constituent parsing computational;treebank surpassing"}, "19a3af37df22c7c646cc99efad3af96cda6e80f0": {"ta_keywords": "multimodal translation task;shared multimodal translation;translation task wmt17;multimodal translation;translation task;wmt17 task based;present shared multimodal;task based nihon;wmt17 task;multimodal;shared multimodal;task wmt17;task wmt17 task;nikhef based nihon;wmt17;translation;nihon nikhef based;task based;task;based nihon present;based nihon nikhef;nihon present;based nihon;nihon;nihon nikhef;integrates nihon nihon;nihon nihon;integrates nihon;nikhef based;present integrates nihon", "pdf_keywords": ""}, "51546584aa394d159edcc08f2412ae30dd316f6c": {"ta_keywords": "effective prediction depth;prediction depth;input effective prediction;inside deep models;prediction depth study;effective prediction;accuracy prediction demonstrate;difficulty making prediction;deep models;accuracy prediction;measure accuracy prediction;making prediction;prediction;prediction given input;prediction demonstrate;prediction demonstrate relationship;measure computational difficulty;difficult examples interpretable;making prediction given;problem categorize difficult;deep models showcase;categorize difficult;deep;interpretable groups;computational difficulty;interpretable;categorize difficult examples;examples interpretable;complexity;depth", "pdf_keywords": "learned prediction depth;prediction depths;prediction prediction depth;prediction depth;effective prediction depth;smaller prediction depths;difficulty prediction depth;predicting deep;predictions deep;predictions deep learning;lower prediction depth;deep prediction;prediction depth training;prediction model deep;training deep prediction;prediction depth typically;prediction depth empirical;difficulty called prediction;prediction depth demonstrate;deep network prediction;methodology predicting deep;prediction depth consistent;predicting deep models;using prediction depth;computation depth predictions;considering prediction depth;according prediction depth;prediction depth larger;prediction depth used;example difficulty prediction"}, "ca73cc17ca69fa0807e566c22c7c1711da916281": {"ta_keywords": "nearest neighbor search;approximate nearest neighbor;nearest neighbor;neighbor search;search high dimensional;analysis approximate nearest;approximate nearest;neighbor search high;performance exact search;nearest;exact search method;search method small;exact search;dimensional spaces compare;search method;search high;high dimensional spaces;empirical analysis approximate;small world approach;spaces compare performance;metric nonmetric;metric nonmetric spaces;neighbor;small world;method small world;effectiveness metric nonmetric;search;nonmetric spaces;high dimensional;popular small world", "pdf_keywords": ""}, "56501a3441c2074bbbbe31015d6d41c57d9d285b": {"ta_keywords": "paraphrastic sentence representations;paraphrastic sentence models;training paraphrastic;training paraphrastic sentence;state art paraphrastic;sentence representations variety;code training paraphrastic;sentence representations;sentence models;sentence models desired;paraphrastic;paraphrastic sentence;inference training;inference training desired;models desired language;use inference training;art paraphrastic sentence;desired language parallel;art paraphrastic;training desired language;language parallel data;language parallel;language ease use;desired language ease;desired language;ease use inference;inference;use inference;languages achieving;languages", "pdf_keywords": "embeddings language paraphrase;paraphrastic sentence representations;paraphrastic sentence embeddings;embeddings accuracy paraphrastic;paraphrastic sentence embedding;lingual paraphrase data;trained semantic similarity;sentence embeddings trained;predicting similarity sentences;predicting semantic similarity;learning paraphrastic;learning paraphrastic sentence;lingual semantic similarity;unsupervised semantic similarity;sentence embedding models;sentence embedding learning;trained sentence embedding;users train paraphrastic;models learning paraphrastic;semantic similarity tasks;embeddings trained semantic;semantic similarity cross;semantic similarity sentences;similarity sentences language;semantic similarity significantly;learning inference paraphrastic;sentence embeddings accuracy;similarity cross lingual;english semantic similarity;sentence similarity"}, "ce458be308f2c75edc53366272fa6e744fda7902": {"ta_keywords": "sense disambiguation unsupervised;word sense disambiguation;unsupervised word sense;disambiguation unsupervised way;word sense similarity;sense disambiguation;sense disambiguation called;disambiguation unsupervised;sense similarity;sense similarity input;unsupervised word;disambiguation;perform word sense;disambiguation called mnogoznal;new unsupervised word;disambiguation called;mnogoznal unsupervised combines;features word sense;mnogoznal unsupervised;word sense;called mnogoznal unsupervised;unsupervised combines best;similarity input;unsupervised combines;unsupervised way;target word ability;input target word;similarity;word ability;datasets n_ boson", "pdf_keywords": ""}, "9195186cf44876d0d1d03b87756c464b760a7f4e": {"ta_keywords": "results comprehensive research;comprehensive research;results comprehensive;present results comprehensive;comprehensive;research;present results;results;present", "pdf_keywords": "modulation based search;segmentation voice activity;voice activity;search metaterm patterns;search short metaterm;based search metaterm;search metaterm;metaterm patterns search;end speech recognition;audio segmentation voice;speech recognition;voice activity detection;patterns search metaterm;algorithm search metaterm;audio segmentation;segmentation voice;segmentation audio;audio segmentation performance;improves segmentation audio;neural network search;toolkit speech;short metaterm patterns;segmentation audio segmentation;attention encoder;multi decoder;decoder;multi decoder architecture;metaterm patterns;long context modeling;beam search approach"}, "4fd6488e38043d680c592170bf7f651c079d0e98": {"ta_keywords": "cell networks static;small cell networks;cell networks;macro base stations;base stations mobile;micro base stations;networks static users;base stations network;networks static;network effectively optimized;stations mobile users;network architecture small;stations network effectively;users network effectively;stations mobile;novel network architecture;architecture small cell;network architecture;users network;proposed network;proposed network structure;base stations;network effectively;stations network;performance proposed network;moving users network;network;base stations having;served macro base;mobile users served", "pdf_keywords": "heterogeneous cellular networks;network model stochastic;small cell networks;lte cellular networks;small cell network;networks high mobility;cell network;cell network macro;cellular networks;network models;cellular networks proposed;handoff traffic network;heterogeneous network model;cell networks;lte cellular;networks combat handoff;network models demonstrate;network network model;network model;network parameters stochasticity;handoff traffic;new network model;novel network model;cell networks high;different network models;model downlink mobile;network heterogeneous users;network model based;operating network model;network network modeled"}, "4ab7b65e1a3b76eb3db064523c862f1325e04971": {"ta_keywords": "recognize individuals parkinson;corpus people parkinson;rate speakers parkinson;different speech recognition;parkinson disease systems;speakers parkinson;speech recognition;individuals parkinson disease;individuals parkinson;speech recognition systems;people parkinson results;parkinson disease;parkinson;parkinson results;performance different speech;people parkinson;parkinson results error;speakers parkinson higher;parkinson higher;parkinson higher control;trained corpus people;systems trained corpus;trained corpus;recognition systems used;systems used recognize;speech;corpus;different speech;disease systems trained;recognition systems", "pdf_keywords": ""}, "3f79b71b887d2ccb733926867a62f69902fcbdab": {"ta_keywords": "adaptive ontology mapping;similarity ontologies vector;structural similarity ontologies;similarity ontologies;adaptive ontology;ontology mapping;generic adaptive ontology;ontologies vector;ontologies vector space;ontology mapping approach;ontologies;ontology;based constraint satisfaction;constraint satisfaction solver;linguistic structural similarity;mapping approach called;structural similarity;mapping approach;similarity finally improve;improve mapping;constraint satisfaction;mapping;satisfaction solver;satisfaction solver activated;similarity finally;similarity;adaptive method based;mapping accuracy interactive;based constraint;generic adaptive", "pdf_keywords": ""}, "7954b31ce1f6ad935808b7cf62c34bc118d20a9a": {"ta_keywords": "heterogeneity expected outcome;heterogeneity expected;recover correct heterogeneity;regions heterogeneity expected;identifying regions heterogeneity;expected outcome decision;heterogeneity;correct heterogeneity region;correct heterogeneity;regions heterogeneity;outcome decision;heterogeneity region;finding decision;outcome decision process;heterogeneity region semi;finding decision maker;decision maker high;decision maker;decision process;expected outcome;likelihood finding decision;decision process based;high variance assumed;method identifying regions;variance assumed context;maker high variance;identifying regions;high variance;healthcare datasets;semi synthetic experiment", "pdf_keywords": "measure heterogeneity decision;heterogeneity decision group;heterogeneity decision;regions heterogeneity decision;heterogeneity statistically;based causal inference;heterogeneity decision making;heterogeneously generated causal;sample based causal;causal inference framework;identifying heterogeneity;causal inference;formalizing heterogeneity;data sets heterogeneity;sets heterogeneity statistically;data heterogeneity;heterogeneity based;heterogeneity based approach;decision makers heterogeneous;sample heterogeneity statistically;heterogeneity sample based;method identifying heterogeneity;problem causal inference;identifying heterogeneity sample;use heterogeneity based;causal effect decision;preferences formalizing heterogeneity;region heterogeneity statistically;identifying regions heterogeneity;heterogeneity"}, "4bf5084d21f681c09409bd890daa4bf1c4f9b691": {"ta_keywords": "treatment platelet reactivity;platelet reactivity higher;platelet reactivity occurrence;platelet reactivity;high treatment platelet;treatment platelet;patients average platelet;periprocedural myocardial infarction;platelet count;average platelet count;platelet count statistical;average platelet;platelet;myocardial infarction sample;myocardial infarction;infarction occurrence periprocedural;myocardial infarction occurrence;myocardial infarction higher;events periprocedural myocardial;occurrence periprocedural myocardial;periprocedural myocardial;infarction higher frequent;infarction occurrence;infarction sample;infarction sample 100;reactivity occurrence periprocedural;infarction higher;effect high treatment;infarction;reactivity occurrence", "pdf_keywords": ""}, "c3490ec9b8f695bed2187fb4a4164b1509389ca8": {"ta_keywords": "particle neural network;stochastic evolution particle;simulation stochastic evolution;evolution particle neural;particle neural;simulation stochastic;stochastic evolution;method simulation stochastic;neural network method;evolution particle;use neural network;stochastic;neural network;applied neural network;neural network fixed;particle;method applied neural;simulation;nodes method;nodes method applied;use neural;new method simulation;applied neural;network method;method simulation;network method based;nodes;number nodes method;neural;number nodes", "pdf_keywords": ""}, "7d94d4c6b2db490e08beabd2661df009f1a06d6c": {"ta_keywords": "world data synsets;data synsets;data synsets organized;open source synsets;crowdsourcing project;results crowdsourcing project;results crowdsourcing;crowdsourcing;reading world data;source synsets;source synsets task;present results crowdsourcing;world data project;crowdsourcing project create;world data;synsets organized entries;synsets organized;synsets provide;collection synsets provide;synsets;synsets task;project collection synsets;collection synsets;linguistic topological information;large open source;synsets task reading;data project;synsets provide necessary;open source database;open source", "pdf_keywords": ""}, "02a757548da783d43ffcfd4b60f2cbb0ac71a4bc": {"ta_keywords": "subjective fairness elicitation;elicitation individual fairness;subjective fairness using;fairness elicitation;study subjective fairness;fairness elicitation individual;subjective fairness;human subject fairness;subject fairness constraints;framework subjective fairness;fairness constraints elicited;fairness using human;individual fairness indirectly;fairness indirectly specified;fairness constraints;individual fairness;fairness using;accuracy fairness report;fairness indirectly;fairness constraints prove;fairness report preliminary;subject fairness;accuracy fairness;theorems accuracy fairness;fairness report;error subject fairness;fairness;individuals treated equally;treated equally task;treated equally", "pdf_keywords": "learning fairness constraints;algorithm learning fairness;fairness constrained learning;elicited fairness constraints;empirical fairness constraints;constrained empirical fairness;learning fairness;algorithmic fairness;fairness constraints data;fairness constraints fairness;notion algorithmic fairness;fairness constraints elicited;assigning fairness training;fairness constraints construct;algorithmic framework fairness;algorithmic fairness captured;fairness fairness preserving;fairness constraints fair;learning subjective fairness;deriving fairness constraints;fairness constraints;fairness constrained;fairness preserving;set fairness constraints;fairness elicitation;construct fairness;assigning fairness;empirical fairness;constraints construct fairness;framework fairness elicitation"}, "f7247fefc9efb57ace33425a2981d6aba08da3b7": {"ta_keywords": "statistical dialogue management;statistical dialogue framework;dialogue management using;dialogue management;dialogue process;simulates dialogue process;based statistical dialogue;dialogue management present;dialogue framework;method statistical dialogue;optimization dialogue management;statistical dialogue;dialogue behavior;intention dependency graph;simulation dialogue behavior;dialogue behavior useful;simulates dialogue;dialogue framework idg;model simulates dialogue;simulation dialogue;directed intention dependency;dialogue;using directed intention;framework simulation dialogue;decision process prdm;optimization dialogue;intention dependency;dependency graph;useful optimization dialogue;observable decision process", "pdf_keywords": ""}, "23e42bc79f10234bdceef31441be39a2d9d2a9a0": {"ta_keywords": "knowledge base reasoning;rules knowledge base;logical rules knowledge;knowledge base;rules knowledge;differentiable rules knowledge;base reasoning approach;learning differentiable rules;base reasoning;logical rules;reasoning approach outperforms;approach learn structure;reasoning approach combines;order logical rules;learn structure;reasoning approach;knowledge;learning problem use;approach learning differentiable;existing approaches discovering;approaches discovering;learn structure order;structure order logical;approach learning;differentiable rules;novel approach learning;approach learn;reasoning;use approach learn;discovery new feature", "pdf_keywords": ""}, "06064617f152f5032137204aec739c0c82dbb836": {"ta_keywords": "chime speech separation;speech separation recognition;separation recognition challenge;speech separation;separation recognition;chime speech;task separating tracks;second chime speech;approach second chime;second chime;separating tracks;chime;recognition challenge;recognition challenge challenge;separating tracks proposed;task separating;dataset novel adaptive;challenge challenge combines;tasks baselines challenge;separating;challenge combines;recognition;tracks;separation;dataset novel;approach task separating;tracks proposed approach;baselines challenge designed;baselines challenge;dataset", "pdf_keywords": ""}, "14047a24b23d9e392776229f9d40bee9f8243e4c": {"ta_keywords": "processes active sensors;tracking processes active;active sensors;sensor networks;active sensors proposed;sensors proposed algorithm;tracking processes;sensor networks proposed;connected sensor networks;continuous connected sensor;connected sensor;tracking;sensor;sensors proposed;sensors;artfully tracking processes;method artfully tracking;processes active;continuous time discrete;time discrete continuous;artfully tracking;networks proposed algorithm;continuous time;problem continuous time;networks proposed;accuracy algorithm;discrete continuous;accuracy data representation;proposed algorithm formulated;algorithm formulated", "pdf_keywords": ""}, "0dd1b9ad5aeda250dc61f38cf7018e7a014e91c0": {"ta_keywords": "harmonic oscillator field;dimensional harmonic oscillator;dynamics dimensional harmonic;gaussian field dynamics;field dimensional harmonic;strength harmonic field;harmonic field;harmonic field dimensional;harmonic oscillator exhibits;harmonic oscillator;oscillator field;oscillator field tuned;dimensional harmonic;dimensional gaussian field;gaussian field;varying strength harmonic;effect dimensional gaussian;dependence strength harmonic;dimensional gaussian;oscillator exhibits power;field dynamics dimensional;strength harmonic;field dynamics;harmonic field agreement;oscillator exhibits;gaussian;harmonic;field tuned large;oscillator;dynamics dimensional", "pdf_keywords": ""}, "a67face220a88b6b36f3343a6a017a3536562d5b": {"ta_keywords": "famous guessing game;guessing game defined;game guessing game;brain able predict;search patterns brain;game guessing;guessing game;game accuracy;game seen accuracy;guessing game guessing;accuracy agent;patterns brain;patterns brain brain;accuracy agent improved;probability game seen;seen accuracy agent;example famous guessing;shape game accuracy;brain brain brain;brain brain;probability game;brain use example;exploiting fact brain;defined probability game;game defined probability;predict shape game;brain brain use;brain;accuracy;fact brain able", "pdf_keywords": "trained guessing games;grounded representation neural;generation task trained;visual question answering;tasks learned representations;performing guessing games;representations dialogue agents;question generation task;question generation;learn representations dialogue;game encoder trained;experience learning guess;task trained agent;learning general grounded;using guessing games;question answering encoding;challenging question generation;model question generation;agent trained guessing;task trained;representations downstream tasks;question answering;task learning propagation;general grounded representation;question generation problem;encode semantic knowledge;trained guessing;representation neural;learning guess;propagation learned representations"}, "970383c0a41d7ae1ec4b8abaa3033778203377b9": {"ta_keywords": "neural networks stochastic;stochastic processes model;stochastic processes;stochastic input;processes given stochastic;stochastic input model;stochastic processes given;stochastic process;networks stochastic input;time stochastic processes;stochastic;discrete time stochastic;given stochastic process;time stochastic;given stochastic;networks stochastic;stochastic process demonstrate;prediction large number;simultaneous large scale;processes model;accurate prediction large;prediction large;neural networks;processes model built;model simultaneous large;accurate prediction;combination neural networks;processes given;combination neural;input model", "pdf_keywords": "automatically recognizing speech;automatic speech;recognizing speech noisy;models synthetic corpus;task decoding quizbowl;decoding quizbowl challenging;question answering;noise automatic speech;recognizing speech;decoding quizbowl;speech noisy data;000 noisy sentences;noisy sentences evaluate;synthetic corpus;automatic speech recognition;speech recognition;noisy sentences;speech recognition ar;synthetic corpus 500;using spoken data;spoken data;question answering question;word text learned;factoid question answering;speech noisy;human corpora quizbowl;corpora quizbowl;question answering qa;quizbowl challenging task;searching words noisy"}, "3193766c0439ff29a0a3d176628f8144d6e77231": {"ta_keywords": "xmath1 xmath2 xmath3;xmath2 xmath3 xmath4;xmath0 xmath1 xmath2;xmath16 xmath17 xmath18;xmath15 xmath16 xmath17;xmath9 xmath10 xmath11;xmath3 xmath4 xmath5;xmath1 xmath2;xmath10 xmath11 xmath12;xmath17 xmath18 xmath19;xmath14 xmath15 xmath16;xmath4 xmath5 xmath6;xmath8 xmath9 xmath10;xmath12 xmath13 xmath14;xmath16 xmath17;xmath13 xmath14 xmath15;xmath11 xmath12 xmath13;xmath7 xmath8 xmath9;xmath17 xmath18;xmath3 xmath4;xmath5 xmath6 xmath7;xmath6 xmath7 xmath8;behaviour xmath0 xmath1;xmath0 xmath1;xmath2 xmath3;xmath19 xmath20 xmath22;xmath10 xmath11;xmath15 xmath16;xmath18 xmath19 xmath20;xmath4 xmath5", "pdf_keywords": ""}, "b38ec68c8bab031138606a9b00e9d817be3e1d22": {"ta_keywords": "entity link modeling;jointly modeling links;models topic models;link modeling jointly;topic models;topic models improve;link modeling;modeling links text;induced topics data;topics data;modeling links;links text entities;text entities linked;entity link;entities linked;identify induced topics;induced topics;entity entity link;block models topic;model protein protein;model protein;modeling jointly modeling;jointly modeling;protein protein interaction;text entities;topics;models topic;protein interaction;modeling jointly;apply model protein", "pdf_keywords": ""}, "3f256b31d446015d8cd0f9f3996009cdf2034c5e": {"ta_keywords": "joint language identification;language independent network;speech recognition;connectionist temporal classification;identification speech recognition;attention connectionist temporal;architecture recognize speech;hybrid attention connectionist;language independent architecture;language identification speech;recognize speech;joint language;speech recognition model;language identification;attention connectionist;recognize speech 10;architecture joint language;temporal classification ska;temporal classification;based hybrid attention;multilingual benchmarks augment;language independent;identification speech;multilingual benchmarks;hybrid attention;performance multilingual;novel language independent;performance multilingual benchmarks;model language independent;connectionist temporal", "pdf_keywords": ""}, "c56aced0f0c5cfebefadb530cb08d736c3ac5c05": {"ta_keywords": "unstructured structured structured;propose retrieval augmented;structured structured structured;retrieval augmented framework;structured structured;unstructured structured;retrieval augmented;unstructured unstructured structured;propose retrieval;augmented framework unstructured;structured;framework unstructured unstructured;retrieval;framework unstructured;unstructured unstructured;unstructured;augmented framework;augmented;framework;propose", "pdf_keywords": "code summaries retrieval;code generation summarization;retriever code generator;generating code summary;supplement code generation;code code summaries;code summaries;code search proposed;code natural language;source code generation;code generator;generate retrieved code;datasets code generation;code search;generating code database;code summary large;code generation addressed;code generation;code generation using;code generate retrieved;generation source code;code generator redcoder;generative code generation;datasets code search;code source code;code database;unstructured source code;code summaries written;generation summarization java;retrieves relevant code"}, "a4ce6cd06bc73d81651f7888efa4337fd82a60f0": {"ta_keywords": "detection spoken;word detection spoken;detecting unknown words;unknown word detection;spoken systems method;words spoken systems;word detection;spoken systems;detecting unknown;unknown words spoken;method detecting unknown;aware user state;detecting;unknown words;novel method detecting;detection;aware user;method detecting;unknown word;user state;fact aware user;user state method;user state uses;uses fact aware;problem unknown word;systems method;state method based;use fact aware;words spoken;systems", "pdf_keywords": ""}, "04b364d56995de2228cb1acfb320a935cbcf4440": {"ta_keywords": "weakly supervised segmentation;weakly supervised;approach weakly supervised;robust trust region;supervised segmentation;robust regularized;robust regularized loss;supported robust regularized;supervised segmentation improves;trust region approach;robust trust;quality supervised;quality supervised training;segmentation improves quality;trust region;segmentation;propose robust trust;supervised training;supervised;segmentation improves;region approach weakly;improves quality supervised;robustly sense;supervised training approximately;approach supported robust;regularized loss;robust;regularized loss model;propose robust;supported robust", "pdf_keywords": "weakly supervised segmentation;supervised segmentation;supervised segmentation achieving;segmentation based regularization;low level segmentation;supervised segmentation combines;segmentation combines low;supervised segmentation showwe;weakly supervised;semantic segmentation results;approach supervised segmentation;segmentation achieving state;segmentation weights;semantic segmentation;supervised segmentation approach;training supervised learning;efficiently segmentation weights;segmentation domains;semantic image segmentation;approach weakly supervised;segmentation arbitrary;learning algorithm segmentation;segmentation;training supervised;segmentation approach generalizes;segmentation achieving;segmentation weights proposed;context deep learning;presenting semantic segmentation;segmentation showwe propose"}, "fa774368fcf51cc0fa1bfda59b6a606e163c64b1": {"ta_keywords": "symmetric linear models;model partially symmetric;symmetric linear model;partially symmetric linear;construction partially symmetric;symmetric partially asymmetric;finite dimensional systems;symmetric linear;synthesis partially symmetric;partially symmetric;asymmetric linear inequalities;symmetric partially;linear combination symmetric;symmetric linear linear;partially asymmetric linear;linear inequalities construction;linear models finite;linear model partially;models finite dimensional;inequalities linear inequalities;combination symmetric partially;symmetric;dimensional systems;dimensional systems approach;linear inequalities used;inequalities construction partially;linear inequalities linear;linear inequalities;asymmetric linear;partially asymmetric", "pdf_keywords": "symmetric counting constraints;state counting constraints;symmetrical counting constraints;stochastic switching protocol;constraint generalize counting;counting constraint allows;controllers satisfy counting;stochastic switching;counting constraint generalize;dynamics stochastic switching;counting constraints proposed;constraints called counting;counting constraints construct;switching arbitrary state;counting constraints particular;counting constraints consider;counting constraints;counting constraint;randomized switching arbitrary;satisfy counting constraints;counting constraints demonstrate;generalization counting constraint;global counting constraints;given counting constraint;switching protocol finite;conditions randomized switching;discrete state count;symmetric counting;counting discrete state;state counting discrete"}, "9abf14d4f89bf6c297e1bbd637cd54e1a0335e71": {"ta_keywords": "predicts attribution bibliotor;bibliotor attributions predict;bibliotor attributions;attribution bibliotor attributions;kinds bibliotor attributions;attribution bibliotor;bibliotor attributions edition;attributions predict;bibliotor;attributions edition;predicts attribution;able predict attribution;attributions;different kinds bibliotor;predict attribution;kinds bibliotor;model predicts attribution;attributions edition classic;attribution;predict attribution 87;attribution 87 total;attribution 87;novel unsupervised model;novel unsupervised;document number copies;number copies document;present novel unsupervised;number copies;copies restricted;copies document", "pdf_keywords": "bibliographer style;preferences bibliographers;compositor bibliographer style;bibliographers method;bibliographer style use;preferences bibliographers method;preferences compositor bibliographer;bibliographers original;bibliographers accuracy;compositor bibliographer;bibliographers;bibliographers long tradition;bibliographer;text output bibliographers;bibliographers inform;bibliographers inform analyses;judgements bibliographers;bibliographers method based;based bibliographers original;bibliographers used;bibliographers long;bibliographers original unconstrained;judgements bibliographers accuracy;manual judgements bibliographers;output bibliographers;bibliographers used towe;output bibliographers used;method based bibliographers;based bibliographers;used bibliographers inform"}, "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4": {"ta_keywords": "speeding text compression;text compression;compression method learns;text compression method;flow speeding text;speeding text;compression;compression method;speeding learning;learns soft prompts;prompts backpropagation effective;speeding learning process;effective speeding learning;backpropagation effective;soft prompts backpropagation;backpropagation effective speeding;text;prompts backpropagation;backpropagation;parameters performance;method learns soft;proc flow speeding;parameters performance method;method learns;learns soft;learns;soft prompts;learning;flow speeding;parameter proc", "pdf_keywords": "pretrained language models;adapting pretrained language;learns soft prompts;prompt models trained;adapting language models;soft prompt trained;trained large language;adapting language model;model prompt tuning;input text training;prompt tuning robust;language models downstream;adapting language;text training;language models span;large language model;datasets prompt tuning;language models;prompt tuning improve;prompt trained;text downstream tasks;pretrained language;model tuning prompt;language model perform;tuning pre trained;learning task sentinel;learned prompts approach;prompt tuning;text training span;learned prompts"}, "1a3fcb1e2a416cbc79a011f1a1916aa53f7a2a09": {"ta_keywords": "postures describing human;representation stick figures;stick figures abstractions;postures describing;body postures describing;describing human emotional;human emotional states;postures measuring;body postures;body postures measuring;stick figures;postures;human emotional;stick figure;representation stick;emotional states participants;meaning body postures;postures measuring fraction;effectiveness body postures;figures abstractions study;figures abstractions;stick figure configurations;emotion intent formalize;emotional states;emotion intent;evocative representation stick;number stick figure;configurations terms emotion;describing human;emotion", "pdf_keywords": ""}, "e63c9eb5b623baad0a7805e839e5d9fabad37fce": {"ta_keywords": "information based descriptions;detailed human information;human information;based descriptions;regenerating detailed human;regenerating detailed;human information based;descriptions;detailed human;information;method regenerating detailed;information based;regenerating;detailed;human;method regenerating;novel method regenerating;present novel;novel method;present novel method;novel;based;method;present", "pdf_keywords": ""}, "ab8174a1f1810c1122f90649276a552d2eb1ccd4": {"ta_keywords": "energy particle deep;bose einstein condensate;defined energy bose;particle deep state;energy bose einstein;energy bose;calculation energy particle;einstein condensate bec;einstein condensate;energy particle;condensate particles;condensate particles single;bec condensate particles;particle deep;bose einstein;use bose einstein;defined energy;calculation energy;deep state method;defined defined energy;condensate bec condensate;method calculation energy;bec condensate;condensate bec;particle;particles single defined;deep state;particles;particles single;condensate", "pdf_keywords": ""}, "7618c65685c98fa88526555ae3f62cd5645066ad": {"ta_keywords": "predicting relation entailment;relation entailment based;entailment based relation;relation entailment;entailment based;relation hierarchy existing;relation hierarchy;method predicting relation;based relation hierarchy;predicting relation;entailment;relation;based relation;hierarchy existing database;hierarchy existing;hierarchy;database;new method predicting;method predicting;predicting;existing database;propose new method;new method;method;based;propose new;existing;new;propose", "pdf_keywords": ""}, "6e78e32481218e9391a88e6d0e30c0062ae71bec": {"ta_keywords": "model gesture generation;gesture generation style;gesture generation;gesture style speaker;study gesture generation;unique gesture style;gesture style;new model gesture;model gesture;conditioning unique gesture;pose audio course;generation style transfer;stage learns mixture;learns mixture generative;style transfer;mix stage learns;mixture generative models;pose audio;gesture;unique gesture;dataset pose audio;style transfer proposed;style transfer called;learns mixture;generative models;stage learns;mixture generative;generative models allows;style speaker;style speaker introduce", "pdf_keywords": "generating speech gestures;speech gesture generation;gesture generation;gesture generation style;method gesture generation;gesture style audio;transfer generated gestures;generated gestures;gesture generate;speech gestures;gesture generation introduce;speech gestures speaker;models conditioned gesture;gestures speaker;conditioned gesture style;generated gestures new;gestures speaker annotator;speech gesture based;audio poses gesture;gesture animation generated;drives speech gesture;generate appropriate gesture;models common gesture;speech gesture;utterance gesture based;types gesture generate;problem gesture generation;gesture style;study gesture generation;speaker style generating"}, "f3bca263a92b69c6da872a9a3268f260ba43f690": {"ta_keywords": "language models rnn;discriminative method recurrent;recurrent neural network;neural network language;models rnn lm;language models;speech recognition;vocabulary continuous speech;continuous speech recognition;lm considering discriminative;models rnn;cross entropy;criterion cross entropy;speech recognition task;method recurrent neural;method improves rnn;rnn lm baseline;words discounted training;network language models;cross entropy ce;recurrent neural;discriminative criterion cross;improves rnn lm;recognized words;large vocabulary continuous;discriminative method;rnn lm;discriminative;proposes discriminative method;paper proposes discriminative", "pdf_keywords": ""}, "53880036fb85cc737103c480c613e1912c416010": {"ta_keywords": "semi structured learning;semi structured documents;structured learning framework;structured learning systems;structured learning;information semi structured;structured documents;structured documents framework;extract information semi;learning framework;learning framework used;semi structured;learning systems;construction semi structured;learning systems used;robust semi structured;documents framework based;framework construction semi;documents framework;extract information;master learning algorithm;information semi;learning algorithm;structured;construction semi;learning algorithm invokes;master learning;new framework construction;learning;framework construction", "pdf_keywords": ""}, "a2f4731258830c76af7e3bdb96c4488823219585": {"ta_keywords": "speech robustly noisy;blind source separation;target speech robustly;speech robustly;robust automatic speech;speech interferences reverberant;speech recognition asr;target speech interferences;sparseness based blind;speech recognition;robustly noisy reverberant;frequency masking separate;separate target speech;noise robust automatic;automatic speech recognition;speech interferences;noise robust;source separation;frequency masking;present noise robust;time frequency masking;automatic speech;noisy reverberant environments;asr using sparseness;source separation sss;robustly noisy;interferences reverberant environment;noisy reverberant;frequency masking used;based blind source", "pdf_keywords": ""}, "341c72f55a89572915aa476db7f525c2e0b60eba": {"ta_keywords": "cluster similarity indices;cluster similarity;based cluster similarity;characterization clustering algorithms;clustering algorithms;method characterization clustering;characterization clustering;clustering;similarity indices;algorithms based cluster;correlation coefficient similarity;clustering algorithms based;coefficient similarity measure;similarity indices use;cluster;similarity measure;similarity measure satisfies;based cluster;coefficient similarity;similarity;indices use arccosine;algorithms;use arccosine correlation;algorithms based;arccosine correlation;arccosine correlation coefficient;correlation;indices;indices use;correlation coefficient", "pdf_keywords": "cluster similarity indices;indices cluster similarity;cluster similarity index;validation cluster similarity;characterization cluster similarity;evaluate clustering algorithms;validate clustering;matters cluster similarity;used validate clustering;clustering prove complexity;comparing cluster similarity;index random clustering;indices cluster;cluster similarity;validate clustering process;characterization similarity clusters;index matters cluster;indices accuracy clusters;index improvement clustering;candidate clustering;underlying clustering;evaluate clustering;used evaluate clustering;clustering similarity distance;underlying clustering algorithm;cluster similarity present;similar clustering pair;biological evolution clustering;clustering similarity;similarity indices important"}, "143183584a8ebaad93490f4550295a9cb6cf9817": {"ta_keywords": "scalable probabilistic logics;implementation scalable probabilistic;scalable probabilistic;probabilistic logics;probabilistic logics solving;storage data representation;demo scalable probabilistic;practical data storage;pseudo trees provide;pseudo trees;pseudo trees_;pseudo trees_ use;form pseudo trees_;pseudo trees_ provide;data storage;data storage data;practical implementation scalable;probabilistic;implementation scalable;storage data;data representation;topic pseudo trees;problems pseudo trees_;data representation problems;logics solving practical;scalable;data representation form;storage;logics;logics solving problems", "pdf_keywords": ""}, "fc78af26fd7644867af1abb8fbf2c37b47ad8257": {"ta_keywords": "lingual embedding dictionaries;high fidelity lexicon;lexicon induction dictionaries;downstream lexicon induction;fidelity lexicon induction;cross lingual embedding;embedding dictionaries optimized;embedding dictionaries;lexicon induction;lingual embedding;lexicon induction zero;downstream lexicon;impact downstream lexicon;fidelity lexicon;induction dictionaries constructed;induction dictionaries;dictionaries optimized high;fidelity cross lingual;dictionaries optimized;dictionaries constructed;zero shot tagging;dictionaries;cross lingual;dictionaries constructed using;shot tagging performance;lingual;tagging performance;shot tagging;lexicon;standard evaluation dictionary", "pdf_keywords": "lingual word embeddings;lingual embedding baselines;multilingual refinement;embedding large multilingual;cross lingual embedding;multilingual refinement mwe;lingual embedding;learning cross lingual;bilingual word embeddings;word embeddings empirically;hub multilingual refinement;large multilingual corpora;translation embeddings efficiently;word embeddings useful;languages empirically;word embeddings;multilingual word pairs;multilingual corpora best;new dictionaries baseline;optimal word embedding;multilingual corpora;word embedding;lexicon induction performance;multilingual distant languages;translation embeddings;dictionaries low resource;tagging distant languages;represented languages empirically;linguistics useful machine;distant languages evaluation"}, "682660c7a014e806b924fdf1a2a3d999a9ac13cf": {"ta_keywords": "abstractive summarization source;abstractive summarization;method abstractive summarization;summarization source document;decoder approach summarization;document using neural;neural language generation;parser resulting abstract;summarization performance;summarization performance evaluated;generation standard abstract;approach summarization performance;abstract language;using neural language;abstract language represented;summarization source;standard abstract language;neural language;summarization;approach summarization;abstract words;resulting abstract;abstract words standard;set abstract words;standard parser;parser;language generation;words standard parser;using neural;standard abstract", "pdf_keywords": "summarization progress neural;generating summary text;summarization text annotator;summary text unstructured;abstractive summarization progress;summarization using abstract;summarization text;summarization seq2seq guided;abstractive summarization;summarization progress;abstractive summarization using;guidance improves summarization;approach summarization text;programming guided nlg;summary text;summary source document;document summary;summarization resultswe propose;source document summary;summarization seq2seq;improves summarization resultswe;generating summary;improves summarization;seq2seq summarization;document summary paper;probabilistic approach summarization;summarization using;seq2seq summarization seq2seq;summarization resultswe;guided nlg approach"}, "9d698e034d83eedc05237e629eaad1c0c4e5bbb9": {"ta_keywords": "learnable recursive clause;free recursive logic;recursive logic programs;learnability single recursive;learnable recursive;logic programs polynomial;consisting learnable recursive;nonrecursive clause learnable;recursive logic;clause learnable;recursive clause constant;learnable additional oracle;clause learnable additional;clause programs consisting;free recursive;clause programs;equivalence queries classes;recursive clause;programs consisting learnable;logic programs;single recursive;function free recursive;recursive constant depth;defined pac learnability;clause clause programs;equivalence queries;pac learnability single;programs polynomial time;single recursive constant;recursive", "pdf_keywords": "recursive logic programs;learning recursive logic;recursive clause learnable;learnable recursive clause;logic program learning;learnable clause programs;program learning logic;learning recursive programs;recursive programs learning;learnable recursive;learning logic program;recursive logic program;recursive logic;clause recursive programs;logic programs able;classes recursive logic;recursive clauses database;learning logic;logic programs polynomial;learning recursive clause;recursive program learned;logic programs;learn recursive clauses;recursive programs based;recursive programs;able learn recursive;consisting learnable recursive;recursive clause database;algorithms learn clause;class logic programs"}, "bfe6d67ed1c9119f91774e62fe0f4f328830526e": {"ta_keywords": "training neural conversation;neural conversation models;conversation models;neural conversation;conversation models proposed;conversation data;multi task learning;conversation data generated;hypothesis conversation data;speaker data approach;conversation;task learning;speaker data;task learning approach;training neural;speaker speaker data;mutually exclusive speakers;approach training neural;multi task;speaker;speaker speaker;speakers proposed approach;hypothesis conversation;proposed leverages speaker;learning approach training;based hypothesis conversation;neural;leverages speaker speaker;leverages speaker;learning approach", "pdf_keywords": "learns conversational models;neural conversation models;generated conversational models;conversational models;conversational models optimized;user conversational models;conversational models learned;conversation models;training neural conversation;conversational model learns;personalized conversational model;conversational data model;information conversational models;using conversational models;novel conversational model;conversational models using;conversational data;character conversational models;conversational model;model learns conversational;conversations generated conversational;conversational model retrieving;models using conversation;generated conversational;learns conversational;conversational personal data;conversational models approach;conversational model utilizes;conversational learning;personalized conversational"}, "ca879ec1c04b94de274954dfd09dddfde6cbb4f3": {"ta_keywords": "perceptual age singing;singer perceptual age;singing voice age;age singing voice;change singer perceptual;age singing;controlling perceptual age;voice age;singer perceptual;voice age singer;age singer measure;singer measure individuality;uses age singer;singing voice method;converts perceptual age;perceptual age;voice method;voice timbre experimental;age singer;singing voice;perceptual age having;voice timbre;voice method uses;age singer adopts;method controlling perceptual;method uses age;controlling perceptual;possible change singer;change singer;voice", "pdf_keywords": ""}, "bd6c708a535af588d90025a0e6cf17407bf65434": {"ta_keywords": "bag words model;bert based classifier;words model;model confidence bertarian;confidence bertarian based;based bag words;local explanation effective;words model observe;bag words;confidence bertarian;classifier local explanation;explanation given bert;classifier;given bert based;bertarian based bag;based classifier local;local explanation reduce;based classifier;bert based;classifier local;explanation effective;popular local explanation;words;ability local explanation;model confidence;local explanation;bertarian based;model confidence explanation;reduce model confidence;local explanation sufficient", "pdf_keywords": "crowdsourcing study deception;explanations participants;explanations participants performance;crowdsourcing study evaluate;participants understand models;explanations model;crowdsourcing study;explanations humans;human understanding models;learning model participants;crowdsourcing study compares;models natural language;effect explanations participants;crowdsourcing;present crowdsourcing study;present crowdsourcing;ofwe present crowdsourcing;based explanations offered;likely understand models;models participants;fake compares attributions;awe present crowdsourcing;participants improve model;conduct crowdsourcing;conduct crowdsourcing study;learning models;machine learning models;based explanations;compares attributions;explanations humans ability"}, "bf7481685e63b85ef2586de3f6098f1a5fbe0e2d": {"ta_keywords": "contaminants active sampling;contaminants surface water;organic contaminants surface;water active cartridges;analysis contaminants surface;collect contaminants active;analysis organic contaminants;contaminants surface;novel active sampler;contaminants active;surface water active;sampler analysis organic;active sampler suitable;organic contaminants;suitable analysis contaminants;analysis contaminants;active sampler;resulting active sampler;sampling mode cartridges;collect contaminants;sampler suitable;active sampler analysis;sampling resulting active;surface water;active sampling;used collect contaminants;surface water simple;water active;contaminants;sampler", "pdf_keywords": ""}, "72579f6ce4a413585445c4ef8c8c2fa63ea1b8bc": {"ta_keywords": "depth learning;perturbations deep learning;stochastic perturbations deep;learn optimal stochastic;depth learning able;learns optimal parameters;stochasticity offline gradient;learns optimal;gradient descent optimal;deep learning;gradient descent;optimal stochastic perturbations;deep learning problem;optimization problem learns;problem learns optimal;learn optimal;descent optimal sampling;optimal stochastic;using gradient descent;perturbations deep;descent optimal;learning able accurately;stochastic perturbations;problem learns;gradient based optimization;dubbed depth learning;learning problem;stochastic;sampling discovery stochasticity;learns", "pdf_keywords": "features machine learning;classifier sifting features;features trained;predict features;sifting features trained;machine learning preserving;machine learning classifier;discovering possible features;smile detection framework;classifier sifting;learning classifier;machine learning services;robust feature library;features input classifier;learning called feature;smile detection;feature space preserving;enhancing features data;classifier;inherent features machine;robust feature;control features trained;generated smile detection;sifts features;features data representation;preserving inherent features;learning preserving;features data reduce;machine learning ml;new classifier sifting"}, "80edd01d46228fac7ec0cd14aea1666253b28f4d": {"ta_keywords": "voting profiles;information voting profiles;voting profiles use;voting settings people;social voting settings;settings people vote;vote based ideas;vote based;people vote based;voting settings;social voting;information voting;truthfully information voting;people vote truthfully;vote truthfully information;used social voting;people vote;profiles use heuristics;voting;vote truthfully;ideas reflected preferences;heuristics information;heuristics used social;use heuristics information;preferences paper examine;use heuristics;heuristics;vote;effectiveness heuristics;reflected preferences", "pdf_keywords": "heuristics approval voting;voting process rational;approval voting environments;strategies approval voting;approval voting situations;voting heuristics;voter preferences computationally;voting environments uncertainty;voting heuristics used;participants choose vote;voters approval voting;voters use heuristics;choose vote strategically;human behavior voting;strategically voting;winner approval voting;vote strategically voting;approval voting people;voting social choice;voter preferences;study voting heuristics;approval voting;behavior voting;voting environments;votes affect heuristic;agents vote choose;experiment examines voting;approval voting social;approval voting process;vote selection"}, "1da3a9c194a01c0bff7b6ecda79db9d673810bee": {"ta_keywords": "character level transduction;transduction tasks grapheme;transformer neural;level transduction tasks;level transduction improves;transduction improves performance;transliteration introduce simple;conversion transliteration introduce;conversion transliteration;transduction tasks;phoneme conversion transliteration;level transduction based;transduction improves;transliteration;based transformer neural;transduction based;level transduction;transduction;transliteration introduce;transduction based transformer;storing transforming;transformer neural network;feature guided character;capable storing transforming;grapheme phoneme conversion;storing transforming data;transforming data large;phoneme conversion;transforming;transforming data", "pdf_keywords": "neural machine translation;recurrent models morphological;outperforms recurrent neural;recurrent baseline character;morphological inflection generation;machine translation model;predicting linguistic inflection;outperforms recurrent models;character level transduction;transformer outperforms recurrent;outperform recurrent models;task morphological inflection;machine translation able;model machine translation;models morphological inflection;transduction tasks morphological;outperforms strong recurrent;tasks morphological inflection;machine translation;model outperforms recurrent;outperform recurrent;outperforms recurrent;recurrent neural network;text based neural;translation model;transformer character level;recurrent neural;strong recurrent baseline;morphological inflection;recurrent models"}, "28e81f96eab94e99febcaaee00637825c8a3e664": {"ta_keywords": "machine learning simulator;accelerate machine learning;machine learning presence;interpreting data machine;data parallel;machine learning;machine learning method;data machine learning;use machine learning;data machine;interpret data;data parallel machine;machine algorithm;machine algorithm used;simulator able efficiently;interpreting data;efficiently process data;machine speed algorithm;possible interpret data;learning simulator able;efficiently accelerate machine;learning simulator;algorithm high;process data parallel;data;method interpreting data;parallel machine;algorithm high possible;speed algorithm high;machine speed", "pdf_keywords": ""}, "faf494d0aa25a17aa25930ffb4c750fa59c44849": {"ta_keywords": "speaker embedding reconstruction;speaker verification;speaker embedding;solving speaker verification;speaker embedding objective;speaker verification problem;speaker speaker embedding;performance speaker embedding;effectiveness speaker embedding;networks representation speaker;representation speaker speaker;encoder performance speaker;representation speaker;embedding reconstruction objective;embedding reconstruction;solving speaker;speaker speaker;effectiveness speaker;speaker;objective solving speaker;embedding objective improved;performance speaker;convolutional neural networks;investigate effectiveness speaker;improved using convolutional;embedding;embedding objective;neural networks representation;convolutional neural;using convolutional neural", "pdf_keywords": "speaker encoder learn;learn speaker embeddings;learned speaker embedding;speaker encoder task;encoder speaker verification;speaker embeddings sophisticated;sophisticated speaker embeddings;embedding robust phonetic;encoder task speaker;effectiveness speaker encoder;speaker embeddings significantly;novel speaker encoder;better speaker embeddings;speaker embedding deep;speaker verification tasks;models speaker embedding;speaker embeddings;speaker encoder;task speaker embedding;better speaker embedding;hypothesize speaker encoder;speaker embedding;reconstruct speaker embedding;speaker embedding proposed;speaker embedding robust;speaker encoder extracts;speaker embeddings spontaneous;encoder speaker;speaker speaker encoder;speaker embedding input"}, "7efb1788b5e0fa3b4d9932722286ba1753b42f91": {"ta_keywords": "driven state tracking;state tracking;performance state tracking;state tracking approach;description driven description;description data driven;description driven;driven description data;description process uses;description process;data driven state;effective description driven;state tracking impact;description data;driven description;meta description process;tracking approach relies;schemata define process;tracking;tracking approach;driven state;simple effective description;meta description;schemata define;effective description;zero shot transfer;uses notion schemata;notion schemata;notion schemata define;process uses notion", "pdf_keywords": "dialogue state tracking;tracking dialogue state;state tracking dialogue;dialogue model task;tracking dialogue;task oriented dialogue;dialogue tod systems;track dialogue essential;tasks convey semantics;conversations completion tasks;description driven dialogue;dialogue datasets;track dialogue;dialogue datasets efficient;oriented dialogue modeling;dialogue states based;dialogue modeling;oriented dialogue tod;dialogue modeling neural;approach track dialogue;data conversations completion;task inter linguistic;dialogue state;dialogue model;schema guided dialogue;dialogues underlying model;driven dialogue model;dialogue framework;task oriented task;semantics natural language"}, "9688671a573651955c26d710c12617de26715e78": {"ta_keywords": "codes distributed storage;bound repair bandwidth;miser codes distributed;regenerating codes achieve;bandwidth exact repair;regenerating codes;codes distributed;energy miser codes;repair bandwidth;repair bandwidth exact;codes achieve cut;class regenerating codes;interference repair principle;set bound repair;miser codes;principle interference repair;interference repair;distributed storage;bound repair;distributed storage achieve;codes explicit construction;codes achieve;storage achieve cut;repair systematic nodes;codes explicit;class codes explicit;minimum energy miser;presented class codes;repair principle presented;storage achieve", "pdf_keywords": "regenerating codes distributed;bandwidth bound repair;bound repair bandwidth;distributed storage codes;codes distributed storage;node repair bandwidth;repair bandwidth bound;bandwidth exact repair;repair code nodes;code distributed data;repair node distributed;network coding code;repair bandwidth;repair bandwidth presents;codes distributed;repair bandwidth equal;network coding;finding network coding;code nodes existence;repair bandwidth attained;repair bandwidth exact;regenerating codes achieving;nodes distributed storage;code distributed;network coding problems;distributed storage message;repair bandwidth established;designing regenerating codes;code nodes;distributed storage interference"}, "148efaba70165d9faef0dac28d5fa2538cfa662d": {"ta_keywords": "human ai collaborative;human ai collaboration;ai collaborative decision;human assisted decision;biases human assisted;effectiveness human ai;human ai;optimal human ai;ai collaborative;cognitive biases human;ai collaboration assumptions;assisted decision making;collaborative decision;ai collaboration;collaborative decision making;time allocation strategy;develop time allocation;time allocation;assisted decision;optimal human;biases human;achieves optimal human;cognitive biases;role cognitive biases;ai;second user experiment;user experiment;conduct user experiment;effectiveness human;time based", "pdf_keywords": "bias human ai;biases human ai;biasing decision maker;decision making biasing;cognitive biases decision;bias decision making;exploits cognitive biases;biases decision making;making biasing decision;bias inherent decision;biasing decision;biases inherent decision;task anchoring bias;cognitive biases human;cognitive biases bias;anchoring bias human;ai human decision;anchoring bias decision;model cognitive biases;biased ai;bias decision;biased ai assisted;reducing bias decision;ai assisted decision;overcoming cognitive biases;cognitive biases inherent;modeling biased ai;cognitive biases;problem biasing decision;biases decision"}, "a6219725a9ad2079536c091f02fda2d4da6d62ac": {"ta_keywords": "regenerating codes distributed;codes distributed storage;codes based regenerating;distributed storage systems;code regenerating node;distributed storage;regenerating codes;exact regenerating codes;regenerating codes provided;codes distributed;based regenerating code;regenerating code regenerating;code regenerating;scheme regenerating codes;regenerating code;regenerating node;regenerating node node;distributed mail servers;storage systems;storage systems introduced;distributed mail;simultaneous node failures;distributed;applications like distributed;like distributed mail;node failures uniqueness;systems introduced codes;mail servers;mail servers provided;storage", "pdf_keywords": ""}, "3c1001c04866647650216201feb54c927af3a05b": {"ta_keywords": "learning set clauses;clauses specific;clauses;clauses specific particular;set clauses specific;set clauses;particular context discusses;specific particular context;particular context;describes technique learning;technique learning;learning;context discusses;technique learning set;context;learning set;context discusses applications;describes technique;paper describes technique;technique;describes;paper describes;specific particular;discusses applications;applications;particular;specific;discusses applications technique;applications technique;discusses", "pdf_keywords": ""}, "60f0af1dbc2775a69f64e4351d969ac966659fb2": {"ta_keywords": "sparsity synonymy dictionaries;synonymy dictionaries performs;synonymy dictionaries;dictionaries;dictionaries performs;problem sparsity synonymy;sparsity synonymy;dictionaries performs preprocessing;similar synset clusters;extracted synsets;performance extracted synsets;merging similar synset;similar synset;datasets russian language;synset clusters;synsets;approaches datasets russian;synset;synonymy;synset clusters evaluate;datasets russian;merging similar;performs preprocessing graph;datasets;preprocessing graph;sparsity;approaches datasets;postprocessing merging similar;russian language;russian language discuss", "pdf_keywords": "synsets dictionary synonyms;discovery synsets dictionary;synsets dictionary;synsets dictionary method;induction synsets dictionary;synonymy dictionaries nodes;lexical semantics data;discovery synsets method;automatic induction synsets;word senses based;dictionary synonyms problem;improve synset induction;induction word senses;automatically induction synsets;synonymy dictionaries;similarity lexical semantics;coupled semantic similarity;synset induction method;incompleteness synonymy dictionaries;semantic similarity;dictionary synonyms;discovery synsets;method discovery synsets;synonymy dictionaries hamper;synsets approach based;similarity lexical;dictionaries synonymy dictionaries;induction disambiguation novel;dictionaries synonymy;lexicon additional data"}, "22616702da06431668022c649a017af9b333c530": {"ta_keywords": "advances automated fact;automated fact checking;claim truthfulness accuracy;automated fact;computational neuroscience conclude;checking claim truthfulness;truthfulness accuracy;fact checking claim;fact checking;automation natural language;truthfulness accuracy discuss;neuroscience conclude list;neuroscience conclude;natural language processing;evidence distinguishing;computational neuroscience;use evidence distinguishing;claim truthfulness;natural language;language processing;automation natural;development computational neuroscience;advances automated;recent advances automated;neuroscience;automated;checking claim;task formulations;evidence;computational", "pdf_keywords": "automated fact checking;approaches automated fact;fact checking commonly;politifact fact checking;research automated fact;automated fact;fact checking challenge;textual claims;fact checking discuss;fact checking claims;task fact checking;fact checking domain;natural language processing;fact checking;textual claims context;used fact checking;media fact checking;processing nlp survey;representations textual claims;processing nlp;language processing nlp;nlp survey research;fact checking area;nlp survey;checking area nlp;method fact checking;domain natural language;nlp unifying task;dothe fact checking;evidence context task"}, "6b7f2f30840b0d72484784a15b3be670868a9f95": {"ta_keywords": "interlingual latent embedding;transfer distant languages;cross lingual input;cross lingual transfer;lingual transfer distant;method cross lingual;lingual transfer;cross lingual;lingual input;speech tagging;distant languages utilizes;distant languages;tasks speech tagging;structured priors utilize;imperfect cross lingual;priors utilize labeled;latent embedding;interlingual latent;structured priors;new interlingual latent;lingual;utilizes structured priors;labeled source data;source data unlabeled;utilize labeled source;regularized log likelihood;syntactic tasks speech;latent embedding space;languages utilizes structured;unlabeled target data", "pdf_keywords": "crosslingual parsing target;supervised cross lingual;crosslingual parsing;crosslingual tagging;unsupervised cross lingual;lingual dependency parsing;cross lingual annotations;tasks crosslingual tagging;crosslingual word embedding;perform crosslingual parsing;language parser learns;method crosslingual tagging;target language parser;interlingual latent embedding;crosslingual tagging hidden;parsing distant languages;crosslingual tagging multivariate;parsing target languages;embeddings target languages;cross lingual tasks;distant languages treebanks;cross lingual dependency;languages based deep;language parser;lingual annotations;embeddings syntactic;using cross lingual;embedding given language;lingual annotations propose;method crosslingual transfer"}, "eebc1811c55c2e5e8b3b78d0b0382ad50f22e32a": {"ta_keywords": "fact verification contrastive;robust fact verification;build robust fact;verification contrastive evidence;succinct robust factual;fact verification model;contrastive evidence vitaminc;robust factual;robust factual distortion;fact verification;resource fact verification;evidence vitaminc;verification contrastive;supporting false claims;contrastive evidence;belief checking;support belief checking;robust fact;evidence vitaminc designed;belief checking anomaly;distinct facts simple;distinct facts;resource fact;facts simple succinct;supporting false;000 distinct facts;document checking examples;alternation supporting false;checking anomaly correction;checking examples feature", "pdf_keywords": "fact verification models;fact verification dataset;automated fact verification;fact verification tasks;leveraging factual;fact verification changes;fact verification enforcing;fact verification extraction;factual revisions wikipedia;method leveraging factual;dataset combines wikipedia;leveraging factual revisions;fact verification inference;wikipedia model trained;wikipedia supports claim;automated fact;synthetically constructed fact;constructed fact verification;semi automated fact;wikipedia revisions synthetically;fact verification;quality fact verification;automated factual;use automated factual;accurately predict semantics;factual revision content;dataset claims;automated factual flagging;scale fact verification;supplementary dataset annotated"}, "6dd1e4d97dbdb370a36c25f82a9a9baaa16c836c": {"ta_keywords": "ebola virus coiled;virus coiled coil;vesicular stomatitis virus;virus coiled;peptides corresponding coiled;stomatitis virus;coiled protein gp2;pseudotyped ebola virus;motif pseudotyped virus;stomatitis virus pseudotyped;virus pseudotyped ebola;mutant vesicular stomatitis;infectivity mutant vesicular;coiled terminal motif;single coiled protein;ebola virus;coiled protein;viral genome;coiled coil motif;corresponding coiled terminal;motif single coiled;viral genome examined;protein infectivity mutant;corresponding coiled;gp2 protein infectivity;vesicular stomatitis;motif gp2 protein;mutant vesicular;activity viral genome;coiled terminal", "pdf_keywords": ""}, "7e122cc1a62e2f30951e14b91811896e1866dd7c": {"ta_keywords": "generation music using;music using empirical;generation music;method generation music;music using;music;empirical loss convolutional;loss convolutional neural;convolutional neural network;loss convolutional;convolutional neural;using empirical loss;empirical loss;neural network;neural;novel method generation;convolutional;generation;using empirical;empirical;method generation;novel method;present novel method;loss;network;method;novel;present novel;present;using", "pdf_keywords": ""}, "c7af06170f3d81ab761873a4c1fe0af2736eb0a2": {"ta_keywords": "natural conversation analyze;process natural conversation;interaction automatic prediction;natural conversation;response interaction automatic;conversation analyze;emotional responses events;dialogue decide action;emotion emotional responses;conversation;responses events elicit;automatic prediction;dialogue decide;conversation analyze occurrences;events elicit emotional;interaction automatic;elicit emotional triggers;emotional responses;dialogue;automatic prediction performance;analyze occurrences emotion;social affective process;affective process natural;responses events;affective process;emotional triggers;action response interaction;emotion emotional;emotional triggers utilize;prediction", "pdf_keywords": ""}, "8dd3b88ac87372c9f4428029ac12288ff3405199": {"ta_keywords": "lipid variability neutrophil;variability neutrophil lymphocyte;blood lipid variability;neutrophil lymphocyte ratio;variability neutrophil;lipid variability;lymphocyte ratio nlr;lymphocyte ratio relationship;value neutrophil lymphocyte;lymphocyte ratio;neutrophil lymphocyte;cholesterol ldl risk;lipoprotein cholesterol high;low density lipoprotein;density lipoprotein cholesterol;ldl risk factors;high density lipoprotein;density lipoprotein;lipoprotein cholesterol low;lipoprotein cholesterol;lipoprotein cholesterol ldl;factors value neutrophil;relationship blood lipid;cholesterol high density;cholesterol ldl;cholesterol low density;blood lipid;lipoprotein;value neutrophil;ldl risk", "pdf_keywords": ""}, "36b6abfb32ea56208a2858b558acbdd001c965e9": {"ta_keywords": "translation data parallel;neural machine translation;performance machine translation;machine translation significantly;machine able translation;machine translation able;machine translation;translation significantly improved;translation data;able translation data;translation able translation;parallel infer linguistic;performance neural machine;data parallel;translation significantly;performance neural;parallel data fully;data fully parallel;translation able;parallel data;parallel performance;neural machine;data parallel infer;able translation;data parallel data;fully parallel performance;parallel performance machine;parallel machine;translation;parallel infer", "pdf_keywords": "neural machine translation;improve performance translation;translation generation;performance translation model;machine translation;machine translation opennmt;machine translation generation;processing translation task;memory translation based;translation task parallelized;memory translation;translation task shared;performs translation task;translation task parallel;translation generation wnmt;processing translation;language processing translation;machine translation nmt;translation based neural;task parallel translation;parallel translation task;translation task;translation model;shared memory translation;translation generation held;performance translation;task efficient neural;translation model examine;parallelized shared attention;parallel translation process"}, "47b6023808002dfde031c17b34dcb1b522d3b326": {"ta_keywords": "spectral density electron;density electron gas;dimensional electron gas;density dimensional electron;calculation spectral density;electron gas calculated;density electron;electron gas presence;ultracold atoms magnetic;electron gas;spectral density;field spectral density;magnetic field spectral;atoms magnetic field;spectral density dimensional;dimensional electron;calculation spectral;atoms magnetic;method calculation spectral;ultracold atoms;based calculation spectral;applied ultracold atoms;density dimensional;electron;strong magnetic field;magnetic field method;presence strong magnetic;spectral;field spectral;gas presence strong", "pdf_keywords": ""}, "743d1aae44a12fb37b743ec947fad41cba9831b8": {"ta_keywords": "abstractive summarization generation;summarization generation structured;conditional text structured;generation conditional text;summarization generation;structured meaning representations;abstractive summarization;text structured;text structured meaning;methods abstractive summarization;informativeness resulting text;generation structured meaning;conditional text;generation conditional;meaning representations improved;generation structured;summarization;meaning representations informativeness;computational pragmatics;explicit models distractors;techniques computational pragmatics;consider generation conditional;computational pragmatics performance;pragmatics performance;pragmatics performance existing;models distractors;pragmatics;text improved using;text;resulting text improved", "pdf_keywords": "abstractive summarization generation;summarization generation structured;summarization generation;generation text summarization;task abstractive summarization;abstractive summarization dataset;abstractive summarization improves;summarization informativeness generated;generation structured sentences;summarization improves performance;representations summarization informativeness;abstractive summarization;summarization improves;multisentence document summarization;meaning representations summarization;representations summarization;language generation structured;document summarization;summarization dataset;representations automatic generation;mail abstractive summarization;summarization informativeness;text summarization;attention models;summarization dataset systems;summarization successive;rational language generation;automatic generation text;summarization documentwe present;systems abstractive summarization"}, "ba5e3559a2d54bb0e8d7678c9905b4a77da63f71": {"ta_keywords": "incentive mechanism;simple incentive mechanism;incentivizes truthful responses;propose simple incentive;incentive mechanism incentivizes;mechanism incentivizes truthful;incentive property mechanism;platforms tasks incentive;simple incentive;incentive;incentive property;tasks incentive;truthful responses objective;biases responses;subjectivity observational biases;responses objective evaluations;objective evaluations commerce;responses objective;tasks incentive property;mechanism incentivizes;incentivizes truthful;evaluations commerce platforms;observational biases responses;mechanism strictly payoff;evaluations commerce;commerce platforms tasks;subjectivity;biases;observational biases;subjectivity observational", "pdf_keywords": "incentive mechanism truthful;mechanism incentive truthful;online platforms incentive;platforms incentive models;based incentive mechanisms;automatic fully incentive;known incentive mechanism;incentive truthful feedback;fully incentive based;incentive matching online;mechanism known incentive;incentive mechanisms;incentive agents incentive;incentive mechanism satisfying;incentive agents;incentive mechanism able;incentive truthful;incentive systems;novel incentive mechanism;based incentive mechanismswe;simple incentive mechanism;incentives agent truthful;incentive based incentive;incentive agents models;incentive mechanism;incentive agents able;incentives truthful behavior;mechanism incentive;incentive systems crucial;incentive mechanismswe"}, "52824fb6eb5d3b55fb6634c77dc80f5826964464": {"ta_keywords": "extracting specifications software;method extracting specifications;extracting specifications;programming specifications software;specifications software using;specifications software;datalog code implements;logic programming specifications;specifications software run;datalog code;programming specifications;applied datalog code;inductive logic programming;datalog;applied datalog;method applied datalog;code implements database;logic programming;real world software;method extracting;implements database views;implements database;machine learning;generating examples behavior;machine learning techniques;programming;software;learning techniques method;database views;software using machine", "pdf_keywords": ""}, "0cd693f1a1223f25e89c1f5efdedd7c3b7846691": {"ta_keywords": "data driven queuing;driven queuing network;density available parking;increasing density parking;density parking;driven queuing;changing density parking;traffic congestion;traffic congestion use;overall traffic congestion;queuing network;density parking resulting;queuing network develop;parking resulting traffic;traffic flow;parking determine cruising;cruising parking;cruising parking contributes;traffic;determine cruising parking;overall traffic;queuing;queuing network leverages;available parking determine;resulting traffic flow;parking determine;available parking;data density;data density available;parking", "pdf_keywords": "network parking queues;queue network;queue nodes network;parking queues driven;parking space queue;networks queueing;networks queueing processes;node queue network;parking queues;edge queues transportation;performance networks queueing;network edge queues;queue nodes;queue network apply;queues transportation;nodes queue;queues flexible parking;queues transportation city;performance network parking;nodes queue nodes;parking network;capacity queues tasks;queue capacity;finite capacity queues;search nodes queue;edge queues;parking network constructed;capacity queues;cruising parking network;capacity queues context"}, "1f0524971c20a06d745ab784689eb8833435fde1": {"ta_keywords": "commonalities multiple systems;multiple systems;shared task;systems discovery shared;discovery shared task;shared task problem;multiple systems best;shared task opened;systems discovery;commonalities multiple;characterizing commonalities multiple;systems;available systems discovery;discovery shared;characterizing commonalities;commonalities currently;state art shared;systems best;systems best performing;commonalities currently available;problem characterizing commonalities;available systems;commonalities;art shared task;exploration commonalities multiple;study commonalities currently;task problem characterizing;comparative study commonalities;shared;task", "pdf_keywords": "natural language inference;natural language processing;sentence report discovery;claims natural language;fact extraction verification;results fact extraction;fact extraction;selection natural language;sentence selection natural;generated textual claims;predicting sentences supported;natural language;novel fact extraction;predicting sentences;language inference highly;describing sentence report;sentence representations;automatically recognizing entailment;language inference;sentence selection;successful natural language;language inference compare;selecting sentences;relies predicting sentences;predicting sentences useful;sentence representations perform;sentences collection;sentences collection documents;fever fact extraction;sentence selection pipeline"}, "68731c68773b117250f04509103031109b222d27": {"ta_keywords": "framework distant supervision;entity relation phrases;sentences based local;relation phrases individual;segmenting entity relation;distant supervision experiments;unify segmenting entity;distant supervision design;individual sentences based;distant supervision;phrases individual sentences;sentences based;relation phrases;segmenting entity;unify segmenting;corpora;based local context;phrases individual;real world corpora;local context;sentences;supervision design joint;context signal;local context signal;global structural;framework distant;individual sentences;open called remine;called remine;world corpora", "pdf_keywords": ""}, "fd8e176087335355ff5e81821a616d15ec8d3346": {"ta_keywords": "labels based indirect;synthesizing training labels;training labels based;based indirect supervision;indirect supervision sources;unseen labels generalization;training labels;label relations;indirect supervision;label relations model;automatically synthesizing training;labels generalization;labels based;provided label relations;label spaces overcome;unseen labels;output label spaces;supervision sources;label spaces;leverage indirect supervision;image text classification;labels;output label;supervision sources provide;framework unseen labels;labels generalization bound;supervision sources different;label;user provided label;classification tasks", "pdf_keywords": "generated indirect supervision;leveraging label relations;annotation weak supervision;supervision sources labels;supervision training labels;weak indirect supervision;indirect supervision sources;indirect labeling;new indirect labeling;indirect labeling functions;labels based indirect;indirect supervision training;synthesizing weak supervision;leverage indirect labeling;label relations dependencies;probabilistic label relation;based indirect supervision;indirect supervision weak;learning resulting label;joint indirect supervision;labels generated indirect;weak supervision sources;supervision sources label;labels multiple supervision;manually labeled training;models label relations;label relations existing;label relations;label relations minimizing;weak supervision frameworks"}, "e34f9e9163b13de00707157feda6a8b853c5c82d": {"ta_keywords": "scalable approach deduplication;data deduplication;approach deduplication;reference data deduplication;deduplication eliminate deduplication;approach deduplication eliminate;deduplication;eliminate deduplication;data deduplication problem;eliminate deduplication problem;deduplication eliminate;deduplication problem solved;deduplication problem;deduplication problem completely;data clustered;data clustered combine;shared knowledge reference;combine shared knowledge;fact data clustered;reference data;clustered combine shared;knowledge reference data;expert based approach;clustered;clustered combine;shared knowledge;scalable approach;automatically combining;combining state crowd;combine shared", "pdf_keywords": ""}, "e1d35deec12d18e53ca97a3cf4071526ad47968d": {"ta_keywords": "pretrained language model;models learn linguistic;changes pretrained language;language model stable;language model;pretrained language;learn linguistic knowledge;learn linguistic;linguistic knowledge;model architecture training;structural changes pretrained;models learn;ability models learn;models significantly improve;linguistic;changes pretrained;architecture training;architecture training strategy;language;structural changes;changes resulting models;improve ability models;models;training strategy;structural changes resulting;models significantly;evaluate structural changes;model architecture;knowledge;scale structural changes", "pdf_keywords": ""}, "e2ebf18e0b88752bd3ff905d2fba74213dcd2c51": {"ta_keywords": "sounds generated electrolarynx;electrolarynx based statistical;patterns excitation sounds;control fundamental frequency;generated electrolarynx based;automatically control fundamental;excitation sounds generated;electrolarynx based;speech real time;excitation sounds;sounds generated;electrolaryngeal el speech;generated electrolarynx;fundamental frequency;frequency f0 patterns;fundamental frequency f0;electrolarynx;patterns excitation;automatically control;patterns produced electrolaryngeal;frequency;frequency f0;f0 patterns excitation;control fundamental;predict f0 patterns;real time experimental;method automatically control;statistical f0 prediction;predicts f0 patterns;real time method", "pdf_keywords": ""}, "595a79ca667258ca2a4f5e7775e95a0fb0a0f024": {"ta_keywords": "efficiency free games;stochastic gradient play;games based stochastic;strongly monotone game;monotone game method;optimization class games;monotone game game;monotone game;free games;stochastic gradient;gradient play;gradient play method;based stochastic gradient;equivalent stochastic gradient;free games based;game method;game method applied;unconstrained optimization;stochastic;improving efficiency free;efficiency free;unconstrained optimization class;applied unconstrained optimization;gradient play slightly;equivalent stochastic;optimization;game played power;play method;games;based stochastic", "pdf_keywords": "stochastic gradient play;player optimization known;single player optimization;player optimization;gradient update game;players optimal bound;learning stochastic gradient;players algorithm;game players algorithm;gradient play strongly;gradient play;player individual gradient;players optimal;unbiased estimate player;monotone games method;game iteration complexity;cost function game;players algorithm based;construct gradient estimate;stochastic gradient;prove perturbed game;equilibrium game players;arbitrary regularity game;optimal bound multiplayer;learning stochastic;estimate player;gradient estimate;model existence nash;strongly monotone games;cost performing gradient"}, "36f7827bc344f9c2198dcb29732c525c68dc637a": {"ta_keywords": "locations traveling game;traveling game;using traveling game;computationally tractable allocation;traveling game tsp;traveling game ts;tractable allocation techniques;ts cooperative game;cooperative game agents;tractable allocation;location single vehicle;cooperative game;game ts cooperative;locations traveling;allocation techniques;allocation;vehicle using traveling;cost serve location;correspond locations traveling;calculating cost serve;agents correspond locations;single vehicle;ts cooperative;game agents correspond;calculating cost;single vehicle using;traveling;using traveling;game tsp propose;serve location single", "pdf_keywords": "value traveling;optimal transportation;traveling salesperson game;value traveling problem;traveling salesman problem;approximating value traveling;optimal transportation model;cost distribution shapley;game optimal value;transportation model optimal;model optimal transportation;value traveling salesperson;vehicle routing game;calculating value traveling;tour cooperative game;traveling salesman;optimal value distributed;tour skewley value;length tour cooperative;value related optimal;possible value mean;skewley value generalization;optimal value;cost distribution optimal;game optimal;transport setting cost;related optimal value;value mean;cost allocation;cost allocation problems"}, "ead1e044d284f3deecd32c2d5cc89fe513195a0a": {"ta_keywords": "synonyms input graph;approach graph augmentation;augmentation based equivalence;graph augmentation;graph augmentation based;edges potential synonyms;synonymy relation;synonymy relation approach;transitive edges;synonyms input;potential synonyms input;missing transitive edges;equivalence property synonymy;synonyms;augmentation based;augmentation;property synonymy relation;input graph;synonymy;potential synonyms;addition missing transitive;graph;transitive edges potential;approach graph;equivalence;property synonymy;equivalence property;based equivalence property;transitive;input graph evaluate", "pdf_keywords": ""}, "7e63be5285e6596fbbc6c56bc89f7b6fd8bbe8c5": {"ta_keywords": "model debugging;diagnosing model errors;model debugging categorize;errors model debugging;explanations effective diagnosing;diagnosing model;model explanations effective;effective diagnosing model;debugging categorize textitbugs;model explanations;debugging categorize;hoc model explanations;debugging;diagnose mislabeled training;textitdata model test;categorize textitbugs;contamination diagnose mislabeled;effective diagnosing;examples data contamination;categorize textitbugs based;model errors model;diagnosing;diagnose mislabeled;source textitdata model;model errors;mislabeled training examples;bugs assess;data contamination diagnose;textitbugs based source;errors model", "pdf_keywords": "debugging models increasingly;debugging models;model bugs findings;effective identifying debugging;identifying model bugs;model debugging;diagnosing model bugs;methods model debugging;model debugging present;novel approach debugging;categorize model bugs;identifying debugging problems;explanations tools model;approach debugging;tools model debugging;identifying debugging;model debugging past;model bugs based;used model debugging;model debugging despite;explanations tools;effectiveness categorize bugs;bugs findings;increased use debugging;debugging;bugs findings serve;use debugging;defective models attributions;turning explanations tools;approach debugging early"}, "bd8922f8cc8284553dc9e6db529af309298451fe": {"ta_keywords": "training encoder decoders;training encoder;train attention decoder;attention decoder;decoder networks;decoder networks use;encoder decoders;attention decoder networks;encoder decoders context;method training encoder;text train attention;encoder;translation based augmentation;decoders context low;text based augmentation;decoder;decoders;decoders context;text train;target text train;train attention;augmentation technique training;attention;networks;text based translation;based augmentation;novel method training;target text;text;based augmentation techniques", "pdf_keywords": ""}, "25ddddbd0bd1cfebf1548b2ee91bb1bbd05fdff1": {"ta_keywords": "inputs multimodal transformer;language inputs multimodal;neural agent;neural agent based;multimodal transformer;inputs multimodal;behavior neural agent;predicting behavior neural;multimodal transformer approach;agent based encoding;training predicting behavior;multimodal;neural;behavior neural;language inputs;encoding language inputs;training predicting;large number neurons;neurons single step;neurons;predict behavior large;encoding language;based encoding language;predicting behavior;number neurons;approach training predicting;agent;neurons single;transformer;number neurons single", "pdf_keywords": "vision language navigation;language navigation tasks;language navigation agent;vision andlanguage navigation;visionandlanguage navigation;agent language attention;visionandlanguage navigation vln;challenges visionandlanguage navigation;vision language action;semantic navigation tasks;navigation task encoder;agent trained language;language downstream vision;andlanguage navigation task;vision language;navigation tasks;language navigation;attention language based;strategy vision language;language navigation based;language navigation using;key challenges visionandlanguage;attention language;language attention;approach vision language;history vision language;navigation tasks based;combination attention language;challenges visionandlanguage;navigation task"}, "0805cb1b26577f08f84190445992f7f0584e4742": {"ta_keywords": "analysis complex systems;complex systems;complex systems based;study complex systems;dynamics presence complex;complex environment;analysis evolution dynamics;complex environment approach;evolution dynamics presence;dynamics;complex environment results;evolution dynamics;systems ranging molecular;dynamics presence;compared dynamics;results compared dynamics;biology study complex;analysis complex;complex;compared dynamics presence;analytical numerical;systems;analytical numerical methods;applied variety systems;approach analysis complex;study complex;variety systems;analysis evolution;presence complex environment;based analysis evolution", "pdf_keywords": ""}, "352ac73b7d92afa915c06026a4336927d550cec3": {"ta_keywords": "graph neural;graph neural network;novel graph neural;parameters gp gnns;gnns extraction parameters;network generated parameters;gp gnns extraction;generated parameters;neural network generated;generated parameters gp;gnns extraction;parameters propagation module;generated parameters used;data parameters propagation;novel graph;improvements generated parameters;parameters propagation;extraction parameters data;parameters data parameters;propose novel graph;parameters data;graph;parameters gp;parameters;data parameters;network generated;propagation module generated;neural network;gp gnns;dataset distantly supervised", "pdf_keywords": "reasoning extraction relations;relational reasoning unstructured;natural language relational;predicting relation extraction;relation extraction natural;gnns process relational;natural relation extraction;reasoning extraction;relation extraction distantly;relations reasoning robustly;reasoning context neural;relation extraction;reasoning unstructured text;networks natural language;relation extraction model;relation extraction context;relation extraction based;language relational reasoning;natural language reasoning;relation extraction text;relation extractionwe propose;propose graph neural;relationship extraction;relation extractionwe;hop reasoning extraction;model relation extraction;propagate relationship extraction;relation extraction process;relation extraction human;extraction relations entities"}, "b53689b8c28353106f327f0981b108eb67816053": {"ta_keywords": "based machine translation;improving quality syntax;machine translation;machine translation tptf;rule based preprocessing;syntax based machine;quality syntax based;preprocessing;special preprocessing;quality syntax;novel preprocessing method;based preprocessing;novel preprocessing;syntax based;preprocessing shown superior;preprocessing methods;present novel preprocessing;require special preprocessing;preprocessing method;preprocessing shown;syntax;preprocessing methods does;based preprocessing shown;existing tptf preprocessing;tptf preprocessing;preprocessing strategy illustrate;translation tptf method;special preprocessing strategy;tptf preprocessing methods;preprocessing method purpose", "pdf_keywords": ""}, "21d45b4923ad165fbb6612e08d06f9d786f9b4cc": {"ta_keywords": "knowledge graphs commonsense;symbolic knowledge graphs;training symbolic knowledge;graphs commonsense models;commonsense model automatically;symbolic knowledge text;learn shared commonsense;commonsense models;commonsense model;symbolic knowledge;training symbolic;graphs commonsense;commonsense models approach;distilled commonsense model;manually distilled commonsense;uses symbolic knowledge;language model sharedsense;method training symbolic;shared commonsense;knowledge graphs;knowledge text ensemble;models learn shared;distilled commonsense;knowledge text;shared commonsense use;learn shared;commonsense;general language model;language model;commonsense use method", "pdf_keywords": "symbolic knowledge distillation;distill symbolic knowledge;distill knowledge symbolically;symbolic knowledge graphs;symbolic knowledge graph;symbolic knowledge generate;knowledge symbolically text;generate symbolic knowledge;symbolic knowledge enrich;knowledge distillation;knowledge distillation knowledge;produce symbolic knowledge;knowledge distillation train;knowledge distillation model;symbolic knowledge source;knowledge symbolically;symbolic knowledge;based symbolic knowledge;introduce symbolic knowledge;knowledge distillation carefully;knowledge distillation distill;use symbolic knowledge;demonstrate symbolic knowledge;distill knowledge;knowledge generate commonsense;extension knowledge distillation;knowledge use symbolic;automatic knowledge graph;symbolic knowledge used;knowledge generate"}, "53e0abebd9aef5915f72147d3674596a0051748c": {"ta_keywords": "security personalized services;privacy data protection;security personalized;personalized artificial intelligence;data protection;data protection context;personalized services;personalized services preserved;levels security personalized;privacy data;protection context personalized;artificial intelligence services;privacy;research privacy;security underlying services;research privacy data;intelligence levels security;edge research privacy;intelligence services;personalized artificial;security;intelligence services review;personalized;security underlying;context personalized artificial;protection context;protection approaches;different protection approaches;preserved ii security;protection approaches different", "pdf_keywords": "data protection personalized;protection personalized services;anomaly personalized intelligence;protecting personal data;personalized services challenges;privacy personalization data;personalized artificial intelligence;personalized intelligence artificial;privacy personalization;levelpersonalized data protection;protection personalized;protect data privacy;protect privacy data;privacy data protection;personalized services;protecting data general;protecting privacy personal;personal data privacy;protecting user privacy;tradeoff privacy personalization;personalized intelligence;data privacy;challenges data protection;data general protecting;privacy machine;privacy personal data;general protecting data;security personal data;capabilities personalized services;protection personalized artificial"}, "b6145cc19acfbec31373446a2dba210cc9b1eb7f": {"ta_keywords": "distantly supervised learning;distantly supervised;framework distantly supervised;extract relation examples;relation examples large;extract relation;supervised learning;learning problem jointly;relation examples;supervised;relation;supervised learning problem;distantly;large collection data;jointly solved extract;solved extract relation;framework distantly;examples large collection;novel framework distantly;data sets;high precision data;data proposed framework;precision data sets;learning problem;learning;large collection;jointly;examples large;data proposed;collection data proposed", "pdf_keywords": "supervised relation extraction;supervised relation mentions;structured disease corpus;extracting distantly supervised;distantly supervised relation;entity structured corpus;mentions structured corpora;relation extraction;structured corpus diejob;structured corpus method;supervision label propagation;structured corpus;small structured corpus;relation label propagation;accurate labeling relations;propagation disease corpus;disease corpus;relation extraction method;extract distantly labeled;list structured corpus;structured corpora;entity relation corpora;structured corpus use;relation examples corpus;structured corpora method;algorithm classify mentions;disease corpus dailymed;unstructured testing corpus;combining distant supervision;relation corpora"}, "181e1d4b08dc62237277a6a743576facd8c5e572": {"ta_keywords": "estimate speaker number;estimate speaker;estimating number speakers;ability estimate speaker;target speaker voice;target speaker;number speakers target;estimations estimate target;posterior estimations estimate;estimate target number;posterior estimations;speakers target speaker;speaker number signal;posterior posterior estimations;speakers target;number speakers;voice signal method;speaker voice;estimate target;speaker voice signal;speakers proposed method;speaker number;number speakers proposed;estimations;fixed number speakers;estimations estimate;speaker;voice signal;estimating number;signal use posterior", "pdf_keywords": "voice activity detection;features speech data;speaker diarization based;feature speaker representation;speaker voice activity;approach speaker diarization;performance speaker diarization;speaker diarization;extract speech features;proposed speaker diarization;speakers speech recording;speech data;decoding meeting transcription;speech recording;speaker representation;speech segments using;speaker diarization investigate;inference speech segments;diarization speech;speech recording proposed;speech features;voice activity;features speech;speech segments;speech features speech;speech data performs;extract speech;speaker voice;diarization speech enable;predict number speakers"}, "4236a5f650f5b7ced7512b5072a062b521220b31": {"ta_keywords": "transfer learning framework;transfer learning;propose transfer learning;semantic features classic;temporal semantic features;historical data target;spatio temporal semantic;areas propose transfer;features classic regression;traffic;data target areas;semantic features;transfer;speed urban areas;traffic speed urban;dataabundant areas utilizing;urban areas;rely historical data;historical data;historical data dataabundant;speed urban;spatio temporal;learning framework;data target;various spatio temporal;traffic speed;temporal semantic;urban;data dataabundant areas;features", "pdf_keywords": ""}, "e25ce2a7b28699e1d57803ef977175937ce50923": {"ta_keywords": "annotation high quality;annotation high;word segmentation;estimating pronunciations;based pronunciation estimation;accurately estimating pronunciations;estimating pronunciations context;pronunciation estimation;pronunciation estimation introducing;segmentation word based;natural language processing;annotation;language processing;method annotation high;word based pronunciation;word segmentation word;steps word segmentation;segmentation word;novel method annotation;natural language;method annotation;based pronunciation;pronunciations;word based;pronunciation;segmentation;documents proposed method;quality documents;context natural language;pronunciations context natural", "pdf_keywords": ""}, "0e532d1489d7420cff7ff8aa211ded08e7d57fe9": {"ta_keywords": "online stronglyconvex learning;online learning algorithms;stronglyconvex learning based;stronglyconvex learning;strongly convex loss;convex loss functions;convex loss;use convex loss;constructing convex optimization;convex optimization;hypothesises online learning;online learning;problems strongly convex;strongly convex;convex optimization problem;approach online stronglyconvex;optimal hypothesises online;online stronglyconvex;loss functions proposed;learning algorithms;constructing optimal;learning algorithms apply;constructing convex;loss functions;loss functions present;problem constructing convex;stronglyconvex;constructing optimal hypothesises;optimal;framework constructing optimal", "pdf_keywords": ""}, "1109f787fc8d51feb3bae9bf6e1945dc4a1191e7": {"ta_keywords": "explainable ai human;explainable ai;humans accept explanations;team performance explanations;impact explainable ai;accept explanations explanations;accept explanations;explanations explanations;ai human centered;explanations explanations explain;explanations regardless correctness;ai human;performance underlying team;ai;correctness explanations;accept explanations regardless;human centered team;explanations;team performance;correctness explanations explain;centered team performance;explanations regardless;performance explanations;explanations explain underlying;explain underlying decision;explanations explain;decision making task;underlying decision making;task increase chance;increase chance humans", "pdf_keywords": "human ai explanations;ai explanations;explanations artificial intelligence;ai team people;intelligence ai performance;human ai;making human ai;human artificial intelligence;human ai team;ai explanations dowe;human ai accurate;ai affect decision;explanations effective human;explanations team performance;intelligence ai;ai performance;intelligence ai affect;humans human ai;performance human ai;ai accurate human;intelligence ai assistant;ai;tasks artificial intelligence;accurate human ai;ai assistant;artificial intelligence human;ai assistant automated;tasks artificial;explanations increase team;team performance artificial"}, "328a9fe143639810d6413c2cc901ec3afa6aa607": {"ta_keywords": "models molecular dynamics;continuum models molecular;molecular dynamics;molecular dynamics proposed;models molecular;training continuum models;stochastic linear perturbation;continuum models;learned solving stochastic;perturbation theory learned;stochastic linear;based stochastic linear;solving stochastic linear;linear perturbation theory;solving stochastic;stochastic;framework based stochastic;training continuum;perturbation theory;based stochastic;model physically consistent;molecular;approach training continuum;linear perturbation;model physically;linear perturbation equation;continuum;resulting model mathematically;model mathematically;perturbation", "pdf_keywords": ""}, "350e5f5a89cbb3a23442c9d0d3e59fc50d665dbb": {"ta_keywords": "pricing electricity model;implications pricing electricity;pricing electricity;distribution energy late;notion planck;planck;based notion planck;electricity model;late time market;distribution energy;time market discuss;time market;electricity model based;energy late time;model distribution energy;implications pricing;discuss implications pricing;pricing;electricity;energy late;energy;market discuss implications;market;market discuss;model distribution;distribution;present model distribution;late time;late;present model", "pdf_keywords": ""}, "2226f5a13e3e9faac2e228e95175d3e612b52395": {"ta_keywords": "artificial intelligence sigai;group artificial intelligence;intelligence sigai report;acm special group;activity report acm;artificial intelligence;intelligence sigai;acm special;report acm special;group artificial;report acm;acm;special group artificial;intelligence;2019 report organized;sigai report;annual activity report;special group;artificial;activity report;june 2019 report;group;global members group;report organized following;sigai report covers;annual report;2019 report;annual report membership;activity;membership group", "pdf_keywords": ""}, "c102ac8c779ee0a53dc8e4ee20b4088ac2c7e186": {"ta_keywords": "distribution data multidimensional;np complete distribution;distribution predictions np;end distribution data;np rigorous distribution;complete distribution predictions;distribution data;multidimensional data;data multidimensional data;multidimensional data set;data multidimensional;np complete rigorous;distribution predictions;predictions np rigorous;combines predictions np;predictions np complete;joint np complete;complete distribution;rigorous distribution achieves;multidimensional;model joint np;data set model;distribution achieves;np rigorous;data;rigorous distribution;end distribution;end end distribution;data set;prior models achieve", "pdf_keywords": "speech recognition joint;speech recognition attention;end speech recognition;joint attention decoder;decoding speech recognition;decoding attention model;attention based encoder;decoder joint attention;trained decoding attention;attention decoder trained;speech recognition;decoder attention model;attention convolutional network;attention decoder;recognition joint attention;decoding attention;model joint attention;trained decoder attention;learned attention decoder;decoder attention;recognition attention model;attention joint nonlinear;useful speech recognition;joint attention convolutional;decoder trained attention;attention model trained;spontaneous speech recognition;automatic speech recognition;deep network decoder;decoding speech"}, "1144cc3e86b1cc4160aedddb085d7861d4b528dc": {"ta_keywords": "softmax sampling vocabulary;softmax sampling;sampled softmax sampling;using sampled softmax;sampled softmax;sampling vocabulary significantly;sampling vocabulary;model sampling vocabulary;softmax;corpus using sampled;selecting vocabulary corpus;vocabulary corpus;selecting vocabulary;vocabulary corpus using;method selecting vocabulary;corpus;model sampling;significantly smaller vocabulary;vocabulary significantly;smaller vocabulary;corpus using;standard model sampling;smaller vocabulary used;sampling;vocabulary;vocabulary significantly smaller;vocabulary used standard;vocabulary used;using sampled;sampled", "pdf_keywords": "rnn transducer models;model rnn transducer;rnn transducer architecture;train rnn transducer;rnn transducer machine;asr model rnn;based rnn transducer;networks rnns;speech recognition training;example rnn transducer;rnn transducer;softmax example rnn;neural networks rnns;model rnn;model based rnn;speech recognition asr;usage rnn transducer;rnn transducer teacher;end speech recognition;memory usage rnn;neural machine translation;rnn transducer large;rnns;example rnn;machine translation train;learning deep recurrent;transducer machine translation;softmax train;softmax fast efficient;architecture automatic speech"}, "69e9a040ef633c60533843442529cc68c5f12932": {"ta_keywords": "power iteration clustering;iteration clustering;method clustering datasets;clustering datasets;iteration clustering igcc;efficient method clustering;clustering datasets based;clustering igcc technique;clustering igcc;normalized pairwise similarity;pairwise similarity matrix;clustering;dimensional embedding dataset;datasets based power;similarity matrix approach;method clustering;similarity matrix;power iteration normalized;spectral methods ncut;iteration normalized pairwise;embedding dataset;iterative power iteration;embedding dataset using;spectral methods;pairwise similarity;dimensional embedding;power iteration approach;matrix approach outperforms;truncated power iteration;low dimensional embedding", "pdf_keywords": ""}, "3dc4580a154df87f3a56aa3d16b00c5a935ebe15": {"ta_keywords": "bias citation reviewer;citation reviewer;citation reviewer work;quantify bias citation;identify bias citation;bias citation;effect bias citation;reviewer work submission;quantify bias;investigate effect bias;reviewer work;identify bias;analysis reveals bias;mismatch identify bias;effect bias;reviewer;bias;bias apply;bias present;bias present bias;bias apply different;citation;detect quantify bias;reveals bias present;submission analysis reveals;reveals bias;work submission analysis;submission analysis;present bias;present bias apply", "pdf_keywords": "reviewers cited bias;citation bias peer;bias peer review;cite potential reviewers;citation bias evaluation;suggesting citation bias;citation reviewer;citation bias;reviewers conclude bias;citation bias process;approaches citation bias;reviewers deliberately cite;citation bias present;does citation reviewer;citation bias study;citation bias literature;citation bias significance;study citation bias;cited bias;cited reviewers;citation bias ranking;citation reviewer work;conferences citation bias;effect citation bias;citation bias methodology;impact citation bias;reviewers cited;indicates citation bias;reviewer positively biased;expert reviewers cited"}, "3cfdec4f1664fcdc20fd5a6d3f86e7b40cf19f70": {"ta_keywords": "length encoder decoder;decoder outputs learning;controlling length encoder;control length encoder;decoder outputs neural;neural encoder decoder;length encoder;outputs neural encoder;encoder decoder outputs;length text summarization;neural encoder;control length text;encoder decoder;encoder decoder model;decoder outputs;outputs neural;outputs learning;length text;decoder;encoder;decoder model;outputs learning based;decoder outputs degrading;outputs degrading summary;summary quality learning;decoder model use;text summarization tasks;controlling length;summarization tasks;control length", "pdf_keywords": "summarization task encoder;length neural encoder;generate sentence summaries;length sentences text;managing length sentences;byte summaries;generate summaries;length output encoder;generate summaries various;length neural encoderdecoder;summaries various lengths;output length summary;sequence length neural;byte summaries despite;summaries various lengthsin;summaries text;sentence summaries text;neural encoder decoder;sentence compression based;sequences decoding based;length sentences;length summary;75 byte summaries;output sequences decoding;decoder models generate;summaries text summarization;text summarization task;text summarization;used generate summaries;decoder models propose"}, "7a070c558cdb9c525559d1ad48159551381750c9": {"ta_keywords": "gram machines;issue gram machines;gram machines using;encode knowledge symbolically;scalability issue gram;machines using symbolic;symbolic meaning representations;knowledge symbolically generate;sequence models encode;encode knowledge;gram;models encode knowledge;meaning representations indexed;issue gram;encoded knowledge;questions encoded knowledge;sequence sequence models;programs answer questions;complexity independent text;encoded knowledge apply;generate programs answer;meaning representations;sequence models;knowledge symbolically;symbolically generate programs;answer questions encoded;symbolically generate;programs answer;representations indexed retrieved;specifically use sequence", "pdf_keywords": "grained knowledge representation;knowledge storage;knowledge storage sequence;based knowledge storage;domain knowledge storage;knowledge storage novel;scalable gram machine;knowledge storage auto;knowledge representation;complex semantics bab;semantics bab tasks;model knowledge storage;large corpus text;large web scale;knowledge unstructured data;structured knowledge unstructured;knowledge encoder;text symbolic representations;encoding knowledge expressed;knowledge based neural;propose scalable gram;scalable gram;knowledge unstructured;grained knowledge;unstructured representations knowledge;model large corpus;web scale corpus;encoding knowledge;large corpus;representations large corpus"}, "6516b800482100731f0eb348f678ad30799c839f": {"ta_keywords": "predictive word emergence;word emergence;neology new words;word emergence support;words emerge language;words emerge;distributional semantics paradigm;distributional semantics;phenomenon neology new;phenomenon neology;new words emerge;semantic neighbors;semantic sparsity;semantic;semantic sparsity frequency;semantics paradigm factors;neology new;neology;formalized distributional semantics;analysis phenomenon neology;rates semantic neighbors;factors semantic sparsity;semantics paradigm;emerge language investigate;growth rates semantic;semantic neighbors formalized;emerge language;semantics;new words;language investigate", "pdf_keywords": "predictive word emergence;driven word emergence;neologisms semantics significantly;word emergence;words emerge semantic;emergence new words;word emergence support;neologisms semantics;word emergence using;neologisms generalization word;words emerge language;neologism dictionary words;neologisms semantics using;word distribution context;neologisms generalization;words neologism;words neologism dictionary;neologisms datasets word;distributional semantics;new words semantic;words emerge;distributional semantics hypotheses;terms distributional semantics;frequently used neologisms;new words emerge;neologisms;word distribution;analyze word distribution;distributional semantics tested;neologism dictionary"}, "ac03cf22e2a831ab030ae33b5ddf5f9864917a17": {"ta_keywords": "diverse student populations;largest diverse student;sample largest diverse;diverse student;sample comprises students;student populations;student populations recorded;job fair sample;students world academia;student presentations collection;100 students world;collection student presentations;students world;students;presentations collection student;100 students;fair sample consists;comprises students world;world academia media;populations recorded job;student presentations;consists 100 students;media industry sample;world academia;fair sample;sample;sample largest;largest diverse;comprises students;recorded job fair", "pdf_keywords": ""}, "0a7f95adbaf0e46c93b5f82c74a26f5874c861ac": {"ta_keywords": "controllers traffic ramp;ramp model random;traffic ramp model;ramp metering systems;traffic ramp metering;random phase approximation;approximation proposed control;approximation random phase;ramp metering;traffic ramp;feedback controllers traffic;ramp model;phase approximation simulation;phase approximation model;model random phase;parameter approximation random;controllers traffic;phase approximation proposed;based lumped parameter;random phase;approximation simulation;phase approximation;lumped parameter model;context traffic ramp;parameter model random;lumped parameter approximation;proposed control law;approximation random;metering systems based;approximation model", "pdf_keywords": ""}, "c5e4eafd85949e6aac9d8e98d5e03b2acf444046": {"ta_keywords": "adversarial datasets worse;better adversarial datasets;adversarial datasets;trained adversarial data;adversarial data perform;efficacy adversarial data;adversarial data;adversarial data collection;standard adversarial data;study efficacy adversarial;perform better adversarial;better adversarial;trained adversarial;efficacy adversarial;adversarial data identifying;models trained adversarial;adversarial;differences standard adversarial;standard adversarial;datasets worse;datasets models trained;datasets worse diverse;models trained;domain evaluation sets;models datasets;datasets models;collection domain evaluation;datasets;domain evaluation;models datasets models", "pdf_keywords": "questions adversarially model;questions adversarially;compose questions adversarially;better adversarial datasets;question answering;adversarially model;adversarial datasets worse;perform better adversarial;adversarially;questions improved leveraging;questions train models;adversarial data usually;better adversarial;adc question answering;evaluation adversarial data;evaluated adversarial data;adversarial datasets;trained adversarial data;study efficacy adversarial;trained adversarial;questions train;leveraging data crowdworkers;adversarial test;models trained adversarial;adversarial data use;adversarial data collection;question answering assigning;evaluation adversarial;model evaluation adversarial;adversarial data"}, "575ac3f36e9fddeb258e2f639e26a6a7ec35160a": {"ta_keywords": "intermediate parsing training;supervised parsing high;parsing training;supervised parsing;parsing high level;knowledge treebanks transformer;intermediate parsing;effect intermediate parsing;syntactic knowledge treebanks;parsing;treebanks transformer;high level semantic;treebanks transformer fine;knowledge treebanks;semantic language understanding;efficacy supervised parsing;transformer biaffine parsing;parsing high;level semantic language;explicit syntactic knowledge;parsing training ipt;treebanks;language understanding;parsing head;biaffine parsing;syntactic knowledge;level semantic;language understanding lu;biaffine parsing head;infuse explicit syntactic", "pdf_keywords": "intermediate parsing training;supervised parsing high;parsing training;parsing training intermediate;effect intermediate parsing;intermediate parsing;syntax knowledge downstream;parser following training;transformer biaffine parsing;supervised parsing;exposed intermediate parsing;parsing high level;syntactic knowledge transformers;downstream language understanding;state art parsers;usefulness supervised parsing;train parser;parsers;parser use downstream;leveraging formalized syntactic;parsing;transformers natural language;parser extracting structural;training downstream language;explicit syntax knowledge;biaffine parser extracting;inconsequential original parsing;parser extracting;parser;biaffine parsing"}, "b437cc7c0ae672b188df078b5dd80f97e8dde978": {"ta_keywords": "learning lexical units;unsupervised learning lexical;lexical units speech;units speech recognition;lexical units;learning lexical;machine translation models;translation models;recognition machine translation;units speech;machine translation;translation models constructed;translation models used;speech recognition;tasks speech recognition;speech recognition machine;unsupervised learning;speech recognition requires;parametric bayesian inference;non parametric bayesian;lexical;bayesian inference;accuracy tasks speech;framework unsupervised learning;parametric bayesian;tasks speech;bayesian inference procedures;units existing methods;translation;speech", "pdf_keywords": ""}, "69ba64b20d0a1849ef08d63c39bfafbaac909087": {"ta_keywords": "learning behavioral constraints;behavioral constraints;behavioral constraints obeying;bound expected regret;obeying exogenous constraints;exogenous constraints;learning behavioral;expected regret bcts;constraints obeying exogenous;exogenous constraints characterize;allows learning behavioral;constraints;regret bcts;constraints obeying;regret bcts provide;expected regret;algorithm called bcts;bcts allows learning;constraints characterize;regret;novel algorithm called;learning;algorithm called;behavioral;allows learning;algorithm;novel algorithm;obeying exogenous;bcts allows;constraints characterize upper", "pdf_keywords": ""}, "40bbd3046f1fa86a50e526b3848b4f2bd3a1d873": {"ta_keywords": "nafion perfluorinated polyelectrolyte;battery lithiated nafion;perfluorinated polyelectrolyte polyelectrolyte;polyelectrolyte produced lithiated;perfluorinated polyelectrolyte produced;polyelectrolyte solution lithiated;lithiated nafion perfluorinated;perfluorinated polyelectrolyte;performance battery lithiated;perfluorinated perfluorinated polyelectrolyte;polyelectrolyte polyelectrolyte produced;conventional electrolyte;polyelectrolyte produced polyelectrolyte;produced polyelectrolyte solution;produced polyelectrolyte;polyelectrolyte produced;alternative conventional electrolyte;polyelectrolyte polyelectrolyte;nafion soluble perfluorinated;electrolyte;battery lithiated;nafion perfluorinated;polyelectrolyte solution;polyelectrolyte;lithiated ion alternative;lithiated nafion soluble;produced lithiated ion;conventional electrolyte work;performance battery;solution lithiated nafion", "pdf_keywords": ""}, "059f515bf53bcddeca031fd4a4071c911999a3c6": {"ta_keywords": "supervised apparel invariant;learning apparel invariant;person invariant apparel;apparel invariant feature;apparel invariant person;apparel invariant;invariant apparel invariant;semi supervised apparel;invariant apparel;wearing similar clothes;different clothes unsupervised;apparel invariant performance;supervised apparel;wearing different clothes;clothes unsupervised semi;clothes unsupervised;clothes obtain images;similar clothes;different clothes;similar clothes obtain;changing persons wearing;cloth changing persons;invariant feature learning;learning apparel;person invariant;persons wearing similar;improve person invariant;person wearing different;images person wearing;clothes obtain", "pdf_keywords": "unsupervised apparelsimulation gan;person image clothing;person clothing change;apparel invariant person;learn apparel invariant;apparel invariant feature;apparelperson identification crucial;apparelsimulation gan;person clothing;gan based apparel;characterization apparelperson identification;cloth changing persons;apparelperson identification;wearing similar clothes;improve apparel invariant;apparelsimulation gan synthesize;images different clothings;gan person image;changing persons wearing;synthetic person images;image clothing;clothing change;wear similar clothes;gan synthesize clothin;person image generation;apparel invariant;generate person image;clothings select person;apparel changed person;wearing similar cloth"}, "c96363c42bc8c465902c22b8c33c8704233f519e": {"ta_keywords": "benchmark code generation;code generation natural;natural language commands;challenge natural language;generation natural language;code generation;language language benchmark;language challenge natural;code generation systems;language benchmark;language challenge dataset;natural language challenge;language benchmark performance;annotated languages;annotated languages language;natural language pairs;pairs annotated languages;language commands;present benchmark code;benchmark code;performance code generation;language commands extending;language challenge;language pairs annotated;natural language;896 natural language;commands extending language;benchmark performance code;language pairs;code", "pdf_keywords": "lingual code generation;language snippets seq2seq;languages code generation;code natural language;benchmark code generation;learning language snippets;seq2seq snippets native;language snippets;generating code translationally;generation code summarization;language benchmark;code cross lingual;generation code translationally;code multilingual code;code multilingual;multilingual code;language benchmark cross;multilingual code cross;generation task multilingual;code intelligence resource;multilingual code multilingual;language specific seq2seq;code generation natural;seq2seq snippets;cross lingual code;snippets seq2seq goal;languages code;code generation;code intelligence;task multilingual code"}, "70a321f12a655e305781e2de0ca9617d96e462c3": {"ta_keywords": "data aggregators competition;interactions data market;data market;aggregators competition;aggregators competition users;data market multiple;competition users data;market multiple data;aggregators game;data aggregators;aggregators game admits;game aggregators game;multiple data aggregators;aggregators;model game aggregators;game aggregators;competition users;price anarchy;price anarchy measures;market multiple;equilibria;equilibria calculate price;game admits equilibria;equilibria continuum equilibria;market;data;calculate price anarchy;noncooperative strategic play;equilibria continuum;users data", "pdf_keywords": "data buyer optimal;game data buyers;data buyers estimators;data market formulate;pricing data market;strategic data buyers;market data aggregator;data pricing;data pricing data;data model competition;data buyers strategic;data market models;efficiency data market;data market;optimum data market;data market data;data market theorem;game buyers data;applicable data market;data market applicable;buyers strategic data;data market buyers;data aggregator;data aggregator design;pricing data;data buyers analyze;strategic data;pricing data critical;market data;data buyers compete"}, "bee52c51cbd37d0e48c3ea5f71a08f177d2aff73": {"ta_keywords": "cipher phoneme conversion;phoneme cipher;cipher phoneme;phoneme cipher phoneme;binary classification;cipher phoneme cipher;method binary classification;termed cipher phoneme;approach binary classification;binary classification based;novel cipher phoneme;vectors termed cipher;cipher;classification based structured;termed cipher;novel approach binary;using novel cipher;novel cipher;binary;phoneme conversion method;phoneme conversion;approach binary;classification;classification based;based structured learning;structured learning using;structured learning;phoneme conversion state;method binary;weight vectors", "pdf_keywords": ""}, "7129b62be18487db5e9602e353bb10a4c79a9b92": {"ta_keywords": "predicting procedure names;names stripped executables;procedure names stripped;executables;procedure names;stripped executables approach;executables approach;stripped executables;neural textual;predicting procedure;executables approach combines;static analysis neural;architectures evaluation shows;analysis neural models;approach predicting procedure;representations drive graph;art neural textual;neural models obtain;architectures evaluation;neural models;analysis neural;state art neural;sites use representations;based architectures evaluation;names stripped;graph based;combines static analysis;use representations drive;neural;representations sites use", "pdf_keywords": ""}, "f4cf4246f3882aa6337e9c05d5675a3b8463a32e": {"ta_keywords": "patterns natural language;learning patterns patterns;understanding patterns natural;learning patterns;patterns just patterns;patterns patterns simple;patterns patterns natural;patterns patterns expressed;grounded understanding patterns;patterns simple;patterns patterns;understanding patterns;make patterns patterns;finding patterns patterns;patterns expressed;patterns natural;just patterns patterns;patterns;patterns simple intuitive;finding patterns;patterns patterns action;make patterns;method learning patterns;patterns just;just patterns;patterns expressed action;patterns action;patterns action need;action patterns natural;natural language", "pdf_keywords": "grounded visual language;annotated expert demonstrations;vision language planning;vision language navigation;demonstrations robotics tasks;existing vision language;expert demonstrations robotics;generating expert demonstrations;vision language;model vision language;actions interactions visually;perform vision language;visual language understanding;natural language action;expert demonstrations generated;visual language;novel vision language;vision language based;tasks natural language;vision sequences actions;language based tasks;language navigation tasks;language enabled vision;expert demonstrations network;learning imitation;learning navigation instructions;recent vision language;demonstrations robotics;describing manipulating actions;exploration sequences actions"}, "8f11643b42976433fc3a2ec19feef486929527a1": {"ta_keywords": "xmath2 relativistic field;parameter xmath2 relativistic;xmath2 relativistic;xmath0 context relativistic;electroweak interactions xmath3;relativistic quantum field;calculation lorentz violating;interactions xmath3 parameter;relativistic field theory;lorentz violating parameter;lorentz violating;relativistic quantum;relativistic field;xmath3 parameter xmath4;obtained xmath5 parameter;field theory electroweak;xmath5 parameter xmath6;xmath3 parameter;parameter xmath4 obtained;study xmath1 parameter;interactions xmath3;parameter xmath2;xmath1 parameter xmath2;xmath5 parameter;xmath1 parameter;context relativistic quantum;violating parameter xmath0;parameter xmath0;parameter xmath0 context;parameter xmath4", "pdf_keywords": ""}, "d9e56aa9f69e18c9d37799b86b50d36709cbf711": {"ta_keywords": "ensemble particles average;particles different average;particles opposite average;ensemble particles;particles average;average numbers particles;particles average number;average number particles;numbers particles average;number particles average;particles different;particles particles particles;particles opposite;particles particles;number particles opposite;number particles different;number particles particles;particles number particles;different average numbers;numbers particles different;ensemble;opposite average numbers;number particles;particles;numbers particles;opposite average;particles number;different average;numbers particles number;average numbers", "pdf_keywords": ""}, "59a228f48a83eb0905391f7e454fde0eeb6680ee": {"ta_keywords": "acoustic lattices linguistic;learn language model;speech recognition;automatic speech recognition;lattices simultaneously learning;automatic speech;lm continuous speech;simultaneously learning lexical;accuracy automatic speech;speech recognition aasr;learning lexical;language model lm;language model;training lattices;class acoustic lattices;learning lexical units;lattices linguistic;training lattices simultaneously;uses acoustic lattices;acoustic lattices;acoustic lattices used;continuous speech proposed;performs training lattices;continuous speech;lattices linguistic constraints;speech proposed method;method learn language;learn language;lexical units lm;lm joint learning", "pdf_keywords": ""}, "8b7a8f9a27b8dc73a5b0b62ada14bbab047084fc": {"ta_keywords": "voice conversion vcch;statistical voice conversion;voice conversion;silent speech enhancement;conversion vcch silent;vcch silent speech;speech enhancement propose;speech enhancement;speech enhancement electrol;electrol speech enhancement;enhancement electrol speech;conversion vcch;vcch silent;real time vcch;statistical voice;time statistical voice;vcch capable;electrol speech;capable running vcch;preserving conversion accuracy;vcch capable running;running vcch;voice;vcch little degradation;time vcch capable;silent speech;preserving conversion;digital signal processor;running vcch little;vcch", "pdf_keywords": ""}, "4100256a125d7b56cac693a436bba2b00fae3fa3": {"ta_keywords": "automated captioning acoustic;captioning acoustic signals;captioning acoustic;automated captioning;architecture automated captioning;captioning;models trained corpus;trained corpus;language descriptions acoustic;natural language descriptions;generate natural language;language descriptions;corpus high quality;corpus;decoder;trained corpus high;natural language;descriptions acoustic signals;models trained;decoder generate natural;augmented transformers encoder;descriptions acoustic;decoder generate;encoder decoder;encoder decoder generate;encoder;fact models trained;transformers encoder decoder;acoustic signals model;transformers encoder", "pdf_keywords": ""}, "306c59458cebb35c2d520dd129f09d5c6cc2985f": {"ta_keywords": "xmath1 xmath2 xmath3;xmath0 xmath1 xmath2;xmath2 xmath3 xmath4;xmath1 xmath2;xmath7 xmath8 xmath9;xmath8 xmath9 xmath10;xmath0 xmath1;xmath9 xmath10 xmath11;xmath3 xmath4 xmath5;xmath6 xmath7 xmath8;xmath3 xmath4;behaviour xmath0 xmath1;xmath10 xmath11 xmath12;xmath5 xmath6 xmath7;xmath2 xmath3;xmath4 xmath5 xmath6;xmath9 xmath10;xmath12 xmath13 xmath14;xmath10 xmath11;xmath8 xmath9;xmath0;xmath11 xmath12 xmath13;xmath4 xmath5;xmath6 xmath7;xmath1;xmath11 xmath12;xmath13 xmath14;xmath5 xmath6;xmath7 xmath8;xmath12 xmath13", "pdf_keywords": "parametric paraphrase models;paraphrase models;paraphrase models score;paraphrase database;pairs paraphrase database;similarity lexical paraphrasability;model paraphrase phrases;bigram similarity corpus;embed phrases text;short phrases dataset;embed phrases;paraphrases text;phrases dataset;predicting semantics sentences;paraphrase phrases;models short phrases;capture similarities phrases;lexical paraphrasability method;paraphrases;predicting word similarity;bigrams short phrases;confidence scores paraphrases;phrase corpus;vectors embed phrases;lexical paraphrasability;paraphrase database ppdb;mean phrase corpus;short phrase tasks;parametric model paraphrase;sentences deep learning"}, "bba9b93ab8d9b98cd54001a5ba9673e513a35219": {"ta_keywords": "model learn data;learning data;patient model trained;learn data;data collected patient;problem learning data;learning data form;learned model;data use neural;diagnosis process learned;model trained;clinical data;network model learn;model learn;use neural network;neural network;learned model outperforms;neural network model;patient model;learn data presence;data form clinical;clinical data use;learned model evaluated;form clinical data;demonstrate learned model;process learned model;use neural;sensor data;problem learning;data", "pdf_keywords": ""}, "cc352ea39f820c3effc40ce09369cf59afe361df": {"ta_keywords": "health systems cloud;migrate health systems;cloud platform;cloud services;applications cloud platform;cloud computing environment;approach migrate health;migrate health;applications cloud;cloud computing;server architecture cloud;cloud sector;systems cloud;systems cloud sector;demonstrate cloud services;cloud sector traditional;architecture cloud computing;architecture cloud;cloud;deploying applications cloud;environment demonstrate cloud;health systems;cloud services leveraged;novel approach migrate;environment approach migrate;approach migrate;client server architecture;demonstrate cloud;migrate;server architecture", "pdf_keywords": "health applications cloud;cloudcloud computing;cloudcloud computing promising;cloud based health;cloud platform implemented;cloud platform;cloud platform cloud;cloud based applications;qualitycloud computing emerging;cloud services;using cloud;cloud infrastructure;applications using cloud;cloud infrastructure cloud;using cloud services;clouds health data;applications cloud platform;qualitycloud computing;platform cloud;cloud computing;cloud computing infrastructure;demonstrate cloudcloud computing;leveraging cloud platform;cloudcloud;platform cloud based;applications cloud;infrastructure cloud;hosted cloud;applications hosted cloud;layer cloud infrastructure"}, "589e651c69251ee20a89e075d015eb03b35cf17d": {"ta_keywords": "shared speech encoder;speech encoder;speech encoder used;autoregressive ar decoders;prediction methods speech;nar framework speech;speech translation tt;framework speech translation;trained shared speech;speech translation;tt nar autoregressive;decoders jointly trained;ar decoders jointly;nar autoregressive ar;length prediction methods;autoregressive nar;autoregressive nar framework;nar autoregressive;non autoregressive nar;effective length prediction;speech inputs;shared speech;translation tt nar;translation various length;length prediction;methods speech inputs;autoregressive ar;selecting better translation;ar decoders;encoder", "pdf_keywords": "autoregressive machine translation;speech translation e2e;end speech translation;autoregressive nar decoding;speech translation tt;translation tt systems;masked language model;machine translation;speech translation;end speech text;nar decoding;machine translation tcv;nar decoding text;deployment speech translation;novel encoder decoder;conditional masked language;language model turing;speech recognition;language model;text based translation;speech text;novel encoder;estimation speech recognition;encoder decoder;encoders novel decoders;speech recognition vertex;existing encoders novel;encoder;translation end end;encoder decoder approach"}, "d5123ab81f511027cbe11dc92d99e116fd193158": {"ta_keywords": "approach energy harvesting;energy harvesting;energy harvesting ed;harvesting ed remote;ed remote sensing;node timevarying wireless;remote sensing proposed;remote sensing;based channel probe;sensing proposed;sensing proposed ed;energy decides sample;harvesting;channel probe;timevarying wireless;wireless link source;using stored energy;timevarying wireless link;harvesting ed;channel probe outcome;sensing;source opportunistically samples;data based channel;stored energy decides;stored energy;novel approach energy;energy;sample process communicate;decides probe;ed source opportunistically", "pdf_keywords": "horizon decision process;source optimal action;optimal policy threshold;sampling policy optimal;sensing different processes;policy optimal sampling;harvesting sensor network;simple threshold policies;threshold policy single;extended decision process;optimal decision sampling;buffer optimal policy;source optimal policy;single source monitoring;harvesting source numerical;optimal source activation;infinite horizon decision;policy optimal estimation;updates energy harvesting;threshold policies;optimal decision;energy harvesting;optimal scheduling policy;optimal sampling policy;harvesting sensor;policy optimal;policy minimizes;based optimal channel;policy minimizes expected;source monitoring multiple"}, "ddfd297531f56121b8383bd1eb2bb09189ab2e2b": {"ta_keywords": "transfer emphasis speech;emphasis speech translation;emphasis translation task;emphasis speech;short term memory;speech translation proposed;speech translation;transfer emphasis;evaluated emphasis translation;emphasis translation;translation proposed model;term memory lsm;translation task;memory lsm neural;model transfer emphasis;lsm neural networks;term memory;speech;emphasis;translation task xmath0;target sentence xmath2;participants target sentence;lsm neural;neural networks;model evaluated emphasis;evaluated emphasis;memory lsm;target sentence;neural networks able;neural", "pdf_keywords": ""}, "f0a498014c4ef67c0b72ceb18d95e0d25087fd57": {"ta_keywords": "neural machine translation;bidirectional translation tasks;machine translation systems;machine translation;translation systems;translation tasks;translation tasks proposed;translation systems method;bidirectional translation;softmax binary codes;japanese bidirectional translation;output layer neural;softmax binary;softmax;codes combining softmax;neural machine;layer neural machine;codes english japanese;binary codes english;combining softmax binary;predicting binary code;english japanese bidirectional;codes english;predicting binary;predicting output layer;logarithmic vocabulary size;combining softmax;japanese bidirectional;binary code word;layer neural", "pdf_keywords": "neural machine translation;code word neural;softmax binary code;predicting binary code;softmax binary codes;binary code prediction;machine translation models;softmax binary;binary code representations;using softmax binary;translation accuracy computationally;codes combining softmax;machine translation;binary code word;predicting binary;machine translation systems;softmax;logarithmic vocabulary size;words using binary;combining softmax binary;method predicting binary;predict output words;predict code;words binary binary;code predicted bit;word neural machine;predict code output;translation models;layer logarithmic vocabulary;code prediction"}, "88e2beccbc89b3e3dd793e2502b17c1fa551151d": {"ta_keywords": "storage form generalizedbandwidth;data distributed storage;storage network data;distributed storage;generalizedbandwidth maps;generalizedbandwidth maps maps;data storage;distributed storage network;type data storage;storage network;form generalizedbandwidth maps;data distributed;generalizedbandwidth;constructed data distributed;distributed way data;data distributed way;nodes data distributed;network data stored;form generalizedbandwidth;data storage form;storage;network data;shared nodes data;data size;possible data size;distributed way;nodes data necessarily;data size determined;nodes data;data necessarily shared", "pdf_keywords": ""}, "f4c8539bed600c9c652aba76a996b8188761d3fe": {"ta_keywords": "fidelity translation neural;translation neural machine;fidelity translation;translation neural;levels fidelity translation;implements accurate translation;accurate translation scheme;translation scheme accurate;translation techniques;translation scheme hybrid;used translation techniques;new accurate translation;translation scheme;accurate translation;neural machine;neural machine methods;higher levels fidelity;commonly used translation;translation;levels fidelity;fidelity;neural;machine methods;machine methods vanilla;compare performance;used translation;performance;accurate baseline;accurate baseline used;implements accurate", "pdf_keywords": "trainingneural machine translation;neural machine translation;translation using neural;training algorithm translation;machine translation powerful;performance machine translation;improve performance translation;translation models accuracy;translation models;translation powerful computational;improving translation;machine translation nmt;designing improving translation;machine translation;translation models present;translation systems powerful;machine translation systems;performance translation systems;vocabularyneural machine translation;ofneural machine translation;translation systems;performance translation algorithm;improving translation rare;algorithm translation languages;different translation models;translation systems introduce;multiple translation models;translation task high;translation nmt systems;translation data computational"}, "3e3254bce9c321310d2e9825ed52b30da9879173": {"ta_keywords": "speech segment features;representation speech segment;speech segment;finite state machines;machines proposed representation;state machines;state machines proposed;segment features;segment features based;finite state machine;features based finite;state machine;representation speech;new representation speech;arcs finite state;machines proposed;proposed representation based;speech;search network vis;state machine advantage;representation based;segment;features based;based finite state;finite state;machines;boa linear representation;features;bag arcs boa;unique arcs finite", "pdf_keywords": ""}, "285c50d98dab741a82649b1abcaca8273cb8f253": {"ta_keywords": "graph phoneme g2p;learning based margineme;phoneme g2p conversion;graph phoneme;conversion structured learning;binary classification;online discriminative training;multiclass classification;based margineme algorithm;discriminative training;discriminative training method;online discriminative;sme online discriminative;margineme algorithm;multiclass classification massive;approach binary classification;binary classification based;phoneme g2p;method multiclass classification;margineme algorithm sme;structured learning;g2p conversion structured;structured learning based;art graph phoneme;classification;conversion structured;based margineme;training method multiclass;classification massive;novel approach binary", "pdf_keywords": ""}, "55bdc4ad158e272ccf796ae52b0ab7086a834352": {"ta_keywords": "discover student models;student models;automatically discover student;student models using;machine learning agent;learning agent;learning agent discovered;human generated models;automatically discover;discover student;agent discovered model;demonstrate discovered model;models demonstrate discovered;generated models;models;approach automatically discover;models using;generated models demonstrate;improve instruction;discovered model;new approach automatically;discovered model higher;model used improve;machine learning;models demonstrate;improve instruction strategy;discovered model used;learning;approach automatically;instruction strategy", "pdf_keywords": ""}, "c3177616ad35ef7850ea1e62da1fa3be36943e8b": {"ta_keywords": "employ recursive neural;recursive neural network;recursive neural;words problem model;neural network;dialog;dialog problem employ;problem dialog problem;dialog problem;problem dialog;neural network identification;identification right words;identify right words;right words problem;neural;model problem dialog;words problem;words problem use;problem use identification;problem model;recursive;problem employ recursive;problem model problem;method identify right;right words;employ recursive;method identification right;method identify;network identification;method identification", "pdf_keywords": ""}, "49a049dc85e2380dde80501a984878341dd8efdf": {"ta_keywords": "learning tasks wav2vec;wav2vec novel framework;wav2vec;wav2vec novel;learns speech encoding;tasks wav2vec;wav2vec shown;speech encoding;wav2vec shown outperform;learning speech learns;tasks wav2vec shown;learns speech;speech learns;speech encoding input;speech learns speech;learning speech;inequalities speech encoding;framework learning speech;proc inequalities speech;representations encoding;representations encoding performed;discrete representations encoding;representations perform supervised;learns;encoding;inequalities speech;supervised learning tasks;discrete representations;perform supervised learning;learning tasks", "pdf_keywords": "learning speech representations;audio convolutional neural;speech encoder training;representations speech audio;learns representations speech;supervised learning speech;learning representations speech;speech encoder;speech representations;latent speech representations;latent speech representation;speech representation;speech audio convolutional;speech representations latent;speech representations improves;training utterance predicting;encodes speech audio;data speech;labeled data speech;encoder representation utterance;outcome speech encoder;latents speech representation;speech representations using;learning speech;speech representations masks;speech representations inferred;speech representations called;discrete speech representations;speech audio;encodes speech"}, "5f1bbc96a22a630d3662b3fceb3160091e4bd814": {"ta_keywords": "normalizing remaining distributions;frame normalizing remaining;frame wise sequential;frame normalizing;estimates model pruning;estimation method based;characteristics frame normalizing;remaining distributions;frame wise model;remaining distributions proposed;distributions proposed method;normalizing remaining;wise sequential processing;processing provides frame;estimation method involves;estimation method;sequential processing;model estimation;distributions proposed;non dominant distributions;model pruning;wise model estimation;sequential processing provides;model pruning non;model estimation method;estimation;frame wise;based pruning non;distributions;proposes frame wise", "pdf_keywords": ""}, "bf0105bdd5b0dfc09580697739fb84590d031d0b": {"ta_keywords": "learning cognitive aspects;student cognitive abilities;cognitive aspects student;aspects student cognitive;student cognitive;cognitive abilities method;cognitive aspects;cognitive abilities;learning cognitive;fact student cognitive;problem learning cognitive;cognitive abilities apply;cognitive abilities evaluated;method learning cognitive;abilities evaluated cognitive;evaluated cognitive;cognitive;cognitive context;evaluated cognitive context;aspects student;cognitive context traditional;abilities method;problem learning;uses fact student;method problem learning;aspects;abilities method uses;learning;method learning;fact student", "pdf_keywords": ""}, "8ec127925a8680928d546df7248963e772e07a5d": {"ta_keywords": "noise screening job;noise screening;screening job candidates;candidate aggregate noise;screening job;strategy robust noise;noise level candidate;effects noise screening;screening;robust noise aggregate;individual tests optimal;noise aggregate noise;aggregate noise;determined aggregate noise;screening determined;optimal strategy screening;noise aggregate;robust noise;noise level robust;aggregate noise level;screening determined aggregate;candidates demonstrating optimal;noise level group;tests optimal;tests optimal strategy;strategy screening determined;strategy screening;study effects noise;job candidates demonstrating;noise level", "pdf_keywords": "discrimination measures fair;algorithm fairness;fairness context classification;fairness model performance;algorithm fairness context;consider fairness model;fairness model;discrimination measures;worker classification threshold;classification threshold policy;class discrimination measures;large class discrimination;ramifications algorithm fairness;unskilled worker classification;discrimination;fairness context;class discrimination;worker classification;randomized threshold policy;consider fairness;threshold policies hiring;workers threshold policy;classification threshold;policy randomized threshold;optimal employer policy;fair determination fair;policieswe consider fair;discriminate group algorithm;optimal threshold policy;discriminate group"}, "a34954d9e36ea6c57743f55124a6ae444b951c2c": {"ta_keywords": "parameters deep neural;activation predictions extracted;extract parameters neural;parameters neural;activations neural network;extracting parameters deep;neural network representer;predictions extracted parameters;neural networks pre;parameters deep;activation predictions;neural network simply;deep neural;understanding neural network;pre activation predictions;activations neural;deep neural networks;understanding neural;neural networks;neural network;accurate understanding neural;predictions extracted;networks pre activation;extracting parameters;neural;extracted parameters represented;extract parameters;combinations activations neural;extracted parameters;network representer points", "pdf_keywords": "predictions deep neural;prediction deep;prediction deep neural;neural network predictions;activation prediction neural;model prediction deep;explain predictions deep;prediction training;activation prediction;prediction neural;deep neural;predictions deep;learning deep neural;training point prediction;neural networks understood;prediction terms training;learning deep;deep neural networks;theorem deep neural;learn deep neural;particular prediction training;training point loss;activations training points;learning representation neuron;neural network terms;deep neural network;prediction neural network;pre activation prediction;learning representation;activation value training"}, "ce4eadb324026191c075f1af876403a847329d5b": {"ta_keywords": "features learning problems;features learning;learning problems efficiently;representation features vectors;features vectors;learning problems based;features vectors space;represented setvalued features;vectors space features;representation learning problems;learning problems;representation features;space features learning;based representation features;problems based representation;real world learning;representation learning;new representation learning;features;learning;setvalued features real;world learning problems;features real;setvalued features;naturally represented setvalued;vectors;space features;efficiently naturally represented;features real world;problems efficiently", "pdf_keywords": ""}, "63bc09c11a792abfcbb2d9e2809aa67929f09262": {"ta_keywords": "hypernyms word embeddings;computing hypernyms word;computing hypernyms;word embeddings;hypernyms compute hypernyms;method computing hypernyms;compute hypernyms;word embeddings method;hypernyms compute;model compute hypernyms;compute hypernyms using;hypernyms using;compute hypernyms compute;used compute hypernyms;hypernyms word;compute hypernyms projection;hypernyms;hypernyms projection;embeddings;words machine learning;hypernyms projection set;embeddings method;words machine;set words machine;embeddings method uses;np assisted graph;graph model compute;projection set words;learning framework np;machine learning framework", "pdf_keywords": ""}, "cfb1b39d1a6733f42cc5e8cfd60dc68cafa01d28": {"ta_keywords": "shape document predict;predict shape document;document predict position;document predict;coupled neural networks;shape given document;trained predict shape;predicting shape;predicting shape given;neural networks;predict shape;able predict shape;coupled neural;neural networks trained;shape document;trained predict;nonlinear dynamics;predict position relative;networks trained predict;shape document results;predict position;pair coupled neural;given document based;nonlinear dynamics approach;nonlinear;given document;predicting;problem predicting shape;neural;theory nonlinear dynamics", "pdf_keywords": ""}, "2b110fce160468eb179b6c43ea27e098757a56dd": {"ta_keywords": "automated paraphrase sentences;paraphrase sentences models;semi automated paraphrase;automated paraphrase;sentences models trained;sentences models;paraphrase sentences;sentences;generate adversarial examples;mean field models;paraphrase;generation approximate mean;approximate mean field;adversarial examples follow;adversarial examples;np complete networks;generate adversarial;adversarial;generation approximate;syntactic specifications;syntactic;target syntactic;models trained;target syntactic specifications;field models;used generate adversarial;follow target syntactic;models trained data;approximate mean;trained data", "pdf_keywords": "syntactically controlled networks;generate sentences controlled;syntactic parse model;generate sentences;syntactic parse;simple syntactic parse;syntactically controlled paraphrases;generate input sentences;predicting semantics sentences;generation syntactically controlled;syntactically controlled generation;generative parse;adversarial examples sentiment;source generative parse;generative parse generate;propose syntactically controlled;generation syntactically;paraphrastic sentence embedding;controlled paraphrases;generation generate sentences;syntactically controlled;sentences controlled;sentiment analysis textual;parse generator;syntactic transformations naturally;sentence target syntactic;parser label syntactic;syntactic modifications;sentences controlled language;high level parse"}, "cf0860ab99c63cb7cbd5317fca7cf1fe70e8fb63": {"ta_keywords": "knowledge propagation;differentiable knowledge propagation;virtual knowledge base;using virtual knowledge;virtual knowledge;knowledge propagation called;knowledge base;knowledge base kb;differentiable knowledge;questions using virtual;knowledge;method differentiable knowledge;hop questions using;based use multidimensional;kb method based;multidimensional;propagation;index drkit trained;use multidimensional;questions using;multi hop questions;use multidimensional sparse;multidimensional sparse;multidimensional sparse matrix;hop questions;propagation called drkit;kb method;using virtual;sparse matrix index;complex multi hop", "pdf_keywords": "large corpus queries;corpus virtual knowledge;large text corpus;learning indexed corpus;nlp knowledge bases;question retrieval;search mentions corpus;model large corpus;mentions entities corpus;large corpus;approach question retrieval;mentions corpus entities;entities corpus;entities text corpus;nlp knowledge;question answering;indexed corpus based;corpus entities;learning task text;task text corpus;mentions corpus;embed entities knowledge;knowledge train mention;text corpus virtual;hop question answering;virtual knowledge base;knowledge bases;indexed corpus;corpus queries;search contextual representations"}, "024091a3c0223f27d6456b1a27db18fb08d41e5a": {"ta_keywords": "binarized neural network;neural network joint;based binarized neural;contrastive estimation np;binarized neural;npt noise contrastive;joint model np;noise contrastive estimation;trained using np;np based binarized;np npt trained;training np based;npt trained using;training np;neural network;method training np;contrastive estimation;noise contrastive;npt trained;estimation np npt;network joint model;np npt noise;model np npt;network joint;estimation np;model np;npt np based;np based np;using np npt;based np npt", "pdf_keywords": ""}, "629323c5b9f7c64afac9300212538e488569bd1e": {"ta_keywords": "lexical knowledge dictionaries;ontology using word;dictionaries establishing semantic;structured lexical knowledge;knowledge dictionaries establishing;word embeddings;knowledge dictionaries;word embeddings presented;using word embeddings;establishing semantic relations;lexical knowledge;induction ontology using;semantic relations;semantic relations using;establishing semantic;induction ontology;semantic;relations using word;ontology using;ontology;extracts structured lexical;structured lexical;dictionaries establishing;dictionaries;lexical;embeddings presented proposed;using word;embeddings presented;embeddings;knowledge", "pdf_keywords": ""}, "33aa6c70eac0e4b7eb28d8386e5e4113fdd55203": {"ta_keywords": "automatic question answering;question answering;history entrance exam;automatic question;question answering used;based automatic question;english questions world;exam;multiple english questions;entrance exam;questions world;questions world history;ntcir based automatic;exam evaluated using;exam evaluated;ntcir;entrance exam evaluated;ntcir based;world history entrance;answer multiple english;english questions;history entrance;answering;terms accuracy training;questions;answering used;world examples evaluation;answer multiple;multiple english;answering used answer", "pdf_keywords": ""}, "2e820673ca861a9ece8d36f2b93793b5d2c7e1da": {"ta_keywords": "lightweight block ciphers;block ciphers;block ciphers able;ciphers able achieve;structure encryption;modular structure encryption;encryption;ciphers;lightweight block;structure encryption process;ciphers able;encryption process;high speed data;class lightweight block;high throughput algorithms;achieve high throughput;encryption process use;iteration high throughput;high rate data;speed data low;throughput high rate;high throughput;throughput high;new class lightweight;throughput algorithms;cost high throughput;throughput algorithms allows;high throughput high;algorithms based modular;data low cost", "pdf_keywords": ""}, "49d415cf593be38c6cd97a183dadc7d7b48bab72": {"ta_keywords": "innovations productivity;innovations productivity labor;impact innovations productivity;innovations significantly;innovations compare empirical;innovations;growth evidence innovations;innovations compare;innovations significantly pervasive;intelligence related innovations;related innovations;innovations rigorously constrain;related innovations rigorously;productivity labor demand;related innovations compare;innovations rigorously;productivity labor;productivity;significantly pervasive innovations;pervasive innovations significantly;literature innovations significantly;evidence innovations significantly;impact innovations;firm growth evidence;progress artificial intelligence;data literature innovations;firm growth;study impact innovations;growth evidence;pervasive innovations", "pdf_keywords": ""}, "2225950d1d3e02bc0d88a0c78325d00e0122b576": {"ta_keywords": "recognition overlapping speech;overlapping speech signals;overlapping speech;joint training framework;collection overlapping speech;speech signals framework;data joint training;speech signals end;recognition overlapping;speech signals;signals framework trained;framework recognition overlapping;training framework recognition;training framework;joint training;joint training real;optimized joint training;novel joint training;overlapping;joint training realistic;training framework trained;training realistic data;signals framework;signals end;data joint;signals end end;recognition;realistic data joint;speech;training real world", "pdf_keywords": ""}, "05fb5a180214bf092eeda30baf9f16fb6bd15727": {"ta_keywords": "modifying durational patterns;correcting durational patterns;modifying durational;speech based direct;waveform modification proposed;waveform modification;native speech based;direct waveform modification;speech based;flexibly modifying durational;method correcting durational;durational patterns avoiding;correcting durational;based direct waveform;non native speech;synthesis process waveform;durational patterns non;durational patterns;waveform;direct waveform;waveform segments suffering;waveform segments;speech read japanese;native speech;process waveform segments;analysis synthesis;analysis synthesis process;process waveform;durational;speech read", "pdf_keywords": ""}, "649c1148439a4e875dab414ba67bf8c80350af4a": {"ta_keywords": "neural semantic parsers;neural semantic parser;semantic parsing tasks;semantic parsers;semantic parser;semantic parser maps;semantic parsing;formal meaning representations;different semantic parsing;existing neural semantic;parser maps natural;neural semantic;parsers;parser uses transition;based neural semantic;transition based neural;parser;parsing tasks;natural language utterances;maps natural language;parsing;parsing tasks effective;parser uses;parser maps;representations mrs parser;abstract syntax description;natural language;syntax description language;utterances formal meaning;language utterances formal", "pdf_keywords": "neural semantic parser;transition based parser;semantic parsers;semantic parser;learning semantic parsers;semantic parsers code;semantic parsing;parsing modeling language;semantic parser maps;accuracy semantic parsing;semantic parsing demonstrate;semantic parsing code;formal meaning representations;parsing modeling;semantic parsing table;semantic parsingwe present;semantic parsingwe;just semantic parsing;set semantic parsing;parser use learning;generated parser;neural semantic;abstract syntax parser;parsers;different semantic parsingwe;syntax parser;based neural semantic;parser;encoding semantic representation;based parser"}, "86d84c1c9b0a500f930696ab27c83a4b30477560": {"ta_keywords": "paraphrastic sentence embeddings;learning paraphrastic sentence;learning paraphrastic;paraphrastic text based;paraphrastic text;powerful paraphrastic text;efficiently embed sentence;sentence embeddings;embed sentence word;model learning paraphrastic;baselines crosslingual tasks;paraphrastic;embed sentence;paraphrastic sentence;sentence embeddings able;baselines crosslingual;powerful paraphrastic;simple powerful paraphrastic;crosslingual tasks;crosslingual tasks model;sentence word level;art baselines crosslingual;embeddings;efficiently embed;embeddings able outperform;sentence word;text based learning;crosslingual;corpora;able efficiently embed", "pdf_keywords": "paraphrastic sentence embeddings;interlingual embeddings demonstrate;paraphrastic sentence embedding;interlingual embeddings;learning paraphrastic sentence;learning paraphrastic;sentence embeddings scalable;semantic crosslingual tasks;similarity cross lingual;semantic crosslingual task;semantic similarity cross;performance sentence embeddings;semantic textual similarity;lingual semantic textual;sentences parallel data;semeval semantic crosslingual;task sentence embeddings;sentence embeddings directly;derived interlingual embeddings;embeddings using corpus;parallel corpus mining;sentence embeddings languages;sentence embedding method;sentence embeddings;cross lingual semantic;sentence embeddings using;parallel corpus;learn paraphrastic;learn paraphrastic sentence;semantic crosslingual"}, "65c53ed3575e160eb1e7d0a516353ba52de7e7e5": {"ta_keywords": "identification bid leakage;bid leakage detection;identify bid leakage;bid leakage bid;leakage bid;sealed bid auctions;leakage bid leakage;bid leakage price;bid leakage;exposed bid leakage;auctions exposed bid;russian procurement auctions;bid auctions;price sealed bid;identify bid;auctions exposed;procurement auctions 2014;exposed bid;sealed bid;bid auctions proposed;procurement auctions;leakage price sealed;approach identify bid;auctions 2014;identification bid;based identification bid;auctions 2014 2018;leakage price;auctions;auctions proposed", "pdf_keywords": "bid leakage auctions;leakage auction participants;bid leakage auction;estimating leakage auction;leakage auctions;leakage auctions using;leakage auction phenomenon;leakage auctions results;auctions procurer leaks;leakage auction;leakage fair auctions;leaks opponents bids;probabilities bid leakage;corrupted auctions participants;probability bid leakage;leakage probability bid;sealed bid auctions;bid auctions procurer;bid leakage corruption;bid leakage fair;bid leakage estimation;russian procurement auctions;bidders auctions;bid leakage probability;based bid leakage;bidder winning auction;corruption patterns auctions;auctions auctions corrupted;identifying bid leakage;estimate bid leakage"}, "a9e6222e71dd101d444b7192b3a0636c71edb0a4": {"ta_keywords": "knowledge base content;base content discovery;content discovery;content discovery engine;contextual content;contextual content text;characterizing contextual content;novel knowledge base;knowledge bases;using knowledge bases;knowledge base;text presence unstructured;knowledge bases approach;questions using knowledge;base content;content text;unstructured background approach;characterizing contextual;content;content text presence;contextual;discovery engine;using knowledge;unstructured background;problem characterizing contextual;use novel knowledge;novel knowledge;knowledge;unstructured;text", "pdf_keywords": ""}, "6536f36648d39f0f9f6105562f76704fcc0b19e8": {"ta_keywords": "persistent spatial semantic;spatial semantic representation;spatial semantic;semantic tasks;describing semantic tasks;propose persistent spatial;performs hierarchical reasoning;persistent spatial;semantic tasks enables;semantic representation;agent performs hierarchical;hierarchical reasoning;describing semantic;semantic representation method;method describing semantic;semantic;hierarchical reasoning evaluate;spatial;performs hierarchical;hierarchical;agent;persistent;agent performs;propose persistent;tasks enables construction;representation method describing;describing;tasks;enables construction agent;construction agent", "pdf_keywords": "representation hierarchical robotics;instructions actions hierarchical;capable describing tasks;actions hierarchical;vision language tasks;representation hierarchical;hierarchical representation natural;instructions actions 3d;hierarchical robotics task;spatial semantic representation;persistent spatial semantic;map representations actions;hierarchical representation;navigation manipulation tasks;propose hierarchical representation;mobile manipulation tasks;manipulation tasks;manipulation tasks proposed;natural language commands;level natural language;spatial semantic;semantic voxel;representation hierarchical representation;instruction integrates voxel;semantic voxel map;describing learning tasks;manipulation tasks use;embodied agents;state representation hierarchical;actions hierarchical model"}, "2c0f2a03c3a427cc61359b5e2c31cfefe9850a31": {"ta_keywords": "unsupervised information extraction;information extraction;extracting sets entities;concept names clusters;information extraction method;web using unsupervised;extracting sets;names clusters using;method extracting sets;unsupervised information;using unsupervised information;names clusters;information contained clusters;relies clustering terms;concept names;clustering terms;clusters using information;entities web;clustering terms open;sets entities web;assigning concept names;extracting;concept instance pairs;method extracting;using unsupervised;concept instance;novel method extracting;entities web using;method extracting large;extraction method relies", "pdf_keywords": "websets extract concept;concept clustering efficient;concept names clusters;concept clustering;term clusters hypernyms;clustering terms unstructured;terms unstructured web;coherent concept clustering;technique named websets;tables corpus clustering;clustering web entities;extracting concept instance;knowledge extraction tasks;unstructured web set;knowledge extraction;extract concept instance;named websets websets;named websets;extracts entity sets;hypernyms data websets;extracting tables corpus;nonlinear knowledge extraction;unstructured text clusters;websets extract;websets websets extract;extracting concept;term clusters;unstructured corpus information;clustering entities tables;clusters hypernyms data"}, "ed2cc779c7eb0004bd6dd50538a2cafca092c94f": {"ta_keywords": "tagging historical texts;automatically tagging historical;tagging historical;annotated historical texts;annotated historical;methods tagging historical;automatically tagging;historical texts;historical texts evaluates;tagging;historical texts 15th;method automatically tagging;historical texts important;texts 15th century;texts 15th;annotated;methods tagging;15th century;texts;evaluates methods tagging;historical;texts important;texts important class;texts evaluates;texts evaluates methods;analysis early;evolution paper;data analysis early;structure evolution paper;century", "pdf_keywords": ""}, "5bcd9117899bc2c91db83532dcf587b9d8f8888b": {"ta_keywords": "laws states earth;earth laws states;parts earth laws;formulate laws;formulate laws laws;earth laws;teaching laws states;states earth written;way formulate laws;laws states;laws laws states;laws states united;states parts earth;teaching laws;states earth;earth written form;laws laws;parts earth written;laws;suitable teaching laws;earth written;states parts;parts earth;formulate;states;written form;united states parts;written form suitable;earth;states united states", "pdf_keywords": ""}, "3d1cfefdbe40f7535ada772c260c192bb63bb9fe": {"ta_keywords": "grained textual matching;document similarity training;grained document similarity;textual matching;document similarity;approach document similarity;textual matching model;similarity training;similarity training based;document similarity leverage;grained textual;fine grained textual;textual supervision;textual supervision novel;fine grained document;grained document;textual;similarity;form textual supervision;source supervision train;source supervision;similarity leverage natural;texts leverage optimal;occurring source supervision;similarity leverage;single aspects texts;texts;novel form textual;aspects texts leverage;matching", "pdf_keywords": "learning aspect matching;aspect similarity tasks;learning document similarity;contextualized document representations;document similarity structured;documentlevel similarity train;similarity structured documents;trained leveraging citation;leveraging citation contexts;grained document similarity;aggregates documentlevel similarity;characterizing document aspects;document representations fast;abstract aspect similarity;document similarity evaluation;document representations train;aspect similarity;tasks predicting citations;document similarity benchmarks;aspect matching methods;textual aspects;grained sentence matching;document similarity provides;cited papers corpus;document similarity model;document representations;document similarity largescale;alignments textual aspects;documentlevel similarity;learning characterizing document"}, "5e74d4e041a25e7752a596e2891975df5ba65aa2": {"ta_keywords": "singlechannel speech enhancement;speech enhancement deep;beamforming speech;mvdr beamforming speech;speech enhancement;beamforming speech using;steers beamforming speech;speech noise spatial;beamforming speech analyze;speech analyze mask;obtain speech noise;speech using masks;singlechannel speech;use singlechannel speech;masks obtain speech;speech noise;estimation steers beamforming;minimum variance distortionless;noise spatial covariance;beamforming;response mvdr beamforming;mask prediction;obtain speech;mask prediction affects;enhancement deep network;analyze mask prediction;mvdr beamforming;noise spatial;masks use singlechannel;variance distortionless response", "pdf_keywords": ""}, "a0ab4106dabd6bc067c7b3e4db06807e2c0f6036": {"ta_keywords": "structure natural language;language form nonlinear;natural language form;natural language;simple efficient learning;efficient learning;small scale queries;efficient learning framework;learning;low cost nonlinear;nonlinear;learning framework;characterizing structure natural;structure natural;learning framework problem;nonlinear dynamics;nonlinear dynamics framework;cost nonlinear dynamics;queries;form nonlinear dynamics;language form;queries illustrate;cost nonlinear;example large scale;form nonlinear;scale queries;large scale pretraining;dynamics;characterizing structure;simple efficient", "pdf_keywords": "pretrained language models;large language models;accelerate learning language;learning language models;large scale language;language models solving;customized language models;driven language modeling;large general corpus;language modeling tlm;language models;language modeling;task driven language;scale language models;language models bert;training large language;bioinformatics train language;objective language modeling;efficient framework pretraining;language models paper;language modeling objective;general corpus training;pretraining models terms;language models domains;pretraining pretrained language;pretrained models;training corpus;language models domain;pretraining large scale;pretraining customized language"}, "a4f2e6c38454c9e7b4068a456813d622b91f2663": {"ta_keywords": "measurement speech diadochokinetic;speech diadochokinetic ddk;speech diadochokinetic;diadochokinetic ddk rate;measurement speech;diadochokinetic ddk;problems measurement speech;diadochokinetic;ddk rate identify;ddk rate;double pulse protocol;pulse protocol;double pulse;measurement form data;single short pulse;containing double pulse;protocol measurement form;pulse pulse;rate identify;pulse;pulse containing double;used protocol measurement;ddk;short pulse containing;protocol measurement;rate identify problems;pulse containing;measurement form;short pulse pulse;pulse containing single", "pdf_keywords": ""}, "d408be961d0db8b97c0ca6b2fc7afd3c9dc914e7": {"ta_keywords": "multimodal mobility management;multimodal mobility platform;modular multimodal mobility;mobility platform manageability;mobility platform;multimodal mobility;mobility management capabilities;mobility management;mobility;application modular multimodal;platform manageability;platform manageability extensible;modular multimodal;application modular;extensible application modular;platform architecture resilience;challenges facing platform;platform architecture;capabilities platform;modular;platform;manageability extensible;management capabilities platform;architecture resilience challenges;manageability;multimodal;architecture resilience;resilience challenges facing;manageability extensible extensible;extensible application", "pdf_keywords": ""}, "4fe70c172cc38c2eb15103f0f1eac4e6766c60e6": {"ta_keywords": "stochastic power spectrum;estimation stochastic power;power spectrum noisy;estimated stochastic power;estimating power spectrum;noisy signal generated;signal generated noisy;spectrum generated noisy;generated noisy signal;noisy signal method;spectrum noisy noisy;spectrum estimated stochastic;noisy signal noisy;noisy noisy signal;signal noisy signal;power spectrum estimated;noisy signal;stochastic power;spectrum noisy;signal noisy;estimation stochastic;noisy signal resulting;generated noisy;power spectrum generated;estimated stochastic;power spectrum;resulting power spectrum;method estimating power;noisy noisy;based estimation stochastic", "pdf_keywords": ""}, "efe9fe804f34b18524708b18293508191bda78eb": {"ta_keywords": "grid redundant hardware;faults energy consumption;permanent faults energy;transient permanent faults;task management grid;grid decouples tasks;consumption grid redundant;redundant hardware;energy consumption grid;grid redundant;faults energy;redundant hardware used;permanent faults;dynamic task management;management grid grid;tasks core nodes;management grid;consumption grid;decouples tasks core;faults;grid grid decouples;susceptible faulty execution;tasks core;grid grid grid;dynamic task;grid grid;grid decouples;grid grid approach;decouples tasks;tolerating transient permanent", "pdf_keywords": ""}, "395044a2e3f5624b2471fb28826e7dbb1009356e": {"ta_keywords": "supervised textual embeddings;textual embeddings;supervised textual;textual embeddings use;supervised embeddings;sentence similarity;sentiment classification task;supervised embeddings outperform;sentence similarity entailment;tasks sentence similarity;architecture supervised embeddings;sentiment classification;embeddings outperform;embeddings;similarity entailment;similarity entailment demonstrate;art supervised textual;performance sentiment classification;embeddings outperform state;embeddings use;textual;embeddings use state;state art supervised;similarity;architecture supervised;classification task;sentiment;supervised;entailment;performance sentiment", "pdf_keywords": "sentence neural network;predicting sentence similarity;sentence embeddings based;paraphrastic sentence embeddings;sentence embeddings;train word embeddings;text deep neural;word embeddings;sentences based deep;sentence embeddings used;predicting semantics sentences;word embeddings thorough;predict similarity sentences;sentence neural;universal sentence embeddings;trained annotated textual;textual similarity tasks;embeddings based compositional;classify sentences text;tasks paraphrase classification;sentence similarity;word embeddings use;text representations;textual similarity datasets;paraphrase classification;generating paraphrastic sentence;similarity natural language;compositional models trained;tasks sentence similarity;sentence similarity natural"}, "14551d2bf2584bb1ea7ad69f9a64419bab82bb6e": {"ta_keywords": "detection sound events;sound events using;detection sound;features audio data;sound events;local features audio;sound events experimental;features audio;learning data augmentation;types sound events;audio data effectively;audio data;data augmentation proposed;data augmentation;method detection sound;semi supervised learning;semi supervised;using semi supervised;audio;convolution augmented transformers;combines convolution augmented;convolution augmented;sensor captures;augmented transformers kuramoto;supervised;supervised learning;convolution;augmentation proposed method;events using semi;sensor captures global", "pdf_keywords": ""}, "469ad889bd628e2cf46424f7097c4830719ec740": {"ta_keywords": "talker intelligibility space;estimation talker intelligibility;talker intelligibility;intelligibility space using;intelligibility space;estimation talker;automatic estimation talker;dimensional space intelligibility;improved performance vowel;space intelligibility scoring;performance vowel score;transcription accuracy;vowel score;intelligibility scoring unsupervised;human transcription accuracy;intelligibility scoring;expanded space representations;intelligibility score based;transcription accuracy improved;performance vowel;generate intelligibility score;space intelligibility;space representations including;vowel score shown;space representations;scoring unsupervised mappings;transcription;intelligibility score;score based human;vowel", "pdf_keywords": ""}, "d9944e13a38e5ca685985c9b5c050ec6d300e104": {"ta_keywords": "estimation random walk;walk estimation probability;walk estimation random;random walk estimation;walk equivalent estimation;walk estimation;random walk based;random walk equivalent;concept random walk;probability equivalent estimation;estimation probability;estimation probability equivalent;random walk;equivalent estimation probability;estimation random;method estimation probability;estimation probability given;equivalent estimation random;walk equivalent;method estimation random;estimation;walk based concept;equivalent estimation;walk;method estimation;walk based;new method estimation;probability propose method;propose method estimation;given probability based", "pdf_keywords": ""}, "ba602ea9aaecab5a3ad243211f110ae7db4cc66a": {"ta_keywords": "minimizers performative risk;repeated risk minimization;risk minimization;risk minimization provide;local minimizers performative;multiple local minimizers;local minimizers;minimizers performative;convergence repeated risk;minimizers;performative risk;minimization provide;minimization;minimization provide sufficient;various equilibria setting;repeated risk;equilibria setting;performative risk motivated;performative alignment;situations initial conditions;various equilibria;risk;attraction various equilibria;geometric condition convergence;performative alignment provides;multiple local;equilibria;characterizing region attraction;region attraction;conditions characterizing region", "pdf_keywords": ""}, "ef6a4d8bf248944ca1d0cfdc107d3bb107f57bff": {"ta_keywords": "zeeman field stability;field stability xmath0he;stability xmath0he ep;xmath0he ep cluster;stability xmath0he;zeeman field strength;strength stability xmath0he;field stability;effect zeeman field;field strength stability;effect field stability;zeeman field;xmath0he ep;ep cluster framework;cluster framework standard;ep cluster determined;ep cluster;ep cluster minimized;cluster determined interplay;cluster determined;cluster framework;choice zeeman field;stability;effect zeeman;cluster;xmath0he;cluster minimized;strength stability;interplay field strength;cluster minimized suitable", "pdf_keywords": ""}, "6c68866e6486923d2e8b999de57d450c9d4febab": {"ta_keywords": "neural machine translation;machine translation snp;improve translation quality;translation snp phrase;machine translation nmt;translation quality;translation snp;machine translation;translation quality different;successfully improve translation;soft forced decoding;improve translation;phrase based decoding;phrase based soft;snp phrase based;translation nmt outputs;snp phrase;decoding using phrase;decoding cost rerank;forced decoding cost;improve neural machine;cost rerank neural;translation nmt;forced decoding using;forced decoding;based decoding cost;rerank neural;decoding cost;rerank neural machine;improve neural", "pdf_keywords": ""}, "c2b22a18ea2ed444c6c1f5bb27ab55bda2b44567": {"ta_keywords": "speech translation systems;language emphasis prediction;information speech translation;distribution information speech;translation systems;speech translation;emphasis prediction proposed;emphasis prediction;improves target language;translation systems model;target language;target language emphasis;conditional random fields;information speech;speech;language emphasis;based conditional random;neural network input;random fields crfs;neural network;conditional random;random fields;long distance dependencies;using neural;distance dependencies;using neural network;language;dependencies proposed model;long distance dependence;neural", "pdf_keywords": ""}, "b799d66c710dd82a1b925b9c31e55a0d2d99b624": {"ta_keywords": "model urban dynamics;recovers urban dynamics;urban dynamics human;urban dynamics;urban dynamics capturing;rich urban dynamics;dataset recovers urban;urban dynamics different;mobility dataset;model urban;life mobility dataset;real life mobility;mobility dataset recovers;method model urban;semantics rich urban;dynamics human activities;urban;recovers urban;life mobility;mobility;activities based state;population flows;0793 recovers urban;human activities;human activities based;rich urban;learns semantics;learns semantics rich;dynamics human;based state sharing", "pdf_keywords": ""}, "0c39c0dc296a902e4a5eb85182209f7b9e6053b0": {"ta_keywords": "dynamic network architectures;novel dataflow;present novel dataflow;framework dynamic network;dynamic network structure;dynamic network;novel dataflow based;dataflow based;dataflow;dataflow based framework;frameworks dynamic nns;represents dynamic network;dynamic nns;network architectures;network architectures called;scheduling execution vertex;backpropagation scheduling execution;network structure vertex;performs backpropagation scheduling;backpropagation scheduling;dynamic nns suzaku;execution vertex;network structure;execution vertex function;architectures called cavs;scheduling execution;dynamic;batched execution opportunities;vertex function performs;nns", "pdf_keywords": "deep dataflow graphs;vertex deep learning;deep dataflow;computing deep dataflow;graph vertex deep;programming vertex centric;vertex centric programming;vertex deep;dynamic dataflow graphs;dynamic deep learning;symbolic programming vertex;computing deep;dataflow graphs train;dataflow graphs dynamic;programming vertex;model graph computing;engine computing vertex;recognition deep dataflow;training dynamic nns;deep learning nn;new graph computing;dataflow graphs nns;backpropagation arbitrary graphs;algorithm graph dynamicallywe;graph computing;nn architecture dynamically;implements vertexcentric programming;vertexcentric programming interface;performing vertex centric;vertexcentric programming"}, "57026b2d45fa59c6326b5a1d2e27626403f083ba": {"ta_keywords": "artificial intelligence ethics;artificial intelligence educators;artificial intelligence course;ethics general artificial;intelligence ethics;stand artificial intelligence;intelligence course teach;use artificial intelligence;integrate artificial intelligence;intelligence educators provide;intelligence ethics general;intelligence course;intelligence educators;artificial intelligence;general artificial intelligence;ethics;resources use artificial;educators provide suggestions;use artificial;course teach;intelligence;provide practical case;ethics general;teach stand artificial;educators provide;artificial;educators;teach;case studies links;general artificial", "pdf_keywords": "artificial intelligence ethics;ethics artificial intelligence;ai ethics;ai ethics use;intelligence ai ethics;teaching ethics artificial;ethics artificial;ethics general artificial;course artificial intelligence;robotic intelligence called;artificial intelligence important;artificial intelligence ai;intelligence ai;robotic intelligence;artificial intelligence educators;autonomous systems responsibilitieswe;intelligence called robot;intelligence ethics;artificial intelligence course;use artificial intelligence;artificial intelligence courses;stand artificial intelligence;artificial intelligence use;autonomous systems responsibilities;ai;future artificial intelligence;artificial intelligence;integrate artificial intelligence;learning associated ethical;intelligence systems values"}, "653add540adae12491fade7e18ec4e1e4288b4a7": {"ta_keywords": "advising course planning;students advising course;course planning using;planning using software;students advising;learning process electronic;inform teaching learning;advising course;study performance undergraduate;undergraduate students advising;teaching learning process;inform teaching;software used inform;course planning;used inform teaching;performance undergraduate students;using software tool;using software;software tool software;communicate teaching learning;study performance;performance undergraduate;support teaching learning;software designed;software tool;teaching learning;learning process;software used;tool software designed;software", "pdf_keywords": "automated advising undergraduates;automated advising university;automated advising;development automated advising;support automated advising;recommendation systems student;given students advisors;academic advice students;academic advising;students think recommendations;advising undergraduates;guide student choosing;students advisors;academic advising utilizes;advisors recommend courses;approach academic advising;explain students recommended;student goals statistical;academic advising university;student advisor;advising university;student guide student;recommendations academic advisor;advice students;explains students recommended;academic advising present;based predicted grades;implementation academic advising;student advisor expected;advising effective"}, "6fe62b967376361d7cd55e1033ab968895841d67": {"ta_keywords": "deep learning text;learning text mining;focused concept miner;concepts text data;learning text;text mining;fcm interpretable deep;concepts text;text mining algorithm;concept miner;corpus level concepts;interpretable deep learning;concept miner fcm;interpretable deep;fcm text data;deep learning;online newsgroups crowdfunding;predictive performance fcm;level concepts text;corpus;extract coherent corpus;performance fcm text;fcm text;coherent corpus;newsgroups crowdfunding;concept;concepts;newsgroups crowdfunding platform;text data online;fcm interpretable", "pdf_keywords": ""}, "29001ac04e61dfffb8e24ffd3e351ece12ce44af": {"ta_keywords": "frequency domain masks;mixture signal;mixture signal present;estimates phase mixture;phase mixture signal;signal representations;estimate phase signal;source separation;representations estimate phase;separation providing estimates;masks based discrete;source separation providing;estimation time frequency;discrete representations signal;signal representations used;wsj0 2mix dataset;signal present training;representations signal representations;domain masks based;efficacy source separation;representations signal;estimate phase;providing estimates phase;domain masks;phase signal;time frequency domain;phase mixture;masks based;estimates phase;training inference schemes", "pdf_keywords": "softmax activation mask;masks deep learning;speech enhancement source;softmax activation magnitude;channel speech separation;speech enhancement;frequency masks deep;speech separation based;approach speech separation;approaches speech enhancement;mask based magnitude;source separation representations;speech separation;magnitude mask;model magnitude mask;magnitude mask ability;softmax activation;introduced softmax activation;estimate phase mask;speech separation uses;softmax;use complex mask;optimizing amplitude mask;enhancement source separation;reconstructing magnitude target;deep neural;multi channel speech;complex mask defined;softmax layer;distribution magnitude mask"}, "5dce0fd43a21825bebd8121fd0a28155d524c44c": {"ta_keywords": "increasing electron density;gaussian noise dynamics;effect gaussian noise;gaussian noise;increase electron density;electron density;dynamics dimensional electron;dimensional electron gas;noise dynamics;electron gas presence;field increase electron;noise dynamics dimensional;electron gas;increase electron;electron density expense;increasing electron;strong magnetic field;magnetic field effect;effect gaussian;dimensional electron;presence strong magnetic;electron;strong magnetic;gaussian;magnetic field;expense increasing electron;noise;magnetic;study effect gaussian;gas presence strong", "pdf_keywords": ""}, "4b2d583e22f378f9104814d9f63cda411ddd5825": {"ta_keywords": "lingual sememe prediction;correlations sememes multilingual;sememes multilingual words;semantic space sememe;sememes multilingual;sememe prediction;cross lingual sememe;lingual sememe;space sememe prediction;sememe prediction experiments;low dimensional semantic;multilingual words low;model correlations sememes;correlations sememes;multilingual words;dimensional semantic space;cross lingual;lingual;multilingual;words low dimensional;methods cross lingual;sememes;dimensional semantic;semantic space;space sememe;sememe;semantic;prediction;words low;prediction experiments", "pdf_keywords": ""}, "f1513d72cb5dd6d70541cce0da36b77467128d13": {"ta_keywords": "entrance exam;method entrance exam;exam;novel method entrance;method entrance;present novel method;entrance;novel method;method;present novel;novel;present", "pdf_keywords": ""}, "838fbfd9066dbbac6c10059c5b183046fb1cd9d1": {"ta_keywords": "deep active learning;active learning natural;active learning;study deep active;deep active;learning natural language;empirical study deep;natural language processing;classic uncertainty sampling;learning natural;study deep;uncertainty sampling;learning;large scale empirical;natural language;tasks dataset;deep;outperforms classic uncertainty;significantly improves baselines;multiple tasks dataset;tasks dataset multiple;active;language processing;improves baselines;uncertainty estimates;language processing addressing;uncertainty estimates provided;using uncertainty estimates;baselines usually outperforms;addressing multiple tasks", "pdf_keywords": "active learning task;sampling active learning;active learning;utility active learning;active learning approaches;method active learning;approach active learning;active learning dal;cases active learning;generalizations joint learning;joint learning dws;learning rns generalizations;active learning based;uncertainty based learning;datadependence deep;joint learning;uncertainty reinforcement learning;learning task;deep learning natural;mitigating datadependence deep;bias training;learning dal deep;dal deep learning;deep learning;learning natural language;learning framework describing;reinforcement learning rns;learning dws;datadependence deep learning;bayes backprop variational"}, "5693c74eb8ffde1490ba480fdc963f008243906a": {"ta_keywords": "automatically tag tags;automatically automatically tag;automatically tag;automatically identify tags;tags automatically;tag automatically;tag tag automatically;automatically reassign tags;tag automatically automatically;collection tags automatically;weight collection tags;tags automatically reassign;tags collection;tag tags collection;identify tags collection;tags using;identify tags;tags using user;tag tags;tags;tags collection tags;tags user framework;tags tag;automatically automatically identify;tags user;tags tag tag;reassign tags;reassign tags user;collection tags using;collection tags", "pdf_keywords": ""}, "a8239258abded4f08d1bf270c2e86662f4dc1760": {"ta_keywords": "student learns problem;learns problem solving;learning problem solving;learning problem knowledge;solving knowledge;learns problem;problem knowledge prior;problem solving knowledge;problem introduction learning;student learns;learning knowledge;synthetic student modeling;student learning complex;student learning;solving knowledge prior;introduction learning problem;learning problem;prior knowledge learning;predicting student errors;learning knowledge prior;problem knowledge;learning complex problem;knowledge learning problem;model learning called;knowledge learning;essential learning problem;student modeling;learns;learning called;student modeling error", "pdf_keywords": ""}, "a182a8a0678857df5c513d52469fa707c32e69ec": {"ta_keywords": "underlying translation rules;translation rules;space rule selection;based rule selection;rule selection;syntax based rule;translation rules test;rule selection csrs;selection rule selection;rule selection msm;rule selection rule;rule selection performed;syntax based;entropy based rule;selection rule;space rule;translation tasks outperforms;continuous space rule;translation tasks;set underlying translation;different translation tasks;underlying translation;sentence context set;model different translation;domains sentence context;model syntax based;based rule;rules test;separating domains sentence;syntax", "pdf_keywords": ""}, "4e3935ef7da6bcbb202ec7f8b285c313cadcd044": {"ta_keywords": "information seeking papers;information seeking iqp;information seeking;questions information contained;questions information;iqp questions document;information contained document;questions document;document answer questions;grounded information seeking;document grounded information;seeking iqp questions;papers model seeks;document evidence information;model information seeking;seeking papers model;information contained;model seeks answer;questions document grounded;evidence information;document answer;following questions information;information;evidence information contained;seeking iqp;iqp questions;document evidence;contained document answer;grounded information;contained document evidence", "pdf_keywords": "question answering;seeking question answering;answer large corpus;natural language queries;processing nlp papers;evidence answering questions;nlp papers;natural language processing;processing nlp;question answering quasar;nlp present experimental;access natural language;evidence answering;nlp papers collect;processing nlp present;experts natural language;large corpus text;language processing nlp;comprehend natural language;collect evidence answering;document seeking;answering questions;language queries extract;question writing task;large sample documents;papers use neural;questions search information;document seeking abstract;natural language;optimally answer questions"}, "85e148ac629b1b38556c5fe5f8d657f2eb01a701": {"ta_keywords": "spin orbit interaction;interaction spin orbit;orbit interaction spin;coupled spin orbit;spin orbit coupled;orbit coupled spin;dynamics spin orbit;control spin orbit;effect spin orbit;interaction dynamics spin;suppression spin orbit;spin orbit;enhancement spin orbit;interaction spin;coupled spin;dynamics spin;orbit interaction dynamics;orbit interaction;way spin orbit;orbit interaction responsible;orbit interaction used;control spin;orbit interaction specific;effect spin;suppression spin;orbit coupled;enhancement spin;responsible enhancement spin;responsible suppression spin;specific way spin", "pdf_keywords": ""}, "a997d6e253f08a3e589432c611d6d2a3097d7629": {"ta_keywords": "collaborative research tool;paper tool collaborative;tool collaborative research;collaborative document;collaborative research;collaborative document repository;research paper tool;tool collaborative;style collaborative document;research tool;collaborative;research tool collection;research papers;written style collaborative;papers written annotated;novel research paper;research paper;research papers written;tool collection research;style collaborative;annotated users tool;written annotated users;research;annotated users;collection research papers;written annotated;researchexchange cs;document repository;novel research;users tool written", "pdf_keywords": ""}, "3ec37205c9201fc891ab51da200e361fdc34bfb3": {"ta_keywords": "deep learning architectures;reading documents;deep learning;reading documents choice;reading documents fundamental;reading documents process;documents process reading;learning architectures;purpose reading documents;process reading documents;learning architectures purpose;architectures purpose reading;construction deep learning;documents process;reading;documents fundamental;documents;documents fundamental aspect;strategies construction deep;documents choice best;strategies process reading;documents choice;process reading;purpose reading;learning;architectures;architectures purpose;construction deep;deep;process crucial", "pdf_keywords": "trained word embeddings;embeddings purpose deep;word embeddings present;word embeddings;reading documents answering;comprehension documents increasingly;learning deep learning;contextual embeddings document;learning deep;deep learning deep;characterizing contextual embeddings;deep learning quality;deep learning;contextual embeddings;purpose deep learning;embeddings document;comprehension documents;train deep learning;corpora train deep;deep learning architectures;trained corpora;novel deep learning;reading comprehension current;embeddings present comparative;trained corpora cbrn2;embeddings present;impact deep learning;rc deep learning;embeddings purpose;pre trained corpora"}, "cfad4dc5f1f7fcaf7ca318acf672ad92d47f8413": {"ta_keywords": "hiring paradox fundamental;hiring paradox;empirical measures hiring;hiring function empirically;measures hiring rate;hiring rate;hiring practices market;hiring rate based;hiring function market;rate based hiring;fact hiring decisions;intrinsic conflict hiring;measures hiring;hiring decisions;conflict hiring practices;conflict hiring;based hiring function;hiring decisions based;hiring function;hiring practices;tension hiring rate;hiring;significant tension hiring;fact hiring;tension hiring;based hiring;highlighted fact hiring;economics intrinsic conflict;economics;field economics", "pdf_keywords": "plagues hiring algorithms;automated hiring decisions;algorithmic hiring employer;hiring algorithms;hiring algorithm empirically;automated hiring;hiring decisions algorithms;hiring algorithms unpack;hiring screening;hiring algorithm;hiring screening simply;hiring hiring pipeline;interpretation automated hiring;hiring paradox plagues;algorithmic hiring algorithm;diagnose mitigate hiring;algorithmic hiring;novel algorithmic hiring;hiring pipeline;paradox algorithmic hiring;hiring pipeline taken;hiring pipeline given;mitigate hiring;able predict hiring;algorithmic approaches employment;job ad targeting;hiring pipeline believe;practice hiring screening;mitigate hiring paradox;hiring process law"}, "f889723a4427e914e4e32547dfd0ca4996170180": {"ta_keywords": "voice conversion challenge;predict prosody target;voice conversion;generation voice conversion;predict prosody;propose predict prosody;prosody target speaker;target text prediction;speaker target dependent;prosody target;target speaker;text prediction tp;latest generation voice;target speaker target;speaker target;conversion challenge 2020;generation voice;text prediction;voice;evaluation conversion;referred target text;evaluation conversion process;speaker;target text;conversion challenge;prosody;quantitative evaluation conversion;prediction tp;subjective evaluations results;objective subjective evaluations", "pdf_keywords": "speech prediction;predict prosody linguistic;predict correct speech;purpose speech prediction;training prosody model;predict prosody;speech prediction based;proposes predict prosody;speaker learning;representation target speaker;learning advanced speaker;speaker learning advanced;speaker learning based;method training prosody;new prosody modeling;voice conversion challenge;advanced speaker learning;speech generated;existing speech based;prosody modeling;speech generated combination;prosody modeling technique;quality speech generated;training prosody;voice conversion;voice conversion text;model target speaker;speech recognition task;high quality speech;use prosody"}, "f4cca8ea79e26fa20a91c3d3b769c9f7b82a6207": {"ta_keywords": "spherical microphone array;pinna spectral notches;spherical microphone;virtual spherical microphone;spectral notches median;extraction pinna spectral;spectral notches method;microphone array discussed;microphone array;spectral notches icr;notches icr pinna;notches median plane;spectral notches;signal form pinna;method extraction pinna;database pinna spectral;notches median;microphone;notches method applied;notches method;pinna spectral;extraction pinna;spherical array;form pinna spectral;spherical array simulate;notches icr;median plane virtual;bessel series spherical;array simulate signal;median plane", "pdf_keywords": ""}, "8c38bffc058d558e7c734032ba63942865e05ae4": {"ta_keywords": "neural question answering;queries incomplete knowledge;incomplete knowledge bases;question answering;deductive reasoning;deductive reasoning faithful;deductive reasoning leads;faithful deductive reasoning;method deductive reasoning;knowledge bases;reasoning faithful deductive;complex queries incomplete;complex queries;knowledge bases inserting;deductive;novel method deductive;queries incomplete;reasoning leads better;question answering leads;incomplete knowledge;reasoning faithful;method deductive;queries;performance complex queries;reasoning;reasoning leads;faithful deductive;answering;new method neural;method neural", "pdf_keywords": ""}, "e4a6bc3ac385b8982bbbe0a2a5ac0c79101ec979": {"ta_keywords": "propagation gaussian wave;gaussian wave distribution;gaussian noise propagation;distribution gaussian wave;wave distribution gaussian;gaussian wave medium;noise propagation gaussian;gaussian wave;gaussian noise;space gaussian wave;propagation gaussian;region gaussian wave;wave distribution localized;localized region gaussian;effect gaussian noise;region space gaussian;gaussian;distribution gaussian;wave distribution necessarily;space gaussian;wave localized;noise propagation;region gaussian;wave distribution;effect gaussian;localized medium wave;wave localized region;wave distribution wave;distribution wave;medium wave localized", "pdf_keywords": ""}, "9bbeb4f0e48032df19f9f6a08839da5d2e60e8eb": {"ta_keywords": "broadcast speech recognition;multimodal broadcast speech;broadcast speech;masking algorithm multimodal;algorithm multimodal broadcast;multimodal broadcast;effectiveness multimodal broadcast;speech recognition;bayes risk decoding;risk decoding;binary masking algorithm;enhance binary masking;discriminative language modeling;binary masking;second chime challenge;algorithm multimodal;speech recognition using;decoding;discriminative language;masking algorithm;chime challenge;broadcast;combine discriminative language;recognition using prior;risk decoding post;method second chime;language modeling minimum;chime challenge demonstrate;modeling minimum bayes;prior distributions time", "pdf_keywords": ""}, "8f963beca679cb1129df0a944c6de4b126e20fd5": {"ta_keywords": "seq2seq decoder;seq2seq models pre;state seq2seq decoder;external seq2seq decoder;seq2seq models;seq2seq decoder long;updated seq2seq decoder;seq2seq decoder hidden;integrate seq2seq models;term memory lstm;seq2seq;trained language model;memory lstm;state seq2seq;hidden state seq2seq;short term memory;lstm memory;memory lstm memory;lstm memory cell;external seq2seq;integrate seq2seq;language model;state updated seq2seq;methods integrate seq2seq;lstm;updated seq2seq;pre trained language;term memory;language model proposed;trained language", "pdf_keywords": "encoder seq2seq neural;seq2seq neural;term memory seq2seq;seq2seq memory cell;memory seq2seq;memory seq2seq memory;trained encoder seq2seq;seq2seq memory;seq2seq decoder cell;pre decoding seq2seq;seq2seq decoder;seq2seq decoder long;rnn seq2seq model;sequence seq2seq model;combination seq2seq decoder;state seq2seq decoder;fusion seq2seq;seq2seq seq decoder;decoding seq2seq powerful;fusion seq2seq model;based encoder seq2seq;train seq2seq;rnn seq2seq;train seq2seq model;sequence seq2seq;methods train seq2seq;encoder seq2seq;seq2seq neural network;decoding seq2seq;sequence sequence seq2seq"}, "b2c47dd46bf7087b754aed45f06b6196cf2b1c28": {"ta_keywords": "imaging acute abdomen;abdomen using radiographic;diagnosis acute abdomen;acute abdomen using;acute abdomen;acute abdomen chapter;diagnostic imaging acute;radiographic technique equations;using radiographic technique;abdomen using;radiographic technique;abdomen;imaging acute;abdomen chapter;diagnostic imaging;abdomen chapter devoted;radiographic;using radiographic;chapter diagnostic imaging;diagnosis acute;diagnostic;kolmogorov smirnov equation;imaging;smirnov equation;motion obtained;kolmogorov smirnov;smirnov equation equations;devoted diagnosis acute;diagnosis;motion obtained equations", "pdf_keywords": ""}, "46d87d4614d9353f1b7d527333073ef9109bfaea": {"ta_keywords": "sourced item labeling;item labeling;crowd sourced item;labeling;item labeling problem;solving crowd sourced;labeling problem;labeling problem called;method solving crowd;solving crowd;crowd sourced;sourced item;crowd;item;sourced;method solving;method;problem called;new method solving;problem;introduce;solving;introduce new method;new method;introduce new;new;called", "pdf_keywords": ""}, "82ae0d4b41046ccedb435ece08a61f198cf77bb9": {"ta_keywords": "unsupervised generation text;game report corpus;generation text content;based unsupervised generation;generation text;unsupervised generation sentence;large corpus games;unsupervised generation;corpus games approach;corpus games;content large corpus;method unsupervised generation;report corpus testbed;generation sentence accurately;report corpus;sentence derive basketball;text content;corpus testbed develop;corpus testbed;describes content;large corpus;generation sentence;corpus;accurately describes content;basketball game report;based unsupervised;sentence transitions;game report;objectives explicit content;content", "pdf_keywords": "style text generation;generated sentences;generated sentences single;text generation joint;text generation;transfer text generation;text generation able;text generation style;sentences soft templates;hybrid attention copy;text generation method;text generation using;data text generation;learning document generation;natural language generation;stylistic control data;attention copy;90 generated sentences;existing sentences soft;content style representations;style exemplar sentences;aspects text generation;sentences single machine;novel attention copy;text generation paper;syntactically controlled learning;hybrid attention;language generation combines;text improving;hybrid attention coder"}, "bc1832e8b8d4e5edf987e1562b578bd9aa5e18a9": {"ta_keywords": "similarity acoustic conditions;similarity acoustic;words recorded mismatched;respect similarity acoustic;comparison recorded data;summarizing neural networks;sequence summarizing neural;based comparison recorded;summarizing neural;recorded mismatched condition;data respect similarity;recorded mismatched;comparison recorded;neural networks proposed;recorded data sequence;acoustic conditions method;words recorded;data sequence summarizing;selecting data;million words recorded;neural networks;selecting data respect;data high accuracy;mismatched training test;method based comparison;acoustic conditions;similarity;training test set;method selecting data;based comparison", "pdf_keywords": ""}, "e212f788c701370af02b138d2a61e180cddfb138": {"ta_keywords": "translating single source;translating single target;target language multiple;pruning target language;single target language;source language multiple;translating original language;multiple target languages;single source language;language translating;method translating single;language multiple target;information language translating;translating single;framework translating single;method translating;target language searching;language translating original;aggregating information language;target language;propose method translating;language target language;language multiple;target languages;searching best translation;target languages method;framework translating;target languages general;source language;language target", "pdf_keywords": ""}, "c5dfc5fe7102fd8647edd1c9483aded82557e544": {"ta_keywords": "recourses interpretability explanations;global counterfactual explanations;recourses interpretability;counterfactual explanations provide;counterfactual explanations;correctness recourses interpretability;summary recourses individuals;recourses entire population;recourse costs population;accurate summary recourses;minimizing overall recourse;recourses individuals;optimizes correctness recourses;recourse costs;global counterfactual;interpretability explanations minimizing;correctness recourses;summary recourses;explanations provide interpretable;overall recourse costs;overall recourse;summary recourses entire;recourses;recourses individuals framework;construct global counterfactual;recourse;comprehensive summary recourses;interpretability explanations;counterfactual;predictive model evaluation", "pdf_keywords": "counterfactual explanations recourse;generate recourse counterfactuals;counterfactual recourse generation;global counterfactual explanations;counterfactual prediction;counterfactual prediction using;recourse counterfactuals;counterfactual recourse;learning global counterfactual;interpretable explanations recourses;predictive models counterfactual;counterfactual explanations provide;method counterfactual prediction;predict recourse;counterfactual explanations;recourse counterfactuals given;prediction using recourse;framework counterfactual recourse;predict recourse user;models counterfactual;learn global counterfactual;actionable counterfactual;models counterfactual nature;generate recourses interpretable;elementary counterfactual explanations;recourses interpretable;explanations recourses;global counterfactual explanation;explanations recourses context;explanations recourse real"}, "90ed32fa521b9e85f1c9efe356619814a2e79961": {"ta_keywords": "diagonalization quantum;closed quantum method;spectral density closed;diagonalization quantum equivalent;density closed quantum;exact diagonalization quantum;calculation spectral density;closed quantum;spectral density;calculation spectral;method calculation spectral;equivalent calculation spectral;quantum method;quantum method applied;quantum method based;quantum equivalent calculation;diagonalization;spectral;based exact diagonalization;exact diagonalization;density closed;quantum equivalent;quantum;density;new method calculation;method calculation;closed;equivalent calculation;method applied;method based exact", "pdf_keywords": ""}, "a75869d69cc86f501939c237ae4711aa2885f6a6": {"ta_keywords": "translation meta learning;low resource translation;resource translation meta;translation meta;multilingual high resource;resource translation;low resource languages;high resource language;resource language tasks;propose meta learning;resource translation frame;language tasks;resource language;multilingual;multilingual high;based multilingual high;resource languages;meta learning;meta learning strategy;languages based multilingual;meta learning problem;diverse languages vo;resource languages based;diverse languages;language tasks use;translation frame low;languages diverse languages;different languages;different languages diverse;languages diverse", "pdf_keywords": "translation meta learning;resource machine translation;low resource translation;tuning machine translation;neural machine translationwe;resource translation meta;machine translation low;translation low resource;neural machine translation;machine translationwe propose;new machine translation;translation meta;machine translation tasks;machine translation;machine translation language;machine translation characterized;machine translationwe;resource translation;machine translation explicitly;framework machine translation;resource translation furthermore;machine translation nmt;low resource learning;machine translation ng;multilingual transfer learning;translation tasks;learning meta learning;translation language;translation based tasks;translation language pairs"}, "18e70ad07561cf09a2d7f0da992a0e87a5e5c0a8": {"ta_keywords": "recognition speech method;recognition speech;speech recognition speech;method recognition speech;recognition speech recognition;speech method based;based recognition speech;speech recognition;speech method;method based recognition;based recognition;recognition;new method recognition;speech;method recognition;propose new method;method based;method;new method;propose;propose new;new;based", "pdf_keywords": ""}, "8cebfae7cd436241eb5c3442e687a913a75a5531": {"ta_keywords": "sense induction disambiguation;task sense induction;perform sense induction;sense induction;sense induction simo;russian language task;words share sense;induction disambiguation group;disambiguation group words;induction disambiguation;share sense granularity;sense granularity participants;disambiguation group;induction simo russian;share sense;sense granularity;language task;group words share;simo russian language;russian language;disambiguation;cluster contexts word;perform sense;language task perform;shared task sense;context cluster contexts;context cluster;given context cluster;group words;task sense", "pdf_keywords": "word sense induction;sense induction russian;words representing senses;russian english semantic;sense induction disambiguation;word sense;useful understanding linguistic;english semantic processing;recognition complex words;linguistic;understanding linguistic;standard word sensethis;word sensethis;semantic words;induction russian language;standard word sense;informed word sense;russian language participants;word sensethis paper;homonyms sense inventory;task word sense;language participants;semantic structure words;classify semantic words;semantic index linguistic;use linguistic;linguistic properties;optimally linguistic;linguistic similarity;understanding linguistic properties"}, "c6bb04f3d8000b7e800f6359082de39548c7da79": {"ta_keywords": "locality nonparametric language;nonparametric language models;structural locality nonparametric;locality information models;locality nonparametric;web demonstrate locality;use structural locality;adding locality information;locality information;locality features;structural locality;nonparametric language;locality features improve;adding locality;demonstrate locality features;language models;approach adding locality;locality;language models propose;demonstrate locality;examples local;retrieving examples local;examples local neighborhoods;models access features;local neighborhoods;nonparametric;models adding learned;local neighborhoods experiments;information models;local", "pdf_keywords": "learning locality;utilizing structural locality;structural locality ubiquitous;incorporating structural locality;learning captures locality;adding locality information;locality domain features;incorporate locality information;locality improve applicability;domain locality;locality information models;locality learning;notion structural locality;locality learning relate;learning locality learning;structural locality;locality ubiquitous;locality information;locality information propose;given domain locality;domain locality sufficient;differences structural locality;relevant locality information;locality domain;generalized incorporate locality;structural locality defined;structural locality propensity;locality non parametric;specific locality domain;locality propensity text"}, "97846070369f66c3080a0803be58e96963dec581": {"ta_keywords": "social networks pandemic;networks pandemic;social networks internet;internet infrastructure pandemic;use social networks;networks pandemic 2011;social networks;epidemics pandemic;2016 epidemics pandemic;epidemics pandemic related;internet remains weak;spread online misinformation;2016 epidemics;disrupt internet;effort disrupt internet;use social;widespread use social;2015 2016 epidemics;epidemics;infrastructure pandemic currently;networks internet remains;infrastructure pandemic;internet remains;harmful websites;harmful websites current;pandemic currently causing;pandemic related attacks;pandemic 2011 2015;spread online;disrupt internet infrastructure", "pdf_keywords": ""}, "08f6819e66318cd49cddefd5d690a752d1098da7": {"ta_keywords": "argumentative component claims;argumentative argumentative component;argumentative component;study argumentative argumentative;argumentative argumentative;classification claims shared;argumentative;domain classification claims;classification claims;conceptualizations claims;claims different datasets;study argumentative;comparative study argumentative;different conceptualizations claims;conceptualizations claims different;component claims;datasets different conceptualizations;claims shared properties;claims shared;claims different;claims;domain classification;component claims different;datasets;cross domain classification;shared properties lexical;properties lexical;datasets different;properties lexical level;different datasets", "pdf_keywords": "argumentative sentences classifier;argumentative argumentative dataset;claims argumentative discourse;claim identification argumentative;identification argumentative sentences;analyze claims argumentative;argumentative dataset;identification argumentative;claims argumentative;argumentative analysis documents;arguments documents conversational;argumentative discourse students;argumentative analysis;argumentative discourse conceptualization;argumentative discourse;discourse conceptualization claims;argument argumentative discourse;novel approach argumentative;claim detection context;argumentative sentences;approach argumentative analysis;argumentative dataset set;arguments documents;linguistic rhetoric complexity;high level argumentative;detecting arguments;dataset claims corpora;detecting arguments text;argumentative sentences use;assertions text propose"}, "9f73c3f86026c21d0e5e55c70462952c6ada1175": {"ta_keywords": "training deep;training prioritizing examples;training deep neural;neural networks prioritizing;accelerates training deep;accelerates training prioritizing;backprop accelerates training;deep neural;event learning;selective backprop;prioritizing examples high;deep neural networks;event event learning;networks prioritizing examples;selective backprop converges;training prioritizing;prioritizing examples;selective backprop accelerates;high loss iteration;called selective backprop;event learning approaches;shows selective backprop;backprop;backprop converges;backprop converges target;networks prioritizing;neural;backprop accelerates;neural networks;loss iteration evaluation", "pdf_keywords": "training deep;learning selective backprop;training deep learning;training deep neural;computationally expensive backpropagation;expensive backpropagation;expensive backpropagation steps;deep learning;approach training deep;deep learning selective;deep neural;deep learning increases;accelerates training deep;paced training deep;method training deep;learning bottleneck neural;error rates deep;deep neural networks;bottleneck neural networks;neural networks prioritizing;rates deep learning;networks selective backprop;novel deep learning;learning bottleneck;high loss iteration;bottleneck neural;traditional deep learning;training exploiting sparse;speedup state neural;learning accelerated"}, "dc984ea8be018a0244b40468d13f7b734ab55bac": {"ta_keywords": "neural machine translation;machine translation nmt;translation lexicons efficiently;augmenting discrete translation;translation nmt systems;machine translation;efficiently encode translations;discrete translation lexicons;encode translations low;translations low frequency;discrete translation;translation nmt;translation lexicons;encode translations;lexicons efficiently encode;translations low;lexicons efficiently;translations;performance neural;enhance performance neural;low frequency words;standard nmt probability;probability standard nmt;performance neural machine;nmt probability;nmt probability using;nmt systems augmenting;interpolation experiments corpora;corpora improvement;corpora improvement lebesgue", "pdf_keywords": "lexical translation probabilities;neural machine translation;machine translation;machine translation engine;machine translation nmt;translation probabilities predictive;lexicons translation process;probabilistic lexicons neural;lexicon based translation;attentional nmt models;context machine translation;translation probabilities;based translation model;translation probabilities using;transform lexical translation;predicting lexicon target;translation model improves;perform machine translation;probabilistic lexicons;predicting lexicon;attention vectors;translation estimate probability;probability target word;discrete probabilistic lexicons;probabilistic lexi;lexicons neural machine;translation engine computational;vectors attentional nmt;translation model;content word translation"}, "0533ccdc4840eed0fe1769b5e78da912631be609": {"ta_keywords": "nonlinear optical solitons;theory nonlinear optical;nonlinear optical field;optical solitons based;nonlinear optical;consistent nonlinear optical;optical solitons;solitons based phase;optical field theory;solitons based;theory nonlinear;equation phase space;self consistent nonlinear;optical field;solitons;space representation nonlinear;representation nonlinear ordinary;nonlinear ordinary;nonlinear;nonlinear ordinary differential;representation nonlinear;phase space representation;consistent nonlinear;differential equation phase;phase space;field theory phase;theory phase space;based phase space;equation phase;present theory nonlinear", "pdf_keywords": ""}, "4b73f4956c31cd10994c73b21e2c38a60a68d03e": {"ta_keywords": "assigning given assignment;complexity assigning;assignment problem reduced;given assignment problem;assignment problem;assignment problem problem;assigning weighted;assigning;assigning weighted average;complexity assigning given;weighted average disjoint;time complexity assigning;assigning given;average disjoint preferences;problem problem assigning;problem assigning weighted;order weighted averages;reduced problem assigning;averages polynomial time;assignment;weighted averages;order weighted;problem assigning given;weighted averages polynomial;problem assigning;given assignment;weighted average;using order weighted;average disjoint;approach problem assigning", "pdf_keywords": "optimal assignment agents;optimal assignment items;optimal assignment utilitarianity;assignment characterize optimal;assignment algorithm;egalitarian assignment finding;agents sided matching;finding optimal assignment;assignment finding optimal;characterize optimal assignment;optimal assignment;matching problem algorithm;determine optimal assignment;algorithm assigning items;assignment formulation;maximal assignment;algorithm assigning;novel algorithm assigning;maximal assignment provide;sided matching problem;assignment problem owa;owa assignment algorithm;assignment egalitarian assignment;preference matching process;assignment agent given;assignments optimal respect;optimal assignment np;assignment agents objects;assignment cases fair;assignment closely related"}, "b131cf78363993e4126b2562a156bd9d046c8bc4": {"ta_keywords": "translation languages pivot;languages pivot languages;pivot languages;languages pivot;words pivot language;pivot languages method;pivot language;pivot language distinguish;method translation languages;interoperability languages strumicarus;constituent words pivot;translation languages;useful translation languages;translation languages united;words pivot;languages strumicarus;languages proposed method;languages strumicarus bari;interoperability languages united;language distinguish syntactic;interoperability languages;strumicarus bari languages;method translation;syntactic roles proposed;languages proposed;new method translation;using interoperability languages;languages method;pivot;distinguish syntactic roles", "pdf_keywords": ""}, "18e5fb8cec55a75b288a499c57d77ede541dc049": {"ta_keywords": "question answering commonsense;question answering benchmarks;commonsense question answering;answering commonsense framework;shot question answering;answering commonsense;question answering;results commonsense;answering benchmarks;empirical results commonsense;results commonsense question;commonsense framework;commonsense;language models training;answering;answering benchmarks data;commonsense framework transforms;commonsense question;existing knowledge resources;zero shot;pre training models;zero shot question;knowledge sources;language models;pre existing knowledge;framework zero shot;knowledge sources data;knowledge resources form;models training;neuro symbolic framework", "pdf_keywords": ""}, "5b1bb1f6ed091dfd53adf7ebbcda2c48a3b67c2c": {"ta_keywords": "semantic frame induction;estimatively semantic frame;supervised semantic frame;word context labeling;frame induction task;semantic frame;context labeling;frame induction;estimatively semantic;frame induction combines;supervised semantic;context labeling method;subtask estimatively semantic;structure word context;word context;method supervised semantic;induction task estimatively;unsupervised task;semantic;unsupervised task predicting;applied unsupervised task;labeling;context;induction combines structure;supervised;frame;induction task;estimatively subtask;labeling method applied;labeling method", "pdf_keywords": "clustering verbs frame;verb clustering role;verb clustering;embeddings disambiguate verb;task clustering verbs;clustering verbs;word context embeddings;verbs frame slots;unsupervised semantic frame;embeddings enhanced syntactical;labeling verb roles;clustering semantic role;word sense induction;unsupervised semantic;verbs frame;trained embeddings disambiguate;agglomerative clustering tasks;clustering tasks sentence;verbs frame specific;clustering semantic;semantic labeling verb;semantic frame;semantic frame role;representations verb results;embeddings disambiguate;approach unsupervised semantic;context embeddings;trained embeddings;clustering similar word;pre trained embeddings"}, "384bf224d91a1691c9e6384201483121e2e7ddab": {"ta_keywords": "exact subspace clustering;subspace clustering;subspace clustering based;community recovery algorithm;algorithm exact subspace;clustering;clustering based concept;clustering based;exact subspace;recovery algorithm based;recovery algorithm;concept community recovery;community recovery;novel algorithm exact;algorithm based equivalence;algorithm exact;subspace;novel algorithm;present novel algorithm;algorithm significantly;algorithm;algorithm significantly fundamental;community;algorithm based;based equivalence principle;based concept community;concept community;efficient accurate characterize;equivalence principle;recovery", "pdf_keywords": ""}, "e01aa6f8ce625469b6f161d7ab9e61a60ac33798": {"ta_keywords": "latency streaming codes;optimal online code;streaming codes;streaming codes analysis;low latency streaming;latency streaming;optimal offline rate;rate optimal online;online code;optimal offline;online code constructions;offline rate settings;optimal online;offline rate;achieve optimal offline;match optimal offline;streaming;setting low latency;online schemes;low latency;online schemes match;codes analysis reveals;latency;codes analysis;code constructions recently;rate optimal;code constructions;offline;fact online schemes;rate settings", "pdf_keywords": "optimal streaming coding;optimal streaming code;streaming code optimal;streaming coding channels;streaming coding;online streaming codes;streaming codes;streaming code channels;packets offline coding;streaming codes based;streaming code;online coding schemes;size streaming code;coding channel packets;optimal offline coding;offline coding schemes;rate optimal streaming;distributed coding delay;coding channel packet;coding message packets;distributed coding communication;coding channels burst;optimal streaming;rate distributed coding;setting streaming codes;offline coding scheme;optimal online code;streaming codes match;complexity streaming codes;streaming code linear"}, "6b7004138ee2de5ec52e500cae4e65390e961e16": {"ta_keywords": "kernel cutting segmentation;kernel cut solvers;approach kernel cutting;cutting segmentation;kernel cutting;cutting segmentation based;kernel cut;segmentation;regularization spectral relaxation;existing kernel cut;linear kernel spectral;kernel spectral;cut solvers;problem segmentation variety;kernel spectral bound;segmentation based;segmentation variety;problem segmentation;regularization spectral;segmentation based combination;segmentation variety data;standard regularization spectral;spectral relaxation approach;linear kernel;data partitioning tasks;regularization;spectral relaxation;partitioning tasks demonstrate;new linear kernel;new approach kernel", "pdf_keywords": ""}, "e0236106e51984e4ea6bbbd1fb5ce57abf3e4e5e": {"ta_keywords": "virus masks;identify virus masks;virus masks used;virus determine person;risk virus spread;mask images people;reduce risk virus;facial mask images;pandemic characterized appearance;appearance facial mask;worldwide pandemic;viruses led worldwide;virus spread;facial mask;risk virus;social distance measure;pandemic;crisis pandemic;identify virus determine;mask demonstrate use;virus determine;masks used identify;identify virus;worldwide pandemic causing;mask images;masks;economic crisis pandemic;mask demonstrate;mask;crisis pandemic characterized", "pdf_keywords": ""}, "af92dd61340808f3008a84ae57803bb4aa57d03b": {"ta_keywords": "pose audio participants;predicting pose avatar;pose audio;audio body pose;pose avatar audio;consisting pose audio;predicting pose;body pose participants;pose participants;model predicting pose;pose;pose participants proposed;pose avatar;body pose;body pose conditioned;avatar audio body;pose conditioned;set body pose;pose conditioned interlocutor;avatar audio;consisting pose;audio participants;adaptive attention monadic;audio body;attention monadic;interlocutor audio participants;data consisting pose;adaptive attention;attention;audio participants confirming", "pdf_keywords": "avatar pose forecasting;predicting avatar pose;pose audio conversation;avatar pose audio;body pose conversation;model predicting avatar;pose conversation;predicting avatar;methods predicting avatar;pose conditioned audio;pose audio;pose person specific;pose person;pose conversation setting;human pose person;audio body pose;pose dyadic conversations;pose forecasting participant;pose audio participants;pose avatar;audio pose;audio pose history;pose interlocutor audio;consisting pose audio;body pose avatar;person model;avatar pose;future body pose;pose avatar using;pose forecasting"}, "5b1c0152bbb12ece2a8817c727e33e6d5c503065": {"ta_keywords": "distributed machine learning;learning algorithms codes;codes equip robustness;coding theoretic;coding theoretic perspective;robustness noise;algorithms codes;equip robustness noise;algorithms codes equip;distributed machine;machine learning algorithms;distributed;equip robustness;coding;learning algorithms;present coding theoretic;machine learning;robustness;algorithms;theoretic perspective distributed;perspective distributed machine;codes equip;present coding;perspective distributed;codes;learning;noise;machine;equip;theoretic perspective", "pdf_keywords": ""}, "c395595cf7be23f7d90cbca98d8c7861ebfd884d": {"ta_keywords": "learning classification metrics;classification metrics;classification metrics values;machine learning classifiers;highly metrics roc;classifiers human machine;classifiers human;classifiers;metrics roc;learning classifiers human;machine learning classification;human machine learning;learning classifiers;machine learning tasks;metrics roc received;comment toxicity misinformation;metrics overstate performance;learning classification;datasets current metrics;comment toxicity;score highly metrics;tasks comment toxicity;highly metrics;classification;toxicity misinformation score;machine learning;aligns machine learning;user annotator facing;user annotator;misinformation score highly", "pdf_keywords": "classifiers social computing;facing machine learning;classifiers social;toxic social computing;agree classifier;learning classifiers human;disagreement annotator classifiers;classifiers human facing;machine learning tasks;learning classification metrics;crowdsourcing accuracy search;classifiers human;agree classifier approach;classifiers;crowdsourcing accuracy;tasks comment toxicity;agree classifier argue;classifier thismany tasks;impact crowdsourcing accuracy;social computing tasks;comment toxicity misinformation;classifier argue;learning classifiers;classifier argue disagreement;disagreement deconvolution generalization;comment toxicity;people agree classification;comment toxic social;agree classification;machine learning classifiers"}, "37074b2b9cebd89e4a92d20f41eec7360e11fe5a": {"ta_keywords": "streaming speech recognition;online speech recognition;streaming speech;novel streaming speech;maskpredict connectionist temporal;speech recognition performance;speech recognition;topological maskpredict proposed;topological maskpredict connectionist;topological maskpredict;autoregressive nar maskpredict;maskpredict topological maskpredict;maskpredict topological;combines topological maskpredict;nar maskpredict topological;connectionist temporal classification;speech recognition based;maskpredict proposed;maskpredict connectionist;maskpredict;nar maskpredict;maskpredict proposed combines;maskpredict experimental;temporal classification proposed;online speech;temporal classification;maskpredict experimental results;vanilla maskpredict;vanilla maskpredict experimental;connectionist temporal", "pdf_keywords": "streaming speech recognition;e2e streaming speech;streaming automatic speech;streaming speech;predicting end speech;recognition streaming amplitude;speech recognition amplitude;speech recognition;automatic speech recognition;automatic speech;blockwise attention encoder;attention encoder blockwise;speech recognition combines;recognition streaming;encode input audio;speech recognition combining;end end streaming;end speech using;attention encoder;topic streaming speech;speech recognition context;speech using dynamic;method automatic speech;attention encoder used;based attention encoder;end streaming automatic;inference input audio;speech recognition based;streaming amplitude;token trained amplitude"}, "bd6018632a360cb567da8e50e1717ff526503845": {"ta_keywords": "stochastic beam search;conditional stochastic beam;stochastic beam;noisy beam search;conditional stochastic sampling;optimally decoding noisy;optimal conditional stochastic;noiseless partial differential;probabilistic conditional stochastic;noiseless partial;conditional stochastic;generated conditional stochastic;decoding noisy noisy;stochastic sampling pdfs;decoding noisy;conditional conditional stochastic;stochastic sampling pdf;stochastic sampling;beam search;optimally decoding;noisy noisy beam;method optimally decoding;class noiseless partial;noisy beam;beam search pmsn;stochastic;sample optimal conditional;probabilistic conditional;pmsn class noiseless;novel probabilistic conditional", "pdf_keywords": "stochastic beam search;weights stochastic beam;optimization stochastic beam;conditional stochastic beam;sampling algorithm beam;algorithm beam search;stochastic beam;beam search algorithm;weights stochastic;algorithm sampling conditional;sampling constraint strategy;algorithm sampling;example beam search;noisy beam search;beam search;allocating weights stochastic;sampling conditional sets;beam search nonstandard;beam search consider;stochastic generalization;sampling constraint;optimization stochastic;present stochastic beam;conditional sampling flexible;beam search method;beam search generalizes;flexible sampling;powerful sampling algorithm;efficient powerful sampling;principle conditional sampling"}, "6494cd26511c076186673c9a636d21d1dfed8d5a": {"ta_keywords": "student supersupervised array;implemented student supersupervised;student supersupervised approach;conventional student supersupervised;supersupervised array;student supersupervised;teacher network learning;supersupervised array ss;implicitly student network;student teacher network;student network;network learning method;features input network;features implemented student;teacher network;learning method enhanced;student network experiments;supersupervised approach;network learning;learning method;noisy signals training;signals training;supersupervised approach uses;features input;enhanced features input;method enhanced features;learning;supersupervised;array ss;input network", "pdf_keywords": ""}, "c8f9313ce8416a7be079935d1cbb637705f75182": {"ta_keywords": "individuality using translation;translation dictionary language;translation dictionary;translation model;translation dictionary using;using translation dictionary;construction translation dictionary;translation model probabilities;thesaurus gram statistics;estimation translation model;transforming individuality using;improvements quality translation;dictionary language model;thesaurus gram;automatic construction translation;using thesaurus gram;method transforming individuality;using translation;dictionary language;dictionary using thesaurus;transforming individuality;gram similarity;individuality using;statistics estimation translation;using gram similarity;quality translation;gram similarity experimental;gram statistics;using thesaurus;language model", "pdf_keywords": ""}, "8c7628641450203b0aa959b5a69729ff906760ff": {"ta_keywords": "speaker wise attractors;sequentially generate speaker;decoder embeddings speaker;embeddings speaker;generate speaker wise;speakers encoder;embeddings speaker overlaps;speakers encoder decoder;generate speaker;number speakers encoder;end neural diarization;decoder based attractor;neural diarization;neural diarization eend;speaker overlaps obtained;speaker wise;number speakers;unknown number speakers;diarization eend;attractor calculation module;attractor calculation;attractors attractors estimated;sequence encoder;speaker overlaps;based attractor calculation;attractors estimated using;sequence encoder decoder;sequence sequence encoder;speaker;attractors estimated", "pdf_keywords": "predicting speaker attractors;speaker diarization eend;speaker diarization systems;end speaker diarization;speakers neural network;novel speaker diarization;speaker diarization;approaches speaker diarization;speaker diarization method;np speaker diarization;speakerwise attractors calculated;harmonics neural networks;clustering underlying speaker;speaker diarization protocol;end neural diarization;predicting speaker;state speaker diarization;vector clustering speakers;speaker attractor method;speakerwise attractors;sequence speakerwise attractors;method predicting speaker;neural diarization;speakers neural;speaker diarization permutation;underlying speaker vector;speaker kernel representation;speaker attractor;diarization eend based;problem speaker diarization"}, "5aea95e1ae78a66474051a330ded374e199b658c": {"ta_keywords": "jumping knowledge networks;graph based representations;graph models structure;bioinformatics citation networks;graph models;citation networks;networks combines structure;knowledge networks combines;jumping knowledge;neighborhood aggregation;graph based;knowledge networks;features neighborhood representation;neighborhood representation;neighborhood representation achieve;behavior graph models;framework jumping knowledge;behavior neighborhood aggregation;aware behavior graph;aggregation resulting graph;resulting graph based;behavior graph;networks;social bioinformatics citation;graph;networks combines;social bioinformatics;neighborhood aggregation resulting;bioinformatics citation;learning framework jumping", "pdf_keywords": "graphs jumping networks;nodes graphs representations;learning graph representations;aggregation subgraph features;graphs representations learned;learning graphs jumping;aggregation subgraph;graph representations;adapt aggregation subgraph;graphs representations;underlying graph essential;learning graphs diverse;learning graphs;underlying graph structure;jumping knowledge networks;known neighborhood aggregation;representations nodes graphs;graph representations closely;learning graph;graph representations distributions;subgraph features introduce;underlying graph model;graph network;subgraph features;adapting underlying graph;follow neighborhood aggregation;learning graphs able;graph essential;nodes neighborhood complex;massive graph network"}, "564dec6eab6115ecd604f22738ce0b47777f6e17": {"ta_keywords": "clustering speech recognition;estimation clustering speech;clustering speech;speech recognition vbec;variational bayesian estimation;modeling speech classification;speech recognition;bayesian estimation clustering;propose variational bayesian;acoustic modeling speech;estimation clustering vbec;speech classification based;speech recognition procedures;probabilistic framework speech;speech classification;variational bayesian;classification based variational;modeling speech;probabilistic bayesian estimation;based variational posterior;framework speech recognition;variational posterior distribution;probabilistic bayesian;variational posterior;vbec based variational;based probabilistic bayesian;estimation clustering;recognition procedures acoustic;recognition vbec based;clustering vbec", "pdf_keywords": ""}, "8c5465eb110d0cab951ca6858a0d51ae759d2f9c": {"ta_keywords": "interpretable neural predictions;classifier learns rationale;rationale classifier learns;interpretable neural;learns rationale approach;learns rationale;approach interpretable neural;rationale classifier;neural predictions;rationale extraction;neural predictions based;rationale extraction explore;selects rationale classifier;model selects rationale;models rationale;novel approach interpretable;rationale model;models rationale model;rationale;selects rationale;interpretable;neural;neural network models;approach interpretable;classifier learns;rationale model selects;training neural;work rationale extraction;attention;predictions based", "pdf_keywords": "rationales text classifiers;example sentiment classification;extracting rationales learning;text classifiers;sentiment classification;learns words rationale;extraction text rationales;text classifier;text classification;text text classifier;method text classification;classifier learns words;based text classifier;regularization based text;rationale machine learning;text classifiers approach;text classifier generate;sentiment classification problem;words rationale machine;classifier natural languagewe;text classification problem;level sentiment annotations;sentiment annotations;classifier learns;sentiment analysis train;text classification based;learns make rationale;rationales learning;based double neural;learning classifier"}, "2660dbba723573266edb2a0a4929e6847ae83212": {"ta_keywords": "vector speaker adaptation;speaker adaptation;rate conversational systems;error rate conversational;speaker adaptation technique;conversational model based;rate conversational;conversational systems;conversational model;language model fusion;conversational systems approach;vector speaker;novel vector speaker;approach conversational tasks;conversational tasks;conversational tasks llllllllllllllll;ratio language model;word error rate;present conversational model;approach conversational;conversational;language model;effectiveness approach conversational;reduces word error;reducing word error;speaker;ratio language;density ratio language;present conversational;llllllllllllllll reduces word", "pdf_keywords": "combination sound neural;language model fusion;acoustic feature sequences;combine encoder prediction;integration encoder prediction;multiplicative integration encoder;sound neural;sound neural network;automatic speech;information streams conversational;streams conversational architecture;prediction network embeddings;end automatic speech;encoder prediction network;network combine encoder;speech recognition;combination neural;streams conversational;speaker adaptation;conversational architecture;predictive networks_ specifically;predictive networks_;external language model;end acoustic feature;prediction network vectors;conversational systems;neural networks iteratively;speaker adaptation external;prediction network;combine encoder"}, "94f22d7a8b48784b3d8975616e20d8028a08162f": {"ta_keywords": "model xmath1 coupling;generalization xmath4 coupling;model xmath3 coupling;xmath1 coupling generalization;xmath4 coupling;generalization xmath2 model;xmath1 coupling;dimensional xmath0 model;coupling generalization xmath2;xmath4 coupling xmath5;xmath5 coupling;xmath0 model;coupling xmath5 coupling;xmath3 coupling;xmath2 model;xmath0 model xmath1;coupling xmath5;xmath5 coupling used;xmath2 model xmath3;xmath3 coupling natural;dimensional xmath6 model;model xmath1;xmath6 model model;xmath6 model;model xmath3;zero dimensional xmath0;zero dimensional xmath6;generalization xmath2;generalization xmath4;dimensional xmath0", "pdf_keywords": ""}, "d119cc4051ed1206a0dac963cd23a84acf77fea7": {"ta_keywords": "rules deployment forecasted;deployment forecasted outcome;decision rules deployment;uncalibrated forecaster;accurately threshold decisions;predicted accurately calibration;deployment forecasted;forecasted outcome decision;decision loss predicted;takes uncalibrated forecaster;predicted accurately threshold;probabilistic forecasts predict;rely probabilistic forecasts;ensure decision loss;threshold decisions provide;threshold calibration exactly;probabilistic forecasts;threshold calibration;rule necessarily predicted;forecaster;forecasts predict loss;forecasts predict;called threshold calibration;necessarily predicted accurately;calibration called threshold;threshold decisions;predicted accurately;forecasted outcome;loss predicted accurately;forecasts", "pdf_keywords": "threshold calibration forecaster;threshold calibrated forecaster;forecaster threshold calibration;threshold decision forecasted;forecaster threshold;obtain forecaster threshold;accurately predicted threshold;threshold calibration predictive;forecaster empirically threshold;threshold calibration provably;predicted threshold decisions;threshold calibration decision;calibrations threshold losswe;calibrations threshold loss;provably outputs threshold;predicted threshold;algorithm threshold calibration;propose threshold calibration;forecastwe propose threshold;distribution calibration threshold;empirically threshold calibration;achieving threshold calibration;calibrations threshold;decision threshold loss;threshold calibration necessary;calibration threshold calibration;calibrated forecaster empirically;threshold calibration context;calibration threshold;outputs threshold calibrated"}, "6a9795853e5f39325deb0d916fe22d9e5a202a9f": {"ta_keywords": "ampres printed books;ampres printers original;early ampres printed;publishers early ampres;ampres printed;early ampres printers;ampres printers;papers early ampres;identifications successful publishers;printed books;gemindrome identification printers;printers original sloan;printers used identify;printed books method;identifying characterizing printed;prints early identifications;identification printers;printers original;identification printers used;publishers early;identify published papers;successful publishers early;characterizing printed;identify published;characterizing printed prints;printed prints;used identify published;published papers early;publishers;successful publishers", "pdf_keywords": ""}, "5270b626feb66c8c363e93ba6608daae93c5003b": {"ta_keywords": "knowledge transformers;knowledge transformers ensuring;factual knowledge transformers;knowledge modification;effective knowledge modification;scenarios updating knowledge;updating knowledge protecting;updating knowledge;transformers ensuring model;knowledge protecting privacy;transformers ensuring;key components transformers;specific factual knowledge;unmodified facts task;knowledge protecting;components transformers model;factual knowledge;transformers model especially;biases stored models;transformers model;degrade unmodified facts;effective knowledge;stored models;modifying specific factual;useful scenarios updating;knowledge;unmodified facts;components transformers;discovery key components;transformers", "pdf_keywords": "memorization language models;transformers memorize specific;improving memorization;improving memorization generalization;pretrained language models;models learned language;transformers memorize;learned language models;language models learned;models explicit memory;memorization language;memorization generalization large;task improving memorization;make transformers memorize;memorization generalization;machine memorization;accuracy language models;novel approach memorization;factual knowledge transformers;approach memorization language;memorization;language model trained;learning machine memorization;implicit memorization language;employ explicit memory;memorize specific;language models building;language models;knowledge transformers;minimization memory modification"}, "7b99c51d562e33309a46601c846abbe72a65c6a4": {"ta_keywords": "intermediate transfer tasks;transfer tasks variety;transfer tasks;best intermediate transfer;intermediate transfer;transfer;efficient embedding;embedding based methods;efficient embedding based;demonstrate efficient embedding;identifying best intermediate;fine tuning approaches;methods identifying best;computational expensive shot;embedding based;computational cost methods;datasets outperform computational;datasets;tasks variety scenarios;best intermediate;embedding;computational expensive;tasks;tuning approaches;solely respective datasets;estimates computational cost;methods demonstrate efficient;methods identifying;accuracy methods provide;identifying best", "pdf_keywords": "intermediate transfer learning;transfer learning;task transfer learning;transfer learning provide;transfer learning focus;intermediate task dataset;tasks intermediate transfer;intermediate task transfer;tasks machine learning;trained models intermediate;intermediate task selection;transfer multi task;selecting intermediate tasks;models intermediate task;intermediate task data;intermediate tasks easily;task data rich;sequential adapter training;best intermediate task;commonsense reasoning tasks;method pretraining deep;task data pretrained;pretraining deep bidirectional;large set tasks;task transfer;11 target tasks;learning pipeline task;target tasks best;method training adapters;tasks intermediate"}, "c47c8c2527bf2ca8339c342f44db2218a0cbcbbd": {"ta_keywords": "knowledge graph identification;identification knowledge graph;knowledge graph construction;knowledge graph;extraction knowledge base;information extraction knowledge;probabilistic soft logic;knowledge base construction;results knowledge graph;problem knowledge graph;graph identification knowledge;extraction knowledge;knowledge base;information extraction;use probabilistic soft;soft logic;soft logic ppl;problem information extraction;probabilistic soft;graph identification key;graph identification;graph identification present;identification knowledge;graph construction;use probabilistic;results knowledge;probabilistic;graph construction performing;logic ppl recently;logic ppl", "pdf_keywords": ""}, "4eb22b488052c430170139c492674aa05512f7bf": {"ta_keywords": "force optimization forging;forging process molecular;optimization forging;optimization forging process;forging forging process;molecular lennard jones;forging deforming force;process molecular lennard;deforming force optimization;forging forging;molecular lennard;force optimization;forging process;forging process characterized;final forging forging;forging deforming;jones msc forging;lennard jones msc;lennard jones;forging;results forging deforming;msc forging process;generate final forging;lennard;present results forging;results forging;final forging;msc forging;deforming force;optimization", "pdf_keywords": ""}, "f394c5101d7bfc3d8055f9391a83f7e2395dec4a": {"ta_keywords": "parallelizing supervised programs;learning algorithm parallelized;algorithm parallelizing supervised;learning algorithm parallelizing;program parallelized;parallelizing supervised;underlying program parallelized;program parallelized use;supervised programs algorithm;algorithm parallelized;algorithm parallelizing;algorithm parallelized version;handannotated openmp parallelization;parallelization;supervised programs;openmp parallelization;parallelized;parallelized version npb;parallelizing;parallelization directives;parallelized version;openmp parallelization directives;parallelized use;performance supervised;parallelized use approach;performance supervised learning;programs algorithm;programs algorithm combines;estimate performance supervised;supervised learning algorithm", "pdf_keywords": ""}, "f41e6c832c9e0d5360b66ee7681d3b1ffd2d9c3d": {"ta_keywords": "task learning sub;learning hierarchical;learning hierarchical structure;task learning;model task learning;task learning addresses;problem learning hierarchical;hierarchical structure input;planning scene navigation;decomposing task learning;sub planning scene;hierarchical;planning scene;hierarchical structure;model task;learning sub problems;object manipulation model;sub planning;based decomposing task;scene navigation;learning sub;object manipulation;navigation object manipulation;problem learning;addresses problem learning;presents model task;planning;problems sub planning;learning;decomposing task", "pdf_keywords": "learning hierarchical task;manipulation task hierarchy;hierarchical task structure;task learning language;task learning sub;tasks agents hierarchical;hierarchical task;task knowledge hierarchical;hierarchical task learning;solve task learning;hierarchical task modeling;object agent learns;task oriented navigation;vision language navigation;goals task learning;present hierarchical task;learns prior task;look task learning;task manipulation;learns navigate;planning object manipulation;task learning;agent learns navigate;object agent trained;perception planning actions;planning scene navigation;decomposition task learning;agents perception planning;object manipulation task;decompose task learning"}, "c54ad6e29f3e516eecf0a72bd1f95b80e8617116": {"ta_keywords": "compressive phase retrieval;complexity compressive phase;complexity phase retrieval;based phase retrieval;compressive phase;low complexity compressive;phase retrieval approach;complexity compressive;phase retrieval problem;phase retrieval;complexity phase;sparse graph code;compressive;comparable complexity phase;problem based phase;sparse graph;construction sparse graph;constructive capacityapproaching algorithm;memory complexity xmath0;recover signal components;construction sparse;complexity xmath0;algorithm low complexity;based construction sparse;recover signal;sparse;measurements recover signal;complex vector complexity;memory complexity;low complexity", "pdf_keywords": "compressive phase retrieval;sparse phase retrieval;recovery sparse phase;sparse graph phase;approach compressive phase;applicable compressive phase;recovering sparse signal;sparse graph codes;compressive phase;propose phase retrieval;phase retrieval propose;phase retrieval arbitrary;sparse phase;4k compressive phase;phase retrieval algorithm;recovering sparse complex;sparsegraph code;phase retrieval based;used phase retrieval;layer phase retrieval;applicable compressive sensing;compressive sensing arbitrary;phase coded code;compressive sensing;modular approach compressive;phase retrieval method;sparsegraph code used;recovering sparse;graph codes phase;construct sparsegraph code"}, "044b502e5a00b5eeff1dd078ea03f491ca2c37bf": {"ta_keywords": "continuous phoneme recognition;phoneme recognition;phoneme recognition task;speech recognition;speech recognition task;quantitative speech recognition;structural classification;wfsts structured perceptron;structural classification methods;approaches continuous phoneme;experiments continuous phoneme;state transducers;transducers wfsts structured;structured perceptron;state transducers wfsts;finite state transducers;task quantitative speech;compares structural classification;structured perceptron conditional;crf based features;lecture transcription task;continuous phoneme;lecture transcription;transducers wfsts;quantitative lecture transcription;quantitative speech;transcription task;recognition task quantitative;recognition task;features proposed methods", "pdf_keywords": ""}, "b9057dce43181a30aa3e0435c8ffc4c0b6f8f127": {"ta_keywords": "graphs defeasible inference;handcrafting meaningful graphs;generates meaningful graphs;faster predictive graphs;defeasible inference tasks;predictive graphs;graphs transfer learning;inference tasks;graphs human accuracy;meaningful graphs defeasible;generated graphs human;generates graphs;generates graphs transfer;meaningful graphs;defeasible inference task;generated graphs;graphs defeasible;performance generated graphs;method generates graphs;graphs human;graphs transfer;graphs;defeasible inference;inference task;inference task matching;inference tasks method;inference;method handcrafting meaningful;handcrafting meaningful;transfer learning", "pdf_keywords": "reasoning inference graphs;generate inference graphs;defeasible reasoning leveraging;inference graphs humans;inference graphs support;humans inference graphs;reasoning leveraging;inference graphs;generate inference;inference graphs useful;defeasible reasoning generates;reasoning generates;reasoning constructing;graphs humans inference;useful reasoning constructing;inference graphs transfer;graphs useful reasoning;reasoning leveraging power;support inference graphs;reasoning inference;reasoning generates accurate;paper generate inference;defeasible inference;inference combines graph;learning related reasoning;supplement defeasible reasoning;representation graph generation;graph generation;generation graph representation;defeasible inference recently"}, "b143ee344fe3af4169bde8af8b682a2835dae4a4": {"ta_keywords": "task furn agents;agents achieve;agents work;agents able perform;agents work piece;available agent models;models demonstrate agents;demonstrate agents achieve;agent models;agents;agent models demonstrate;agent;agents able;novel feature agents;feature agents;demonstrate agents;feature agents able;tasks finding;novel task furn;complex tasks;available agent;tasks finding right;rate novel task;piece furniture living;work piece furniture;piece furniture;perform complex tasks;complex tasks use;simple tasks finding;furniture living room", "pdf_keywords": "coordinated navigation agents;agent collaborative task;multi agent task;cooperative tasks visual;navigation agents;learning agents organize;agents parts exploration;coordinated navigation;agents share actions;describing motion agents;coordinating actions planning;agents organize interactions;navigation agents complementary;cooperative learning embodied;learning agents;agent collaborative;navigation agents using;single agent actions;cooperative tasks;agents perform cooperative;agent task;moving set agents;unstructured navigation agents;motion agents;organize interactions agents;multi agent collaborative;coordinated actions;agents trained follow;agent task furn;task moving furniture"}, "c3930cb34241a42e03ed02cbc83a3c87dddd60cc": {"ta_keywords": "quality generated stories;automatically generated stories;quality generated story;generated stories;learn generate story;generated stories predict;generate story;use generated stories;generated stories use;generate story predict;stories predict quality;story predict quality;generated stories resulting;stories used generate;stories use generated;generated story use;generated story;stories use reinforcement;generate collection stories;stories predict;predict quality generated;quality automatically generated;stories resulting;quality generated;story predict;stories resulting collection;scoring systems generate;automatically generated;resulting collection stories;predict quality", "pdf_keywords": ""}, "c9d7b1f9b13d6ea4ff45b908285cc65af959cc5b": {"ta_keywords": "probability word language;assigns probability word;assigning probability word;probability word basis;assignment probability word;assigns probability;assigning probability;method assigns probability;languages generated grammars;conditions assignment probability;languages generated;language method;generated languages;languages generated languages;probability word;assignment probability;language method applied;problem assigning probability;generated grammars;applied languages generated;measurable features language;word language consider;generated languages generated;language consider methods;word language;generated grammars derive;applied languages;method applied languages;languages;features language method", "pdf_keywords": ""}, "3050735eb35af3527276aa1952f79eb2483df3f0": {"ta_keywords": "belief conversational process;utterance process proof;conversational process estimate;level belief conversational;belief conversational;valid representation utterance;representation utterance process;utterance process;representation utterance;conversational process;belief unconstrained context;estimate level belief;proof performance robust;statistically valid representation;utterance;level belief unconstrained;belief unconstrained;robust statistically valid;conversational;proof performance;robust statistically;level belief;performance robust;process proof performance;statistically valid;initialized robust statistically;robust;robust process;unconstrained context process;performance robust process", "pdf_keywords": ""}, "a556914c1b32372d47a36f2826cbe143ddae95ca": {"ta_keywords": "self supervised taxonomy;supervised taxonomy expansion;self supervised;propose self supervised;existing taxonomy expansion;natural self supervision;taxonomy expansion model;node attachment prediction;taxonomy expansion;taxonomy expansion generate;attachment prediction task;natural supervision existing;supervision existing taxonomy;supervised taxonomy;attachment prediction;natural supervision;feature representations query;self supervision signals;task learns feature;supervised;learns feature;attachment task learns;supervision signals steam;leverages natural supervision;learns feature representations;self supervision;paths query terms;prediction task anchor;query terms;expansion generate natural", "pdf_keywords": ""}, "d85c0032d7bb0bd220eb2df8ba6d2130bc87e79e": {"ta_keywords": "iterative pseudolabel;pseudolabel method eend;pseudo labeling;propose iterative pseudolabel;end neural diarization;pseudolabel;pseudo labeling approach;propose pseudo labeling;pseudo label;pseudo label achieves;neural diarization;iterative pseudolabel method;pseudolabel method;labeling approach end;proposed pseudo label;neural diarization eend;diarization eend;diarization eend propose;labeling;labeling approach;end neural;dataset proposed pseudo;end end neural;committeebased training method;unlabeled data target;diarization;unlabeled data;using unlabeled data;model using unlabeled;neural", "pdf_keywords": "segmentation speaker diarization;unsupervised speaker diarization;label speaker diarization;algorithm speaker diarization;speaker diarization eend;end speaker diarization;segmentation speaker;speaker diarization;speaker diarization using;speaker diarization generate;speaker diarization approach;speaker diarization fully;challenging speaker diarization;speaker diarization simultaneously;speaker diarization performance;method speaker diarization;speech segmentation speaker;speaker diarization obtain;speaker diarization welltuned;speaker diarization scenarios;traditional speaker diarization;direction speaker diarization;speaker diarization fullywe;performance speaker diarizationwe;speaker diarizationwe propose;diarization performance speaker;predicting speaker activity;steps speech segmentation;segmentation speech;speaker diarizationwe"}, "571b4425498549c56c0828a824dc453ff6f482fc": {"ta_keywords": "delay tdma protocol;distributed networks delay;networks delay sensitive;tdma protocol distributed;zero delay tdma;control tdma protocol;delay tdma;tdma protocol;tdma protocol zmac;networks delay;protocol distributed networks;protocol distributed;delay hybrid adaptive;adaptive zero delay;knowledge scheduler qzmac;zero delay hybrid;adaptive distributed fully;delay hybrid;medium access control;scheduler proposed scheme;popular zero delay;adaptive distributed;access control tdma;delay sensitive fully;delay sensitive;hybrid medium access;self adaptive distributed;scheduler proposed;scheduler qzmac;zero delay", "pdf_keywords": ""}, "0823f2187eeed53be8fd452decf6ed9a6a6cd124": {"ta_keywords": "semantic interactions particles;semantic interactions;particles study interactions;interactions particles study;study interactions particles;study semantic interactions;interactions particles;interactions particles essential;information processing;particles essential understanding;interactions particles fundamental;processing study interactions;interactions particles particular;information processing study;particles study;particles particular context;interactions;semantic;mechanisms information processing;study semantic;particles;problem information processing;study interactions;particles essential;processing study;particles particular;particles fundamental;processing;mechanisms information;underlying mechanisms information", "pdf_keywords": ""}, "5de24203bf98ae7f4c514bc0bd2a310caa47a047": {"ta_keywords": "vehicle scheduling;solve vehicle scheduling;train coordination grid;vehicle scheduling problem;efficient train coordination;train coordination;scheduling problem vrsp;scheduling problem;efficient train;scheduling;coordination grid;paper efficient train;coordination grid conducted;traffic networks paper;approaches solve vehicle;problem traffic networks;solve vehicle;flatland competition 2020;flatland competition;conducted flatland competition;grid conducted flatland;traffic networks;train;grid conducted;competition 2020;vehicle;agents involved model;model environment agents;competition novel approaches;networks paper", "pdf_keywords": "reinforcement learning neur;deep reinforcement learning;reinforcement learning rl;reinforcement learning systems;learning reinforcement;learning reinforcement learning;dimensional reinforcement learning;reinforcement learning reinforcement;approaches reinforcement learning;reinforcement learning;class reinforcement learning;reinforcement learning problem;planning learning;deep reinforcement;dimensional reinforcement;reinforcement learning participants;rail grid learning;dimensional dimensional reinforcement;reinforcement learning modifications;results deep reinforcement;vehicle scheduling;learning problem rns;planning learning context;circuit uk game;rescheduling vehicles;railway switches competition;vrsp rescheduling vehicles;mapf reinforcement learning;train;reinforcement"}, "254d1b8cf247ae8b19e017f7ba758d670207ddda": {"ta_keywords": "novel beamforming network;speech recognition network;beamforming network;predicts optimal beamforming;optimal beamforming;beamforming network task;beamforming;speech recognition;optimal beamforming parameters;novel beamforming;automatic speech recognition;introduces novel beamforming;beamforming parameters;automatic speech;task automatic speech;recognition network;spatial filtering optimal;spatial filtering;beamforming parameters frequency;task spatial filtering;recognition network takes;trained network;generative model trained;array signals predicts;model trained network;accuracy cost network;generative model;filtering optimal;network takes array;network task spatial", "pdf_keywords": ""}, "1f5a1e959147e989e12846a5bd1d20234ef667d7": {"ta_keywords": "proteases patients prothrombin;patients prothrombin;proteases patients;plasma concentration proteases;patients proteases;732 patients proteases;concentration proteases patients;direct oral proteases;oral proteases;patients proteases dabigatran;oral proteases used;prothrombin region significantly;patients prothrombin complex;prothrombin;proteases dabigatran rivaroxaban;bleeding events patients;concentrates prothrombin;bleeding events;severe bleeding events;prothrombin region;concentration proteases;concentrates prothrombin region;proteases used sample;prothrombin complex concentrates;prothrombin complex;proteases dabigatran;study severe bleeding;proteases used;complex concentrates prothrombin;proteases", "pdf_keywords": ""}, "148f055083666c72945eea79833a19494f5f57c0": {"ta_keywords": "statistically significant objects;significant objects property;significant objects;class statistically;example class statistically;class statistically significant;number objects property;objects property members;objects property;certain class objects;objects defined number;class objects objects;class objects;number objects;property members certain;objects property property;objects defined;objects objects defined;defined number objects;objects objects;statistically significant;objects;certain class object;object defined number;class object;property members;class object defined;object defined;property;members certain class", "pdf_keywords": ""}, "924ce584acc148be29ef905c228fda7fe552c0c2": {"ta_keywords": "stochastic logic programs;probabilistic language;new probabilistic language;computation personalized pagerank;stochastic logic;extension stochastic logic;probabilistic language called;personalized pagerank;pagerank;relational learning algorithm;new probabilistic;personalized pagerank linearized;probabilistic;logic programs;present new probabilistic;relational learning;pagerank linearized;pagerank linearized version;constructing proofs logic;earlier relational learning;logic programs biased;proof space based;related computation personalized;path ranking algorithm;constructing proofs;logic related computation;proofs logic;ranking algorithm;biased short derivations;proofs logic related", "pdf_keywords": "probabilistic logic network;probabilistic logic trained;relational logic network;computation personalized pagerank;markov logic networks;probabilistic logic;logic networks;expressive probabilistic logic;large knowledge bases;scheme personalized pagerank;knowledge graph rich;personalized pagerank proppr;statistical relational learning;personalized pagerank ppr;knowledge bases;pagerank proppr based;learning large databases;structured facts web;relational learning solutionwe;knowledge bases based;personalized pagerank;knowledge graph;pagerank proppr;logic network;knowledge base;probabilistic language;proof based probabilistic;logic networks capable;resulting knowledge graph;scalable large knowledge"}, "8c4d1e81c277f71cd9e3c9a0af356203c7948dca": {"ta_keywords": "automated transcription correction;transcription correction task;transcription automated;transcription correction;transcription transcription automated;automated transcription;transcription automated transcription;quality transcription correction;improve quality transcription;transcription;transcription transcription;overcoming shortage transcription;quality transcription;transcription transcription transcription;shortage transcription;shortage transcription transcription;features transcription;essential features transcription;features transcription process;transcription process;transcription process demonstrate;novel predictive linguistic;predictive linguistic;predictive linguistic model;linguistic model captures;correction task;correction task approach;correction task open;automated;end documentation document", "pdf_keywords": "language transcription;end language transcription;recognition endangered languages;language transcription style;automated transcription document;transcription document;automated transcription;transcriptions;morphological phonological;underlying morphological phonological;endangered language documentation;transcription document elliptic;tonal phonology;end transcription;transcription format literature;study tonal phonology;morphological phonological processes;transcription format;transcription;endangered languages els;bimoraic lexical stems;novice transcriptions;language corpus chitl;corpus;transcription style;endangered language el;corrects novice transcriptions;end end transcription;endangered languages;lexical stems"}, "c5ed3d1a2ce418610a6fc9b5520a4f845279969a": {"ta_keywords": "erasure coded queries;queries parity;parity queries;queries using parity;inference parity queries;parity queries using;serving paritym encodes;models parity serving;parity models decodes;coded queries;multiple queries parity;queries parity query;parity models new;parity models parity;parity models trained;coded resilience prediction;paritym encodes;serving systems parity;parity models;erasure coded resilience;encodes multiple queries;implement parity models;parity serving paritym;parity query;using parity models;models implement parity;parity query performs;introduce parity models;systems parity models;parity serving", "pdf_keywords": ""}, "657329c633709dd1ac34a30d57341b186b1a47c2": {"ta_keywords": "sparse attention patterns;attention sparse routing;attention sparse;self attention sparse;dynamic sparse attention;sparse attention;attention patterns;attention patterns avoid;large search time;overall complexity attention;language modeling wikitext;complexity attention n1;complexity attention;query large search;large search;search time;language modeling;self attention;attention;learning dynamic sparse;models language modeling;attention n1;avoid problem searching;endows self attention;sparse routing;dynamic sparse;sparse;searching;sparse routing module;searching particular", "pdf_keywords": "sparse attention matrix;attention sparse;sparse attention module;sparse attention models;sparse attention patterns;sparse attention based;sparse attention long;sparse recurrent attentionwe;attention sparse routing;sparse attention;dynamic sparse attention;based sparse attention;self attention sparse;sparse attention problem;comparable sparse attention;uses sparse attention;approach sparse attention;sparse attention better;attention long sequences;attention matrix structure;method sparse attention;bias sparse attention;sparse recurrent;efficient attention long;attention models language;work sparse recurrent;structure attention matrix;pattern attention matrices;attention matrix;conventional attention matrices"}, "ba4a34680e09e77984624c95f5245d91b54373f6": {"ta_keywords": "tauri benchmark multilingual;multilingual machine learning;benchmark multilingual machine;benchmark multilingual;cross lingual tauri;model cross lingual;multilingual machine;lingual tauri model;lingual tauri;lingual tauri tauri;tauri tauri benchmark;cross lingual;tauri benchmark;lingual tauri set;multilingual;novel cross lingual;introduce cross lingual;tasks tauri model;tasks tauri;lingual;tauri set tasks;tauri model cross;set tasks tauri;tauri;tauri tauri;tauri model able;tauri model;machine learning models;learning models;learning models evaluate", "pdf_keywords": "crosslingual generalization machine;lingual generalization benchmark;describing crosslingual generalization;evaluation multilingual encoders;lingual generalization performance;benchmark cross lingual;crosslingual language model;crosslingual generalization demonstrate;cross lingual generalization;evaluation cross lingual;crosslingual generalization;lingual benchmark evaluation;generalization translation multilingual;multilingual encoders xtreme;crosslingual natural language;cross lingual retrieval;multilingual learning evaluate;cross lingual linguistic;multilingual learning;cross lingual benchmark;lingual generalization translation;prediction translation tasks;multilingual encoders;language cross lingual;characterizing cross lingual;languages tasks;crosslingual data crosslingual;crosslingual question answering;crosslingual model describing;ransfer evaluation multilingual"}, "927ff874d3ed9307356d256c31b79a0624b3c9d5": {"ta_keywords": "speaker chime challenge;multi speaker chime;speaker chime;chime challenge;models speech diarization;chime challenge challenge;dynamic models speech;speech diarization;speech diarization recognition;models speech;micro phone multi;phone multi speaker;diarization recognition everyday;multi speaker;phone multi;multi micro phone;challenge jhu multi;linguistic dynamic models;jhu multi micro;micro phone;diarization recognition;chime;speaker;proposed models challenge;advanced linguistic dynamic;second challenge jhu;speech;challenge jhu;multi micro;linguistic dynamic", "pdf_keywords": "speaker chime challenge;speaker recognition;diarization recognition speaker;speech recognition speaker;microphone speaker diarization;speaker diarization recognition;recognition speaker diarization;diarization speech recognition;speech recognition;recognition speaker;speech recognition everyday;multi speaker chime;speaker diarization deep;speaker recognition separating;recognition speaker signals;speech activity detection;simultaneous speaker diarization;improve speech recognition;speaker diarization far;speaker diarization;speaker chime;enhance speaker diarization;speaker diarization speech;speaker diarization reduce;microphone;quality speaker recognition;multimicrophone setting challenge;microphone multi;microphone multi speaker;multi microphone"}, "c4efaeccd7f0d900b1df95dadf51bad74264f613": {"ta_keywords": "strategising probabilistic serial;probabilistic serial rule;pure nash equilibrium;strategising probabilistic;nash equilibrium profiles;probabilistic serial;nash equilibrium guaranteed;computation pure nash;nash equilibrium;rule pure nash;problem strategising probabilistic;serial rule;algorithms optimal manipulation;probabilistic;polynomial time algorithms;optimal manipulation rule;polynomial time;optimal manipulation;algorithms optimal;strategising;optimal;serial rule present;rule utility;pure nash;equilibrium profiles rule;manipulation rule utility;time algorithms optimal;nash;equilibrium guaranteed;algorithms", "pdf_keywords": "nash equilibrium utility;preferences provide computational;preference allocation;preference rule complexity;preference mechanism;rule allocation goods;computation nash;nash equilibrium profiles;agents based preferences;nash equilibrium exist;preference allocation best;computation nash equilibrium;preference allocation orders;pure nash equilibrium;rational decisions allocation;agent expected utility;agent based allocation;existence computation nash;decisions allocation;allocation guaranteed consecutivity;boundedly rational decisions;preferences agents present;computing different preference;nash equilibrium;agent game conjecture;utility allocation;agents given allocation;agents identical allocation;game fractional assignment;nash equilibrium guaranteed"}, "605bae6c397e4829dde7ff7b8ddb84782ec6e607": {"ta_keywords": "influenza virus replication;infectious diseases selection;selection infectious diseases;diseases selection infectious;virus selection selection;virus selection;map influenza virus;selection infectious;diseases selection;virus replication cycle;called virus selection;based selection infectious;influenza virus;virus replication;present selection infectious;map influenza;comprehensive map influenza;stochastic search;stochastic search present;influenza;selection selection;selection selection selection;connected stochastic equations;stochastic equations;selection selection problem;selection problem represented;selection;stochastic;infectious diseases;problem based selection", "pdf_keywords": ""}, "a18b49fae647ae08711c2384611b3537485e8408": {"ta_keywords": "machine translation simultaneous;translation style machine;machine translation;machine translation performs;translation simultaneous interpretation;speech translation;style machine translation;data learn translation;translation performs simultaneous;speech translation employs;translation simultaneous;able capture translation;learn translation style;simultaneous interpretation data;capture translation;translation employs simultaneous;capture translation style;translation style;translation performs;present speech translation;simultaneous interpreter;interpretation data learn;performs simultaneous interpreter;learn translation;translation employs;employs simultaneous interpretation;simultaneous interpreter year;performance simultaneous interpretation;interpretation data collected;translation", "pdf_keywords": ""}, "417259d40d0d8b3ca7ebdcf811aa9f7814d5c0c5": {"ta_keywords": "model parameters saxophone;parameters saxophone saxophone;parameters saxophone;saxophone saxophone model;saxophone model;stiffness saxophone model;saxophone model jointly;reed stiffness saxophone;saxophone saxophone;saxophone model couples;saxophone;pitch reed model;stiffness saxophone;reed model parameters;configuration pitch reed;mouthpiece control parameters;reed model;static reed model;estimate tone hole;pressure reed stiffness;pitch reed;blowing pressure reed;reed model generating;pressure reed;tone hole configuration;estimate tone;pressure mouthpiece control;input pressure mouthpiece;reed stiffness;hole configuration pitch", "pdf_keywords": ""}, "4302e981e3ec118b68e0b3fcf1820b3f6ecfa988": {"ta_keywords": "argumentation theory practice;argumentation theory;arguments theory used;relationship argumentation theory;arguments theory;argumentation;study relationship argumentation;quality arguments theory;performance arguments theory;arguments;quality arguments;relationship argumentation;performance arguments;predict quality arguments;relative performance arguments;practice theory used;performance arguments finally;theory practice;arguments finally theory;practice theory;theory practice theory;arguments finally;theory used;relative performance;practice;theory used predict;performance;theory;predict relative performance;study", "pdf_keywords": ""}, "15251fa3a3bcf695bf153d0856886cab9a3145ea": {"ta_keywords": "text summarization;text summarization provides;summarizing summarizing;summarizing summarizing proposed;summarization provides unified;summarization provides;problem summarizing summarizing;summarization;framework text summarization;summarizing;summarizing proposed;problem summarizing;summarizing proposed method;view problem summarizing;text;comprehensive evaluation;comprehensive;comprehensive evaluation provide;methods comprehensive evaluation;framework text;datasets;new framework text;datasets outperforms;methods comprehensive;proposed method improvement;variety datasets outperforms;method variety datasets;method improvement;evaluation provide simple;provides unified view", "pdf_keywords": "summarization stage learning;summarization train;stage learning summarization;learned summary base;learning summarization model;summarization stage;learned summary meta;learning summarization;text summarization stage;summaries learned summary;performance summarization train;summarization train model;summarization systems optimal;performance text summarization;summaries learned;candidate summaries learned;improve performance summarization;summarization model leverages;summarization text model;summarization systems;neural extractive summarization;summarization accuracy improved;summary selecting sentences;summarization systems formulating;significantly improved summarization;summarization model;summarization summaries combination;learned summary;improved summarization accuracy;furthermore learned summary"}, "f9e3b7c6ca7d534694148bd0c7c37c1ef896a784": {"ta_keywords": "end speech recognition;single channel speech;speech recognition;speakers different languages;speech speaker single;channel speech speaker;speech recognition integrates;speech speaker trained;speaker trained sequence;mixed speech speaker;channel mixed speech;speech speaker;speaker single channel;multilingual;channel speech;multilingual end;trained sequence acoustic;speaker trained;sequence acoustic features;present multilingual end;multilingual end end;present multilingual;speaker single;different languages;acoustic features generated;10 different languages;speaker;acoustic features;sequence acoustic;optimized various linguistic", "pdf_keywords": ""}, "400e083a18ab94bbf45b0820693fb5035684dd7c": {"ta_keywords": "semantic similarity network;semantic similarity networks;semantic similarity;similarity network;use semantic similarity;similarity network presented;similarity networks;context semantic similarity;similarity networks method;semantic description;experiments semantic similarity;semantic description given;similarity networks results;similarity network method;use semantic;semantic;meaning recognition context;construction semantic description;context semantic;construction semantic;similarity;recognition context semantic;experiments semantic;results experiments semantic;meaning recognition;based use semantic;method construction semantic;problem meaning recognition;description given sentence;sentence based", "pdf_keywords": ""}, "71a85e735a3686bef8cce3725ae5ba82e2cabb1b": {"ta_keywords": "prediction model underspecified;model underspecified behavior;model underspecified;identify underspecified behavior;underspecified behavior;machine learning models;underspecified behavior common;clinical risk prediction;learning machine learning;mismatch training deployment;underspecified behavior significant;machine learning;training deployment domains;underspecified;identify underspecified;underspecified behavior machine;mismatch training;behavior machine learning;risk prediction;learning models intended;training deployment;domain underspecified behavior;machine learning machine;learning models;risk prediction model;learning machine;structural mismatch training;deployment domain underspecified;feature empirical data;domain underspecified", "pdf_keywords": "underspecification deep learning;underspecified machine learning;underspecification machine learning;learning pipeline underspecified;deep learning pipelines;machine learning substantial;underspecification learning;underspecification deep;models deep;machine learning pipelines;models deep learning;underspecification predictive;deep learning case;underspecification learning estimation;learning pipelines predictive;underspecified image classification;underspecification supervised;deep learning models;machine learning pipeline;learning ml pipelines;underspecification supervised machine;datasets underspecification;robustness deep;model sensitive underspecification;robustness class deep;predictors observe underspecification;problem underspecification learning;models trained;study robustness deep;learning models increasingly"}, "dd961bb9e2a70f3819a13b13402fe585ae384226": {"ta_keywords": "equilibria probabilistic serial;pure nash equilibrium;pure nash equilibria;equilibria probabilistic;indivisible goods agents;probabilistic serial rule;nash equilibria exist;probabilistic serial;agents pure nash;nash equilibrium;nash equilibrium conp;study equilibria probabilistic;compute pure nash;nash equilibria;nash equilibrium yields;assigning indivisible goods;profile pure nash;goods agents pure;equilibrium yields assignment;indivisible goods;probabilistic;goods agents;agents present linear;serial rule prominent;yields assignment truthful;serial rule;randomized rule assigning;conp complete agents;equilibrium conp complete;complete agents", "pdf_keywords": "probabilistic serial equilibria;preference expected utility;objects indivisible agents;pure nash equilibrium;equilibria probabilistic serial;randomized social choice;allocated agents randomized;pure nash equilibria;agents randomized;preferences assignment based;agents randomized way;perfect nash equilibrium;assignment rules existence;random assignment rules;preferences utility models;indivisible agents;equilibria probabilistic;assignment problem agents;agents express preferences;deterministic way agents;nash equilibrium profile;agent incentive;ordinal preferences assignment;nash equilibrium;computation equilibria probabilistic;nash equilibrium pne;based truthful preferences;allocated agents;preferences assignment;preferences prove pure"}, "86d55c5a098689438ceb1d52bdd768da3b47f55f": {"ta_keywords": "predict dynamics sensors;dynamics networked sensors;networked sensors algorithms;dynamics sensors;sensors algorithms;dynamics sensors able;sensors able predict;dynamics sensors important;sensors algorithms consist;networked sensors;characterization dynamics networked;dynamics networked;predict dynamics;filtering process algorithms;able predict dynamics;learning process filtering;algorithms able predict;sensors;supervised learning process;process filtering;filtering process filtering;filtering process;algorithms accurate characterization;sensors important design;process filtering process;process algorithms able;process algorithms;sensors able;supervised learning;characterization dynamics", "pdf_keywords": "centralized tracking stochastic;distributed sensor subset;tracking stochastic process;stochastic sensor;tracking stochastic;stochastic markov decision;estimation problem markov;suitable stochastic markov;centralized estimation problem;stochastic markov chain;optimal sensor subset;markov decision process;modeled stochastic markov;subset selection stochastic;selection stochastic process;thresholding markov chain;stochastic sampling algorithm;markov process known;distributed tracking consider;centralized tracking process;propagation stochastic sensor;stochastic markov;algorithms based stochastic;stochastic sampling;dynamic sensor subset;algorithm centralized tracking;solve centralized estimation;stochastic sensor dense;tracking distributed;distributed tracking"}, "2c0ebf5479db7f76c1e15512676c16b9032343fb": {"ta_keywords": "transmission line shift;shift edge transmission;line controlled shift;line shift controlled;position edge transmission;transmission line controlled;shift edge controlled;controlled shift edge;line shift edge;line shift;controlled shift;edge transmission line;shift controlled;transmission line;shift edge high;shift controlled position;edge transmission;shift edge;performance transmission line;line controlled;edge controlled position;performance transmission;controlled position edge;transmission;high performance transmission;shift;edge controlled;controlled position;position edge;controlled", "pdf_keywords": ""}, "0d360a1256ccdfca58cf98d12243df8407fd442d": {"ta_keywords": "attack pretrained model;untrusted pretrained weights;model attack;model attack exploits;uses untrusted pretrained;attack pretrained;pretrained weights attack;weights attack attacks;model uses untrusted;attacks model attack;exposing untrusted link;link exposing untrusted;new attack pretrained;attacks model;untrusted pretrained;exposing untrusted;weights attack;untrusted link;uses untrusted;attack attacks model;untrusted link door;users link weights;attacks;attack exploits;users download weights;link door attack;download weights models;link weights pre;link weights;attack exploits fact", "pdf_keywords": "trained models exploits;language models attack;vulnerabilities pre trained;learning attack;learning attack exposes;machine learning attack;trained poisoned data;models exploits;breaking deep learning;models exploits recently;models attack exploits;model exploits leak;adversarial perturbation attack;model exploits;models attack;model trained poisoned;adversarial;data poisoning embedding;novel adversarial;attack classical deep;pre trained models;adversarial perturbation;poisoning based embedding;present novel adversarial;attack pre trained;novel adversarial perturbation;poisoning embedding;trained models fine;pre trained neural;learning strong fine"}, "c14254fd285706e549d0dcc57ae74680164c9afc": {"ta_keywords": "inverse reinforcement learning;based inverse reinforcement;inverse reinforcement;risk sensitivity reinforcement;sensitivity reinforcement learning;model passengers decisions;gradient based inverse;reinforcement learning algorithm;passengers decisions;passengers decisions regarding;reinforcement learning;sensitivity reinforcement;learning algorithm minimizes;minimizes loss function;reinforcement learning canonical;algorithm minimizes loss;model passengers;minimizes loss;ride sharing;risk sensitivity;passengers;example model passengers;risk;reinforcement;decisions regarding ride;learning algorithm;regarding ride sharing;loss function;loss function defined;based inverse", "pdf_keywords": "learning inverse risk;inverse reinforcement learning;risk sensitive reinforcement;risk equivalently reinforcement;based inverse reinforcement;risksensitivity loss aversion;derive reinforcement learning;inverse reinforcement;employs convex risk;risk sensitive learning;sensitive reinforcement learning;reward theorems inverse;reinforcement learning valuation;inverse risk sensitive;inverse risk;learned optimal policies;risk loss seeking;optimal policies risk;theorems inverse risk;optimal policy learned;convex risk metrics;minimize risk losses;convex risk;behavior reinforcement learning;reinforcement learning theorems;novel inverse risk;concerning inverse risk;equivalently reinforcement learning;policy learned agent;aversion derive reinforcement"}, "e10dba1d4a56a81429d6ec4c9b7bdc15ea75474b": {"ta_keywords": "secure estimation process;malicious sensor proposed;sensors detection secure;malicious sensor;secure estimation linear;detection secure estimation;sensor proposed estimation;secure estimation;estimation linear time;presence malicious sensor;using sensor observations;sensor observations;estimation process;multiple sensors detection;observations multiple sensors;sensors detection;estimation process considered;deals secure estimation;varying process observations;detection secure;using sensor;estimation linear;estimation scheme;multiple sensors;sensor proposed;sensor measurements numerical;sensor observations retaining;observations retaining sensor;proposed estimation scheme;retaining sensor measurements", "pdf_keywords": ""}, "5403fd71810d098e572d9bd0f9ec10e96d6b6336": {"ta_keywords": "point transmission control;point point transmission;transmission control problem;point transmission;transmission control;problem noisy network;noisy network;network large state;noisy network large;probability transition graph;method applied wireless;applied wireless level;wireless level;probability transition;solving point point;structure optimal policy;network large;policy reduce complexity;optimal policy;representation probability transition;applied wireless;method solving point;optimal policy reduce;control problem noisy;network;transmission;transition graph;solving point;wireless level shown;reduce complexity problem", "pdf_keywords": ""}, "967b2d10b8b378f1da43fd4d9107826e540e1112": {"ta_keywords": "predicting motion languages;embedding language pose;language pose joint;motion languages;learns joint embedding;joint language pose;joint embedding language;language pose;motion languages use;pose joint embedding;pose p2p learns;embedding language;predicting motion;embedding space learned;language pose p2p;joint language;joint embedding;learns joint;pose joint;pose;p2p learns joint;joint embedding space;embedding;problem predicting motion;called joint language;motion;curriculum learning;languages;learns;pose p2p", "pdf_keywords": "animation natural language;natural language animation;embeddings human motion;embedding language pose;motion joint language;language pose model;pose data humanannotated;dataset human motion;joint language embeddings;joint embedding language;predicting human motion;learn joint embedding;language pose generate;predict pose sequences;generate pose sequences;language pose proposed;action natural language;predict poses person;language embeddings human;embeddings human;conditional pose sequences;pose generate animation;embedding language;pose sequences text;language embeddings;language animation;learned embedding;language pose;predict poses;sentence model trained"}, "0bdf1f3b79f4df5d5e11af1ea00379e1461e22fa": {"ta_keywords": "partial dependence plots;dependence plots;dependence plots pdps;selection interesting plots;interesting partial dependence;partial dependence;selecting interesting partial;plots pdps use;plots pdps;plots;dependence;automated probabilistically selecting;interesting plots;automated probabilistically;machine learning;machine learning applications;probabilistically selecting interesting;generalization multiple use;including selecting models;datasets;interesting plots showing;probabilistically selecting;datasets including selecting;datasets including;method automated probabilistically;features;selecting models;features model response;features model;interesting partial", "pdf_keywords": ""}, "d95973f0f0d86b758154e9a5f3d7434430d7856c": {"ta_keywords": "proton deuteron collision;section proton deuteron;deuteron collision method;method calculation proton;proton cross section;cross section proton;calculation proton;calculation proton proton;proton deuteron;proton proton cross;proton cross;proton densities;proton densities propose;section proton;range proton densities;deuteron collision;reproduce proton proton;proton proton;range proton;large range proton;proton;reproduce proton;accelerated gradient free;accelerated gradient;gradient free method;novel accelerated gradient;able reproduce proton;collision method;free method calculation;deuteron", "pdf_keywords": ""}, "194c5644c49e9e1b87990439fae05c98ba8b4fbb": {"ta_keywords": "synthesis procedures annotated;materials semantically structured;annotating materials semantically;synthesis sentences resource;materials semantically;semantics synthesis;express semantics synthesis;materials synthesis;semantics synthesis sentences;materials synthesis presence;synthesis presence semantic;synthesis sentences design;analysis materials synthesis;resource annotating materials;structured representations synthesis;synthesis sentences;annotating materials;synthesis procedures;representations synthesis sentences;synthesis;semantically structured;dataset 230 synthesis;semantic noise resource;graphs express semantics;semantically structured representations;synthesis presence;annotated labeled graphs;representations synthesis;resource analysis materials;semantic noise", "pdf_keywords": "annotated materials synthesis;synthesis annotations;semantically annotated materials;synthesis procedures annotated;synthesis annotations variety;dataset synthesis annotations;synthesis extraction models;documents describing synthesis;describing synthesis;annotated annotated materials;annotated materials knowledge;synthesis text;synthesis extraction;annotated materials;automated synthesis;synthesis procedures dataset;annotated semantic structure;procedures annotated semantic;annotated semantic;understanding materials synthesis;synthesis procedures raw;molecules using annotations;evaluation synthesis extraction;materials synthesis data;synthesis data;annotation process;annotations generate;extracting annotated;describing synthesis target;semantically annotated"}, "03b68259f9e70d2007d40e5331c9ff31f2bb46b9": {"ta_keywords": "activity recognition method;activity recognition;proposes activity recognition;activities using labeled;user activities using;activities using;user activities;acceleration sensor data;activity;end user activities;acceleration sensor;activities;unlabeled acceleration sensor;training data using;training data;labeled sensor data;sensor data;labeled unlabeled acceleration;appropriate training data;sensor data obtained;activities using selected;user physical characteristics;unlabeled acceleration;acceleration;recognition method models;user physical;end user physical;labeled sensor;sensor data confirmed;data using information", "pdf_keywords": ""}, "e11d6a031d5f85f372b0fda3ab62ca4ce2d89f2c": {"ta_keywords": "optimize transition rules;transition logic circuits;transition logic circuit;rules transition logic;optimization transition rules;rules transition method;transition rules transition;applied transition logic;choose transition rules;transition rules given;transition rules;transition logic;rules transition;optimize transition;logic circuits method;logic circuits;optimization transition;designer optimize transition;logic circuit;logic circuit used;logic circuit allows;rules given design;method optimization transition;designer choose transition;transition method applied;transition method;choose transition;method applied transition;applied transition;circuits method based", "pdf_keywords": ""}, "90fbeb4c871d3916c2b428645a1e1482f05826e1": {"ta_keywords": "captioning encoder;captioning encoder decoder;code captioning encoder;captioning source code;image captioning source;code captioning;image captioning;source code captioning;captioning source;tasks image captioning;decoder learning framework;encoder decoder learning;captioning;decoder learning;existing encoder decoder;encoder decoder;art encoder decoder;novel encoder decoder;encoder;encoder decoder model;propose novel encoder;decoder;art encoder;novel encoder;existing encoder;state art encoder;encoder decoder systems;decoder model propose;decoder model;decoder systems tasks", "pdf_keywords": "captioning attentive encoder;attention learned representation;networks rnns;attention mechanism encoder;encoder review network;representation generated attention;sequence encoders;represented sequence encoders;sequence sequence learning;networks rnns applications;sequence learning;predict captioning task;neural networks rnns;predict captioning;recurrent neural networks;learned representation encoder;representation neural;rnns applications image;representation encoder;decoder introduces attention;decoder model trained;encoder modeled sequence;encoder decoders tasks;representation encoder decoder;review network encoder;recurrent neural;rnns;model machine translation;reviewer conventional encoder;review network captioning"}, "ddd74358d7e11535ee77e2c323dd662d115a0f20": {"ta_keywords": "conditioned object grounding;features quadcopter motion;object grounding;object grounding strategy;grounding able capture;features quadcopter;object grounding able;quadcopter motion accurately;quadcopter motion;quadcopter;essential features quadcopter;approach shot language;novel shot language;shot language conditioned;motion accurately predicts;shot language;grounding;grounding able;grounding strategy able;approach able capture;grounding strategy;approach shot;motion accurately;conditioned object;language conditioned object;strategy able capture;capture;motion;novel approach shot;novel shot", "pdf_keywords": "propose shot language;objects training;objects training propose;objects training 10;use shot language;shot language;shot representation learning;objects based augmented;observes objects training;grounding natural language;shot segmentation component;utilizes shot segmentation;uses shot language;shot segmentation;annotation training;natural language instruction;objects utilizes shot;propose shot representation;object context grounding;visual language;shot language conditioned;handle visual language;mentions instructions objects;objects prior;instruction natural language;object level annotation;grasping tasks encoded;objects language;conditioned object grounding;level annotation training"}, "97943a5dee3c6e36d01a6099acb9ec360ad0ee19": {"ta_keywords": "sequence s2s portmanteau;approaches portmanteau generation;portmanteau generation;neural sequence sequence;portmanteau generation problem;s2s portmanteau generation;neural sequence;generation strategy noisy;unsupervised candidate generation;level neural sequence;generation strategy;generation problem accomplished;generation problem based;sequence s2s;generation problem;sequence sequence s2s;character level neural;s2s portmanteau;novel approaches portmanteau;generation;candidate generation strategy;approaches portmanteau;portmanteau;neural;sequence sequence;sequence;strategy noisy channel;level neural;candidate generation;accomplished combination robust", "pdf_keywords": "novel word formation;novel language model;predict character sequence;language model predict;predict input words;word formation;large corpus words;neural sequence tosequence;language representation;model vocabulary words;character sequence;new words combining;neural sequence;word formation phenomenon;language model vocabulary;models large corpus;generation roots words;character level neural;portmanteau novel word;predict character;generalized portmanteau generation;language model;large corpus;character sequence reverse;words noisy channelstyle;form new words;character embedding train;level neural sequence;novel language;characterize linguistic semantics"}, "30f86d38f0660af5ea2e16d996434c72eee8c5ee": {"ta_keywords": "data mining distributed;toolkit data mining;data mining named;mining distributed services;mining data mining;data mining;mining distributed;data mining data;mining data;types data mining;mining named espnet;platform toolkit data;toolkit data;distributed services used;distributed services;used distributed services;services used distributed;introduce toolkit data;distributed services following;distributed;used distributed;toolkit;platform toolkit;mining;mining named;source platform toolkit;open source;espnet new;espnet;named espnet new", "pdf_keywords": "setup speech recognition;recognition speech processing;speech recognition;end speech recognition;neural network toolkits;speech processing;end speech processing;speech recognition speech;recognition speech;illustrated speech recognition;speech processing named;speech recognition tasks;identification decoding speech;speech method based;theart speech recognition;speech processing experiments;problem speech recognition;speech recognition problem;advanced neural machine;signal processing neural;setup speech;recognition problem speech;decoding speech method;processing neural network;neural network;dynamic neural network;decoding speech;deep learning engine;speech method;proposed deep learning"}, "36bca9d41de386fce5dce06999a45a802a7c4f41": {"ta_keywords": "acyclic preference networks;preference networks;preference networks bp;generating acyclic preference;networks applicable rank;rank networks provide;rank networks;acyclic bp networks;generating bp networks;rank rank networks;random generation acyclic;networks provide general;networks method;networks;networks applicable;based random generation;networks method based;networks bp networks;random generation;networks provide;bp networks method;uniformly generating acyclic;acyclic preference;bp networks;networks bp;based random;generating acyclic;bp networks applicable;method based random;random", "pdf_keywords": ""}, "0c5bfa2d4bb351a479073cb358c3ae6f7ecf0476": {"ta_keywords": "nonlinear languages feature;nonlinear languages framework;nonlinear language;nonlinear languages;novel nonlinear languages;representation nonlinear languages;nonlinear language class;class nonlinear language;nonlinear data structure;framework representation nonlinear;novel nonlinear data;npnpnp class nonlinear;languages feature extraction;nonlinear data;representation nonlinear;features annotators npnpnp;data structures feature;use novel nonlinear;novel nonlinear;nonlinear;structures feature extraction;class nonlinear;present novel nonlinear;features annotators;feature extraction;annotators npnpnp;languages feature;feature extraction feature;data structure analysis;analysis framework representation", "pdf_keywords": ""}, "a2aa642db090b3aa28a44ccbc3c51fdb0be8335b": {"ta_keywords": "non neural parsers;neural parsers zero;benchmark treebanks;benchmark treebanks generalize;neural parsers;performance benchmark treebanks;parsers zero shot;treebanks generalize comparably;treebanks generalize;treebanks;parsers zero;parsers;performance domain corpora;non neural;performance neural non;neural non neural;corpora;domain corpora;neural non;zero shot;performance neural;neural;study performance neural;benchmark;zero shot setting;performance benchmark;generalize comparably new;performance domain;domains evaluate performance;evaluate performance benchmark", "pdf_keywords": "neural parsers;non neural parsers;parsers neural parsers;parser optimally trained;neural parsers neural;parsers neural;parsers learn better;training parser;neural parsers test;parsers learn;constituency parsers;results benchmark treebanks;recent nonneural parsers;parsers better;nonneural parsers domains;benchmark treebanks;benchmark treebanks generalization;nonneural parsers;parsers higher generalization;parsers better domain;structures better parsing;better domain parsers;correct training parser;statistical parsers learn;parsers;treebanks generalization evaluate;domain parsers better;novel parser;parsers generalize;parser low cost"}, "de5834305ea419c25b17f0c8d27bad6a5feb311a": {"ta_keywords": "chess commentary dataset;chess commentary;scale chess commentary;commentary dataset propose;commentary dataset;descriptions individual moves;chess game introduce;commentary texts;natural language descriptions;commentary;moves chess game;generate natural language;truth commentary texts;moves chess;chess;chess game;truth commentary;ground truth commentary;trainable neural;end trainable neural;large scale chess;pragmatic aspects game;language descriptions;trainable neural model;individual moves chess;language descriptions individual;natural language;game state commented;neural;moves", "pdf_keywords": ""}, "47234fca1b14666d72bc5df0e2d911ff7cdea688": {"ta_keywords": "hyperedges respectively stochastic;hyperedges robust stochastic;stochastic block models;block models stochastic;models stochastic block;partitions hyperedges robust;optimal partitions hyperedges;stochastic block;optimal partitions hypergraph;respectively stochastic block;partitions hypergraph objects;study stochastic block;block models edge;hypergraph objects robust;partitions hypergraph;partitions hyperedges;hypergraph objects;classical stochastic block;nodes weights hyperedges;hyperedges robust;weights hyperedges;objects robust stochastic;weights hyperedges respectively;stochastic;models stochastic;measures modeled nodes;models edge weights;robust stochastic;hypergraph;respectively stochastic", "pdf_keywords": "hypergraph spectral clustering;spectral clustering hypergraph;detecting clustering hypergraphs;clusters edges hypergraph;hypergraph clustering partition;hypergraph clustering;clustering hypergraphs;hypergraph clustering algorithms;clustering hypergraph;algorithms hypergraph spectral;clustering hypergraphs based;clustering hypergraph proposed;stochastic hypergraph;hypergraphs based spectral;edges hypergraph proposed;problem hypergraph clustering;clustering new hypergraph;partition hypergraphs consistency;develop hypergraph clustering;consider stochastic hypergraph;stochastic hypergraph model;partition hypergraphs planted;edges hypergraph;hypergraph model edges;hypergraph spectral;partition hypergraphs block;hypergraph proposed algorithm;edges node hypergraph;keywords hypergraph clustering;hypergraphs consistency partition"}, "9a41111cf881b052555985bd8cf304ef9fc4f6d5": {"ta_keywords": "information extraction task;information extraction;extraction task determining;task information extraction;determining item list;task determining item;extraction task;determining item;constraints items list;items list proposed;item list;items list;constraints items;task determining;coupling constraints items;performance task information;item list method;list proposed method;improving performance task;extraction;task information;based identification coupling;list method based;performance task;identification coupling constraints;list;method improving performance;list proposed;identification coupling;list method", "pdf_keywords": "corresponding structured corpus;structured corpus;augmenting large corpus;structured corpus building;target corpus graph;graph target corpus;large corpus coupling;derived structured corpus;structured corpora;distantly supervised information;corpus coupling constraints;structured corpus use;corpus augment graph;supervised information extraction;entities structured corpora;corpus coupling;nodes target corpus;large corpus;related entities structured;corpus graph corresponding;corpus graph;labeling relations significantly;target corpus;corpus building augmented;learning extracts mentions;corpus graph derived;corpus;extract relations structured;supervised learning structured;labeling relations"}, "c8a95217cde1bc893b230297250918818aa01dd7": {"ta_keywords": "photon generation coherent;lattice resulting coherent;coherent state photon;generate coherent state;excitation photon lattice;resulting coherent state;generation coherent state;coherent state achieved;coherent coherent state;generation coherent coherent;generate coherent;photon lattice;optical lattice;coherent state single;state photon generation;optical lattice method;coherent state;coherent state used;dimensional optical lattice;coherent coherent;resulting coherent;single photon generated;photon generation;used generate coherent;photon generated single;generation coherent;coherent state dimensional;photon generated;state single photon;single atom lattice", "pdf_keywords": ""}, "71cdf94d13cc6c497dcc2dcb20893fe64cfaf62e": {"ta_keywords": "supervised text generation;self supervised text;clusters predicting text;predicting text generation;text generation predicts;generation selected topics;text generation experiments;text generation;word clusters predicting;supervised text;predicting text;word clusters;word clusters possible;guide text generation;text generation selected;topically controlledlable language;centers word clusters;scale topically controlledlable;topics uses unlabeled;self supervised;topically controlledlable;framework self supervised;clusters predicting;selected topics uses;topics framework;unlabeled text guide;topics uses;predicting centers word;supervised;unlabeled text", "pdf_keywords": "topics generated sentences;trained generated sentences;predicts topics embedding;text generator predicts;topics generated text;predicts topics;generated sentences generated;generates sentences plausible;generated topics;generates sentences;text model predicts;generating sentences;generated topics related;generated sentences;user generated topics;prompt topics generated;topic goal generator;topics generated;generated topics include;topics predicting;topics predicts;sentences generated;candidate topics predicting;topics embedding input;text generation;language useful predicting;chosen topics generated;method generating sentences;approach predict linguistic;topics predicts topics"}, "edb49aa423afc210facec998277923c4b75e4648": {"ta_keywords": "spinel zinc sd;normaltype spinel zinc;zinc sd sc;spinel zinc;zinc sd;antiferromagnetic phase transition;properties normaltype spinel;antiferromagnetic phase;studied neutron diffraction;structural phase transition;occurs antiferromagnetic phase;neutron diffraction;normaltype spinel;structural properties normaltype;spectroscopy arpes structural;sc studied neutron;neutron diffraction function;antiferromagnetic;structural phase;arpes structural phase;resolved photoemission spectroscopy;zinc;sd sc sr;occurs antiferromagnetic;phase transition temperature;structural properties;photoemission spectroscopy;phase transition cubic;photoemission spectroscopy arpes;studied neutron", "pdf_keywords": ""}, "4e749b2e0728044af44d50a708fc99d49359ea0b": {"ta_keywords": "parallelized unsupervised learning;neural networks parallelized;parallelized neural networks;networks parallelized neural;parallelized parallelized neural;accuracy parallelized neural;advantage parallelized neural;parallelized neural;parallelized unsupervised;unsupervised learning models;unsupervised learning;parallelized parallelized;networks parallelized;variety parallelized unsupervised;accuracy parallelized;parallelized;advantage parallelized;learning models;substantial advantage parallelized;neural networks terms;neural networks able;models context parallelized;context parallelized parallelized;neural networks;performance variety parallelized;terms accuracy parallelized;context parallelized;learning;learning models context;unsupervised", "pdf_keywords": "transduction tasks translating;language translation tasks;unsupervised model translation;corpora languages tasks;translation tasks languages;translating languages written;translating languages;languages tasks;transducers unsupervised character;translation tasks;character level transduction;model translation tasks;related languages written;translation task significantly;model transliteration;translation tasks combination;tasks translating closely;present model transliteration;tasks informal romanization;languages written different;transduction tasks analyze;tasks translation characterization;languages tasks different;linguistics computational;romanization decipherment;languages remain elusive;level gram language;decipherment related language;accuracy romanization decipherment;script language russian"}, "9700940262cd5e797ab81eee464c3b3a16295cba": {"ta_keywords": "speech enhancement static;derived speech enhancement;speech enhancement;speech enhancement pre;capabilities speech enhancement;enhancement static adaptation;method speech dereverberation;speech dereverberation preprocessing;dynamic capabilities speech;variance adaptation propose;component derived speech;static adaptation;speech dereverberation;variance adaptation;static adaptation scheme;derived speech;focus variance adaptation;evaluation method speech;enhancement static;using adaptive;adaptive training scheme;using adaptive training;adaptive training;optimized using adaptive;adaptive;method speech;adaptation scheme;capabilities speech;adaptation scheme focus;enhancement", "pdf_keywords": ""}, "0d2a1c0724743de0cb74463466b075598ba36c45": {"ta_keywords": "periodic potential motion;periodic potential particle;periodic particle motion;motion particle periodic;particle periodic potential;periodic particle;assumed periodic particle;periodic potential;particle periodic;case periodic potential;motion assumed periodic;potential motion particle;potential particle motion;potential particle;motion particle;potential motion;assumed periodic;study motion particle;passes potential particle;potential particle assumed;particle motion;periodic;presented case periodic;particle motion measured;particle assumed stationary;case periodic;stationary motion;particle;time passes potential;motion measured time", "pdf_keywords": ""}, "f01f4808263ecfa221f856c34d3420166dbf5930": {"ta_keywords": "confusion level driver;estimating confusion level;trained multimodal sensor;classifier trained multimodal;driver using classifier;trained multimodal;car navigation;estimate confusion level;multimodal sensor;car navigation performance;driver confused;classifier trained;confusion level;implemented car navigation;multimodal sensor data;driver;convolutional neural;convolutional neural network;classifier;lstm;driver confused method;using classifier trained;multimodal;memory lstm;neural network;network neural;uses convolutional neural;driver using;neural network neural;level driver", "pdf_keywords": ""}, "ff7b5379641875be7357766af0b1e2bd55c74cc8": {"ta_keywords": "retrieval documents outperforms;retrieval documents;facilitate retrieval documents;differentiable search index;search index;search index dsi;learns text text;facilitate retrieval;introduce differentiable search;directly relevant docids;retrieval;search index used;learns text;text model;text text model;search;text model maps;string queries directly;differentiable search;string queries;queries directly relevant;used facilitate retrieval;paradigm learns text;demonstrate differentiable search;index used facilitate;queries directly;index;maps string queries;index dsi new;relevant docids", "pdf_keywords": "document retrieval tasks;searching semantically structured;document retrieval task;docid representation indexing;retrieval learns text;information retrieval learns;document retrieval;models document retrieval;indexing retrieval tasks;searching documents;information retrieval;structured semantic docids;docids document learning;docid representations unstructured;standard document retrieval;approach information retrieval;searching semantically;novel indexing retrieval;indexing retrieval;docid representation learning;semantic docids;representations unstructured docidthis;improved docid representations;trainable unstructured indexing;indexing unstructured;document representation structured;retrieval tasks approach;dramatically simplifying retrieval;indexing unstructured data;unstructured docidthis"}, "3cd4ae1cac866f853bb3276d215cff18df371b67": {"ta_keywords": "estimating signal signal;estimating signal;based feature signal;feature signal;noise based feature;feature signal proposed;bayes risk decoding;signal proposed feature;background noise based;problem estimating signal;risk decoding;transformation minimum bayes;risk decoding shown;background noise proposed;signal form background;proposed feature transformation;feature transformation minimum;signal signal form;signal signal;noise proposed approach;background noise;decoding;risk minimum bayes;noise based;signal form;feature transformation;signal;signal proposed;feature transformation efficient;bayes risk minimum", "pdf_keywords": ""}, "ceef266c59698999c9283a0cda852d8bc1ce27ea": {"ta_keywords": "contextual word representation;embedding space contextual;word representation isotropy;fine tuning embedding;token types embedding;tuning embedding space;improve embedding space;word representation;improve embedding;representation isotropy embedding;tuning embedding;embedding space fine;enhance isotropy embedding;embedding space changes;space contextual word;types embedding space;isotropy embedding space;used improve embedding;embedding space;isotropy embedding;space contextual;embedding space adding;types embedding;embedding;contextual word;representation isotropy;structures token types;contextual;token types;structures token", "pdf_keywords": "linguistic representations improve;embedding models semantic;training linguistic representations;semantic downstream tasks;pre trained embedding;semantic regression;training linguistic;trained language models;pre training linguistic;models semantic regression;semantic regression task;linguistic knowledge downstream;trained embedding;embedding models bert;trained embedding models;high performance semantic;semantic downstream;performance semantic downstream;word representations;linguistic representations;knowledge downstream tasks;encoded word representations;language models;word representations fine;embedding models;representations improve performance;isotropy embedding;representations fine tuning;linguistic knowledge encoded;pre trained language"}, "b719fc66b173f8e9e0624317bb00abf10a4d5606": {"ta_keywords": "video shot basketball;histogram video shot;shot basketball games;basketball game model;shot basketball game;basketball games;algorithms video shot;histogram video;deep learning temporal;basketball game;video shot;segmentation algorithms video;based histogram video;shot basketball;basketball;learning temporal segmentation;temporal segmentation;model temporal segmentation;algorithms video;temporal segmentation algorithm;temporal segmentation algorithms;continuous frame difference;continuous frame;learning temporal;basketball games attracted;vector continuous frame;learning model temporal;deep learning model;deep learning;attentions multimedia", "pdf_keywords": ""}, "18ef33a6e040b49ba475e586202932cecbafba0d": {"ta_keywords": "event influence generation;generating event influences;event influence;dataset event influence;influence generation challenges;influence generation;event influences;trained language models;generating event;influences conditioned context;event influences conditioned;distance inferred knowledge;conditioned context distance;context distance inferred;language models;method generating event;event;context distance;language models evaluated;large dataset event;inferred knowledge;inferred knowledge method;dataset event;conditioned context;influence;pre trained language;influences conditioned;influences;judgments closeness reference;human judgments closeness", "pdf_keywords": "event influence generation;generate event influences;generate event influence;natural language network;context question answering;generated event influences;influence generation datasets;language models generate;reasoning event influence;context relationship events;question answering;influence generation task;event influencewe introduce;curated event influence;event discovering;event discovering important;reasoning chain generated;event influence evaluation;question answering benchmark;models generate event;event influence graphs;generating target events;generate event;source events context;performance question answering;generated event;event terms automated;natural language learning;knowledge generation;context experiments"}, "e5acad5bba23a8c3a9f7cd24f7694ab10357ebc7": {"ta_keywords": "dereverberation speech recognition;speech dereverberation separation;speech dereverberation;separation performance reverber;multichannel reverberant;channel reverberant;voice activity detection;end dereverberation speech;decent speech dereverberation;multi channel reverberant;operates multichannel reverberant;speech recognition;channel reverberant achieves;speech recognition operates;dereverberation speech;reverber;performance reverber;reverberant achieves decent;reverberant achieves;implementing voice activity;multichannel reverberant condition;voice activity;dereverberation separation performance;implementing voice;reverberant;proposed implementing voice;reverberant condition improves;wsj1 2mix corpus;reverberant condition;recognition operates multichannel", "pdf_keywords": "beamforming speech recognition;advanced speech processing;dereverberation beamforming speech;beamforming speech;speech processing advanced;separation speech recognition;advanced voice activity;voice activity detection;speech recognition dereverberation;speech recognition multichannel;dereverberation neural beamforming;multichannel speech recognition;training dereverberation beamforming;speech processing;advanced advanced speech;speech recognition;voice activity;signal neural beamformer;neural beamformer;multichannel speech;neural beamforming;based speech recognition;neural beamformer method;optimization dereverberation beamforming;speech channels;dereverberation beamforming separation;context multichannel speech;dereverberation beamforming approach;speech channels method;denoising separation speech"}, "4b9b7240ef9b6bc442044684ed5646ef02897d87": {"ta_keywords": "academic advising domain;advising domain planning;planning competition domain;academic advising;planning competition;advising;propose academic advising;advising domain;domain planning competition;planning;stochasticity multivalued domain;competition domain exhibits;domain planning;competition domain;propose academic;academic;domain variables concurrent;variables concurrent actions;stochasticity multivalued;concurrent actions;domain exhibits;competition;true stochasticity multivalued;multivalued domain;multivalued domain variables;domain;stochasticity;domain variables;variables concurrent;exhibits true stochasticity", "pdf_keywords": ""}, "c3aa698b562e91f78a042b938ffce1877b6e859c": {"ta_keywords": "natural artificial biological;artificial biological;biological artificial biological;artificial biological biological;biological artificial;biological biological artificial;natural artificial;artificial biological artificial;artificial;biological;biological biological;natural", "pdf_keywords": ""}, "728a6850882a0d8ef5551949cc2baee1e1667cd8": {"ta_keywords": "optimal bribery schemes;bribery schemes voting;optimal bribery;finding optimal bribery;bribery schemes;schemes voting;bribery;schemes voting domains;optimal set agents;voting domains problem;maximal number agents;complexity finding optimal;voting domains;selection conjecture problem;equivalent finding optimal;related algebraic complexity;algebraic complexity finding;algebraic complexity;complexity finding maximal;agents use selection;finding optimal;selection conjecture;problem finding optimal;use selection conjecture;optimal;finding optimal set;complexity finding;optimal set;selection problem related;set agents", "pdf_keywords": ""}, "df8ae2068d17d969db6ab2d27108776e99413975": {"ta_keywords": "natural language inference;inference natural language;language inference np;language inference;inference np text;text graph data;known text graph;graph text andgraph;np text graph;text graph;text graph text;text andgraph data;graph text;linguistic data;text andgraph;linguistic data improve;inference natural;natural language;inference np;np inference;inference;andgraph data;andgraph data method;inference problem known;graph data;performance np inference;method inference natural;np inference problem;inference problem;power linguistic data", "pdf_keywords": "entailment model conceptnet;natural language inference;learning entailment;learning entailment low;knowledge source conceptnet;graph based entailment;neural network entailment;entailment model deep;trained entailment model;predict entailment relationships;reasoning knowledge bases;classifiers predict entailment;entailment model classifying;conceptnet external knowledge;textual entailment;purpose learning entailment;network entailment model;determining textual entailment;trained entailment;entailment relationships;based entailment model;texts textual entailment;language inference tasks;guided examples entailment;predict entailment;entailment relationships premises;knowledge source graph;textual entailment demonstrate;external knowledge source;utilizes entailment model"}, "a7abd783de8d21d640e41d31ec89f2c1caec4e42": {"ta_keywords": "sense disambiguation disambiguation;word sense disambiguation;sense disambiguation;unsupervised word sense;disambiguation disambiguation;learning model disambiguation;disambiguation disambiguation able;disambiguation performed using;able correct disambiguation;disambiguation performed;disambiguation scores;disambiguation able correct;correct disambiguation scores;disambiguation;disambiguation able;correct disambiguation;sense predictions texts;application unsupervised word;model disambiguation performed;unsupervised word;unsupervised machine learning;model disambiguation;correct sense predictions;word sense;sense predictions;unsupervised;unsupervised machine;implements novel unsupervised;novel unsupervised machine;predictions texts using", "pdf_keywords": "word sense disambiguation;sense disambiguation disambiguation;sense disambiguation;sense representations disambiguation;unsupervised word sense;knowledge free disambiguation;word disambiguation based;supports interpretable disambiguation;interpretable disambiguation context;interpretable disambiguation;disambiguation based;disambiguation words;word disambiguation;sense disambiguation open;perform disambiguation words;free disambiguation;disambiguation based precessing;interpretable word sense;disambiguation context;disambiguation disambiguation;word sense predictions;disambiguation models;disambiguation context uses;language perform disambiguation;word senses unsupervised;disambiguation models implemented;word clusters;free disambiguation implements;disambiguation context paper;clustering word clusters"}, "58961f0ea3291ddab697fbe5be999a0793b0efaf": {"ta_keywords": "codes dss distributed;distributed storage codes;storage codes distributed;codes distributed distributed;distributed distributed error;dss distributed distributed;codes distributed;dss rely distributed;distributed storage systems;failures distributed distributed;distributed distributed storage;dss distributed;distributed networks dss;storage distributed distributed;distributed storage distributed;storage systems distributed;storage systems immune;distributed error correcting;systems distributed distributed;correcting codes dss;distributed error;rely distributed distributed;distributed storage;distributed distributed distributed;codes dss;storage codes;failures distributed;storage distributed;distributed distributed;systems distributed", "pdf_keywords": ""}, "f75e691daae9133941c9a083e319b39bd837d456": {"ta_keywords": "joint knowledge embeddings;entity alignment joint;knowledge graphs unified;knowledge embeddings;knowledge embeddings method;alignment joint knowledge;knowledge graph completion;entity alignment improve;entity alignment;improvements entity alignment;improve knowledge graph;alignment improve knowledge;approach entity alignment;knowledge graph;aligned entities;knowledge graphs;encodes entities relations;aligned entities present;set aligned entities;entities relations;various knowledge graphs;embeddings method jointly;entities relations various;jointly encodes entities;joint knowledge;entities;entity;relations various knowledge;low dimensional semantic;entities present iterative", "pdf_keywords": ""}, "80a085a79ac6cee94f21d21ab8ca302458c4e131": {"ta_keywords": "loss shared data;protecting information loss;information loss shared;information loss;network cloud;shared data network;protecting information;data network cloud;network cloud framework;communicated data maintaining;cloud;shared data;data network;loss function known;framework protecting information;loss shared;version parametric loss;loss function reduces;inference accuracy loss;reduces information;parametric loss function;loss function;cloud framework;parametric loss;communicated data;content communicated data;cloud framework based;accuracy loss function;function reduces information;data maintaining", "pdf_keywords": "noisy data cloud;spread noisy data;data collective attacks;offloaded machine learning;noise learning;learning noise;learning noise networks;cloud compromising accuracy;data desired noise;noise networks;learning data storage;data cloud;accuracy privacy;protecting privacy large;noise learning able;protecting privacy data;spread personal data;preserving privacy;secure data sharing;network autonomous noise;flattened noise learning;data cloud compromising;privacy large databases;noisy data;privacy large class;data sharing;noisy data critical;secure separation datasets;noise networks based;learning noise tensor"}, "464b47a6a395fa1338e230254965cf5f669e715c": {"ta_keywords": "discovery polysynthetic languages;empirical discovery polysynthetic;sparse discovery;discovery strategy polysynthetic;provide sparse discovery;sparse discovery strategy;discovery polysynthetic;good sparse discovery;effective sparse discovery;model sparse discovery;polysynthetic languages based;polysynthetic languages;sparse discovery presence;strategy polysynthetic languages;empirical discovery;novel empirical discovery;neurons provide sparse;discovery strategy remarkably;sparse;stream model sparse;provide sparse;remarkably effective sparse;babbling model remarkably;model sparse;effective sparse;discovery strategy babbling;babbling stream model;polysynthetic;discovery;good sparse", "pdf_keywords": ""}, "ca57443fcb87f03267fccee162a4924c56062c6f": {"ta_keywords": "dynamics xmath0he curvature;curvature dynamics xmath0he;related curvature xmath0he;gaussian curvature dynamics;xmath0he curvature;curvature xmath0he;xmath0he curvature necessarily;effect gaussian curvature;curvature xmath0he good;gaussian curvature;curvature dynamics;dynamics xmath0he;curvature related;related curvature;curvature;curvature curvature;curvature necessarily;curvature curvature related;curvature necessarily large;curvature related curvature;model xmath0he;large depends curvature;depends curvature curvature;depends curvature;xmath0he good model;good model xmath0he;effect gaussian;gaussian;xmath0he;xmath0he good", "pdf_keywords": ""}, "d3304b926cfcd91110bd5ba01db21d26ce5fca2d": {"ta_keywords": "paraphrastic sentence embeddings;learning paraphrastic sentence;learning paraphrastic;data learning paraphrastic;sentence embeddings data;type sentence embeddings;learn paraphrastic sentence;learn paraphrastic;sentence embeddings;translation learn paraphrastic;sentence embeddings evaluate;neural machine translation;paraphrastic sentence;sentence embeddings setting;paraphrastic;par type sentence;machine translation learn;embeddings data quality;embeddings setting par;type sentence;embeddings evaluate pairs;machine translation;embeddings evaluate;embeddings;embeddings data;par type;neural machine;par manually type;sentence;manually type pairs", "pdf_keywords": "embeddings based paraphrastic;sentence embeddings paraphrastic;sentence embeddings translations;paraphrastic sentence embeddings;embeddings paraphrastic sentence;paraphrastic sentence embedding;embeddings paraphrastic;embedding model paraphrastic;paraphrastic word embedding;embeddings translations bilingual;neural networks paraphrastic;neural machine translation;paraphrastic sentence embeddingswe;learning paraphrastic sentence;paraphrases translation bilingual;learning paraphrastic;sentence embeddings data;trained simple paraphrastic;bilingual sentence pairs;networks paraphrastic sentence;sentence embeddings finding;sentence embedding model;data learning paraphrastic;sentences downstream tasks;sentence embeddings;generate sentential paraphrases;train paraphrastic sentence;learn paraphrastic sentence;sentential paraphrases translation;paraphrase pairs ability"}, "305a1251a68fb16835876d8c99de498472c0cd8f": {"ta_keywords": "coded computation scheme;coded computation schemes;enables coded computation;coded computation;known coded computation;code proposed locality;design coded computation;view coded computation;coded computation lens;locality codes demonstrate;locality codes;computation lens locality;best known coded;known coded;local decoding scheme;approach enables coded;coded;computation schemes significantly;muller code proposed;lens locality codes;enables coded;functions viewpoint locality;local decoding;computation schemes;reed muller code;computation scheme;defined code rederives;defined code;decoding scheme;computation scheme function", "pdf_keywords": ""}, "3332dc72fbe3907e45e8a500c6a1202ad5092c0f": {"ta_keywords": "context spectral clustering;representations target spectrogram;spectral clustering;source separation;spectral clustering approach;separation context spectral;source separation context;single channel mixtures;mixtures multiple speakers;spectrogram;target spectrogram;problem source separation;spectrogram proposed;channel mixtures multiple;target spectrogram proposed;channel mixtures;spectrogram proposed model;affinity matrix preliminary;signal quality mixtures;affinity matrix;pairwise affinity matrix;context spectral;clustering;affinity matrix approximates;mixtures multiple;ideal affinity matrix;single channel;network representations target;pairwise affinity;multiple speakers", "pdf_keywords": "clustering speech separation;deep clustering speech;separation deep learning;source separation deep;clustering speech;model speech separation;segmentation audio;method separating speech;separating speech;separating speech noise;acoustic source separation;deep clustering;speech separation task;scale speech separation;approach deep clustering;propose deep clustering;speech separation;segmentation audio based;based segmentation audio;speech separation experiments;audio based embeddings;speech separation able;separation deep;deep clustering use;separate speech;method separate speech;source separation;deep networks trained;separating speakersspeech;partition based training"}, "6e7cfed8815cce163efac9d17b1109849c050c6b": {"ta_keywords": "zeeman field spin;spinor xmath0 heisenberg;spin orbit coupling;dynamics spinor xmath0;spin orbit interaction;heisenberg model spin;coupling strength zeeman;spin dynamics spinor;coupling strength spin;spinor xmath0;model spin orbit;dynamics spinor;field spin dynamics;field spin orbit;spin dynamics;strength spin orbit;control spin orbit;spin orbit;coupling strength zee;effect zeeman field;xmath0 heisenberg model;model spin;field spin;control spin;strength zeeman field;effect zeeman;xmath0 heisenberg;orbit coupling;spinor;orbit coupling strength", "pdf_keywords": ""}, "7038b181f776e9cd587d4d61cb68692fdac8ec26": {"ta_keywords": "dynamical mode decomposition;signalized traffic flow;control signalized traffic;modes signalized operator;oscillatory modes operator;mode decomposition dm;mode decomposition;oscillatory modes signalized;signalized traffic;known dynamical mode;modes signalized;traffic flow networks;modes operator;modes operator use;mode decomposition dmd;signalized operator;dynamical mode;traffic flow;signalized operator generalization;oscillatory modes;uses dynamical mode;control signalized;analysis control signalized;approximation oscillatory modes;dmd oscillatory modes;operator generalization known;decomposition dmd oscillatory;flow networks approach;operator generalization;generalization known dynamical", "pdf_keywords": "traffic flow signalized;signalized traffic flow;dynamic mode decomposition;vehicle flows estimation;dynamical mode decomposition;signal phases flow;phase separation traffic;traffic flow data;flow signalized;measured vehicle flows;queuing dynamics signalized;structure traffic flow;separation traffic flow;prediction traffic flow;flow data phase;signalized traffic;control signalized traffic;patterns traffic flow;flows estimation;flow signalized networks;mode decomposition approach;timing parameters traffic;estimation length queue;flows estimation length;phases flow data;koopman operator estimation;analysis dynamics queue;predict structure traffic;dynamics signalized network;dynamics signalized"}, "1e58c9d1153d2f25d94b3a12b785bd7abe43bd1c": {"ta_keywords": "semi supervised learning;semi supervised;change point detection;learning unsupervised;supervised learning unsupervised;learning unsupervised manner;autoencoder learned;learned autoencoder;autoencoder learned representations;supervised learning;supervised;learned autoencoder learned;classification;improved representations classification;framework semi supervised;neural network learn;superior learned autoencoder;learn improved representations;unsupervised manner change;autoencoder;representations classification;learned representations;class changes examples;representations classification provide;learning;improved representations;classification provide;change point;class changes;point detection", "pdf_keywords": "semi supervised sequential;semi supervised classification;classification using change;change point detection;implementation semi supervised;detection change points;semi supervised learning;semi supervised;supervised semi supervised;unsupervised change point;supervised semi;approach supervised semi;change point detectionwe;method semi supervised;supervised sequential classification;unsupervised detection change;detecting changes;learning detection change;change points sequence;supervised sequential;unsupervised change;sequential classification;sequence labeled data;sequential classification using;generated change points;classification non recurrent;points improve classification;change points arise;classification recurrent;supervised classification"}, "5bad092098ba7400e19468a06cb8b238c43b7637": {"ta_keywords": "spin polarized electron;polarized electron spin;electron spin;electron spin orbit;spin orbit coupling;zeeman field spin;strength electron spin;coupling spin polarized;orbit coupling spin;coupling spin;coupling strength electron;polarized electron;spin polarized;field spin orbit;spin orbit;effect zeeman field;field spin;control spin orbit;electron strongly;affected field spin;effect zeeman;orbit coupling strength;electron strongly affected;orbit coupling;strength electron strongly;orbit coupling used;electron;strength electron;control spin;spin", "pdf_keywords": ""}, "c81bb5ff79e8c7f65a3e28b7ba52d90deaa32fde": {"ta_keywords": "collaborative editing code;online collaborative editing;collaborative editing;collaborative editing particularly;success collaborative editing;editing code switching;code switching positively;code switching;online collaborative;collaborative;code switching performance;effect code switching;editing particularly borrowing;performance online collaborative;editing code;success collaborative;languages predictor social;borrowing technical language;social effect code;editing;editing particularly;technical language use;language use;language use different;associated success collaborative;social;social influence;code;predictor social influence;switching performance online", "pdf_keywords": ""}, "f26f17ec49f2593bcc926051394871480a80c0c2": {"ta_keywords": "crosslingual word embedding;crosslingual word similarity;monolingual embedding;monolingual embedding spaces;lexicon induction crosslingual;bilingual lexicon induction;embedding expresses monolingual;expresses monolingual embedding;word embedding;bilingual lexicon;sets bilingual lexicon;approach crosslingual word;word embedding expresses;induction crosslingual word;data sets bilingual;crosslingual word;embedding spaces probability;novel approach crosslingual;word similarity;induction crosslingual;expresses monolingual;approach crosslingual;lexicon induction;sets bilingual;monolingual;bilingual;embedding expresses;crosslingual;embedding;embedding spaces", "pdf_keywords": "bilingual word embedding;bilingual embedding;bilingual word embeddings;learned bilingual embedding;monolingual embedding;unsupervised method bilingual;bilingual word mapping;bilingual embedding evaluate;supervised bilingual mappings;empirically bilingual mappings;supervised methods bilingual;monolingual embedding spaces;supervised bilingual;word embedding mappings;structure bilingual words;expresses monolingual embedding;method supervised bilingual;embedding expresses monolingual;learning bilingual word;bilingual lexicon iteratively;empirically bilingual;bilingual lexicon induction;target word embeddings;word embeddings compute;reconstructing distribution words;learned bilingual lexicon;word embedding;bilingual lexicon;bilingual mappings;bilingual mappings widely"}, "28028458d75bf9281200389a880741eb6d06a3a4": {"ta_keywords": "datalog specifications large;datalog specifications;recovery datalog specifications;extract specifications benchmark;method recovery datalog;datalog;programming extract specifications;logic programming extract;specifications large software;specifications benchmark modules;inductive logic programming;recovery datalog;extract specifications;benchmark modules test;benchmark modules;large software systems;specifications benchmark;logic programming;large software;precision recall extensions;programming extract;modules test suite;recall extensions method;novel method recovery;software systems;recall extensions;test suite;precision recall;benchmark;method recovery", "pdf_keywords": ""}, "436380dd75d8ff3f2debb29913bd2fe8dde0b684": {"ta_keywords": "separating speech mixtures;speech mixtures based;approach separating speech;source separation approach;speech mixtures;separating speech;context source separation;source separation;matrix factorization;source separation presence;matrix factorization problem;problem source separation;mixture approach based;complex matrix factorization;mixtures based;mixture approach;interfering speaker;mixtures based phase;based phase mixture;presence interfering speaker;phase mixture approach;separation approach;separation approach demonstrated;novel approach separating;noise benchmark image;gaussian noise benchmark;approach separating;phase mixture;factorization;separation presence interfering", "pdf_keywords": "matrix factorization nmf;matrix factorization;complex matrix factorization;negative matrix factorization;matrix factorization single;matrix factorization cmf;matrix factorization used;source separation factorize;factorization nmf proposed;factorization nmf methods;factorization nmf;separation factorize;separation factorize magnitude;factorization single channel;matrix included decomposition;matrices speech minimize;factorization cmf approach;matrix phase input;factorize magnitude input;matrices speech;incorporating phase decomposition;factorization;phase input matrix;input matrix generally;factorization single;complex matrix phase;matrix phase;factorization cmf;methods source separation;factorization used"}, "6992f54509c139455c3cffa9b0e4ae5c19ebff82": {"ta_keywords": "effect xmath0 xmath1;effect xmath0;xmath9 xmath10 xmath11;xmath8 xmath9 xmath10;xmath7 xmath8 xmath9;xmath2 xmath3 xmath4;xmath0 xmath1 xmath2;xmath10 xmath11 xmath12;xmath1 xmath2 xmath3;xmath11 xmath12 xmath13;xmath6 xmath7 xmath8;xmath5 xmath6 xmath7;xmath3 xmath4 xmath5;xmath10 xmath11;xmath9 xmath10;xmath0 xmath1;xmath1 xmath2;xmath8 xmath9;xmath4 xmath5 xmath6;xmath11 xmath12;xmath12 xmath13;xmath2 xmath3;xmath3 xmath4;xmath11;xmath0;xmath4 xmath5;xmath6 xmath7;xmath13;xmath7 xmath8;xmath12", "pdf_keywords": ""}, "30c6be4c7f549a2ec7328d24ecc0a54fbf90d41c": {"ta_keywords": "optimal control wireless;optimal control policies;problem optimal control;optimal control;choosing optimal control;partial differential equations;pdes large state;control wireless networks;state space method;use stochastic process;equations pdes large;control wireless;equations pdes;solving partial differential;pdes;process choosing optimal;stochastic process choosing;differential equations pdes;control policies proposed;method solving partial;based use stochastic;pdes large;stochastic process;control policies;use stochastic;policies proposed method;partial differential;applied problem optimal;solving partial;stochastic", "pdf_keywords": ""}, "4da018847a0f44378e6a1ded93fee672a3c7c370": {"ta_keywords": "end speech recognition;speech recognition;speech recognition called;novel non parametric;tokens non parametric;mean field model;end speech;tc mask tribov;non parametric mean;end end speech;t_ tc mask;model non autoregressive;non parametric;recognition;mask tribov t_;tc mask;parametric mean field;recognition called mask;non autoregressive end;recognition called;model non;autoregressive end end;mean field;speech;autoregressive end;mask tribov;non autoregressive;tribov t_ tc;mask tribov achieves;called mask tribov", "pdf_keywords": "models speech recognition;end speech recognition;speech recognition asr;automatic speech recognition;speech recognition;speech recognition proposed;models speech;connectionist temporal classification;predict connectionist temporal;automatic speech;mask predict connectionist;flow models speech;recognition asr based;recognition asr;temporal classification mask;predict connectionist;fast inference speed;end automatic speech;speech recognition use;asr models attention;model mask ctc;decoder model;decoder model mask;encoder decoder model;classification mask predict;temporal classification achieves;temporal classification;mask ctc based;fast inference;sequence learning"}, "abaf39dc9d1b156ddf387230611f5102378d052c": {"ta_keywords": "unsupervised post correction;large corpora approach;large corpora;data large corpora;corpora;corpora approach combines;corpora approach;sequence model attention;power large corpora;large corpora ability;attention averaging single;post correction combines;single input correction;post correction;input attention averaging;corpora ability;attention averaging methods;corpora ability use;attention averaging;input correction;multi input attention;model attention averaging;correction new decoder;input attention;word error rates;decoder multi;decoder;input correction new;decoder multi input;new decoder multi", "pdf_keywords": "correcting ocr based;text correction ocr;correcting ocr;error correction ocr;character recognition ocr;recognition ocr exploits;recognition ocr critical;text transcribed;method correcting ocr;recognition ocr;based text correction;document recognition ocr;ocr input;text dataset transcribed;sequence model attention;ocr input random;text correction;optical character recognition;transcription error correction;attention based sequence;ocr critical achieving;hypothesis ocr input;character recognition;accurate reading text;multi input attention;accurate robust text;text transcribed domains;correction ocr;ocr problem based;correction ocr problem"}, "63bfe58735f44b0af24da3c2cb6b1651b001b83c": {"ta_keywords": "distributed secret sharing;complexity secret sharing;secret sharing networks;secret sharing network;secret sharing;sharing networks satisfies;sharing networks satisfy;distributed secret;algorithm distributed secret;problem distributed secret;shares secret;sharing networks;communication complexity secret;shares secret participants;pass shares secret;complexity secret;sharing network networks;networks satisfies condition;sharing network;networks satisfying propagating;satisfying propagating dealer;networks satisfy condition;bounds communication complexity;networks satisfies;network networks satisfying;networks satisfy;directly pass shares;propagating dealer condition;secret participants;secret participants instead", "pdf_keywords": ""}, "5d69380565aa258bfa54005c9ba05e30675be227": {"ta_keywords": "exploratory learning hierarchical;learning hierarchical;learning hierarchical semi;method exploratory learning;exploratory learning;hierarchical semi supervised;supervised tasks;semi supervised tasks;novel method exploratory;supervised;supervised tasks method;hierarchical;novel classes;novel novel classes;semi supervised;learning;novel classes context;hierarchical semi;based discovery;method based discovery;method exploratory;based discovery novel;discovery;discovery novel;classes;exploratory;discovery novel novel;novel method;tasks;tasks method based", "pdf_keywords": ""}, "62d6ccd01c2e022a385add5e689b4561b0fbfd88": {"ta_keywords": "speech segment proposals;based speaker diarization;speaker diarization;speaker diarization method;novel speaker diarization;proposals computes speaker;speaker diarization rsp;overlapped speech segment;speaker embeddings;computes speaker embeddings;proposal based speaker;speech segment;speaker embeddings experiments;computes speaker;generates overlapped speech;overlapped speech;segment proposals;segment proposals computes;diarization datasets;based speaker;experiments diarization datasets;speaker;embeddings experiments diarization;propose novel speaker;diarization datasets reveal;diarization;results diarization datasets;diarization method;novel speaker;region proposal based", "pdf_keywords": "speech segment proposals;predicts speech segment;speaker classification boundary;speech segmenting;speaker diarization rnsd;overlapped speech segment;data speech segmenting;speech segment;speaker diarization framework;speaker diarization;based speaker diarization;speaker diarization combines;novel speaker diarization;speaker recognition challenge;speaker classificationspeaker recognition;proposals computes speaker;speaker recognition;speaker diarization novel;perform speaker classification;obtain speech segment;framework speech segmenting;speech segmenting extract;speaker diarization method;speech prediction speaker;prediction speaker embeddings;speech prediction;speaker verification;speaker verification unified;speaker verification based;speaker classification"}, "9ba545841b837fa077579290e252eb00351ebeb0": {"ta_keywords": "non convex learning;convex learning method;convex learning;convex learning proposed;distributed non convex;polynomial time convex;convex non convex;non convex problems;non convex;convex problems proposed;convex non;convex problems;learning method called;learning proposed method;learning method;time convex non;novel distributed non;convex;time convex;propose novel distributed;learning proposed;distributed non;novel distributed;distributed;method efficient scalable;class polynomial time;polynomial time;learning;scalable provide convergence;marina distributed non", "pdf_keywords": "distributed learning compression;new distributed optimization;distributed optimization;communication compression strategy;distributed optimization method;communication compression;distributed nonconvex optimization;distributed optimization biased;communication channel compression;convex distributed learning;complexity distributed learning;method distributed optimization;learning distributed communication;compression networks analysis;distributed learning bounded;novel communication compression;present distributed optimization;distributed learning arbitrary;channel compression networks;distributed learning non;compressing compression network;distributed learning algorithm;algorithm distributed learning;distributed learning problems;compression network;concerning distributed learning;distributed learning theorems;distributed learning linear;learning distributed;network distributed learning"}, "ea1f61270480a8dec54ec571c0e6ce116d096241": {"ta_keywords": "polyphonic sound event;sound event detection;hidden model polyphonic;segment sound event;model polyphonic polyphonic;polyphonic model proposed;polyphonic polyphonic model;polyphonic polyphonic;sound event;polyphonic sound;polyphonic model;detecting segment sound;polyphonic;model polyphonic;sound event post;novel polyphonic sound;event detection;propose novel polyphonic;novel polyphonic;event detection method;framewise detection methods;framewise detection;multilabel classification;segment sound;multilabel classification problem;process detection;required framewise detection;apply multilabel classification;multilabel;hybrid convolutional long", "pdf_keywords": ""}, "598321d9c3eb5c035b449e19e539b6fa04b3802a": {"ta_keywords": "copies maximum entropy;maximum entropy data;extract maximum entropy;maximum entropy;entropy data;observation maximum entropy;million copies binary;entropy data set;maximum entropy proportional;copies binary;entropy;copy binary;single copy binary;copies maximum;binary;entropy proportional;number copies maximum;xmath0 million copies;copy binary method;xmath1 million copies;entropy proportional square;binary method;binary method based;copies single copy;number copies method;single copy;number copies;copies;extract maximum;copies method", "pdf_keywords": ""}, "8f1f43408baf1ccb0ec3e7985592326c83ee276d": {"ta_keywords": "chat oriented dialog;machine translation smdm;statistical machine translation;dialog experiments combination;dialog experiments;building chat oriented;machine translation;oriented dialog experiments;dialog;approaches building chat;building chat;translation smdm approaches;chat oriented;oriented dialog;translation smdm;chat;example statistical machine;statistical machine;smdm approaches building;smdm approaches;smdm;translation;approaches provides best;example statistical;combination approaches provides;machine;statistical;approaches provides;compares;example", "pdf_keywords": ""}, "07a9f47885cae97efb7b4aa109392128532433da": {"ta_keywords": "models translation noisy;translation noisy noisy;translation quality attention;translation noisy;attention based models;models attention based;high translation quality;models attention;quality attention based;noisy output models;models translation;based models translation;attention based;class attention based;translation quality;yield high translation;original models attention;input noisy;noisy input noisy;input noisy output;noisy noisy input;quality attention;noisy input;attention;noisy output;noisy noisy noisy;class attention;noisy noisy;noisy;new class attention", "pdf_keywords": "attention improve translation;hard coded attention;neural machine translation;machine translation outperforms;simpler attention distributions;translation use attention;coded attention outperforms;attention simpler attention;machine translation model;machine translation elementary;crucial translation quality;attention encoded;random attention equivalent;coded attention variant;coded attention improves;attention distributions;attention equivalent;attention distributions finally;outperforms standard attention;attention simpler;coded attention describing;assignment attention encoded;attention optimal;simpler attention;coded attention;attention heads encoder;translation outperforms;self attention optimal;translation systems results;coded self attention"}, "8afbc4188be9e9452ce1fe868ebe217179d36793": {"ta_keywords": "linear regression speech;adaptive speech automatic;adaptive speech recognition;adaptive speech;adaptive adaptive speech;training adaptive speech;regression speech exemplars;regression speech;speech recognition naimark;recognition speech method;recognition naimark speech;automatic recognition speech;speech automatic recognition;naimark speech method;recognition speech;regression background noise;speech recognition;speech exemplars based;speech automatic;speech method;speech method applied;linear regression;background noise method;training adaptive adaptive;speech method shown;based linear regression;linear regression linear;regression linear regression;linear regression background;regression linear", "pdf_keywords": ""}, "3426fadf73a5ce418486e640b26b3d2470d932b5": {"ta_keywords": "language adversarial classification;end speech recognition;language adversarial;paired language adversarial;experiments multilingual end;multilingual end;speech recognition models;results experiments multilingual;speech recognition;multilingual end end;adversarial classification objective;experiments multilingual;multilingual;adversarial classification;recognition models trained;languages experiments;end speech;languages experiments demonstrate;end end speech;objective paired language;adversarial;paired language;models trained;languages;language;pretraining objectives context;speech;large number languages;additional pretraining objectives;pretraining objectives", "pdf_keywords": "multilingual speech recognition;model multilingual pretraining;language adversarial classification;language model trained;adaptation multilingual endto;speech model trained;pretrain language prediction;multilingual convolutional neural;model language adversarial;models trained languages;language adversarial;paired language adversarial;lingual speech recognition;adaptation multilingual;phoneme objective adversarial;multilingual speech;target language adaptation;multilingual model adapted;resource multilingual convolutional;multilingual pretraining;multilingual models;language adversarialwe;languages model pretrained;multilingual models designed;multilingual pretraining batch;models support multilingual;multilingual convolutional;paired language adversarialwe;end speech recognition;train model multilingual"}, "8de431e0e62653711136836642af38179731c2f0": {"ta_keywords": "erasure coded distributed;coded distributed storage;secure codes distribution;data erasure coded;erasure coded;secure codes;coded distributed;storage systems algorithms;codes distribution;codes distribution data;distributed storage systems;construction secure codes;information theoretic code;distributed storage;erasures algorithms designed;guarantee security data;security data maintaining;erasures algorithms;codes optimal;data erasure;theoretic code structure;storage systems;theoretic code;distribution data erasure;coded;class codes optimal;codes optimal terms;algorithms construction secure;structure class codes;security data", "pdf_keywords": "distributed storage codes;codes distributed storage;storage codes distributed;storage codes ensure;secure codes distributed;optimal codes storage;theoretic erasure codes;regenerating codes distributed;distributed networks codes;codes storage;security distributed storage;explicit storage codes;secure distributed storage;storage codes;codes distributed networks;storage repair codes;codes storage data;recovery distributed storage;repair codes distributed;coding security distributed;erasure codes;consider distributed storage;erasure codes repair;algorithms distributed storage;distributed storage;distributed storage systems;secure codes;codes distributed;distributed storage nodes;bound storage repair"}, "fdf1aec2da3597010c31138159574b1016019f73": {"ta_keywords": "preference reasoning communities;choice preference reasoning;social choice preference;preference reasoning decision;preference reasoning powerful;preference reasoning;computational social choice;choice preference;social choice;impact social choice;contributions social choice;preference;reasoning decision making;reasoning decision;reasoning communities;decision making;computational social;empirical impact social;decision making process;empirical contributions social;contributions computational social;decision making significant;choice;decision;reasoning;reasoning powerful important;reasoning powerful;impact social;empirical contributions computational;social", "pdf_keywords": ""}, "538deb39d57bef62833c492a56c796a2bafa340f": {"ta_keywords": "method active learning;active learning;active learning resulting;perform active learning;active learning support;active learning powerful;support vector machines;learning support vector;learning resulting kernels;vector machines svms;machines svms;kernel perform active;vector machines;machines svms use;svms;svms use linear;data mining propose;support vector;function kernel;data resulting kernels;scale data mining;svms use;data mining;perform active;basis function kernel;kernels;features data;essential features data;function kernel perform;qualitative features data", "pdf_keywords": ""}, "63e7e3b16e03da62a2c535ac9cfccfa3ae48b292": {"ta_keywords": "team decision experiments;measure team utility;team optimize;team optimize performance;performance arbitrary team;team utility;arbitrary team mate;accuracy team decision;centered team optimize;team utility quantifies;training artificial intelligence;human centered team;artificial intelligence ai;training artificial;measure team;intelligence ai;artificial intelligence;accuracies accuracy team;accuracy team;arbitrary team;ai;intelligence ai used;team decision;ai used human;decision experiments linear;decision experiments;method training artificial;introduce measure team;individual accuracies accuracy;centered team", "pdf_keywords": "classifier predict team;human ai teams;team informed artificial;optimizing team;team utility maximization;teamcentric optimization;predict team predictions;optimizing team performance;teamcentric optimization objective;problem optimizing team;ai teams joint;machine _supercritical teammate;performance human ai;new teamcentric optimization;ai teams;intelligence predict user;task human predict;maximizes team utility;optimizing performance team;predict team;metadecision prediction team;teams automated;optimize performance team;teams automated assistance;human ai;accurate teammate human;team scenario human;prediction team scenario;team utility methods;teamwork improving performance"}, "b2baf9e053c32abfb3c8658b9bc6d6790ae671cb": {"ta_keywords": "detection unknown words;detect unknown words;language using eye;natural language method;words natural language;natural language using;eye movement features;using eye movement;natural language;eye movement;unknown words natural;machine learning method;unknown words;novel eye movement;terms accuracy detection;words natural context;learning method detection;machine learning;accuracy detection;detect unknown;accuracy detection proposed;language using;learning method;features able detect;using eye;movement features;detection;novel machine learning;words natural;features proposed approach", "pdf_keywords": ""}, "15931520cce546bbf19b4cebeb4161c4debeabe7": {"ta_keywords": "behavior single winner;behaviors multi winner;mechanical turk;obtained mechanical turk;voting behavior;winner approval voting;voting behavior single;examine voting behavior;voting scenarios varying;voting scenarios;multi winner approval;approval voting scenarios;mechanical turk people;generally manipulate vote;examine voting;winner approval;uncertainty using behavioral;manipulate vote;winner multi winner;multi winner;approval voting;manipulate vote obtain;voting;single winner multi;vote obtain better;winning set human;single winner;outcome identify optimal;winner multi;vote obtain", "pdf_keywords": "agents manipulate votes;voting behavior study;human voting behavior;capture voting behavior;voter behavior utility;voting behavior uncertain;examine voting behavior;voting uncertainty;voting using behavioral;consistent human voting;approval voting scenarios;voting behavior;approval voting uncertainty;model voter behavior;voting process inherent;voting uncertainty simple;voter behavior;voting behavior applying;voting scenarios;voters behavior;voters behavior consider;voting scenarios varying;human voting;winner approval voting;behavior utility voter;election based votes;predicting voters behavior;accuracy predicting voters;examine voting;models voters responses"}, "931cbd9d689e9fd6bd91f4e8e1dbdd7fbb6df9de": {"ta_keywords": "speakerbeam e2e network;speakerbeam e2e;end speakerbeam e2e;recognize mixtures speakers;target speaker mixture;speaker mixture promising;speaker mixture;end speakerbeam;extracting target speaker;speakerbeam;results mixtures speakers;end end speakerbeam;target speech recognition;speech enhancement diarization;mixtures speakers;speech recognition;mixtures speakers able;target speaker;speech enhancement;novel speech recognition;speech recognition results;mixtures speakers discuss;speaker;terms speech enhancement;speech recognition based;able recognize mixtures;speakers able enhance;recognition based end;recognition results mixtures;recognize mixtures", "pdf_keywords": ""}, "a445adf335aa5212f929f67c1ca56a62c221b43a": {"ta_keywords": "oracle identify unknowns;discovery unknowns;discovery unknowns various;unknowns guide discovery;unknowns various applications;identify unknowns;unknowns various;causes unknowns;guide discovery unknowns;unknowns;oracle identify;model agnostic methodology;unknowns guide;feedback oracle identify;discovery;causes unknowns various;agnostic methodology;identify unknowns guide;underlying causes unknowns;propose model agnostic;uses feedback oracle;oracle;agnostic methodology uses;guide discovery;model agnostic;feedback oracle;propose model;methodology uses feedback;methodology;applications", "pdf_keywords": "discovering unknowns predictive;identifying unknowns predictive;discovering unknowns;unknowns predictive;problem discovering unknowns;informed discovery unknowns;discovering unknowns occurring;unknowns predictive models;discovery unknowns arise;unknowns discover unknowns;unknowns arise systematic;predictive model unknowns;discover unknowns;unknowns given predictive;task discovering unknowns;discover unknowns approach;datasets demonstrate predictions;identify unknowns intelligently;systematically searching unknowns;identifying unknowns;strategy discovering unknowns;unknowns discover;systematic biases training;unknowns intelligently;discovery unknowns;problem informed discovery;problem discovering;unknowns arise;informed discovery;unknowns occur systematic"}, "947750c717a5fbd17fc52758322d1ca201c4c6bc": {"ta_keywords": "unorganized information extraction;information extraction;information extraction aftermath;unstructured text;unstructured text file;unstructured unstructured text;information extraction key;extraction key words;unorganized information;disaster japan consists;database unorganized information;2011 disaster japan;disaster japan;unstructured;unstructured unstructured;aid unorganized information;unique unstructured unstructured;unique unstructured;text file;containing unique unstructured;extraction key;database unorganized;aftermath 2011 disaster;extraction;extraction aftermath 2011;database;phrases text;words phrases text;text;japan consists collection", "pdf_keywords": ""}, "c45a23e7c565169c5a55898683aceac458c116bb": {"ta_keywords": "chime speech separation;speech separation recognition;speech recognition;speech separation;model speech enhancement;advanced speech recognition;3rd chime speech;extraction advanced speech;speech enhancement language;chime speech;speech recognition achieve;recognition challenge chime;model speech;separation recognition challenge;separation recognition;speech enhancement;3rd chime;sri 3rd chime;beamforming robust feature;chime proposed combines;combines beamforming robust;challenge chime proposed;chime;beamforming robust;challenge chime;networks model speech;advanced speech;chime proposed;feature extraction advanced;combines beamforming", "pdf_keywords": ""}, "2b0aa68ef2c1773642ca91627a4fc03f536cc5fc": {"ta_keywords": "spin orbit interaction;interaction spin orbit;orbit interaction spin;coupled spin orbit;spin orbit coupled;orbit coupled spin;dynamics spin orbit;control spin orbit;effect spin orbit;interaction dynamics spin;suppression spin orbit;spin orbit;enhancement spin orbit;interaction spin;coupled spin;dynamics spin;orbit interaction dynamics;orbit interaction;way spin orbit;orbit interaction responsible;orbit interaction used;control spin;orbit interaction specific;effect spin;suppression spin;orbit coupled;enhancement spin;responsible enhancement spin;responsible suppression spin;specific way spin", "pdf_keywords": ""}, "f762ce106b37728df1126375981a02a589e0497c": {"ta_keywords": "stochastic gradient descent;randomized coordinate descent;proximal stochastic gradient;gradient descent;importance sampling mini;stochastic gradient;variants proximal stochastic;mini batch sampling;reduction importance sampling;proximal stochastic;coordinate descent;sampling mini batch;importance sampling;gradient descent tt;sampling mini;batch sampling;batch sampling quant;concepts variance reduction;stochastic;mini batch;randomized coordinate;variance reduction importance;variance reduction;coordinate descent tt;randomized;gradient;variants proximal;theory tt randomized;sampling;linear complexity propose", "pdf_keywords": "convergence stochastic gradient;stochastic gradient descent;stochastic gradient methods;stochastic gradients convex;stochastic gradients;computation stochastic gradients;convergence stochastic methods;stochastic gradients provide;stochastic gradients theorems;stochastic gradient supposing;stochastic sequence gradients;stochastic gradient;stochastic gradient learning;analysis stochastic gradient;properties stochastic gradients;stochastic gradients best;sequences stochastic gradient;stochastic gradients computed;gradient random stochastic;stochastic optimization theorems;stochastic gradient random;stochastic gradient vector;proximal stochastic gradient;given stochastic gradients;stochastic gradient fi;stochastic methods generalize;stochastic gradients desired;objective stochastic gradients;estimating stochastic gradients;stochastic gradients prove"}, "ec084dac14a069da2e924ff7f3d5d2fb75b9b39a": {"ta_keywords": "multinomial inverse regression;developed multinomial inverse;dimensional logistic regression;inverse regression high;high dimensional logistic;multinomial inverse;regression high dimensional;estimation high dimensional;logistic regression coefficients;logistic regression;high dimensional response;coefficients novel estimation;estimation general high;inverse regression;regression high;dimensional logistic;estimation high;effective estimation general;logistic regression guidelines;general high dimensional;high dimensional;regression coefficients novel;effective estimation;novel estimation;developed multinomial;multinomial;dimensional response;stable effective estimation;estimation general;novel estimation technique", "pdf_keywords": "sentiment preserving dimension;modeling sentiment text;sentiment preserving;estimating multinomial inverse;keywords multinomial inverse;text regression;modeling sentiment;classify text regression;multinomial inverse regression;sentiment document particularly;quantifying closely sentiment;estimating multinomial;sentiment distribution examine;sentiment distribution;sentiment correlates document;sentiment text involves;labeling sentiment;sentiment text;phrases representative;sentiment text method;sentiment correlates;joint modeling sentiment;text regression framework;keywords multinomial;methodology estimating multinomial;multinomial logistic regression;labeling sentiment document;framework sentiment preserving;analysis sentiment text;multinomial logistic"}, "bc251481aa5566b1e86a8dbd0417cdf858205e3b": {"ta_keywords": "fake news detection;preference aware fake;fact based models;aware fake news;news detection framework;news detection;fact based;preferences pattern fact;preference aware;pattern fact based;modeling preference differences;aware fake;fake news;modeling preference;models joint detection;based models joint;making pattern fact;detection framework;fact;detection framework pref;pattern fact;news;based models;learns respective preferences;based models focus;based models framework;preferences pattern;framework modeling preference;respective preferences pattern;integrating pattern fact", "pdf_keywords": "detecting fake news;detect fake news;fake news detection;false news detection;fake news content;patterns fake news;fact based fake;fake news posts;fake news predict;news detection effective;news detection;aware fake news;fake news pieces;news posts based;news based text;news detection systems;news detection tasks;news detection framework;fact based models;detecting fake;fact pattern based;joint fake news;comprehensively detecting fake;news content;fake news researchers;detect fake;preference aware fake;variety fake news;fact based model;fact checking articles"}, "dc2f6f092fa04e334dfe2e8592b6d597e00b97ca": {"ta_keywords": "aware semantic classes;automatically extracted hypernyms;sense aware semantic;hypernymy extraction terms;hypernymy extraction;extracted hypernyms;semantic classes;induced semantic classes;extracted hypernyms using;semantic classes filtering;quality hypernymy extraction;aware semantic;semantic classes using;distributional semantics using;domain taxonomy induction;using distributional semantics;distributional semantics;filtering noisy hypernyms;hypernyms using approach;using induced semantic;taxonomy induction;inducing sense aware;hypernyms using;hypernyms;noisy hypernyms;induced semantic;semantic;large scale crowdsourcing;taxonomy induction task;crowdsourcing study", "pdf_keywords": "word sense clustering;extraction hypernyms sense;hypernyms sense clusters;based extraction hypernyms;extract hypernyms lexical;extraction hypernyms;sense clusters lexical;extract word sense;automatically extracting hypernyms;extracting hypernyms;sense aware semantic;automatically extract hypernyms;hypernymy extraction terms;automatically extracted hypernyms;hypernymy extraction;sense clustering text;extract hypernyms;hypernyms lexical resources;extracted hypernyms;extracted hypernyms using;hypernyms lexical;usedword sense induction;quality hypernymy extraction;clustering word senses;clusters lexical resources;clusters lexical;clustering semantic;method extract hypernyms;clustering semantic classes;cluster vocabulary underlying"}, "02a83a01d6236149e4ead01e202b2453f9590e9e": {"ta_keywords": "group fairness diagnostic;group fairness demonstrate;group fairness;fairness objectives elements;accuracy fairness objectives;optimize accuracy fairness;fairness diagnostic based;fairness objectives;fairness diagnostic;offs group fairness;accuracy fairness;fairness demonstrate;fairness;different groups individuals;objectives elements tensor;attributes frame optimization;groups individuals treated;based confusion matrix;groups individuals;trade offs group;different groups;measure different groups;groups;confusion matrix split;individuals treated differently;characterizing trade offs;directly optimize;optimize;confusion matrix;according protected attributes", "pdf_keywords": ""}, "c263507db2c15a8b2e3c955bda7b3c29a1ebd106": {"ta_keywords": "high dimensional data;nonlinear learning;use nonlinear learning;nonlinear learning perform;supervised learning large;nonlinear learning use;supervised benchmarking large;learning use nonlinear;supervised benchmarking;perform supervised benchmarking;learning large set;supervised learning;dimensional data;dimensional data based;benchmarking large data;perform supervised learning;dimensional data particular;learning large;large data sets;learning perform supervised;structure high dimensional;large set data;high dimensional;supervised;large data;benchmarking large set;benchmarking large;data sets;benchmarking;measuring structure high", "pdf_keywords": "datasets intent detection;intent detection real;intent detection;datasets intent;new datasets intent;bert based classifier;queries live chat;intents;predict semantics natural;understanding natural language;platforms bert based;predict semantics;able predict semantics;semantics natural language;natural language;natural language able;fine grain intents;intent;intents sourced;inunderstanding natural language;nlu platforms bert;natural language important;queries measure accuracy;accuracy scope queries;semantics natural;language important machine;bert based;platforms bert;queries live;intents sourced spread"}, "9bd170248355047067f05349d57110cc8e4de5cf": {"ta_keywords": "extract maximum entropy;maximum entropy distribution;maximum entropy;maximum entropy large;maximum entropy proportional;observation maximum entropy;entropy large number;entropy distribution large;entropy distribution;entropy large;entropy;large number copies;entropy proportional;copies used extract;copies single copy;entropy proportional square;extract maximum;number copies;method extract maximum;copies method used;number copies method;used extract maximum;copies method;single copy;copies method applied;copies;single copy method;number copies single;number copies used;copy method based", "pdf_keywords": ""}, "9952fe8cbd09e4fc89dc7d76595d138e36c7d7b5": {"ta_keywords": "evaluation transferability bert;transferability bert based;transferability bert;bert based ranking;labels competitive bert;bert based neural;competitive bert based;neural ranking models;transfer learning pseudo;neural ranking;ranking models datasets;ranking models accurate;competitive bert;transfer learning;based neural ranking;systematic evaluation transferability;bert based;evaluation transferability;ranking models;bert;learning pseudo labels;benchmark datasets;models datasets evaluation;ranking;based ranking models;datasets evaluation;transferability;pseudo labels competitive;datasets evaluation performed;learning pseudo", "pdf_keywords": "evaluation transferability bert;transferability bert based;bert based ranking;transferability bert;bert based rankingwe;retrieval model bert25;bert based neural;neural ranking models;labeling using bert;trained shot queries;ranking models pseudo;shot transfer learning;training bert based;neural ranking;ranking models datasets;transfer learning pseudo;transfer models trained;shot evaluation transferability;question answering;natural language capture;ranker trained document;training bert;shot models trained;transfer learning deep;transfer learning;competitive training bert;using bert based;ranking models;shot queries results;neural ranker trained"}, "1f76ee8472ec3a41511540f62e4676317df14ea5": {"ta_keywords": "perceptual age singing;singer perceptual age;perceptual age singer;age singing voice;age singing;singing voice conversion;singing voice method;singer perceptual;age singer perceived;target singer perceptual;singing voice timbre;method converts singing;converts singing voice;singing voice;voice method;voice conversion method;voice conversion;controlling perceptual age;converts singing;voice timbre arbitrary;voice timbre;based perceptual age;arbitrary target singer;arbitrary source singer;singer perceived listener;source singer arbitrary;singer perceived;uses perceptual age;singer arbitrary target;age singer", "pdf_keywords": ""}, "5a5fb155b5fc518389a7fe67b55271e143ad695d": {"ta_keywords": "community recovery hypergraphs;censorshipsored block model;generalized censorshipsored block;hypergraphs characterize information;recovery hypergraphs characterize;study generalized censorshipsored;generalized censorshipsored;limit community recovery;recovery hypergraphs using;censorshipsored block;hypergraphs characterize;recovery hypergraphs;hypergraphs;hypergraphs using;hypergraphs using maximum;problem community recovery;community recovery;limit community;theoretic limit community;study community recovery;information theoretic limit;information theoretic;censorshipsored;block model;characterize information theoretic;maximum likelihood;block;number observations node;community;observations node", "pdf_keywords": "recovery communities hypergraphs;communities hypergraphs;recovering communities hypergraph;communities hypergraphs fundamental;community recovery hypergraphs;graphs parity measurement;hypergraph partitioning stochastic;sample complexity recovery;sparse hypergraph partitioning;recovery hypergraphs generalized;communities hypergraph;sparse hypergraph;hypergraph partitioning;detecting community sparse;threshold sample complexity;graphs parity;community sparse random;communities hypergraph noisy;sample complexity characterize;complexity recovery;minimizes sample complexity;sparse random graph;sample complexity ensuring;sparse sparse hypergraph;valid sample complexity;random graph threshold;recovery hypergraph;large sample complexity;sample complexity;hypergraphs generalized"}, "17c7c92db1f4ace842f9db6b44bfce264308b628": {"ta_keywords": "approach labelsupervised deep;unsupervised labeled parsing;labelsupervised deep;labelsupervised deep inside;phrase vectors deep;recursive autoencoders diora;learned phrase vectors;inside recursive autoencoders;recursive autoencoders;labeled parsing;empirically unsupervised labeled;recursive autoencoders df;unsupervised labeled;autoencoders diora;novel approach labelsupervised;induce span labels;autoencoders;autoencoders diora use;deep inside recursive;vectors deep;autoencoders df induce;autoencoders df;vectors deep inside;approach labelsupervised;labelsupervised;span labels;phrase vectors;labeled;labeled parsing evaluate;span labels evaluate", "pdf_keywords": ""}, "753fd6952c9f06f3bbd46e37129acc3f7a984896": {"ta_keywords": "text generation;text generation guide;ended text generation;generation guide trained;generator guide trained;text based guide;performance text based;generation guide;open ended text;text based;generator guide;guide improving performance;passages input generator;guide improving;guided passage learned;guide trained using;based guide improving;text;performance text;input generator;standard generator guide;guided passage;generator able relevant;passage learned;trained using following;study performance text;guided;trained using posterior;guide trained;passage learned jointly", "pdf_keywords": "generation retriever task;generating informative conversations;wikipedia generation task;wikipedia generation;language generation tasks;text generation systems;generation retriever;text generation;retriever better generator;improving retrieval;useful retrieved passage;retriever generator;models retriever generator;generation task free;retrieval augmented language;retriever supervision improves;retriever relevant passages;retrieved passages generator;improving retrieval quality;generator underperform retriever;retrieve passages textual;retrieved passages useful;retriever generator underperform;additional context generator;generating informative;tasks like generating;thoroughly evaluate retrievalaugmented;like generating informative;generation tasks like;context generator"}, "7fa3a5318ac45b2fd93a0130f0ceba9995ffa3c0": {"ta_keywords": "slang framework mining;similarities slang framework;slang languages experimental;slang framework;slang languages;terms slang languages;differences similarities slang;similarities slang;similar terms slang;computing cross cultural;cultural differences named;terms slang;slang;entities finding similar;differences named;differences named entities;finding similar terms;cultural differences similarities;finding similar;mining cross cultural;languages;languages experimental results;cross cultural differences;framework mining cross;languages experimental;framework finding similar;named entities finding;cross cultural;cultural differences;differences similarities", "pdf_keywords": ""}, "8306e4a566e2b1279d5d67b40facc8e1e345c4e3": {"ta_keywords": "supported strong convergence;strong convergence theory;strong convergence;smooth nonconvex regime;convergence rates techniques;compression analyze convergence;stochastic approximation;momentum bidirectional compression;techniques smooth nonconvex;theory error feedback;stochastic approximation variance;convergence theory error;convergence theory extensions;nonconvex regime;convergence rates;convergence theory;extensions strong convergence;nonconvex regime cases;approximation variance reduction;analyze convergence rates;smooth nonconvex;approximation variance;feedback ef21 supported;bidirectional compression;convergence;momentum bidirectional;error feedback ef21;feedback ef21;ef21 supported strong;setting momentum bidirectional", "pdf_keywords": "distributed optimization;distributed optimization problems;distributed optimization method;novel distributed optimization;suitable distributed optimization;distributed optimization large;stochastic optimization framework;stochastic optimization;ef21 distributed optimization;stochastic gradient descent;stochastic approximation framework;stochastic approximation proximal;stochastic approximation partial;stochastically convergent approximation;new stochastic approximation;convex optimization composite;stochastically compressed method;stochastic approximation powerful;gradient stochastically compressed;convergence distributed learning;descent method stochastic;layer stochastic optimization;stochastic approximation;based stochastic approximation;stochastic gradient method;applicable optimization noisy;convex optimization;compression momentum proximal;optimization composite optimization;compressed gradient computation"}, "bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713": {"ta_keywords": "multilingual phoneme recognition;phoneme recognition;phoneme recognition using;multilingual phoneme;using allophone annotation;use allophone annotation;allophone annotation;method multilingual phoneme;annotation use allophone;allophone annotation use;recognition using allophone;allophone annotation associate;phone set phonemes;tuning allophone utterances;allophone utterances;phonemes appear language;set phonemes;using allophone;use allophone;set phonemes appear;allophone utterances leads;phonemes;multilingual;phonemes appear;allophone;fine tuning allophone;tuning allophone;phoneme;method multilingual;phone set", "pdf_keywords": "multilingual phoneme recognition;phone recognition languages;phoneme recognition models;phoneme recognition model;phoneme recognition;accuracy phoneme recognition;present phoneme recognition;model phone recognition;phone recognition new;specific phonemes allophone;acoustic models language;multilingual universal phoneme;phoneme specific allophone;universal phoneme models;phone recognition;phoneme recognition stiff;phonemes allophone;benchmark accuracy phoneme;phonemes allophone layer;speech recognition great;phoneme models;phone representations quickly;language specific phonemes;language present phoneme;universal phone representations;phone recognition uses;phoneme models allows;language phoneme specific;reasonable phone recognition;accuracy phoneme"}, "c64843e51f24773c895511ba9befa8a9bc4924a9": {"ta_keywords": "perceptual age singing;singer age experimental;age singing voice;age singing;perception singer age;voice use statistical;statistical voice conversion;use statistical voice;singer age;statistical voice;age source singer;convert perceptual age;perceptual age source;voice conversion;demonstrate perceptual age;singing voice;control perceptual age;singing voice use;singing voice corresponds;perceptual age;arbitrary target singer;acoustic features modified;singer arbitrary target;listeners perception singer;voice conversion convert;singer arbitrary;acoustic features;source singer arbitrary;voice;perception singer", "pdf_keywords": ""}, "8ef0ca924ae88ac2bb2803c49589722b52efc5b4": {"ta_keywords": "affective interaction corpus;affective interactions languages;social affective interactions;social affective interaction;expressive social affective;interaction corpus proposed;affective interactions;interaction corpus;affective interaction;social affective;study social affective;languages proposed corpus;proposed corpus based;highly expressive social;proposed corpus;proposed corpus used;affective;corpus proposed;corpus based;expressive social;corpus based known;corpus proposed corpus;datasets study social;interactions languages english;interactions languages proposed;interactions languages;corpus;data interactions languages;corpus used generate;highly expressive datasets", "pdf_keywords": ""}, "192fc995631e3443bb7f291a971089bd06e61017": {"ta_keywords": "human annotation science;annotation science;classification new questions;annotation science question;interface human annotation;human annotation;annotation;open problems;question answer pairs;new questions;new questions improved;classification new;classification schema;open problem;build classification schema;open problem open;questions;questions improved;classification schema proposed;problem open problems;classification;interface human;build classification;problems computer science;participants;answer pairs;schema proposed;open problems computer;question answer;introduce novel interface", "pdf_keywords": ""}, "39ffb5e9f2f36df42ef8ea010499e484c913e79e": {"ta_keywords": "normalization historical language;corpora normalization algorithm;use corpora normalization;corpora normalization;historical language corpora;normalization historical;language corpora method;use corpora;language corpora;corpora method;normalization;automatically normalize data;normalization algorithm based;automatically normalize;normalization algorithm;normalize data;corpora;data use corpora;normalize data use;corpora method uses;normalize;method normalization historical;algorithms automatically normalize;novel method normalization;method normalization;historical language;different algorithms automatically;algorithms automatically;historical;language", "pdf_keywords": ""}, "26f427d2b27828f2893e95344342570699e9c589": {"ta_keywords": "code data encoded;data encoded nf;data encoded ni;converting data encoded;data encoded;ki code data;encoded nf;encoded nf kf;encoded ni ki;code conversions;introduce convertible codes;encoded ni;desired decodability;encoded;desired decodability properties;code data;allow code conversions;convertible codes;nf kf code;maintaining desired decodability;kf code;code conversions resource;decodability properties maximum;code pairs;ki code;code pairs allow;convertible codes new;ni ki code;pairs allow code;decodability properties", "pdf_keywords": ""}, "5c333f11431d1f0d04ced62b712c8d05ebac0891": {"ta_keywords": "speech enhancement experiments;model speech enhancement;speech enhancement;speech enhancement incorporates;noise estimated speech;enhancement incorporates diffusion;estimated speech signal;probabilistic model speech;gaussian noise estimated;noise estimated;novel diffusion probabilistic;non gaussian noise;proposed model speech;estimated speech;model speech;gaussian noise;diffusion probabilistic;noise characteristics unseen;speech signal able;diffusion probabilistic model;speech signal;datasets noise characteristics;conditional diffusion signal;noise ratio;noise characteristics;noise ratio demonstrate;diffusion signal high;diffusion signal;generate conditional diffusion;enhancement", "pdf_keywords": "enhancement noisy speech;generative speech enhancement;speech enhancement noisy;speech enhancement model;speech enhancement algorithm;speech enhancement based;approach speech enhancement;speech enhancement;speech signal diffusion;context speech enhancement;speech enhancement problems;speech enhancement work;conditional diffusion noisy;systems speech enhancement;speech enhancement characterized;speech synthesis;novel speech enhancement;world speech enhancement;generative models speech;lagging speech enhancement;quality speech exploiting;potential speech synthesis;enhancement noisy;generative speech;enhancement based diffusion;noisy speech;data noisespeech enhancement;models speech;observed noisy speech;speech noisy"}, "c82854b8d4c715da141d34c73bf9bda67adf307c": {"ta_keywords": "polarization news topics;polarity based topics;polarization news;sentiment polarity comments;sentiment polarity;assign sentiment polarity;using sentiment polarity;models predict polarity;polarity comments;polarity comments serve;sentiment polarity comment;predict polarity based;predict polarity;responses news topics;polarity comment;observed using sentiment;semisupervised latent variable;assign sentiment;latent variable models;degree polarization news;news topics observed;news topics;polarity comment volume;semisupervised latent;multitarget semisupervised latent;community responses news;using sentiment;political beliefs community;topics cause communities;topics blog", "pdf_keywords": ""}, "30a30781c66c758e8e59cdb00c368f3add99768b": {"ta_keywords": "speaker recognition;speaker recognition invariant;strategy speaker recognition;speaker interface;training strategy speaker;speaker information experiments;speaker information;speaker speaker interface;augmentation self supervision;characteristics speaker information;experimental results speaker;speaker interface significant;speaker interface recent;acoustic characteristics speaker;characteristics speaker;speaker speaker;speaker;self supervision models;results speaker;results speaker speaker;recognition invariant augmentation;self supervision training;self supervision model;popular speaker speaker;novel self supervision;previous self supervision;contrastive learning training;self supervision;contrastive learning;strategy speaker", "pdf_keywords": "speaker embedding augmentation;predict speaker embedding;trained speaker embeddings;learn speaker embedding;speaker embedding extractor;adversarially trained speaker;augmentation learn speaker;unsupervised speaker recognition;speaker supervised learning;self supervised speaker;speaker network training;speaker discriminative network;discriminative speaker network;speaker embedding;effective speaker embeddings;speaker embeddings;novel speaker embedding;speaker identification dataset;speaker embeddings self;benchmark speaker discovery;supervised speaker;speaker recognition easily;supervised speaker recognition;speaker recognition;speaker supervised;bias speaker representation;speaker embeddings approach;discriminative network acoustic;performance speaker supervised;perform supervised speaker"}, "84370f2fa3ac21ed3c6a30144fbdb377157b8853": {"ta_keywords": "preference reasoning based;conditional preferences;representation conditional preferences;preference reasoning;conditional preferences group;model conditional preferences;preferences group agents;approach preference reasoning;agents represent preferences;conditional preferences pcps;preferences single agent;represent preferences single;represent preferences;elementary operators nets;operators model conditional;based representation conditional;preferences single;preferences group;preference;reasoning based representation;reasoning based;approach preference;elementary operators model;preferences;preferences pcps terms;elementary operators;use elementary operators;operators nets;conditional;representation conditional", "pdf_keywords": ""}, "d66110a315f5f216b42d99cfafec31e8e30a03ea": {"ta_keywords": "target speaker extraction;speaker extraction target;extraction target speaker;speaker extraction;target extraction extraction;target extraction signal;extraction target extraction;target extraction using;obtain target extraction;extraction signal estimation;target extraction;target extraction target;target target extraction;estimation target extraction;extraction signal estimated;estimate target extraction;extraction extraction signal;extraction target;extraction signal;extraction signal performed;extraction signal presence;target speaker;extraction extraction;signal estimation target;extraction using;extraction;signal estimated stochastic;transducer used obtain;transducer transducer;transducer", "pdf_keywords": ""}, "05d1e21f5f0c8209cc125f2e9ccd3a62d6479114": {"ta_keywords": "document similarity metric;similarity pairs documents;use document similarity;document similarity;pairs documents based;determining similarity pairs;determining similarity;similarity pairs;method determining similarity;similarity metric;similarity metric method;pairs documents;similarity;tables common database;common database tables;documents based;common database;documents based use;tables tables common;tables common;database tables;used determine tables;tables containing names;tables containing;database consisting tables;determine tables;determine tables tables;database;tables tables containing;documents", "pdf_keywords": ""}, "f9409302c4d8201481fe65675bc6f0fa32e01df7": {"ta_keywords": "noise propagation light;gaussian noise propagation;propagation light medium;effect gaussian noise;gaussian noise increase;propagation light;significant propagation light;gaussian noise;medium effect gaussian;light medium energy;noise propagation;light medium effect;noise increase energy;medium energy light;increase energy light;energy light;effect gaussian;factor propagation light;energy light known;light medium;gaussian;light;noise increase;energy light used;effect significant propagation;light used increase;study effect gaussian;significant propagation;light known;light used", "pdf_keywords": ""}, "59d3f6a14e20efdf54216188e227e58a351237e5": {"ta_keywords": "generative fingerprint content;generative fingerprint generative;generative fingerprint;fingerprint generative;generative fingerprint architecture;generative fingerprint fingerprint;generative fingerprint pattern;achieved generative fingerprint;fingerprint generative fingerprint;model generative fingerprint;novel generative fingerprint;fingerprint architecture fake;fake image attribution;broken generative fingerprint;fingerprint content;fingerprint network;fingerprint fingerprint network;fingerprint content ir;image attribution;architecture fake image;fingerprint architecture;discriminability fingerprint;fake image;fingerprint fingerprint;experiments fake image;fingerprint pattern;fingerprint help content;fingerprint;fingerprint pattern novel;discriminability fingerprint help", "pdf_keywords": "fingerprint gan images;fingerprint gan generated;disentangle fingerprint gan;gans learned fingerprint;gan fingerprint disentangling;fingerprint gan;gan violating fingerprint;gan fingerprint;propose gan fingerprint;images generated gan;fingerprint fake image;gan generated images;gan images produce;gan images;fake images sources;fake images network;gan networks;fake image attribution;fingerprint disentangling network;gan networks based;attributing fake images;fake images generated;generated gan;gan generated;representation fake image;networks gans;distinct different gans;generator fake image;facilitate fake image;different gans learned"}, "207c64b36fbd6accf7067366a251d071e8dd03a7": {"ta_keywords": "dynamics mountain flora;complex dynamics mountain;dynamics mountain investigated;mountain flora comprises;dynamics mountain;mountain flora;range complex dynamics;complex dynamics;multispecies unbound species;species unbound range;dynamics;flora comprises unboundest;investigation complex dynamics;unbound species unbound;species unbound;unbound species;flora comprises;mountain investigated;consistent multispecies model;mountain investigated means;unbound range complex;complex multispecies unbound;multispecies model;complex multispecies;mountain;model applied complex;multispecies model model;flora;self consistent multispecies;species", "pdf_keywords": ""}, "4f6d64eec6eaa38177ae45ad6315cf25d1535294": {"ta_keywords": "history selection conversational;attentive history selection;selection conversation histories;history attention mechanism;selection conversational questions;conversation history;selection conversational;encode conversation history;conversation histories;conversational questions answering;conversation history develop;conversation histories demonstrate;selection conversation;method attentive history;develop history attention;history selection;soft selection conversation;history attention;attentive history;conversational questions;position encode conversation;questions answering;encode conversation;conversation;attention mechanism;attention;conversational;attention mechanism ham;answering use language;histories", "pdf_keywords": "history attention model;conversational search tasks;embed conversation history;conversation history modeling;history conversation embedding;selection conversation histories;grained history attention;characterize history conversation;task conversational search;conversational search model;dataset conversational search;learning embed conversation;conversational search;history attention mechanism;conversation histories;use history attention;model conversational task;turns history attention;conversational search takes;conversation history;conversation embedding;conversation histories use;method conversational search;passage conversation histories;conversation use neural;conversation history context;conversation history natural;able characterize conversations;attention model;conversational task"}, "e8135016ff3bd33ace936e50247fd650fcc58a7a": {"ta_keywords": "translation evaluation neural;translation accuracy;translation evaluation;translation accuracy presence;translation lexicon minimum;method translation evaluation;discrete translation lexicon;translation lexicon;tl translation accuracy;evaluation neural machine;evaluation neural;discrete translation;use discrete translation;nai tl translation;neural machine;method translation;novel method translation;neural machine method;lexicon minimum risk;translation;tl translation;neural;lexicon minimum;task characterizing nai;risk training method;training method;training method applied;characterizing nai tl;characterizing nai;nai tl", "pdf_keywords": "predicting translation;predicting translation language;machine translation model;neural machine translation;method predicting translation;model improve translation;machine translation;translation accuracyneural networks;translation model;machine translation task;machine perform translation;outcome machine translation;lexical translation probabilities;translation lexicons improve;translation probabilities;translation task knowledge;machine translation nmt;translation model incorporate;improve translation accuracyneural;translation accuracy;translation task;translation language;target language predictions;language predictions;translation lexicons minimum;translation lexicons;translation probabilities target;evaluates accuracy translation;translation accuracyneural;accuracy translation"}, "7847419becbc04596b79f804f844cf9719e875ea": {"ta_keywords": "unannotated demonstrations parsing;learning hierarchical policies;parsing demonstrations sequences;hierarchical policies demonstrations;parsing demonstrations;demonstrations parsing demonstrations;demonstrations parsing;level actions train;actions train;sparse natural language;demonstrations sequences named;actions train model;learning hierarchical;policies demonstrations using;unannotated demonstrations;level subtask descriptions;demonstrations sequences;low level actions;reusable skills autonomous;action sequences goals;skills autonomous;high level subtasks;primarily unannotated demonstrations;high level subtask;subtask descriptions;level actions;skills autonomous decision;level subtasks;subtask descriptions descriptions;action sequences", "pdf_keywords": "learning demonstrations natural;learning demonstrations;unannotated demonstrations parsing;demonstrations natural language;subtask descriptions training;generate actions learn;learning hierarchical policies;parsing demonstrations sequences;tasks natural language;generate subtask descriptions;partial subtask descriptions;hierarchical policies demonstrations;unannotated demonstrations automatically;parsing demonstrations;learning task general;demonstrations parsing demonstrations;demonstrations parsing;subtask descriptions robust;training demonstrations generated;descriptions generate actions;embedding tasks natural;subtask descriptions generate;level subtask descriptions;embedding tasks;sparse natural language;actions train;level actions train;actions training;learning task;supervised reinforcement"}, "ad734a3f530a6c338af6bf2bf678e5af05477c1a": {"ta_keywords": "complexity secret sharing;communication complexity secret;secret sharing network;secret sharing general;complexity secret;communication complexity algorithm;bounds communication complexity;secret sharing;problem secret sharing;communication complexity;sharing general networks;networks satisfying propagating;consider problem secret;sharing network networks;condition communication complexity;lower bounds communication;complexity algorithm;complexity algorithm theta;network networks satisfying;sharing network;networks derive lower;bounds communication;networks derive;problem secret;complexity;networks satisfying;general networks derive;satisfying propagating dealer;network networks;networks", "pdf_keywords": ""}, "11154216ca898590e7b2f0339587e3378c2c646c": {"ta_keywords": "drivers driving confidence;drivers warning information;affect drivers driving;driving confidence psychology;drivers warning;technology affect drivers;driving confidence;psychological characteristics drivers;drivers driving;affect drivers;quantifying driving;ahead expressway work;characteristics drivers warning;warning information work;warning information facing;expressway work zone;zone ahead expressway;connected vehicle environment;vehicle technology affect;zone connected vehicle;method quantifying driving;warning information;provide warning information;information warning information;connected vehicle technology;warning information warning;driving;facing work zone;work zone ahead;psychology facing work", "pdf_keywords": ""}, "87951cea6573eed827986371a35025e478d3c184": {"ta_keywords": "novel stochastic gradient;stochastic gradients;stochastic gradient;stochastic stochastic gradient;analyzing stochastic gradients;stochastic gradient method;stochastic gradients present;stochastic gradient sg;stochastic gradients mini;monotone stochastic gradients;gradients mini batching;importance sampling variants;variants stochastic stochastic;variants stochastic;analyze variants stochastic;gradient sg algorithm;importance sampling;independent importance sampling;gradient sg;introduce novel stochastic;variants monotone stochastic;gradient method sg;novel stochastic;mini batching strategies;sampling variants monotone;stochastic;stochastic stochastic;gradients present rates;analyzing stochastic;gradients", "pdf_keywords": "guarantees stochastic extragradients;stochastic extragradients;stochastic version extragradient;stochastic extragradient;stochastic extragradient sele;stochastic extragradients stochastic;stochastic extragradient methods;extragradients stochastic gradient;extragradients stochastic;method stochastic extragradient;framework stochastic extragradient;stochastic extragradient seg;stochastic variational inequality;strategies stochastic variational;stochastic variational;gradient descents stochastic;stochastic gradient;stochastic gradient descents;gradient descent stochastic;stochastic gradient descent;convergence stochastic gradient;consider stochastic optimization;stochastic optimization;extragradient seg variational;descents stochastic;descent stochastic;stochastic version;stochastic optimization methods;analyze stochastic gradient;guarantees stochastic"}, "dfa7bdea128b899d348ed32a84a7ccb1da4340e4": {"ta_keywords": "robust dialog systems;dialog response based;dialog systems;generate dialog response;building robust dialog;dialog systems using;robust dialog;generate dialog;retrieve generate dialog;dialog response;dialog;short term memory;neural networks retrieve;generation retrieval tasks;memory ls neural;read user utterance;generation retrieval;user utterance;retrieval process generation;retrieval tasks;term memory ls;ls neural networks;term memory;retrieval tasks propose;retrieval task;systems using neural;sources retrieval task;retrieval task propose;neural networks;response based existing", "pdf_keywords": ""}, "3675958405f3ad1633d565efa36b4eb3004bcf59": {"ta_keywords": "quality intermediate video;optimizing video upload;social live streaming;bandwidth intermediate video;video optimizing bandwidth;live streaming applications;intermediate video optimizing;novel video upload;video upload solution;live streaming;streaming applications designed;streaming;video upload;streaming applications;video optimizing;intermediate video;methods optimizing video;optimizing video;intermediate video approach;intermediate video given;optimizing bandwidth intermediate;video given viewing;video approach;real world mobile;optimizing bandwidth;mobile network;world mobile network;mobile network traces;network traces enhance;video approach uses", "pdf_keywords": ""}, "0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b": {"ta_keywords": "neural lattice language;lattice language models;language models;lattice language;language modeling;multiple language modeling;granularities neural lattice;language models models;language model;language modeling tasks;language modeling paradigm;neural lattice;new language modeling;multiword lexical;sentence marginalize lattice;multiword lexical items;language model experiments;seamlessly incorporate linguistic;multiple granularities neural;lexical items language;models construct lattice;intuitions including polysemy;incorporate linguistic intuitions;lexical items;granularities neural;linguistic intuitions including;linguistic intuitions;lexical;items language model;linguistic", "pdf_keywords": "neural language models;neural lattice language;neural language model;lattices neural language;language model neural;languages neural lattice;lattice language models;sentences languages neural;language neural;modeling segmentation sentence;lattice language model;neural language;language modeling tasks;learning neural lattice;language modeling;language neural activity;phrase patterns lattice;using neural lattice;language models;neural machine translation;languages neural;incorporating lattices neural;context neural;studying language neural;task language modeling;language models computational;introduce neural lattice;language models approximate;language model learns;language models models"}, "4786e10003655be97feee21b9d9894a88a62885f": {"ta_keywords": "multilingual transfer based;distance multilingual transfer;multilingual transfer;method multilingual transfer;multilingual transfer powerful;transfer based unsupervised;unsupervised truth inference;methods distance multilingual;inference limited supervision;unsupervised truth;based unsupervised truth;distance multilingual;truth inference limited;truth inference;novel method multilingual;supervision method outperforms;method multilingual;multilingual;transfer based;limited supervision method;supervision method;transfer;unsupervised method effective;based unsupervised;unsupervised method;methods distance unsupervised;transfer powerful;inference limited;inference;distance unsupervised", "pdf_keywords": "source models unsupervised;shot multilingual transfer;shot model generative;target language annotated;zero shot multilingual;multilingual transfer models;annotations outperforms;noisy annotations outperforms;annotators zero shot;supervised transfer ensembling;entityannotated corpus;shot multilingual;reasoning noisy annotations;unreliable annotators;supervised baselines ensembling;entityannotated corpus propose;noisy annotations;language annotated;source models;annotated tiny corpus;unsupervised transfer supervised;language networks;low resource supervised;multilingual transfer inspired;annotators;named entityannotated corpus;outputs unreliable annotators;unreliable annotators zero;learning shot model;transfer supervised"}, "4a06bfa86cbccdf5e55dcec3505cdc97b8edb288": {"ta_keywords": "parallel graphs algorithm;parallel graphs;cross sections graphs;relative probabilities vertices;set parallel graphs;probabilities vertices set;probabilities vertices;graphs algorithm;graphs algorithm based;vertices set parallel;relative probabilities algorithm;sections graphs;graphs context relative;sections graphs context;algorithm evaluation cross;graphs;determining relative probabilities;sections set parallel;graphs context;evaluation cross sections;relative probabilities present;relative probabilities;probabilities algorithm;probabilities algorithm applied;cross sections;cross sections set;cross sections cross;sections cross sections;context relative probabilities;parallel", "pdf_keywords": ""}, "622f980030f766e5eb3989f36eea4459ccc948bf": {"ta_keywords": "incremental adaptation;proposed incremental adaptation;novel incremental adaptation;incremental adaptation framework;incremental adaptation involves;characteristics incremental adaptation;incremental adaptation key;adaptation characteristics incremental;noise proposed incremental;posterior distributions acoustic;distributions acoustic model;adaptation framework;adaptation framework applicable;adaptation framework based;distributions acoustic;adaptation key technique;quick stable adaptation;novel incremental;adaptation;recognition speech;updating posterior distributions;successively updating posterior;stable adaptation;experimentally proposed incremental;propose novel incremental;adaptation key;incremental;proposed incremental;acoustic model;acoustic model parameters", "pdf_keywords": ""}, "ae06bc1e8e67c27b89329ebcfe61b71625d853f6": {"ta_keywords": "explainability techniques human;explainability techniques text;existing explainability techniques;evaluating existing explainability;assigned explainability techniques;diverse explainability techniques;explainability techniques;existing explainability;assigned explainability;scores assigned explainability;explainability techniques downstream;text classification tasks;diverse explainability;explainability;text classification;techniques text classification;architectures compare saliency;classification tasks neural;annotations salient input;human annotations salient;compare saliency;annotations salient;classification tasks;set diverse explainability;text classification apply;downstream text classification;compare saliency scores;saliency scores;saliency;salient input", "pdf_keywords": "learning task explainability;annotated saliency explanations;explainability techniques human;task explainability techniques;saliency explanations despite;explainability techniques datasets;explanations best models;explanationability techniques perform;saliency explanations;identify explainability;identify explainability techniques;task explainability;explainability automatic;explainability techniques gradient;techniques identify explainability;explanationability techniques;explainability techniques text;existing explainability techniques;novel explainability technique;novel explainability;explanations datasets;outcome explanationability techniques;explainability techniques downstream;evaluating existing explainability;explainability techniques widely;explainability techniques perform;explainability property predict;explanationability;novel approach explainability;explanations datasets use"}, "43896ea7d488100d135645fbb4be6e7eb2e7f4e2": {"ta_keywords": "set valued features;valued features;feature set strings;value feature set;feature vector representation;valued features example;rule learning algorithms;propose feature vector;rule learning;tree rule learning;feature set;decision tree;value feature;feature vector;vector representation allows;decision tree rule;learning algorithms easily;represented set valued;features;learning algorithms;features example real;features example;set strings;feature;vector representation;set strings argue;allows value feature;representation allows value;represented set;set valued", "pdf_keywords": ""}, "adb80a4190fdb6a019d57f18ae072ca93f494b7e": {"ta_keywords": "shallow acoustic model;weight matrices shallow;matrices shallow shallow;shallow shallow acoustic;low rank approximation;shallow acoustic;matrices shallow;approximation low rank;low rank model;rank approximation weight;noisy speech recognition;training low rank;rank approximation low;network low rank;approximation weight matrices;acoustic model;method low rank;discriminative training low;acoustic model applied;applying low rank;applied low rank;rank approximation;rank approximation underlying;low rank;task low rank;shallow shallow shallow;weight matrices;shallow shallow;model performing discriminative;evaluated noisy speech", "pdf_keywords": ""}, "f3762141fd64bee8d09e55ad4c83057cd4e002d4": {"ta_keywords": "coalition utility learning;correlations learning utility;correlation coalition utility;game correlation coalition;correlations learning;competing game correlation;correlation coalition;cooperative agents competing;game correlation;leveraging correlations learning;utility learning;correlation utility;utility learning based;learning utility;cooperative agents;correlation utility function;leveraging correlations;utility non cooperative;learning utility non;non cooperative agents;coalition utility;generate correlation utility;agents highly correlated;approaches leveraging correlations;cooperative;utility function agents;non cooperative;estimated correlations;estimated correlations generate;utility function agent", "pdf_keywords": ""}, "388d41b99c9c0867301f345c65877a2796225ead": {"ta_keywords": "target speaker utterances;transcribes target speaker;speech recognition asr;function target speaker;speaker automatic speech;speaker accuracy training;secondary target speaker;sample target speaker;automatic speech recognition;target speaker automatic;automatic speech;speech recognition;multiple speakers speech;target speaker;target speaker proposed;speech recognition apsa;novel auxiliary loss;auxiliary loss function;speaker utterances monaural;speaker utterances;speakers speech;auxiliary loss;speaker accuracy;interference speaker accuracy;speaker proposed loss;speakers speech given;speaker automatic;loss function target;recognition asr;maximize interference speaker", "pdf_keywords": "speaker extraction model;speaker automatic speech;speaker accuracy training;autonomous speech speakers;function target speaker;speech separation task;speaker separation;speech recognition asr;accuracy target speaker;speaker extraction;complete speech separation;target speaker method;acoustic models trained;speech networks method;interference speaker utterance;speech recognition;based speaker extraction;sequence based speaker;target speaker utterances;accuracy interference speaker;speech model based;representation speaker separation;interference speaker accuracy;speech model;processing speech networks;speech networks;speech separation;automatic speech recognition;given speech model;network autonomous speech"}, "7a8f8109e65ed9a6048859681a825eb5655e5dd2": {"ta_keywords": "sentence representations pretrained;pretrained word embeddings;word embeddings;computing sentence representations;sentence representations;parameterization sentence embedding;representations pretrained word;sentence embedding;word embeddings use;sentence embedding combination;representations pretrained;embeddings;embedding combination random;embeddings use;embedding;embedding combination;embeddings use new;pretrained word;representations;method computing sentence;new parameterization sentence;parameterization sentence;random parameterization xmath0;random parameterization;computing sentence;new parameterization;sentence;parameterization xmath0 matrix;use new parameterization;combination random parameterization", "pdf_keywords": "trained word embeddings;sentence representations pretrained;pretrained word embeddings;word embeddings improve;sentence embedding models;sentence encoders trained;learning sentence representations;sentence encoders embeds;word embeddings high;models sentence embeddings;embeds sentence encoders;encoders sentence embeddings;word embeddings;sentence embeddings significantly;sentence neural network;empirical sentence encoders;representations pretrained word;word embeddings make;sentence embeddings based;random sentence encoders;sentence encoders significantly;sentence representations;network sentence encoders;computing sentence representations;sentence embeddings;accuracy sentence encoders;sentence encoders describing;sentence encoders;language sentence encoders;predicting sentence structures"}, "03058f9a39d37a8bee635969eed227d59bbc8152": {"ta_keywords": "stochastic differential equations;solving stochastic differential;adjoint sensitivity method;stochastic differential;gradients stochastic differential;method solving stochastic;hypotheses stochastic differential;based adjoint sensitivity;solving stochastic;compute gradients stochastic;gradients stochastic;adjoint sensitivity;stochastic;sensitivity method;equations based adjoint;competing hypotheses stochastic;hypotheses stochastic;sensitivity method use;based adjoint;partial differential equations;differential equations competing;method compute gradients;adjoint;differential equations based;sensitivity;differential;partial differential;differential equations;compute partial differential;differential equations use", "pdf_keywords": "gradientbased stochastic;including stochastic differential;constructing stochastic differential;backward stochastic differential;gradients stochastic dynamics;simpler stochastic differential;differentiation stochastic differential;stochastic adjoint sensitivity;gradientbased stochastic variational;stochastic differential;differentiation stochastic;learning stochastic differential;stochastic differential equations;underlying stochastic differential;gradients underlying stochastic;embeddings stochastic differential;gradients stochastic;based stochastic differential;parameterized stochastic differential;dynamics stochastic differential;method stochastic differential;compute gradients stochastic;linear stochastic differential;differential equations stochastic;numerical differentiation stochastic;gradients solutions stochastic;solution stochastic differential;equation stochastic differential;gradient based stochastic"}, "4eb62ee328ceac9976c72bca65570d73ca0e8b64": {"ta_keywords": "bose einstein condensates;2d bose einstein;dimensional 2d bose;einstein condensates;einstein condensates becs;repulsive interaction repulsive;characterized repulsive interaction;interaction characterized repulsive;repulsive interaction;repulsive attractive interaction;becs repulsive interaction;created repulsive interaction;2d bose;interaction repulsive becs;repulsive interaction present;interaction repulsive;condensates becs presence;repulsive becs repulsive;condensates becs;bose einstein;becs created repulsive;becs repulsive;becs presence repulsive;repulsive becs;characterized repulsive;condensates;repulsive attractive;attractive interaction characterized;dynamics dimensional 2d;presence repulsive attractive", "pdf_keywords": ""}, "30a6a5614727017e7d7981f87df57d17713501a0": {"ta_keywords": "personalized incentives users;incentives users digital;personalized incentives;incentives users;design personalized incentives;algorithm offers incentives;offers incentives desirable;offers incentives;incentives desirable accurate;incentives desirable;incentives;based idea bandits;bandits class models;idea bandits;digital social networks;algorithm offers;bandits;users digital social;novel algorithm offers;social networks approach;idea bandits class;social networks;digital social;bandits class;personalized;users digital;novel algorithm;propose novel algorithm;approach design personalized;design personalized", "pdf_keywords": "bandit algorithms agents;bandit algorithms;theory bandit algorithms;problem bandit algorithms;bandit problem agents;novel bandit algorithm;bandit algorithm;bandit algorithms derive;bandit approaches;bandit approaches approach;bandit algorithm based;rewards matching agents;bandit analysis;bandit algorithm additionally;matching empirical rewards;symmetric bandits;matching predefined incentive;bandit problem;traditional bandit approaches;consider problem bandit;optimal incentives users;approach bandit analysis;dimensional bandit algorithm;personalized incentives agents;simulations dimensional bandit;bandit analysis context;incentives agents resource;bandit technique;predict rewards matching;problem bandit"}, "357ff26120dd220d7132f8083697d54b007ef260": {"ta_keywords": "performance shared models;asynchronous shared model;shared model framework;shared models;novel framework benchmark;shared model;benchmark performance shared;task lightweight prediction;shared model results;framework benchmark;framework benchmark performance;frozen shared model;lightweight prediction;asynchronous shared;shared models variety;performance shared;asynchronous asynchronous shared;lightweight prediction heads;benchmark performance;model framework;benchmark;model results framework;based collisionless asynchronous;framework learn task;model framework use;novel framework;collisionless asynchronous;collisionless asynchronous asynchronous;task lightweight;framework learn", "pdf_keywords": ""}, "9f24b8f93ed00a1592e02fdb0edf5ebf0d8752ff": {"ta_keywords": "assignment papers reviewers;automated assignment papers;automated assignment;reviewers conference peer;assignment algorithm based;proposed assignment algorithm;assignment algorithm;conference peer review;problem automated assignment;papers reviewers;papers reviewers conference;maximize review quality;assignment papers;design assignment algorithm;peer review;reviewers conference;maximize review;objective maximize review;optimally fair analysis;accuracy fairness objective;statistical accuracy fairness;near optimally fair;review quality disadvantaged;papers design assignment;review focus fairness;conference peer;optimally fair;total quality papers;maximizing total quality;fairness objective maximize", "pdf_keywords": ""}, "6a4deeb40aed8a4d56c8d9401c94b6c7a769e8c3": {"ta_keywords": "faceted query;faceted query example;scientific literature search;query documents;query document;task faceted query;input query document;query documents drawn;50 query documents;query document present;literature search;search;search evaluate models;aspects input query;literature search evaluate;scientific literature;task scientific literature;documents drawn computational;grained aspects input;faceted;machine learning;input query;task faceted;finer grained aspects;introduce task faceted;machine learning venues;document;grained aspects;models trained;query example", "pdf_keywords": "faceted literature search;search facets annotated;faceted search facets;searching facets;literature search qbe;faceted search;search facets;searching facets query;search facets query;scientific literature search;faceted qbe search;task searching facets;method search documents;domain search facets;search documents similar;task faceted search;retrieval especially context;search documents;literature search introduce;abstract facet queries;faceted search test;information retrieval;literature search;query facets;task faceted literature;type information retrieval;faceted literature;faceted queries;literature search present;research corpus nsym"}, "356ea9b29101ec6974a7a97b62266b0e7e58d6bf": {"ta_keywords": "magnetic field xmath0;xmath2 xmath3 states;xmath3 states;xmath5 state strongly;xmath3 states xmath4;xmath4 state;structure xmath1 xmath2;field xmath0 structure;xmath0 structure xmath1;xmath7 state xmath8;xmath0 structure;xmath7 state;xmath5 state;xmath4 state xmath5;structure xmath1;xmath6 state xmath7;xmath8 state;states xmath4;xmath6 state;states xmath4 state;xmath1 xmath2 xmath3;determine xmath6 state;state xmath7 state;xmath1 xmath2;state xmath5 state;state xmath7;xmath2 xmath3;state xmath5;state xmath8 state;state xmath8", "pdf_keywords": ""}, "9e0d3161b13481418b7e85e3a691d23d67cf1e68": {"ta_keywords": "privacy leaking images;automatically identify privacy;identify privacy leaking;privacy leaking;leaking images;leaking images use;identify privacy;privacy;regionaware graph convolutional;feature channels regionaware;graph convolutional network;graph convolutional;automatically identify;regionaware feature maps;image crucial regions;correlation self attention;convolutional network;dynamic regionaware graph;channels regionaware feature;regionaware feature;correlated feature channels;image crucial;feature channels;spatially correlated feature;feature maps;cluster spatially correlated;regions cluster spatially;crucial regions cluster;dynamic regionaware;regionaware graph", "pdf_keywords": "detecting privacy images;predict privacy images;private images correlations;privacy images large;privacy image sharing;image privacy dataset;unawareness privacy image;image privacy;objects aware sharing;privacy images;identify private images;privacy image;privacy leaking image;privacy online images;detecting privacy;drag privacy;correlations objects aware;potential privacy images;privacy images publicly;object detectors;leaking image detection;private images;objects aware;propose drag privacy;predict privacy presence;pretrained object detectors;need predict privacy;predict privacy;aware sharing;drag privacy leaking"}, "a3c9d1c5e403f35e5694778b86832f0f9a7d87e6": {"ta_keywords": "fast similarity search;search large speech;similarity search large;similarity search;large speech data;neighborhood graph indexing;efficiently finds similar;fast similarity;nearest neighbor graph;speech data;graph indexing;speech data set;method fast similarity;graph indexing method;finds similar query;similar query exploring;nearest neighbor;indexing;reduced nearest neighbor;efficiently finds;similarity;utterances proposed method;query exploring graph;indexing method characterized;neighbor graph proposed;neighbor graph;neighborhood graph;similar query;large speech;exploring graph best", "pdf_keywords": ""}, "77910e51a40d17157fc798325d06edfa6cff18d6": {"ta_keywords": "domain code languages;languages structured external;code languages structured;resampling nl code;nl code pairs;structured external knowledge;retrieval natural language;natural language nl;code pairs source;code languages;language nl pairs;nl code;generation open domain;open domain code;code pairs;languages structured;structured external;testbed resampling nl;language nl;natural language;domain code;pairs online programming;pairs source database;data augmentation retrieval;external knowledge method;online programming;nl pairs online;method generation open;resampling nl;external knowledge", "pdf_keywords": "generating extracting code;knowledge code generation;extracting code source;external knowledge code;generate code use;extracting code;widely used apis;mined code snippets;code generation models;state art syntax;documentation relevant apis;code generation;apis;domain code generation;code snippets intents;generate code;code snippets;code generation task;code generation generally;code source document;relevant apis;code generation exploits;used apis;semantic parsing useful;code source;code use generate;documentation process generating;semantics natural language;method code generation;programming language reference"}, "948b68677c4f3bcbb1bae7f1d4e1fd5a103f03d4": {"ta_keywords": "speech enhancement quality;quality speech enhancement;optimize speech enhancement;speech enhancement automatic;speech enhancement;speech enhancement adapting;enhancement automatic speech;experiments speech enhancement;improve quality speech;adapting dereverberation beamforming;dereverberation beamforming subnetworks;optimize speech;enhancement adapting dereverberation;dereverberation beamforming;beamforming subnetworks dereverberation;quality speech;jointly optimize speech;automatic speech;automatic speech recognition;enhancement quality improved;developed multichannel end;speech recognition;enhancement quality;speech recognition asr;enhancement automatic;recently developed multichannel;developed multichannel;multichannel end;model experiments speech;multichannel end end", "pdf_keywords": ""}, "5edaab1fa078a5c468e3fb26d267ca49be32e70e": {"ta_keywords": "preference elicitation aggregation;preference elicitation questions;predicting elicitation preference;preference elicitation;elicitation preference elicitation;elicitation preference;amazon mechanical turk;framework preference elicitation;elicitation aggregation;mechanical turk framework;elicitation aggregation plackett;effective predicting elicitation;turk framework effective;predicting elicitation;mechanical turk;effective elicitation questions;aggregation plackett luce;turk framework;elicitation questions given;elicitation questions;cost effective elicitation;luce model features;preference;effective elicitation;elicitation;group decision experiments;decision experiments amazon;aggregation plackett;luce model;aggregation", "pdf_keywords": "preference elicitation aggregation;preference elicitation;preference elicitation procedure;elicit preferences;preference elicitation discrete;approach preference elicitation;agent preference aggregation;preferences agents ranked;agents preferences experiments;alternatives elicitation rankings;elicitation rankings;preference elicitation assumed;elicitation rankings fundamental;preference aggregation;ask preference elicitation;according preferences agents;winning alternatives elicitation;preferences agents;elicitation aggregation;preferences experiments;preferences experiments thatwe;elicit preferences regular;ranked according preferences;random market eliciting;ranked alternatives;framework preference elicitation;elicitation aggregation plackett;features elicit preferences;predict ranked alternatives;online market elicitation"}, "98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de": {"ta_keywords": "lingual entity linking;method soft gazetteers;soft gazetteers outperforms;named entity recognition;soft gazetteers incorporates;entity recognition models;soft gazetteers;entity recognition;neural named entity;entity linking experiments;entity linking;cross lingual entity;lingual entity;models cross lingual;named entity;gazetteers incorporates ubiquitously;gazetteers outperforms existing;information literature neural;literature neural named;gazetteers incorporates;gazetteers outperforms;low resource languages;existing named entity;outperforms existing named;entity;gazetteers;linking;literature neural;lingual;resource languages", "pdf_keywords": "named entity recognition;entity recognition ner;entity recognition models;entity recognition;soft gazetteer features;representation soft gazetteer;soft soft gazetteer;soft gazetteer model;proposed soft gazetteer;soft gazetteer introduce;method soft gazetteer;using soft gazetteer;novel soft gazetteer;soft gazetteer;lingual entity linking;gazetteer lowresource knowledge;using gazetteer soft;gazetteer soft soft;natural language processing;gazetteer features candidate;gazetteer features named;gazetteer soft;gazetteer features;gazetteer features built;processing ner annotated;gazetteer lowresource;entity linking based;entity linking;limited gazetteer lowresource;gazetteer features advantage"}, "55b61befce42280c3d57331121c7d349dd8be4cf": {"ta_keywords": "delay evaluation speech;simultaneous speech translation;evaluation speech translation;speech translation considering;speech translation helps;speed simultaneous speech;translation considering accuracy;speech translation;accuracy delay evaluation;text speech;evaluation speech;presented text speech;translation helps improve;reducing delay results;considering accuracy delay;accuracy delay;simultaneous speech;delay evaluation;accuracy speed simultaneous;delay results;reducing delay;importance reducing delay;accuracy speed;delay results presented;speech;delay;trade accuracy speed;human judgements users;translation;translation considering", "pdf_keywords": ""}, "a949ba38194ad43c86925acec6705b434d5a920f": {"ta_keywords": "intuitive anti bunching;bunching counter intuitive;anti bunching counter;anti bunching;generate counter intuitive;anti bunching xmath0;counter intuitive vertices;counter intuitive anti;intuitive counter intuitive;intuitive counter;bunching counter;intuitive anti;counter intuitive;counter intuitive counter;number counter intuitive;analysis counter intuitive;bunching;bunching xmath0;counter intuitive context;intuitive;intuitive vertices;bunching xmath0 xmath1;generate counter;counter;method generate counter;analysis counter;number counter;anti;intuitive context method;intuitive context", "pdf_keywords": ""}, "22b6e88a2f234fc5646f6239f9040a776e841a97": {"ta_keywords": "induction bilingual lexicon;bilingual lexicon corpus;induction bilingual;method induction bilingual;bilingual lexicon;translation accuracy transcriptions;transcriptions using translation;sentence aligned transcriptions;accuracy transcriptions induction;transcriptions induction;translation accuracy;translation accuracy step;using translation accuracy;bilingual;transcriptions induction process;aligned transcriptions method;aligned transcriptions;transcriptions method;lexicon corpus small;lexicon corpus;accuracy transcriptions;accuracy transcriptions using;transcriptions;transcriptions using;corpus;transcriptions method based;step using translation;using translation;corpus small;corpus small quantities", "pdf_keywords": ""}, "9abd13caa32b1a90e32462a884a512f8666e80cc": {"ta_keywords": "context independent parsing;independent parsing;independent parsing problem;parsing;s__ parsing experiment;parsing problem based;s__ parsing;parsing experiment;parsing problem;experiments s__ parsing;scenarios natural language;parsing experiment s__;natural language;context independent;natural language setting;approach context independent;language setting experiments;follow scenarios natural;context;follow scenarios;approach context;scenarios natural;multifarious follow scenarios;novel approach context;language;handle multifarious follow;process known phase;multifarious follow;process proposed;phase", "pdf_keywords": "learning follow query;dependent semantic parsing;semantic parsing;sentences based reinforcement;semantic parsing inferences;semantic parsers;semantic parsing novel;extensibility semantic parsers;query splitnet trained;parsing inferences existing;semantic parsing method;semantic parsers introduce;parsing inferences;learning follow;context independent parser;context dependent semantic;sequence learning;dependent semantic;parsing;knowledge splitting spans;reinforcement learning follow;approach extensibility semantic;follow query splitnet;learnable intermediate structure;independent parser;structure span queries;learn splitting spans;pair natural language;span queries;sentences resulting representations"}, "3c8853d4ae3ad2633c47e840a48951d62b64a5b4": {"ta_keywords": "view graph learning;graph learning model;graph learning;multi view graph;graph sparsification label;semi supervised;multi view learning;view graph;label propagation unified;extraction graph sparsification;semi supervised classification;view learning;approach semi supervised;adaptive label propagation;graph sparsification;label propagation;factor extraction graph;framework label propagation;label propagation model;label propagation optimized;view learning baselines;sparsification label propagation;latent factor extraction;graph construction;model adaptive label;propagation optimized graph;adaptive label;extraction graph;optimized graph construction;supervised", "pdf_keywords": ""}, "df29486c04eafd004f2f0816e84c798783802cdf": {"ta_keywords": "converting text visual;conversion text visual;text visual representation;text visual;faster conversion text;converting text;conversion text;uses conversion text;based conversion text;method converting text;language conversion improved;visual representation;visual representation method;using conversion text;visual representation significantly;conversion text specific;language conversion;text;representation significantly faster;specific language conversion;language uses conversion;text specific language;improved using conversion;faster conversion;converting;conversion improved using;significantly faster conversion;conversion improved;visual;specific language improvement", "pdf_keywords": ""}, "298a68153859303ee70b3ef1525ee9c7031e32f5": {"ta_keywords": "perception p2 bot;bot models understanding;p2 bot models;behavior chit chats;incorporates chit chats;bot models;p2 bot;persona perception p2;tool p2 bot;understanding mutual persona;mutual persona perception;chit chats;bot transmitter;p2 bot transmitter;chit chats valuable;chats valuable tool;persona perception;mutual persona;feature modern cognitive;persona;bot transmitter receiver;expressed behavior chit;bot;behavior chit;perception p2;chit chats oldest;understanding expressed behavior;modern cognitive;chats valuable;chats", "pdf_keywords": "person based filtering;dialogue generation interlocutors;interlocutor dialogue generation;conversational systems;dialogue generation dataset;person person person;conversational systems explicitly;persona perception method;mutual persona perception;receiver framework conversational;dialogue construction based;dialogue process;conversational agents;person person;approaches generating dialogue;persona best interaction;information persona chatbot;generating dialogue;extract information persona;infer user persona;dialogue agents;detect human interactions;interaction persona;interaction persona described;capable generating dialogue;generation conversational agents;generating dialogue responses;dialogue construction;conversational deep learningneural;dialogue responses exploiting"}, "251a80dd4126fed3d6ae64f00dc24479f0ba5662": {"ta_keywords": "electron gas magnetic;dimensional electron gas;dynamics dimensional electron;electron gas;gas magnetic field;magnetic field introduction;gas magnetic;dimensional electron;electron;dynamics dimensional;magnetic field;problem dynamics dimensional;introduction problem dynamics;dynamics;magnetic;field introduction;problem dynamics;lecture presented 2nd;field introduction based;introduction problem;gas;introduction;presented 2nd;introduction based lecture;pedagogical introduction;simple pedagogical introduction;lecture presented;dimensional;pedagogical introduction problem;introduction based", "pdf_keywords": ""}, "46dab5eb9c11bd49893e2dafa7d1b720a0aa2b3d": {"ta_keywords": "reading comprehension tasks;paragraphs reading comprehension;questions paragraphs reading;comprehension tasks approach;reading comprehension;comprehension tasks predict;results reading comprehension;comprehension tasks;paragraphs reading;questions paragraphs;modeling interaction questions;paragraphs;predict social media;social media content;words fine grained;reading;interaction questions paragraphs;comprehension;fine grained gating;results reading;media content social;grained gating;content social media;social media tag;content social;content;words;grained;fine grained;tasks predict social", "pdf_keywords": ""}, "e92677eb974a2814d57de54e2c3733cbd92e2c00": {"ta_keywords": "hierarchical coding scheme;distributed computing;efficient parallel decoding;distributed computing systems;propose hierarchical coding;problem distributed computing;parallel decoding;hierarchical coding structure;hierarchical coding;distributed computing motivated;based hierarchical coding;parallel decoding reducing;world distributed computing;decoding cost computing;low latency computation;reducing decoding costs;coding scheme;coding structure;decoding reducing decoding;computation analyze decoding;coding structure consisting;coding;reducing decoding;efficient parallel;latency computation;distributed;enables efficient parallel;decoding cost;decoding costs;latency computation analyze", "pdf_keywords": "distributed quantum computation;hierarchically distributed quantum;quantum computation systems;quantum computation;hierarchical coding scheme;distributed quantum;hierarchical computing presented;coding scheme hierarchical;cost hierarchical coding;based matrix multiplication;matrix multiplications proposed;hierarchical computing;multiplication matrix task;hierarchical computingthis;hierarchical computingthis paper;efficient parallel decoding;matrix multiplications;hierarchical coding;multiplication matrix multiplication;realized parallel decoding;hierarchical computational structure;propose hierarchical coding;example hierarchical computingthis;parallel decoding;distributed computing;multiplication matrix;based hierarchical coding;matrix multiplication;matrix multiplication mhd;matrix multiplication matrix"}, "0e9e334e2647307f8fa7f9937d93f3ca9095e351": {"ta_keywords": "variational inequalities problems;lipschitz variational inequalities;variational inequalities;point variational inequalities;saddle point variational;saddle point problem;method solve saddle;method solving saddle;solving saddle point;solve saddle point;alpha method method;alpha alpha method;alpha method;inequalities problems based;lipschitz variational;solving saddle;monotone lipschitz variational;alpha method uses;solve saddle;saddle point;inequalities problems;inequalities;method variant known;cases monotone lipschitz;monotone lipschitz;point variational;method variant;method method variant;variational;saddle", "pdf_keywords": "extragradient method monotone;method monotone operator;constraint operator monotone;operator monotone variant;lipschitz monotone iterate;method monotone;operator optimal point;monotone operator;monotone operator corresponding;operator optimal;operator monotone;monotone iterate;convex optimization;convex optimization problems;monotone lipschitz variables;vip monotone lipschitz;consider optimization method;monotone iterate error;operators vip monotone;monotone version method;convex optimization problem;monotone lipschitz monotone;problem monotone lipschitz;monotone variant restricted;monotone variant;iteration stochastic extragradient;optimal point method;strategy smooth convex;lipschitz monotone;existence extragradient methods"}, "75ba422d90c488b1388345865e0525208331bb3d": {"ta_keywords": "private data mcmc;private data markov;preserving private data;dynamics preserving private;preserving private;efficiency preserving private;problems preserving private;mcmc problems preserving;private data;carlo mcmc models;power preserving private;carlo mcmc problems;monte carlo mcmc;data mcmc models;mcmc models effective;mcmc models;markov chain monte;mcmc models important;data mcmc;mcmc models problems;mcmc problems;carlo mcmc;chain monte carlo;markovian dynamics preserving;private;problems non markovian;non markovian dynamics;non markovian;mcmc;monte carlo", "pdf_keywords": "bert based softmax;private stochastic gradient;softmax;nonlinear language processing;differential privacy accuracy;stochastic gradient descent;based softmax;private model trained;gradient models supervised;private differential tag;differentially private model;differentially private stochastic;task differential privacy;gradient descent dp;differential privacy good;gradient descent;based softmax layer;privacy good predictor;differential privacy;nonlinear language;differential privacy description;softmax layer;typical nonlinear language;deep learning ds;equation bert based;differential tag tagging;learning task differential;privacy description computational;models supervised learning;deep learning"}, "9d3e33875ec39001e72313fb919f66242ee97880": {"ta_keywords": "discovery linguistic units;linguistic units generically;linguistic units_ universal;universal linguistic units;linguistic units language;linguistic units;linguistic units_;term linguistic units_;units_ universal linguistic;units language based;discover universal linguistic;units language term;units language;discovery linguistic;universal linguistic;method discovery linguistic;language term linguistic;languages robust;units generically;languages robust noise;linguistic;units_ universal;term linguistic;units generically present;language term;units;languages;units_;variety languages robust;method discover universal", "pdf_keywords": "unsupervised phoneme discovery;phoneme discovery acoustic;automated speech units;phoneme discovery;discovery linguistic units;acoustic unit discovery;discover understand spoken;speech units research;speech recognition;language orthography project;speech units image;using automated speech;language orthography;automated speech;finding efficient spoken;supervised acoustic orthography;speech units generated;words language orthography;acoustic language;learn speech units;discovery linguistic;resource speech features;discovery acoustic unit;discovery cross language;speech features document;speech units;speech recognition lds;speech image;research unsupervised phoneme;speech features"}, "7d7469e059c6890c24d42931c697df835329f26a": {"ta_keywords": "estimate noise mixture;estimating noise mixture;noise mixture model;noise mixture;mixture model signal;method estimating noise;estimating noise;estimate noise;applied estimate noise;mixture model;signal model based;signal model proposed;signal model;squared error mmse;model signal model;mean squared error;mmse method proposed;model signal;mmse method;maximum mean squared;signal model resulting;noise;error mmse method;method estimating;squared error;novel method estimating;word error rate;error rate proposed;mean squared;error rate", "pdf_keywords": ""}, "f7f6160d4e9e3bf7f36bacbc9f15e916a6f226de": {"ta_keywords": "multichannel speech enhancement;enhancement speech recognition;speech recognition arsa;speech enhancement;adequate speech enhancement;speech recognition single;speech enhancement speech;speech enhancement ability;multichannel speech;components multichannel speech;enhancement speech;automatic speech recognition;speech recognition;automatic speech;end automatic speech;evaluation novel multichannel;corpus advanced signal;recognition arsa architecture;novel multichannel;novel multichannel end;arsa architecture integrates;multichannel end;multichannel;multichannel end end;available corpus advanced;recognition arsa;integrates components multichannel;advanced signal processing;single neural network;recognition single neural", "pdf_keywords": ""}, "99c87e16c56b8a113124779734951f11bd662d5d": {"ta_keywords": "game improving energy;energy consumption building;energy efficiency building;efficiency building game;occupants social game;energy reduction building;energy efficiency;energy consumption;reducing energy consumption;improving energy efficiency;improving energy;consumption building;reducing energy;social game improving;efficiency building;consumption building motivated;behavior occupants social;social game;energy reduction;energy;present social game;building game inspired;energy harvesters estimate;occupants social;importance reducing energy;performance existing energy;utility function;building game;significant energy reduction;existing energy", "pdf_keywords": "social game occupancy;social game energy;occupancy votes game;propose social game;game improving energy;social game simulate;implementation social game;results social game;occupancy game game;game energy savings;social game;occupancy game;player social game;game energy;social game improving;efficiency building game;behaviors social game;game occupancy;game inducing occupants;behavior social game;social game usewe;occupancy occupancy game;present social game;occupants behavior social;social game investigate;votes social lottery;votes social;energy consumption occupants;social lottery;occupants behave energy"}, "4697ef43450f173e12b1e22b77e976dc56fdf5fe": {"ta_keywords": "algorithm compressive sensing;compressive sensing based;novel compressive sensing;compressive sensing;sensing based attack;novel compressive sampling;compressive sampling;pursuit cosamp algorithm;algorithm compressive;compressive sampling matching;attack proposed algorithm;matching pursuit cosamp;modified basis pursuit;sampling matching pursuit;norm attack experimental;basis pursuit cosamp;white box attacks;novel compressive;box attacks mnist;matching pursuit;proposes novel compressive;cad algorithm compressive;compressive;basis pursuit;uses novel compressive;based adaptive defence;attack experimental;norm attack;attack experimental results;algorithm norm attack", "pdf_keywords": "attack compressive sampling;attack compressive;algorithm compressive sensing;underlying compressive sensing;compressive sensing;propose compressive sensing;compressive sensing proposed;novel compressive sensing;algorithms based compressive;algorithm compressive;based compressive sensing;proposes compressive sensing;based recovery adversarial;compressive sensing based;type attack compressive;compressive sampling;recovery adversarial images;recovery adversarial;attack estimate recovery;underlying compressive;underlying underlying compressive;based compressive;compressive sampling matching;modified basis pursuit;propose compressive;propose novel compressive;sampling matching pursuit;basis pursuit using;norm attacks gradient;cad algorithm compressive"}, "ce3d6673b7eebdd0198940316d18e383e9597c9a": {"ta_keywords": "learning contextual bandits;predictors optimal regret;optimal regret mathcalo;contextual bandits;contextual bandits help;bandits help loss;optimal regret;bandits;bandits help;regret mathcalo minqrttt;adversarial versus stochastic;loss predictors;regret mathcalo;predictors optimal;loss predictors provide;optimal;adversarial;including adversarial;adversarial versus;including adversarial versus;problem learning contextual;problem learning;settings including adversarial;learning contextual;mathcalo minqrttt sqr;help loss predictors;learning;multiple predictors optimal;regret;minqrttt sqr", "pdf_keywords": "learning contextual bandits;algorithms contextual bandits;bandit algorithms;contextual bandits;minimax regret learning;multiarmed bandits learner;bandits help losspredictors;contextual bandits help;bandits learner;bandits learner access;context multiarmed bandits;empirical regret learning;robustness bandit algorithms;contextual bandits parameter;rate robustness bandit;regret learning rounds;multiarmed bandits;robustness bandit;bandit algorithms standard;results regret optimal;improve minimax regret;regret optimal;bandits parameter free;regret learning;regret learning model;bandits;minimax regret;bandits help;fixed loss exploration;bandits parameter"}, "29b3a609f2b5cb10cffb80a6aaf96413a4a9998e": {"ta_keywords": "based compression schemes;compression schemes based;compression schemes introduce;compression schemes achieve;compression schemes;efficient based compression;iflows based compression;efficient compression;compression schemes enables;based compression;compression ratios efficient;incorporated based compression;state art compression;enables efficient compression;compression;compression ratios;present based compression;art compression;art compression ratios;bandwidth propose efficient;flow transformations iflows;distribution algorithm incorporated;uniform distribution algorithm;transformations iflows based;distribution algorithm;invertible flow transformations;numerical invertible flow;efficient terms bandwidth;bandwidth propose;transformations iflows", "pdf_keywords": "lossless image compression;compression based flow;lossless compression based;efficient lossless compression;algorithm lossless compression;efficient compression propose;generate lossless compression;lossless compression;coding algorithm lossless;lossless compression iflow;efficient compression;image compression;image compression based;compression iflow based;flow models compression;method lossless compression;compression propose;enabling efficient compression;state art compression;minimal value compression;compression propose novel;value compression achieved;compression ratios coding;compression architecture;compression based;lossless compression input;invertible flow transformations;lossless image;compression based uniform;image compression arising"}, "24ee54c8d5a01197e015d40be4277cfbb727394f": {"ta_keywords": "sharing bikes city;plan sharing bikes;sharing bikes traffic;sharing bikes;shared bikes;ride sharing bikes;captures sharing bikes;sharing bikes bounded;bikes city;set shared bikes;bikes traffic;performance sharing bikes;riders ride sharing;bikes city set;shared bikes setup;bikes;ride sharing;bikes traffic allows;bikes setup;bikes bounded area;bikes setup captures;bikes bounded;city set shared;traffic allows riders;datasets city demonstrates;driven plan sharing;datasets city;plan sharing;riders ride;allows riders ride", "pdf_keywords": ""}, "ea5cfce90444b17b36da07840b2f0cafb54ab0a7": {"ta_keywords": "deceptive corpus similar;use deceptive corpus;deceptive corpus;automatic detection deception;detection deception compare;features deceptive corpus;corpus use deceptive;detection deception;detection deception based;deceptive corpus order;detection deception higher;accuracy detection deception;deception based comparison;deception compare features;deception compare;comparison features deceptive;use deceptive;deception higher ask;deception based;deception;deception higher;features deceptive;deceptive;questions accuracy detection;similar english corpus;corpus similar;english corpus;corpus similar english;corpus features;corpus features questions", "pdf_keywords": ""}, "e3a1b2a19356dc685d78630ae2a8852ad6c86200": {"ta_keywords": "proximity aware ranking;search engine pruning;relevance features pruning;pruning proximity information;pruning proximity;aware ranking;aware ranking function;aware ranking functions;method pruning proximity;proximity information based;variant proximity;use proximity aware;use variant proximity;variant proximity aware;proximity information;based use proximity;search engine experiments;ranking;improve performance search;combination relevance features;relevance features;ranking functions;ranking function;proximity aware;features pruning;use proximity;search engine;ranking functions use;features pruning procedure;performance search engine", "pdf_keywords": ""}, "5e24aa9fdf5466e96d314dfcde973fccec02995d": {"ta_keywords": "online learning averaging;learning averaging data;learning averaging;pass online learning;online learning;learning algorithm improves;batch learning;line learning algorithm;averaging data algorithm;learning algorithm;batch learning methods;single pass online;pass online;balanced winnow algorithm;line learning;state batch learning;learning methods;averaging data;margin balanced winnow;accuracy single pass;winnow algorithm novel;improves accuracy single;winnow algorithm;novel line learning;learning;averaging;accuracy single;balanced winnow;algorithm improves accuracy;algorithm improves", "pdf_keywords": ""}, "e2854bf66ed86a5dc74183bae5fde18e65699833": {"ta_keywords": "polyphonic sound detection;speech recognition multi;speech recognition;neural network lagrangian;polyphonic;polyphonic sound;method polyphonic sound;field speech recognition;method polyphonic;new method polyphonic;memory model neuronal;neural;neuronal network proposed;sound detection;neural network;model neural;recognition multi label;hidden memory model;bidirectional hidden memory;model neuronal network;model neuronal;neuronal network;recognition multi;sound detection based;model neural network;neuronal;network lagrangian;extends model neural;multi label classification;network lagrangian lagrangian", "pdf_keywords": ""}, "21c9c624bc328686cef4bb1f80a786a5027d8886": {"ta_keywords": "citation aware scientific;scientific information extraction;tools facilitate citation;citation graph referential;citation aware;facilitate citation aware;citing cited papers;content citation graph;structure citation graph;citation graph;referential links citing;extracting information scientific;facilitate citation;cited papers;links citing cited;links citing;structure content citation;content citation;citing cited;information scientific documents;information extraction;cited papers improve;citing;structure citation;graph referential links;scientific documents approach;information extraction tasks;papers improve extraction;automatically extracting information;citation graph lead", "pdf_keywords": "citation aware saliency;salient entity classification;citation aware information;textual information citations;salient entities documents;neural network citation;citation graph referential;topic saliency extraction;extracting salient entities;content citation graph;extract citation aware;extracting salient entity;citation aware;citation graph information;citation aware scientific;salient entities text;document citation graph;citation graph knowledge;information citations;documents using neural;related entity extraction;related topic saliency;predicting structure citation;relation extraction models;topic saliency;information relation extraction;entities document citation;finding salient entities;information extraction tasks;relation extraction"}, "50c651e9f94f9d4927a726af0ef44818179d87da": {"ta_keywords": "translation applications parser;parser based multilingual;geo query corpus;semantic parser;semantic parser use;semantic parsing tasks;semantic parsing;handle semantic parsing;query corpus;query corpus designed;machine translation applications;multilingual geo query;machine translation tasks;machine translation;parser used efficiently;semantic information corpus;efficiently extract semantic;parser based;parser;applications parser based;parser use;translation applications;parser used;perform machine translation;present semantic parser;translation tasks;use machine translation;parsing;extract semantic;applications parser", "pdf_keywords": ""}, "cd0702deabaa8b7ccfba077f89dcc24e48ae1d47": {"ta_keywords": "relevance ranking mixture;baseline relevance ranking;relevance ranking;relevance ranking data;relevance ranking effective;likelihood relevance ranking;model relevance ranking;document ranking;ranking document ranking;traditional subtopic retrieval;ranking document;relevance mmr ranking;outperform baseline relevance;subtopic retrieval use;subtopic retrieval;query likelihood relevance;document ranking evaluate;mmr ranking document;retrieval use statistical;baseline relevance;retrieval;novel query likelihood;marginal relevance mmr;ranking mixture;ranking mixture model;relevance mmr;retrieval use;maximal marginal relevance;ranking data;mixture model relevance", "pdf_keywords": ""}, "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d": {"ta_keywords": "accelerating inference;accelerating inference large;approach accelerating inference;natural language processing;multilayers natural language;classification regression tasks;inference large expensive;language processing;inference large;meta consistency classifier;natural language;classification;inference;stop prediction;classification regression;regression tasks;classifier;consistency classifier;decide stop prediction;approach classification;approach classification regression;meta consistency;prediction;novel approach accelerating;classifier decide stop;stop prediction process;language processing approach;approach accelerating;use meta consistency;accelerating", "pdf_keywords": "adaptively early exiting;early exit classifier;early classifier;predictions faster classifiers;adaptive adaptively early;classic early classifier;classifier early;exit classifier early;classifier early exiting;adaptively predict;predictions pre trained;adaptively early;predicting accuracy;classifier meta early;faster classifiers consistent;method adaptively predict;predicting accuracy neural;accuracy able predict;early predictions consistent;trained predict;accuracy predictions;classifiers based stages;early exit classifierwe;prediction stop;predictions simple classifiers;ensure predictions faster;prediction stop computation;accuracy predictions present;method predicting accuracy;adaptively predict level"}, "77cd3ae8a0b9ef6865d5324a4d62280e6f7a1053": {"ta_keywords": "adversarial network augmentation;network augmentation trained;data augmentation methods;data augmentation;augmentation trained map;data augmentation using;augmentation trained;augmentation methods data;augmentation methods end;investigate data augmentation;speech recognition e2e;network augmentation;methods data augmentation;different data augmentation;augmentation methods;end automatic speech;automatic speech;augmentation using;augmentation using text;augmentation provided pretrained;automatic speech recognition;label augmentation;recognition e2e distanttalk;label augmentation provided;pseudo label augmentation;text speech;augmentation;speech recognition;cycleconsistent generative adversarial;augmentation provided", "pdf_keywords": "speech deep learning;speech recognition e2e;labeled speech deep;automatic speech;paired speech based;end automatic speech;automatic speech recognition;speech recognition;augmentation methods training;framework paired speech;conventional speech recognition;speech deep;data augmentation methods;speech extend;extend conventional speech;signal speech extend;speech perform approximate;paired speech;speech recognition introducing;data augmentation;pseudo label augmentation;speech based pseudo;label augmentation pseudo;augmentation pseudo labeled;augmentation methods end;deep learning dss;label augmentation effective;propose augmentation methods;recognition e2e distanttalk;deep learning"}, "3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c": {"ta_keywords": "neural semantic parser;predicted semantic parsers;semantic parser reranking;semantic parsing tasks;semantic parsers;semantic parser;parser reranking best;semantic parser test;semantic parsing;parser reranking;semantic parsers implement;test semantic parsing;parsers implement reranker;parsing tasks improving;existing neural semantic;parser test semantic;neural semantic;predicted semantic;competitive neural semantic;strong baseline parser;parsers;list predicted semantic;parsing tasks;parser;parsing;parsers implement;baseline parser;parser test;reranker competitive neural;semantic", "pdf_keywords": ""}, "89d15c9de3608157ff746af7368556149b50e037": {"ta_keywords": "language predicting sememe;word language predicting;language predicting;predict word language;language modeling;language estimating sememe;language modeling based;language modeling used;predict word;predicting sememe;word language estimating;applying language modeling;predicting sememe distribution;language estimating;framework language modeling;modeling based sememes;sememes enables predict;used predict word;enables predict word;based sememes;word language;given textual context;textual context;estimating sememe;sememes;distribution given textual;based sememes enables;predicting;estimating sememe distribution;textual", "pdf_keywords": "language sense prediction;words language modeling;predicting language model;driven language model;language modeling;prediction sequence words;lexical sememe knowledge;predicting language;predicting linguistic meaning;sememes minimum semantic;prediction word prediction;words sememes model;predicting linguistic;word prediction;language modeling task;word prediction model;atomic semantic units;atomic semantic;language modeling model;neural language sense;language modeling dataset;model language modeling;language modeling inspired;leverage lexical sememe;sememe sense generation;language model;sense word decoder;word prediction based;prediction model semantic;called word prediction"}, "9dbd86f089c2132dc46d316750d9786d60d5d720": {"ta_keywords": "annotate historical language;annotation tool historical;annotate historical;web based annotation;historical language data;annotation taggers;wants annotate historical;annotation tool;based annotation tool;pro annotation taggers;language data tool;language data;tool historical language;annotation;standard language data;based annotation;annotate;language data standard;pro annotation;historical language;wants annotate;used wants annotate;data standard language;tool based library;standard language;abs pro annotation;data tool available;based library;data tool based;data tool", "pdf_keywords": ""}, "6f1ca0249eafa36a5762ac53f6ba2a4ee2133456": {"ta_keywords": "approach speech recognition;speech recognition called;speech recognition;new approach speech;approach speech;speech;recognition called;recognition;introduces new approach;new approach;paper introduces new;paper introduces;introduces new;introduces;approach;paper;new;called", "pdf_keywords": "human transcripts audio;transcripts audio;characterizing transcription;transcripts audio audiobooks;accurately characterizing transcription;decoder detect transcription;corpus human transcripts;transcription errors corpus;transcription;fully formatted transcript;accuracy transcription;transcription error detection;opus codec;transcription errors filter;human transcripts;speech recognition toolkits;detect transcription errors;sequence audio;characterizing transcription errors;formatted transcript;recognition speech;detect transcription;transcripts;speech recognition provide;corpus speech method;protocol opus codec;novel speech recognition;transcript;transcript end end;approach recognition speech"}, "22b7a7c9faa8f340520ae1418c9cf8d960aaeec0": {"ta_keywords": "logic symbolic language;symbolic knowledge based;support symbolic logic;symbolic logic;symbolic logic symbolic;logic symbolic;knowledge based reasoning;implementation symbolic logic;symbolic language based;neuro symbolic knowledge;symbolic logic powerful;symbolic logic large;symbolic knowledge;symbolic language;complexity symbolic logic;questions symbolic language;reasoning able support;logic powerful tool;based reasoning able;neuro symbolic;able support symbolic;support symbolic;solving natural language;based reasoning;reasoning able;symbolic;implementation symbolic;logic large;questions symbolic;logic", "pdf_keywords": ""}, "eeec05fc11b2e0b40b3b0800bc50930e240cafeb": {"ta_keywords": "requisite structure corpus;corpus understood characterizing;structure corpus understood;corpus based;structure corpus;structure corpus based;corpus based analysis;corpus understood;corpus;background information terms;characterizing background information;background information context;specific given corpus;requisite structure;requisite structure features;given corpus;pre requisite structure;characterizing pre requisite;characterize pre requisite;infer background information;understood characterizing background;background information process;background information;infer background;intrinsic background information;characterizing background;information context specific;used infer background;information terms set;information context", "pdf_keywords": ""}, "525b7f73744f5650391be4678d6d51ddaf23ed72": {"ta_keywords": "detector noise detectors;detect detector noise;detector noise detector;detector noise;detector noise provide;noise detectors detect;noise detector noise;linear detector detector;noise detectors;noise provide detector;noise detector detect;noise detector;linear detector;generic linear detector;detectors detect detector;detector detector detector;detector detector detect;detector detect detector;detector detector;detectors detect;provide detector detector;detect detector;detector detect;detector;detectors;provide detector;noise;noise provide;detect;hypothesis generic linear", "pdf_keywords": ""}, "d338bcd1e34a8259e123465203b05c5bf21aa12a": {"ta_keywords": "preserve speaker individuality;japanese erj speech;speaker individuality;speaker individuality context;method preserve speaker;sound modes erj;preserve speaker;acoustic mode extracted;read japanese erj;featureless sound modes;acoustic mode;speech proposed method;featureless sound;english read japanese;japanese erj;speaker;power acoustic mode;read japanese;power featureless sound;erj speech proposed;sound modes;erj speech;japanese;library erj modes;erj modes;power acoustic;modes erj extracted;speech;low power acoustic;erj modes represented", "pdf_keywords": ""}, "095bc69eddbf73fabf58a929d2be9a99c1b533a6": {"ta_keywords": "computational preference reasoning;preference reasoning advance;preference reasoning;preference reasoning form;computational social choice;social choice repository;computational preference;associated specific preferences;computational social;specific preferences;preference;preferences;choice power computational;advance social choice;choice repository;preferences accessed preflib;choice repository available;social choice;preflib repository preference;http preflib org;preflib repository;reasoning advance social;introduce preflib repository;power computational social;social choice power;preferences accessed;repository preference reasoning;specific preferences accessed;preflib org_;preflib org", "pdf_keywords": ""}, "85099e075880a4844f3de77006a80c73daf99a4c": {"ta_keywords": "metastable state generated;generation metastable metastable;creation metastable states;metastable metastable state;metastable state method;metastable states metastable;metastable state;metastable states;generation metastable;different metastable states;states metastable state;states metastable;states different metastable;simultaneous creation metastable;metastable metastable;metastable states different;creation metastable;method generation metastable;different metastable;metastable;state generated phase;generated phase space;generated phase;state generated;represented phase space;represented phase;use phase space;phase space ss;phase space;ss represented phase", "pdf_keywords": ""}, "f7ce4c7ec30c846cc122393deee98f1eacd24049": {"ta_keywords": "dialogue state tracking;dialogue state tracker;improved dialogue state;dialogue state;state dialogue accurate;baseline dialogue state;state dialogue;resulting state dialogue;short term memory;dialogue accurate better;state tracking challenge;improved dialogue;propose dialogue state;achieve improved dialogue;dialogue;dialogue accurate;baseline dialogue;state tracking accuracy;tracker based long;memory neural networks;combine baseline dialogue;state tracking;tracking challenge dstc;tracking challenge;memory neural;state tracker based;term memory neural;neural networks ldss;propose dialogue;state tracker", "pdf_keywords": ""}, "bd24b47165407a8b2d32016645ca71f7c9213636": {"ta_keywords": "classifying email;classify email;classifying email best;strategy classify email;problem classifying email;classify email combine;sender convey message;intended sender ability;sender ability;classifying;message intended sender;email combine ability;email best strategy;ability sender convey;classify;receiver convey message;intended sender;email combine;convey message intended;message intended reader;sender convey;email best;convey message;ability sender;sender ability receiver;sender;email;problem classifying;combine ability sender;best strategy classify", "pdf_keywords": ""}, "b54d49cdf57abf7f7e7bcc0e946f450fd807f829": {"ta_keywords": "inverse reinforcement learning;inverse reinforcement;problem inverse reinforcement;reinforcement learning irl;constraints agent behavior;constraints agent;reinforcement learning;considering constraints agent;expert agent given;expert agent;behavior expert agent;reinforcement;agent behavior;learning irl seek;explain behavior expert;approach problem inverse;inverse;idea explain constraints;behavior relevant constraints;agent;problem inverse;behavior expert;agent given;agent behavior relevant;explain constraints;learning irl;constraints;agent given set;constraints behavior;constraints motivated", "pdf_keywords": "inverse reinforcement learning;learning constrained demonstrations;inverse reinforcement;demonstrations selection constraints;problem inverse reinforcement;inference constraints demonstrations;constrained demonstrations selection;constraints future demonstrations;constraints demonstrations based;demonstrations selection;constrained demonstrations;learning constraints;demonstrations use reinforcement;demonstrations constrained;reinforcement learning irl;constraints demonstrations computational;behaviors robots demonstrations;behavior robots demonstrations;demonstrations computational task;constraint learning;demonstrations selection restrictive;learning constraints generally;constraints necessary learning;constraints demonstrations;learning constraint;learning constraint constraints;learn constraints;demonstrations constrained optimization;robots demonstrations;demonstrations computational"}, "1a2410823486613e327892f05b38d3070f2d712c": {"ta_keywords": "harmonic trap particles;particles harmonic trap;confined harmonic trap;traps dynamics particles;harmonic trap;particles confined harmonic;lattice particles harmonic;traps dynamics;particles hop traps;trap particles confined;hop traps dynamics;traps particles hop;hop traps particles;lattice particles hop;trap particles;particles harmonic;dynamics lattice particles;traps particles;trap particles allowed;confined harmonic;dimensional lattice particles;lattice particles;dynamics dimensional lattice;particles hop;dynamics particles;dynamics lattice;hop traps;particles similar dynamics;particles confined;dynamics particles similar", "pdf_keywords": ""}, "3bb1e24eb3429f807397833105d1e137d9927767": {"ta_keywords": "active sequence labeling;sequence labeling;extra labeled sequences;sequence labeling method;labeled sequences;labeled sequences iteration;generating extra labeled;sequence labeling 27;improve label efficiency;label efficiency active;discriminator sequence mixup;efficiency active sequence;label efficiency;effective data augmentation;labeling;active sequence;method improve label;data augmentation;improve label;labeling method;extra labeled;data augmentation method;augments samples generating;discriminator sequence;simply augments samples;sequence mixup;labeled;labeling method eq;label;samples generating extra", "pdf_keywords": "sequence labeling tasks;active sequence labeling;samples sequence labeling;sequence labeling;sequence labeling task;sequence labeling methods;sequence labeling important;sequence labeling eq;labeled samples sequence;sequence embedding;resource sequence labeling;sequence labeled;efficient labeling;sequence embedding algorithm;efficiently generated deep;generates sequence labeled;labeled sequencesin;leveraging human annotations;sequence labeled sequencesin;labeling tasks train;efficient labeling limited;learning eq sequence;labeling eq generate;samples deep learning;provably efficient labeling;sequences mixup label;label selection sequence;labeling tasks;labeling task;input samples embedding"}, "17c9a0f1a287c08bb2c1c1df47fa51ce1e428c4e": {"ta_keywords": "prediction shape;prediction shape given;accurate prediction shape;shape given object;nonlinear dynamical method;use nonlinear dynamical;given object based;given object;nonlinear dynamical;use nonlinear;nonlinear;shape given;dynamical method based;dynamical method;efficient accurate prediction;based use nonlinear;prediction;shape;object based;accurate prediction;object;body interaction;interaction o_ body;o_ body interaction;dynamical;n_ body interaction;body interaction o_;method efficient accurate;combination n_ body;method efficient", "pdf_keywords": ""}, "a064010cf6fe594b2506a8fecd16dc0040211daa": {"ta_keywords": "translation multilingual data;multilingual data low;translation multilingual;multilingual data;approach translation multilingual;data translation;achieve higher translation;data translation channel;achieve better translation;multilingual;low resource language;higher translation;translation channel;higher translation gains;better translation gains;translation gains existing;correspondence data translation;translation channel underlying;approach translation;resource language;translation gains;novel approach translation;resource language method;data low resource;translation;better translation;language method;language idea;low resource;language proposed approach", "pdf_keywords": "embedding improves translation;multilingual word embedding;multilingual training low;improving performance multilingual;multilingual training;standard semantic embedding;target vocabulary embedding;performance multilingual training;embed words vocabulary;vocabulary embedding;target vocabulary embed;word embedding;vocabulary embed;vocabulary embed embedding;translation low resource;semantic embedding;word embedding method;vocabulary embedding embedding;embed words;performance multilingual;semantic embedding method;improves translation accuracy;embedding built word;neural machine translation;multilingual word;low resource languages;translation accuracy encoding;embedding improves;embedding method improving;decoder inefficient training"}, "2fbb75d7947808698f1554e4d400ec5ecb5ef998": {"ta_keywords": "comprehension task long;reading comprehension task;reading comprehension;typical reading comprehension;task long document;comprehension task order;performance global prediction;global prediction model;prediction model long;performance typical reading;global prediction;long document;model long document;reading;comprehension task;task long;typical reading;document proposed model;prediction model;improving performance global;model improves performance;prediction;model improves;long document proposed;improving performance;comprehension;performance global;proposed model improves;improve performance;improve performance typical", "pdf_keywords": "reading comprehension model;improving answer selection;models reading comprehension;answer selection task;reading comprehension dataset;answer selection helps;answer selection;dataset answer selection;challenging reading comprehension;predict answer question;adapted answer selection;context reading challenge;reading comprehension work;reading comprehension;apply reading comprehension;long documents weighted;comprehension model;answer selection long;comprehension dataset;reading challenge;predictions portions documents;comprehension model reduce;problem context retrieval;comprehension dataset answer;documents weighted;documents weighted global;selection answer candidates;narrativeqa dataset challenging;context retrieval;reading challenge capture"}, "c7c93601b52b1bcc68ec1f8b2c77c54f1b358ab9": {"ta_keywords": "pairwise comparison models;test pairwise comparison;pairwise comparison data;pairwise comparison;comparison models;comparison data;test pairwise;sample test pairwise;comparison data test;range pairwise comparison;comparison;sample tests investigate;pairwise;sample tests;theoretic lower bounds;assumptions proving information;information theoretic lower;tests investigate;data test;bounds range pairwise;distributions samples competitive;proving information theoretic;tests;data test simple;range pairwise;lower bounds;assumptions distributions samples;samples competitive;samples competitive state;test", "pdf_keywords": ""}, "e59adee86b666ad76164b3446cfee5068a15e5c9": {"ta_keywords": "fault tolerance abft;guided fault tolerance;tolerance abft neural;abft neural networks;intensity guided fault;efficient nn layers;neural networks nns;abft neural;fault tolerance;nn layers intensity;nns use intensity;nn layers;efficient nn;tolerance abft;guided fault;selects efficient nn;neural networks;abft reduces execution;intensity guided abft;guided abft reduce;abft selects efficient;guided abft;guided abft reduces;layers intensity guided;abft propose intensity;abft reduce execution;layers intensity;adaptive approach intensity;neural;approaches abft", "pdf_keywords": "faults deep learning;nn inference gpus;inference optimized gpus;inference gpus widely;tolerance deep learning;inference gpus;nn inference exploits;perform deep computations;parallelism deep learning;deep computations;deep computations shared;execution nn inference;errors deep learning;deep learning nn;fault tolerance nns;learning errors deep;deep learning intensity;architecture enable deep;fault tolerance deep;fullyconnected deep learning;layers nns gpus;bandwidth nn inference;nns gpus improve;efficiently compute nns;detection fault tolerance;pipeline deep learning;optimized gpus;deep networks nns;high compute optimized;deep learning pipeline"}, "e6fa88f4af68aa7be4ae91940892eee52571997c": {"ta_keywords": "shot object detection;improve shot object;object detection challenging;object detection accuracy;shot object;object detection;approach shot object;detection accuracy;meta learning;detection challenging;approach shot;meta learning method;new approach shot;improve shot;detection challenging task;object detection based;detection;learning method fine;detection based fine;implementation meta learning;used improve shot;shot;learning method;objects;rare objects;detection accuracy xmath0;learning;detection based;fine tuning;especially rare objects", "pdf_keywords": "shot feature extractors;shot object detection;detection shot objects;shot detection performance;object detection shot;shot classification large;shot detection novel;shot classification;identified shot detection;shot learning;shot learning propose;shot detection;detect shot objects;approach shot detection;detection shot object;shot image classification;benchmark detection shot;detection low shot;shot objects outperforms;detection shot;object detection challenging;object detection low;novel object detection;shot object objects;objects shot;capture object detection;shot feature;rare objects shot;shot objects;learning based shot"}, "db0a3ce9f315f650fe5220101c5677778de39fee": {"ta_keywords": "parallel text parser;parser machine translation;discriminative parser machine;learning discriminative parser;discriminative parser;translation accuracy factorizing;machine translation;text parser trained;boost translation accuracy;parser trained;aligned parallel text;machine translation using;translation accuracy;parallel text;boost translation;parser machine;text parser;parser trained generated;parser;translation using aligned;learning discriminative;used boost translation;translation using;method learning discriminative;using aligned parallel;discriminative;aligned parallel;translation;parallel;large margin training", "pdf_keywords": ""}, "06f4de06fc37576e1e381cd76e375d57852047b9": {"ta_keywords": "neural machine translation;machine translation nmt;translation nmt systems;efficient machine translation;machine translation;reliability text neural;text neural machine;machine translation demonstrate;text neural;translation nmt;improving reliability text;neural machine;reliability text;translation;polish shared task;neural;translation demonstrate power;task xmath0 np;nmt systems;nmt;translation demonstrate;text;task xmath0;shared task xmath0;transformer novel;nmt systems method;robust efficient transformer;novel approach robust;transformer;improving reliability", "pdf_keywords": ""}, "cd3595f65519e4af6bcd073790ac32acdafadf55": {"ta_keywords": "shot image generation;generation shot image;image generation shot;image generation;generation shot;image generation continuous;shot image;pretrained models regularization;10 shot image;weights adaptation;method shot image;examples 10 shot;pretrained models;novel method shot;generating high quality;combining pretrained models;method shot;weights adaptation demonstrate;changes weights adaptation;adaptation;10 shot;generating;combining pretrained;based combining pretrained;models regularization;algorithm generating;regularization;shot;pretrained;variable weights", "pdf_keywords": "generation shot images;shot image generation;generative model captures;multimodal image generation;unsupervised generation shot;image generation;generative models improvement;adapt pretrained generative;unconditional image generation;approach generate generative;generative adversarial networks;novel approach generative;pretrained generative;weights pretrained generative;pretrained generative model;generative models;generate generative;generative matching networks;generative model learned;generative adversarial;generative generative models;generative matching;generative models use;artistic domains;image generation low;generate generative model;image generation based;generation shot;based unsupervised generation;multimodal image translation"}, "6887537de3655a25c75bf4d0833f51e72331bdad": {"ta_keywords": "noisy signal enhanced;noisy signals enhanced;enhanced signal detecting;signal enhanced signal;enhanced signals method;signals enhanced signals;enhanced signal;signal enhanced;enhanced signals;signals enhanced;extended signal obtain;extended signal;extended signal method;detecting noisy signal;signal detecting noisy;use extended signal;converting noisy signals;converts noisy signal;phase extended signal;signal detecting;detection noise estimation;detecting noisy;including detection noise;detection noise;noisy signals;noise estimation;noisy signal surrounding;noisy signal;signal obtain estimate;signal obtain", "pdf_keywords": ""}, "a8fc183c089bd596ccc48b3d666f8814e1b41e55": {"ta_keywords": "generative code models;inexact generative code;generative code;inference comment generation;program synthesis;perform program synthesis;program synthesis left;introduce inexact generative;inexact generative;generation generate code;generative;comment generation;generate code;code models;code models able;type inference comment;comment generation variable;generate code large;code large corpus;generation generate;type inference;large corpus code;right generation generate;tasks type inference;bidirectional context substantially;generate;inference comment;code evaluate models;synthesis;generation variable naming", "pdf_keywords": "generative code engine;code generation;code generation tasks;code generation present;generative code models;incoder generative code;code guided synthesis;causal masking generate;generation code based;generative code;inference comment generation;generative language;code textstring generation;generative code model;code generation code;trained generate code;comment generation paper;program synthesis;binary generative code;generation code textstring;standard code generation;comment generation;generation code;binary code generation;perform program synthesis;generation editing;generate code;editing trained generate;source binary generative;program synthesis left"}, "9a43dda4b01dde5d513c431564098e4d8794a7a5": {"ta_keywords": "document level sentiment;level sentiment method;taxonomy product reviews;sentiment method;level sentiment;sentiment method combines;product reviews method;clusters words;clusters words represented;supervised classification document;classification document level;reviews method outperforms;supervised classification;use clusters words;reviews method;sentiment;classification;method supervised classification;classification document;clustering;product reviews;state art clustering;clustering approach;method supervised;words represented semantic;semantic spaces;new method supervised;represented semantic spaces;supervised;taxonomy product", "pdf_keywords": ""}, "2f201c77e7ccdf1f37115e16accac3486a65c03d": {"ta_keywords": "optimally prune network;optimal pruning network;prune network;quantization problem optimal;possible prune network;pruning network;pruning network nodes;prune network nodes;quantization partial differential;optimally prune;nodes possible prune;quantization partial;quantization problem;optimal pruning;demonstrate quantization partial;describes optimally prune;nodes small pruning;problem optimal pruning;apply quantization problem;quantization;demonstrate quantization;apply quantization;perturbation apply quantization;pruning;large demonstrate quantization;pruning rate large;pruning rate;pruning rate pruning;possible prune;small pruning rate", "pdf_keywords": "pruning neural networks;withstand adversarial;network withstand adversarial;withstand adversarial attacks;learning model adversarial;model adversarial;train adversarially;pruning pretrained networks;neural networks adversarial;adversarially trained;stochastic activation pruning;train adversarially trained;pruning neural;networks adversarial;adversarial;provides robustness adversarial;models robust adversarial;able train adversarially;model robust adversarial;model adversarial example;networks adversarial examples;robustness adversarial;trained stochastically;adversarial examples increasing;adversarially;robust adversarial;adversarial attacks;adversarial example;method pruning neural;adversarial example resulting"}, "136235d2a3dc4f1c995eaf977aec9c42114da850": {"ta_keywords": "morphosyntactic features uncertainty;dyadic morphosyntactic features;description dyadic morphosyntactic;dyadic morphosyntactic;morphosyntactic features;features uncertainty resource;morphosyntactic;features uncertainty;description dyadic;uncertainty resource;resource description dyadic;uncertainty;features;empirical resource description;empirical resource;dyadic;empirical;new empirical resource;new empirical;results new empirical;description;resource description;resource;results;present results;present results new;present;results new;new", "pdf_keywords": ""}, "8a09c90f6e9a3f6c3b172e5059c7af47f528f66b": {"ta_keywords": "semantic reinforcement;semantic reinforcement called;approach semantic reinforcement;reinforcement thematic;called reinforcement thematic;strategy reinforce message;reinforce message way;reinforcement strategy reinforce;thematic uses reinforcement;reminiscent classical reinforcement;reinforcement thematic uses;reinforce message;reinforcement called reinforcement;approach learn latent;reinforcement;learn latent space;reinforcement called;reinforcement strategy;learn latent;strategy reinforce;called reinforcement;classical reinforcement;semantic;classical reinforcement strategy;represent letters similarities;uses reinforcement strategy;reinforcement strategy use;unsupervised approach learn;represent letters;reinforce", "pdf_keywords": "letters generative way;letters generative;similarities use logograms;letter images;match letters generative;generative image;generative way use;create generative;generative way;logograms;generate interesting images;semantic reinforcement;learning theme word;visually resemble letters;letter image;tools create generative;communicate logo generation;represent letters;text;letter images method;use logograms;generative;letters theme visually;logo generation;cues reinforce word;resemble letters use;replica letter image;semantic reinforcement text;letter image treat;semantic reinforcement called"}, "1022696090666eab5c82ebc07d63c0de2fca2521": {"ta_keywords": "soft joins based;soft joins;perform soft joins;relational database joining;joins based;extension relational databases;relational databases;joins;database joining;new relational database;relational database;joins based equivalence;relational databases used;extension relational;whirl extension relational;new relational;database joining called;databases;database;relational;present new relational;data world wide;inductive classification;inductive classification tasks;data world;world wide web;joining called whirl;joining;number inductive classification;classification tasks", "pdf_keywords": ""}, "71124b00b873e85aa55b07100cd5b492e5b1d73d": {"ta_keywords": "verifier data integrity;quantum key distribution;japanese quantum key;data integrity untrusted;provided quantum key;quantum key;protect data integrity;data integrity certification;data integrity realized;implemented japanese quantum;integrity untrusted users;data integrity;based universal2 hash;realized verifying integrity;integrity untrusted;key distribution network;verifying integrity data;integrity data;integrity certification data;integrity data data;integrity realized verifying;verifying integrity;scheme protect data;untrusted users using;verifier based universal2;protect data;integrity certification;universal2 hash;verifier based;japanese quantum", "pdf_keywords": "secure distributed storage;storage authentication integrity;verifiable secret sharing;storage authentication;based verifiable secret;secret sharing based;integrity verified sharing;propose secure sharing;network data integrity;storage data integrity;arbitrary secret sharing;authenticated secret sharing;secure sharing scheme;distributed storage share;verifiable secret;security data integrity;secure sharing;transmission storage authentication;secure distributed;authentication integrity;data integrity guarantee;hiding data integrity;qkdwe propose secure;guarantee integrity data;shared secret sharing;data integrity simultaneously;secret sharing scheme;theorem security shared;verifiable cryptographic tools;theoretically secure distributed"}, "eb7a64195ef4a268f79fa6740f128387f2696c65": {"ta_keywords": "agents maximize reward;maximize reward environment;constraints demonstrations reinforcement;maximize reward;demonstrations reinforcement learning;autonomous reinforcement;learning learn maximize;agents maximize;autonomous reinforcement learning;reinforcement learning learn;reinforcement learning allows;learn maximize;reinforcement learning;reward environment;demonstrations reinforcement;maximize environmental rewards;learn maximize environmental;learning allows agents;bandit technique learn;reinforcement;allows agents maximize;problem autonomous reinforcement;contextual bandit technique;contextual bandit;reward environment follow;bandit technique;maximize environmental;uses contextual bandit;constraints demonstrations;rewards test algorithms", "pdf_keywords": ""}, "6cddfbed35c46937588bd9d6b846ca2855953cea": {"ta_keywords": "neural lattice based;neural network lattice;neural lattice lattices;neural lattice;lattice posterior;represented neural lattice;lattice posterior score;network lattice posterior;present neural lattice;architecture predicting posterior;words lattice;lattice words;lattice words coupled;predicting posterior probabilities;lattice based;model represented neural;lattice lattices;lattices lattice;lattice;lattice lattices form;lattice based architecture;number words lattice;discrete lattice words;lattices;posterior probabilities model;network lattice;predicting posterior;represented discrete lattice;lattices form;number lattices lattice", "pdf_keywords": "model lattice encoder;present lattice encoder;lattice encoder;performance lattice encoders;novel lattice encoder;translation accuracy lattices;lattice encoder lattice;propose lattice encoder;model translating lattices;encoder lattice scores;lattice encoder combines;translating lattices;attentional encoder decoder;lattice encoders;attentional encoder;encoder lattice nodes;lattice sequence model;output attentional encoder;lattice models forward;lattices automatic speech;machine translation model;encoder lattice;encoding lattice scores;encoder called lattice;lattice sequence tree;training lattice model;directed encoder lattice;translating lattices extend;lattice encoder called;called lattice encoder"}, "58a2e825884bc86e650fffafb86a2833117852c5": {"ta_keywords": "model hubs ranking;pre trained models;models rank pre;exploiting model hubs;hubs ranking tuning;models model hub;trained models rank;rank pre trained;ranking tuning pre;trained models;tuning pre trained;hubs ranking;pre trained;trained models model;model hub;models rank;extracted pre trained;features extracted pre;trained models estimate;best ranked ptm;model hubs;ranked ptm;ranking tuning;ptms tasks;model architecture target;model hub various;label evidence;types ptms tasks;rank pre;ranked ptm fine", "pdf_keywords": "transferability pre trained;pre trained models;trained models pre;models pre trained;model pre trained;trained model pre;pre training deep;pre trained model;trained models transferability;features pre trained;learning supervised pre;ranking pre trained;pre trained neural;scalable pre trained;pretrained models;supervised pre trained;pretrained models using;transfer learning supervised;metric pre trained;approach pretrained models;pretrained models perform;labeled models trained;pre labeled models;improve transfer learning;trained models benchmark;trained models ranking;exploiting pre trained;tuningpre trained models;transfer learning;trained models tasks"}, "e6cec3044688f1701b4b72b4b2189f215abc3759": {"ta_keywords": "mechanisms crowdsourcing experiments;reward mechanisms crowdsourcing;mechanisms crowdsourcing;crowdsourcing experiments involving;crowdsourcing experiments;crowdsourcing;experiments involving truthful;truthful behavior agents;beliefs arbitrary mechanisms;data sets mechanisms;behavior agents;new reward mechanisms;reward mechanisms;behavior agents mechanisms;involving truthful behavior;agents;heterogeneity agents abilities;beliefs arbitrary;parallel data sets;reward;agents mechanisms;existence heterogeneity agents;allow beliefs arbitrary;arbitrary mechanisms;parallel data;data sets;truthful behavior;agents abilities;agents abilities allow;propose new reward", "pdf_keywords": ""}, "549df5fc83c382cbdf633dc782fa67bf2f983f2c": {"ta_keywords": "distributed database protected;data stored distributed;distributed storage data;robust distributed storage;protect data data;distributed database;way protect data;stored distributed database;stored distributed;propose distributed storage;distributed storage propose;protect data;distributed storage;distributed database used;storage propose distributed;data distributed;database protected differentially;data data breach;data breach threats;activations distributed database;database protected;data distributed large;design distributed storage;agents data stored;data breach;robust distributed;storage data;protected differentially distributed;agents data;data stored", "pdf_keywords": ""}, "f61886d138497431cbeaa7bb73051bfb7a745026": {"ta_keywords": "linguistic structures captions;captions relations;structures captions;captions relations individual;structures captions method;captions method;captions method captures;provided captions relations;captions;information provided captions;image level supervision;generation linguistic structures;generation linguistic;context subjects objects;provided captions;method generation linguistic;crowdsourced image;combination crowdsourced image;linguistic structures;crowdsourced image level;subjects objects leverages;subjects objects approach;phrasal sequential context;localization subjects objects;sequential context improve;subjects objects;triplets context subjects;combination crowdsourced;context subjects;leverages combination crowdsourced", "pdf_keywords": "scene graph generation;scene graphs learning;learned scene graphs;scene graph representation;generate scene graphs;supervised visual relation;scene graph;scene graphs;context based captions;scene graph use;weakly supervised visual;weakly supervised object;understanding scene graphs;visual relation detection;learning image descriptions;scene graphs used;scene graphs outperforms;specific scene graph;scene graphs method;method scene graph;context generate scene;describing understanding scene;weakly supervised;representation learned scene;object detection context;generate scene;aspects scene graph;truth scene graphs;captions localizes subjects;linguistic context generate"}, "275aaa20ba853c40a461f224eefbf06730bf03a9": {"ta_keywords": "nonconvex optimization algorithm;points nonconvex optimization;nonconvex optimization;saddle points nonconvex;finding negative curvature;negative curvature near;negative curvature;curvature near saddle;near saddle points;gradient descent;simple gradient descent;algorithm finding negative;points nonconvex;gradient descent based;saddle points;optimization algorithm polynomially;propose simple gradient;nonconvex;optimization algorithm;near saddle;descent based algorithm;curvature near;optimization;simple gradient;gradient;finding negative;saddle;curvature;algorithms proposed literature;algorithm polynomially better", "pdf_keywords": "accelerated gradient descent;accelerated gradient descents;perturbed gradient descent;algorithm based hessian;quadratic gradient descent;nonconvex optimization algorithm;points gradient descent;gradient descent;linear gradient descents;points nonconvex optimization;gradient descent algorithm;algorithm escaping saddle;gradient descent methods;gradient descent stationary;saddle points gradient;perturbed accelerated gradient;gradient based hessian;gradient descent method;gradient descent pgd;method gradient descent;algorithm quadratic gradient;nonconvex optimization;gradient descents applicable;gradient descents;quadratic gradient algorithm;descent method gradient;minima nonconvex optimization;based accelerated gradient;hessian power method;hessian detectwe propose"}, "e6924d247b56980260e4c68dbc51b947409e4764": {"ta_keywords": "chiral dynamics xmath0;xmath0 state chiral;chiral symmetry breaking;breaking chiral dynamics;symmetry breaking chiral;breaking chiral symmetry;symmetry breaking xmath2;symmetry breaking xmath1;breaking chiral;breaking affected chiral;chiral phase transition;state chiral phase;breaking xmath2 state;transition region chiral;chiral dynamics;symmetry breaking affected;affected chiral symmetry;state related chiral;breaking xmath1 state;breaking consequence chiral;dynamics xmath0 state;effect chiral symmetry;chiral phase;symmetry breaking;chiral symmetry;region chiral symmetry;state chiral;xmath1 state related;symmetry breaking consequence;xmath1 state", "pdf_keywords": "stochastic gradient descent;stochastic gradient estimators;stochastic gradient methods;local stochastic gradients;stochastic gradient algorithms;stochastically distributed gradient;local stochastic gradient;performance stochastic gradient;computation stochastic gradients;stochastic gradients provide;algorithms stochastic gradients;stochastic gradient crucial;stochastic gradient refinement;stochastic gradients widely;stochastic gradients;stochastic gradient convergence;stochastic gradient fixed;stochastic gradients stochastic;method stochastic gradient;gradients stochastic gradients;framework stochastic gradient;stochastic gradient method;stochastic gradient;drift stochastic optimization;analysis stochastic gradients;underlying stochastic gradient;gradient method stochastically;gradient refinement stochastically;gradients stochastic;model stochastic gradient"}, "a1fc0041ef89ed5371317c8e2cc5effa8f38ae48": {"ta_keywords": "super structure estimation;structure estimation methods;structure estimation;sub super structure;super structure;search local clusters;neighbors hops superstructure;algorithm propose local;local clusters;local clusters formed;super sub super;hops superstructure;local search strategy;exact search local;clusters;propose local search;sub super;nodes;nodes high accuracy;hops superstructure demonstrate;hundreds nodes;superstructure;super sub;local search;clusters formed variable;hundreds nodes high;exact search;estimation methods;scale hundreds nodes;class super sub", "pdf_keywords": "estimation super structure;super structure estimation;estimation linear hierarchical;reliable causal discovery;causally informed probabilistic;substructure estimation noisy;causal discovery;estimating super structures;hierarchical structure noisy;optimal sparse causal;matrix causally informed;based estimation super;estimation noisy noisy;learning super structure;estimation noisy;informed probabilistic network;sparse causal graphs;detection super structures;sparse causal;causal discovery assumptions;effective estimating super;method estimation super;structure robust noise;estimation true structure;estimation super;probabilistic network;structure estimation;estimated super structure;covariance matrix causally;redundancy estimation faithfulness"}, "0a227a21172f7344ad911aeefc40ae4ec82d7cac": {"ta_keywords": "metaphor annotated datasets;metaphor identification creation;metaphor annotated;metaphor identification;useful metaphor identification;metaphor identification paper;features useful metaphor;methodology metaphor identification;creation metaphor annotated;metaphor;novel annotated corpora;annotated corpora;useful metaphor;describing creation metaphor;creation metaphor;annotated datasets;novel annotated;annotated datasets identification;annotated;methodology metaphor;creation novel annotated;corpora;describing;covers methodology metaphor;describing creation;identification new features;datasets identification new;identification creation novel;datasets;datasets identification", "pdf_keywords": ""}, "178f424d0f156cbf5b35eb241fc00b27a0a3808b": {"ta_keywords": "chime speech separation;speech separation recognition;architecture speech enhancement;speech enhancement recognition;ssr sequence training;speech enhancement;recurrent neural network;sequence training based;short term memory;memory recurrent neural;speech separation;chime speech;term memory hybrid;sequence training;network architecture speech;neural network ssr;term memory recurrent;memory hybrid ssr;recurrent neural;architecture speech;2nd chime speech;memory recurrent;neural network architecture;training based ssr;term memory;ssr sequence;ssr combines short;neural network;network ssr sequence;hybrid ssr sequence", "pdf_keywords": ""}, "317ed59456d76b500a7eb63b181df9e8b795976b": {"ta_keywords": "parking urban environments;framework parking urban;new framework parking;parking urban;framework parking;parking;urban environments;urban;present new framework;new framework;framework;environments;new;present new;present", "pdf_keywords": "parking socially optimal;socially optimal parking;parking optimal;parking optimal rate;parking congestion propose;optimal parking;parking parallel queues;parking congestion;optimal queueing equilibria;managers model parking;parking infrastructure game;pricing mechanism parking;street parking optimal;parking demand based;optimal parking infrastructure;parking managers model;parking demand;level parking congestion;relationship parking congestion;parking planning;equilibria queuing game;parking congestion city;queueing equilibria;optimal description parking;queues impose game;parking urban environments;queueing equilibria queuing;optimal queueing;parking availability;parking area game"}, "56823e326f2515f73662b176054fbee0895e0c44": {"ta_keywords": "requests users assistant;navigation users web;users web form;users assistant;assists user navigating;users assistant assists;web form;user navigating form;user navigating;method navigation users;web form method;assistant assists user;users web;navigation users;assistant user;assistant user specialist;navigating form;navigating form provides;interaction assistant user;requests users;based interaction assistant;navigation;user forward;interaction assistant;alternative user forward;stream requests users;assists user;usefulness assistant;user forward ambiguous;form provides alternative", "pdf_keywords": ""}, "7891ec1d8ba2abf238326dc6e8862cc4431a6f5c": {"ta_keywords": "network random lattice;multihop network random;priority wireless relay;wireless relay node;network random;assigning priority wireless;relay node based;wireless relay;lattice path objective;random lattice path;priority wireless;random lattice;relay node;number nodes network;deployment multihop network;nodes network;interference fringes network;convex hop costs;multihop network;relay;nodes network model;network model;network model formulated;costs number nodes;network propose;network propose model;lattice path;sum convex hop;network;fringes network", "pdf_keywords": ""}, "cdf5eb63e9c2434073e811aba50ae80ede9d15f6": {"ta_keywords": "focused retrieval documents;crowdsourcing_ search;retrieval documents;crowdsourcing_ search conducted;retrieval documents document;retrieval search service;focused retrieval search;retrieval search;new focused retrieval;called crowdsourcing_ search;document corpus focus;document corpus;documents document corpus;collection focused retrieval;focused retrieval;search conducted using;crowdsourcing_;search conducted;search service;retrieval;corpus focus new;corpus focus;called crowdsourcing_;corpus;service called crowdsourcing_;public key distribution;search;search service called;key distribution service;documents document", "pdf_keywords": ""}, "1afe82d34c182d43cbcc365d26e704058aa32351": {"ta_keywords": "corpus voice conversion;voice conversion based;voice conversion;based voice conversion;integration based voice;corpus voice;parallel corpus voice;speaker model proposed;density model speaker;model speaker model;model speaker;voice;conversion based mixture;speaker model;dynamic features model;combines parameter generation;conversion based;dynamic features;use dynamic features;features model optimization;conversion based use;constrained dynamic features;parameter generation algorithm;dynamic features proposed;parameter generation;model integration based;accuracy model integration;based voice;speaker;requirement parallel corpus", "pdf_keywords": ""}, "2c871df72c52b58f05447fcb3afc838168d94505": {"ta_keywords": "knowledge context bert;bert propose knowledge;knowledge neurons pretrained;knowledge neurons;concept knowledge neurons;cloze task bert;neurons express knowledge;task bert;knowledge stored pretrained;task bert propose;context bert;context bert present;knowledge neurons examine;bert present preliminary;neurons pretrained language;knowledge stored;bert present;bert;storage factual knowledge;pretrained language models;express knowledge;knowledge;concept knowledge;stored pretrained language;introduce concept knowledge;bert propose;introducing concept knowledge;pretrained language;knowledge attribution;knowledge context", "pdf_keywords": "knowledge neurons express;knowledge neural;knowledge neurons knowledge;neurons knowledge expression;analysis knowledge neurons;knowledge neurons;understood knowledge neurons;knowledge neurons producing;activation knowledge neurons;knowledge neurons accordingly;concept knowledge neurons;knowledge neurons propose;knowledge neurons edit;knowledge neurons understood;leveraging knowledge neurons;utilize knowledge neurons;knowledge neurons tend;knowledge neurons demonstrate;pretrained factual knowledge;neurons knowledge;knowledge neurons update;shows knowledge neurons;knowledge neurons proposed;neurons propose knowledge;underlying knowledge neural;neurons update knowledge;influence knowledge neurons;neurons understood knowledge;factual knowledge pretrained;knowledge neurons positively"}, "26c2aad87810418b09e0f5b80352dd4d2536afe3": {"ta_keywords": "social skills training;social skills;social skills includes;acquire social skills;skills training sst;procedure social skills;discomfort social interaction;social interaction;social interaction acquire;computer based training;interaction acquire social;training sst;anxiety discomfort social;training sst established;discomfort social;sst performed human;social;skills training;decrease human anxiety;role play reinforcement;acquire social;sst;human anxiety;skills includes virtual;skills modeling role;skills modeling;user speech language;based training;human anxiety discomfort;skills", "pdf_keywords": ""}, "07cedc7899497f2f4ee6f4736e03b78accb47b74": {"ta_keywords": "relational neighbor classifier;semi supervised learning;neighbor classifier;vote relational neighbor;novel semi supervised;semi supervised;neighbor classifier large;relational neighbor;graph walk outperforms;weighted vote relational;benchmark datasets labels;supervised learning method;supervised learning;supervised;labeled data;outperforms weighted vote;vote relational;labeled;random graph walk;reduces labeled data;graph walk;datasets labels;walk outperforms weighted;authoritative instances training;based random graph;neighbor;weighted vote;classification;labeled data required;reduces labeled", "pdf_keywords": ""}, "d4d26ccbf1e64e725b5bffc08ab28a72e271facb": {"ta_keywords": "context dependent multidimensional;cross context dependent;analysis cross context;context dependent;cross context;multidimensional;dependent multidimensional;comprehensive analysis cross;analysis cross;comprehensive analysis;present comprehensive analysis;comprehensive;context;present comprehensive;analysis;dependent;cross;present", "pdf_keywords": ""}, "26d5c7ad2778c77a1b8734dceb34fe38a1179e2f": {"ta_keywords": "detection malwares mobile;malwares mobile devices;malwares mobile;presence malwares mobile;real time malicious;detection malwares;detect presence malwares;model detection malwares;malicious application rnm;presence malwares;malwares;malicious application;time malicious application;real time m1;m1 model detect;mobile devices use;mobile devices;devices use framework;framework real time;detect;model detect;malicious;application rnm model;time malicious;model detect presence;application rnm;model detection;mobile;detection;novel model detection", "pdf_keywords": ""}, "9045bf2a9c1e2b9621c69c57f991d10880e91f18": {"ta_keywords": "randomness submatrix detection;randomness submatrix;used randomness submatrix;randomized reduction demonstrate;randomized reduction;sparse submatrix detection;input randomized reduction;independence sparse submatrix;submatrix detection testing;submatrix detection;wise independence sparse;sparse submatrix;finding randomness;independence sparse;finding randomness input;submatrix detection problem;randomness input randomized;problem finding randomness;leakage used randomness;randomized;computational hardness;randomness input;randomness;computational hardness variety;demonstrate secret leakage;hardness variety statistical;secret leakage;reduction demonstrate secret;submatrix;input randomized", "pdf_keywords": "logspace hardness statistical;randomized logspace hardness;logspace hardness computing;hardness computing logspace;logspace hardness algorithm;problem complexity statistical;randomized conjecture logspace;randomized logspace algorithms;conjecture logspace hardness;computational hardness statistical;hardness computing clique;computing logspace hardness;complexity statistical problems;clique conjecture logspace;randomized logspace algorithm;complexity statistical;complexity studied statistical;logspace hardness planted;computational complexity planted;compute logspace hardness;logspace hardness;theorems randomized logspace;logspace hardness testing;complexity planted hypothesis;hardness host statistical;complexity planted clique;logspace algorithms solving;computational hardness;infer computational hardness;logspace algorithms able"}, "24a2f68cf81ba3ee55e7a87d0770374ab8e99858": {"ta_keywords": "learnable equivalence queries;programs learnable equivalence;recursive programs learnable;learnable equivalence;equivalence queries class;class recursive programs;recursive programs;equivalence queries;polynomial predictability;polynomial predictability model;programs learnable;computationally diicult;computationally diicult learning;used polynomial predictability;class recursive;diicult learning;recursive;relaxing constraints class;diicult learning problem;learnable;predictability;computationally;class leads computationally;constraints class;predictability model relaxing;predictability model;equivalence;queries class restricted;queries class;able used polynomial", "pdf_keywords": ""}, "5ed4b17a4b7932619f0969e1f5acae76e90f7bdd": {"ta_keywords": "recursive semantic parsing;nonnested query generation;nested query generation;query generation;novel recursive semantic;semantic parsing framework;semantic parsing;query generation problems;query generation problem;sql query layer;recursive semantic;parsing framework;query layer;progressive nonnested query;query layers;parsing;query layers experimental;recparser generate nested;different query layers;generate nested sql;nonnested query;query layer layer;parsing framework called;predicting different query;nested query;complicated nested query;utterance predicting;nested sql query;called recparser generate;nested sql", "pdf_keywords": ""}, "feb403bb5a064ab68b2db655b80a7417f7cfc9f3": {"ta_keywords": "relation tree based;relational learning approach;relational learning;relational networks tree;tree based relational;based relation tree;based relational networks;relation tree;novel relational learning;relational networks;features relational;features relational data;pairwise features relational;based contrastive induction;hidden variables relational;based relational;relational networks efficient;variables relational data;relational data;variables relational;relational;contrastive induction snd;contrastive induction;relational data proposed;novel relational;propose novel relational;networks tree based;based relation;relation;networks tree", "pdf_keywords": ""}, "2fb44f1317bc51a1e011a5a44d817ad9104e29e8": {"ta_keywords": "differentially private text;private text rewriting;code differentially private;private auto encoder;differentially private auto;encoder text rewriting;differentially private;analysis differentially private;private text;descrip code differentially;text rewriting descrip;auto encoder text;encoder text;text rewriting adept;tight privacy guarantees;rewriting adept descrip;rewriting descrip code;code differentially;private mechanism;providing tight privacy;private mechanism showing;auto encoder;encoder;privacy guarantees;rewriting descrip;text rewriting;privacy guarantees quantify;tight privacy;error private mechanism;descrip code achieves", "pdf_keywords": "differential privacy;autoencoder differentially private;privacy use randomization;differential privacy best;standard differential privacy;new class privacy;privacy use nonlinear;differential privacy opposed;class privacy;called differential privacy;differentially private;differentially private determine;curator privacy;differentially private process;class privacy problems;privacy problems curator;increase curator privacy;privacy opposed formalin;curator privacy use;privacy;privacy best;model private access;curator use randomization;private process guarantees;generalized case private;private mechanism;generalize notion private;private allows private;privacy opposed;sensitivity private mechanism"}, "b3848d32f7294ec708627897833c4097eb4d8778": {"ta_keywords": "language models dialog;neural language models;neural language;dialog fine tuning;language models;signal noise filtered;lamda language models;noise filtered;models dialog;language models called;filtered signal noise;noise filtered signal;family neural language;models dialog fine;dialog;signal noise;fine tuning annotated;tuning annotated;signal noise ratio;neural;noise ratio;tuning annotated data;ratio signal noise;noise;filtered signal;dialog fine;lamda language;noise ratio ratio;filtered signal using;filtered", "pdf_keywords": "groundedness dialog models;quality dialog models;dialog models;future dialog models;dialog modeling;safety dialog response;neural language models;dialog models approach;dialog response generation;models specialized dialog;improving quality dialog;dialog models used;safety dialog;quality safety dialog;human dialogs;dialog fine tune;ended dialog models;safety specificity dialog;tuning neural language;dialog models use;neural language model;ended dialog modeling;dialog responses;human human dialogs;dialogue response optimized;fine tuning annotated;language models learn;trained conversational;trained neural language;dialog modeling problem"}, "17c5e16d16585a01fbfd90ff39f6799952675b21": {"ta_keywords": "bilingual speech recognition;model bilingual speech;speech recognition monolingual;framework bilingual speech;proposed model bilingual;bilingual speech;modeling framework bilingual;model bilingual;final bilingual output;factorized final bilingual;bilingual output;recognition monolingual;framework bilingual;speech recognition conditionally;recognition monolingual code;bilingual;final bilingual;bilingual output obtained;monolingual code switched;monolingual code;given monolingual information;monolingual information;speech recognition;given monolingual;code switched corpora;monolingual information demonstrate;switched corpora;monolingual;obtained given monolingual;switched corpora demonstrate", "pdf_keywords": "model bilingual speech;jointly modeling bilingual;conditional model bilingual;bilingual speech recognition;model bilingual task;modeling bilingual;bilingual task modeled;speech recognition monolingual;simultaneously model bilingual;model bilingual situations;bilingual systems conditional;model bilingual pretrained;bilingual speech;novel bilingual conditional;switched speech monolingual;factorized model bilingual;bilingual conditional;model multilingual speech;proposed model bilingual;bilingual task bilingual;multilingual speech recognition;model bilingual;based constraints bilingual;bilingual task;task bilingual;model bilingual systems;task bilingual task;modeling bilingual andwe;bilingual pretrained monolingual;model bilingual model"}, "c204d40384d39c59cd7249bde4cd8615972acaac": {"ta_keywords": "robustness machine translation;current machine translation;translation systems;machine translation systems;machine translation;machine translation mt;translation systems ability;translation mt task;language pairs;translation;translation mt;cover language pairs;task improving robustness;improving robustness machine;robustness machine;languages pairs;pairs languages pairs;pairs languages;language pairs languages;languages pairs fewshot;improving robustness;cover language;challenges facing twinning;twinning;twinning real world;robustness;shared task improving;twinning real;world cover language;facing twinning real", "pdf_keywords": ""}, "024aa0b78e2a29d07533ee1c6e3b2e875ae45618": {"ta_keywords": "influences people conversation;people conversation data;estimating influences people;estimation influence people;conversation data proposed;conversation data;people conversation;estimating influences;estimation influence;model estimating influences;influence people;probabilistic model estimating;influences people;model estimation influence;propose probabilistic model;conversation;estimated using expectation;using expectation maximization;propose probabilistic;estimating;model estimating;meeting data;influence;estimated using;expectation maximization;model assumes people;probabilistic model;efficiently estimated using;expectation maximization em;assumes people", "pdf_keywords": ""}, "ffc211476f2e40e79466ffc198c919a97da3bb76": {"ta_keywords": "decentralised learning agents;method decentralised learning;learning agents;decentralised learning;learning agents based;agents based centralised;generative reinforcement learning;generative reinforcement;set generative reinforcement;based method decentralised;method decentralised;agents based;agents guarantees consistency;reinforcement learning;reinforcement learning tasks;decentralised version agent;decentralised decentralised policies;consistency decentralised decentralised;consistency decentralised;agent action proposed;agent action;decentralised policies evaluate;decentralised policies;guarantees consistency decentralised;agents;structure agents guarantees;structure decentralised;agents guarantees;decentralised decentralised;reinforcement", "pdf_keywords": "learning agents value;learning joint actions;deep multi agent;agent reinforcement learning;unit micromanagement starcraft;learning agents;micromanagement starcraft;multi agent reinforcement;reinforcement learning multiagent;reinforcement learning game;micromanagement starcraft ii;learning joint action;starcraft ii learning;learning reinforcement;multiagent reinforcement learning;agent optimal actions;learning reinforcement learning;learning centralised;agent reinforcement;learn optimal strategy;actions deep neural;task heterogeneous agents;method decentralised learning;reinforcement learning;multiagent reinforcement;mean reinforcement learning;reinforcement learning reinforcement;learning game;optimal actions;reinforcement learning structure"}, "a6b431df3b3d40c98d8d623cab559a9cddd41662": {"ta_keywords": "generative dialog model;generative dialog;data generated dialog;described generative dialog;generated dialog algorithm;dialog model algorithm;dialog model;generative schema assisted;generated dialog;dialog algorithm;dialog algorithm based;generative schema;schema assisted learning;based generative schema;dialog;dialog policy training;generative connective tissue;schema guided algorithm;schema guided;learning data generated;generative connective;algorithm based generative;presents schema guided;guided algorithm learning;schema assisted;generative;specific dialog;experiments generative connective;based generative;dialog policy", "pdf_keywords": "trained dialog task;trained dialog;schema trained dialog;shot dialog approach;zero shot dialog;dialog generation;shot dialog;learning train dialog;dialog policies training;task oriented dialog;dialog data achieve;dialog systems schema;dialog task;dialog systems predict;dialog data;dialog systems;rich dialog structure;schemas dialog data;dialog structure;dialog model;dialog model schema;dialog structure graph;meaningful schemas dialog;dialog leverage schema;dialog approach;task conditioned dialog;schema dialog leverage;dialog approach leverages;variety dialog generation;dialog"}, "62763dbdd47f144c73663b6c6b5d95caeb318e43": {"ta_keywords": "reconstructing structured matrix;matrix permutation rank;permutation rank model;propose permutation rank;rank underlying matrix;matrix low rank;permutation rank;matrix fixed rank;structured matrix entries;model matrix permutation;rank model matrix;matrix entries partially;structured matrix;matrix propose permutation;reconstructing structured;low rank assumptions;rank model improved;problem reconstructing structured;matrix entries;matrix permutation;reconstructing;rank assumptions;rank rank underlying;partially observed noise;fixed rank underlying;underlying matrix propose;consider problem reconstructing;rank model;underlying matrix;assume underlying matrix", "pdf_keywords": "rank matrix completion;movie rank matrix;movie matrices rank;matrix rank decompositions;estimation permutation rank;noisy matrix completion;rank decompositions;approximated low rank;rank user movie;rank decomposition;matrix low rank;low rank approximated;low rank matrix;matrices low rank;permutation rank decomposition;matrices rank user;rank approximation matrix;constructing rank approximation;matrix completion best;lower rank approximation;rank model matrices;rank matrices convex;matrix permutation rank;rank matrices;rank approximation;permutation rank matrix;fact rank approximation;submatrix rank approximation;rank approximated low;negative rank matrices"}, "bd49e66af9755e6138967eba6aeb37d8190d2b4f": {"ta_keywords": "extracting pairs spouses;pairs spouses text;spouses text method;spouses text;extract bias couple;bias couple parents;extracting pairs;method extracting pairs;pairs spouses;xcite extract bias;xcite extract;couple parents;extracting;extract bias;bias couple;couple;method extracting;couple parents end;materia xcite extract;extract;spouses;xcite;text method based;present method extracting;text method;materia extract bias;xcite uses;parents end argument;text;silla materia xcite", "pdf_keywords": "relation extraction tasks;relation extraction datasets;bias relation extraction;relation extraction;patterns relation extraction;natural language inference;family semantic parsers;biases natural language;nln interpret explanations;knowledge natural language;producing explanationguided representations;semantic parsers;underlying explanations training;natural language;existing natural language;natural language explanations;explaining natural language;construct explanations data;semantic parsers use;family semantic;explanations use bert;explanations training;using family semantic;explanations training significantly;sentence producing explanationguided;producing explanationguided;able reproduce sentences;nln interpret;language inference datasets;explanationguided representations"}, "5e51edfcef2b28594c63cce97c08752dfd438af0": {"ta_keywords": "confidence weighted learning;weighted learning structured;margin confidence weighted;soft margin confidence;multiclass confidence weighted;structured soft margin;learning structured learning;structured learning;structured learning proposed;learning structured;weighted learning;weighted learning extends;margin confidence;multiclass confidence;extends multiclass confidence;confidence weighted;margin support vector;soft margin support;graph phoneme conversion;support vector machines;phoneme conversion task;soft margin;machines allowing margin;regularization inspired soft;inspired soft margin;phoneme conversion;learning extends multiclass;vector machines allowing;vector machines;method structured soft", "pdf_keywords": ""}, "eebfece29b7a5c2202f1ec53ef49d6fdb75ce0ea": {"ta_keywords": "learning complex temporal;temporal functions sparse;teacher neuron intrinsic;leaky integrate neuron;integrate neuron;neuron intrinsic;parameters learned synaptic;neuron recover parameters;functions sparse feedback;integrate neuron recover;learned synaptic weights;teacher neuron;intrinsic parameters learned;sparse feedback signals;learned synaptic;neuron intrinsic parameters;integrate resonate neuron;neuron;learning complex;parameters teacher neuron;online learning complex;parameters learned;sparse feedback;online learning;resonate neuron;neuron recover;resonate neuron recover;synaptic weights;complex temporal functions;feedback signals", "pdf_keywords": "learn structure spike;spike approach learns;spike train neural;spiking neural;spiking neural networks;neuronal dynamics;dynamics spiking neural;including spiking neural;neural dynamics;neural network spiking;neurons trained;spiking systems neural;spike times gradientneural;dynamics neurons;neuronal network;neuron neuronal dynamics;demonstrate neural dynamics;neuronal network based;model dynamics neurons;neurons optimised;neurons optimised predict;neurons neural;networks including spiking;neuron;neuron potential spike;dynamics neurons present;neurons;neurons trained integrate;neuron used learn;oscillations neuronal network"}, "e31efa7295e5d6681607ed8ef9c45300d64227aa": {"ta_keywords": "partial information voting;approval voting scenarios;information voting;information voting multi;winner approval voting;multi winner approval;manipulation instead voters;approval voting;generally manipulate vote;voters tend prioritize;manipulate vote;voting scenarios;voting multi winner;manipulate vote obtain;voting scenarios people;winner approval;voting multi;prioritize candidates;tend prioritize candidates;vote obtain better;prioritize candidates highest;identify optimal manipulation;voting;voters tend;optimal manipulation instead;outcome identify optimal;optimal manipulation;voters;effect partial information;vote obtain", "pdf_keywords": "agent approval voting;vote optimally optimal;strategic voting behavior;strategy truthful voting;utility voting;strategic voting;utility voting truthfully;voting process agents;strategies voting;voting heuristic;voting heuristic best;vote optimally;strategies voting truthfully;voting best heuristic;truthfully vote optimally;voting strategy subjects;voting process choice;uncertainty voting;voting behavior agents;winner approval voting;voting strategy;elections uncertainty voting;based preferences voting;uncertainty voting process;alternative strategies voting;vote best heuristic;voting frugal heuristics;participants uniformly vote;approval voting best;manipulation uncertainty votes"}, "99c4007b1f6cb905788479db7fc886168f05e57c": {"ta_keywords": "speech recognition asr;robust automatic speech;automatic speech recognition;speech recognition;nonlinear recurrent deep;recognition asr based;fully nonlinear recurrent;nonlinear semilocal deep;automatic speech;proposed hybrid rnn;recognition asr;neural network rnn;deep neural network;nonlinear recurrent;networks snns proposed;neural networks snns;deep neural networks;rnn proposed hybrid;hybrid rnn;hybrid rnn rnn;semilocal deep neural;recurrent deep neural;network rnn proposed;rnn rnn based;networks snns;neural network;neural networks;fully nonlinear semilocal;nonlinear fully nonlinear;rnn based", "pdf_keywords": ""}, "c783e1fb3ce8514f981925ee590c00884660ee4e": {"ta_keywords": "multimodal documents models;generative internet;structured multimodal documents;fully generative internet;corpus structured multimodal;multimodal documents;generative internet used;generative models trained;structured multimodal;generative models;text image representations;fully generative;multimodal;generative;used fully generative;large corpus structured;family generative models;corpus structured;disambiguation world wide;trained large corpus;image representations world;fully disambiguation world;large corpus;image representations;world wide web;fully disambiguation;disambiguation world;semantically distinguish text;new family generative;corpus", "pdf_keywords": "generative modeling text;model hypertext transformers;causally masked generative;hypertext transformers learn;hypertext transformers;large corpus structured;bidirectional context control;generative modeling bidirectional;generative models trained;causally masked language;generation hyper text;reproduce unstructured text;generating semantically coherent;enabling generative;masked generative models;masked enabling generative;decoder tasks;trained large corpus;multimodal tasks;generate semantically coherent;hyper text language;generating semantically;trained novel causally;corpus structured;masked generative modeling;significantly efficient ofannotated;allows bidirectional context;bidirectional context present;tasks encoded rich;masked language modeling"}, "80b92f762e116d4513da27792822897ca3915247": {"ta_keywords": "graph preserves privacy;privacy nodes preserving;preserves privacy nodes;preserving privacy;privacy nodes;guarantees strong privacy;preserves privacy;strong privacy;method preserving privacy;preserving privacy context;strong privacy demonstrate;privacy context discrete;privacy;intrinsic clustering;privacy demonstrate approach;intrinsic clustering underlying;based intrinsic clustering;clustering underlying graph;privacy demonstrate;privacy context;preserving edges;nodes preserving edges;edges method robust;nodes preserving;preserving edges method;robust noise guarantees;clustering underlying;underlying graph preserves;graph preserves;clustering", "pdf_keywords": "privacy graph convolutional;privacy preserving graph;graphs large privacy;privacy large graphs;preserve privacy graph;privacy graph;graph based deep;private training graph;graph neural;graph convolutional networks;graph neural network;learn graph representation;graph training;train graph neural;graph convolutional;graphs presence deep;results privacy preserving;privacy preserving gcns;discrete graph training;graph networks;output individual privacy;graph partitioning deep;training results privacy;graph representation query;large privacy preserved;deep learning natural;privacy preserving;train graph networks;privacy individual nodes;algorithm preserves privacy"}, "3d5b51fc30ffacdcc8424618555accb36756ccc9": {"ta_keywords": "stochastic points method;points stochastic points;stochastic points;points stochastic;novel stochastic points;stochastic points stochastic;minimization problem smooth;method convex nonconvex;unconstrained minimization;consider unconstrained minimization;xmath0 points design;points method;xmath0 points;unconstrained minimization problem;method convex;stepsize selection schemes;minimization;analyze method convex;points method analyze;space xmath0 points;convex nonconvex;points design;convex nonconvex cases;minimization problem;iteration complexity;analyze iteration complexity;points design novel;analyze iteration;selection schemes;method analyze iteration", "pdf_keywords": "stochastic point method;methods stochastic point;stochastic points method;stochastic optimization;solving stochastic optimization;algorithm stochastic points;stochastic direct search;search methods stochastic;method stochastic direct;approach stochastic descent;novel method stochastic;method solving stochastic;method stochastic;stochastic descent;stochastic optimization problem;methods stochastic;stochastic derivative objective;stochastic variant method;propose stochastic points;novel approach stochastic;method random search;method based stochastic;stochastic point theorems;based stochastic points;free algorithm stochastic;stochastic point;random search method;point theorems stochastic;stochastic points;algorithm stochastic"}, "845aad7b99f48526fe003c775836091521624471": {"ta_keywords": "russian lexicographic network;week russian lexicographic;word week russian;russian lexicographic;predicting word;predicting word week;lexicographic network;lexicographic network model;model predicting word;lexicographic model;week russian;lexicographic model slava;celebrated lexicographic model;lexicographic;word week;slava bilinear classification;based celebrated lexicographic;celebrated lexicographic;bilinear classification;bilinear classification problem;slava bilinear;weighted average recall;model slava bilinear;week articles;classification;data week articles;russian;word;predicting;average recall", "pdf_keywords": ""}, "a3cd9c4f8fa52c5e23885c2f82931d7e0f7d4b45": {"ta_keywords": "checking dispensing powder;barcode symbology consisting;barcode symbology employs;barcode symbology;dispensing data symbology;dimensional barcode symbology;linear barcode symbology;checking dispensing;using dimensional barcode;dispensing powder;new checking dispensing;dimensional barcode;dispensing powder using;store dispensing;used store dispensing;dispensing data;employs linear barcode;barcode;linear barcode;symbology used store;powder using dimensional;data symbology;data symbology used;store dispensing data;generic dose symbology;dose symbology used;dose symbology;powder using;number used items;dispensing", "pdf_keywords": ""}, "697e6eecb0e77ba56c685bb99b221d959739d13b": {"ta_keywords": "geo tagging images;automatic geo tagging;geo tagging;tagging images approach;tagging images;tags automatic geo;latent dirichlet allocation;tagging;model user tags;user tags;dirichlet allocation;user tags automatic;user tags implemented;tags;dirichlet allocation ld;dataset user tags;flickr archive comparing;tags automatic;images flickr;images flickr archive;using images flickr;tags implemented approach;latent dirichlet;flickr archive;tags implemented;flickr;city using images;novel latent dirichlet;ld models;ld models city", "pdf_keywords": ""}, "e42b3ead5ff04adfa95c87e0180561f0c3ba4af4": {"ta_keywords": "algorithm learns convex;learns convex;learns convex combination;reinforcement learning systems;vanilla reinforcement learning;approach reinforcement learning;reinforcement learning variety;reinforcement learning;learning systems hard;proposed algorithm learns;state action constraints;algorithm learns;action constraints proposed;new approach reinforcement;hard state action;action constraints;outperforms vanilla reinforcement;convex combination weights;points convex set;vanilla reinforcement;approach reinforcement;benchmark control tasks;reinforcement;represented convex combination;learning systems;benchmark control;points convex;represented convex;convex set represented;convex set", "pdf_keywords": "constraints policy network;vertex policy network;constraints represented convex;algorithm learns convex;learned control policies;learns convex;learns convex combination;constraints network;constraints policy leveraging;novel vertex policy;safety propose convex;learning learned control;safe reinforcement learning;geometry constraints network;vertex policy;action state constraints;constraints action state;affine control;model predictive control;affine control systems;safety constraints policy;policy leveraging geometric;convex optimization;leverage convex combination;learned control;constraints action;policy meets constraints;convex polytope;constraints network architecture;constraints policy"}, "e54a4e49917eb3da18c2f239be70a68fbd3274c3": {"ta_keywords": "technical debt peer;debt peer review;replication scientific software;technical debt reported;technical debt;documentation scientific packages;review documentation scientific;scientific software discussed;documentation scientific;debt reported reviewers;replication scientific;scientific software;type technical debt;peer review documentation;study replication scientific;scientific packages;findings study replication;rsci studied prevalent;debt peer;rsci studied;study replication;software discussed;peer review;debt reported;rsci;debt;review documentation;package relevance findings;replication;software", "pdf_keywords": "debt peer review;documentation peer review;scientific software findings;debts peer review;peer review documentation;scientific software researchers;documentation debt study;debt documentation participants;technical debts peer;documentation types debt;technical debt documentation;documentation peer;peer review scientific;technical debt software;taxonomy technical debt;scientific software cited;technical debt ubiquitous;reveal documentation debt;documentation debt;documentation debt recurrenttechnical;software researchers;debt tt scientific;documentation requirements debt;software findings;debt types peer;debt software projects;debt ubiquitous software;software cited literature;analyze technical debts;data technical debt"}, "59d225fcb08ce66935e0285a9936ee158c4fdb97": {"ta_keywords": "questions entailment trees;entailment trees generating;entailment trees;multistep entailment trees;explain questions entailment;entailment trees strong;trees multistep entailment;entailment steps;entailment steps facts;questions entailment;explanations form trees;trees generating explanations;multistep entailment steps;generating explanations;generating explanations form;multistep entailment;entailment;create explanations;create explanations form;trees strong language;approach explain questions;skill create explanations;explanations form;explain questions;strong language model;language model;language model partially;explanations;conclusions hypothesis approach;hypothesis approach train", "pdf_keywords": "entailment trees generating;entailment trees generate;questions entailment trees;entailment trees tasks;entailment tree demonstrate;entailment trees corpus;entailment trees;entailment trees context;entailment tree;explain questions entailment;step entailment trees;generating explanations unstructured;entailment trees diverse;entailment tree relies;trained entailment tree;entailment tree method;explanations form entailment;task entailment trees;approach generating explanations;generative reasoning;discrete entailment trees;form entailment trees;given entailment tree;generative model entailment;entailment trees facts;approach generate explanations;generating explanations;called entailment trees;generate deep explanation;entailment steps"}, "deedb9b61a01d686b28e6034770fccc142e77fab": {"ta_keywords": "performance natural language;predictions unseen languages;language processing experiment;predicting performance natural;natural language processing;predicting performance;model predicting performance;experimental task input;language processing;natural language;produce meaningful predictions;predictive model predicting;predictive;meaningful predictions unseen;task input model;predicting;model predicting;predictions unseen;experts outline predictor;meaningful predictions;experimental task evaluated;predictive model;predictions;tasks experimental task;tasks experimental;input model trained;trained set experimental;unseen languages;human experts outline;language", "pdf_keywords": "translation task predict;performance natural language;performance new nlp;performance machine translation;language processing tasks;machine translation task;model natural language;language model perform;processing nlp task;universal dependency parsing;dependency parsing task;machine translation;machine translation machine;machine perform translation;language processing nlp;task predict performance;nlp task;natural language processing;target language model;new nlp natural;nlp task method;processing nlp;translation machine perform;task predict;tasks able predict;dependency parsing;based machine translation;translation task high;machine learning tasks;language processing"}, "4cfbd97a5b42695697f70a9f28ee29711f6ca433": {"ta_keywords": "making trustworthy predictions;trustworthy predictions method;trustworthy predictions;features adversarial;features adversarial attacks;similar features adversarial;adversarial attacks training;adversarial;adversarial attacks;novel inputs autonomous;datasets driving scenarios;real world driving;learned prediction;detect novel scenarios;inputs autonomous systems;identifying novel inputs;inputs autonomous;driving datasets driving;datasets driving;driving datasets;driving scenarios house;attacks training dataset;driving dataset similar;world driving datasets;driving dataset;similar driving dataset;predictions method leverages;predictions method;information learned prediction;novel inputs", "pdf_keywords": ""}, "10e88416035a8a3cbef0e65f8967df650abd0a00": {"ta_keywords": "word sense disambiguation;sense disambiguation named;sense disambiguation;unsupervised word sense;disambiguation named;disambiguation;word sense;unsupervised word;novel unsupervised word;novel unsupervised;unsupervised;word;present novel unsupervised;sense;named;novel;present novel;present", "pdf_keywords": "sense disambiguation unsupervised;word sense disambiguation;disambiguation sentences language;supervised word sense;disambiguate word method;disambiguation unsupervised;implements disambiguate word;sense disambiguation;sense disambiguation context;disambiguation parameterized word;unsupervised word sense;disambiguate word;disambiguation sentences;context based disambiguated;disambiguation unsupervised does;sense disambiguation called;sense disambiguation parameterized;disambiguation context;disambiguation;disambiguation context world;disambiguation called;based disambiguated method;parameterized word sense;semantic similarity;method disambiguation sentences;disambiguate;trained semantic similarity;trained semantic;disambiguation parameterized;perform word sense"}, "4fffa5245d3972077c83614c2a08a47cb578631e": {"ta_keywords": "approach speech representation;speech representation learning;speech representation;supervised approach speech;acoustic language model;unit bert approach;novel self supervised;combined acoustic language;bert approach;self supervised;bert approach combines;self supervised approach;hidden unit bert;acoustic language;unit bert;learn combined acoustic;representation learning combines;language model continuous;language model;representation learning;supervised;approach speech;bert;accuracy clustering;combines accuracy clustering;predict input;ability predict input;accuracy clustering step;clustering;supervised approach", "pdf_keywords": "self training speech;supervised learning speech;supervised approaches speech;self supervised learning;training speech recognition;learning speech representation;predictions speech audio;supervised selftraining;training discrete speech;self supervised approaches;novel self supervised;supervised selftraining methods;unsupervised learning speech;self supervised;representation learning speech;training speech;learning speech;learning speech recognition;speech representation learning;method self supervised;based self labeling;speech audio;learning speech systems;approaches speech audio;speech audio challenging;trained standard speech;predictions speech;self labeling;speech representation;step self supervised"}, "520e82c0f35a14ecf78b93de3673bb8b2a3212fc": {"ta_keywords": "extraction timeline noisy;extraction timeline;timeline noisy training;timeline task distantly;2015 timeline task;timeline task;approach extraction timeline;timeline noisy;produce timeline experiments;produce timeline;timeline experiments;task distantly supervised;timeline;models produce timeline;events temporal expressions;semeval 2015 timeline;timeline experiments semeval;propose distantly supervised;distantly supervised;distantly supervised approach;temporal expressions combine;temporal expressions;events temporal;2015 timeline;temporal;anchor events temporal;noisy training data;supervised approach extraction;training data documents;supervised", "pdf_keywords": ""}, "0115d5d37f7cdc7b8d2147c0bb348e714432e899": {"ta_keywords": "channel speech enhancement;speech enhancement sse;speech enhancement;single channel speech;telephone audio;improvement language identification;language identification systems;channel speech;signal telephone audio;telephone audio domain;performance telephone audio;language identification proposed;language identification;shortterm memory rl;memory rl neural;long shortterm memory;identification performance telephone;shortterm memory;audio domain experimental;neural network enhance;enhancement sse language;audio domain;noisy signal telephone;audio;sse language identification;rl neural;rl neural network;significant improvement language;domain adaptation;signal telephone", "pdf_keywords": ""}, "cc2c3df6b09166c54e670d347bfe26dae236ac73": {"ta_keywords": "ofsupervised learning dimensional;learning dimensional 2d;problem ofsupervised learning;dimensional 2d systems;ofsupervised learning;2d systems global;learning dimensional;2d systems;dimensional 2d;2d;learning;dimensional;global aspect problem;solving problem ofsupervised;aspect related global;problem ofsupervised;systems global aspect;global aspects problem;global aspect;aspect problem;systems global;local aspect related;problem related global;local aspect;aspect problem related;problem argue global;problem local aspect;argue global aspect;aspects problem local;ofsupervised", "pdf_keywords": ""}, "f7979c6690562c5f8bf700e3fd184c4d1df0a54c": {"ta_keywords": "neural entity linking;lingual entity linking;entity linking models;improves entity linking;entity linking accuracy;entity linking;cross lingual entity;entity linking framework;lingual entity;low resource languages;linking models;low resource language;neural entity;linking accuracy;high resource language;linking models transferred;resource language train;resource languages transfer;language zero shot;level neural entity;linking accuracy 17;linking;resource languages;cross lingual;languages transfer;resource language;novel cross lingual;information high resource;language train character;language train", "pdf_keywords": "lingual entity linking;neural entity linking;crosslingual knowledge linking;lingual entity link;entity link monolingual;cross lingual entity;shot entity linking;lingual entity;disambiguation using knowledge;entity mention multilingual;entity mentions crosslingual;entity linking;link monolingual;entity linking uses;entity linking el;neural machine translation;link monolingual low;languages support linking;linking assume bilingual;feature crosslingual tagging;linking arbitrary languages;crosslingual tagging;mentions crosslingual text;bilingual lexical resources;neural entity;knowledge linking improves;knowledge linking;knowledge linking wikipedia;shot neural entity;lexical resources bridge"}, "3b0a1a10d8f7496226635c5c3b8475fcd10d890d": {"ta_keywords": "performance redundant requests;redundant requests distributed;requests distributed storage;redundancy requests optimal;optimal redundant requestsing;having redundancy requests;redundancy requests;redundant requestsing;latency performance redundant;sending redundant requests;redundant requests;distributed storage systems;redundant requestsing policies;distributed storage;memoryless service times;redundant requests help;requests distributed;service times memoryless;designing optimal redundant;instantaneous having redundancy;times memoryless service;performance redundant;storage systems;optimal redundant;latency performance;sending redundant;storage systems primary;memoryless service;requests optimal;redundancy", "pdf_keywords": ""}, "f826381aea632791b6007e427a9587c11b239b6a": {"ta_keywords": "dialogue tasks deep;exploring dialogue tasks;dialogue tasks;dialogue tasks demonstrate;exploring dialogue;strategies dialogue tasks;method exploring dialogue;xmath2pi networks greatly;xmath2pi networks;xmath0dy neural network;tasks deep process;efficiency xmath2pi networks;xmath0dy neural;dialogue;tasks deep;exploration efficiency xmath2pi;strategies dialogue;xmath3pi network;based xmath3pi network;xmath1pi network;structure xmath0dy neural;structure xmath1pi network;xmath1pi network discover;based xmath3pi;deep process;novel strategies dialogue;xmath3pi;xmath2pi;xmath1pi;tasks demonstrate exploration", "pdf_keywords": ""}, "f07a326e21395f025a87b2d77cac7e8ca502f002": {"ta_keywords": "featureless multimodal inference;featureless featureless multimodal;featureless multimodal;dimensional featureless featureless;multimodal inference;dimensional featureless;high dimensional featureless;multimodal inference problem;featureless featureless;featureless featureless featureless;proposed inference framework;inference framework;featureless;inference framework based;structure medical domain;medical domain;inference;proposed inference;inference problem proposed;problem proposed inference;knowledge domain;medical domain use;characterizing structure medical;inference problem;use knowledge domain;based representation domain;multimodal;knowledge domain perform;representation domain;data problem knowledge", "pdf_keywords": "predicting label sentences;prediction label sentences;nli models learn;deep neural;learning language model;language understanding models;task learning language;dnn learn linguistic;multi task learning;task learning;train deep neural;deep neural models;predict entailment;knowledge data augmentation;large corpus;large corpus develop;models learn entailment;predict entailment model;language model;deep learning;nli models;language model pre;using deep;label sentences;learning modelin;incorporating domain knowledge;domain knowledge;approach based deep;entailment medical domain;novel learning modelin"}, "d95aafa571e9cb6795cc28ecf257ead123664e3c": {"ta_keywords": "mrf segmentation potentials;common regularization energies;segmentation potentials standard;mrf segmentation;segmentation potentials;like mrf segmentation;standard pairwise clustering;regularization energies;common regularization;regularization energies like;regularization;combining common regularization;techniques integrated regularization;integrated regularization;pairwise clustering;integrated regularization functionals;segmentation;pairwise clustering criteria;segmentation model;regularization functionals existing;regularization functionals;segmentation model combining;new segmentation model;new segmentation;propose new segmentation;clustering;clustering criteria;bound optimization techniques;potentials standard pairwise;clustering criteria like", "pdf_keywords": "clustering image segmentation;kernel clustering spectral;segmentation image clustering;image segmentation clustering;clustering image segmentation_;kernel clustering;dimensional image clustering;clustering spectral;image clustering;segmentation clustering model;optimal segmentation;kernels clustering;clustering image;segmentation clustering;regularization clustering;image clustering approach;clustering regularization models;space kernel clustering;segmentation images particular;clustering regularization;energy clustering;regularization clustering clustering;image segmentation;optimal segmentation achieved;segmentation images;continuous regularization clustering;clustering spectral bound;clustering clustering energy;segmentation;segmentation functionals"}, "250e4a8f5155f1f9f60b2dee3e8da8024338db4d": {"ta_keywords": "sentiment target clustering;achieved clustering document;clustering document;clustering document global;sense clustering document;unsupervised clustering;similarity document sentiment;classification document documents;unsupervised clustering clustering;means unsupervised clustering;document sentiment target;clustering based global;global sense clustering;sense clustering;sense clustering achieved;documents means unsupervised;clustering achieved;clustering;document sentiment;achieved clustering;clustering clustering;improve classification document;clustering achieved clustering;global similarity document;clustering based;classification document;target clustering;target clustering achieved;clustering clustering based;based global similarity", "pdf_keywords": ""}, "8eda71ecad19cdef6092e76276eba48312ec7063": {"ta_keywords": "dynamics dense data;dense data;discrete time models;discretize discrete representations;discrete representations document;representations document query;dense data discretize;document query encoders;query encoders analyze;discrete representations;query encoders experiments;description dynamics dense;models description dynamics;data discretize discrete;query encoders;time models;representations suggest models;time models description;dynamics dense;discrete time;proposed discrete time;encoders analyze;description dynamics;discretize discrete;document query;representations document;recently proposed discrete;data discretize;output document query;analyze output document", "pdf_keywords": "document query representations;dense retrieval models;query document representation;interpret dense retrieval;query representations;query representations analyze;dense retrieval based;retrieval models present;retrieval models;dense retrieval;topics deep learning;query document encoders;encoders document query;dense embeddings interpret;passage ranking task;document representation;passage ranking;embeddings interpret dense;document representation method;words deep learning;retrieval based discrete;matrix natural language;natural language processing;deep learning di;consider passage ranking;embeddings interpret;output dense embeddings;retrieval based;discrete query document;captures discrete query"}, "e0c66240239263f16159eef166a391d3939ae2d5": {"ta_keywords": "sequential reading analysis;models sequential reading;sequential reading;shortest passage;sbc shortest passage;passage sbc benchmarks;reading analysis;shortest passage sbc;benchmarks including shortest;reading analysis bab;models sequential;analysis bab shortest;sequential;including shortest path;bab shortest path;shortest path sbc;path sbc shortest;reading;shortest path;benchmarks;benchmarks including;including shortest;bab shortest;benchmarks despite;sbc benchmarks;sbc shortest;passage sbc;sbc benchmarks despite;shortest;study performance variety", "pdf_keywords": "question answering tasks;question answering best;models reading comprehension;question answering;reading comprehension tasks;answering question answering;attention readers randomization;named entity answering;question attention layer;entity answering;answering best models;reading comprehension dataset;entity answering question;comprehension tasks;attention readers;gated attention readers;reading comprehension finding;scale reading comprehension;proposed reading comprehension;reading comprehension;answering tasks;comprehension tasks named;question attention;achieve comprehension task;models proposed reading;machine reading evaluation;answering question;machine reading;readers randomization;large scale reading"}, "3105b5863d4597058bf51aeda40db53394075784": {"ta_keywords": "ultracold atoms tournaments;generation ultracold atoms;atoms tournaments method;ultracold atoms;atoms tournaments;level xmath0 atoms;level accuracy atoms;accuracy atoms target;generation ultracold;atoms target method;applied generation ultracold;accuracy atoms;xmath0 atoms;atoms bound level;method generation ultracold;xmath0 atoms xmath1;number atoms target;atoms target;atoms xmath1;atoms xmath1 number;xmath1 number atoms;atoms;atoms bound;number atoms;ultracold;based assumption atoms;assumption atoms bound;level xmath0;assumption atoms;tournaments method", "pdf_keywords": ""}, "446efa0bcf3528b51332a12495cb56784dd8bad3": {"ta_keywords": "transfer learning;transferable different embeddings;transfer learning framework;embeddings graphs trained;embeddings learned graphs;embeddings learned;novel transfer learning;different embeddings learned;unsupervised learning;graphs trained including;unsupervised learning lvo;graphs trained;learning framework unsupervised;framework unsupervised learning;embeddings graphs;learned graphs;features different embeddings;graphs generic transferable;different embeddings graphs;learning ability transfer;unsupervised learning ability;transfer unary features;advantages unsupervised learning;learned graphs generic;embeddings;embeddings task;different embeddings;embeddings task specific;including glove embeddings;embeddings elmo embeddings", "pdf_keywords": "embeddings graphs trained;learning graphs encode;features graphs trained;graphs trained including;dependencies learned graphs;tasks learned graphs;graphs trained;unsupervised learning graphs;learning universal graph;latent graphs transfer;learned graph;embeddings graphs;learning graphs;learned graphs;learning latent relational;deep transfer learning;relational graphs capture;learning generic graphs;deep transfer;learned graphs generic;graphs encode information;graphs transfer network;learned graphs propose;latent relational graphs;graphs trained able;graph predictor encodes;inputs learned graph;feature transfer learning;graphs encode;novel deep transfer"}, "549dae68d04eefad88885c64a4d946205e524b79": {"ta_keywords": "document embedding representations;vectors document embedding;document embedding;document clustering sentiment;document clustering;embedding;embedding representations;clustering sentiment;embedding representations stable;clustering sentiment classification;including document clustering;isometrically invariant mappings;vectors document;sentiment classification;analysis document data;document data;isometrically invariant;representations;classification;isometrically;class isometrically invariant;representations stable;document data evaluate;clustering;vectors;invariant mappings;document;set vectors document;mappings set vectors;class isometrically", "pdf_keywords": "embedding patterns;topological representations text;generate embedding patterns;embedding patterns set;patterns generate embedding;topological representations;metric space representations;documents text classification;embedding patterns subset;binary sentiment classification;clustering documents;representations text documents;utility topological representations;text classification;persistence diagrams compact;based concept similarity;documents embedded stochastic;text classification tasks;clustering documents based;persistence diagrams patterns;similarity underlying structures;sentiment classification;representations sentences documents;representations range text;distributed representations sentences;space representations used;predicting clustering documents;space representations;sentences documents embedded;text datasets"}, "35c710f5fdacc71a675832f6beaa2dbfe301d0ce": {"ta_keywords": "example based dialog;based dialog systems;active learning;dialog systems;active learning framework;based dialog;dialog systems framework;presents active learning;dialog;strategies selecting inputs;selecting inputs;alternatives random selection;selected inputs proposed;selecting inputs selected;selected inputs;based uncertainty sampling;sampling strategies selecting;domains complexity utterances;inputs selected;uncertainty sampling strategies;inputs selected inputs;learning framework;random selection;random selection domains;selecting;selection;complexity utterances;uncertainty sampling;strategies selecting;learning framework construction", "pdf_keywords": ""}, "25efc17ba82ba4af29f2e03868de74e1ea66d025": {"ta_keywords": "multilingual text video;learns contextual multilingual;text video search;contextual multilingual embeddings;models video search;instructional video dataset;learns contextual;video search;multilingual embeddings empirically;video search propose;multilingual embeddings;model learns contextual;video search non;training vision models;text video;embeddings empirically demonstrate;contextual multilingual;video dataset;embeddings empirically;video dataset multi;embeddings;models video;existing models video;learns;model learns;non english search;multilingual text;training vision;search non english;new instructional video", "pdf_keywords": "multilingual multimodal representations;multilingual multimodal video;multilingual video training;learns contextualize multimodal;multilingual models video;learns contextual multilingual;contextual multilingual multimodal;multilingual text video;model multilingual multimodal;contextualize multimodal embeddings;multilingual representation learning;multilingual multimodal;model multilingual video;multilingual video;multilingual multimodal pre;multilingual features video;multilingual multimodal transformers;trained model multilingual;multilingual instructional videos;video extract multilingual;multimodal embeddings;multimodal embeddings model;use multilingual multimodal;multimodal representations videos;multilingual videowe;multimodal pre training;multilingual videowe propose;contextualize multimodal;lingual transfer video;multilingual representations"}, "e60b88313fad52c1ef8dd02b482785651d09ad66": {"ta_keywords": "separating neural networks;weights separation neural;bounded weights separation;separation sub networks;separation neural networks;weights separation;bounded weights networks;separation neural network;weights networks bounded;separating neural;sub networks separation;method separating neural;networks bounded weights;networks separation sub;networks separation;based separation neural;problem separating neural;weights networks;separation neural;networks poly bounded;poly bounded weights;neural networks poly;neural network sub;networks bounded;bounded weights method;bounded weights;method based separation;sub networks networks;sub networks method;networks sub networks", "pdf_keywords": "neuorons approximated network;size depth networks;hidden neuorons approximated;theorems approximation polynomials;polynomial number neurons;depth underlying polynomials;size depth neural;neuron prove theorems;depth networks poly;approximated network;depth neural networks;networks poly bounded;neuorons approximated;approximated network depth;theorems approximation;neural networks exponentially;hidden neuron;approximated network small;hidden neuron prove;hidden neuorons;approximation polynomials;depth networks;depth neural;networks exponentially bounded;cell hidden neuron;number hidden neuorons;prove theorems approximation;bounded weights approximated;network small depth;weights approximated poly"}, "a1c5af2a531c64f1c06e806d7986cd878ec3c33a": {"ta_keywords": "explainers data model;evaluate popular explainers;choose explainer study;explainer study proposes;model fraud analysts;explainer study;explainers data;choose explainer;popular explainers data;analysts popular explainers;evaluation methodology xai;explainers;model fraud;explainers worse;fraud analysts;popular explainers;explainer;application grounded evaluation;methodology xai;methodology xai test;popular explainers worse;fraud analysts popular;grounded evaluation methodology;grounded evaluation;deployed model fraud;fraud detection;xai test;xai test real;fraud detection task;real world fraud", "pdf_keywords": "methodology explainable ai;explainable ai methods;evaluating explainers;evaluating explainers efficient;method evaluating explainers;explanations utility;evaluating explainers demonstrate;explanations performance;interpretability explainers machine;explainable ai;xai explainability explainabilityunderstanding;explanations utility human;evaluate explainability methods;explanations application;explanations application grounded;evaluate explainability;evaluating explainers distributed;approach evaluate explainability;compare explanations utility;explainer end users;approach interpretability explainers;showing explanations application;explanations performance analysts;interpretability explainers;explainers machine learning;assess compare explanations;called xai explainability;generalize explainers structured;hoc explanations application;showing explanations"}, "4fa4e39ade763085a75146392b997b7d4da49725": {"ta_keywords": "weak supervision text;supervision text classification;using weak supervision;contextualize text proposed;framework weak supervision;terms contextualize text;contextualize text;classifier contextualize;train classifier contextualize;weak supervision user;text classification;terms contextualize;supervision text;text using weak;text classification leverages;user seed words;weak supervision;contextualize;classifier contextualize existing;supervision user seed;provided terms contextualize;contextualize existing;text proposed framework;characterizing text;contextualize existing datasets;characterizing text using;seed words;seed words resulting;supervision user;classification", "pdf_keywords": "text classification contextualize;contextualized weak supervision;contextualize word occurrences;words contextualized;contextualize corpus;contextualize corpus model;text classification contextualized;contextualize weakly supervised;contextualize interpretation words;documents context supervised;corpus contextualize;words contextualized document;contextualized corpus;classification contextualize;contextualize word;rank words contextualized;classification contextualized corpus;context supervised;classification contextualized;corpus contextualize interpretation;supervision text classification;weak supervision text;contextualized highly label;finegrained text classification;framework contextualize word;weakly supervised document;supervision fully contextualized;predicting meaning documents;representations contextualize corpus;contextualize plain corpus"}, "462e36e5e296900c80dcd36173340f9c29e36c80": {"ta_keywords": "hmm based probabilistic;sst hmm based;sharedstate triphone hmm;structure sst hmm;triphone hmm sst;triphone hmm;constructing sharedstate triphone;hmm based;sharedstate triphone;probabilistic probabilistic criterion;probabilistic probabilistic;based probabilistic probabilistic;sst hmm;probabilistic criterion criterion;probabilistic criterion;based probabilistic;triphone;probabilistic;hmm sst hmm;tree structure sst;valid determining tree;hmm sst;determining tree structure;determining tree;method constructing sharedstate;constructing sharedstate;structure sst;tree structure;criterion based;criterion criterion based", "pdf_keywords": ""}, "f05741b65a1d644f2fae4c654dae315a7451ee85": {"ta_keywords": "heterogeneous topic web;text network exploration;topic atlas;named topic atlas;topic web quantified;topic atlas exhibit;topic web;exhibit heterogeneous topic;heterogeneous topic;text networks;relationships heterogeneous topic;life text networks;topic web demonstrate;propose probabilistic generative;probabilistic generative model;model text links;text links;generative model text;text network;probabilistic generative;paper propose probabilistic;named topic;network exploration extensive;network exploration;task text network;propose probabilistic;generative model;text links different;probabilistic;links different relationships", "pdf_keywords": "topic atlas probabilistic;heterogeneous topic web;heterogeneous web topics;topic modeling;explore heterogeneous topic;topics heterogeneous;topic model;text network exploration;topic model generative;topic models;topic atlas;topics heterogeneous relationships;document networks;topics large document;propose topic atlas;topic models structured;topic atlas able;collections topic modeling;topics topic models;topic web homogeneous;standard topic models;driven topic model;based topic model;topic models unified;topic models wewe;topic atlas prototype;reproduce topic relationships;topic models results;topic modeling able;develop topic atlas"}, "fe54832083f65eade8e2847627d330a24df22488": {"ta_keywords": "multi channel electroencephalogram;channel electroencephalogram;channel electroencephalogram eweg;electroencephalogram;electroencephalogram eweg proposed;electroencephalogram eweg;spectral transform stft;covariance matrices frequency;background noise single;short time spectral;removing background noise;noise single trial;time spectral transform;recorded multi channel;multi channel signal;potentials recorded multi;spectral transform;related potentials recorded;event related potentials;pseudo erp data;noise single;potentials recorded;time spectral;background noise;using pseudo erp;channel signal;transform stft;transform stft representing;matrices frequency bins;multi channel", "pdf_keywords": ""}, "d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5": {"ta_keywords": "models text generation;text generation structural;text generation;challenges text generation;text generation discuss;generated text;pretrained language models;language models text;existing pretrained language;survey pretrained language;models text;language models;architectures pretrained language;generation structural information;generated text conclude;pretrained language;language models model;properties generated text;generation structural;generated;text;generation discuss challenges;generation;architectures pretrained;challenges text;generation discuss;generation discuss adapt;language;existing pretrained;survey pretrained", "pdf_keywords": "text generation pretrained;text generation models;models text generation;text generation tasks;natural language generate;generation natural language;text generation task;challenges text generation;text generation;pretrained models text;text generation unstructured;neural generation models;text generation discuss;pretrained language models;formulate text generation;text generation advance;input text generation;definition text generation;neural generation;text generation developments;text generation mainly;strategies text generation;robust text generation;solutions text generation;text generation remain;generated text finetuning;text generation class;text generation preserving;language models text;models machine translation"}, "a2221b03211408ac2db0559b9a54c1d72b5f560c": {"ta_keywords": "music annotation tasks;music annotation;downstream music annotation;genre classification auto;classification auto tagging;music genre classification;self supervised learning;approach named musicoder;genre classification;auto tagging;self supervised;new self supervised;tagging;supervised learning approach;classification auto;auto tagging tasks;named musicoder;named musicoder performance;annotation;supervised learning;learning approach named;models music genre;annotation tasks;music genre;supervised;musicoder;art models music;models music;evaluated downstream music;classification", "pdf_keywords": "music annotation tasks;music acoustic encoder;downstream music annotation;music auto tagging;classification music auto;music annotation;music auto taggingin;encoder named musicoder;music acoustic data;acoustic encoder;able predict music;acoustic encoders network;acoustic music representation;predict music;method predict music;pre train encoder;music representation;acoustic encoder named;acoustic model training;acoustic frames encoder;classification music;code input music;acoustic encoders;network acoustic encoders;training encoder;unlabeled music;training encoder deep;unlabeled music acoustic;training continuous acoustic;encoder deep"}, "74e9053d6f44f4507bd40bbea999ee65f0cbefb2": {"ta_keywords": "predict words nonsensical;able predict words;predicting words speech;speech called predicting;predicting words;predict words;called predicting words;predicting words given;method predicting words;words given prediction;statements able predict;features nonsensical statements;words nonsensical demonstrate;words nonsensical;nonsensical statements able;words speech called;words speech;nonsensical statements;idea word function;called predicting;word function;speech;features nonsensical;word function input;predicting;able predict;prediction;speech called;new method predicting;predict", "pdf_keywords": "predictions natural language;natural language classification;language classification tasks;predict nonsensical examples;examples make predictions;interpret text classification;natural language processing;accurate predictions natural;language classification;predictions deep neural;classification tasks reduce;natural language;making natural language;classification tasks;predictions deep;composition predictions deep;nlp interpretation methods;trained make predictions;neural models robustness;processing nlp interpretation;text classification;input examples highly;neural models widely;predict answer question;interpretation natural language;neural models trained;predicting outcome neural;language sentences approaches;text classification decision;natural language sentences"}, "01a21d74fb7414404851872f23cdca42243ab6a8": {"ta_keywords": "batch related convolutional;mini batch training;batch training;dataset global information;collects dataset global;batch training introduce;dataset global;batch related;batch;progressively collects dataset;called batch related;mini batch;structure called batch;dataset;tuning perspective aggregation;called batch;using mini batch;collects dataset;convolutional cell;utilization dataset global;aggregation utilization dataset;related convolutional cell;model fine tuning;convolutional cell bconv;utilization dataset;global information latent;perspective aggregation utilization;perspective aggregation;related convolutional;convolutional", "pdf_keywords": "batch related convolutional;batchrelated convolutional cellwe;iterations person identification;batchrelated convolutional;learning based batch;identification based batch;called batchrelated convolutional;mini batch training;person identification model;batch training;related convolutional cell;related convolutional cells;convolutional cellwe propose;convolutional cells;convolutional cell;convolutional cellwe;person identification crucial;batch iterations person;new deep learning;classification task surveillance;related convolutional;bias mini batch;transfer learning;batch training introduce;model based deep;convolutional cell cell;person identification problem;deep learning;model person reid;learning layers person"}, "c2dd1c332f65fea3a66f4a982428f31ce1a9dc70": {"ta_keywords": "deductive databases;queries integrate information;structured information sources;deductive databases ranked;databases ranked retrieval;common database representation;style deductive databases;allows queries integrate;database representation;knowledge integration;sources common database;integration structured information;database representation method;information multiple web;knowledge integration allows;queries integrate;common database;method knowledge integration;database;information retrieval;databases;ranked retrieval methods;databases ranked;retrieval methods information;retrieval method allows;allows queries;ranked retrieval;information retrieval method;structured information;method allows queries", "pdf_keywords": ""}, "eca07d2b351d81719b33c913a87c63d6930ee7f5": {"ta_keywords": "einstein condensate xmath0;bose einstein condensate;einstein condensate;component bose einstein;condensate xmath0 xmath1;condensate xmath0;bose einstein;effect component bose;xmath2 xmath3 xmath4;component bose;xmath1 xmath2 xmath3;xmath12 xmath13 xmath14;xmath1 xmath2;xmath2 xmath3;xmath7 xmath8 xmath9;xmath8 xmath9 xmath10;xmath13 xmath14;xmath3 xmath4;xmath3 xmath4 xmath5;xmath6 xmath7 xmath8;xmath4 xmath5 xmath6;xmath13 xmath14 xmath15;xmath0 xmath1 xmath2;xmath5 xmath6 xmath7;xmath12 xmath13;xmath11 xmath12 xmath13;xmath2;xmath14 xmath15 xmath16;xmath6 xmath7;xmath14", "pdf_keywords": ""}, "3a95fab610d5ff49fbdb7a4d8760b02c51df0013": {"ta_keywords": "anonymizing clinical notes;paradigm anonymizing clinical;paradigms anonymizing clinical;anonymizing clinical;privacy paradigm anonymizing;anonymizing;new privacy;private health information;introduce new privacy;new privacy paradigm;sensitive data family;secure paradigms anonymizing;paradigm anonymizing;privacy;health information secured;paradigms anonymizing;notes guarantees private;sensitive data;clinical notes guarantees;privacy paradigm;including sensitive data;clinical notes;search secure;used search secure;health information;information secured including;search secure paradigms;information secured;data family history;clinical notes demonstrate", "pdf_keywords": "anonymization clinical notes;embeddings anonymized clinical;word embeddings anonymized;word anonymized embedding;anonymized clinical data;anonymized embedding achieved;obfuscationize text preserving;embeddings anonymized;notes approach obfuscation;anonymized embedding;anonymized embedding method;obfuscationize text;obfuscation text;able obfuscationize text;obfuscation text observed;techniques anonymization clinical;anonymization technique clinical;novel techniques anonymization;anonymized data;anonymization clinical;clinical notes securing;denoise anonymized clinical;anonymized corpora;embedding method obfuscation;data anonymization;content word anonymized;refers obfuscation text;propose new obfuscation;anonymizing data;obfuscationize"}, "76b95833fd0e242896d231abdea8dc01a167c7a6": {"ta_keywords": "drift function stochastic;estimating drift function;stochastic differential equations;estimation drift function;drift function;drift function systems;stochastic differential;systems stochastic differential;equations drift function;function stochastic differential;approximate equations drift;estimating drift;drift function class;approach estimating drift;estimation drift;class stochastic differential;applied estimation drift;function stochastic;function class stochastic;stochastic;systems stochastic;equations drift;function systems stochastic;class stochastic;differential equations posterior;drift;equations posterior state;posterior state vector;equations posterior;posterior state", "pdf_keywords": "stochastic differential;stochastic differential equations;model stochastic differential;drift function stochastic;given stochastic differential;stochastic differential equation;sde given stochastic;drift estimated sparse;systems stochastic differential;function stochastic differential;estimating drift functions;given stochastic;estimating drift function;estimate drift function;posterior process drift;stochastic process;stochastic process based;stochastic;model stochastic;general stochastic;drift function model;estimate drift;applicable model stochastic;markov process noise;stochastic model approach;stochastic model;trajectories stochastic differential;process noise modeled;function stochastic;general stochastic model"}, "a660429b77e932af1c1d7d3f0554f4b17c044082": {"ta_keywords": "clusters similar terror;formation latent clusters;similar terror groups;latent clusters;latent clusters similar;mode weighting entropy;terror groups based;entropy mode weighting;terror groups;weighting entropy based;weighting entropy;entropy mode;entropy based;neumann entropy mode;von neumann entropy;groups based;approach formation latent;clusters similar;entropy based approach;formation latent;neumann entropy;similar terror;terror;entropy;empirical;clusters;patterns;emerges baseline empirical;empirical approach;groups", "pdf_keywords": "terrorist attacks empirical;global terrorism database;terrorism database;analyzing global terrorist;patterns terrorist global;similar terrorist groups;characterize terrorist groups;organization terrorist networks;terrorist groups complex;hidden patterns terrorist;terrorist networks;terrorist networks organization;study terrorist networks;clusters terrorist groups;patterns terrorist;terrorist networks potential;clusters similar terrorist;terrorist groups based;latent clusters terrorist;terrorist groups context;dynamics terror groups;terror groups based;known terror groups;identify characterize terrorist;terrorist groups expanding;terrorist groups;terrorist events related;behavior terrorist groups;information global terrorism;attacks network similarity"}, "8512718bafa447f9b433da9e809215dfc28b6b28": {"ta_keywords": "performance natural language;performance performance prediction;grained performance prediction;performance prediction;prediction different nlp;performance prediction model;nlp tasks;natural language processing;language processing nlp;processing nlp;nlp tasks propose;reliability performance prediction;estimating performance natural;different nlp tasks;estimating performance;present performance prediction;performance prediction different;language processing;processing nlp problem;purpose estimating performance;nlp;prediction model performing;nlp problem;natural language;different nlp;nlp problem examine;examine performance performance;performance performance;performance;understand reliability performance", "pdf_keywords": "performance natural language;predict performance nernstrograph;performance prediction tensor;prediction task tensor;holistic performance prediction;predicting performance machine;language processing nerns;performance prediction task;training performance prediction;predicting performance natural;performance tensor networks;grained performance prediction;predicting linguistic;predicted performance generally;reliability performance prediction;predicted performance predicted;grained evaluation nernstrograph;performance predicted performance;performance prediction improve;formulate performance prediction;predicting performance;novel performance prediction;performance prediction;predicting reliability performance;performance predicted;performance prediction models;predicting linguistic structure;natural language processing;evaluation nernstrograph systems;predicted performance"}, "84702b091af8842b6bbe457e5435c343a9824693": {"ta_keywords": "condensate repulsive interaction;repulsive interaction condensate;interaction condensate repulsive;bose einstein condensate;repulsive condensate repulsive;generate repulsive condensate;repulsive condensate;condensate repulsive;condensate repulsive attractive;dynamics component bose;repulsive interaction;interaction condensate;einstein condensate bec;einstein condensate;attractive repulsive interaction;repulsive interaction used;component bose einstein;condensate bec presence;bose einstein;condensate bec;bec presence repulsive;generate repulsive;presence repulsive interaction;repulsive attractive repulsive;repulsive;condensate;component bose;attractive repulsive;repulsive attractive;used generate repulsive", "pdf_keywords": ""}, "54e7209e692ca4f5c85f0e68df34040b3cfa8bad": {"ta_keywords": "coded matrix multiplication;coded matrix;level coded matrix;sparsity level coded;matrix multiplication short;encoding scheme achieve;short dot scheme;encoding scheme;new encoding scheme;matrix multiplication;mds matrix propose;matrix propose;mds matrix;matrix propose new;separable mds matrix;sparsity existing schemes;dot scheme optimal;designing mds matrix;propose new encoding;multiplication short dot;coded;mds matrix fixed;strictly larger sparsity;limits sparsity level;limits sparsity;encoding;dot scheme;matrix;matrix fixed designing;distance separable mds", "pdf_keywords": ""}, "1acbfc7d3e245bd3146e9e24eae7550aa2d03482": {"ta_keywords": "activation matrices stochastic;activation matrices stochastically;stochastic dynamics neural;network stochastic;stochastic dynamics network;dynamics network stochastic;matrices stochastic dynamics;network stochastic process;matrices stochastically transformed;matrices stochastic;stochastically transforming stochastic;matrices stochastically;stochastic dynamics stochastic;stochastic dynamics represented;stochastic dynamics;transforming stochastic dynamics;stochastically transformed;dynamics stochastic;stochastically;transforming stochastic;stochastically transforming;stochastic;constructed stochastically transforming;stochastic process stochastically;dynamics stochastic process;stochastic process stochastic;process stochastically transforming;process stochastic dynamics;constructed stochastically;models constructed stochastically", "pdf_keywords": "deep networks rank;initialization deep networks;layer rank networks;random initialization deep;rank collapse networks;rank preserving network;rank networks;rank collapse deep;rank network preserved;initialize deep neural;initialization deep neural;deep linear neural;rank naturalizable network;batch normalized networks;network nn deep;deep linear;initialization deep;networks rank collapse;rank networks does;layers convergence nnn;linear relu networkswe;initialize deep;deep networks spectral;layers deep neural;deep network nn;learning nn matrix;networks infinite depth;avoid deep networks;normalized networks finally;nn deep network"}, "796f29cee975603c7a1469df1eb21ed5142ecff5": {"ta_keywords": "literary evidence retrieval;retrieve quotes literature;passage collection literary;collection literary documents;evidence retrieval;quoted passage collection;literary documents;passage collection;retrieve quoted passage;problem literary evidence;collection literary;quotes collection literary;evidence retrieval use;dense retrieval;problem dense retrieval;retrieval use random;dense retrieval use;literary documents use;use random passage;literary evidence;quotes literature;quotes literature compare;retrieval;quotes collection;random passage;passage model retrieve;literature;retrieval use;retrieve quotes collection;documents use random", "pdf_keywords": "literary evidence retrieval;dataset literary evidence;retrieving evidence literary;large dataset literary;sources evidence literary;literary literature dataset;dataset literary literature;dataset literary texts;dataset literary;scholarly excerpts literary;embeds scholarly claims;describing literary literature;evidence retrieval;excerpts literary analysis;evidence literary claims;inferentially quotation literary;corpus literature;literary evidence;literary claims train;scholarly excerpts;literature dataset;literary claims relic;excerpts literary;text literary analysis;task literary evidence;evidence retrieval models;empirical corpus literature;evidence literary;literary texts;excerpt literary analysis"}, "d10e410765699a75628a1437b93f0d0fc3dc0aa6": {"ta_keywords": "semi supervised learning;semi supervised;performing semi supervised;labeled seed instances;learning propagating labels;unlabeled instances graph;labels labeled seed;propagating labels labeled;labeled seed;seed instances unlabeled;supervised learning propagating;unlabeled instances;propagating labels;instances unlabeled instances;instances unlabeled;supervised;labeled;supervised learning;labels labeled;labels;unlabeled;instances graph;learning propagating;instances graph using;seed instances;page seeds;graph using link;high page seeds;seeds;efficient large datasets", "pdf_keywords": ""}, "48aa33ad92566cb60ef348ffa438e4712f618b03": {"ta_keywords": "images lesions tooth;infrared swir reflectance;extracted teeth lesions;teeth lesions microct;lesions tooth;teeth lesions;lesions tooth proximal;lesions microct measurements;lesions swir images;transillumination images lesions;swir reflectance transillumination;swir reflectance;wavelength infrared swir;depth hidden lesions;infrared swir;extracted teeth;reflectance transillumination images;tooth proximal occlusal;reflectance transillumination;short wavelength infrared;images lesions;hidden lesions swir;wavelength infrared;infrared;clinical probe capable;18 extracted teeth;lesions swir;developed clinical probe;clinical probe;reflectance", "pdf_keywords": ""}, "3fb78bee6cb39588a1a4cbb4e0abce5e362aa130": {"ta_keywords": "bounds adversarial bandits;regret bounds adversarial;adversarial bandits;adversarial bandits derive;bandits derive regret;derive regret bounds;regret bounds obtained;regret bounds;comparison regret bounds;bounds adversarial;bandits derive;investigation regret bounds;bandits;regret bounds kt;time horizon regret;derive regret;horizon regret;adversarial;variant mirror descent;provide comparison regret;mirror descent;regret;comparison regret;actions time horizon;mirror descent provide;investigation regret;recent investigation regret;descent provide comparison;bounds;time horizon", "pdf_keywords": ""}, "e6ffeb4b9d808d6c9b8d388a7cbb431ac96bf194": {"ta_keywords": "proton proton antiproton;proton antiproton proton;antiproton proton proton;antiproton proton;proton antiproton;effect proton;effect proton proton;study effect proton;proton proton;proton;proton proton proton;antiproton;results study effect;effect;study effect;results study;report results study;results;study;report results;report", "pdf_keywords": ""}, "99053e3a708fc27709c9dab33110dc98b187c158": {"ta_keywords": "finance knowledge;gold reasoning programs;questions financial data;annotate gold reasoning;financial data;finance knowledge complex;large corpus financial;datasets annotate gold;reasoning programs dataset;acquiring finance knowledge;financial data aim;gold reasoning;finqa questions financial;corpus financial reports;questions financial;analysis large datasets;financial reports;large datasets;finance;corpus financial;large datasets annotate;large scale dataset;humans acquiring finance;financial;reasoning programs;dataset finqa;annotate gold;dataset finqa questions;acquiring finance;datasets", "pdf_keywords": "reasoning financial data;financial experts annotated;financial valuation tasks;analyzing data financialwe;reasoning programs empirical;analyzing financial reports;financial valuation models;models financial reports;financial data;annotated learning;reasoning financial;annotated learning tasks;annotated numerical reasoning;learning modeling corpus;numerical reasoning financial;data financialwe introduce;reasoning program generation;meaningful questions financial;annotated questions;analyzing financial;financial data model;generation reasoning programs;supporting facts financial;nlp program gold;financial reports generate;financial reports;reasoning programs;questions financial;answered nlp program;facts financial reports"}, "0acbdcac9edf74cc2c1e98bd59e301c9300977d0": {"ta_keywords": "automatically tag tags;automatically automatically tag;automatically tag;tags automatically;tags tags automatically;tag automatically;tag automatically automatically;tag tag automatically;automatically reassign tags;weight collection tags;tags automatically reassign;automatically assigning weight;automatically identify informative;automatically automatically identify;informative unlabeled instances;unlabeled instances automatically;tags;tag tags;tags framework;suggesting informative unlabeled;automatically automatically suggesting;tags tags;tags tag;informative unlabeled;identify informative unlabeled;tag tags tags;tags framework able;automatically automatically automatically;automatically suggesting informative;automatically assigning", "pdf_keywords": ""}, "ee2e171d6a897ee5d0b0bde2d5f2548b52d3a840": {"ta_keywords": "cognitive help students;students meta cognitive;cognitive help better;meta cognitive help;cognitive help classroom;cognitive help;help students meta;cognitive help context;meta cognitive;effect meta cognitive;performance meta cognitive;cognitive help likely;study students meta;students meta;help students;performance students meta;help classroom study;help classroom;cognitive;using tutoring;service students meta;tutoring service students;tutoring;teaching compare performance;using tutoring service;classroom study students;tutoring service;learning teaching compare;learning teaching;classroom study", "pdf_keywords": ""}, "9633928f72cda45d102fb6740291d47137d0a5ca": {"ta_keywords": "attacks federated learning;federated learning;backdoor attacks federated;federated learning federated;federated learning ability;learning federated;federated learning incorporates;federated learning environment;model federated learning;features federated learning;attacks federated;tasks federated learning;federated pruning;federated pruning method;learning federated model;presents federated pruning;present federated pruning;ability perform federated;pruning method backdoor;perform federated;backdoor attacks;method backdoor attacks;federated;features federated;model federated;federated model federated;key features federated;method backdoor;federated model;federated tasks federated", "pdf_keywords": "attacks federated learning;networks federated pruning;attackers distributed pruning;efficient federated learning;federated pruning algorithm;learning distributed pruning;federated pruning;federated pruning method;backdoor attacks federated;new federated pruning;federated pruning technique;propose federated pruning;systems federated pruning;realistic federated learning;performance federated learning;federated learning distributed;federated neuron pruning;machine learning federated;federated pruning server;learning federated learning;federated learning;improve attacks backdoor;federated pruning process;distributed pruning;federated learning simple;federated learning systems;distributed way pruning;situation federated pruning;backdoor attack performance;federated learningin"}, "7731e3dec97c48498b585408d44615346ade144a": {"ta_keywords": "language variation social;variance community sense;language specificity community;community sense;characterize language variation;language variation;variation social networks;language variation using;measure variance community;community sense bert;activity characterize language;variation social;networks characterizing senses;variance community;social networks characterizing;community using rabi;characterize language;specificity community using;characterize language specificity;characterizing senses words;words context group;language specificity;group activity characterize;social networks;variance index bert;bert characterize language;community;specificity community;frequency approximation bert;networks characterizing", "pdf_keywords": "language reddit communities;subreddits word definitions;subreddits use characterization;social media lexical;word usage subreddits;variation online communities;subreddits distinct communities;subreddits use different;languages subreddits web;subreddits new languages;reddit communities;communities thousands words;new languages subreddits;usage subreddits distinct;online communities use;subreddits word;online communities computational;language popular communities;reddit communities community;identify popular subreddits;online communities increasingly;subreddits identify popular;usage subreddits;online communities;online communities world;subreddits technology;clusters word representatives;subreddits technology subredditswe;languages subreddits;variation subreddits"}, "b116e5044fe047fc48307795af1f3e11b3a9401c": {"ta_keywords": "variational bayes algorithm;overfitting model independent;overfitting model;variational bayes;overfitting;present variational bayes;degree overfitting model;compute degree overfitting;bayes algorithm;calculation degree overfitting;bayes algorithm calculation;bayes compute;version laplace algorithm;algorithm laplace;degree overfitting;bayes;laplace algorithm based;model independent setting;algorithm laplace algorithm;use bayes;use bayes compute;laplace algorithm laplace;laplace algorithm;setting use bayes;bayes compute degree;model independent;version laplace;variational;popular version laplace;model", "pdf_keywords": ""}, "4b18303edf701e41a288da36f8f1ba129da67eb7": {"ta_keywords": "zero shot learning;shot learning based;shot learning;domain adaptation;domain adaptation methods;approach zero shot;zero shot;weights layer learned;learning based general;layer learned;casting domain adaptation;layer learned given;learning;features attributes classes;attributes classes linear;learned given environment;learning bound generalisation;classes linear layers;provide learning bound;attributes classes;real datasets;learning based;learning bound;datasets;features attributes;linear layers network;adaptation methods;attributes;learned given;relationships features attributes", "pdf_keywords": "zero shot learning;zeroshot learning;zeroshot learning combines;framework zeroshot learning;recognition zero shot;shot learning method;shot learning approach;shot learning;shot learning based;shot learning domain;attributes database supervised;shot learning resulting;attribute based classification;learning combines attribute;shot learning able;classify attribute;efficient zero shot;new zero shot;features attributes classes;classify attribute signatures;learning approach attribute;approach zero shot;attribute predictions;zero shot;random forests;forests classify attribute;random forests classify;attribute predictions new;attributes classes;zeroshot"}, "b29dd2c50da0dc4589eafac58007f6be7e13c501": {"ta_keywords": "geometry indoor environments;3d geometry indoor;identifying objects scene;indoor environments;room objects;geometry indoor;model enclosing room;enclosing room objects;room objects beds;identifying objects;simultaneously identifying objects;indoor environments beds;objects scene;scene use generative;understanding 3d;objects scene use;3d geometry;objects beds;indoor;relative dimensions locations;understanding 3d geometry;camera model enclosing;enclosing room;cabinets simultaneously identifying;integrates camera model;environments beds cabinets;camera model;method understanding 3d;objects beds tables;3d", "pdf_keywords": ""}, "873b83326ad1f98549beb85bdb130a40a61e1f9b": {"ta_keywords": "uncalibrated scoring functions;scoring functions;scoring functions variety;scoring function directly;new scoring function;scoring function;scoring function problem;language model achieves;selecting predictive;apply scoring function;form competition;language model;surface form competition;task apply scoring;uncalibrated scoring;selecting predictive function;form competition reweighing;predictive;introduce new scoring;apply scoring;scoring;new scoring;choice datasets;problem language model;calibrated uncalibrated scoring;problem selecting predictive;predictive function;shot performance calibrated;zero shot performance;language", "pdf_keywords": "inference models generative;shot inference models;zero shot learning;zero shot inference;generative models perform;generative models;models generative;datasets demonstrating generative;generative models choose;shot inference;tasks use generative;generative models used;shot learning;scoring shot model;demonstrating generative models;demonstrate generative models;generative;shot learning strategy;generative model;models generative model;fact generative models;use generative;generative model used;demonstrating generative;models learning;zero shot questions;probability scoring methods;prediction natural language;use generative model;score hypotheses surface"}, "9fe579e54712ba82c4f1c93e46409613f592df16": {"ta_keywords": "spin orbit interaction;dynamics coupling spin;coupling spin orbit;effect coupling spin;spin orbit;coupling spin;magnetic field coupling;motion particle magnetic;dynamics coupling;orbit interaction motion;orbit interaction;particle magnetic;particle magnetic field;overall dynamics coupling;interaction motion particle;spin;field coupling;field coupling used;magnetic field;effect coupling;coupling;magnetic;interaction motion;motion particle;coupling used;orbit;coupling used obtain;dynamics;motion particle used;study effect coupling", "pdf_keywords": ""}, "178d51c35c03e3ccaae2409c32a3c2001cefe7eb": {"ta_keywords": "incremental estimation;derive incremental estimation;incremental estimation algorithm;new incremental estimation;solution incremental estimation;posterior refinement modeled;analytically derive incremental;algorithm based posterior;estimation algorithm;derive incremental;process posterior refinement;based posterior distributions;estimation algorithm based;posterior distributions process;posterior distributions model;estimation algorithm proposed;incremental;posterior refinement;new incremental;distributions process posterior;process posterior;estimation;propagation mechanism posterior;mechanism posterior distributions;based posterior;posterior distributions;propose new incremental;general solution incremental;refinement modeled analytically;solution incremental", "pdf_keywords": ""}, "415d4231cab5ddee73e2ed536d033d5c31f24b4a": {"ta_keywords": "information extraction biomedical;extraction biomedical text;open information extraction;information extraction;biomedical text based;biomedical text;extraction biomedical;extract seeds text;category extract;seeds category extract;text based neural;semi supervised bootstrapping;category extract seeds;supervised bootstrapping;supervised bootstrapping approach;semi supervised;use semi supervised;open information;seeds text;initial ontology;biomedical;extraction;ontology small;ontology;given initial ontology;initial ontology small;text seeds;extract;facts text;supervised", "pdf_keywords": ""}, "8163c4010fc103343518d49db5974577593972f6": {"ta_keywords": "accuracy liar detection;liar detection;liar detection higher;liar detection similar;automatic detection deception;detection deception accuracy;detection deception;detection deception based;accuracy detection deception;deception accuracy liar;question accuracy liar;liar partner compare;deception accuracy;liar partner;concept liar partner;answer accuracy liar;deception based concept;accuracy liar;concept liar;deception;based concept liar;deception based;liar;partner compare accuracy;compare accuracy detection;question accuracy;detection similar question;answer accuracy;accuracy detection;partner compare", "pdf_keywords": ""}, "fa5c7406d09af3f06a3a7ead49975e3ee90ed584": {"ta_keywords": "robot shared autonomy;shared autonomy explains;shared autonomy explain;shared autonomy understood;shared autonomy;communicating robot shared;efficiency communicating robot;robot shared;autonomy explains efficiency;shared autonomy lens;autonomy explain efficiency;autonomy explains;lens shared autonomy;autonomy lens shared;autonomy explain;autonomy understood lens;communicating robot;autonomy understood;autonomy;autonomy lens;robot lens shared;efficiency communicating;communicating robot lens;explains efficiency communicating;process communicating robot;explain efficiency communicating;understood lens shared;efficiency process communicating;robot;communicating", "pdf_keywords": "accomplished robot language;shared autonomy communicating;robot language;autonomy communicating humans;communicating humans robots;sharing task human;robot language crucial;task human robot;shared autonomy human;autonomy human robot;interaction humans robot;robots combines linguistic;task interaction humans;autonomy communicating;separating humans robots;task accomplished robot;human robot assisted;shared autonomy combines;shared autonomy use;shared autonomy shared;knowledge end robot;robot assisted;humans robots combines;robot separated sharing;humans robot based;tasks human separatedwe;autonomy shared;ordering interaction humans;human robot separated;humans robot"}, "9527352b925f9fa36c40966ed755afd22301b0aa": {"ta_keywords": "decision quality decision;quality decision determine;decision makers method;decision quality;evaluate quality decision;quality decision applied;impact decision quality;group decision makers;decision determine;quality decision;decision makers using;quality decision group;decision group decision;decision makers;generic decision makers;method evaluate quality;constraints quality decision;decision determine able;evaluate impact decision;decision applied;group decision;decision group;evaluate quality;generic decision;impact decision;applied generic decision;decision applied generic;discriminate bad good;decision;constraints quality", "pdf_keywords": ""}, "74c80622b91894efbe4ae9ce1428e4d699b05516": {"ta_keywords": "distributed convex optimization;methods distributed convex;distributed convex;dual stochastic gradient;stochastic gradient oracle;primal dual stochastic;convex optimization;convex optimization problems;gradient oracle methods;dual stochastic;primal dual oracles;stochastic gradient;oracle methods distributed;gradient oracle;dual oracles propose;methods distributed;optimization problems networks;dual oracles;convex;convergence terms duality;introduce primal dual;distributed;primal dual;method rate convergence;duality gap probability;optimization problems;networks proposed methods;optimization;optimal terms communication;steps primal dual", "pdf_keywords": ""}, "c00e4564ea054c14c83cb564af6c37e47c8ab367": {"ta_keywords": "centralized active tracking;trajectory neighboring sensors;sensor coupled trajectory;active tracking model;coupled sensors model;sensors approximated trajectory;coupled sensors;neighboring sensors model;active tracking;single sensor coupled;sensor coupled;tracking model;tracking model model;neighboring sensors;model single sensor;coupled trajectory model;sensors model;neighboring sensors approximated;number coupled sensors;sensors model consists;coupled trajectory neighboring;model approximated trajectory;tracking;single sensor;coupled trajectory;sensors approximated;trajectory model approximated;centralized active;approximated trajectory neighboring;trajectory model", "pdf_keywords": "process estimation stochastic;centralized tracking markov;tracking markov chain;estimation stochastic;tracking markov;estimation stochastic model;markov chain estimator;sense stochastic process;iterative state estimation;matrix model stochastic;estimation kalman like;learning markov chain;stochastic dynamics approach;estimation process problem;state estimation kalman;state estimation achieve;state model stochastic;estimation physical processes;measurement stochastic dynamics;noisy state markov;estimation kalman;stochastic control;model stochastic process;model based stochastic;tracking discrete time;estimation process;stochastic approximation online;based stochastic process;markov chain based;stochastic process model"}, "98ef0db84e62aef969629264c9de1f4d0013f3b9": {"ta_keywords": "nondestructively combining knowledge;knowledge multiple tasks;knowledge composition classifier;multiple tasks nondestructive;extraction knowledge composition;combining knowledge;learned multiple tasks;combining knowledge multiple;knowledge composition;adapters extract task;tasks nondestructive;extraction knowledge;stages knowledge extraction;tasks nondestructive manner;knowledge extraction;knowledge extraction knowledge;tasks called adapters;extract task specific;separating stages knowledge;extract task;knowledge multiple;composition classifier;composition classifier effectively;nondestructively combining;adapters combine nondestructive;multiple tasks;method nondestructively combining;representations learned multiple;combine nondestructive manner;combine nondestructive", "pdf_keywords": "tasks sentiment analysis;multi task learning;learned multiple tasks;knowledge multiple tasks;cross task learning;diverse nlu tasks;adapting multi task;natural language tasks;nlu tasks sentiment;transfer learning;knowledge tasks;knowledge tasks improve;task learning;combine information tasks;adapters learn task;transfer learning strategy;task learning catastrophic;task learning novel;approach transfer learning;knowledge composition classifier;learn task;transfer learning called;adapter combines knowledge;tasks sentiment;transfer learning process;adapters multi task;leverages knowledge tasks;learn task specific;training adapters tasks;nlu tasks"}, "43953a051b6518f32fc37734cfc49942baeac5a1": {"ta_keywords": "spectral variation utterances;speaker spectral variation;intra speaker spectral;speaker spectral;variation utterances;utterances conduct experimental;predicting prosodic parameter;predicting prosodic;parameter differences utterances;differences utterances;differences utterances conduct;method predicting prosodic;prosodic parameter differences;utterances conduct;prosodic parameter;utterances;mel cepstrum spectral;prosodic;investigate intra speaker;cepstrum spectral;variation utterances sentence;intra speaker;cepstrum spectral parameter;mel cepstrum;focusing mel cepstrum;spectral variation;utterances sentence;utterances sentence focusing;spectral parameter propose;spectral", "pdf_keywords": ""}, "4077c1986f32817801b3082ce8dde514424f71a1": {"ta_keywords": "crowdsourcing synset cleansing;crowdsourcing synset;workflow crowdsourcing synset;synset cleaning;synset cleaning improved;synset cleansing;workflow crowdsourcing;scale synset cleaning;novel workflow crowdsourcing;synset cleansing shown;crowdsourcing;synset;nodes scale synset;remove confirm nodes;scale synset;confirm nodes respectively;confirm nodes;removed empirical validation;shown scale synset;cleaning improved;nodes;adding confirm nodes;improved adding remove;adding remove confirm;nodes respectively removed;cleaning improved adding;cleansing;nodes respectively existing;cleaning;nodes respectively", "pdf_keywords": ""}, "2344cca985dd4e2e2519838b2353b5c295e73036": {"ta_keywords": "neural machine comprehension;machine comprehension;answering questions arc;knowledge reasoning types;definitions knowledge reasoning;knowledge reasoning;questions arc automated;sentences resulting dataset;query use neural;reasoning types;types necessary answering;reasoning types necessary;query demonstrate sentences;labels challenge;necessary answering questions;questions arc;answering questions;labels challenge set;definitions knowledge;use neural machine;recognition data mining_;neural machine;comprehension;use neural;distribution labels challenge;challenge set;questions;challenge set demonstrate;data mining_;pattern recognition data", "pdf_keywords": "question annotators;annotating knowledge;questions question annotators;using annotating knowledge;question annotators high;annotation interface knowledge;reasoning labels annotators;annotating knowledge corpus;annotation evaluation knowledge;sophisticated annotation;sophisticated annotation interface;annotators sophisticated annotation;fact annotators;using annotators sophisticated;annotators sophisticated;knowledge reasoning types;definitions knowledge reasoning;knowledge reasoning type;annotators based natural;define annotators question;questions keywords science;dataset using annotators;answering questions arc;fact annotators required;annotators based;annotations relevant results;knowledgeunderstanding linguistic semantics;annotations;annotators question likely;uses fact annotators"}, "05fb1eea6381ccd21bde53495c7707546aa234c7": {"ta_keywords": "emergence named entities;novel named entities;entities social media;named entities social;named entities;novel named entity;social media messages;named entity;messages novel named;named entity np;nodes form novel;messages novel;media messages novel;entities social;neural network incorporates;entities;emergence named;neural network;neural;novel pattern recognition;nodes;identify np nodes;form neural network;entity;social media;novel pattern;approach emergence named;features identify np;pattern recognition features;novel approach emergence", "pdf_keywords": ""}, "3132a18a441ab6067066e4d4d85608b058c9ed33": {"ta_keywords": "detection abrupt changes;identifies changes autocorrelation;changes time series;detection change points;changes autocorrelation;changes autocorrelation statistics;autocorrelation statistics data;time series data;abrupt changes time;autocorrelation;autocorrelation statistics;detection abrupt;time series;detection change;autoencoders;autoencoders novel loss;abrupt changes;autoencoders novel;learning partially time;use autoencoders;time invariant representation;time invariant;based use autoencoders;use autoencoders novel;identifies changes;partially time invariant;series data based;method identifies changes;data iteratively learning;real life data", "pdf_keywords": "change point detection;detection change points;detect change points;detect changes time;identifying change points;automatically identify change;automatically detect change;change points detect;identify change points;learning time frequency;data detect change;detect changes;detect change;autoregressive breakpoint detection;changes time series;time series learning;features time domain;change points based;detection change;way identifying change;change points time;identifying change;features discrete time;able detect changes;features autoencoder based;continuous variable detection;frequency invariant features;features time;time domain autoencoder;learns partially time"}, "91e605a125f64207a242693d0dc1c862080f6c27": {"ta_keywords": "automatic phoneme recognition;phoneme recognition;translation phoneme lattices;phoneme recognition leveraging;improving automatic phoneme;improves automatic phoneme;phoneme lattices phoneme;phoneme recognition factors;automatic phoneme;phoneme lattices;lattices phoneme;lattices phoneme able;translation phoneme;based translation phoneme;phoneme able learn;recognition leveraging translation;leveraging translation speech;phoneme;learn important lexical;lexical entries extracting;phoneme able;extracting lattices;leveraging translation;lexical entries;recognition factors xmath0;important lexical entries;extracting lattices demonstrate;entries extracting lattices;lexical;important lexical", "pdf_keywords": ""}, "b953a582cc79c33054b295c20c1201e8d5bd8243": {"ta_keywords": "finds student models;student models;student models using;models using clustering;approach finds student;clustering;automaticallygenerated problem content;using clustering;clustering algorithm;automated approach knowledge;automated approach finds;finds student;algorithm based automaticallygenerated;knowledge engineering;using clustering algorithm;clustering algorithm based;based automaticallygenerated problem;knowledge engineering effort;using algebra dataset;algebra dataset;automated approach;algebra dataset experimental;approach knowledge engineering;results discovered model;automaticallygenerated problem;propose automated approach;propose automated;student;discovered model;automaticallygenerated", "pdf_keywords": ""}, "d9d0d908e3f652ee350f4919d4c2ab972ada1ca4": {"ta_keywords": "coreference questions bowl;new coreference dataset;coreference annotation framework;new coreference annotation;coreference annotation;coreference dataset;text data coreferences;community new coreference;set coreference questions;coreferences;coreference questions;new coreference;data coreferences develop;data coreferences;coreference structure data;information coreference;coreference dataset complete;coreferences develop simple;coreferences develop;information coreference structure;present new coreference;coreference;introduce new coreference;coreference structure;bowl quiz community;extract information coreference;set coreference;questions bowl quiz;bowl quiz;complete set coreference", "pdf_keywords": ""}, "f18ec4e0bce2e4d847954c9692959d88ba8a9b66": {"ta_keywords": "secure estimation stochastic;estimation stochastic;estimation stochastic process;accurate estimation stochastic;filtering algorithm secure;algorithm secure estimation;adaptive filtering algorithm;adaptive filtering;stochastic process vehicle;secure estimation;joint filtering random;filtering random;filtering random variables;novel adaptive filtering;modeling complex networks;filtering algorithm;stochastic;networks paper proposes;slow v2x communication;estimation proposed algorithm;stochastic process;complex networks paper;based joint filtering;networks paper;joint filtering;filtering;stochastic process prerequisite;vehicle slow v2x;v2x communication;complex networks", "pdf_keywords": ""}, "5e27712db641bc8f16c510292f7fd5440acd563d": {"ta_keywords": "stacked graphical learning;stacked relational datasets;relational datasets stacked;datasets stacked graphical;related data stacked;graphical learning meta;data stacked graphical;inferring stacked graphical;datasets stacked;graphical learning features;graphical learning implemented;propose stacked graphical;data stacked;stacked graphical;meta learning scheme;graphical learning;meta learning algorithm;stacked relational;graphical learning algorithm;implemented meta learning;instances stacked graphical;inferring stacked;meta learning;related instances stacked;learning meta learning;set stacked relational;learning implemented meta;learning scheme inferring;relational datasets;learning algorithm base", "pdf_keywords": ""}, "73e868f74376814a4c08eca6ce043fe7c7aefeed": {"ta_keywords": "modeling graphs personalized;graphs discrete valued;graphs personalized;modeling graphs discrete;graphs personalized page;modeling graphs;model modeling graphs;modeling graphs demonstrate;graphs discrete;discrete valued fms;valued fms personalized;personalized page fms;graphs;fms personalized page;fms personalized;discrete valued;variable discrete valued;personalized page;graphs demonstrate connection;graphs demonstrate;connection discrete valued;personalized;page fms;model modeling;valued fms;modeling;discrete variable discrete;connection discrete variable;variable discrete;popular model modeling", "pdf_keywords": ""}, "f6160c3196288b9e435dc6f86024f56e6b5ab722": {"ta_keywords": "computing allocations fair;allocations fair maximize;allocations fair;complexity computing allocations;computing allocations;fairness concepts envy;tractable fairness concepts;tractable fairness;fair maximize social;allocations;focus tractable fairness;welfare sum utilities;fairness concepts;complexity computing;computational complexity computing;computational complexity;proportionality item agents;item agents proportionality;maximize social welfare;envy freeness item;envy freeness;fair maximize;concepts envy freeness;complexity;agents proportionality item;social welfare sum;freeness item agents;maximize social;study computational complexity;welfare sum", "pdf_keywords": ""}, "579095d50eab27ace24a1ea0e97af2f70191dc7c": {"ta_keywords": "tissue graphical model;tissue based graphical;tissue graphical;properties tissue graphical;graphical properties tissue;tissue types graphical;cells graphical model;features biological tissue;cells graphical;new cells graphical;graphical model;graphical model based;graphical model describing;stacked graphical model;tissue particular view;graphical model allows;graphical model used;stacked graphical;based stacked graphical;biological tissue particular;types graphical model;biological tissue;biological tissue based;different tissue types;describing features biological;present graphical model;description different tissue;predict features biological;properties tissue;tissue types", "pdf_keywords": ""}, "0c61265a4325df4b97389f92e5e4f5df412f8e97": {"ta_keywords": "localization power transformers;partial discharge localization;discharge localization proposed;discharge localization model;discharge localization power;discharge localization;method partial discharge;accurate localization power;acoustic electrical joint;localization proposed efficient;localization power;acoustic electrical;power transformers consideration;efficient accurate localization;power transformers;characterized partial discharge;localization proposed;accurate localization;localization model;localization model characterized;problem partial discharge;novel acoustic electrical;transformers consideration multi;partial discharge;localization;multi path propagation;electrical joint method;propagation impact proposed;transformers consideration;path propagation", "pdf_keywords": ""}, "46a2960e409c39901c1efd07a6adfc5f26e22ee8": {"ta_keywords": "popular email client;techniques popular email;client thunderbird;email client thunderbird;proposed data mining;typical email client;thunderbird report;additions email clients;client thunderbird report;thunderbird;email client;techniques typical email;email clients;email client report;data mining;popular email;data mining techniques;permanent additions email;additions email;typical email;mining techniques popular;mining techniques;thunderbird report deployment;email;recently proposed data;client report;proposed data;client report results;client;clients", "pdf_keywords": ""}, "d6a7d2e9f2caf3e8eb615580f4ee8329ff9a271d": {"ta_keywords": "drivers cruising parking;cruising parking neighborhood;cruising parking;cruising parking multi;population drivers cruising;rate drivers cruising;percentage drivers cruising;parking multi dimensional;parking neighborhood;parking;estimating population drivers;parking neighborhood popular;parking multi;neighborhood curbside parking;50 drivers neighborhood;curbside parking;popular curbside parking;finite capacity queues;capacity queues;parking spot;population drivers;curbside parking spot;drivers cruising;drivers neighborhood;parking spot 50;capacity queues use;parking spot significant;estimate percentage drivers;drivers neighborhood curbside;network finite capacity", "pdf_keywords": ""}, "4d01d6b445077ad0f1c9d85af93f9ed9239f3c33": {"ta_keywords": "speech tagging historical;tagging historical texts;speech tagging;texts tagging;tag texts tagging;method speech tagging;texts tagging accuracy;tagging historical;tag texts;tagging accuracy;tagger trained tagger;tagging accuracy evaluated;tagging;use tagger;use tagger trained;tagger;corpora historical german;trained tagger;tagger ability;trained tagger ability;tagger trained;based use tagger;corpora historical;historical texts based;historical texts;historical german;texts different corpora;tagger ability tag;different corpora historical;ability tag texts", "pdf_keywords": ""}, "189e6bb7523733c4e524214b9e6ae92d4ed50dac": {"ta_keywords": "neural sequence taggers;sequence taggers;sequence taggers source;taggers source task;tagging microblogs;spread tagging microblogs;transfer learning deep;recurrent networks domains;spread tagging;tagging microblogs examine;problem transfer learning;deep hierarchical recurrent;transfer learning;transfer learning neural;recurrent networks;hierarchical recurrent networks;learning neural sequence;tagging;taggers source;point spread tagging;hierarchical recurrent;task plentiful annotations;taggers;learning deep;deep hierarchical;annotations used improve;learning deep hierarchical;annotations;fewer available annotations;microblogs", "pdf_keywords": "learning sequence tagging;tagging based deep;sequence tagging based;sequence tagging;approach sequence tagging;sequence tagging source;efficiently tag sequence;transfer learning sequence;recurrent networks domains;tagging penn treebank;transfer learning deep;framework sequence tagging;learning tasks chunking;tagging source task;language networks;word level rnns;transfer learning architectures;recurrent networks;hierarchical recurrent networks;transfer transfer learning;sequence tagging exploits;highly efficient recurrent;entity recognition powerful;transfer learning;problem transfer learning;proposes transfer learning;transfer learning framework;deep hierarchical recurrent;tag sequence characters;transfer cross lingual"}, "58b628792d3eb22a034a871ed3cf373afe591928": {"ta_keywords": "erasure codes efficient;erasure codes;codes distributed;new codes distributed;reed solomon codes;codes distributed hep;solomon codes;solomon codes new;family erasure codes;sudarshan distributed systems;distributed systems hep;distributed hep lma;distributed hep;codes efficient;standard reed solomon;distributed systems;distributed systems distributed;distributed distributed;systems distributed;systems distributed distributed;distributed;codes new codes;erasure;sudarshan distributed;lma sudarshan distributed;new codes;reed solomon;systems hep lma;functional theory hep;standard reed", "pdf_keywords": "distributed storage codes;storage coding;erasure codes efficiently;erasure coding viable;storage erasure coded;coded erasure coding;storage codes optimal;codes efficiently repairable;erasure coding;erasure codes;code data storage;erasure coded erasure;solomon distributed storage;coded erasure;storage coding module;codes logarithmic locality;storage codes;locally repairable codes;distributed storage theorems;repairable codes marginally;codes block locality;erasure coded;hydrogen storage coding;distributed data storage;distributed storage stable;repairable codes;distributed storage;stable storage distributed;locality coded blocks;distributed storage important"}, "58174f5bb9f9815b52a99fa03ec42f2b44f2d550": {"ta_keywords": "automatic set acquisition;instances large corpus;large corpus text;language independent sets;corpus text documents;lexicons proposed approach;search language;web based set;corpus text;large corpus;lexicons proposed;automatic set;search language independent;asia automatic set;set acquisition automatically;gnu documents automatically;named asia automatic;text documents gnu;corpus;patterns search language;based set expansion;text documents;form lexicons proposed;form lexicons;automatically finds instances;automatically finds;lexicons;sets proposed approach;documents automatically;produces form lexicons", "pdf_keywords": ""}, "2bafaffe45ba66685c87e2d0a598222a9a68ae13": {"ta_keywords": "quantum information processing;quantum information;field quantum information;quantum;field quantum;progress field quantum;information processing presented;information processing;processing presented survey;processing presented;processing;progress field;survey progress field;information;survey progress;progress;survey;presented survey;field;survey based;survey based work;presented survey based;catalogue;survey includes catalogue;survey includes;work author survey;presented;author;author survey;includes", "pdf_keywords": ""}, "108ec3512cdf2e89ba3067f5b10eaa4a96df9347": {"ta_keywords": "transfer vectors acoustic;transfer vector adaptation;vectors acoustic modeling;continuous speech recognition;training transfer vectors;vector adaptation scheme;vector adaptation;vectors acoustic;speech recognition;transfer vectors;transfer vector;standard transfer vector;adaptation scheme transfer;acoustic modeling;acoustic modeling proposed;transfer vector represented;proposed adaptation scheme;speech recognition based;adaptation scheme based;novel adaptation scheme;adaptation scheme;continuous speech;training transfer;adaptation scheme continuous;proposed adaptation;scheme continuous speech;scheme transfer vector;fine training transfer;based directional statistics;recognition based coarse", "pdf_keywords": ""}, "b575d272036740e03fcf67d64db969557843f629": {"ta_keywords": "user annotated hashtags;annotated hashtags;annotated hashtags associated;hashtags associated social;predicting user annotated;hashtags associated;hashtags;social media posts;methods predicting user;predicting user;user annotated;associated social media;predicting;character composition model;social media;annotated;methods predicting;problem predicting user;character composition;use character composition;media posts approach;posts approach;use character;posts approach combines;posts;problem predicting;media posts;mechanics use character;art methods predicting;approach problem predicting", "pdf_keywords": "tweet deep learning;representations tweets learning;encoder predicting hashtags;model predicting hashtags;predicting hashtags;vectorspace representations tweets;predict hashtags;performance predicting hashtags;representations tweets;models predict hashtags;tweets learning;predicting hashtags social;predict hashtags unstructured;predicting hashtags held;tweets learning complex;learn distributed representations;learning distributed representations;distributed representations words;hashtags associated posts;annotated hashtags;user annotated hashtags;hashtags social media;annotated hashtags associated;associated hashtags train;hashtags train;words embeds representations;hashtags train models;hashtags unstructured structured;hashtag given tweet;associated hashtags"}, "d34712c217046ccf8063efe083fbb1e6cbfc0340": {"ta_keywords": "distributed content delivery;content delivery networks;delivery networks cdns;distributed content;deployment distributed content;coding cdn architecture;content delivery;cdn architecture;networks cdns goal;maintaining high availability;networks cdns;erasure coding cdn;cdn architecture balance;higher availability;high availability;deployment distributed;availability close shortest;delivery networks;coding cdn;write load servers;distributed;availability;design deployment distributed;shortest latency;servers maintaining;cdns;higher availability close;close shortest latency;cdns goal use;high availability close", "pdf_keywords": ""}, "f8e580fcf34ee6da50989bbde685634018cbe224": {"ta_keywords": "dialog state tracking;dialog state challenge;advanced dialog state;5th dialog state;dialog state;advanced dialog;present advanced dialog;dialog;5th dialog;attention based tracker;designed 5th dialog;rule based trackers;compared rule trackers;rule trackers;combines attention based;rule trackers refining;state tracking;attention based;dstc5 combines attention;state challenge dstc5;tracker rule based;state tracking designed;challenge dstc5 combines;topic slot level;state challenge;topic slot;based tracker rule;tracker rule;combines attention;based tracker", "pdf_keywords": ""}, "8a6d2e134b3b2df6291af8e36e126ae55d50649c": {"ta_keywords": "paraphrastic sentence embeddings;learn sentence embeddings;sentence embeddings improves;learning paraphrastic;learning paraphrastic sentence;sentence embeddings models;approach learning paraphrastic;sentence embeddings;structure sentence embeddings;sentence embeddings state;paraphrastic;embeddings improves;recurrent network learn;paraphrastic sentence;recurrent network;embeddings state art;use recurrent network;embeddings improves state;embeddings models outperform;learn sentence;embeddings;embeddings models;network learn sentence;use recurrent;embeddings state;approach use recurrent;recurrent;transfer supervised;resulting models learn;models learn", "pdf_keywords": "embedding word sequence;short term memory;sentence embeddings improve;sentence embedding model;word embeddings;recurrent networks;embedding sentences;term memory recurrent;lsm recurrent networks;sentence embeddings;regularizing word embeddings;sentence embedding;recurrent network;embedding sentences natural;recurrent neural network;recurrent networks outperformed;embedded sentence sequence;natural language neural;paraphrastic sentence embeddings;models sentence compression;long term memory;memory recurrent neural;sentence embeddings revisiting;paraphrastic sentence embedding;language neural;word embeddings generative;method embedding sentences;term memory;embedding word;sequence short words"}, "3911b13a61a3a57674cc8c70c760f545de8aeea2": {"ta_keywords": "large truthful equilibrium;truthful equilibrium;mechanism truthful information;evaluations large truthful;truthful payoff measure;truthful information processing;truthful equilibrium approximately;payoffs agents symmetric;mechanism truthful;truthful information;truthful payoff;evaluations mechanism;expected payoffs agents;agents performing evaluation;large scale evaluations;evaluations mechanism based;new truthful payoff;new mechanism truthful;particular answer mechanism;answer mechanism;equilibrium game;equilibrium game induced;strict equilibrium game;evaluations large;information processing large;agents symmetric;agents symmetric equi;payoff measure captures;payoffs agents;game induced mechanism", "pdf_keywords": ""}, "f335e2256b31d9458c10c61e60bb8bed9dcaf1d9": {"ta_keywords": "vision language navigation;navigation based training;training approach learns;language navigation;vision language;generalization demonstrate learns;approach learns;learns effectively limited;language navigation based;learns;training paradigm vision;learns effectively;approach learns uses;navigation;training data generalizes;guide improve generalization;demonstrate learns;demonstrate learns effectively;learns uses path;training paradigm;navigation based;training approach;novel training paradigm;improve generalization demonstrate;learns uses;paradigm vision language;improve generalization;generalizes better unseen;effectively limited training;training data", "pdf_keywords": "learns navigate;learning navigate;learning navigate visual;learns navigate start;demonstrate learning navigate;lsp learns navigate;learning paradigm navigating;instructions approach learns;visual agent trained;learns multiple instructions;learning agent;vision language navigation;agent able navigate;learning guide agent;navigation uses reinforcement;learning multiple instructions;agent able learn;approach learns;approach learns multiple;navigate visual;navigating visual environment;navigating visual;navigate visual environment;instructions navigation;learns multiple;constrained navigation paradigm;visual agent;learning lsp learns;instructions navigation demonstrate;reinforcement learning vln"}, "5f23482a8c06ca1ae3e4577e3fdd9213884dac85": {"ta_keywords": "probabilistic trees generated;generated probabilistic trees;trees generated probabilistic;generating probabilistic trees;trees probabilistic trees;probabilistic trees probabilistic;probabilistic trees;probabilistic trees trees;trees probabilistic;algorithms generating probabilistic;probabilistic algorithms generating;process probabilistic trees;generated probabilistic;generating probabilistic;generated probabilistic process;trees trees generated;probabilistic algorithms;trees generated;probabilistic;probabilistic process probabilistic;trees trees;probabilistic process;dynamics probabilistic algorithms;process probabilistic;dynamics probabilistic;trees;algorithms generating;study dynamics probabilistic;generating;algorithms", "pdf_keywords": ""}, "1e638d235a512cc76d00713639259540342c6fbe": {"ta_keywords": "relation extraction;relation extraction model;relation extraction classification;applied relation extraction;end relation extraction;extract semantic information;extract semantic;processing extracted information;extracted information parallel;method extract semantic;semantic information data;extraction classification tasks;extracted information;extracted information model;extraction classification;semantic information;information parallel processing;parallel processing extracted;extraction model model;relation;processing extracted;extraction model;semantic;information parallel;end relation;information data;information data set;extraction;method extract;parallel processing", "pdf_keywords": ""}, "c07fdc95bbf533f8709f8e39c069c1e22b73a7dc": {"ta_keywords": "current bearing cavity;cavity neutral current;charged current bearing;cavity cavity neutral;neutral current bearing;bearing neutral current;current bearing neutral;cavity neutral;cavity created neutral;bearing cavity;bearing cavity created;bearing nc cavity;current bearing;current bearing nc;mode charged current;bearing neutral;cavity created charged;cc field cavity;neutral current nc;ec field cavity;neutral current;field cavity;created neutral current;field cavity created;nc cavity cavity;cavity cavity;dynamics single mode;single mode charged;charged current;created charged current", "pdf_keywords": ""}, "26c65dad79da20aa67df21a6c10e509a964f0841": {"ta_keywords": "linking data unstructured;validation linking data;novel validation linking;validate linking;validation linking;unstructured database based;used validate linking;unstructured database;linking data;data unstructured database;validate linking hypothesis;linked entity statements;linked entity;data unstructured;entity statements validation;link entity;database used validate;link entity mentions;entity mentions database;database based;database;database data;mentions database;set linked entity;evaluate linking;mentions database used;available database data;linking;database based ability;ability link entity", "pdf_keywords": ""}, "d4d25eaa373087ac80810d79afff863ef1bae3c3": {"ta_keywords": "seat activity wheelchair;activity wheelchair;activity wheelchair users;wireless weight identification;wheelchair application;method wheelchair application;wheelchair users use;wisat detect weight;validate method wheelchair;wheelchair users;wheelchair;method wheelchair;seat movements validate;characterizing seat activity;weight identification wisat;detect weight shifts;seat activity;identification wisat detect;detect weight;weight shifts seat;method wireless weight;seat movements;shifts seat movements;weight identification;wireless weight;wisat detect;method characterizing seat;characterizing seat;identification wisat;shifts seat", "pdf_keywords": ""}, "a4cd428d196bf041c22592216f15246b98b91915": {"ta_keywords": "dynamics xmath0 symmetric;xmath1 symmetric xmath2;symmetric xmath2 xmath3;xmath0 symmetric xmath1;xmath1 symmetric;symmetric xmath1;xmath0 symmetric;symmetric xmath2;xmath2 xmath3 xmath4;symmetric xmath1 symmetric;dynamics xmath0;study dynamics xmath0;xmath16 xmath17 xmath18;xmath3 xmath4 xmath5;xmath9 xmath10 xmath11;xmath12 xmath13 xmath14;xmath10 xmath11 xmath12;xmath3 xmath4;xmath4 xmath5 xmath6;xmath15 xmath16 xmath17;xmath11 xmath12 xmath13;xmath17 xmath18;xmath17 xmath18 xmath19;xmath13 xmath14 xmath15;xmath17;xmath8 xmath9 xmath10;xmath16 xmath17;xmath7 xmath8 xmath9;xmath14 xmath15 xmath16;xmath2 xmath3", "pdf_keywords": ""}, "8a94106364576f0aa79dccfb30f0536514408249": {"ta_keywords": "rank learning;baseline rank learning;accurate rank learning;rank learning introduce;robust accurate rank;pairs meta learning;linear baseline rank;outlying pairs learning;meta learning algorithm;baseline rank;accurate rank;document pairs meta;meta learning;selecting document pairs;learning algorithm robust;rank;eects outlying pairs;new meta learning;pairs learning;pairs meta;document pairs;pairs learning process;eects outlying;selecting document;learning algorithm;outlying pairs;algorithm robust;learning algorithm capable;undesirable eects outlying;performance linear baseline", "pdf_keywords": ""}, "abcaec70b463ed925c29180437ed581c971952cf": {"ta_keywords": "plural response candidates;response selection;response plural response;adaptive response selection;selects appropriate response;response plural;appropriate response plural;response selection method;plural response;select appropriate response;response based collaborative;satisfaction single response;response candidates;response based;response candidates proposed;collaborative filtering select;response example database;appropriate response based;user satisfaction single;improves user satisfaction;multi response;multi response example;uses collaborative filtering;single response;user satisfaction;increased user satisfaction;collaborative filtering;response adaptive response;collaborative filtering experimental;response example", "pdf_keywords": ""}, "f46a3a5dc70a70292175e6c7ad505b8206cb070c": {"ta_keywords": "speech separation recognition;speech separation;second speech separation;step speech separation;recognition moving talker;speech fragments challenge;talker second recognition;speech recognition;separation recognition challenge;challenges step speech;collection speech fragments;speech fragments;speech recognition systems;separation recognition recognition;separation recognition;recognition medium vocabulary;moving talker second;second recognition medium;generation speech recognition;talker second;recognition recognition moving;power moving talker;recognition moving;speech;recognition medium;moving talker;step speech;second speech;collection speech;talker", "pdf_keywords": ""}, "1756376bf7cf0d0a7bec881d663b57907a361ecf": {"ta_keywords": "incremental editing structured;incremental tree edits;editing structured;modeling tree edits;editing structured data;incremental editing;tree edits implementing;model incremental editing;tree edits;tree edits directly;data structural edits;learning represent edits;structural edits;implementing entire editing;represent edits imitation;incremental tree transformations;tree structured data;incremental tree;edits imitation learning;tree structured;edits implementing;editor learns iteratively;edit encoder learning;novel edit encoder;represent edits;structural edits focus;proposed editor learns;generate incremental tree;data proposed editor;edits imitation", "pdf_keywords": "editing tree structured;incremental editing tree;incremental editing structured;editing structured;editing structured data;subtrees neural editor;incremental_tree_edit _we propose;abstract syntax trees;editing trees;neural editor editing;syntax trees;revision tree structured;editing sentences computational;editing tree;syntax trees computer;tree editor data;unsupervised editing trees;edits given tree;incremental editing;propose neural editor;tree editor;incremental_tree_edit;editing trees common;incremental_tree_edit _we;data neural editor;tree unsupervised editing;tree structured data;neural editor edit;data structural edits;editor algorithm based"}, "5eba3e525056cac6112cf0b13b62d86ba66661d9": {"ta_keywords": "accurately predict syntactic;predict syntactic features;active learning heuristics;active learning;active learning approach;syntactic features;predict syntactic;syntactic features proposed;existing active learning;syntactically distinct sub;based active learning;syntactically distinct;confusion syntactically distinct;reduce confusion syntactically;selecting instances optimally;syntactic;learning heuristics empirical;selecting instances;syntactically;outperforms existing active;method selecting instances;novel method selecting;instances optimally;distinct sub samples;learning heuristics;learning approach used;optimally reduce confusion;confusion syntactically;selecting;sub samples", "pdf_keywords": "active learning method;output tagsactive learning;confusion active learning;tagsactive learning;active learning;uncertain training instances;active learning al;oracle annotations trained;tagsactive learning al;learns predict tags;selecting uncertain training;level active learning;predicting semantics;novel active learning;method predicting semantics;predict tags efficient;annotations trained;predicting semantics new;annotations trained using;training instances annotating;accuracy oracle annotations;machine learning tasks;annotated tokens;annotation arbitrary words;prediction linguistic tasks;prediction linguistic;arbitrary machine learning;predict tags;languages using uncertainty;based inferred semantics"}, "84bc74d875e748aa0f11ac0c5e3000b16484b053": {"ta_keywords": "generating entailment preserving;generating entailment;entailment preserving;entailment preserving entailment;preserving entailment;preserving entailment withing;method generating entailment;entailment withing perturbations;entailment withing;entailment;generated instances;fever2 shared task;newly perturbed instances;instances common patterns;perturbed instances using;perturbed instances;instances;instances common;sample submission fever2;classification;newly generated instances;withing perturbations instances;perturbations instances common;patterns training;generated instances construct;instances construct sample;losses classification accuracy;patterns training data;common patterns training;absolute losses classification", "pdf_keywords": "fever2 shared task;generating adversarial instances;1fever2 baseline adversarial;adversarial instances;adversarial instances introduce;used generate adversarial;generate adversarial examples;adversarial dataset generated;generating adversarial;data adversarial instances;adversarial instances relatively;generate adversarial;robustness adversarial instances;adversarial;method generating adversarial;adversarial attack;adversarial dataset;adversarial examples;fever2 shared;simple adversarial;adversarial examples reduction;use adversarial;adversarial instances method;evaluate robustness adversarial;data adversarial;baseline adversarial dataset;iteration fever2 shared;baseline adversarial;simple adversarial attacks;adversarial attacks"}, "ecab8208e5182d4b3b0d6183928e816301d2366d": {"ta_keywords": "stochastic gradient descent;efficiently training linear;backward splitting sgd;splitting sgd empirically;gradient descent forward;stochastic updates;applying stochastic updates;gradient descent;stochastic gradient;efficiently training;update derive stochastic;training linear;stochastic updates nonzero;sgd empirically;splitting sgd;elastic net update;results stochastic gradient;derive stochastic gradient;forward backward splitting;sgd empirically validate;features dynamic programming;training linear model;subproblem computation update;elastic net;backward splitting;updates nonzero features;backward splitting fos;proper elastic net;computation update derive;dynamic programming algorithm", "pdf_keywords": "sparse learning forward;learn sparse optimization;efficient training sparse;training sparse;regularization method sparse;training sparse linear;sparse learning;sparse stochastic gradient;learn sparse;learning rate sparse;algorithm sparse learning;regularization stochastic gradient;sparse optimization;descent sde sparse;train regularized stochastic;train sparse stochastic;elastic net regularization;net regularization algorithms;hybrid regularization stochastic;regularization stochastic;novel sparse stochastic;sparse optimization arbitrary;used learn sparse;regularized stochastic;net regularization approach;stochastic gradient descent;regularized stochastic models;net regularization algorithm;net regularization;regularization algorithms"}, "02cc92287c6614b6a2aa982007471f16b3450013": {"ta_keywords": "problems natural language;natural language actions;natural language processing;addressing open problems;natural language;relation natural language;open problems;open problems natural;language actions plans;language processing;language processing discuss;introduce problem formulation;language actions;problem formulation allows;plans introduce problem;mentioned open problems;problem formulation enables;problem formulation;building dataset;problems natural;steps addressing open;introduce problem;actions plans introduce;building dataset enables;meaningful steps addressing;plans introduce;actions plans;addressing open;language;work building dataset", "pdf_keywords": ""}, "ddc502b6c0d08fefe5b77639e4737cd8c7bce25c": {"ta_keywords": "meaningful structure web;pages using similarity;structure web pages;recognizing meaningful structure;structure web;recognize meaningful structure;similarity information;using similarity information;web pages effective;meaningful structure;producing meaningful structure;similarity information method;method recognizing meaningful;using similarity;web pages using;web pages;meaningful structure context;meaningful structure half;similarity;pages effective;structure context unwrapper;recognizing meaningful;variety web pages;pages using;pages effective producing;structure;recognize meaningful;pages;anwrapper able recognize;unwrapper", "pdf_keywords": ""}, "ab42ad9698386cc15a30a8c7885fa82b260f537b": {"ta_keywords": "estimating location parking;estimate location parking;spot using bayesian;location given parking;location parking;location parking spot;given parking spot;parking spot method;parking spot estimate;parking spot using;probabilistic estimate location;parking spot;estimating location;given parking;method estimating location;bayesian markov process;parking;bayesian markov;data bayesian markov;spot estimate time;estimate location;bayesian approach;bayesian;data bayesian;bayesian approach method;spot estimate;markov process used;using bayesian approach;using bayesian;markov process", "pdf_keywords": ""}, "3abcd0ffc54c3a16c9dc5e5d3ea59eaa43070127": {"ta_keywords": "designing machine learning;create machine learning;standard machine learning;machine learning algorithms;learning algorithms framework;learning algorithms precluded;machine learning;algorithms precluded dangerous;learning algorithms experiments;learning algorithms;algorithms framework;algorithms precluded;algorithms;algorithms framework simplifies;algorithms experiments;regulating undesirable behavior;learning;propose general flexible;undesirable behavior use;dangerous behavior;specifying regulating undesirable;regulating undesirable;precluded dangerous behavior;undesirable behavior;framework designing machine;general flexible framework;designing machine;dangerous behavior caused;flexible framework;behavior", "pdf_keywords": "designing machine learning;learning algorithms resistant;learning algorithms safe;creating machine learning;algorithm predict students;create machine learning;algorithms machine learning;machine learning algorithms;machine learning discrimination;algorithm able predict;algorithm designed predict;learning algorithms called;learning algorithms designed;learning algorithm capable;learning algorithm designed;regression algorithm designed;data able predict;algorithm capable predicting;learning algorithms able;learning algorithms exhibit;learning produce regression;behavior machine learning;regression classification;learning algorithms precluded;behaved machine learning;learning algorithms framework;machine learning algorithm;learning algorithm agent;predict behavior;learning algorithms ensure"}, "37241cdc693b9c2daf49557f18c1ad6a15247239": {"ta_keywords": "document binarization;new document binarization;document binarization scheme;state art binarization;binarization;binarization scheme;binarizing;art binarization;thresholding method proposed;consistently binarizing;binarization scheme intended;thresholding;niblack thresholding method;binarization solutions;binarizing range;niblack thresholding;art binarization solutions;consistently binarizing range;thresholding method;color document images;popular niblack thresholding;binarizing range degraded;based segmentation;based segmentation applied;segmentation;degraded color document;intended consistently binarizing;algorithm based segmentation;document images proposed;document images", "pdf_keywords": ""}, "f66a17836380c0c79c1b42a9219cf8fde6524287": {"ta_keywords": "entity labeled corpus;study entity occurrence;entity occurrence;entity occurrence model;labeled corpus model;entity labeled;learned entity labeled;common source search;occurrence model learned;corpus model used;labeled corpus;corpus model;occurrence model;source search engine;source search;corpus;occurrence;learned entity;search engine;model learned entity;entity;study entity;case study entity;search;labeled;common source;questions posed common;common;questions posed;posed common source", "pdf_keywords": ""}, "a817785f0100f3fadc5c1203974d151d5b093310": {"ta_keywords": "parallel programming library;performance parallel programming;parallel programming;rikvold parallel computer;traditional parallel programming;performance parallel;rikvold traditional parallel;sarkar rikvold parallel;rikvold parallel;comparison performance parallel;parallel computer;programming library developed;programming library;node performance parallel;traditional parallel;parallel computer single;parallel;programming library evaluated;developed sarkar rikvold;library developed sarkar;sarkar rikvold;node performance;single node performance;library evaluated comparing;comparison performance;programming;comparing performance;rikvold;library developed;sarkar rikvold traditional", "pdf_keywords": ""}, "9336a2ff833d0b4bc914e2282ad04e19d27bc2be": {"ta_keywords": "copper ground grid;difference ground grid;steel ground grid;ground potential difference;ground potential rise;ground grid;potential difference ground;ground grid calculation;rise ground potential;ground potential;potential rise ground;ground grid significant;ground grid points;calculation ground potential;potential difference copper;flat steel ground;copper ground;shows ground potential;difference ground;grid calculation 220kv;potential difference flat;results copper ground;flat steel grid;potential difference;steel grid related;steel ground;calculation ground;calculation shows ground;difference copper flat;reducing potential difference", "pdf_keywords": ""}, "4d5f9a0aba65ba6294c543ba5e6108e6d690f133": {"ta_keywords": "domain learning;propose domain learning;domain learning algorithms;data learn extractors;domain dependent data;learn extractors generalize;learn extractors;extractors generalize section;extractors generalize;data domains comparative;dependent data learn;domain dependent;classes data domains;predict shape document;domains comparative;learning algorithms;data learn;learning algorithms able;extractors;data domains;domains;conditional distributions classes;domains comparative user;shape document presence;learning;distributions classes data;different sections document;document presence domain;document;domain", "pdf_keywords": ""}, "c69da8266e2f3f67febf22b8f2bf91623346d283": {"ta_keywords": "vaccination prevalent tweets;tweets earliest pandemic;websites shared tweets;patterns vaccination inauthentic;topic vaccination prevalent;vaccination inauthentic propagation;shared tweets period;topic vaccination;prevalent tweets;prevalent tweets earliest;days topic vaccination;common patterns vaccination;vaccination prevalent;vaccination inauthentic;shared tweets;patterns vaccination;tweets earliest;tweets period february;tweets period;tweets;vaccination;earliest pandemic bursty;pandemic bursty;pandemic bursty pattern;websites shared;pandemic;majority websites shared;earliest pandemic;network majority websites;websites shared days", "pdf_keywords": ""}, "cd9bfa6266cab4bf4b04c82746a5b650f83b57e4": {"ta_keywords": "non convex optimization;convex optimization problems;efficiently global minimizer;algorithms non convex;convex optimization;guarantees optimization algorithms;non convex problems;guarantees optimization;global minimizer exploiting;performance guarantees optimization;optimization algorithms non;global minimizer;general non convex;convex problems;minimizer exploiting;non convex;convex problems solved;minimizer exploiting structure;optimization algorithms;efficiently global;optimization problems;global performance guarantees;optimization;solved efficiently global;optimization problems start;minimizer;classes non convex;convex;algorithms non;performance guarantees", "pdf_keywords": "optimization principle supergravity;non convex optimization;gradient nonconvex optimization;gradient free optimization;convex optimization methods;convex optimization problems;optimization convex optimization;gradients convex optimization;learning non convex;convex optimization;supergravity known optimal;method convex optimization;convex optimization principle;optimization convex;optimizationin paper introduce;convex optimization method;smooth optimization;convex optimization order;convex optimization main;bound nonconvex optimization;smooth optimization problem;convex optimization motivated;gradient optimization problems;nonconvex optimization methods;convex optimization linear;smooth optimization large;strongly convex optimization;convergence nonconvex optimization;convex optimization technique;convex optimization does"}, "d385d8563192569b229bde762fcd4d57ce2b3ee2": {"ta_keywords": "automatically generating definitions;generating definitions technical;definitions technical terms;large corpus definitions;definitions technical;generating definitions;technical terms domain;definitions shared;corpus definitions;corpus definitions scaled;definitions;automatically generating;technical terms;corpus;terms domain;set definitions shared;method based language;large corpus;terms domain method;method automatically generating;definitions scaled;scaled large corpus;language model designed;set definitions;definitions shared user;essential features domain;definitions scaled set;based language model;language model;features domain method", "pdf_keywords": ""}, "63d7e40da7f0d37308b8e97fca4a14a26a6b52ea": {"ta_keywords": "incentivizing data excellence;data excellence;data practices high;data cascades prevalent;data quality;data practices;data quality discuss;impact data quality;incentivizing data;report data practices;designing incentivizing data;data cascades;data excellence class;data;evidence data cascades;high stakes ai;evidence data;ai resulting safer;ai;report data;empirical evidence data;stakes ai interviews;ai interviews 53;citizen ai;ai interviews;citizen ai resulting;ai resulting;stakes ai;impact data;empirical evidence", "pdf_keywords": "data practices artificial;data practices high;poor data practices;data high stakes;data quality critical;data practices;study data quality;challenges data quality;data critical;implications data quality;data curating data;data quality development;data quality data;data quality report;data appropriators;data curating;data quality;data quality challenges;data quality associated;data practices challenges;data good;data beneficial;data propose;data social scientific;data quality implications;data informative beneficial;data use;data use data;quality data;data management curating"}, "7707af52b3e19bfd3fc07c2be5aed044e5d7953a": {"ta_keywords": "networks trained persistent;deep networks persistent;persistent persistent connectivity;persistent connectivity persistent;trained persistent persistent;networks persistent;trained persistent;persistent connectivity achieved;persistent connectivity;connectivity persistent connectivity;demonstrate persistent connectivity;persistent connectivity originals;connectivity persistent;preserves connectivity;preserves connectivity originals;networks persistent filtration;functions trained persistent;persistent persistent;persistent persistent persistent;persistent persistent filtration;experimentally demonstrate persistent;validating deep networks;persistent;filtration preserves connectivity;networks trained;combining persistent persistent;connectivity originals experimentally;connectivity achieved training;demonstrate persistent;persistent filtration preserves", "pdf_keywords": "topological features deep;topological features images;topological loss;existing topological loss;topological loss functions;trained persistent homology;lossy topological;recognition topological features;recognition topological;topological classification;topological classification maps;hidden topological features;topological features;topological features robust;topological nontopological loss;images network predicts;homology based loss;topological descriptor;disconnections deep networks;images neuronal;approach topological classification;global topological descriptor;propagation topological features;features deep;networks neuronal;extracted topological features;training deep;detect topological;lossy topological constraints;topological mask captures"}, "f74ccbc8988b7f0b847c480d4e8bea3082f4f931": {"ta_keywords": "search optimal strategy;optimal strategy game;learn optimal strategy;generative model search;strategy use stochastically;optimal strategy;search optimal;strategy game;efficiently search optimal;player selection based;strategy game player;optimal strategy use;game player selection;player selection;learn optimal;optimal strategy proposed;models learn optimal;approach search optimal;generative models learn;model efficiently search;generative models;selection based;generative model;use generative models;strategy;selection;train generative model;train generative;strategy proposed;model search process", "pdf_keywords": ""}, "3f5b7fcb6fc50ba80318ab959f3d63253cd0ef6b": {"ta_keywords": "acoustic event detection;acoustic event;proposes acoustic event;events classification;event detection;event detection aed;interdependence events classification;aed classifier chains;detection event proposed;detection aed classifier;events classification performs;binary detection event;detection event;classifier chains;classifier chains new;chains new classifier;aed classifier;recordings dataset;classifier based probabilistic;iterative binary detection;binary classifiers;real recordings dataset;multiple binary classifiers;recordings dataset demonstrate;binary detection;new classifier based;classifier;event proposed method;recordings;acoustic", "pdf_keywords": "acoustic event detection;classify acoustic events;sound event detection;acoustic events based;acoustic activity phase;method acoustic event;classify acoustic;method classify acoustic;sound events;sound event;acoustic events;estimation acoustic activity;acoustic scene classification;acoustic event;proposes acoustic event;interdependence events classification;event detection;event detection based;events classification proposed;events classification;classify events;acoustic activity;controlled sound event;classify events according;sound events 2017;recording events;event detection aed;detection event iteration;speaker diarization based;method estimation acoustic"}, "e6beab7c192d7fb04c8bfb0886464fd719cd3421": {"ta_keywords": "machine learning deployments;predicting accuracy machine;accuracy machine learning;predicting accuracy;model prediction accuracy;prediction accuracy;learns threshold model;learns threshold;machine learning model;atc learns threshold;threshold model confidence;method predicting accuracy;thresholded confidence atc;confidence atc learns;machine learning;prediction accuracy exceeds;confidence model prediction;thresholded confidence;labeled source data;performance machine learning;unlabeled target data;model prediction;using labeled source;data unlabeled target;model using labeled;novel method predicting;average thresholded confidence;model confidence;labeled source;source data unlabeled", "pdf_keywords": ""}, "d25c4bf23b4b951f2417e4a8a44574c99608e9d7": {"ta_keywords": "linear chirplet transform;frequency analysis seismic;chirplet transform proposed;analysis seismic data;time frequency analysis;chirplet transform;linear chirplet;chirplet transform general;general linear chirplet;seismic data;seismic data used;frequency analysis;analysis seismic;frequency analysis method;time frequency;novel time frequency;approach time frequency;chirplet;seismic;frequency;transform general linear;transform proposed method;distribution identify features;general linear cascade;features data proposed;linear cascade;transform proposed;linear cascade cascades;identify features data;data proposed method", "pdf_keywords": ""}, "4a160efbe80c38cd5eb2f92c7c095b49b113397d": {"ta_keywords": "code generation retrieval;code natural language;code generation;functionality ieeetran cls_;learning performance code;retrieval context ieeetran;functionality ieeetran;natural language queries;performance code generation;plugin ieeetran cls_;ieeetran cls_;plugin ieeetran;creation new code;language queries;ieeetran;provide plugin ieeetran;generation retrieval;language queries use;ieeetran cls_ provide;context ieeetran cls_;generation retrieval context;machine learning performance;orchestrate functionality ieeetran;new code natural;queries use plugin;code natural;code;context ieeetran;natural language;new code", "pdf_keywords": "code retrieval engine;code generation retrieval;python code generation;code natural language;generation code retrieval;python code snippets;code snippets python;snippets generator retrieval;generate code snippets;code generation engine;code retrieval;natural language code;python code search;code retrieval plugin;developing code python;ide code snippets;retrieval code snippets;code generation ide;code snippet generation;code snippets generator;code search engine;code retrieval investigate;snippets python;code python language;code retrieval results;natural language snippets;code retrieval model;nl2code generation retrieval;code snippets ideally;snippets python framework"}, "e7ce1b01d2928514710bba044ac2af758c975d99": {"ta_keywords": "quality selfish routing;selfish routing game;equilibrium quality selfish;selfish routing;costs network congestion;equilibrium overestimation costs;congestion costs multiplicative;estimates congestion costs;routing game;congestion costs;network congestion;congestion decreases equilibrium;estimates congestion;routing game types;equilibrium overestimation;equilibrium quality;estimate costs network;type estimates congestion;users estimate costs;network congestion decreases;costs network;equilibrium;congestion;decreases equilibrium overestimation;overestimation costs;routing;quality selfish;study equilibrium quality;congestion decreases;users estimate", "pdf_keywords": "routing networks uncertainty;routing game uncertainty;multicommodity selfish routing;selfish routing networks;networks uncertain users;dynamics selfish routing;networks uncertainty users;performance selfish routing;commodity selfish routing;consider selfish routing;congestion estimation uncertain;uncertainties multicommodity selfish;selfish routing game;uncertainty affects congestion;uncertainty underlying network;selfish routing consider;routing networks uncertain;uncertainty decreases congestion;networks uncertainty;aversion selfish routing;selfish routing;commodity game uncertainty;networks uncertain parameters;estimation uncertain users;routing networks congestion;uncertainty multi commodity;uncertainty equilling multi;cost uncertainty theorem;multicommodity routing game;equilibrium networks uncertain"}, "caf40157a7a1d72ae3a6946169c992d8c973b743": {"ta_keywords": "gingival regression;gingival regression model;models gingival regression;gingival regression problem;diagnosis gingival diseases;gingival diseases associated;gingival diseases;important diagnosis gingival;diagnosis gingival;models gingival;popular models gingival;gingival;disease epidemiological models;epidemiological models;dental problems;regression model;epidemiological models aim;dental;regression problem arises;regression;cell division dental;disease epidemiological;dental problems problem;diseases associated;dental implants dental;regression problem;diseases associated early;implants dental;diseases;early stage diseases", "pdf_keywords": ""}, "1b57ffe73ae95f339015c174ec574b59f99ea553": {"ta_keywords": "approach preservation features;subset features preserved;based perturbation maximization;features preserved;preservation features;perturbation maximization;preservation features presence;search subset features;gradient based perturbation;features preserved characterized;preserved characterized gradient;perturbation maximization method;subset features;feature space approach;characterized gradient based;gradient based;subset input feature;maximization method discovers;features;uncertainties inherent search;feature;approach robust;based search subset;approach robust uncertainties;feature space;features presence;based perturbation;space approach robust;robust uncertainties;approach preservation", "pdf_keywords": ""}, "c6048cd0b1368be0e62633ef723f9d691323102c": {"ta_keywords": "speaker clustering experiments;applied speaker clustering;speaker clustering;multiscale mixture model;gibbs sampling proposed;blocked gibbs sampling;gibbs sampling;mixture model proposed;continuous multiscale mixture;mixture model;sampling based model;proposed sampling based;multiscale mixture;sampling based methods;clustering experiments nonstationary;noise proposed sampling;sampling based;sampling proposed estimating;estimating continuous multiscale;sampling proposed;conventional sampling based;proposed sampling;sampling method based;estimation improved clustering;method applied speaker;novel sampling method;sampling;conventional sampling;clustering experiments;compared conventional sampling", "pdf_keywords": ""}, "40e292d16168fcb8ac87c20682b827ad17a999dd": {"ta_keywords": "predicting user experience;predict user experience;user experience index;user experience based;assisted learning mcao;experience index entropic;learning mcao framework;index user experience;learning mcao;method predicting user;experience based use;predicting user;machine learning assisted;user experience;mcao framework;experience index;able predict user;mcao framework method;use machine learning;predict user;novel method predicting;experience based;method predicting;mcao;method able predict;based entropic index;machine learning;assisted learning;learning assisted;predicting", "pdf_keywords": ""}, "6332d5bb0e6af89471ffc6157e3816c029b3ae83": {"ta_keywords": "electron gas strong;field electron gas;electron gas heated;gas strong magnetic;electron gas excited;dimensional electron gas;electron gas;magnetic field electron;heated strong magnetic;behavior dimensional electron;strong magnetic field;excited transverse magnetic;strong magnetic;gas excited transverse;dimensional electron;gas heated strong;field electron;electron;transverse magnetic;magnetic field;transverse magnetic field;gas strong;gas excited;magnetic;gas heated;excited transverse;heated strong;gas;dynamic behavior dimensional;dynamic behavior", "pdf_keywords": ""}, "8b3c0dd95167d4d63161038493a691ee5cdc76b3": {"ta_keywords": "monolingual sentence matching;sentence matching based;sentence matching;matching based convolutional;able score similarity;monolingual sentence;score similarity;score similarity competitive;convolutional neural;similarity;hand aligned dataset;similarity competitive;convolutional neural networks;novel method monolingual;monolingual;similarity competitive manner;high performance aligning;method monolingual sentence;based convolutional neural;matching based;aligned dataset;matching;aligned dataset model;trained knowledge based;model trained knowledge;hand aligned;trained knowledge;convolutional;performance aligning;neural", "pdf_keywords": "processing text simplification;text simplification challenge;constructions text simplification;natural language processing;text simplification operation;corpus natural language;text simplification;simplification make texts;extraction phrase level;sentence alignment wikipedia;remains text simplification;matches large corpus;large corpus natural;combines similarity sentences;modify existing corpus;automatic text processing;text processing;phrase level matches;sentences natural language;text simplification make;operation natural language;estimate similarity sentences;extract phrase level;text processing text;sentence level alignment;corpus human text;corpus natural;existing corpus;similarity sentences natural;large corpus"}, "e4de1009eb7b3524bf7d19bdcebced80035a47cf": {"ta_keywords": "asynchronous neighbor discovery;neighbor discovery protocol;energy asynchronous neighbor;asynchronous neighbor;neighbor discovery;asynchronous group testing;discovery protocol;neighbors network nodes;discovery protocol internet;neighbors network;asynchronous group;novel asynchronous group;efficient energy asynchronous;iot relax assumption;network nodes codeword;active neighbors network;protocol internet things;active neighbors;synchronization;neighbors;energy asynchronous;things iot;iot;neighbor;internet things iot;things iot relax;iot relax;synchronization formulate;group testing scheme;set active neighbors", "pdf_keywords": ""}, "dbb159b288930c6be32c2d5b91373ca1e341e633": {"ta_keywords": "stereo based speech;dictionary learning proposed;dictionary learning;using dictionary learning;speech feature using;speech feature;based speech feature;speech processing proposed;enhancement stereo based;enhancement stereo;feature using dictionary;speech processing;stereo based;approach enhancement stereo;stereo;scale speech processing;learning proposed;large scale speech;using dictionary;learning proposed approach;scale speech;represented mixture inputs;based speech;mixture inputs signal;new approach enhancement;dictionary;inputs signal;mixture inputs;speech;inputs", "pdf_keywords": ""}, "2f3ec666ba50c6a9ce74abad6a5127ea38a05bca": {"ta_keywords": "repair distributed storage;storage repair distributed;distributed storage nodes;simultaneous storage repair;storage nodes;storage nodes nodes;distributed storage;storage repair;node repair erasure;repair distributed;repair erasure decoding;node repair;storage space repair;simultaneous storage;simultaneous minimization storage;minimization storage;nodes nodes encoded;space repair bandwidth;problem node repair;nodes encoded;minimization storage space;storage;framework simultaneous storage;erasure decoding;repair erasure;repair bandwidth;nodes encoded using;storage space;erasure decoding constituent;network error detection", "pdf_keywords": "codes distributed storage;code distributed storage;message distributed storage;regenerating codes distributed;existing distributed storage;distributed storage;storage networks framework;distributed storage networks;storage networks;distributed storage network;data storage network;storage repair bandwidths;storage network way;codes distributed;storage network;twin code distributed;bandwidths regenerating codes;storage network introduced;coding node capable;erasure codes capable;storage medium approach;data storage;based erasure codes;regenerating codes framework;storage network paper;availability associated coding;regenerating codes;coding node;capable storing data;simultaneous minimization storagea"}, "8f2182846d5d4cfbc216b5e4c00411e021dc4776": {"ta_keywords": "classify 128 diagnoses;measurements model trained;trained simple empirical;engineered features trained;features trained;features trained model;model trained;irregularly sampled clinical;multilayer perceptron trained;trained model;sampled clinical measurements;128 diagnoses;trained model outperforms;perceptron trained;sampled clinical;novel method training;clinical measurements model;128 diagnoses given;model trained simple;demonstrate trained model;training model;training model classify;trained model used;multilayer perceptron;diagnoses;model classify 128;including multilayer perceptron;trained simple;successfully classify;model classify", "pdf_keywords": "recurrent neural networks;trained sparse clinical;weights recurrent nets;feature recurrent neural;recurrent nets;recurrent nets achieve;recurrent neural network;short term memory;train recurrent neural;recurrent nets use;novel feature recurrent;feature recurrent;network rnn classify;recurrent neural;replication recurrent nets;rnn classify;sparse clinical data;trained sparse;neural network rnn;deep learning;based recurrent neural;memory neural multilayers;classify 128 diagnoses;uses recurrent neural;memory neural;diagnoses based long;len trained sparse;weights recurrent;clinical data train;predict simultaneously diagnoses"}, "bcffee102a99f726ddfe765906babb01b8226269": {"ta_keywords": "acceleration collision particles;dynamics collision particles;particle acceleration collision;collision particles collision;collision particles;processes collision particles;particles collision;particles collision particles;study dynamics collision;dynamics collision;collision particles common;acceleration collision;phenomenon collision;common phenomenon collision;phenomenon collision driven;collision driven processes;particle acceleration;mechanism particle acceleration;collision;processes collision;collision particles key;driven processes collision;collision driven;particles common phenomenon;particles;dynamics;particle;particles common;acceleration;understanding mechanism particle", "pdf_keywords": ""}, "322ef476e90a487c8f9797bece7799b69af9e5c1": {"ta_keywords": "coded matrix multiplication;product coded matrix;coded matrix;dimensional coded matrix;multiplication product coded;coded computation;coded computation scheme;matrix multiplication analysis;new coded computation;matrix multiplication context;matrix multiplication product;matrix multiplication;high dimensional coded;multiplication context distributed;distributed computing propose;distributed computing;product coded scheme;dimensional coded;multiplication analysis reveals;product coded;coded scheme achieves;multiplication analysis;coded scheme;product product coded;based product coded;coded;multiplication product;matrix;propose new coded;multiplication", "pdf_keywords": ""}, "822395760906f4940df68aa33925b6bf9123bac2": {"ta_keywords": "extract power data;measuring power data;power data data;power data set;power data;detecting measuring power;method extract power;use extract power;data data set;extract power;measuring power;data data;data set;data set used;analysis data;data set use;data set method;used extract power;analysis data presence;data;data presence noise;based analysis data;detecting measuring;source capable detecting;noise source;noise source capable;capable detecting measuring;data presence;set used extract;method used extract", "pdf_keywords": ""}, "24d28783f6061bd1e91fb60417ac8b3646305a49": {"ta_keywords": "automatically sequencing component;information extraction systems;automatically sequencing;sequencing component;control automatically sequencing;information extraction;sequencing component level;blackboard communication components;like classification extraction;large scale information;scale information extraction;systems presented architecture;architecture large scale;sequencing;tasks like classification;extraction feature computation;blackboard communication;classification extraction;extraction systems presented;extraction systems;component level tasks;declarative control automatically;classification extraction feature;components declarative control;presented architecture based;communication components declarative;communication components;components declarative;architecture based;weight blackboard communication", "pdf_keywords": ""}, "c8648d04f52e49167d1a4443a4830709cf3331ff": {"ta_keywords": "defender complexity model;defender complexity;theoretic model defender;model defender complexity;computing optimal strategies;game theoretic model;optimal strategies;game theoretic;strategy target complexity;optimal strategies context;assigning suitable strategy;security deployment game;deployment game theoretic;model defender;suitable strategy target;strategy target;computing optimal;optimal;strategies context security;target complexity;problem computing optimal;defender;target complexity model;strategy;suitable strategy;strategies;security;security deployment;deployment game;strategies context", "pdf_keywords": "game optimal polynomial;strategy polynomial time;optimal strategy polynomial;game polynomial time;games optimal strategy;game optimal;strategy polynomial;computing optimal strategies;games optimal;strategy game optimal;optimal strategies player;strategies correspond allocations;games arbitrary complexity;stackelberg model games;optimal strategy follower;homogeneous optimal strategy;optimal polynomial time;model games optimal;programs optimal strategy;optimal strategy;solving security games;finding optimal strategy;optimal strategies;game polynomial;polynomial time target;polynomial programs;bipartite optimal strategy;sum game defender;linear programs polynomial;finding strategy game"}, "651468a69da74dab716cebbd179a5cbb8e672c14": {"ta_keywords": "learns demonstrations arbitrary;sparse reinforcement learning;algorithm learns demonstrations;learns demonstrations;novel sparse reinforcement;sparse reinforcement;learns demonstrations far;demonstrations training algorithm;demonstrations far optimal;learning reinforcement;learning reinforcement learning;reinforcement learning reinforcement;reinforcement learning;demonstrations training;demonstrations arbitrary;demonstrations arbitrary setting;reinforcement learning algorithm;replay buffer demonstrations;demonstrations guaranteed fully;influence demonstrations training;reinforcement;learns;demonstrations guaranteed;realizable algorithm learns;algorithm learns;based initializing replay;buffer demonstrations guaranteed;novel sparse;demonstrations;initializing replay", "pdf_keywords": "demonstrations learning;suboptimal demonstrations learning;incrementally learning demonstrations;demonstrations learning useful;learning useful demonstrations;learning demonstrations;learning suboptimal demonstrations;exploiting demonstrations sparse;method learning demonstrations;demonstrations sparse environments;demonstrations algorithm learn;imitation learning silfd;trained suboptimal demonstrations;algorithm learn demonstrations;demonstrations expert imitation;demonstrations selection training;imitation learning;imitation learning reinforcement;demonstrations sparse;expert imitation demonstrations;learning demonstrations ldfd;self imitation learning;learn demonstrations;learning demonstrations thatwe;learn demonstrations noisy;imitation demonstrations rewards;sparse environment demonstrations;demonstrations selection;useful demonstrations algorithm;imitation demonstrations"}, "1cb7015c0a8015c65844876459809ecac917ec02": {"ta_keywords": "multi channel acoustic;channel acoustic model;channel acoustic;acoustic model single;input multi channel;trained multi channel;multi channel trained;multi channel input;speech enhancement se;speech enhancement;single multi channel;acoustic model;multi channel;channel input;tal speech enhancement;channel input branches;acoustic;single channel;randomly multi channel;channel trained;heterogeneous input multi;novel heterogeneous input;pipeline trained multi;channel randomly multi;single channel randomly;trained multi;starting single channel;input multi;heterogeneous input;proposed training pipeline", "pdf_keywords": ""}, "899055ad2f0863cf1931c41f04da8b1dd7382607": {"ta_keywords": "levels lipid variability;visit variability lipid;lipid variability variability;lipid variability;variability lipid profiles;a1c levels lipid;variability lipid;hemoglobin a1c levels;lipid profiles risk;lipoprotein cholesterol ldl;levels lipid;density lipoprotein cholesterol;low density lipoprotein;nonhigh density lipoprotein;lipoprotein cholesterol nonhigh;lipoprotein cholesterol;lipid profiles;cholesterol ldl nonhigh;relationship hemoglobin a1c;lipoprotein cholesterol non;cholesterol nonhigh density;cholesterol ldl;density lipoprotein;hemoglobin a1c;a1c levels visit;cholesterol non hdl;risk factors cardiovascular;cholesterol nonhigh;lipoprotein;factors cardiovascular disease", "pdf_keywords": ""}, "173c73077a421680f12576524e85dff4b890c17e": {"ta_keywords": "distance projected distributions;wheeler vernet theorem;distributions maximized projection;projected distance function;computing projected distance;projected distance;projection distance projected;maximized projection distance;projection distance;distance projected;projected distributions maximized;projected distributions;projected distributions present;theorem states distance;states distance projected;method computing projected;maximized projection;vernet theorem;vernet theorem states;algorithm projected;algorithm projected version;computing projected;numerical algorithm projected;wheeler vernet;distributions maximized;distance;projected;projection;celebrated wheeler vernet;projected version", "pdf_keywords": "distance projected distributions;distances empirical distributions;quantification distances empirical;projected wasserstein distance;wasserstein distance sample;robust wasserstein distance;wasserstein distance projection;distances empirical;computing distance samples;wasserstein distance proposed;considering distances empirical;kernel projected wasserstein;kernels nonparametric sample;uncertainty quantification distances;wasserstein distance;dimensional testing uncertainty;reproducing kernel space;projection robust wasserstein;empirical ksp distance;distance samples;quantification distances;projected distributions;deep kernels nonparametric;tests based projection;distance projection highly;dimensionality reduction uncertainty;distance samples given;distance projected;high dimensional testing;kernels nonparametric"}, "4ae15dbb068cc962b39dca07d87b22fe5dcd5f6a": {"ta_keywords": "grid operations privacy;privacy generation transmission;adversary infer private;smart grid analyze;electricity smart grid;private parameter data;problem privacy generation;infer private parameter;smart grid operations;privacy metric performance;privacy generation;smart grid;private parameter;new privacy metric;privacy metric;privacy;problem privacy;operations privacy;privacy consumers;new privacy;privacy consumers introduce;tradeoff smart grid;operations privacy consumers;considers problem privacy;introduce new privacy;able infer private;infer private;electricity smart;transmission electricity smart;private", "pdf_keywords": "estimate privacy energy;privacy energy consumption;estimating privacy energy;analyze privacy energy;privacy generation electricity;privacy energy;privacy aware load;grid operations privacy;data privacy energy;privacy energywe consider;privacy performance grid;privacy energywe;consumption smart grid;smart grid smart;smart grid;finally privacy energywe;smart grids;propose model privacy;estimate privacy;model privacy;estimate privacy network;model privacy aware;grid smart grid;privacy consumers proposes;smart grid operations;power consumption smart;smart grids relies;consumption data privacy;privacy metric help;privacy metric based"}, "513552a56668279d6cd0857a4399fe8a63d92145": {"ta_keywords": "condensate thermalization lorentz;chiral condensate thermalization;phase chiral condensate;thermalization lorentz violating;lorentz force thermalization;force thermalization lorentz;chiral condensate;violating phase chiral;thermalization lorentz;lorentz violating phase;phase chiral;condensate thermalization;violating phase strongly;lorentz violating;chiral;force thermalization;violating phase affected;violating phase;phase strongly;phase strongly affected;effect lorentz force;lorentz force;condensate;presence lorentz force;thermalization;effect lorentz;lorentz;presence lorentz;phase affected;affected presence lorentz", "pdf_keywords": ""}, "8113f05360c6483e52b3e261fc9efce671e0aaa6": {"ta_keywords": "unsegmented speaker recordings;recognition unsegmented speaker;trained separation diarization;overlapping speech diarization;separation diarization recognition;speaker recordings combines;speech diarization;speaker recordings;unsegmented speaker;diarization recognition components;diarization recognition;separation diarization;overlapping speech;modular recognition unsegmented;recordings combines independent;recognition unsegmented;problem overlapping speech;recordings combines;independent trained separation;trained separation;recognition components;recognition components study;modular recognition;recordings;propose modular recognition;recognition;diarization;speaker;combines independent trained;speech", "pdf_keywords": "separation speech sources;trained separation diarization;speaker separation diarization;unsegmented speech recordings;method separation speech;recognition unsegmented conversations;separation diarization automatic;separation diarization recognition;speaker separation;unsegmented conversations combines;diarizing unsegmented speech;pipeline speaker separation;separation speech;speech recordings;trained clean utterances;speech recognition;speech recognition asr;speech recordings based;speech processing;joint speech recognition;independently trained separation;trained separated audiowe;automatic speech;unsegmented conversations;speech recordings best;novel sequential separation;overlapping speech;diarization automatic speech;unsegmented speech;trained separation"}, "29437d98b9e6f45bef7029f3ce1237b8b284464f": {"ta_keywords": "data text generation;text generation;text generation style;hybrid attention copy;sentence automatic adaptions;attention copy;generation style approach;data text;text;hybrid attention;enhanced new content;generation style;content;templates learn imitate;attention;learns weak supervisions;attention copy mechanism;sentence automatic;soft templates learn;new content;copy mechanism learns;develop hybrid attention;weak supervisions enhanced;supervisions enhanced;supervisions enhanced new;content record;templates learn;adaptions faithfully content;given sentence automatic;problem data text", "pdf_keywords": ""}, "d63edef60d674408819bb015b64b7f42470e151b": {"ta_keywords": "druglike molecules model;protein binding pocket;molecules model designed;generation druglike molecules;molecules model;molecule design;protein binding;target molecules characterized;molecules characterized 3d;denovo molecule design;pocket conditional rnn;molecule design problem;druglike molecules;denovo molecule;target molecules;binding pocket descriptor;information protein binding;rnn crnn model;3d information binding;conditional rnn;novel generative model;applied denovo molecule;molecule;conditional rnn crnn;molecules characterized;problem target molecules;generative model;structural information protein;crnn model control;binding pocket conditional", "pdf_keywords": "models protein binding;prediction protein binding;describing binding proteins;predict structure molecules;prediction structure molecules;protein structures binding;protein binding pocket;trained binding proteins;descriptor protein binding;model novo molecule;model proteins specific;model proteins;model generation protein;protein binding pockets;models protein;binding pocket protein;binding proteins;molecules generative model;protein binding;proteins specific binding;novo molecule design;proteins generative models;knowledge protein binding;protein binding highly;structures binding affinity;structure information protein;generation protein structures;pocket information molecular;protein structures based;pocket identified protein"}, "1ddf9d306ae27113f55ea3d4eee12c8441235656": {"ta_keywords": "hybrid josephson junction;resonators josephson junction;josephson junction coupled;josephson junction coupling;input resonators josephson;mode waveguide hybrid;resonators josephson;study hybrid josephson;josephson junction;hybrid josephson;waveguide hybrid;single mode waveguide;waveguide resonators coupled;waveguide hybrid composed;mode waveguide resonators;mode waveguide;waveguide mode;waveguide mode waveguide;waveguide resonators;mode waveguide mode;coupled resonators single;resonators single mode;composed coupled resonators;junction coupling;waveguide;junction coupling controlled;coupled resonators;junction coupled single;resonators coupled;junction coupled", "pdf_keywords": ""}, "b03feec6f5b898484fdfdc3cd12f084afbe77036": {"ta_keywords": "pairs rank learning;rank learning pairwise;document pairs rank;rank learning;learning pairwise preferences;outlying document pairs;pairs rank;learning pairwise;pairwise preferences introduce;meta learning algorithm;meta learning;pairwise preferences;document pairs;new meta learning;rank;effects outlying document;outlying document;preferences introduce;studies effects outlying;learning algorithm;learning algorithm suppresses;pairwise;preferences;pairs;learning;statistically significant performance;outlying;effects outlying;preferences introduce new;paper studies effects", "pdf_keywords": ""}, "f388c2be45e4415fcb59cf43a3b29463cf7e7940": {"ta_keywords": "fact checking assessment;fact checked journalists;fact checking relates;fact checking;statements fact checked;discuss fact checking;task fact checking;natural language processing;truthfulness claim construction;using statements fact;statements fact;checking assessment truthfulness;checked journalists;fact checked;assessment truthfulness claim;natural language;mainstream natural language;truthfulness claim;checked journalists available;assessment truthfulness;language processing tasks;task fact;language processing;introduce task fact;checking relates mainstream;journalists;dataset using statements;checking relates;statements;addressed discuss fact", "pdf_keywords": ""}, "9352dfd127dcfce8013eb350e0229cc72b9bd203": {"ta_keywords": "attention augmented attention;augmented attention;super resolution;augmented attention ss;self attention augmented;attention augmented;resolution ss panchromatic;method super resolution;panchromatic images;high resolution;reconstruct high resolution;resolution high projected;ss panchromatic images;panchromatic images based;high resolution high;resolution high resolution;attention;high projected image;super resolution ss;self attention;resolution high;fully convolutional network;attention ss mechanism;features reconstruct high;scale features reconstruct;achieved convolutional;fully convolutional;achieved convolutional network;resolution;attention ss", "pdf_keywords": ""}, "4104d632d1cff0c9314cde344e2b1da06e662c5b": {"ta_keywords": "semi supervised loss;speech text loss;text loss asr;new semi supervised;supervised loss method;supervised loss;loss asr loss;asr loss provide;loss asr;semi supervised;asr loss;text loss;speech text modalities;speech text;differentiable speech text;data quantity speech;end differentiable speech;quantity speech text;corpora;corpora provide extensive;corpora provide;differentiable speech;loss method;loss method combining;gains corpora;new semi;gains corpora provide;consistent gains corpora;loss;text modalities", "pdf_keywords": ""}, "7e43dad7fbae3a7db47adc6b89c76acbd2fb225f": {"ta_keywords": "design instructional video;instructional video videos;instructional articles design;structure instructional articles;articles design instructional;instructional articles;videos method;video videos method;instructional video;videos method based;hierarchical structure instructional;video videos;instructional articles publicly;structure instructional;110k instructional articles;videos;wiki project website;method based wiki;design instructional;using wiki;instructional;based wiki project;procedures using wiki;wiki project;wiki compared;using wiki compared;wiki;video;based wiki;wiki compared methods", "pdf_keywords": "procedures wikihow corpus;hierarchical knowledge base;procedures based wikihow;wikihow corpus;domain hierarchical knowledge;simple hierarchical knowledge;resulting knowledge base;procedures based wiki;knowledge base;outcome natural language;hierarchical knowledge;reranking steps wikihow;natural language processing;steps wiki model;knowledge base fine;text goal prediction;growing understanding hierarchy;hierarchical original task;wiki decompose steps;articles simple hierarchical;hierarchical knowledge key;retrieval wiki articles;theory nlp;retrieval wiki;wiki model constructed;wiki model;hierarchy procedures higher;predicting level text;based wiki decompose;step wikihow use"}, "15a6c3d32ae1daefba3c4b40146de8efdf16ec8d": {"ta_keywords": "oscillator energy density;particle harmonic oscillator;harmonic oscillator energy;density particle harmonic;energy density particle;harmonic oscillator computed;calculating energy density;harmonic oscillator;calculation energy density;analysis energy density;oscillator energy;particle harmonic;energy density;harmonic oscillator method;oscillator computed;density particle;oscillator computed exactly;exactly energy density;oscillator method;oscillator method based;computed exactly energy;method calculating energy;oscillator;analysis energy;calculation energy;calculating energy;based calculation energy;particle;harmonic;based analysis energy", "pdf_keywords": ""}, "950c2c041db52c416e49fb0945078f6463c501b8": {"ta_keywords": "incentives energy consumers;incentives energy;design incentives energy;incentives using aggregated;aggregated disaggregated energy;incentives based disaggregated;disaggregated energy consumption;energy consumption data;consumers based aggregated;energy consumers based;aggregated data algorithm;based aggregated disaggregated;energy consumers;consumption data approach;incentives based;design incentives based;disaggregated data algorithm;incentives using;energy consumption;aggregated disaggregated;generates incentives using;approach design incentives;based disaggregated data;aggregated data;consumption data;disaggregated energy;company generates incentives;using data mining;data mining data;data mining", "pdf_keywords": "utility incentive modeled;utility learning incentive;incentives consumers algorithm;utility incentive;choose utility incentive;designing incentives utility;incentive design energy;learning incentive design;constructing incentives consumers;utility functions incentives;incentives consumers estimating;utility learning;incentives consumers;incentives consumers based;learning incentive;consumers estimating utility;incentives utility;incentive modeled concave;energy consumers model;incentive modeled;utility models;utility models solution;incentive compatible demand;consumers algorithm;utility company incentives;agent utility function;incentive design;consumers model utility;estimating utility functions;incentives utility company"}, "f637d061704579531a8b8e03ef6e8331ba117490": {"ta_keywords": "compared tex math;tex math notationtex;inline tex math;tex math notation;notationn inline tex;notation tex math;item compared tex;tex math;items notation tex;math notationn inline;math notationtex;notationtex math;notationtex math notationn;estimating probability;math notationtex math;notation tex;compared tex;notationtex;notationn inline items;estimating probability given;inline items notation;notationn inline;inline tex;math notationn;inline formula tex;formula tex math;notation notation inline;problem estimating probability;math notation notation;notation inline", "pdf_keywords": ""}, "7626f73c3b013b5b7bf293c1cc22d2835b6579b3": {"ta_keywords": "xmath1 xmath2 annihilation;xmath2 annihilation xmath3;annihilation xmath3 xmath4;xmath2 annihilation;annihilation xmath3;production xmath0 xmath1;xmath1 xmath2;xmath0 xmath1 xmath2;production xmath0;xmath2;xmath3 xmath4;xmath0 xmath1;xmath3 xmath4 xmath5;xmath9 xmath10 xmath11;xmath10 xmath11 xmath12;xmath1;xmath4 xmath5 xmath6;model production xmath0;xmath6 xmath7 xmath8;xmath8 xmath9 xmath10;xmath7 xmath8 xmath9;xmath5 xmath6 xmath7;xmath0;xmath11 xmath12 xmath13;xmath10 xmath11;xmath3;xmath4 xmath5;xmath12 xmath13 xmath14;xmath4;xmath11 xmath12", "pdf_keywords": ""}, "bf0b66e0e328df1df42b075422c8fecdd95736c0": {"ta_keywords": "student learning tool;learning project linear;project using student;learning project;describes student learning;student learning;student selects test;student test;problem student used;problem student;using student test;student test case;test case student;learning tool;students shape problem;project linear algebra;learning tool able;teach students;problem student selects;supervised learning project;linear algebra project;project linear;algebra project using;students;student selects;using student;student choice problem;results student choice;tool able teach;shape problem student", "pdf_keywords": ""}, "4ffca5d623950e2396089e7fc1621b4a477436cb": {"ta_keywords": "capture stylistic structure;stylistic control using;stylistic control;problem stylistic control;model capture stylistic;capture style sentence;capture stylistic;stylistic structure;stylistic;stylistic structure text;sentence automatic adaption;style sentence;sentence high fidelity;text model learns;style sentence high;problem stylistic;sentence able capture;text model;sentence automatic;model learns imitate;structure text model;learns imitate given;imitate given sentence;adaption underlying sentence;given sentence automatic;experiments restaurants sports;text;sentence able;approach problem stylistic;exemplar based model", "pdf_keywords": ""}, "97e033d79b6aebab1927ab9232afa8268e198481": {"ta_keywords": "distributed caching;distributed caching based;caching based video;cache content placement;scale distributed caching;content placement cache;cache users topology;caching based;placement cache;placement cache users;cache content;caching;network codes optimization;cache;video demand network;cache users rate;based video demand;cache users;topology selection cache;demand network codes;selection cache users;video demand;problem cache content;selection cache;distributed algorithm;codes optimization;network codes;large scale distributed;codes optimization design;distributed", "pdf_keywords": ""}, "f5ca46585818771e64ee9449c930748fbee35cba": {"ta_keywords": "interactively correct explanations;generates explanations precise;accuracy explanation structures;generates explanations;approach generates explanations;explanations easily adapted;improve accuracy explanation;explanations given reasoning;accurate original explanations;explanations introduce;structures corrected explanations;reasoning task approach;given reasoning task;reasoning task;users interactively correct;corrected explanations;explanation structures corrected;corrected explanations precise;explanations precise original;explanations precise;explanations introduce new;precise original explanations;interactively correct;original explanations introduce;explanation structures;original explanations easily;explanations;reasoning;correct explanations;explanations easily", "pdf_keywords": "defeasible reasoning automatically;defeasible reasoning repository;defeasible reasoning domains;reasoning domains predict;generate inference graph;reasoning domains;inference graphs;reasoning repository;generate inference;defeasible inference task;reasoning domains present;reasoning automatically;inference graphs defeasible;accuracy reasoning domains;graphs defeasible reasoning;defeasible reasoning context;defeasible inference query;graph defeasible inference;inference graph;defeasible inference;graph defeasible reasoning;improve accuracy reasoning;reasoning repository generated;defeasible reasoning;structure generate inference;inference graph defeasible;defeasible reasoning propose;feedback natural language;explainable natural language;model reasoning task"}, "97db55b196cf0c768644a392a7e6c79d1c65207e": {"ta_keywords": "reverberant speech enhancement;speech enhancement algorithm;online speech enhancement;speech enhancement;speech enhancement task;noisy reverberant speech;frame online speech;enhancement algorithm based;enhancement algorithm;reverberant speech;add algorithm inverse;overlap add algorithm;predicting current frame;frame proposed algorithm;overlap add;based overlap add;predict current frame;istft proposed algorithm;evaluated noisy reverberant;fluctuation theory istft;add algorithm;noisy reverberant;time fluctuation theory;current frame frame;algorithm based overlap;inverse short time;current frame;enhancement;fluctuation theory;frame frame proposed", "pdf_keywords": "online speech enhancement;speech enhancementwe;speech enhancementwe propose;monaural speech enhancement;speech enhancement task;noisyreverberant speech enhancementwe;speech enhancement;frame online speech;dnn predicts frame;enhanced monaural speech;speech enhancement leverage;sub frame prediction;frame online prediction;frame prediction novel;prediction sub frame;predict sub frame;frame prediction;predicts frame frame;frame prediction essential;enhancementwe propose frame;predicts frame;context noisyreverberant speech;noisyreverberant speech;deep neural;frame convolution deconvolution;deep neural network;sub frame output;frame online model;frame output frame;sub frame signalwe"}, "45ce9fce4a4eea9f72688885182aee0c84786fab": {"ta_keywords": "translation musical data;musical data trained;encoder translation musical;translating musical music;method translating musical;musical data;translating musical;translation musical;musical instruments genres;music musical instruments;musical instruments;instruments genres;waveforms encoder trained;musical music;instruments genres styles;novel encoder translation;unconstrained waveforms encoder;music musical;instruments;waveforms encoder;musical music musical;encoder translation;encoder datasets;musical;music;encoder trained;datasets resulting encoder;data trained end;novel encoder;based novel encoder", "pdf_keywords": "autoencoder pathways musical;domain translation audio;translating musical domains;music based deep;musical domain network;generative model music;generative audio;wavenet variational autoencoder;domain wavenet autoencoder;domain variational autoencoder;speaker identity encoder;translation prediction music;method translating music;wavenet autoencoder;audio style transfer;outcome generative audio;model music synthesis;generative audio synthesis;translation audio;generative style transfer;model music;musical domains;audio conversions translation;encoder domain variational;given music domain;wavenet autoencoder shared;music synthesis;converting music;variational autoencoder;translation audio method"}, "782a50a48ba5d32839631254285d989bfadfd193": {"ta_keywords": "entity representations;entity representations human;performance entity representations;entity identity embeddings;entity probing tasks;recognizing entity identity;recognizing entity;entity probing;learning models entity;models entity probing;involving recognizing entity;high performance supervised;high performance entity;entity;finegrained entity types;finegrained entity;supervised high performance;entity identity;probabilities finegrained entity;representations create supervised;performance supervised tasks;identity embeddings;entity types;models entity;performance supervised;supervised tasks;create supervised high;create supervised;performance entity;identity embeddings post", "pdf_keywords": "entity representations;adapting entity typing;entity representations simultaneously;accurately embed entity;embedding entities;embed entity descriptions;predict entity mention;task embedding entities;entities embed knowledge;creating entity representations;entity representations use;entity disambiguation conll;embedding entities context;language processing entities;interpretable entity representations;embed knowledge base;entities embed;embed entity;entity disambiguation;entity linking;representations entities embed;embed entity types;mentions entity;entity typing models;processing entities text;entity typing;entity linking based;named entity disambiguation;coreference arc prediction;entity mention"}, "d26683135c70d7b2a61ce5f70fb49b4fa22cf9c4": {"ta_keywords": "mallows models pairwise;sampling mallows model;conditional mallows models;mallows model pairwise;algorithms mallows models;sampling mallows;particular sampling mallows;markovian learning algorithms;learning algorithms mallows;markovian learning;sampling arbitrary ranking;mallows models;mallows models particular;conditional mallows;class markovian learning;distributions conditional mallows;models particular sampling;algorithms mallows;models pairwise comparison;new class markovian;markovian;repeated insertion model;mallows model;class markovian;generalized repeated insertion;model allows sampling;mallows;models pairwise;arbitrary ranking distributions;insertion model allows", "pdf_keywords": "ranking preference sampling;pairwise preference elicitation;preference sampling;learning ranking preference;preferences arbitrary ranking;ranking preference distributions;modeling learning preferences;pairwise preferences sampling;ranking preference distribution;preferences sampling arbitrary;preferences pairwise comparisons;preference sampling present;preference informative data;preference distribution observations;estimation ranking preference;pairwise comparisons preferences;models ordinal preference;preferences sampling;observations pairwise preferences;based pairwise preference;preference distributions;learning preferences;preference elicitation;sampling partial ranking;pairwise preference;preference informative;preference distributions keywords;preference profiles evidence;preferences prediction;competing preferences method"}, "205d67dfe0112df846bc4b221fa2665b0434d441": {"ta_keywords": "phase transition xmath0;xmath1 model transition;xmath2 xmath3 phase;spin liquid phase;xmath3 phase;liquid phase transition;xmath3 phase occurs;model transition xmath2;dynamics spin liquid;transition xmath2;transition xmath2 xmath3;transition xmath0;transition xmath0 limit;phase transition;xmath7 parameter transition;xmath1 model;spin liquid;liquid phase;critical value xmath4;limit xmath1 model;critical value xmath8;transition occurs critical;phase occurs critical;xmath2 xmath3;xmath1;dynamics spin;xmath2;xmath3;xmath0 limit;xmath0 limit xmath1", "pdf_keywords": ""}, "7df95dceaba3f4fb45e2b9de29caf7fbce20e25c": {"ta_keywords": "bose einstein condensates;2d bose einstein;bose einstein condensate;dynamics dimensional bose;dimensional bose einstein;gaussian noise dynamics;dimensional 2d bose;dimensional bose;gaussian noise;einstein condensates;effect gaussian noise;einstein condensate effect;2d bose;einstein condensate;einstein condensates presence;bose einstein;noise dynamics dimensional;critical exponents condensates;noise dynamics;gaussian;exponents condensates;strong magnetic field;effect gaussian;presence strong magnetic;condensates presence strong;strong magnetic;magnetic field effect;magnetic field;condensates presence;condensate effect", "pdf_keywords": ""}, "83a2582b94aeaaa97b2f52af8d827d28dc4690bf": {"ta_keywords": "stuttered speech recognition;speech recognition algorithm;new speech recognition;speech recognition;principles speech recognition;speech recognition active;speech recognition based;pauses stuttered speech;algorithm named quiet;named quiet pauses;stuttered speech;quiet pauses stuttered;quiet quiet pauses;recognition algorithm;recognition algorithm named;quiet pauses;steps speech developed;pauses stuttered;speech developed new;speech developed;developed new speech;named quiet quiet;speech;steps speech;recognition based principles;named quiet;based principles speech;recognition based;recognition active;necessary steps speech", "pdf_keywords": ""}, "930445d9cda71d6ff857e69aa5bb4b1bef7d31e5": {"ta_keywords": "quantization reinforcement learning;quantization reinforcement;approach quantization reinforcement;robust reinforcement learning;reinforcement learning drifting;stationarity use quantization;quantization construct robust;robust reinforcement;learning drifting non;robust robust reinforcement;extend robust reinforcement;quantization;quantization construct;learning drifting;approach quantization;reinforcement learning case;reinforcement learning;quantization_;use quantization construct;reinforcement learning framework;use quantization;drifting non stationarity;comprehensive approach quantization;reinforcement;learning case drifting;drifting non;non stationarity particular;non stationarity;non stationarity use;construct robust", "pdf_keywords": "discounted reinforcement learning;discounted reinforcement;consider discounted reinforcement;reinforcement learning gradually;problem discounted reinforcement;models reward state;stochastic bandit feedback;constructing pseudo reward;reinforcement learning;state bandit models;feedback sequential decision;reinforcement learning stochastic;approach reinforcement learning;consider reinforcement learning;sequential decision;sequential decision making;consider stochastic bandit;bandit models mdps;reinforcement learning rl;stochastic bandit;state learning;reward state transition;temporal drifts reward;preserving algorithm bandit;consider sequential decision;pseudo reward;learning protocol;ad dynamics advertiser;auction ad;online auction ad"}, "d3231772937a2182b2377d028417245c49868dd1": {"ta_keywords": "neural machine translation;neural sequence models;machine translation nmt;machine translation;model errors neural;entire wmt15 english;neural sequence;translation nmt present;errors neural machine;procedure neural sequence;translation nmt;neural machine;sequence models;wmt15 english german;inference procedure neural;errors neural;wmt15 english;surprisingly beam search;search errors model;procedure neural;model entire wmt15;beam search fails;transformer base model;neural;sequence models based;beam search;search depth;search depth search;model scores transformer;beam search depth", "pdf_keywords": "neural machine translation;machine translation nmt;powerful machine translation;neural sequence models;machine translation;model score deep;hypothesis deep neural;deep neural model;machine translation nnt;score deep neural;accuracy translation results;translation results noisy;improve accuracy translation;accurate translation results;deep neural;machine translation technique;neural model best;best hypothesis deep;translation technique finds;translation nmt powerful;finds best translation;accelerate translation process;inference procedure neural;accuracy translation;model errors neural;translation process improve;exact inference nmt;translation nmt present;technique translation noisy;inference nmt models"}, "6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b": {"ta_keywords": "scaling laws neural;linearized neural networks;network linearized neural;linearized neural;laws neural networks;scaling laws;power law scalings;law scalings;neural networks important;network linearized;law scalings saturated;scalings;scaling;performance neural networks;scaling regimes;neural networks;scalings saturated;laws neural;neural networks identify;networks;known empirical power;empirical power;related scaling regimes;related scaling;accelerating algorithms;models;performance neural;connected network linearized;identify related scaling;parameters dataset size", "pdf_keywords": "neural scaling laws;neural network scaling;scaling neural networks;neural scaling;learning scaling large;scaling neural;categorizing neural scaling;learning scaling;scaling properties training;scaling regimes neural;scaling linear neural;scaling test loss;loss dataset scales;learning modes scaling;scaling laws neural;deep models standard;scaling learning;modes scaling neural;loss deep neural;deep learning models;universal scaling behavior;scaling universal learning;scaling learning curves;consider scaling neural;size scaling regime;properties deep learning;scaling features networks;size neural networks;limited scaling regimes;network scaling bounded"}, "4e1b16fd719354b0a9e92075be66c85d4b95082c": {"ta_keywords": "plasma spin polarized;xmath0he plasma spin;electron gas spin;plasma spin;spin polarized electron;polarized electron gas;zeeman field spin;gas spin polarized;polarized electron;dynamics spin polarized;electron gas;spin polarized;dilute xmath0he plasma;xmath0he plasma;spin dynamics spin;spin dynamics;gas spin;electron gas dilute;field spin dynamics;dynamics spin;effect zeeman field;effect zeeman;electron gas used;control spin dynamics;field spin;electron;control spin;zeeman;spin;plasma", "pdf_keywords": ""}, "713844009469478141671c53a3b73cd12caf9df0": {"ta_keywords": "zeeman field stability;field stability xmath0he;stability xmath0he ep;xmath0he ep cluster;stability xmath0he;zeeman field strength;strength stability xmath0he;field stability;effect zeeman field;field strength stability;effect field stability;zeeman field;xmath0he ep;ep cluster framework;cluster framework standard;ep cluster determined;ep cluster;ep cluster minimized;cluster determined interplay;cluster determined;cluster framework;choice zeeman field;stability;effect zeeman;cluster;xmath0he;cluster minimized;strength stability;interplay field strength;cluster minimized suitable", "pdf_keywords": ""}, "810420af4fa5f3ed932724aea5f7b66d3bd592b2": {"ta_keywords": "learning queries world;learning queries;problem learning queries;www learning methods;world wide web;use machine learning;using machine learning;machine learning;machine learning methods;web www learning;learning methods address;queries world wide;wide web;www learning;learning methods;queries world;wide web www;learning methods construct;learning methods effective;web;effective detecting new;detecting new;problem learning;data obtained world;web www treat;detecting new instances;queries;address problem learning;web www;unknown concept use", "pdf_keywords": ""}, "e5e74d312679eae8f2a2943e16f2efebcb5cc50f": {"ta_keywords": "surface wind speed;wind speed surface;speed wind surface;speed surface wind;wind surface surface;speed surface surface;wind surface;surface wind;determining wind speed;quantity determining wind;speed surface;speed wind speed;surface key quantity;surface surface;determining wind;wind speed wind;wind speed;surface surface key;speed wind;surface key;surface;key quantity determining;wind;quantity determining;speed;key quantity;determining;quantity;key", "pdf_keywords": ""}, "e35357ac461a669fe7e4b877ee1fad0dfda26303": {"ta_keywords": "style transfer languages;transfer languages style;style transfer context;style transfer;attribute transfer;shot style transfer;style labelled corpora;transfer problem languages;transfer languages;style transfer problem;attribute transfer tasks;statements shot style;neutralization text anonymization;gender neutralization text;problem languages style;languages style;languages style labelled;neutralization text;gender neutralization;simplification gender neutralization;corpora;text anonymization code;shot style;results attribute transfer;labelled corpora;statements shot;transfer context;text anonymization;style labelled;labelled corpora introduce", "pdf_keywords": "style transfer sentences;semantics style transfer;style transfer language;style transfer languages;style preserving semantics;corpus style transfer;multilingual style transfer;style transfer methods;transfer natural language;approach style transfer;style transfer natural;style transfer deep;transfer languages style;transfer sentences extract;style transfer diffur;style checking corpus;learn style transfer;style transfer;style preserved translation;style transfer possible;transfer sentences;effective style transfer;style transfer powerful;different style transfer;style transfer systems;supervised machine translation;machine translation improves;style transfer fundamental;style transfer outperform;method style transfer"}, "e97c5b206c1f308b821917bc2f584b5f1faad547": {"ta_keywords": "measuring cardinal scores;cardinal scores based;cardinal scores;testing ranking;induced ranking;induced ranking method;testing ranking problems;method measuring cardinal;ranking;based induced ranking;measuring cardinal;ranking method flexible;ranking method;estimators rely ranking;ranking method based;ranking problems;rely ranking;rely ranking method;empirical bayes shrinkage;variety testing ranking;scores based;bayes shrinkage framework;cardinal;bayes shrinkage;scores based induced;based empirical bayes;empirical bayes;scores;bayes;outperforming possible estimators", "pdf_keywords": "ranking estimation;estimate ranking;estimating ranking;ranking estimators;method estimating ranking;assigning ranking reviewers;information estimate ranking;ranks cardinal estimators;ranking reviewers provide;ranking cardinal scores;ranking reviewers;rely ranking estimators;estimates ranking;ranking estimation consider;rankings items estimators;ranking using reviewers;iteratively estimates ranking;based ranking cardinality;estimating ranking set;ranking estimator outperforms;ranking estimator;ranking estimators flexible;estimators rely ranking;ranking estimator strictly;iteratively estimating ranking;estimators ranks cardinal;estimating ranking using;estimate ranking iteratively;ranking estimator based;based ranking estimator"}, "2d3fcbaf28e650471b942f221c5fa3c178b1b72a": {"ta_keywords": "accelerated directional search;directional search method;unconstraint optimization;directional search;unconstraint optimization problem;constrained optimization;consider unconstraint optimization;optimization consider unconstraint;unconstraint method;unconstraint xmath2;constraint xmath3;constrained optimization consider;context constrained optimization;unconstraint method stable;optimization problem xmath0;dimension unconstraint method;xmath3 dimension unconstraint;propose accelerated directional;xmath1 dimension unconstraint;optimization;dimension constraint xmath3;accelerated directional;constraint;consider unconstraint;unconstraint;constraint xmath3 dimension;dimension unconstraint xmath2;xmath2 dimension constraint;optimization consider;unconstraint xmath2 dimension", "pdf_keywords": ""}, "b345057638e60eee581fea6c7110a98e3b9ebe61": {"ta_keywords": "modeling text streams;topic modeling text;contrastive topic modeling;events text streams;topic modeling;emerging events text;localized emerging events;text streams;topics time labeled;contrastive topic;text streams method;topics;text streams uses;topics time;emerging events;modeling text;principled contrastive topic;identify patterns text;patterns text documents;localized emerging;events text;spatially localized emerging;early detection;streams uses likelihood;hidden content new;content new document;method early detection;methods topics time;text documents;topic", "pdf_keywords": "detecting emerging topics;characterizing emerging topics;identify new topics;contrastive topic modeling;topic modeling;topic dynamics;online topic modeling;characterize emerging topic;emerging topics unstructured;topic dynamics based;contrastive topic model;emerging topics online;model topic dynamics;predicting novel topics;topics unstructured text;topic model robust;topic model predicting;topic dynamics illustrate;patterns topic dynamics;new topics unstructured;topic model;emerging topics;topic modeling approach;topic modeling framework;emerging topic associated;novel topics unstructured;topics unstructured;topic model allows;novel topics method;topic associated events"}, "2038086c604f1f8841d086cd5cc6052e546ffc24": {"ta_keywords": "control spin dynamics;valve spin orbit;dynamics spin valve;dynamics spin orbit;spin dynamics used;dynamics spin;spin dynamics spin;spin orbit interaction;spin dynamics;spin valve spin;dynamics spin dynamics;orbit interaction spin;control spin;interaction spin dynamics;spin orbit;effect spin orbit;spin valve;interaction spin;valve spin;effect spin;used control spin;spin;orbit interaction used;study effect spin;orbit interaction;dynamics used control;dynamics used;dynamics;orbit;valve", "pdf_keywords": ""}, "9850d2b41c6c5be039649d6422306121b760169d": {"ta_keywords": "oblivious coding schemes;distributed data storage;oblivious update algorithms;generic oblivious coding;data storage schemes;oblivious coding;oblivious fully constrained;oblivious update;storage schemes fully;schemes fully oblivious;storage schemes;constrained present oblivious;distributed data;generic oblivious;algorithms data storage;present generic oblivious;data storage systems;present oblivious update;applicable distributed data;schemes data storage;data storage;schemes applicable distributed;class distributed data;coding schemes;storage systems restricted;oblivious fully;storage systems fully;fully oblivious fully;data storage especially;oblivious", "pdf_keywords": ""}, "71bcdfe5b6be3a0d08ce4bde45acdfd0f738e2f7": {"ta_keywords": "inverse reinforcement learning;based inverse reinforcement;inverse reinforcement;model passengers decisions;models passengers decisions;ride sharing passengers;passengers proposed algorithm;reinforcement learning algorithm;gradient based inverse;passengers decisions;data model passengers;model ride sharing;passengers decisions regarding;ride sharing;sharing passengers;reinforcement learning;model passengers;regarding ride sharing;models passengers;ride sharing proposed;ride sharing company;analyzing data ride;sharing passengers proposed;passengers;data ride sharing;company models passengers;decisions regarding ride;learning algorithm minimizes;learning algorithm;world model ride", "pdf_keywords": ""}, "1843c91e9692484b574ef40961f1d0443a56ddf4": {"ta_keywords": "simple incentive mechanism;incentive mechanism;incentive mechanism obtaining;objective feedback commerce;equilibrium game induced;incomplete feedback agents;theorem equilibrium;strict equilibrium game;propose simple incentive;equilibrium game;feedback commerce;feedback commerce platforms;simple incentive;feedback agents forced;optimal agents symmetric;agents symmetric equilibria;feedback agents;asymptotically optimal agents;induced theorem equilibrium;equilibrium asymptotically optimal;commerce platforms mechanism;equilibrium;incentive;constraint strict equilibrium;theorem equilibrium asymptotically;optimal agents;strict equilibrium;game induced theorem;confronted incomplete feedback;objective feedback", "pdf_keywords": ""}, "4c0a915b9389e6489753a968085ee12833131d0a": {"ta_keywords": "oprtihm peer review;peer review process;peer review;challenges oprtihm peer;review process;review process provides;systemic challenges oprtihm;review process tutorial;systemic challenges;oprtihm peer;peer;introduction tutorial systemic;challenges oprtihm;tutorial systemic challenges;review;tutorial systemic;challenges;systemic;process;important open problems;process tutorial provide;open problems;open problems envisage;process provides brief;process tutorial;process provides;introduction;provides brief introduction;oprtihm;problems", "pdf_keywords": ""}, "df53aabeca68a8c0076f7e110f2cc7df7d010e7a": {"ta_keywords": "separation achieved perturbative;xmath0 separation achieved;xmath0 separation;xmath1 limit split;particles separated;split configuration particles;distance xmath0 separation;particles separated distance;split level split;split split level;level split;level split configuration;split level;behaviour split split;achieved perturbative expansion;separated distance xmath0;configuration particles separated;large xmath1 limit;split split;perturbative expansion performed;separation achieved;large xmath1;limit split;split;xmath1 limit;behaviour split;perturbative expansion;split configuration;performed large xmath1;xmath1", "pdf_keywords": "doa speech networks;sound source localization;source localization classify;deep models speaker;splitting source mixture;speech networks;speech networks propose;estimation multiple sources;independent speech separation;models speaker independent;approach sound source;information single speaker;speech separation;predictor source split;source splitting based;audio mixture;speech separation approach;source splitting;multi source localization;simultaneously audio mixture;splitting source;speech models;source split based;speech models proposed;end speech recognition;source split;single speaker sample;speaker independent speech;end speech models;speech recordings"}, "b03c7ff961822183bab66b2e594415e585d3fd09": {"ta_keywords": "attention heads models;multiple attention heads;using multiple attention;multiple attention;attention heads;percentage attention heads;large percentage attention;attention heads removed;neural models trained;heads models trained;pruning models;percentage attention;pruning models potential;algorithms pruning models;models trained;attention;neural models;models trained using;trained using multiple;memory efficiency;heads models;memory efficiency accuracy;performance examine greedy;neural;greedy algorithms pruning;pruning;efficiency accuracy improvements;multiple heads;multiple heads practice;algorithms pruning", "pdf_keywords": "multilayer attention models;generalize multilayer attention;multilayer attention;attention multilayers;multilayer attention multilayers;attention multilayers high;attention heads multilayer;predicting attention heads;attention multilayers use;capable predicting attention;attention model capable;attention models present;attention models;decomposable attention predict;predict accuracy attention;predicting attention;heads multilayer attention;method predicting attention;attention predict linguistic;attention model predict;attention model;extract attention heads;model single attention;extract attention;deep learning attention;rank attention model;model decomposable attention;vector attention heads;attention multilayers apply;vector attention"}, "315432474166ff7abbc6351e8ff07fcccbd68458": {"ta_keywords": "tweets containing misinformation;misinformation terms tweets;url shared tweets;covid 19 twitter;tweets containing urls;higher rate tweets;urls high quality;quality misinformation sources;quality url shared;rate tweets;low quality url;quality url;tweets;shared tweets;misinformation sources misinformation;tweets users;rate tweets containing;twitter;tweets containing;tweets users discuss;quality health information;19 twitter streams;quality health sources;misinformation sources;low quality misinformation;terms tweets;misinformation shared;quality misinformation;sources misinformation;containing misinformation shared", "pdf_keywords": "misinformation social media;misinformation social networks;social media viral;misinformation sources social;sources social media;social media information;links social media;information shared misinformation;information sources social;social media;social media news;social media using;online social media;media viral;prevalence misinformation social;misinformation social;misinformation spreading;information networks misinformation;spread misinformation;future misinformation spreading;social media generated;social media predict;tweets literature misinformation;spread misinformation spread;social media important;misinformation spreading relationship;misinformation quality information;social media community;social media understood;misinformation sources correlated"}, "92acaf505a9c738e56ed70759e8d0062f3c520d6": {"ta_keywords": "audio segmentation neural;speech recognition asr;automatic speech recognition;autoregressive model audio;model audio segmentation;automatic speech;audio segmentation;speech recognition;realize automatic speech;segmentation non autoregressive;recognition asr;segmentation neural;recognition asr non;segmentation neural network;asr non autoregressive;compared baseline autore;autore;model audio;baseline autore;autoregressive model;non autoregressive model;autoregressive;autoregressive model realize;audio;neural network proposed;non autoregressive;segmentation non;segmentation;concatenates segmentation non;accuracy low rtf", "pdf_keywords": "audio segment attention;automatic speech;attention segment audio;segmentation streaming audio;audio segment;automatic speech recognition;segment audio;acoustic feature sequences;speech recognition asr;segment audio using;speech recognition combines;speech recognition;audio segment generated;realize automatic speech;segment audio segment;audio segment proposed;acoustic feature segment;approach speech recognition;applied segment audio;audio method;able segment audio;streaming audio method;audio using simple;audio method based;audio;segment audio key;attention weights acoustic;audio using;streaming audio;model acoustic feature"}, "f75d05e759447c2aedb7097728f29f9a520d9bc1": {"ta_keywords": "long range language;range language models;long range context;range context models;range language;language models actually;language models including;context models improves;language models;context models;long range;long sequence benchmark;range context;use long range;providing long range;jk_ s__eywords results;range context perform;analysis long range;grained analysis long;jk_ s__eywords;dataset jk_ s__eywords;sequence benchmark dataset;sequence benchmark;s__eywords results;long sequence;language;context;s__eywords results reveal;s__eywords;actually use long", "pdf_keywords": "range language models;long range language;large language models;context large language;long range context;language models;language models accurate;longer sequences models;short range contexts;language transformers efficient;language models lms;natural language transformers;range language;range contexts random;sequence distant context;large language;predicting perplexity long;distant context model;machine translation transformers;language transformers;long range models;sentences accuracy predictions;context model learns;order linguistic models;linguistic models;range context improves;longsequence benchmark datasetwe;models helpful sequence;longsequence benchmark;long short range"}, "1dfa71ecab0c25c5fdd6b2df83a41e944ffa5d58": {"ta_keywords": "text based multinomial;word sequence;multinomial distribution models;text based;frequencies occurrence models;occurrence models;behavior word sequence;description text based;based multinomial distribution;number words;word sequence case;occurrence models able;multinomial distribution;based multinomial;reducing number words;frequencies occurrence;text;occurrence;multinomial;description text;higher frequencies occurrence;number words small;models description text;distribution models;graph models;statistical models;graph models used;statistical models description;distribution models constructed;words", "pdf_keywords": ""}, "0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d": {"ta_keywords": "neighbors language models;nearest neighbors language;neighbors language model;neighbors language;nonparametric nearest neighbors;language models;language models applications;nearest neighbors;proposed nearest neighbors;sample representations lefschetz;language model;nonparametric nearest;speedup inference;speedup inference speed;inference speed;tableaux resulting representations;tableaux;inference speed retaining;deployment nonparametric nearest;language model shown;6x speedup inference;generate sample representations;sample representations;nearest;recently proposed nearest;representations lefschetz;proposed nearest;representations lefschetz type;nonparametric;form tableaux", "pdf_keywords": "nonlinear language models;language modeling benchmark;parametric language model;non parametric language;models nlps;benchmark domain adaptation;models nlps promising;language models nlps;nonlinear nonlinear language;language model knn;nonlinear language;language models;lm adaptive retrieval;nonlinear learning;pretrained parametric nonlinear;domain adaptation;language modeling;model nlm;adaptive retrieval;adaptive retrieval improved;memorization applications spanning;2020 adaptive retrieval;machine translation tasks;faster conventional retrieval;deep nonlinear;parametric language;model nlm utilizes;knn lm adaptive;performance nearest neighbors;used adaptive retrieval"}, "83cbe142d445a521aefa11acbd184e176085e7c7": {"ta_keywords": "mistaken voters information;trusting voter anomalous;voter anomalous updates;trusting voters suspicious;information sources unbiased;mistakenly mistaken voters;trusting voters;unbiased sources biased;biased sources;trusting voter;voters suspicious voters;voters trusting voters;voters suspicious;sources biased sources;biased sources model;unbiased sources;suspicious voters;mistaken voters;voters trusting;suspicious voters types;voters information;sources unbiased;sources unbiased sources;voter anomalous;sources biased;voters information source;voters types information;classes voters trusting;information sources;incorporated trusting voter", "pdf_keywords": "biased communication rational;biased information;based biased information;biased voter model;biased information sources;rational voter model;trusting voter model;influence rational voter;biased communication;communication rational reasoning;polling biased;decisions based biased;examples voter biases;preferred biased communication;voter biases;biased sources;biased voter;biased communication preferred;polling biased media;voter knowledge;voter biased;voter biases biasesed;voter knowledge target;biased communication communicating;prediction voter bias;biased agents;rational reasoning models;voter bias;model voter behavior;voter model inspired"}, "6a79ff7465d8249d9c8a50fa5f2e0a3e308b436d": {"ta_keywords": "domain adaptation retrieval;domain adaptation;unsupervised domain adaptation;domain adaptation method;adaptation retrieval tasks;scenario domain adaptation;adaptation retrieval;novel unsupervised domain;dense retrieval approach;unsupervised domain;retrieval approach;query generator pseudo;dense retrieval;retrieval approach points;labeling cross encoder;retrieval;query generator;retrieval tasks;retrieval tasks investigated;cross encoder proposed;generator pseudo labeling;cross encoder;pseudo labeling;encoder;combines query generator;encoder proposed;encoder proposed method;adaptation method;art dense retrieval;adaptation", "pdf_keywords": "domain dense retrieval;training domain adaptation;retrieval models trained;unsupervised domain adaptation;domain adaptation;models dense retrieval;training domain domains;queries supervised training;supervised training domain;dense retrieval models;encoder domain dense;query trained;dense retrieval tasks;dense retrievers efficient;domain adaptation methods;dense retrieval model;dense retrieval propose;domain cross encoder;domain adaptation dense;new dense retrieval;dense retrieval corpus;dense retrieval;retrieval propose new;training domain;gpl dense retrieval;method dense retrieval;domain adaptation method;training target domain;cross encoder domain;query trained output"}, "3e3f55cb25b919c4e8158195fd3ce2f23cfa7723": {"ta_keywords": "language bias vqa;counterfactual inference framework;bias vqa dataset;novel counterfactual inference;language bias directly;counterfactual inference;language bias sensitive;language bias;bias sensitive vqa;analysis language bias;performance language bias;vqa dataset proposed;bias vqa;propose novel counterfactual;counterfactual;novel counterfactual;idea language bias;vqa dataset;vqa cp dataset;bias sensitive;bias directly;bias;bias directly related;inference framework analysis;answer inferred knowledge;cp dataset augmented;inferred knowledge demonstrate;sensitive vqa cp;sensitive vqa;balanced vqa", "pdf_keywords": "visual answering;counterfactual inference framework;based counterfactual representation;counterfactual model debiased;introduction visual answering;counterfactual representation;novel counterfactual inference;counterfactual samples debiased;counterfactual inference;visual answering powerful;language bias vqa;proposes counterfactual inference;counterfactual inference based;counterfactual vqa;visual attention vqa;propose counterfactual inference;debiasing vqa works;framework debiased quantumqa;debiased quantumqa vqa;debiasing vqa;counterfactual patterns videos;called counterfactual vqa;visual question answering;recent debiasing vqa;predicting counterfactual;introduce counterfactual model;counterfactual inference towe;generate counterfactual;based counterfactual;counterfactual model"}, "7b29f45df975ed1e4c3864b6ab4483f11086aa76": {"ta_keywords": "data induction embeddings;pretrained word embeddings;induction embeddings effective;induction embeddings;word embeddings;embeddings effective;word embeddings context;embeddings effective cases;embeddings;data induction;utility pretrained word;pretrained word;problem data induction;embeddings context;embeddings context problem;induction;pretrained;utility pretrained;word;context problem data;investigate utility pretrained;data;effective cases providing;effective cases;cases providing;20 points favorable;cases providing gains;points favorable;gains 20 points;problem data", "pdf_keywords": "pretrained word embeddings;embeddings trained pre;pre trained embeddings;pretrained embeddings;trained word embeddings;neural machine translation;trained embeddings effective;embeddings useful bilingual;embeddings multilingual neural;trained embeddings useful;recommendations pretrained embeddings;vocabulary embeddings trained;embeddings surprisingly effective;embeddings trained;embeddings effect multilingual;pretrained embeddings bewe;trained embeddings;small alignment embeddings;translation task high;word embeddings;word embeddings multilingual;embeddings beneficial training;alignment embeddings beneficial;embeddings multilingual;performance nmt translation;embeddings effective;multilingual neural machine;multilingual translation improved;embeddings neural machine;nmt tasks embeddings"}, "7a684045afae2ccf40338ff07b8fa429bad93a57": {"ta_keywords": "scientific content crawled;insights accelerates discovery;discovery novel scientific;scientific insights novel;pages identify scientific;novel scientific insights;discovery novel scalable;accelerates discovery novel;identify scientific content;scientific insights framework;insights novel features;extract novel insights;novel insights data;based discovery novel;accelerates discovery;based discovery;scientific content pages;scientific insights accelerates;novel scientific;discovery novel;identify scientific;scientific insights;discovery;present discovery novel;insights novel;content crawled;scientific content;framework based discovery;insights accelerates;extensible data mining", "pdf_keywords": ""}, "fc5d79301a0876201c95954a764ec374b8eb236e": {"ta_keywords": "adaptation domain sentences;translation domain sentences;domain target sentences;neural machine translation;translation domain target;machine translation;machine translation nmt;domain sentences using;translation domain;based translation domain;domain sentences;pseudo domain corpus;domain corpus achieves;domain corpus;domain sentences demonstrate;adaptation domain;method translation domain;adapt adaptation domain;target sentences;translation nmt;target sentences designed;method based translation;sentences designed adapt;novel method translation;sentences using neural;translation nmt method;improvements adaptation;substantial improvements adaptation;method translation;adaptation performance unadapted", "pdf_keywords": "adapting translation;neural machine translation;adapting translation strategy;method adapting translation;domain language adaptation;parallel sentences adaptation;improves translation accuracy;machine translation nmt;leveraging adaptation lexicon;translation model translation;machine translation model;machine translation models;translation models;translation models describing;improve translation accuracy;translation domain source;translation lexicons domain;machine translation;translation model robust;adaptation lexicon extracted;parallel translation model;sentences adaptation performance;translating domain unseen;neural network translate;adapting large corpus;sentences adaptation;adaptation lexicon;translate domain unseen;translation model;improves translation"}, "75abecb4568366d89e89c3c9d39574b9c1c028a5": {"ta_keywords": "document navigational;document navigational performance;document retrieval;original document navigational;document retrieval use;retrieval use document;document useful selecting;mechanism document retrieval;selecting best document;navigational mechanism document;use navigational;navigational performance popular;later use navigational;navigational performance;document useful;retrieval use;best document;navigational;use navigational mechanism;retrieval;best document later;document;document significantly;use document;document later use;navigational mechanism;document form;document form determine;present novel navigational;useful selecting best", "pdf_keywords": ""}, "ec9367ab933a142124eecd3232fe2d933d93a144": {"ta_keywords": "bose gas dynamics;bose gas harmonic;dynamics dimensional bose;dimensional bose gas;harmonic trap initially;gas harmonic trap;harmonic trap;interaction condensate harmonic;dimensional bose;condensate harmonic;condensate harmonic field;qualitatively different bose;harmonic field dynamics;gas harmonic;gas dynamics qualitatively;gas dynamics;initially excited harmonic;bose gas;different bose gas;excited harmonic;harmonic field cooled;excited harmonic field;trap initially excited;state dynamics characterized;state dynamics;steady state dynamics;harmonic;harmonic field;dynamics characterized time;density states result", "pdf_keywords": ""}, "51c2321244b0a489970e1b52c59b049fdcc5cd46": {"ta_keywords": "lingual question answering;machine translation accuracy;translation accuracy cross;accuracy cross lingual;cross lingual data;perform machine translation;translation accuracy;question answering clqa;machine translation using;manual machine translation;translation accuracy closely;machine translation perform;machine translation;cross lingual question;translation perform machine;question answering;set machine translation;create cross lingual;cross lingual;translation using data;translation using;lingual data;lingual data set;lingual question;relationship machine translation;lingual;answering clqa;translation perform;answering clqa create;translation", "pdf_keywords": ""}, "bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d": {"ta_keywords": "contextual language understanding;truly contextual language;contextual language;contextual information;truly contextual;contextual;contextual information factor;representations believe framing;progression contextual information;language understanding;roadmap truly contextual;progression contextual;possible progression contextual;framing serve roadmap;framing;information factor representations;believe framing;language;believe framing serve;factor representations believe;factor representations;framing serve;representations;representations believe;present brief;understanding;information factor;information;brief history possible;brief", "pdf_keywords": "nlp levels world;large text corpora;semantics text neural;natural language representations;scale natural language;representations natural language;nlp levels;distribution natural language;distributed representations deep;natural language predict;processing nlp levels;predictions natural language;representations large context;characterizing natural language;text neural;natural language domain;neural language models;natural language models;purposeful contextualization language;large scale web;predict semantics text;language learning promising;large context;deep representations;processing nlp;language representations;deep learning wide;predict linguistic semantics;predicting linguistic semantics;predict semantic structure"}, "6b98bef930182a848c027dece1bfb58ca706449d": {"ta_keywords": "pronunciation assisted sub;sub word modeling;word modeling;word modeling proposed;word modeling ppa;method pronunciation assisted;speech recognition;pronunciation information word;pronunciation information;pronunciation assisted;performance pronunciation assisted;assisted sub word;propose pronunciation assisted;leverages pronunciation information;pronunciation;sub word;improve performance pronunciation;method pronunciation;method leverages pronunciation;performance speech recognition;novel method pronunciation;propose pronunciation;performance pronunciation;accuracy segmentation text;accuracy segmentation;segmentation;segmentation text;leverages pronunciation;segmentation text used;degree accuracy segmentation", "pdf_keywords": "pronunciation dictionary model;predicting pronunciation words;sub words pronunciation;directly model pronunciation;model pronunciation information;sub word modeling;model hybrid speech;sub word models;approach predicting pronunciation;pronunciation assisted sub;hybrid speech recognition;detect sub words;sub word extraction;predicting pronunciation;words pronunciation dictionary;sub word alignments;pronunciation dictionary;based pronunciations words;propose pronunciation dictionary;text speech recognition;training subword model;model pronunciation;speech recognition;automated speech translation;pronunciation words;subword model;words pronunciation;automated speech;pronunciation words speech;pronunciation information"}, "8d7db1b1290e5d6f802e9f1075ef197cb55d754f": {"ta_keywords": "problem wave recognition;wave recognition;wave recognition approach;wave recognition large;area wave recognition;wave models;wave models including;wave model fully;wave model;class wave models;recognition;warning problem wave;wave lens;recognition approach;wave lens applicable;problem wave;lens applicable wave;applicable wave model;applied problem wave;problem area wave;class wave;wave;recognition approach based;recognition large class;area wave;applicable wave;equivalence wave lens;recognition large;large class wave;approach early warning", "pdf_keywords": ""}, "acbf4f9a4457cf2884e6018e4653519beef2833a": {"ta_keywords": "guinea pig cytomegalovirus;pig cytomegalovirus gpcmv;pig cytomegalovirus;cytomegalovirus gpcmv model;congenital cytomegalovirus cmv;human congenital cytomegalovirus;cytomegalovirus gpcmv;cytomegalovirus cmv;congenital cytomegalovirus;cytomegalovirus cmv infection;cytomegalovirus;cmv infection;cmv infection primary;cell infection mutations;toalanine alteration pentameric;epithelial cell infection;infection mutations;infection mutations important;infection primary cell;cell infection;cmv;amino acid toalanine;alteration pentameric complex;toalanine alteration;gpcmv model;pentameric complex peclide;gpcmv;alteration pentameric;infection primary;infection", "pdf_keywords": ""}, "790d3503fa95ec32f04c280bd9a52fef6bf1e874": {"ta_keywords": "simulations traffic flow;differencing methods traffic;traffic flow accuracy;models traffic flow;finite differencing methods;numerical simulations traffic;methods traffic flow;finite differencing schemes;traffic flow;accuracy finite differencing;differencing schemes work;differencing schemes;friedmans finite differencing;simulations traffic;finite differencing;traffic flow validation;differencing methods;macroscopic models traffic;flow accuracy upwind;models traffic;differencing methods verified;flow accuracy finite;flow accuracy;accuracy upwind forward;compares accuracy upwind;methods traffic;accuracy numerical simulations;flow validation upwind;traffic;accuracy upwind", "pdf_keywords": ""}, "0fcfa0ef253a81c103854e1dc123d90e7310a0e1": {"ta_keywords": "private deep learning;differentially private deep;differentially private;impact differentially private;differentially assisted deep;private deep;deep learning dp;assisted deep learning;deep learning dsg;private;deep learning;differentially assisted;known differentially assisted;differentially;known differentially;assisted deep;dsg known differentially;learning dp;learning dp popular;learning dsg known;training models;training models significant;learning dsg;learning;learning dsg recently;model performance;model performance compared;deep;impact differentially;dp popular", "pdf_keywords": "private deep learning;differentially private deep;algorithmdifferentially private deep;privacy deep learning;learning algorithmdifferentially private;learn differential privacy;differentially private dgg;private dp learning;differentially private;differential privacy;learndifferentially private dp;methods differentially private;learndifferentially private;new differentially private;private dgg algorithms;differential privacy sr;differentially differentially private;differentially private sr;private deep;algorithmdifferentially private;effect differentially private;privacy deep;pate differentially private;teachers learndifferentially private;deep learning algorithmdifferentially;deep learning dgg;privacy preservation deep;deep learning provide;learning provide fairness;private dp"}, "634bbe75c34b82e664f1e9f083314b5bdb6ba187": {"ta_keywords": "data contaminated eye;contaminated eye blinks;ocular artifacts based;blinks ocular artifacts;wiener filter;wiener filter filter;artifacts based probabilistic;eye blinks ocular;contaminated eye;signal reducing contamination;blinks ocular;eye blinks;using classical attention;filtered signal;multichannel wiener filter;classical attention;attention task results;classical attention task;ocular artifacts;filtered signal estimates;attention task;excess filtered signal;target signal reducing;based probabilistic generative;probabilistic generative model;information event signal;signal reducing;enhances target signal;generative model proposed;probabilistic generative", "pdf_keywords": ""}, "79a6f290cfe8652575e7bb65cfed519bca8f3bd3": {"ta_keywords": "noise increase lattice;lattice gas effect;lattice gas presence;dimensional lattice gas;lattice gas;gaussian noise dynamics;gaussian noise;gaussian noise increase;effect gaussian noise;lattice spacing;noise dynamics dimensional;gas effect gaussian;dynamics dimensional lattice;noise dynamics;increase lattice spacing;gas presence harmonic;lattice spacing factor;potential effect gaussian;dimensional lattice;lattice;gaussian;increase lattice;case dimensional lattice;effect gaussian;presence harmonic potential;harmonic potential effect;noise increase;noise;harmonic potential;presence harmonic", "pdf_keywords": ""}, "9cda754187545c3cc8f9e9f134c08707269d0fae": {"ta_keywords": "fully encoder speech;encoder bert;single encoder bert;encoder bert objective;encoder speech;novo speech translation;training fully encoder;encoder speech language;performance novo speech;fully encoders task;novo speech;fully encoder;fully encoders;encoders task;encoder;bert objective unlabeled;advantages fully encoders;fully encoders advantages;single encoder;speech translation;bert objective;encoders;encoders task demonstrate;encoders advantages fully;speech language;combination single encoder;encoders advantages;bert;speech language combines;pre training fully", "pdf_keywords": "training speech text;supervised speech text;speech translation tasks;training unannotated speech;trained unannotated speech;unannotated speech data;model unannotated speech;speech training unannotated;speech text self;text self supervised;training unannotated text;supervised speech;text model deep;unannotated speech text;trained text linguistic;learns speech training;multimodal speech text;supervised approaches speech;learns speech;speech text models;framework learns speech;coresponding speech translation;learn multimodal speech;trained text;pre trained text;pre training speech;training speech;joint speech text;speech text model;downstream performance speech"}, "b50d03ecd9f2055b32451e3c04138a0da07b0f69": {"ta_keywords": "meeting monitoring;meeting analyzer;meeting monitoring uses;analyzer meeting monitoring;xmath0 meeting analyzer;meeting analyzer meeting;threshold meeting monitoring;timelatency xmath0 meeting;xmath0 meeting;onset meeting approach;real timelatency xmath0;meeting approach uses;monitor participants;meeting;speech recognition monitor;participants enter meeting;recognition monitor thelatency;analyzer meeting;onset meeting;meeting approach;enter meeting;wireless sensor speech;meeting present;sensor speech;threshold monitor participants;monitor participants enter;timelatency threshold monitor;monitor thelatency;timelatency threshold meeting;timelatency xmath0", "pdf_keywords": ""}, "2bd54adb3b5588281396a4b5dae7db09496b2c61": {"ta_keywords": "nucleus scattering lhc;proton nucleus scattering;scattering lhc energies;scattering lhc;proton proton interaction;proton interaction proton;proton interaction;lhc energies analysis;proton nucleus cross;interaction proton;interaction proton nucleus;description proton proton;proton proton;values proton;description proton;nucleus scattering cross;lhc energies;present description proton;calculation proton;experimental data proton;applied calculation proton;calculation proton nucleus;values proton nucleus;proton;proton nucleus;qcd approach applied;different values proton;qcd approach;data proton nucleus;nucleus scattering", "pdf_keywords": "recognizing language semantics;language model predictions;recognizing language;language model checkers;language models;language models perform;language processing;language semantics;generating sentences;natural language processing;problem recognizing language;knowledge language models;language model;language semantics useful;language semantics fundamental;natural language;spelling language model;language semantics simple;machine reading comprehension;method generating sentences;accuracy spelling language;collected spelling language;models reading comprehension;knowledge language;explain language semantics;summarize knowledge language;perform better lexical;language processing process;sentences high identify;semantics"}, "5e657bc8097c12649d027ca3c16ff7d37df1354d": {"ta_keywords": "languages machine translation;machine translation;test languages experiments;machine translation settings;test languages;performance test languages;languages machine;languages experiments;languages experiments sets;outperforms heuristic baselines;training data;method learns weight;data scorer optimized;sets languages machine;learns weight training;learns weight;heuristic baselines;translation settings;experiments sets languages;learns;translation settings method;method learns;training data data;data data scorer;weight training data;data scorer;propose method learns;consistently outperforms heuristic;translation;outperforms heuristic", "pdf_keywords": "multilingual machine translation;multilingual training optimized;optimizing performance multilingual;training multilingual linguistic;translation model training;multilingual optimization;optimize multilingual data;different multilingual optimization;method multilingual training;optimize multilingual;method training multilingual;training multilingual systems;performance multilingual machine;resource multilingual translation;multilingual translation;multilingual optimization objectives;training multilingual;multilingual translation large;multilingual training multiple;performance multilingual;multilingual training;multilingual linguistic models;machine translation model;scorer optimize multilingual;models translate languages;multilingual machine;training multiple languages;multilingual data;scale multilingual training;optimizing resource multilingual"}, "302ae0d991d62dee82b63530b487a50469810af4": {"ta_keywords": "discovering interpretable spatial;interpretable spatial operations;finding interpretable spatial;interpretable spatial;spatial operations rich;spatial operations 3d;spatial operations;spatial actions;spatial actions propose;automatically discovering interpretable;spatial;complex spatial actions;discovering interpretable;pairs spatial operations;propose new neural;natural language descriptions;operations 3d;complex spatial;operations 3d blocks;pairs spatial;dataset pairs spatial;neural architecture achieves;rich natural language;neural;3d blocks world;natural language;finding interpretable;neural architecture;new neural architecture;new neural", "pdf_keywords": "embedding patterns language;language representation framework;annotators constructing models;language representation;natural language longstanding;patterns language representation;model embedding patterns;natural language;model based language;natural language previously;linguistic spatial reasoning;explaining natural language;annotators constructing;embedding patterns;learning representations natural;create realistic representations;annotators;build interpretable models;new model embedding;meaning natural language;able predict sentences;interpretable language language;richer natural language;interpretable language;model embedding;semantics language;spatial reasoning build;underlying semantics demonstrate;linguistic spatial;desired geometric linguistic"}, "a73d83e50b5687455336a2adce32a069c77ba163": {"ta_keywords": "spin dynamics antiferromagnetic;spin heisenberg antiferromagnet;dynamics antiferromagnetic heisenberg;heisenberg antiferromagnet magnetic;dynamics spin heisenberg;heisenberg antiferromagnet;antiferromagnetic heisenberg;dynamics antiferromagnetic;field spin dynamics;spin dynamics spin;magnetic field spin;antiferromagnet magnetic field;spin dynamics;spin dynamics studied;antiferromagnet magnetic;dynamics spin;control spin dynamics;spin heisenberg;antiferromagnetic;antiferromagnetic order;antiferromagnet;antiferromagnetic order parameter;field spin;dependent strength antiferromagnetic;control spin;effect field spin;strength antiferromagnetic order;spin flop model;strength antiferromagnetic;flop model magnetic", "pdf_keywords": ""}, "13608821aa3b369526221182dfbd3a8842549652": {"ta_keywords": "relays wireless networks;deployment relays wireless;optimal power allocation;deployment relays;relays distance;relays distance source;relays wireless;power allocation given;number relays distance;power allocation;given number relays;problem deployment relays;deployed network numerical;relays;number relays;wireless networks;allocation given number;allocation given;network numerical;provide optimal power;performance deployed network;optimal power;placement policy performance;networks provide optimal;deployed network;optimal;allocation;provide optimal;placement policy;total cost decision", "pdf_keywords": "distributed relay optimal;relay nodes optimal;relays optimal allocation;source distributed relay;relay optimal distance;optimal distance relay;relay optimal;relay channel optimum;optimal locations relays;relays optimal;distributed relay;relay channel network;wireless relay networks;deployment wireless relays;multi relay channel;assignment relay channel;relay placement wireless;deployment wireless relay;using propagation relays;optimal placement relay;wireless multi relay;placement relays optimal;channel relay;propagation relays;relay channel;relay deployment introduction;assignment wireless relay;power allocation relays;relay networks;relay channel obtain"}, "81e684d01bbfb1f4143bb2ffea36cc4791f0530c": {"ta_keywords": "reverberant automatic speech;reverberation time estimation;dereverberation method reverberation;approach reverberant automatic;reverberation;reverberant automatic;method reverberation;reverberation time;approach reverberant;speech recognition asr;novel approach reverberant;method reverberation time;reverberant;automatic speech recognition;dereverberation techniques feature;speech recognition;channel dereverberation method;automatic speech;estimation multichannel beamforming;recognition asr;single channel dereverberation;recognition asr task;time estimation multichannel;channel dereverberation;dereverberation techniques;state art dereverberation;multichannel beamforming;estimation multichannel;beamforming state art;dereverberation method", "pdf_keywords": ""}, "14dddd1d8cb2e8c5f4e9998fef84e715cb321ac9": {"ta_keywords": "trust user artificial;interpersonal trust trust;interpersonal trust;intelligence model trust;trust people defined;trust implicit;trust trust;trust trust user;trust trust people;model trust;model trust implicit;trust people;identical interpersonal trust;formalization contractual trust;contractual trust trust;trust user;model trust inspired;trust;contractual trust;trust inspired;trust inspired identical;present model trust;defined sociologists model;sociologists model;user artificial intelligence;user artificial;sociologists model rests;sociologists;artificial intelligence model;interpersonal", "pdf_keywords": "ai define trust;human ai trust;human trust ai;trust artificial intelligence;ai trust context;trust ai;trust ai present;ai trust fundamental;ai trust based;ai trust;trust ai use;ai trust argue;trust artificial;gaining trust ai;ai trust considering;definition trust artificial;formalization trust artificial;ai trust thatthis;ai trust outline;extrinsic trust artificial;trust intrinsic reasoning;trust interaction;trust intrinsic trust;trust defined sociologists;ai trust rooted;trust intrinsic;formalization extrinsic trust;intrinsic trust;formalization trust;human trust"}, "941e42ee75fc2bf07078bcfbd14bdf9ca7fe99ff": {"ta_keywords": "embeddings improves language;lingual word embeddings;improving language models;word embeddings improves;improves language models;language models pre;word embeddings method;word embeddings;language models;use cross lingual;embeddings improves;cross lingual word;improving language;method improving language;training cross lingual;language models order;cross lingual;language na threatened;na threatened language;language na;language results;lingual word;lingual;improves language;applied language na;models pre training;embeddings method;embeddings;language results use;threatened language results", "pdf_keywords": ""}, "981152cb3f1e11c9cee6af2275b57ef79c621934": {"ta_keywords": "text generation based;text generation;based text generation;text generation proposed;method text generation;generation based neural;resemblance human text;human text results;human text;generated samples achieving;generated samples;network based text;diversity generated samples;generation proposed method;text results algorithm;based text;text;generation based;diversity generated;text results;generation proposed;increase diversity generated;generated;generation;algorithm increase diversity;probability tedious possibly;probability tedious;inverse probability weighting;tedious possibly repetitive;based neural", "pdf_keywords": "generates repetitive sentences;text generated distribution;generate large vocabulary;neural text generation;text generation;word distribution repeated;generated texts;fluency generated text;word distribution;learning used generate;entropy word distribution;diversity neural text;text generation use;text generation suffers;distribution fluency generated;samples contain repetition;vocabulary high probability;neural text;generates repetitive;generated text;text generated;generate samples contain;method generate samples;repetitive sentences higher;repetitive sentences;generate samples;generated text method;generated text result;demonstrated generated texts;sentences higher diversity"}, "1ea337ac24503d9da8dd9bbf98aac0bfd5920834": {"ta_keywords": "speaking style transformation;transcripts speech results;transcripts speech;faithful transcripts speech;speech results clean;transcripts speech recognition;creating clean transcripts;speech results;clean transcripts human;clean transcripts;clean transcripts propose;transforming faithful transcripts;speaking style;transcripts propose model;transcripts human;transcripts propose;transcripts;results clean transcripts;label speaking style;faithful transcripts;speech recognition results;speech recognition;speech;human stenographers;transcripts human consumption;lecture speech;model faithful transcripts;speech demonstrates;performed human stenographers;human stenographers creating", "pdf_keywords": ""}, "b8b5b95a0471e0553a0e6cd5086f384cf0f4d4d8": {"ta_keywords": "translation paralinguistic information;translation paralinguistic;speech speech translation;method translation paralinguistic;paralinguistic information target;speech translation;target language speech;speech translation translation;paralinguistic information;estimated emphasis sequence;emphasis level translation;translates estimated emphasis;language speech speech;language speech;translation translation process;translation process;paralinguistic;translation process accomplished;information target language;speech speech;target language;translation module;estimated emphasis;word utterance resulting;word utterance;translation module translates;emphasis sequence;utterance resulting sequence;speech;level translation module", "pdf_keywords": ""}, "0f2ea810c16275dc74e880296e20dbd83b1bae1c": {"ta_keywords": "labelled list data;predicting distribution;predicting distribution data;method predicting distribution;labelled list;list data generated;data represented random;list data;problem predicting distribution;distribution data;represented random matrix;labelled;data represented;data generated;data form labelled;random matrix;random matrix corresponding;distribution data form;document data represented;form labelled list;predicting;represented random;method predicting;provide method predicting;corresponding document data;document data;list;distribution;data;corresponding query generated", "pdf_keywords": "query attention;attention reader model;attention machine comprehend;attention flow architecture;query contextual embeddings;contextual embeddings attention;query attention mechanism;documents based attention;attention reader;multi query attention;embeddings attention;embeddings attention mechanism;attention machine;attention flow;answer query document;gated attention reader;query representations;refine query representations;comprehension tasks machine;questions documents model;query embedding;query embedding intermediate;text gated attention;attention ga reader1;answer queries;query representations multiple;generative attentive reader;learning attention machine;comprehension tasks;based attention flow"}, "e58edbeb41f3d2d24832e6e3abb94baac754e3f7": {"ta_keywords": "text summarization metrics;summarization metrics;summarization metrics using;automatic text summarization;text summarization;reliability automatic text;summary level evaluation;summarization;automatic evaluation metrics;metrics using summary;automatic evaluation;reliability automatic evaluation;evaluation metrics reliable;extractive evaluation;abstractive extractive evaluation;extractive evaluation settings;evaluation metrics;evaluation metrics abstractive;evaluate reliability automatic;automatic text;using summary;datasets evaluate reliability;metrics reliable datasets;metrics abstractive extractive;metrics reliable;using summary level;popular datasets evaluate;summary level;metrics using;datasets evaluate", "pdf_keywords": "summarization datasets compared;text summarization assessing;summarization heavily rely;summarization assessing;comparing extractive summaries;extractive summarization heavily;predicting summarization;text summarization datasets;method predicting summarization;predicting summarization documents;abstractive summarization systems;summarization datasets;summarization systems;suitable predicting summarization;extractive summaries systems;comparing abstractive summaries;summaries compare;summarization heavily;summarization systems conduct;summarization assessing reliability;extractive abstractive summarization;summarization systems including;predict summarization documents;extractive summarization;abstractive summaries systems;summarization documents;generated summaries;text summarization;summaries systems;text summarization able"}, "0180c56bfbfb21243f8605e4c6f6aab2779d3ef0": {"ta_keywords": "natural language explanations;language explanations optimal;natural language explanation;explanation natural language;language explanations;explanations optimal action;explanations optimal;natural language;ap decision process;decision process mdp;action recommended ap;time natural language;user interacts mdp;user interacts;interacts mdp policy;language explanation explanation;explanation logic;interacts mdp;explanation logic allowing;explanations;language explanation;real time natural;mdp user interacts;ap decision;decision process;explanation designed;process mdp;ap;capable incremental;data explanation logic", "pdf_keywords": ""}, "197fcdfe05d0892ee7b4a98ef6fa74dfbcd14b48": {"ta_keywords": "convexity crowdsourcing inference;investigate convexity crowdsourcing;convexity crowdsourcing;crowdsourcing inference;crowdsourcing inference problem;crowdsourcing;inference ground truth;inference axioms;inference ground;inference axioms impossible;ensure convexity inference;convexity inference;inference;joint inference ground;function inference axioms;convexity inference problem;inference problem involves;joint inference;inference problem hand;truth answers worker;involves joint inference;inference problem;objective function inference;answers worker;ground truth answers;answers worker abilities;assumptions objective;natural assumptions objective;function inference;assumptions objective function", "pdf_keywords": "crowdsourcing convex theorems;convex inference crowdsourcing;crowdsourcing convex;model crowdsourcing convex;inference crowdsourcing theorem;inference crowdsourcing;crowdsourcing tasks axioms;crowdsourcing theorem states;convex inference human;crowdsourcing theorem;crowdsourcing theorem theorem;guaranteeing convex inference;functions crowdsourcing tasks;natural assumptions crowdsourcing;crowdsourcing tasks;inference human computation;assumptions crowdsourcing;model crowdsourced;probabilistic model crowdsourced;model crowdsourcing;functions crowdsourcing;ensure convexity inference;assumptions crowdsourcing unfortunately;convex inference;reasonable model crowdsourcing;crowdsourcing unfortunately impossible;method crowdsourcing;convexity inference;information accuracy crowdsourcing;model crowdsourced crowdsourced"}, "18c00a9b1e6fde799ec5100cf0b1f37c306d061f": {"ta_keywords": "approach web search;web search;web search case;use data centric;data centric;data oriented;use relational database;data oriented approach;search case studies;information web particular;data centric data;information web;centric data oriented;approach web;based hypothesis database;database consisting information;relational database;database;consisting information web;hypothesis database structured;web particular topic;use statistical learning;oriented approach web;hypothesis database;database structured way;relational database consisting;search;search case;database structured;use relational", "pdf_keywords": ""}, "5c283474bbb4838160410e24d33ce89ebaf32c07": {"ta_keywords": "speaker clustering known;model speaker clustering;speaker clustering;speaker clustering impact;clustering known variational;model speaker;estimate multiscale;approaches estimate multiscale;standard model speaker;estimate multiscale version;speaker;optimization methods multiscale;clustering known;clustering;known variational method;multiscale;methods multiscale;clustering impact;variational method recently;multiscale version;variational method;likelihood estimator comparison;maximum likelihood estimator;maximum likelihood;clustering impact difference;version maximum likelihood;likelihood estimator;known variational;multiscale version standard;likelihood", "pdf_keywords": ""}, "2acc25a01a7ab7cd6b1a75d534ad29ea7d26f92d": {"ta_keywords": "subtopic retrieval using;retrieval subtopic retrieval;retrieval subtopic;subtopic retrieval;performing subtopic retrieval;retrieval problem subtopic;subtopic retrieval subtopic;problem subtopic retrieval;relevance ranking;subtopic retrieval concerned;baseline relevance ranking;relevance ranking data;likelihood relevance ranking;subtopics query topic;traditional retrieval problem;non traditional retrieval;relevance ranking shown;relevance mmr ranking;retrieval using;retrieval using statistical;outperform baseline relevance;retrieval concerned finding;query likelihood relevance;traditional retrieval;retrieval;subtopics query;different subtopics query;retrieval problem;baseline relevance;retrieval concerned", "pdf_keywords": ""}, "0d3baef146655c5727452ccc0dd680d21d92ae4e": {"ta_keywords": "model consumer heterogeneity;consumer heterogeneity based;consumer heterogeneity;heterogeneity based hierarchical;hierarchical version gaussian;approach model consumer;hierarchical structure model;model consumer;heterogeneity based;gaussian distribution approach;consumer;approach based hierarchical;gaussian distribution;heterogeneity;version gaussian distribution;distribution approach proposed;distribution approach;hierarchical structure;based hierarchical;gaussian;kind hierarchical structure;following characteristics model;based hierarchical version;characteristics model;hierarchical;valid kind hierarchical;version gaussian;kind hierarchical;distribution;structure model", "pdf_keywords": ""}, "0735fb79bf34698c1df4461a05ed51c232c412e4": {"ta_keywords": "transformer encoder;transformer learn;transformer encoder form;model transformer encoder;transformer learn given;problems transformer learn;encoder form programming;encoder;computational model transformer;transformer;analyze problems transformer;programming language;programming language model;model transformer;encoder form;programming;sorting dyck languages;histograms sorting dyck;represented specific attention;problems transformer;given task represented;language model;form programming language;propose computational;task represented;dyck languages;computational;language model used;histograms sorting;analyze problems histograms", "pdf_keywords": "program neural transformer;program neural;transformer ability program;expressive efficient computation;transformers based random;neural transformer;processing language;access processing language;programming language;transformers trained recurrent;transformers expressive;random variable generator;patterns transformers trained;asp program neural;encoder form programming;simple expressive efficient;representation transformer;transformer encoder simple;learned transformer;computation task represented;processing language asp;rnns;assignment rnns;representation transformer using;transformer encoder;approach representation transformer;trained transformers;transformer using random;operations language aspa;networks rnns"}, "6fea118a29d78340ae26c465ff06e80e55efbe3b": {"ta_keywords": "incremental reading model;question answering;answer question model;question answering problem;incremental reading;based incremental reading;question model;approach question answering;read text answer;text answer question;language models model;answering problem based;unidirectional language models;language models;question model incorporate;reading model;question answer question;problem based incremental;answering problem;language models necessarily;text answer;answer question answer;reading model model;based incremental;incremental;answer question;answering;read text;reading;unidirectional language", "pdf_keywords": "incrementally answering questions;incrementally answering;framework incrementally answering;comprehends text incrementally;question answering;comprehension question answering;answering introduce incremental;models text comprehension;context question answering;question answering inspired;read answer slice;method reading comprehension;text comprehension;incremental models text;question answering introduce;text incrementally possible;reading comprehension;information incrementally understand;text incrementally;comprehension problem squad;information incrementally;answering questions called;continual learning learn;answering questions;reads comprehends text;text comprehension formulate;incrementally understand;information learned slice;reading comprehension context;continual learning"}, "92731a953ad063eab1bc90dc541fb956f147a6ba": {"ta_keywords": "designing preference processing;preference based processing;designing preference based;preference processing;designing preference;dining experience enjoyable;preference based graphical;preference based;making dining experience;preference processing problem;table use preference;dining experience;problem designing preference;process making dining;use preference based;making dining;dining;preference;use preference;table use;graphical representation table;table;processing problem designing;enjoyable consider problem;based processing;based graphical representation;experience enjoyable consider;based processing process;representation table use;experience enjoyable", "pdf_keywords": ""}, "0791fe161d947d1e4d3af279b261155b88bc9ddf": {"ta_keywords": "regularizing deep neural;approach regularizing deep;regularizing deep;series temporal clustering;novel approach regularizing;temporal clustering;temporal clustering proccessing;postulated temporal clustering;deep neural;temporal clustering advantageous;regularizing sequences partial;approach regularizing;regularizing sequences;regularizing;approach regularizing sequences;deep neural networks;neural networks;neural;clustering proccessing invariance;time series temporal;series temporal;proccessing time series;clustering advantageous predicting;clustering proccessing;neural networks based;networks;sequence partial differential;time series;sequences partial differential;clustering", "pdf_keywords": "augmentation temporal clustering;introduce temporal clustering;temporal clustering;investigate temporal clustering;postulated temporal clustering;temporal clustering invariance;temporal clustering approximate;uses temporal clustering;temporal clustering framework;series clustering timestamps;time series clustering;clustering timestamps;temporal coarsening;clustering invariance proccessing;data augmentation temporal;temporal coarsening irregular;temporal clustering exploits;regular present clustering;deterministic temporal coarsening;timestamps construct clusters;shorten time series;regularly spaced timestamps;clustering approximate invariance;clustering timestamps test;clustering invariance;clustering invariance proposed;series clustering;time series regularly;irregular time series;time series define"}, "8d35230fec724398bed3f5939e9fa6a94f55a785": {"ta_keywords": "differentially private algorithms;private machine learning;differential privacy;privacy machine learning;learned differentially privately;differentially private;large differentially private;interplay differential privacy;differential privacy machine;differentially private machine;functions differentially private;private datasets;private algorithms;private algorithms present;differentially privately;data private datasets;data private;private datasets number;public data private;privacy;differentially privately upper;privacy machine;learned differentially;privately;private;privately upper bounds;private machine;address learned differentially;public data;loss functions differentially", "pdf_keywords": "differentially private learning;differential privacy learning;learning differential privacy;differentially private algorithms;differentially private algorithm;differentially private data;differentially private clustering;theorem differential privacy;differential privacy theorem;algorithms differentially private;private machine learning;differential privacy privacy;differential privacy;differential privacy data;private algorithms differentially;differential privacy laplacian;differential privacy sample;preserve differential privacy;differentially private;private kernel learning;privacy differentially private;introduce differentially private;differential privacy able;analysis differentially private;differentially private proposed;privacy preserve differential;training differentially private;propose differentially private;differentially private propose;differential privacy preserve"}, "6eae6230ae277b6915706ec05241c8db6b9fab86": {"ta_keywords": "similarity search library;similarity search similar;similarity search;new similarity search;search similar;search similar existing;space based search;method similarity search;search library based;search library designed;similarity;based search library;present new similarity;new similarity;similar existing library;search library;metric space based;new method similarity;based search;library designed efficient;library implements competitive;method similarity;non metric;effective non metric;non metric space;related library;search;similar existing;library based new;metric", "pdf_keywords": ""}, "8872e32284467fcbeadd1edd2f11aff077de4ccf": {"ta_keywords": "rule learning algorithm;proposed rule learning;new rule learning;rule learning;learning algorithm irep;learning algorithm;learning algorithm large;learning algorithm competitive;based new rule;new rule;benchmark problems irep;algorithm irep;rule;algorithm based new;benchmark;algorithm irep recently;proposed rule;benchmark problems;recently proposed rule;eecientcientciently efficient propose;algorithm competitive recently;algorithm competitive;improved error rates;eecientcientciently efficient;algorithm large;learning;algorithm based;extremely eecientcientciently efficient;algorithm;algorithm large collection", "pdf_keywords": ""}, "47442ea4c28d631a9d46a9c23454684b834e49ea": {"ta_keywords": "predicting semantics interreference;predicting semantics;interreference words based;semantics interreference words;approach predicting semantics;interreference words;distributional semantics corpus;semantics interreference;semantics corpus present;semantics corpus;knowledge underlying corpus;essential distributional semantics;distributional semantics;underlying corpus;unsupervised neural;use unsupervised neural;interreference;underlying corpus approach;unsupervised neural networks;corpus;corpus present new;corpus present;semantics;new dataset predicting;corpus approach;dataset predicting;neural;corpus approach performs;unsupervised;words based", "pdf_keywords": ""}, "df873bde0b44e543634d109a7a8b1ba7dfaa8187": {"ta_keywords": "learns latent ontological;topic models;models topic models;topic models applied;latent ontological structure;structure input corpus;latent ontological;input corpus;facts corpus;learns latent;unsupervised model learns;corpus match learned;ontological structure input;identifies facts corpus;model learns latent;corpus identifies facts;corpus;facts corpus match;ontological structure;propose unsupervised model;unsupervised model;learned structure model;input corpus identifies;ontological;stochastic block models;block models topic;model learns;based mixture stochastic;corpus identifies;learned structure", "pdf_keywords": ""}, "22dd93fe1a0b8e9cb83eaff6e2ecca0cd6693294": {"ta_keywords": "voice activity detection;recognition online speech;speech recognition online;connectionist temporal classification;speech recognition;automatic speech recognition;speech recognition pid;voice activity;end automatic speech;automatic speech;temporal classification ctc;approach voice activity;temporal classification;speech proposed method;approach connectionist temporal;online speech;connectionist temporal;activity detection vad;classification ctc;voice;recognition pid architecture;online speech proposed;recognition online;activity detection;recognition pid;novel approach connectionist;connectionist;classification ctc novel;approach voice;approach connectionist", "pdf_keywords": "voice activity detection;recognition voice activity;threshold detecting speech;speech recognition;recognition online speech;speech recognition online;online speech recognition;detect voice segments;speech recognition voice;detecting speech;recognition short audio;automatic speech recognition;detect speech;detect voice;speech recognition e2e;integrate voice activity;voice activity;recognition voice;able detect speech;recognition audio;able detect voice;automatic speech;integrates voice activity;voice segments;end automatic speech;recognition audio recordings;online speech interface;detecting speech paper;offline recognition;speech interface"}, "9d332ad27bfce66ee725b413aa07bd93c355efdf": {"ta_keywords": "filters natural language;transformations filters natural;creation transformations filters;create transformations filters;transformations filters;natural language models;natural language processing;transformations filters manner;novel data augmentation;data augmentation;data augmentation techniques;language processing framework;filters natural;novel pythonbased;filters manner;language models;natural language;present novel pythonbased;filters manner similar;language processing;filters;novel pythonbased framework;augmentation techniques create;techniques create transformations;create transformations;creation transformations;augmentation;transformations;pythonbased;popular natural language", "pdf_keywords": "spin orbit coupling;dynamics xmath0he xmath1he;spin orbit interaction;spin orbit;dynamics xmath0he;interaction dynamics xmath0he;represent critical noise;xmath2he xmath3 interaction;effect spin orbit;xmath3 interaction;critical noise amplitude;coupled pulse sequences;range spin orbit;reduce spin orbit;information pulse sequences;synchronization coupled pulse;noise containing phase;synchronization coupled;critical noise containing;wide range spin;xmath3 interaction attractive;spin;containing critical noise;critical noise;simultaneous synchronization coupled;xmath3;xmath2he xmath3;phase information pulse;xmath1he xmath2he xmathwe;simultaneous synchronization"}, "8057a5e7bcb0be7059a6e632124bc861b533c794": {"ta_keywords": "chime challenge chime4;challenge chime4 uses;challenge chime4;chime challenge;4th chime challenge;channel track task;task 4th chime;forward neural network;chime4;chime4 uses alternating;based channel track;forward neural;channel track;chime4 uses;track task 4th;strategy forward neural;4th chime;alternating pattern recognition;track;neural network based;chime;neural network fnm;neural network;track task;pattern recognition;pattern recognition ar;recognition ar nonlinear;based acoustic model;development neural network;neural", "pdf_keywords": ""}, "88167f36dced91c279162d68af7225f2b4e2091c": {"ta_keywords": "language model pre;pre trained models;language data performance;human language data;model pre trained;language data;performance language model;language model;trained models directly;trained models;data sets intrinsic;intrinsic data performance;trained models certain;non human language;pre trained class;data performance language;human language;intrinsic data;trained class data;performance model pre;pre trained;close trained models;models directly data;performance language;effect intrinsic data;performance close trained;models directly;data performance;language;sets intrinsic", "pdf_keywords": "transformer language models;downstream natural language;corpora models trained;masked language models;underlying corpora trained;language models generalized;language models;transformer language model;language models corpora;models natural language;pre training deep;pre trained models;pre trained unstructured;models pre trained;tune language models;training deep;trained models downstream;training deep transformer;pre trained structured;pre training models;natural language tasks;trained unstructured;corpora models;dimensional natural languagewe;language model lsp;transformer language;deep transformer models;language model;trained structured;transformers pre trained"}, "f752bf6f8c1502b8cb58aa1483ef598f9fc0d44c": {"ta_keywords": "random matrix elements;random rank arbitrary;uniformly random rank;rank arbitrary net;nets uniformly random;random matrix;random rank;set random matrix;matrix elements nets;generating nets uniformly;generating nets;counting ranking complete;counting ranking;nets uniformly;method counting ranking;ranking complete set;nodes xmath3 rank;method generating nets;arbitrary net;rank arbitrary;elements nets;matrix elements;ranking complete;number nodes;complete set random;nodes;arbitrary net present;ranking;xmath3 rank arbitrary;xmath3 rank", "pdf_keywords": ""}, "5ebe542ee1a7eab7aad8e36ed53dbdd7ebd98c8d": {"ta_keywords": "curvilinear neural network;accuracy curvilinear neural;based autoencoders architecture;autoencoders architecture;autoencoders architecture able;network based autoencoders;autoencoders;based autoencoders;high accuracy curvilinear;method curvilinear neural;encapsulation autoencoders architecture;accuracy curvilinear;curvilinear neural;based encapsulation autoencoders;encapsulation autoencoders;neural network architecture;neural network;high accuracy cur;novel method curvilinear;neural network based;curvilinear;method curvilinear;accuracy cur;neural;network architecture method;high accuracy;architecture able achieve;network architecture;architecture method;extremely high accuracy", "pdf_keywords": ""}, "73484141ca58d9714ac592e3667de416322b51eb": {"ta_keywords": "wavelet transform;wavelet transform new;crossing representation wavelet;representation wavelet transform;representation wavelet;wavelet;zero crossing representation;reconstruction actual image;image proposed method;representation applied reconstruction;transform new representation;new zero crossing;proposed zero crossing;zero crossing;transform;applied reconstruction;crossing representation extended;crossing representation;applied reconstruction actual;applied actual image;transform new;image proposed;iterative linear;uses iterative linear;actual image proposed;iterative linear operations;method proposed zero;new zero;new representation applied;representation extended dimensional", "pdf_keywords": ""}, "29263fa3632951be0ca617988d7c9ce651e74393": {"ta_keywords": "multilingual training encoder;multilingual training performance;encoder decoders multilingual;decoders multilingual model;multilingual training beneficial;model multilingual training;decoders multilingual;multilingual training;translation systems compare;effects multilingual training;multilingual model;multilingual model multilingual;translation systems;model multilingual;multilingual;performance machine translation;varieties multilingual training;machine translation systems;machine translation;effects multilingual;training encoder decoders;low resource languages;study effects multilingual;languages lrls;training encoder;training beneficial encoders;different varieties multilingual;varieties multilingual;decoders low resource;languages", "pdf_keywords": "multilingual translation models;translation models multilingual;multilingual machine translation;training multilingual models;performance multilingual translation;multilingual training encoder;decoder multilingual trained;encoder multilingual training;multilingual models encoders;tuning multilingual models;training multilingual machine;train multilingual models;translation models train;encoders multilingual models;improving multilingual machine;models train multilingual;multilingual translation;improving multilingual;fine tuning multilingual;multilingual training multilingual;models target multilingual;multilingual models beneficial;multilingual models target;multilingual encoders;multilingual models;training multilingual;useful improving multilingual;multilingual neural machine;multilingual trained;multilingual encoders decoders"}, "8b1be80cc1fabcd9ccea76d9a8830e2b07e71f0c": {"ta_keywords": "packing optimal raspberries;raspberries optimal packing;raspberries optimal shape;optimal raspberries optimal;vibration optimal raspberries;optimal raspberries determined;optimal raspberries;metamaterial analysis optimal;raspberries optimal;optimal packing optimal;optimal packing;packing optimal;bouquet raspberries optimal;optimal vibration optimal;metamaterial analysis;optimal shape;detuning optimal vibration;vibration detuning optimal;shape bouquet raspberries;optimal shape determined;vibration optimal detuning;optimal vibration;optimal vibration detuning;combination optimal vibration;analysis optimal shape;metamaterial;raspberries determined;present metamaterial analysis;vibration optimal vibration;vibration optimal", "pdf_keywords": ""}, "76b36a059c0d8d66a1bf910de32b34dba19482fa": {"ta_keywords": "streaming encoder decoder;streaming encoder;blockwise synchronous decoding;algorithm streaming encoder;synchronous decoding;encoder decoder;decoding algorithm streaming;synchronous decoding algorithm;encoder decoder combines;efficiency proposed decoding;decoder combines endpoint;encoder;proposed decoding;decoding;proposed decoding algorithm;decoder;decoding algorithm achieves;decoding algorithm;decoder combines;streaming;algorithm streaming;latency reduction;achieves latency reduction;endpoint prediction;novel blockwise synchronous;endpoint prediction endpoint;algorithm achieves latency;prediction endpoint;achieves latency;blockwise synchronous", "pdf_keywords": "encryption endpoint encoder;synchronous decoding algorithm;synchronous decoding;blockwise synchronous decoding;synchronization encoder endpoint;encryption endpoint;endpoint encoder decoder;based encryption endpoint;decoding algorithm hybrid;stream based encryption;block synchronization encoder;synchronization encoder;beam search endpoint;decoding algorithm;encoder endpoint;endpoint encoder;decoding;endpoint prediction search;endpoint prediction stream;encoder endpoint post;decoding algorithm amplitude;encryption;endpoints resumes decoding;beam search algorithm;decoder;search endpoint prediction;encoder decoder;endpoint prediction;encoder;endpoint prediction endpoint"}, "caabc3d0c5ece9d44fb2216a347362d4609934c1": {"ta_keywords": "programming languages trained;code polysourcer trained;12 programming languages;model large language;languages trained models;programming languages;large language model;language model code;large language;programming languages single;polysourcer trained 249gb;languages single machine;languages trained;code polysourcer;trained 249gb code;language model;programming;open source model;model code polysourcer;12 programming;languages;polysourcer outperforms models;polysourcer trained;available polysourcer outperforms;models open sources;source model trained;languages single;code 12 programming;available polysourcer;polysourcer outperforms", "pdf_keywords": "language models code;code natural language;trained code language;source languages code;code language trained;code language;source language model;large language models;code models;generalized languages code;large language model;language models;open source language;open source languages;new large language;source languages;lingual corpus code;source language;language model code;programming languages provide;various programming languages;code synthesizing;languages code;codex generalized languages;programming languages literature;pretrained language models;programming languages;languages provide models;popular programming languages;language models using"}, "3a8129e6fe3ad9bc3a51e44da32424e38612e4cc": {"ta_keywords": "proof called belief;quality proof called;quality proof;measuring quality proof;proof depth manner;belief propagation;belief propagation bp;proof depth;called belief propagation;proof method effective;proof method;length proof method;bp proof depth;measuring length proof;proof called;propagation bp proof;length proof;bp proof;formulas measuring;belief;proof;called belief;formulas measuring length;measuring quality certain;quality certain;propagation;class formulas measuring;effective measuring quality;method measuring quality;measuring quality", "pdf_keywords": "database tensorial reasoning;tensorial deductive database;deductive database tensorial;embedding probabilistic logic;relational logic embedding;logic embedding;stochastic logic programming;knowledge deep learning;deductive database;probabilistic logic programming;logic embedding method;probabilistic logic programs;large knowledge bases;logic programming models;stochastic logic program;popular stochastic logic;stochastic logic;database tensorial;semantics probabilistic logic;probabilistic logic program;knowledge bases big;deductive database called;logic programs network;knowledge bases;knowledge embedding;logic programming;probabilistic logic;knowledge embedding improve;logic programming model;logic programs"}, "891fd2690a21f29b2ab54ee2249261d93c8cbc5c": {"ta_keywords": "crowdsourcing worker answers;setting crowdsourcing worker;crowdsourcing worker;setting crowdsourcing;crowdsourcing;worker answers questions;stage setting crowdsourcing;worker answers;answers questions;answers looking reference;change answers looking;change answers;answers looking;mechanisms incentivize workers;allowed change answers;answers questions allowed;incentivize workers;answers;workers act appropriately;workers;answer mathematically formulate;reference answer mathematically;incentivize workers act;develop mechanisms incentivize;incentivize;mechanisms incentivize;reference answer;questions;questions allowed change;looking reference answer", "pdf_keywords": ""}, "0ce6db2fb8c691ff8a89bd01f379ce92b1d248d0": {"ta_keywords": "contagion classification soft;contagion classification;models contagion classification;clustering based multinomial;probabilistic models contagion;classification soft clustering;occurrence introduce probabilistic;soft clustering;classification soft;soft clustering based;based multinomial distribution;words higher frequencies;multinomial distribution;classification;clustering;frequencies occurrence introduce;based multinomial;probabilistic models;frequencies occurrence;statistical learning;introduce probabilistic models;models contagion;introduce probabilistic;multinomial;probabilistic;statistical learning tools;contagion;clustering based;higher frequencies occurrence;comprehensive set statistical", "pdf_keywords": ""}, "6994b9860248aea10f8b8bac74e87afd3fcdc842": {"ta_keywords": "semistructured documents;sets named entities;applied semistructured documents;semistructured documents written;semistructured document;semistructured document written;applied semistructured document;named entities;named entities using;named entities method;semistructured;applied semistructured;expanding sets named;entities using web;benchmark sets languages;method applied semistructured;entities using;method expanding sets;entities;language utility;sets languages;expanding sets;sets named;entities method;language utility method;sets languages superior;documents written;language experimental results;entities method applied;document written", "pdf_keywords": ""}, "49f9afa4d0405019d01b55529ce4167380acc103": {"ta_keywords": "input speech waveform;direct waveform modification;speech waveform;articulatory movements based;waveform modification methods;improvements synthetic speech;synthetic speech;waveform modification proposed;waveform modification;synthetic speech keeping;filter input speech;generation unobserved articulatory;speech waveform time;direct waveform;unobserved articulatory movements;proposed direct waveform;method direct waveform;articulatory movements;unobserved articulatory;using vocoderbased excitation;waveform;input speech;vocoderbased excitation signal;articulatory;speech keeping capability;vocoderbased excitation;unmodified modified spectral;using vocoderbased;avoid using vocoderbased;signal generation", "pdf_keywords": ""}, "e1bb329621de73d08c47beae9b5439a1c244eb1a": {"ta_keywords": "novelty detection;various novelty detection;novelty detection based;novelty detection scenarios;learning method novelty;method novelty detection;contrastive learning method;shifted instances training;novel contrastive learning;contrastive learning;novelty;various novelty;detection based contrasting;method various novelty;method novelty;contrasting shifted instances;instances training;shifted instances;novel contrastive;propose novel contrastive;image benchmark datasets;detection;instances training scheme;detection scenarios;various image;sample instances similar;image benchmark;instances similar;contrastive;detection scenarios including", "pdf_keywords": "novelty detection generative;novelty detection;approach novelty detection;novelty detection framework;detect novelty;method novelty detection;learning classifying shifted;learning method novelty;detect novelty data;learning able detect;novelty encoded similarity;detection generative;used detect novelty;detection supervised;novelty encoded;detection deep;contrast based distribution;novelty data similarity;learning contrastive;detection generative framework;detection supervised learning;supervised detection;classifying patterns contrastive;learning improve detection;contrastive learning;supervised contrastive;contrastive learning method;supervised learning contrastive;novelty data;classifying shifted data"}, "931a103258c96a1230dc5c7e38a1cd0b095b9d62": {"ta_keywords": "learn language model;language model directly;language model;language model construction;gram language model;language model prior;large vocabulary speech;approach language model;model prior linguistic;learning word;vocabulary speech;using acoustic model;acoustic model;learning word boundaries;acoustic model scores;potentially large vocabulary;prior linguistic knowledge;boundaries gram language;gram language;word boundaries gram;integrated learning word;learn language;linguistic knowledge;linguistic knowledge proposed;large vocabulary;model construction using;model construction;prior linguistic;vocabulary;construction using acoustic", "pdf_keywords": ""}, "a5690b0a514a7cbc913871e41e54c9ad4f6362db": {"ta_keywords": "robustness machine translation;machine translation;machine translation mt;improving robustness machine;task improving robustness;translation mt;robustness machine;language pairs;translation mt focus;improving robustness;translation;pairs english;pairs english english;language pairs english;consisting noisy comments;focus language pairs;reddit best improvement;mt focus language;noisy comments;robustness;english japanese evaluate;english english japanese;japanese evaluate systems;noisy comments reddit;comments reddit best;english japanese;shared task improving;language;task improving;focus language", "pdf_keywords": "translation systems robustly;robustness machine translation;machine translation training;improves translation accuracy;machine translation presence;machine translation systems;machine translation;translation systems;robustly understand translate;task machine translation;machine translation mt;neural machine translation;translation accuracy;proposed machine translation;improves translation;optimize quality translation;translation training task;translation training;ability machine translation;translation improves translation;quality translation output;translate noisy text;translation accuracy combining;translation improves;translation extraction;translation mt models;translation data;quality translation;strength machine translation;translation directions based"}, "6aecc93c2d61da073b70dec19795172ca1ff3405": {"ta_keywords": "counterfactual invariant predictors;counterfactual invariance formalization;learning approximately counterfactual;introduce counterfactual invariance;counterfactual invariance outof;predictors access counterfactual;counterfactual invariance;counterfactual invariant;predictions connect counterfactual;connect counterfactual invariance;approximately counterfactual invariant;introduce counterfactual;counterfactual;change model predictions;counterfactual examples;approximately counterfactual;invariant predictors;invariant predictors access;access counterfactual examples;input change model;model predictions;invariance formalization;model predictions connect;access counterfactual;connect counterfactual;outof domain model;invariance formalization requirement;invariance outof domain;predictors;domain model", "pdf_keywords": "predictions counterfactual invariance;predictor counterfactual data;learning counterfactual invariant;predictor counterfactual invariance;learning counterfactual;counterfactual invariant predictors;predictors counterfactual invariance;predictor counterfactual;predictors counterfactual;counterfactual invariance achieved;counterfactually invariant predictor;predictions counterfactual;enforcing counterfactual invariance;predictor better counterfactually;counterfactual invariance underlying;counterfactual invariance data;counterfactual invariance defined;counterfactual invariance using;counterfactual invariance;counterfactual invariance measured;counterfactual invariance actually;counterfactual data;counterfactual invariance formalize;counterfactual invariance fundamental;counterfactual data actually;behavior predictor counterfactual;counterfactual invariance impossible;effects predictor counterfactual"}, "dfd4beb1ecf70b07eb4a52e6ae58f3357e66f478": {"ta_keywords": "robust speech recognition;robust speech;model robust speech;performance robust speech;speech recognition;speech recognition model;noise model robust;text recognition variety;text recognition;incorporating noise model;rates text recognition;recognition model;model robust;incorporating noise;noise model;recognition model based;method incorporating noise;robust;recognition;representations computationally;recognition variety;based empirical representation;empirical representation;performance robust;empirical representation class;representations computationally inexpensive;valued representations computationally;recognition variety settings;increase performance robust;real valued representations", "pdf_keywords": "noise robust deep;speech recognition noisy;noisy speech dataset;learning noise robust;learning noise;training noise speech;noise speech recognition;recurrent autoencoders;deep recurrent autoencoders;robust speech recognition;robust deep recurrent;robust speech;recurrent autoencoders rls;noise speech model;autoencoders rls denoise;robust deep;deep learning;learning deep learning;noisy speech using;learning deep;recognition noisy noisy;deep learning deep;noisy speech;approach robust speech;noise robust models;synthesizing noisy speech;recognition noisy;model based deep;autoencoders;data augmentation models"}, "a427334e296b6be27c3a9c7d6b942d6468e487b8": {"ta_keywords": "optimal deployment;network deployment cases;optimal deployment policy;deploy nodes network;network deployment;connected network deployment;relay placement;performance optimal deployment;relay placement problem;optimal connected network;problem relay placement;deploy nodes;deployment policy numerical;objective deploy nodes;deployment cases objective;optimal connected;deployment cases;deployment;deployment policy;problem optimal connected;nodes network;problem optimal;placement problem;relay;nodes network high;placement problem evaluate;optimal;formulate problem relay;deploy;network high connectivity", "pdf_keywords": ""}, "823956ee7b994735f3605f426a71e7f85d86f1d4": {"ta_keywords": "unsupervised frame induction;frame induction task;frame induction;frame induction demonstrate;perform unsupervised frame;unsupervised frame;task unsupervised frame;frame induction cast;unsupervised task unsupervised;results unsupervised frame;unsupervised task;approach unsupervised;approach unsupervised task;based approach unsupervised;induction task vogel;perform unsupervised;graph based;propose graph based;triples perform unsupervised;induction task;task unsupervised;induction cast;induction cast problem;unsupervised;triclustering;propose graph;graph based approach;induction;dependency triples;cast problem triclustering", "pdf_keywords": "clustering natural language;induce semantic frames;frame induction clustering;clustering frame induction;verb class clustering;semantic frames;frame induction framework;unsupervised frame induction;frame induction context;verb classes frame;frame induction task;models semantic frames;semantic frames used;representations semantic frames;approach frame induction;semantic frames respective;semantically structured;semantic frames shown;clustering polysemous verb;classes frame induction;graph clustering frame;frame induction techniques;frame induction;induce semantic;syntactically analyzed corpus;frame evoking predicates;method frame induction;graph clustering natural;annotation present graph;related underlying text"}, "6eb5029dabd60eb47fddebb5919c613d399fddc6": {"ta_keywords": "discriminability speech recognizer;sequential discriminant analysis;speech recognizer proposed;speech recognizer;sequential discriminant;speech recognition;discriminability speech;improve discriminability speech;method speech recognition;extension sequential discriminant;discriminant analysis;discriminant analysis method;speech recognition tasks;speech recognition called;illustrated speech recognition;discriminant;recognizer proposed method;discriminability;improve discriminability;used improve discriminability;analysis method speech;training criterion proposed;recognition called sequential;sequential maximum mutual;recognizer proposed;mmi training criterion;method illustrated speech;method speech;based sequential maximum;information mmi training", "pdf_keywords": ""}, "edca37b2004861513c54e7e97b64d4e00e72003f": {"ta_keywords": "clause learnable polynomials;learnable polynomials single;learnable polynomials;single clause learnable;learning polynomials single;clause learnable;learning polynomials;learning polynomial graph;learning polynomial;method learning polynomials;polynomials single clause;equivalent learning polynomial;clause equivalent learning;predicting reducibilities polynomials;polynomial graph;reducibilities polynomials single;reducibilities polynomials;polynomials single;polynomial graph graph;polynomials;result polynomials single;polynomial;learnable;single clause;result polynomials;single clause equivalent;single clause use;clause;clause equivalent;method learning", "pdf_keywords": ""}, "49775f20431c4a605c5dcc7111c9fe785bf00c62": {"ta_keywords": "partial differential learning;pde learns strategy;differential learning pde;differential learning;learns strategy optimal;learning pde learns;strategy partial differential;learns strategy;pde learns;learns strategy suboptimal;superior pde learns;learning pde;risk aversion objective;strategy partial;given risk aversion;learning;based strategy partial;learns;risk aversion;optimal given risk;strategy superior pde;strategy optimal;aversion objective proposed;partial differential;aversion objective;strategy optimal given;partial;aversion objective demonstrate;risk;given risk", "pdf_keywords": "risk reinforcement learning;coherent risk reinforcement;policy gradient tractable;policy gradient methods;consider policy gradient;gradients risk allocation;learning policy function;policy gradient reinforcement;policy gradients;risk gradient;propose policy gradient;policy gradient;convergence policy gradient;gradient value policy;sensitive policy gradient;policy gradient algorithm;coherent risk gradient;sensitive policy gradients;policy gradient pg;policy evaluation optimization;risk reinforcement;risk objective policy;estimate policy gradient;optimality coherent risk;class policy gradients;gradient reinforcement learning;optimality policy class;optimal risk allocation;risk gradient dominated;optimal policy"}, "4350ce87dd3ec067f1e583ad415f71ef4ba6075e": {"ta_keywords": "novel dependency parser;modified dependency parser;dependency parser;dependency parser easy;original dependency parser;parser use training;dependency parser use;linguistic information annotated;training domain adaptation;extract useful linguistic;use domain adaptation;parser easy train;domain adaptation;domain adaptation approach;parser use domain;parser easy;parser;annotated data;information annotated data;parser use;domain adaptation advantage;useful linguistic information;information annotated;training domain;introduce novel dependency;linguistic information;use training domain;useful linguistic;train modified dependency;dependency", "pdf_keywords": ""}, "598e2d69d3573adae1f0e3bbe54d10c43f48e0b0": {"ta_keywords": "proximal stochastic gradient;stochastic gradient method;stochastic gradient;optimization distributional drift;method proximal stochastic;stochastic tracking method;iterates proximal stochastic;stochastic optimization distributional;convergence guarantees stochastic;stochastic optimization;proximal stochastic;guarantees stochastic optimization;stochastic tracking;proposed stochastic tracking;gradient method proximal;distributional drift;guarantees stochastic;distributional drift use;gradient method efficient;stochastic;gradient noise time;track iterates proximal;proposed stochastic;optimization error gradient;drift use efficiently;optimization distributional;gradient noise;error gradient noise;asymptotic convergence guarantees;efficiency proposed stochastic", "pdf_keywords": "graph minimizes stochastic;stochastic gradient graph;graph optimized stochastic;stochastic optimization;convex optimization stochastic;stochastic learning;performance stochastic gradient;learning stochastic;optimization stochastic;minimizes stochastic gradient;optimized stochastic;stochastic gradient;stochastic graph;optimized stochastic learning;stochastic approximation;stochastic algorithms;algorithm stochastic graph;stochastic learning algorithm;stochastic optimization time;gap stochastic optimization;stochastic approximation algorithm;stochastic graph representation;learning stochastic tracking;proximal stochastic gradient;optimal learning continuous;online learning stochastic;stochastic tracking goal;stochastic optimization framework;guarantees stochastic algorithms;stochastic graph model"}, "22f93927c487e0e0e0d2844489423bcb5d21b45c": {"ta_keywords": "lattice gas harmonic;harmonic trap exhibits;gas harmonic trap;harmonic trap;dimensional lattice gas;lattice gas;dynamics dimensional lattice;gas harmonic;regime density states;density states;density states high;transition regime density;phase transition regime;phase transition;transition low density;phase transition low;dimensional lattice;lattice;harmonic;dynamics dimensional;trap exhibits rich;parameters dynamics;dynamics;trap;regime density;parameters dynamics dimensional;density regime;states high regime;regime density low;high density regime", "pdf_keywords": ""}, "19803adec3b97fb2e3c8097f17bf33fabf311795": {"ta_keywords": "trained language models;pre trained language;contrastive regularization confidence;language models;language models lps;trained language;regularization confidence;regularization confidence based;tuning pre trained;contrastive regularization;combines contrastive regularization;regularization;fine tuning pre;sentence pair framework;pre trained;outperforms strongest baseline;training tasks sequence;token sentence pair;improves model;improves model fitting;sequence token sentence;tuning pre;reweighting framework improves;framework fine tuning;framework improves model;fine tuning;baseline;strongest baseline;sentence pair;language", "pdf_keywords": "weakly supervised;trained language models;weakly supervised classification;weak label labeling;labeled examples training;semi supervised;focus weakly supervised;improved training supervised;semi supervised learning;contrastive learning;samples contrastive learning;generated weak supervision;models weak supervision;weak supervision based;propose contrastive regularized;language models weak;extended semi supervised;contrastive representation learning;learn weak supervision;fully supervised;labels pretrained text;usual supervised learning;contrastiveregularized self training;contrastive self training;novel supervised label;strong weak label;supervised learning small;fully supervised models;novel contrastive learning;usual supervised"}, "53dc99155c52979e311a403571f1b1d57ff73b48": {"ta_keywords": "predict displacement field;able predict displacement;displacement field material;predict displacement;approach predict displacement;material based generative;material microstructure predictions;displacement field;microstructure predictions;generative data digital;digital image correlation;image correlation measurements;microstructure predictions compared;image correlation;based generative data;field material based;generative data;displacement;element analysis based;knowledges material microstructure;material microstructure;digital image;field material;field material controlled;element analysis;data digital image;material based;based generative;finite element analysis;generative", "pdf_keywords": "soft tissue modeling;tissue modeling;tissue modeling conventional;tissue tissue modeling;tissue modeling context;tissue modeling extending;tissue modeling systematically;biological tissue modeling;tissue modeling aims;model tissue;model studying tissue;tissue mechanics;approach model tissue;tissue mechanics powerful;predict tissue tissue;able predict tissue;model tissue response;types tissue mechanics;predict tissue;modeling soft biological;tissue proposed framework;driven material modeling;materials applicability neural;tissue based neural;material modeling tasks;soft biological tissue;neural operator learning;mechanical response tissue;prediction material displacements;neural operator model"}, "6bca949b7ce69d6a43120d75e65f43d4c5a80ed4": {"ta_keywords": "learning incentive design;machine learning incentive;learning incentive;incentive design;need incentive design;incentive design techniques;incentive design tools;incentive design introduce;incentive design problems;incentive design new;class incentive design;outcomes incentive design;need incentive;incentive;class incentive;new class incentive;outcomes incentive;exotic outcomes incentive;motivated need incentive;machine learning;self interested parties;combination machine learning;machine learning new;interested parties avoid;using machine learning;interested parties;help self interested;self interested;design problems solved;learning", "pdf_keywords": ""}, "0ee468b9b709a2610c4b574d67218e7960350224": {"ta_keywords": "augmentation single text;novel data augmentation;neural machine translation;augmentation strategy text;data augmentation strategy;data augmentation;scalable data augmentation;machine translation nmt;data augmentation algorithm;nmt based augmentation;machine translation;translation datasets;translation nmt based;augmentation strategy;augmentation;augmentation algorithm;set translation datasets;text based neural;augmentation algorithm called;translation datasets method;based augmentation single;translation nmt;augmentation single;based augmentation;set translation;performs set translation;text;neural machine;translation;single text", "pdf_keywords": "augmentation machine translation;translation task leveraging;augmentation support translation;neural machine translation;augmentation text based;machine translation;machine translation nmt;experiments translation datasets;novel data augmentation;augmentation text;learning task augmenting;data augmentation strategy;data augmentation;solved machine translation;translation datasets;augmenting empirical data;augmentation framework;data augmentation target;data augmentation machine;proposed data augmentation;generalization known augmentation;translation task specifically;task augmenting empirical;data augmentation model;alternatives word dropout;replaces words source;translation task;augmentation framework random;known augmentation framework;word dropout algorithm"}, "4b22503d6da9ff3222d94106cc7425ea4fea43af": {"ta_keywords": "parsing social media;dependency parsing social;dependency parsing;library entropic parsing;entropic parsing languages;parsing social;entropic parsing;model dependency parsing;parsing languages;parsing;social media data;parsing languages shown;parallelizable efficient library;efficient library;highly efficient library;efficient library entropic;efficient model dependency;dependency;parallelizable efficient;parallelizable;parallelizable highly efficient;novel parallelizable efficient;novel parallelizable;parallelizable efficient model;present novel parallelizable;library entropic;highly parallelizable efficient;parallelizable highly;social media;highly parallelizable", "pdf_keywords": ""}, "1fa69608666e66452df56b1f71282def7ac16035": {"ta_keywords": "complexity bribery manipulation;bribery manipulation problems;computational complexity bribery;complexity bribery;bribery manipulation;bribery problem;social choice bribery;choice bribery;choice bribery problem;bribery problem asks;bribery;voting agents strategically;voting agents;asks voting agents;votes induce;voting scenario;voting scenario given;strategically misrepresent votes;misrepresent votes induce;votes induce preferred;results voting scenario;budget constraints manipulation;computational complexity problems;outcome computational complexity;computational complexity;manipulation problems arise;problem asks voting;complexity problems;asks voting;affect results voting", "pdf_keywords": ""}, "2f7c03f0d3c6f51728e925a874c49a25559cc6b3": {"ta_keywords": "neural representation query;query augmented neural;document combines query;query document combines;compositional questions long;answer compositional questions;questions long documents;query document;performs query document;compositional questions demonstrate;utilized answer compositional;answer compositional;compositional questions;representation query method;long documents multihop;questions long;combines query;document combines;representation query;neural representation utilized;combines query result;multihop based neural;compositional;documents multihop based;query;query augmented;query method;long documents;neural representation;based neural representation", "pdf_keywords": ""}, "c13c400d1f481863d57ec265d296b0a08ec77876": {"ta_keywords": "speech recognition asr;end audio segmentation;automatic speech recognition;audio segmentation non;automatic speech;speech recognition;audio segmentation;autoregressive np decoding;realize automatic speech;recognition asr;connectionist temporal classification;segmentation non autoregressive;recognition asr non;end audio;end end audio;asr non autoregressive;loss segmentation;ctc loss segmentation;temporal classification ctc;loss segmentation model;audio;classification ctc loss;decoding;autoregressive np;non autoregressive np;np decoding;temporal classification;np decoding using;autoregressive;np decoding pipeline", "pdf_keywords": ""}, "0a2ba7b1c05062d2bb7cd35e218fe08d6ea29488": {"ta_keywords": "representing email meeting;email meeting information;meeting information graph;email meeting;meeting information;addresses relevant attendees;attendees finding;relevant attendees finding;representing email;framework representing email;approach finding email;attendees finding set;meeting;attendees;attendees given given;finding email addresses;attendees given;given meeting;given given meeting;email addresses;information graph introduce;finding set email;information graph;relevant attendees;email addresses relevant;finding email;tasks finding email;relevant attendees given;email;meeting evaluate", "pdf_keywords": ""}, "fd54706252a094d592feadf53a0a3ffed4af9295": {"ta_keywords": "speech separation recognition;field speech recognition;3rd chime challenge;speech recognition problem;chime challenge;speech recognition systems;speech recognition;chime challenge seeks;problem speech separation;far field speech;speech separation;results 3rd chime;underlying speech recognition;separation recognition context;3rd chime;field speech;separation recognition;recognition systems challenge;simulations underlying speech;recognition systems;chime;signal processing;recognition problem;applied problem speech;recognition;performance far field;recognition context;recognition problem challenge;problem speech;recognition context real", "pdf_keywords": ""}, "142407d3cb61067e88d385f95ae238c74b19d554": {"ta_keywords": "identify social network;social network structure;network social network;social network social;social network;social network users;behavior social network;network social;graph ad hoc;graph distributed according;graph distributed;network structure dominated;graph structure dominated;network users sharing;nodes graph distributed;underlying graph nodes;distributed according graph;hoc structure graph;graph nodes;network structure;nodes underlying graph;graph structure;according graph ad;users sharing common;graph ad;structure graph structure;underlying graph;identify social;nodes underlying;graph nodes graph", "pdf_keywords": ""}, "75983c55a489d526427fe399ce2670376168a2f0": {"ta_keywords": "parallel corpora phrase;phrase based paraphrases;paraphrase process;paraphrase process method;parallel extraction phrase;paraphrase context domain;lemma paraphrase process;paraphrases context domain;corpora phrase based;parallel corpora;based parallel corpora;statistical machine translation;phrase based statistical;corpora phrase;based paraphrases context;based paraphrases;paraphrases;definition paraphrase;paraphrases context;paraphrase context;domain corpora method;definition paraphrase context;extraction phrase based;domain corpora;extraction phrase;machine translation;corpora method;machine translation use;use lemma paraphrase;paraphrase", "pdf_keywords": ""}, "5bc188b4ab7b27649236fad6a686b2cfe6368219": {"ta_keywords": "document topic modeling;topic modeling presented;topic modeling;similarity given document;document topic;commonsense based algorithm;commonsense measure similarity;commonsense based;given document underlying;based concept commonsense;concept commonsense measure;concept commonsense;commonsense;algorithm effective texts;algorithm document topic;commonsense measure;given document;document underlying;clustering;document underlying point;clustering technique;view using clustering;topic;using clustering;using clustering technique;modeling presented algorithm;based algorithm document;similarity;texts length composition;measure similarity", "pdf_keywords": ""}, "d8d49cc56b303d6ed0e821f8593e2f7acd1b4fb4": {"ta_keywords": "predicting sound events;predicting sound;method predicting sound;sound detection;sound events based;sound detection systems;sound detection implements;benchmark sound detection;collisionless sound detection;sound events;benchmark sound;implemented collisionless sound;collisionless sound;set benchmark sound;conformer based learning;detection systems;detection systems performance;sound;conformer based;novel method predicting;based learning architecture;detection;based learning;learning architecture;method predicting;learning architecture method;detection implements;predicting;end conformer based;learning architecture evaluated", "pdf_keywords": ""}, "a61ef7be5b5c9fbc6654f7c17fa595976652416b": {"ta_keywords": "matching organ tissue;matching organ;tissue patient matching;performed matching organ;mechanism matching organ;patient matching performed;matching performed matching;data matching performed;matching performed;performed matching;data matching;patient matching;matching;available data matching;mechanism matching;new mechanism matching;organ tissue patient;organ tissue;tissue patient using;tissue patient;organ;tissue;patient using;data;patient using available;propose new mechanism;available data;using available data;patient;mechanism", "pdf_keywords": ""}, "ff1a1e39a94b9ca31e6013d12bc2d27f7a31567c": {"ta_keywords": "end speech recognition;speech recognition;attention model;speech recognition problem;extends attention model;attention model conventional;separate attention function;attention function outputs;attentions head decoder;attention function;end end speech;end speech;incorporating multiple attentions;encoder decoder;model conventional encoder;conventional encoder decoder;encoder;multiple attentions;encoder decoder framework;conventional encoder;decoders combined;outputs decoders combined;separate attention;outputs decoders;decoders;decoder;decoder framework incorporating;head decoder;network architecture end;represented separate attention", "pdf_keywords": "attention based network;head attention model;multiple decoders attention;attention based multi;attention model multi;decoders attention integrated;multi head attention;head attention framework;speech dynamics attention;decoders attention;attention model multiple;model multiple attentions;attention encoders;attention model proposed;attention models;attention models especially;decoders attention integrates;attention model;attention based encoder;attention functions head;based attention models;end speech recognition;attention model incorporating;multiple attentions calculated;multiple attentions;attention encoders essential;andlinguistic processes attention;attention framework based;integrated single attention;performance attention model"}, "3b4b5e72a2f84d079d0d1d825309c2f6ded76539": {"ta_keywords": "estimation transfer vectors;estimation transfer vector;class estimation transfer;estimates transfer vector;transfer vectors based;transfer vector proposed;transfer vectors;gaussian class estimation;transfer vector;estimation transfer;approach estimation transfer;transfer vector use;approximate estimates transfer;estimates transfer;variational bayes method;class estimation;variational bayes;use variational bayes;novel approach estimation;gaussian class;tied gaussian class;use tied gaussian;approach estimation;transfer;vector proposed approach;bayes method derive;tied gaussian;estimation;gaussian;bayes method", "pdf_keywords": ""}, "77e5c4fa595466aa51d29327a60f9d4af4436876": {"ta_keywords": "incrementally learned knowledge;learn incrementally learned;incrementally learned;approach learn incrementally;explanation based learning;learn incrementally;learned knowledge approach;learnability named abductive;learnability;mistake bounded learnability;learned knowledge;bounded learnability named;bounded learnability;learnability named;approach learn;based learning;learning;based learning ebl;novel approach learn;abductive explanation based;knowledge;knowledge approach;knowledge approach based;learned;based empirical discovery;learning ebl algorithm;learning ebl;empirical discovery;incrementally;named abductive explanation", "pdf_keywords": ""}, "3c0d4dc4237934e37467f4ede3af859bcb140abf": {"ta_keywords": "transformer using token;boosting total person;count given transformer;token valued feature;valued feature extraction;token valued regression;tokens corresponding image;person count;feature extraction;image patches transformer;total person count;boosting;transformer;boosting total;person count given;patches transformer;transformer layers extensive;features global information;transformer using;transformer used extract;extract features;transformer layers;valued feature;transformer used;patches transformer layers;used extract features;token valued;investigate problem boosting;extract features global;pure transformer", "pdf_keywords": "features crowd counting;crowd counting datasets;supervised crowd counting;crowd counting localization;benchmark crowd counting;crowd counting extensive;crowd counting using;describing crowd counting;crowd counting based;approach crowd counting;crowd counting tasks;crowd counting challenging;crowd counting;method crowd counting;crowd counting method;crowd counting setup;crowd counting point;supervised crowd;features crowd;problem crowd counting;task crowd counting;point supervised crowd;framework supervised crowd;cnns regards capturing;cnns regards;object convolutions supervised;cnns;objects crowd approach;level features crowd;describing crowd"}, "9f49ed155d7575d181d16dd5bc92b754cae0bea9": {"ta_keywords": "rank constrained optimization;blind identification method;identification problem rank;based rank constrained;problem rank constrained;rank constrained;novel blind identification;blind identification;identification method based;known subspace;identification method;known subspace able;constrained optimization admits;optimization assume inputs;constrained optimization;optimization admits convex;identification problem;constrained optimization assume;lie known subspace;convex relaxation;admits convex relaxation;method based rank;formulate identification problem;problem rank;convex relaxation efficacy;optimization admits;optimization assume;admits convex;subspace able;subspace able formulate", "pdf_keywords": ""}, "91a2d496553cfee2b66906f704b8e3d081e2d1bf": {"ta_keywords": "inductive logic programming;flipping clauses predicting;clauses predicting fault;logic programming ilp;ilp flipping clauses;logic programming;clauses flipping;flipping clauses flipping;programming ilp flipping;flipping clauses;flipping clauses number;allows flipping clauses;clauses flipping clauses;clauses predicting;programming ilp;comparison inductive logic;inductive logic;ilp flipping;languages evaluate;predicting fault;languages evaluate performance;programming;easy evaluate clauses;predicting fault density;flipping;class languages evaluate;languages;logic;evaluation user directed;comparison inductive", "pdf_keywords": ""}, "2bbb33ab8124e5078ec39e821a25c24c20a31b9b": {"ta_keywords": "crowdsourcing workshop;parallel workshop crowdsourcing;second workshop crowdsourcing;workshop crowdsourcing workshop;workshop crowdsourcing;crowdsourcing workshop crowdsourcing;crowdsourcing;workshop crowdsourcing conjunction;crowdsourcing conjunction;crowdsourcing conjunction second;second workshop;parallel workshop;crowdsourcing conjunction 47th;second workshop future;workshop;workshop future science;hungary second workshop;workshop future;series parallel workshop;conference large data;conjunction second workshop;science technology open;technology open research;international conference;open research;international conference large;technology open;research took place;47th international conference;large data", "pdf_keywords": ""}, "0e96925c57b3325e7e37c1964b518e9276024cbf": {"ta_keywords": "spin liquid phase;dynamics spin liquid;disordered phases;liquid phase xmath0;disordered phases occurs;ordered disordered phases;spin liquid;spins transition;spins transition related;phase xmath0 model;spins discontinuous;dynamics spin;phase transition;magnetic field phase;phase transition ordered;field phase transition;phase xmath0;liquid phase;phases occurs critical;field disorder transition;disorder transition;study dynamics spin;spins discontinuous large;finite number spins;disorder transition continuous;transition ordered disordered;number spins transition;phases occurs;field phase;number spins discontinuous", "pdf_keywords": ""}, "7c2ff8fa0d24ed712e4bc2dbdb370a1cd62c965b": {"ta_keywords": "harmonic trap particle;particle harmonic trap;harmonic trap;trap particle;trap particle initially;motion particle harmonic;particle harmonic;motion particle described;dynamics motion particle;particle described time;particle motion;motion particle;called motion particle;motion particle motion;particle motion particle;particle initially excited;states particle initially;particle described;harmonic;states particle;trap;particle initially;dynamics motion;particle;dynamics;density states particle;time dependent density;described time dependent;study dynamics motion;released called motion", "pdf_keywords": ""}, "3d846cb01f6a975554035d2210b578ca61344b22": {"ta_keywords": "semi supervised learning;semi supervised;distantly supervised entity;graph embeddings;present semi supervised;graph embeddings train;embeddings instance graph;based graph embeddings;supervised entity;supervised entity extraction;entity classification;graph jointly predicting;graph instances predict;distantly supervised;including distantly supervised;extraction entity classification;supervised learning framework;instance graph;graph instances;entity extraction;supervised learning;context graph jointly;train embeddings instance;predicting class label;supervised;neighborhood context graph;instance graph instances;entity extraction entity;embeddings train;graph jointly", "pdf_keywords": "semi supervised embedding;supervised learning graph;graph embeddings;graph embeddings propose;graph based learning;graph based supervised;embeddings entities graphs;embeddings entities graph;based graph embeddings;semi supervised learning;supervised embedding;learns embeddings entities;learning graph;classification graph context;graph embeddings given;labels learn embeddings;learning embeddings entities;graph labels learn;graphs approach learns;learns embeddings;embeddings given graph;accurate graph embeddings;context semi supervised;graph predicting context;learning embeddings;labels semi supervised;graph instances train;training classification graph;supervised embedding approach;approach learns embeddings"}, "b6de9d0ca42a03967287aa7abfd59479e086a35a": {"ta_keywords": "automatic gloss finding;algorithm automatic gloss;semi supervised learning;gloss finding;semi supervised;clustering knowledge bases;novel semi supervised;called semi supervised;automatic gloss;gloss finding glofin;knowledge bases;hierarchical clustering knowledge;clustering knowledge;knowledge bases ranging;learning algorithm automatic;glofin based hierarchical;based hierarchical clustering;supervised learning;hierarchical clustering;knowledge bases demonstrate;supervised;variety knowledge bases;supervised learning algorithm;gloss;clustering;finding glofin based;learning algorithm called;algorithm automatic;glofin based;based hierarchical", "pdf_keywords": ""}, "d39478dd8d825bbd6c963d6a5ef2cee6857f6c21": {"ta_keywords": "argumentation powerful tool;argumentation powerful;argumentation;making statements easy;making statements;statements easy;difficulty making statements;statements;statements easy correctly;discuss;easy correctly talk;talk discuss;correctly talk discuss;overcoming difficulty;overcome difficulty;overcoming difficulty making;powerful tool overcoming;talk discuss recent;difficulty;tool overcoming;overcoming;talk;easy correctly;difficulty making;easy;tool overcoming difficulty;discuss recent;correctly talk;work overcome difficulty;powerful tool", "pdf_keywords": ""}, "358d7d6333d3edd530e37efd8004cb9da8cfd5d4": {"ta_keywords": "structured procedural knowledge;procedural knowledge extraction;instructional cooking videos;extraction instructional cooking;action structured;predict action structured;knowledge extraction instructional;cooking videos;extraction instructional;benchmark structured;cooking videos proposed;semantic role labeling;benchmark structured procedural;labeling visual action;videos proposed task;action structured form;instructional cooking;procedural knowledge;structured;action detection;visual action detection;role labeling visual;segmentation semantic role;structured procedural;propose benchmark structured;knowledge extraction;role labeling;action detection perform;semantic role;structured form", "pdf_keywords": "extraction narrative videos;corpus annotated videos;video natural language;knowledge extraction narrative;clip sentence classification;narrative videos benchmark;role labeling captures;semantic role labeling;extraction narrative;role labeling video;narrative videos;annotated videos;encode semantic role;corpus recipe videos;extracting structured knowledge;procedural knowledge extraction;annotated videos dataset;knowledge video segments;describing actions videos;extracting structured;knowledge instructional videos;structure instructional videos;instructional video analysis;extract contexts verbs;tuples utterances video;procedural knowledge video;extraction verbs arguments;sentences transcripts spanning;sentences transcripts;role labeling"}, "adf726bdcdddacee1c70d911b8f84b6a16841a32": {"ta_keywords": "type textual reviews;textual reviews;textual reviews proposed;product type textual;extracting prominent aspects;extract prominent aspects;reviews proposed framework;product types;aspects given product;different product types;type textual;product types proposed;textual;product type;given product type;reviews proposed;framework extracting prominent;reviews;extracting prominent;novel framework extracting;accurately extract prominent;product type user;user experience experiments;extract prominent;framework extracting;prominent aspects;extracting;accurately extract;prominent aspects given;consisting different product", "pdf_keywords": ""}, "0dff2b00fd6e8e7b5f3c0707b0e51e3628988420": {"ta_keywords": "aware text rewriting;privacy aware backtranslation;privacy aware text;obfuscating sensitive attributes;text rewriting task;text rewriting;aware backtranslation;aware backtranslation methods;effective obfuscating;effective obfuscating sensitive;obfuscating;obfuscating sensitive;methods effective obfuscating;backtranslation methods task;new privacy;backtranslation methods;backtranslation;task explore privacy;new privacy aware;aware text;propose new privacy;sensitive attributes;rewriting;privacy aware;privacy;explore privacy aware;text;rewriting task;explore privacy;attributes", "pdf_keywords": "text rewriting protecting;sensitive information text;aware text rewriting;sensitive information rewritten;obfuscating sensitive attributes;privacy aware rewriting;privacy aware text;rewriting protecting sensitive;sensitive attributes rewritten;text rewriting optimized;sensitive attributes text;privacy aware translation;sensitive attributes generated;text design privacy;protecting sensitive attributes;leakage sensitive information;information rewritten text;protecting sensitive information;attributes rewritten text;text rewriting able;rephrased privacy aware;sensitive information preserving;obfuscating gender;tasks obfuscating gender;text rewriting;providers rewriting text;gender obfuscation model;attributes generated text;sensitive information extensive;sensitive attributes"}, "694c0c5a4d0176e29bb85e1b9ca8ea84075fbbbb": {"ta_keywords": "reconfigurable neural network;hnn reconfigurable neural;reconfigurable neural;forward neural network;neural network rnn;neural networks nns;neural network;architecture choice softmax;forward neural;feed forward neural;dimensional neural networks;neural networks;neural network hnn;rnn architecture family;rnn architecture;network rnn architecture;dimensional neural;softmax;network rnn;family dimensional neural;network hnn reconfigurable;neural;rnn;hnn reconfigurable;choice softmax;networks nns hybrid;networks;nns hybrid feed;networks nns;nns hybrid", "pdf_keywords": ""}, "194a1e5f9af0ea00b22def879d90b926187fbb64": {"ta_keywords": "dynamics spin heisenberg;heisenberg model spin;spin heisenberg model;model spin dynamics;spin dynamics model;dynamics model spin;spin dynamics spin;spin orbit interaction;spin dynamics;interaction spin dynamics;model spin orbit;dynamics spin;control spin dynamics;orbit interaction spin;sensitive spin orbit;spin heisenberg;effect spin orbit;interaction spin;model spin;spin orbit;model sensitive spin;effect spin;control spin;sensitive spin;heisenberg model;spin;study effect spin;used control spin;heisenberg;orbit interaction", "pdf_keywords": ""}, "c02da00857c33fa39b115c0eb6c655ff6cf96878": {"ta_keywords": "particle harmonic trap;trap harmonic oscillator;harmonic trap method;harmonic trap harmonic;harmonic trap;trap harmonic;particle harmonic;density particle harmonic;harmonic oscillator frequency;case harmonic trap;harmonic oscillator;harmonic oscillator problem;energy density particle;solution harmonic oscillator;calculating energy density;order harmonic oscillator;trap method based;oscillator frequency method;trap method;frequency order harmonic;method calculating energy;frequency method;energy density;oscillator frequency;solution harmonic;frequency method applied;calculating energy;oscillator problem method;exact solution harmonic;harmonic", "pdf_keywords": ""}, "ca201db9980e49647feedf39eb30b19f074bf68a": {"ta_keywords": "predict shape transcription;shape transcription;shape transcription signal;transcription;transcription signal approach;transcription signal;approach transcription problem;approach transcription;denormalization model trained;novel approach transcription;transcription problem;trained acoustic signal;model trained acoustic;acoustic signal denormalization;transcription problem combines;novel convolutional neural;novel convolutional;trained acoustic;signal denormalization;convolutional neural network;convolutional neural;neural network;signal denormalization model;use novel convolutional;predict shape;neural;machine learning;network model trained;model trained;neural network model", "pdf_keywords": "spontaneous speech transcription;speech transcription;end speech recognition;speech corpus;speech transcription theory;end neural transcription;transcription standard words;corpus spontaneous speech;formatted speech recognition;transcription fully formatted;trained corpus;fully formatted speech;transcription fully;professionally transcribed;transcribed earnings calls;end automatic speech;underlying speech corpus;transcription standard;transcription;model transcription standard;models trained corpus;trained corpus 000;end orthographic speech;professionally transcribed earnings;transcribed earnings;model transcription;automatic speech;speech corpus graphs;transcribed;utterances predictions"}, "48530f3d6425f2f150f07ccdd61ba951951a0a7d": {"ta_keywords": "tasks domain adaptation;adapting pretrained neural;domain adaptation;multilingual domain adaptation;domain adaptation multilingual;domain adaptation experiments;domains adaptation;domains adaptation demonstrate;adaptation multilingual domain;adapting pretrained;approach adapting pretrained;pretrained neural machines;adaptation experiments domains;experiments domains adaptation;pretrained neural;layers pretrained model;adaptation multilingual;specific layers pretrained;pretrained model injecting;injecting pretrained model;layers pretrained;model injecting pretrained;pretrained model;adapting;machines new domains;neural machines new;injecting pretrained;adaptation;new domains approach;multilingual domain", "pdf_keywords": "domain adaptation multilingual;adaptation multilingual nmt;multilingual adaptation experiments;multilingual adaptation;training multilingual models;adaptation multilingual;adapting new languages;adapters training multilingual;ii multilingual adaptation;domain adaptation;tasks domain adaptation;massively multilingual dataset;adaptation ii multilingual;training multilingual;adapting translation model;train massively multilingual;machine translation trained;adapting translation;domain adaptation demonstrate;massively multilingual model;approach domain adaptation;domain adaptation ii;multilingual models high;adapting large models;resource massively multilingual;multilingual models;translation trained;adaptation tasks domain;multilingual translation task;machine task translation"}, "e3a85c5defe60f1f394fc4e7245fc071a249cf5b": {"ta_keywords": "learning parameters behavioral;behavior neighborhood game;behavioral model learning;behavioral model occupants;behavioral model;simulates behavior person;social game simulates;game understand occupants;occupants behavior neighborhood;understand occupants behavior;game simulates behavior;occupants behavior;neighborhood game;behavioral;neighborhood game site;game simulates;parameters behavioral model;occupants office building;social game;model learning framework;based social game;model occupants office;behavior neighborhood;use statistical learning;model learning;learning parameters;learning framework;method learning;simulates behavior;estimate parameters behavioral", "pdf_keywords": ""}, "c99e050b83360e5cbeee8fd2957aaab5b31aa638": {"ta_keywords": "memory language models;generative sequence model;language models;measure memory language;language models including;language models use;discrepancies generative sequence;memory language;term discrepancies generative;sequence model;generative sequence;discrepancies generative;sequence model true;including emphmiscalibrated miscalibrated;generative;memory;measure memory;art language models;state art language;use discrepancies improve;models including emphmiscalibrated;language;long term discrepancies;discrepancies improve model;models use prediction;emphmiscalibrated miscalibrated furthermore;discrepancies improve;emphmiscalibrated miscalibrated;term discrepancies;use prediction", "pdf_keywords": "language models calibrating;language models observe;language models;language model trained;language model training;generations language models;entropy human languages;language models including;neural language models;improve language model;language model;languages based empirical;language models true;language model provide;data language model;generative sequence model;model predictions empirical;memory model trained;entropy generated text;properties language models;box language model;memory model learning;models calibrating;black box language;predictions empirical;empirical data language;term generations language;model predictions depend;predictions empirical data;miscalibrated entropy rates"}, "7ad913d1c6eddbdad1ab4571ab91f00f055ab735": {"ta_keywords": "parallelization fast recurrence;fast recurrence attention;parallelization long;parallelization fast;parallelization;parallelization long form;sru parallelization fast;recurrence attention mechanisms;sru parallelization;parallelization network;novel architecture parallelization;architecture sru parallelization;generalized parallelization long;architecture parallelization;recurrence attention;architecture parallelization network;parallelization network architecture;generalized parallelization;architecture generalized parallelization;long form speech;fast recurrence;speech;attention mechanisms;novel architecture sru;attention mechanisms context;architecture sru;form speech;network architecture;attention;network", "pdf_keywords": "parallelizable recurrent neural;parametric speech recognition;highly parallelizable recurrent;model parametric speech;speech recognition encoder;parallelizable recurrent;attention parametric model;recurrent neural network;parametric speech;parametric speech sensor;deep recurrent neural;model longform speech;longform speech recognition;attention parametric;neural network structure;encoder deep;speech recognition;encoder deep flexible;recurrent neural;sir parametric speech;encoding nonlinear models;neural network sru;sru parallelization recurrence;scalable representation deep;recognition encoder deep;power attention parametric;speech sensor;recognition encoder;capable encoding nonlinear;convolution encoder implemented"}, "40947612162cc4644f9489721ec1ca94fe7e765c": {"ta_keywords": "thesaurus russian evaluations;semantic relatedness russian;semantic relatedness thesaurus;relatedness thesaurus;relatedness thesaurus used;evaluating semantic relatedness;thesaurus russian;relatedness russian sources;distributional thesaurus russian;evaluation semantic relatedness;sources evaluating semantic;semantic relatedness;thesaurus collection;build thesaurus collection;relatedness russian;thesaurus collection open;russian evaluations resource;thesaurus used generate;thesaurus;thesaurus used;sources evaluation semantic;build thesaurus;distributional thesaurus;evaluating semantic;open distributional thesaurus;used build thesaurus;russian evaluations;evaluation semantic;semantic;russian sources", "pdf_keywords": "russian semantic similarity;evaluate semantic similarity;semantic relatedness measures;measure semantic similarity;evaluation semantic relatedness;semantic similarity words;evaluation semantic similarity;semantic relatedness fundamental;semantic relatedness language;pairs semantic similarity;semantic similarity evaluation;semantic similarity;judgements semantic similarity;thesaurus russian language;relatedness measure semantic;russian semantic;semantic similarity meaning;semantic relatedness;pairs semantic relatedness;study semantic relatedness;accuracy semantic relatedness;relatedness language russian;similarity word pairs;semantic similarity sense;semantically similar;task russian semantic;pronounced semantic similarity;semantic similarity valid;datasets semantic relatedness;similarity large semantic"}, "69379f55de081938ae9d8b91ef549542ed78f5f0": {"ta_keywords": "xmath0 model spin;orbit interaction xmath2;orbit interaction xmath1;orbit interaction xmath0;xmath1 model interaction;xmath2 model interaction;interaction xmath1 model;interaction xmath2 model;spin orbit interaction;interaction xmath0 model;zeeman field spin;interaction xmath2;interaction xmath1;model spin orbit;xmath1 model;xmath2 model;xmath0 model;interaction xmath0;control spin orbit;spin orbit;field spin orbit;effect zeeman field;orbit interaction used;orbit interaction;model spin;xmath1;effect zeeman;xmath2;control spin;xmath0", "pdf_keywords": "learning speaker diarization;advances speaker diarization;modeling speaker diarization;speaker diarization systems;speaker diarization recognition;speaker diarization technologies;speaker diarization based;neural speaker diarization;speaker diarization research;speaker diarization performance;existing speaker diarization;speaker diarization techniques;developments speaker diarization;speaker diarization challenges;automatic speaker diarization;speaker diarization using;speaker diarization approaches;diarization speaker diarization;speaker diarization technology;speaker diarization task;novel speaker diarization;speaker diarization approach;speaker diarization achieved;speaker diarization;approaches speaker diarization;speaker diarization proposed;speaker diarization enabled;proposed speaker diarization;approach speaker diarization;use speaker diarization"}, "d74c5b5ed8eb467dc7f313b70a08880fcd74c39d": {"ta_keywords": "electronic tax local;local electronic tax;electronic tax information;tax local residents;electronic tax;tax information use;local government citizens;tax local;tax information;local residents credibility;government citizens data;local government;opinion local government;local governments;local residents ability;citizens data;local governments public;local residents;residents credibility;residents credibility good;government citizens;consistent local residents;set local governments;use local electronic;local electronic;tax;citizens data consistent;residents ability use;residents;quality local electronic", "pdf_keywords": ""}, "625764f8e3e1334ffbfe5b3139e555499e6df4d5": {"ta_keywords": "updates natural language;linguistic content requests;automatically generating linguistic;natural language websites;natural language documents;generating linguistic content;corpus natural language;content requests updates;natural language;automatically updates natural;automatically updates;content requests;language documents generated;linguistic content;generating linguistic;present automatically updates;performance corpus natural;corpus;language documents;websites pages automatically;requests updates;language websites;language websites pages;corpus natural;automatically generating;performance corpus;requests updates able;pages automatically generating;specific data automatically;user specific features", "pdf_keywords": ""}, "b5241fcbfbf30f6fd8ff1ae19d947dd2ca23244f": {"ta_keywords": "emotion rich events;extract emotion rich;collections emotion rich;large collections emotion;extract emotion;collections emotion;used extract emotion;emotion rich;rich events web;dictionarys events survey;web based dataset;emotion;small dictionarys events;automatically acquiring aggregating;events analyzed;aggregating large collections;collection events analyzed;events web based;rich events;events web;events survey;dictionarys events;dataset;events;events analyzed used;collection events;acquiring aggregating large;resulting collection events;large collections;based dataset", "pdf_keywords": ""}, "21c39ce886dc38dd2006ea25d6bd1eff4cdba0b8": {"ta_keywords": "political blogs model;online political blogs;political blogs;blogs model;blogs model based;behavior users blog;behavior online political;blog function;social media blogs;blogs;users blog function;media blogs model;blogs model able;users blog;blog;media blogs;prediction behavior online;posts social media;online political;content posts social;blog function number;posts social;predict behavior users;social media;analysis content posts;prediction behavior;users type posts;discussion model;behavior online;predict behavior", "pdf_keywords": ""}, "51321a60f5ec2c80253394ef86e8b5fcc768f52a": {"ta_keywords": "clustering matching identifier;based similarity names;similarity names objects;efficiently identify names;matching identifier names;adaptively clustering matching;similarity names;names objects database;clustering matching;high dimensional databases;method adaptively clustering;adaptively clustering;dimensional databases;identify names objects;matching identifier;identifier names;identifier names high;method based similarity;database trained;dimensional databases used;clustering;dimensional databases method;match names objects;able match names;databases;objects database trained;database;names objects large;databases used efficiently;names high dimensional", "pdf_keywords": ""}, "6954a6bb9d6f3e365b26b694c963ae1d62a03444": {"ta_keywords": "additive attention fastformer;additive attention;based additive attention;attention fastformer model;attention fastformer;frame based additive;context modeling linear;fast moving frame;effective context modeling;context modeling;sequences fastformer achieve;moving frame based;attention;sequences fastformer;successive sequences fastformer;frame based;interactions successive sequences;model fast moving;sequences demonstrate efficiency;interactions multiple sequences;moving frame;accelerate pairwise interactions;accelerate pairwise;fast moving;efficient model accelerated;linear complexity fastformer;fastformer efficient model;fastformer efficient;interactions successive;used accelerate pairwise", "pdf_keywords": "query vector attention;attention context aware;context aware attention;attention query;keys additive attention;add attention query;attention keys additive;query key vectors;attention query formin;attention keys;additive attention model;attention model context;additive attention based;vector attention keys;contextual information query;attention context;learn global query;context aware representations;global context representations;use additive attention;aware attention value;vector additive attention;context aware key;attention based model;context representations;additive attention;context representations way;context aware;attention model;global context aware"}, "03006aefccdd0c5c6736ab11ed574d02ba1cc086": {"ta_keywords": "machine translation;target language sentences;machine translation problem;language sentences reordered;time machine translation;translation problem target;semi supervised alternatives;target language;sentences reordered match;sentences reordered;problem target language;language sentences;semi semi supervised;semi supervised;translation problem;supervised alternatives;match order source;order source;translation;sentences;low resource scenarios;order source experiments;low resource;real low resource;minimizing running time;simulated low resource;improvements semi;supervised;improvements semi semi;reordered match order", "pdf_keywords": "neural machine translation;resource machine translation;machine translation large;machine translation;machine translation artificially;machine translation model;machine translation data;translation strategies generalize;machine translation nmt;predict translation;translation strategies;translation large data;create translation strategies;sourceordered target sentences;predict translation quality;translation artificially;target language sentences;effective describing translation;quality machine translation;translation present experiments;translation artificially creates;sentences data augmentation;library predict translation;translation model;translation data;language sentences ordered;describing translation;translation quality large;create translation;resource uhur english"}, "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269": {"ta_keywords": "large scale languages;writing large scale;codexpilot project;software produced codexpilot;version codexpilot project;produced codexpilot project;scale languages;codexpilot project publicly;available code github;scale languages finetuned;publicly available code;version codexpilot;codexpilot;code github;available version codexpilot;evaluating quality software;quality software produced;languages finetuned publicly;software produced;quality software;writing large;languages finetuned;model writing large;languages;available code;software;produced codexpilot;large scale;code github repository;open problems effective", "pdf_keywords": "learning code generation;code generation models;generating accurate code;benchmark large language;eval large language;code generation large;evaluating large language;code generation tool;accurate code samples;code generation;evaluateability generated code;generate code unstructured;analysis code generation;generated code translation;models code generation;code synthesis generation;generate generative programs;learning code;code generation systems;code samples eval;code samples;generative programs variety;program generation;generated code train;inequivalent programs generated;generated code superior;arbitrary code large;generative programs;code unstructured;code machine learning"}, "b5a667bf189a0cfda22bac702d97b601ae6adb6f": {"ta_keywords": "gradient free ascent;free ascent algorithm;ascent algorithm;ascent algorithm solving;reshuffling based gradient;free ascent;algorithm solving convex;convex concave min;concave min max;based gradient free;gradient free;solving convex concave;adversarially generated learn;ascent;cope data adversarially;learn robust models;concave min;convex concave;generated learn robust;based gradient;min max problems;solving convex;gradient;data adversarially;learn robust;data adversarially generated;adversarially generated;adversarially;max problems finite;convex", "pdf_keywords": "distributionally robust optimization;distributionally optimal optimization;distributionally robust learning;robust adversarial distribution;novel distributionally robust;generalized distributionally robust;distributionally robust generalized;distributionally robust formulation;adversarial distribution shifts;distributionally optimal;propose distributionally robust;distributional robust;adversarial distribution;robust distributionally robust;reformulate distributionally robust;distributionally robust;dependent distributional robust;robust optimization;theorems distributionally robust;distributionsally robust robust;bounded convex optimization;study distributionally robust;robust risk minimization;theorems distributionally optimal;distributionsally robust;robust distributionally;consider distributionsally robust;robust robust distributionally;convex optimization;reformulation distributionally robust"}, "26cc9e13a7a76e3cf5f9885d08cdafabd6fbd7ec": {"ta_keywords": "tournament data empirical;realistic tournament data;model randomization tournaments;randomization tournaments;randomization tournaments does;tournament data;generate realistic tournament;tournaments using real;realistic tournament;empirical properties tournaments;tournaments does generate;tournaments using;randomization empirical properties;randomization empirical;properties tournaments using;tournaments;properties tournaments;popular model randomization;tournament;characteristic randomization empirical;tournaments does;league atp world;data empirical;german league atp;data empirical properties;randomization;league atp;model randomization;distributions characteristic randomization;empirical properties real", "pdf_keywords": "knockout tournament seedings;tournaments using random;generate synthetic tournaments;tournament seedings given;tournaments artificial;knockout tournament seeding;tournaments artificial intelligence;tournament seedings;tournament distribution winning;tournament winning distribution;tournament distribution;tournament determined;seeding tournaments;tournaments using real;tournament seeding simple;tournament simple;expressive tournament tournament;seeding tournaments using;synthetic tournaments;tournaments based results;tournament seeding;explore seeding tournaments;simple expressive tournament;tournament determined datasets;tournament simple algebraic;structure tournaments;given tournament fit;expressive tournament;particular tournament;knockout tournament determined"}, "ac46562e61cfef6213a915bbb80d1a1a2901542a": {"ta_keywords": "paper recommendation approach;reviewer information web;technical paper recommendation;gathering reviewer information;paper recommendation;recommendation approach;recommendation approach combines;information retrieval;reviewer information;information retrieval novel;gathering reviewer;principles information retrieval;information web evaluate;procedure gathering reviewer;conference reviewing;reviewing;98 conference reviewing;reviewer;web evaluate approach;technical paper;retrieval novel;web evaluate;recommendation;retrieval novel autonomous;retrieval;conference reviewing committee;information web;methods using preference;reviewing committee;preference data", "pdf_keywords": "method recommending papers;paper recommendation analyze;information retrieval collaborative;recommending conference paper;information reviewer paper;paper recommendation based;reviewer paper sources;recommending papers based;recommendation combining information;papers given reviewer;retrieval collaborative;retrieval collaborative approach;recommendation analyze;reviewer paper information;reviewer preference method;reviewer paper contents;recommending technical papers;provide recommendation algorithm;related reviewer paper;evaluative paper recommendation;reviewer preferences conference;data involving reviewer;reviewer data;approach recommendation combines;assigning reviewers data;technical paper recommendation;recommendation analyze user;recommending conference;recommending papers;paper given reviewer"}, "559fdae33f0b7733b80a7dbcb902c79598a0d26e": {"ta_keywords": "treatment effect estimation;propensity score network;estimating causal effects;estimating causal;settings propose propensity;treatment effects methods;propose propensity score;problem estimating causal;propensity score;propensity;continuous treatment effects;effect estimation;causal effects variety;propose propensity;treatment effect;effect estimation applicable;causal effects;effects methods based;treatment effects;causal;framework treatment effect;effects methods;discrete continuous treatment;effect estimation problem;tpa adversarial;trained tpa adversarial;score network trained;promote independence covariate;architecture treatment effect;continuous treatment", "pdf_keywords": "treatment biases estimation;learning estimation treatment;estimating treatment effects;biases estimation treatment;incorporating treatment biases;bias estimation treatment;treatment effect estimation;estimation treatment effects;estimating treatment;outcomes estimation treatment;estimators embeds treatment;effectively estimate treatment;predicting treatment response;estimate treatment effects;treatment biases;predicting treatment;treatment effect estimators;estimation treatment;treatment effects generative;counterfactual outcomes estimation;method estimating treatment;treatments estimation;model estimation treatment;generative treatment model;treatment selection bias;effects generative treatment;estimating causal effects;aimed estimating causal;estimate treatment;treatment effect models"}, "bf63276c90a803fe0d069ce0a3a4a8236e756363": {"ta_keywords": "gutter drawing inferences;models gutters;models gutters gutter;predictive inference;inferences panels comic;gutter drawing;predictive inference demonstrate;gutters gutter drawing;novel architecture predictive;prediction predictive inference;gutters;predictive models;architecture predictive;gutters gutter;inference demonstrate architecture;architecture able predict;drawing inferences panels;predictive models power;models power neural;models;predictive;prediction predictive;neural architectures;predict predictive;inferences panels;predictive prediction;gutter;able predict predictive;power models gutters;drawing inferences", "pdf_keywords": "dataset comic books;deep learning comics;comics dataset;data comics dataset;comics dataset use;comic book panels;dataset comic;models comics predict;panel textbox comics;textbox comics;text models trained;available dataset comic;learning comics domain;predict text context;predict narrative character;million textboxes literature;textboxes pages comics;learning comics;panels comic book;predict text;comics predict;comics domain text;million panels comic;comic books;story models;text models;textbox comics demonstrate;comic books method;textboxes literature discover;model describing text"}, "83165cf62e62a013c2bad61c98120ccb9a0087ae": {"ta_keywords": "tutorial relevance;present tutorial relevance;present tutorial;tutorial;relevance;present", "pdf_keywords": ""}, "e8deeebc7ff6315115f01fd70a343d62db202888": {"ta_keywords": "quality synsets;high quality synsets;quality synsets able;quality synsets uses;synsets high accuracy;synsets;synsets uses genus;noisy pairs synsets;synsets uses;synsets able handle;pairs synsets;synsets high;synsets able;crowdsourcing generation high;pairs synsets high;novel crowdsourcing generation;crowdsourcing generation;novel crowdsourcing;crowdsourcing;present novel crowdsourcing;species match repository;match repository facilitate;repository facilitate generation;compilations;genus species match;species match;compilations able achieve;match repository;noisy pairs;compilations able", "pdf_keywords": ""}, "b778a7c4001898a1c3888577154d747522f16db4": {"ta_keywords": "adversarial loss divergence;adversarial networks regularization;different adversarial losses;adversarial losses;adversarial loss;component functions adversarial;functions adversarial loss;discriminative adversarial networks;functions generative adversarial;adversarial losses combining;generative adversarial networks;discriminative adversarial;generative adversarial;adversarial networks;adversarial networks allows;functions adversarial;different adversarial;adversarial;based discriminative adversarial;compare different adversarial;component functions generative;networks regularization terms;networks regularization;component functions regularization;generative;loss divergence like;regularization terms networks;loss divergence;functions generative;regularization", "pdf_keywords": "different adversarial losses;adversarial loss divergence;adversarial losses;adversarial losses function;adversarial loss;comparing adversarial losses;adversarial losses based;adversarial losses featuring;adversarial losses proposed;adversarial losses neural;adversarial losses propose;adversarial loss provide;functions adversarial loss;valid adversarial losses;understanding adversarial losses;evaluating adversarial losses;adversarial losses generalized;adversarial loss favorable;data adversarial loss;168 adversarial losses;adversarial losses called;called adversarial loss;adversarial losses decoupling;consider discriminative adversarial;conditions adversarial loss;framework adversarial losses;component functions adversarial;discriminative adversarial;adversarial loss prove;adversarial loss test"}, "0453bab552e83f19dd6ba12061949f128fa9b045": {"ta_keywords": "nonparametric probabilistic learning;probabilistic learning method;probabilistic learning;based expectation maximization;novel nonparametric probabilistic;nonparametric probabilistic;expectation maximization em;expectation maximization;learning method;datasets unknown classes;maximization em formalism;learning method based;maximization em;probabilistic;novel nonparametric;datasets;method based expectation;nonparametric;classes;learning;unknown classes;classes unknown classes;applicable data contains;based expectation;method datasets;present novel nonparametric;data contains;classes unknown;data contains unknown;em formalism method", "pdf_keywords": "multiclass semi supervised;semi supervised multiclass;known semi supervised;supervised multiclass learning;supervised multiclass;semi supervised learning;semi supervised;multiclass learning;robustness semi supervised;semi supervised method;learning methods robust;multiclass learning methods;learning called semisupem;supervised learning;supervised;biologically safe learning;supervised generative;supervised generative learner;safe learning search;safe learning;learn models;model propose supervised;exploratory learning called;propose supervised generative;supervised learning setting;supervised method biologically;able learn models;exploratory learning;algorithm exploratory learning;propose supervised"}, "b36dc8db9930a785edd55ca30328ace2896523e6": {"ta_keywords": "semantic annotation tools;semantic annotation;annotated semantic;tools semantic annotation;annotated semantic corpus;build annotated semantic;purpose semantic annotation;annotation tools developed;annotation tools;corpora polish semantic;semantic language;annotation;tools semantic;semantic corpus;annotation tools tested;polish semantic language;polish semantic;semantic corpus purpose;semantic teams tools;semantic;corpus purpose semantic;different semantic teams;semantic teams;annotated;build annotated;knowledge different semantic;set tools semantic;different semantic;semantic language epsz;purpose semantic", "pdf_keywords": ""}, "87eece8d39d1e25ba87550be8b01af32738cbf2c": {"ta_keywords": "talker speech recognition;recognize overlapped speech;multi talker speech;speech recognition architecture;mixed speech datasets;monaural multi talker;overlapped speech proposed;multi talker;speech recognition;overlapped speech;improve multi talker;speech datasets;speech recognition performance;end multi talker;speech proposed architecture;artificially mixed speech;speech datasets generated;talker speech;talker;wsj0 reading corpus;mixed speech;recognition architecture;reading corpus;recognize overlapped;recognition architecture training;speech;reading corpus experiments;corpus;end monaural multi;monaural multi", "pdf_keywords": ""}, "e2ece7ea0924b4f95f65587973118bea9a44a3d2": {"ta_keywords": "relationships entities software;identify relationships entities;identification relationships entities;discover relationships entities;classes software domain;domain use similarity;entities software domain;software domain refer;entities software;similarity measure identify;method identification relationships;discover relationships;use similarity measure;use similarity;refer classes software;software domain;relationships entities;measure identify relationships;software domain validate;similarity measure;identify relationships;java classes proposed;measure discover relationships;similarity;classes software;java classes;domain refer classes;software domain use;1000 java classes;java", "pdf_keywords": "detection coordinateterm relationships;text entities underlying;software corpus;relationships underlying java;components light matter;used software corpus;heavy fermion materials;approach detection coordinateterm;detection coordinateterm;underlying java;based heavy fermion;underlying libraries relationships;relationships underlying libraries;discover semantic relationship;matter interaction components;fermion materials;accuracy detection coordinateterm;text entities representing;libraries relationships underlying;interaction components light;underlying java code;refer java;text entities based;entities underlying;fermion materials approach;coordinateterm relationships entities;underlying libraries;text entities;discover semantic;used discover semantic"}, "bbc7e533e5bfb388af1afd85bfb7ba17330cae76": {"ta_keywords": "shore power supply;high voltage shore;voltage shore power;cascade bridge inverter;control output voltage;bridge inverter;bridge inverter current;converter chain proposed;voltage shore;voltage converter chain;output voltage converter;high cascade bridge;strategy high voltage;voltage converter;supply based dc;power supply based;high voltage;converter chain;chain proposed control;shore power;inverter current;dc bias suppression;cascade bridge;inverter current realized;high high cascade;proposed control strategy;voltage component transformer;high cascade;dc voltage;output voltage", "pdf_keywords": ""}, "92cee1e209f2d9a311416b0d9fd8a49b0fbe7df2": {"ta_keywords": "learned filters collaborative;symbolic learning methods;filters collaborative;transfer learned filters;symbolic learning;learned filters;filters collaborative setting;learning methods;text categorization;conclude symbolic learning;different learning methods;transfer learned;learning methods direct;generalization performance collaborative;categorization;feature selection;stability different learning;learning methods setting;improving generalization performance;learning methods use;direct transfer learned;stability learning methods;used text categorization;stability learning;investigate stability learning;feature selection method;improving generalization;learning;generalization performance;different learning", "pdf_keywords": ""}, "956aa64b0d5f5802b98bf551d5bab8993b114fd0": {"ta_keywords": "database new similarity;similarity data large;similarity data;known similarity data;similarity function similarity;called similarity data;large database metric;function similarity data;similarity function based;new similarity function;similarity function;based known similarity;known similarity;similarity;function similarity;database metric easy;metric called similarity;database metric;present new similarity;new similarity;large database;data large database;called similarity;database metric called;database;large database new;data large;database new;easy parallelize;use easy parallelize", "pdf_keywords": ""}, "f52f7964febd6d6d72aa23505b50d33e1d4ce0aa": {"ta_keywords": "weakly supervised learning;weakly supervised;model weakly supervised;rule discovery boosting;supervised rule discovery;novel supervised rule;rules data boosting;supervised weight learning;rules pretrained supervised;supervised rule;boosting model weakly;boosting candidate rules;rule discovery;supervised weight;fully supervised models;propose novel supervised;supervised learning;supervised learning model;novel supervised;supervised;discovery boosting;fully supervised;weight learning;supervised models;data boosting candidate;discovery boosting model;data boosting;pretrained supervised;candidate rules data;pretrained supervised weight", "pdf_keywords": "rule discovery boosting;learns effective rules;supervised labeling rules;based rule discovery;rule discovery framework;rules annotate weak;weakly supervised labeling;rule discovery train;rule discovery;rule level annotation;weak formulationly supervised;rule proposal discovering;model weakly supervised;learning rule level;weakly supervised;weakly supervised learning;labeling rules automatically;parameters weakly supervised;discovering rules data;formulationly supervised learning;formulationly supervised;discovering rules;labeling rule framework;discover rule candidates;framework weakly supervised;weakly supervised interactive;rules annotate;supervised learning ldlds;model weak supervision;rules weak labels"}, "60f3e69e4f18e8e8e7dcc4ba66c1e216b49ad982": {"ta_keywords": "common sense knowledge;sense knowledge acquisition;game common sense;game theoretic knowledge;knowledge acquisition gecka;knowledge acquisition presented;knowledge acquisition;sense knowledge;acquisition usable knowledge;knowledge acquisition allows;usable knowledge;theoretic knowledge acquisition;usable knowledge large;knowledge;concept game theoretic;concept game;novel concept game;common sense;engine common sense;novel game common;application game engine;game engine common;game engine;players application demonstrated;knowledge large collection;novel application game;game common;application game;theoretic knowledge;knowledge large", "pdf_keywords": ""}, "09b87b6e7bfbf66d355574d292586595e0185d6e": {"ta_keywords": "design predicts;present design predicts;design;present design;predicts;present", "pdf_keywords": "features predicted lingua;language predicting;task phylogenetically predicting;language predicting evolution;prediction task phylogenetically;phylogenetic feature prediction;languages feature predictionable;structure language predicting;feature classifier phylogenetic;predicting multilingual;global language network;predicting phylogenetic;classification tasks languages;characterizing predicting multilingual;language network;performance phylogenetically predicting;task phylogenetically;classifier phylogenetic;predicting phylogenetic surface;predicted lingua;language typological features;phylogenetically predicting;lingference hybrid feature;classifier phylogenetic trees;phylogenetically predicting phylogenetic;predicting multilingual syntax;phylogenetic feature;languages feature;predicting evolution linguistic;phylogenetic surface features"}, "5b16d138bf16762d43b55b6e21d9b0b61021180e": {"ta_keywords": "lattice electron gas;coupled electron gas;square lattice electron;electron gas strongly;strongly coupled electron;dimensional electron gas;gas strongly interacting;lattice electron;dynamics dimensional electron;electron gas dimensional;analysis electron gas;electron gas;coupled electron;strongly interacting bosonic;gas electron gas;interacting bosonic strongly;electron gas electron;electron gas results;gas electron;dimensional square lattice;bosonic strongly coupled;square lattice;gas dimensional square;interacting strongly coupled;interacting bosonic;electron gas good;lattice;strongly interacting;dimensional electron;strongly interacting strongly", "pdf_keywords": ""}, "b15ea460c77a4ee8aa159a30ab0331deedfcf392": {"ta_keywords": "large language models;language models greatly;training inference routing;base large language;language models;auxiliary expert balancing;assignment experts base;expert balancing;learn balanced routing;large language;expert modules;layers sparse layers;experts base;capacity sparse layers;specialized expert modules;sparse layers;layers sparse;expert balancing loss;training inference;sparse layers sparse;use available experts;heuristics auxiliary expert;expert modules contain;balanced assignment experts;assignment experts;sparse;available experts existing;inference routing;parameters difficult learn;high capacity sparse", "pdf_keywords": "training sparse expert;sparse expert layers;sparse expert;training large sparse;parallel training sparse;sparse expert approaches;training sparse;layer large language;large language models;training large language;previous sparse expert;expert allocation linear;expert allocation;language models greatly;models language learns;token expert allocation;expert layers;sparse models efficiently;large sparse models;language learns compute;data parallel training;optimal assignment expertwe;compute layer training;assignment experts base;expert layers based;large expert training;sparse layers formulate;capacity sparse layers;optimal assignment expert;learns compute"}, "bbb7eb10c45cabaee6e427242fce7180c0217ef1": {"ta_keywords": "learning programs representation;programs representation;learning programs;programs representation purely;representation learning programs;automatically representation general;predicting variable names;path based representation;programming languages;automatically representation;programming languages evaluate;prediction tasks generative;different programming languages;representation purely syntactic;extracted automatically representation;programming;different programming;syntactic extracted automatically;syntactic extracted;variable names;general path based;purely syntactic extracted;prediction tasks;types use representation;programs;path based;tasks generative discriminative;generative discriminative models;representation drive generative;tasks predicting variable", "pdf_keywords": ""}, "cbf941fef87830efa4de98455cfe943917909b66": {"ta_keywords": "superlinear convergence known;methods convex broyden;superlinear convergence classical;superlinear convergence;local superlinear convergence;rates superlinear convergence;quasi newton methods;hessian approximation;hessian approximation obtain;newton methods convex;inverse hessian approximation;determinant hessian approximation;hessian approximation trace;analysis local superlinear;classical quasi newton;convex broyden;convex broyden class;methods convex;hessian;convergence known baxter;newton methods;convergence classical quasi;local superlinear;quasi newton;trace inverse hessian;inverse hessian;logarithm determinant hessian;broyden class analysis;determinant hessian;obtain rates superlinear", "pdf_keywords": "optimization superlinear convergence;methods convex broyden;superlinear convergence algorithm;algorithm superlinear convergence;converges superlinear optimal;hessians converges superlinear;rate superlinear convergence;superlinear convergence rate;superlinear convergence greedy;study superlinear convergence;superlinear convergence determined;superlinear convergence known;superlinear convergence quasi;quasi newton methods;known methods convex;superlinear convergence standard;buuuuuu superlinear convergence;superlinear convergence;nonlinear optimization superlinear;superlinear optimal;superlinear convergence classical;rates superlinear convergence;function superlinear convergence;optimization superlinear;superlinear convergence given;presents superlinear convergence;linearly convergent methods;based convex broyden;superlinear convergence class;characterize superlinear convergence"}, "d8551a4b49aa547ad8884ba9f545480860fcadd1": {"ta_keywords": "modeling predict mechanical;deep neural operator;predict mechanical;predict mechanical responses;implicit neural operator;neural operator architecture;neural operator ifno;neural operator;deep neural;neural network;feature space neural;mechanical responses materials;implicit neural;space neural network;novel deep neural;driven modeling predict;neural;neural network proposed;space neural;modeling predict;operator ifno;driven modeling;data driven modeling;constitutive models;operator architecture;data driven;constitutive models achieve;coin implicit neural;mechanical responses;conventional constitutive models", "pdf_keywords": "learning digital image;neural operator dfn;learning material response;measurements learning;deep neural operators;deep neural operator;neural operator learning;material response prediction;operator learning architecture;predictions digital image;measurements learning solution;learn material models;driven model learns;operator learning;material modeling tasks;predict material responses;predict material;solution operator learning;material responses prediction;operator architecture learning;deep neural;neural operators;learning complex material;using neural operators;material identification modeling;monitoring heterogeneous material;shallow neural network;learning solution operator;deeper neural networks;heterogeneous material modeling"}, "cee25a535ec7165eae38f498a391050077ad9f65": {"ta_keywords": "model speaker clustering;speaker clustering;speaker clustering proposed;oriented speaker clustering;speaker clustering shown;nonparametric model speaker;utterance oriented dirichlet;utterance oriented speaker;dirichlet process mixture;utterances proposed model;agglomerative clustering case;model based utterance;clustering proposed model;large amounts utterances;clustering case;agglomerative clustering;hierarchical agglomerative clustering;mixture model;clustering;process mixture model;model speaker;clustering proposed;mixture model evaluated;clustering case large;based utterance oriented;oriented speaker;problem utterance oriented;scale speaker;speaker;utterance oriented", "pdf_keywords": ""}, "7668b23aadf43bebe5e2d3abf37938b44bd16200": {"ta_keywords": "web search benchmark;new web search;search benchmark new;search benchmark;web search;webqa benchmark new;webqa benchmark;understanding web rich;web rich connected;web rich;scalable multimodal opendomain;version webqa benchmark;web search problem;new benchmark;search;new web;new benchmark significant;understanding web;benchmark new benchmark;benchmark new;cost scalable multimodal;search problem addresses;scalable multimodal;benchmark;benchmark significant improvement;multimodal opendomain;benchmark significant;introduce new web;webqa;web", "pdf_keywords": "create partial wave;event partial wave;streamers 3d wave;concept partial wave;wave turbulence;partial wave wave;wave single event;wave turbulence method;3d wave turbulence;partial wave;called partial wave;partial wave single;waveforms based;3d wave;waveforms;wave wave propose;annotated waveforms;wave wave;annotated waveforms answer;waveform waveforms based;waveform;wave;image annotated waveforms;filtered background composed;waveform waveforms;turbulence method based;waveforms answer;waveforms apply;wave propose;wave single"}, "c688e187cede868e35fc1b53913e0fbbe6e38ea0": {"ta_keywords": "structured prediction;algorithms structured prediction;structured prediction tasks;learning algorithms structured;structured prediction demonstrate;search supervised;proposed search supervised;proposed dataset aggregation;variety structured prediction;search supervised learning;dataset aggregation algorithm;dataset aggregation;learning algorithms searn;prediction tasks;learning algorithms;recently proposed dataset;algorithms structured;supervised learning algorithms;proposed dataset;aggregation algorithm dagger;aggregation algorithm;supervised learning;dataset;supervised;aggregation;algorithms searn;structured;prediction;algorithms searn searn;prediction demonstrate recently", "pdf_keywords": ""}, "2448e63a7bb626d09001fe37e60befdb2919f6e6": {"ta_keywords": "commonsense knowledge extraction;approach commonsense knowledge;commonsense knowledge;knowledge extraction based;knowledge extraction;robust accurate commonsense;commonsense property lookups;novel approach commonsense;extraction based graph;accurate commonsense property;approach commonsense;accurate commonsense;commonsense;commonsense property;representation underlying knowledge;underlying knowledge method;graph based;knowledge method implemented;based graph based;knowledge method;underlying knowledge;extraction based;machine learning provides;robust extractions;graph based representation;based graph;accurate robust extractions;knowledge;machine learning;property lookups", "pdf_keywords": ""}, "cee96ee69adacfdeb648c230d2c9b01011724724": {"ta_keywords": "particles interacting repulsive;repulsive force particles;interacting repulsive force;level particles interacting;interacting repulsive;repulsive force;particles interacting;dynamics level particles;repulsive potential random;repulsive potential;interaction strength particles;force particles initially;placed repulsive potential;force particles;particles particles initially;particles initially;level particles;initially placed repulsive;dynamics;particles initially placed;strength particles particles;random potential;particles particles;dynamics level;particles;determined interaction;random fashion particles;placed repulsive;placed random potential;potential random", "pdf_keywords": ""}, "007371feab4af758b74580c43e74827b3500c67e": {"ta_keywords": "scalable video demand;streaming highly scalable;video demand streaming;demand streaming highly;demand streaming;scalable video;highly scalable video;fractional storage architecture;video demand;fractional storage;simple fractional storage;streaming;streaming highly;scalable highly scalable;storage architecture based;highly scalable;storage architecture;implementation highly scalable;highly scalable resilient;problem highly scalable;scalable highly;scalable;scalable resilient;highly distributed implementation;user demand approach;convex content placement;scalable resilient changes;storage;present scalable highly;distributed", "pdf_keywords": ""}, "2a0cb1a1e78b77fe9981e4935410cf3ea900e370": {"ta_keywords": "spin polarized electron;spin polarized electrons;polarized electron scattering;scattering spin polarized;electron scattering polarized;scattering spin;spin dependent scattering;spin orbit interaction;polarized electron polarized;polarized electron;dependent scattering spin;polarized electrons presence;electron polarized background;electron polarized;polarized electrons;orbit interaction spin;spin polarized;attractive spin polarized;electron scattering;polarized background spin;effect spin orbit;field spin polarized;electrons presence magnetic;interaction attractive spin;interaction spin;spin orbit;interaction spin dependent;effect spin;magnetic field spin;scattering polarized", "pdf_keywords": ""}, "d60b4594fb0404329d9ebf6fd88702ca3479e904": {"ta_keywords": "extracting abbreviations biomedical;abbreviations extraction algorithm;based abbreviations extraction;abbreviations extraction;abbreviations biomedical text;algorithm extracting abbreviations;extracting abbreviations;alignment biomedical text;biomedical text using;biomedical text;abbreviations biomedical;tm based abbreviations;biomedical text method;based alignment biomedical;abbreviations;alignment biomedical;based abbreviations;extraction algorithm report;extraction algorithm;text using translation;translation visual recognition;recognition machine tm;algorithm extracting;biomedical;extracting;based alignment;recognition machine;extraction;present algorithm extracting;text using", "pdf_keywords": ""}, "99546b4d1f2547095bb15eec36e03f64b74a78d4": {"ta_keywords": "popular market dynamics;popular stock market;popular market;popular popular market;market popular;market dynamics popular;length popular market;market epicentres market;market popular xmath1;popular xmath1 market;market market;stock market market;popular stock;market dynamics;popular xmath0 market;epicentres popular stock;stock market;xmath0 market popular;popular community;market;market epicentres;xmath0 market;popular community strong;xmath1 market;end popular popular;activity popular community;xmath1 market epicentres;market market close;epicentres market;market affected", "pdf_keywords": ""}, "5e3d1bece9dd2356fd2b31312bd62c8f7126882d": {"ta_keywords": "entangled states based;entangled states;entangled states method;entangled states used;number entangled states;large number entangled;gaussian state transformation;transformation gaussian state;entangled;number entangled;gaussian state;gaussian transformation gaussian;transformation gaussian;gaussian transformation;states used generate;gaussian;application gaussian transformation;based application gaussian;application gaussian;state transformation;state transformation applied;generate;generate large number;method generate large;generate large;new method generate;method generate;states method;states method applied;used generate large", "pdf_keywords": ""}, "b990331a5394f3642a1fd1791d70bfa2d85d9d1d": {"ta_keywords": "dissemination ofvaccination patterns;underlying dissemination ofvaccination;dissemination vaccine related;vaccine related messages;affect dissemination vaccine;dissemination ofvaccination;dissemination vaccine;ofvaccination patterns predict;ofvaccination patterns;patterns ofvaccination;patterns ofvaccination patterns;uses patterns ofvaccination;social media patterns;guide dissemination vaccine;ofvaccination patterns combination;vaccine;mechanism underlying dissemination;ofvaccination;vaccine related;patterns affect dissemination;messages social media;underlying dissemination;predict social media;social media propose;mechanisms guide dissemination;traditional media social;media propose mechanism;social media;media social;social media currently", "pdf_keywords": ""}, "8b98f7ff3bb1b199db85fc219a5c27b355adf1be": {"ta_keywords": "erbium laser treatment;procedures erbium laser;crown lengthening erbium;lengthening erbium laser;erbium laser;osseous crown restorations;osseous crown lengthening;erbium laser fast;study osseous crown;treatment osseous crown;laser treatment osseous;crown lengthening procedures;invasive osseous crown;crown restorations study;free erbium imaging;erbium imaging;laser treatment;crown lengthening;osseous crown;crown restorations;erbium imaging modality;lengthening erbium;radiation free erbium;lengthening procedures erbium;laser;light minimally invasive;minimally invasive osseous;free erbium;procedures erbium;minimally invasive radiation", "pdf_keywords": ""}, "a604ad4654f31d325b888806e276123a704cb5c8": {"ta_keywords": "support vector machines;classifier based geometric;vector machines svms;minimum error classifier;machines svms derive;margin maximization support;support vector;svms derive;classifiers;machines svms;svms derive geometric;vector machines;error classifier based;classifier;error classifier;maximization support vector;geometric margin maximization;svms;margin general class;based geometric margin;based classifiers;prototype based classifiers;classifier based;margin maximization;geometric margin general;geometric margin;mce training method;derive geometric margin;training method;increases geometric margin", "pdf_keywords": ""}, "652e3c774da47c0c8788111ec886a00d3b8fc637": {"ta_keywords": "compute deformations tumour;compute brain deformations;brain deformations tumour;deformations tumour resection;deformations tumour resected;simulation compute deformations;deformations tumour;compute deformations;brain deformations;biomechanical modelling;biomechanical modelling computer;present biomechanical modelling;biomechanical modelling approach;uses biomechanical modelling;tumour resection approach;deformations;skull use mesh;tumour resection;approach compute brain;geometry extracted patientspecific;present biomechanical;biomechanical;compute brain;cerebrospinal fluid skull;dynamics solver;approach uses biomechanical;explicit dynamics solver;tumour resected;explicit dynamics;dynamics solver problem", "pdf_keywords": ""}, "d0ea87ce3bcd86428d379fd478c365c64f870200": {"ta_keywords": "schema robustness dialogue;robustness dialogue systems;measuring schema robustness;enhance schema robustness;robustness dialogue;services dialogue;unseen services dialogue;schema robustness;dialogue systems;dialogue systems present;schemas unseen services;dialogue systems based;collaborative service systems;measuring sensitivity schemas;measuring schema;beta survey collaborative;enhance schema;method measuring schema;survey collaborative service;collaborative service;dialogue;schemas unseen;schemas;sensitivity schemas unseen;schema;method enhance schema;beta beta survey;beta survey;sensitivity schemas;survey collaborative", "pdf_keywords": "dialogue state tracking;dialogue systems;dialogue systems present;schema guided dialogue;dialogue state approach;dialogue model;standard dialogue state;models standard dialogue;guided dialogue model;guided dialogue state;dialogue systems based;robustness dialogue state;guided dialogue systems;dialogue state;method dialogue systems;improving robustness dialogue;evaluation robustness dialogue;navigating dialogue state;dialogue model evaluate;model dialogue;guided dialogue;robustness dialogue;generative model dialogue;model dialogue public;tracking models schema;standard dialogue;optimally navigating dialogue;dialogue;state tracking models;state tracking dataset"}, "429b65937d4922578a81e1f0ef5aeab7361ae36b": {"ta_keywords": "called semantic parsing;semantic parsing;mapping sentences text;mapping sentences corresponding;parsing based mapping;tools mapping sentences;mapping sentences;map sentences corresponding;semantic parsing based;tools map sentences;sentences corresponding text;text called semantic;parsing;based mapping sentences;sentences corresponding;map sentences;parsing based;corresponding text;semantic;called semantic;search corresponding text;sentences text;corresponding text use;corresponding text called;sentences text called;corresponding text present;class tools mapping;text use tools;sentences;search corresponding", "pdf_keywords": "natural language bash;commands large corpus;language bash commands;sentences bash commands;natural language nl2bash;bash commands expert;command translation;language bash translation;corpus tl scripts;language bash;bash commands;bash commands nl2bash;command line translation;annotate bash;scripting;bash translation;natural language snippets;natural language code;bash commands introduce;specific scripting simply;annotate bash command;commands domain programming;scripting simply;specific scripting;language nl2bash;parsers natural language;using language bash;bash commands paper;commonly used bash;translation natural language"}, "ba45a346690f3c5b6f8c371b5c6cf1d7cce5619d": {"ta_keywords": "blind identification problem;identification physical measurement;identification problem method;applied blind identification;method based identification;based identification physical;method identifying physical;measurements method based;blind identification;method applied blind;physical measurements method;identification problem;identifying physical measurements;measurements method;measurements method applied;identification physical;measurement output method;problem identifying physical;method identifying;new method identifying;physical measurement output;identifying physical;physical measurement;based identification;measurement output;physical measurements;problem identifying;blind;measurement;applied problem identifying", "pdf_keywords": "identification noisy vector;constrained rank minimization;recovered known subspace;regressive exogenous input;estimating impulse response;input realizations solution;blind identification linear;identification linear;estimating impulse;formulated rank minimization;theory identification problem;rank minimization;input arxiv model;rank minimization problem;problem estimating impulse;identification linear matrix;input blind identification;robust noise input;identifying auto regressive;assume unknown input;noise input realizations;identification problem formulated;solution robust noise;measurement noises constraints;method blind identification;resulting solution robust;exogenous input arxiv;identifying arxiv model;subspace assume unknown;known subspace assume"}, "65c2a39f1579a947926ac5746888445ea4afdf6e": {"ta_keywords": "free grammars pcfgs;context free grammars;free grammars;grammars pcfgs;grammars pcfgs overcome;lexicalized context free;neural models lexicalized;models lexicalized;models lexicalized context;grammars;novel neural models;context free;neural models;lexicalized context;modeling formalism;lexicalized;novel neural;pcfgs overcome sparsity;representations achieved modeling;present novel neural;neural;pcfgs overcome;pcfgs;constituents dependencies single;achieved modeling formalism;dependencies single model;representations achieved;constituents dependencies;induce constituents dependencies;stronger results representations", "pdf_keywords": "models constituency parsing;generative grammars;induction generative grammar;learning grammar model;generative grammar achieved;generative grammar;unsupervised grammar induction;context free grammars;grammar induction based;unsupervised learning grammar;learning grammar;modeling lexical dependencies;free grammars induction;generate conditional grammars;grammars induction computational;generative grammar improvedneural;grammars encode dependencies;sentence induction generative;context free grammar;lexicalized constituency parsing;unsupervised dependency parsing;generation conditional grammars;dependency parsing model;grammar improvedneural networks;generative grammars maximize;parsing constituency dependency;class generative grammars;grammar induction focus;formalism constituency parsing;grammar cfg based"}, "562f33611cdc0d8ed6609aa09f153e6238d5409e": {"ta_keywords": "missing data clinical;clinical time series;data clinical time;appropriate missing data;model missing data;missing data data;missing data;data clinical;diagnosis improve predictive;clinical time;predict outcome tests;predictive power clinical;time series method;time series;accuracy diagnosis;power clinical time;improve accuracy diagnosis;predict outcome;accuracy diagnosis improve;tests performed data;model predict outcome;data proposed method;performed data proposed;data proposed;diagnosis;model missing;performed data;appropriate missing;model predict;clinical", "pdf_keywords": "diagnosis neural network;classifying diagnoses;predicting clinical;approach predicting clinical;classifying diagnoses clinical;classification diagnoses using;diagnoses using neural;networks rnns proposed;classification diagnoses;clinical time series;predicting clinical decisions;predicting severity patient;problem classifying diagnoses;diagnoses clinical time;missing data based;networks rnns;missingness data;rnns;indicators missingness data;missing data indicators;rnns proposed method;novel imputation strategy;imputation missing data;recurrent neural network;approach missing data;observations missingness patterns;rnns proposed;predictive models missingness;neural network rnn;models missingness patterns"}, "a36f7d5d8f724168e534925edff97b3680e545c9": {"ta_keywords": "tensor contractions trainable;activation tensors;neural network layers;use tensor contractions;trainable neural network;tensor contractions;activation tensors called;trainable neural;contractions trainable neural;model compression;tensors called contraction;contraction layer;image recognition augmenting;tensors;networks alexnet;significant model compression;tensor;terms activation tensors;recognition augmenting;use tensor;contraction layer tcl;neural network;networks alexnet vgg;based use tensor;network layers;model compression significant;image recognition;neural;popular networks alexnet;tensors called", "pdf_keywords": "tensor compression training;tensor contractions trained;large tensor networks;tensor networks large;training large tensor;tensor networks;tensor contraction layer;activation tensor network;datasets tensor contractions;contractions neural networks;tensor compression improved;tensor network;deep convolutional tensor;supervised classification tensor;tensor compression;performance tensor compression;contraction activation tensor;tensor contractions neural;incorporating tensor contractions;tensor contraction activation;classification tensor;deep learning alex;convolution activation tensor;coding tensor compression;activation tensor processing;network activation tensor;tensor function network;incorporate tensor contractions;contractions trained model;classification tensor size"}, "c2ff76c75acc777e005360e9d4c4d928d95c0432": {"ta_keywords": "codes distributed storage;distributed storage systems;regenerating codes distributed;reliable storage systems;distributed storage;design reliable storage;reliable storage;storage systems fundamental;storage design reliable;optimum regenerating codes;storage systems;codes distributed;storage systems central;storage design;regenerating codes discussed;storage systems selection;regenerating codes;design regenerating codes;information storage design;storage;information storage;distributed networks paper;distributed networks;distributed;field distributed networks;optimum regenerating;codes discussed;networks paper describes;describes design regenerating;codes", "pdf_keywords": ""}, "6c3b8e65dc45cb62172f9425dcff4c48055d47eb": {"ta_keywords": "visualization language food;analyze language food;language food geographic;food social media;language food social;language food implement;language food;food geographic;food geographic locale;language food outperforms;demonstrate language food;connections language food;query visualization language;food social;visualization language;online query visualization;textual features;analyze textual features;food implement analysis;food implement;query visualization;analyze language;textual features greatest;food outperforms majority;food outperforms;geographic locale community;visualization;time query visualization;locale community characteristics;language", "pdf_keywords": "twitter based predicts;food related tweets;using tweets predict;tweets predict;food social media;tweets predict latent;predicting language food;twitter feeds;textual information tweets;generated tweets;personal twitter feeds;user generated tweets;tweets generate;twitter feeds collects;language food predict;food drinks tweets;tweets individuals;feeds collects tweets;tweets topics;information tweets;collects tweets;query tweets;related tweets using;extracted tweets individuals;tweets using tweets;related tweets;social media posts;related posts twitter;tweets framework;generated tweets generate"}, "5e00596fa946670d894b1bdaeff5a98e3867ef13": {"ta_keywords": "vision language modeling;modeling visual textual;language modeling named;vision language;pretraining framework vision;language modeling;visual textual representations;prefix language modeling;framework vision language;textual representations resulting;modeling named vlvl_;language modeling objective;visual textual;textual representations;modeling visual;novel pretraining framework;representations resulting framework;pretraining framework;prefix language;single prefix language;named vlvl_;modeling named;visual;named vlvl_ type;weak supervision trained;textual;joint modeling visual;vlvl_ type reduces;vlvl_ type based;vlvl_ type", "pdf_keywords": "vision language models;training deep bidirectional;bidirectional attention image;learned models captioning;bidirectional attention;language vision tasks;supervised captioning task;visual language model;achieves bidirectional attention;models captioning task;vision language benchmark;supervised captioning;pretraining linguistic representation;learns joint representation;perform supervised captioning;model pretrained generative;training deep;natural language trained;pretrained generative;pretrained generative encoder;pretraining linguistic;models captioning;deep bidirectional;attention image;attention image text;language vision;vision language;learning language models;pretraining paradigm vision;image text tasks"}, "96bb4b49f69419c31857e928969fcaa137e15060": {"ta_keywords": "review based answer;synthesizing review based;amazon product review;method synthesizing review;product review;questions 14m reviews;synthesizing review;reviews known amazon;review based;product review evaluate;review;reviews;14m reviews;14m reviews known;reviews known;review evaluate;based answer question;answer question approach;generation answer propose;generation answer;known amazon product;review evaluate number;based answer;answer question;question approach relies;questions;known amazon;amazon product;models generation answer;amazon", "pdf_keywords": "answerable based reviews;predict answer use;predicting answer question;method predicting answer;predicting answer;answers reviews;answers reviews train;documents collected answer;based questions answering;partially answerable based;answer answer based;collected answer snippets;knowledgeable customer answer;questions answering;answerable based;questions answering answer;frequently partially answerable;answering answer synthesize;knowledge answer question;answer samples collected;answer based;answer snippets;predict answer lengths;systems capable answering;knowledge review document;review document;using knowledge answer;knowledge relevant documents;literature predict answer;answer samples"}, "2873f78efd7adcb118a70f8ea3ca7fa1501e320a": {"ta_keywords": "shot relation classification;relation classification dataset;relation classification;relation classification models;relations build shot;domains instances relations;shot relation;relations;relations build;relation;results shot relation;build shot relation;classification dataset fewrel;classification models domains;instances relations build;instances relations;dataset fewrel;models domains;dataset fewrel present;present shot relation;models domains instances;domains;domains instances;fewrel;fewrel present;classification;present shot;classification models;classification dataset;results shot", "pdf_keywords": "shot relation classification;relation classification;relation classification powerful;relation classification model;tools learn relation;new shot relation;shot relation;learn relation structure;relation extraction;learn relation patterns;predict relation extraction;shot queries propose;learn relation;shot classification;tool learn relation;class shot classification;domains shot learning;queries propose shot;shot queries;extract relations;classify shot tasks;relation good feature;shot classification task;relations domain great;relations;used extract relations;classify shot;relation structure;relation patterns domains;propose shot learning"}, "3f311aee9d25b0284d21274cfc8706d6f0277f87": {"ta_keywords": "dnns quantizing bitwidth;networks dnns quantizing;dnns quantizing;quantization network;networks dnns;quantization network evaluate;nodes quantization network;accuracy deep neural;neural networks dnns;quantizing bitwidth operations;characterize accuracy deep;accuracy deep;quantizing bitwidth;dnns method significantly;deep neural networks;dnns method;method dnns;deep neural;nodes quantization;uses policy gradient;policy gradient based;quantization;policy gradient;effectiveness method dnns;method dnns method;learning algorithm learn;sensitivity nodes quantization;quantizing;bitwidth operations;neural networks", "pdf_keywords": "quantization deep learning;deep quantization neural;quantization deep;deep quantized neural;deep quantization;level quantization deep;optimal deep quantization;deep quantization levels;deep quantization framework;uncertainty deep quantization;quantization uncertainty deep;quantization levels deep;quantized neural network;quantization neural network;quantization networks challenging;quantization networks;deep quantized;quantization neural;relqr deep quantization;dnns compromised quantization;learn quantization;computing quantization networks;quantized neural;layer wise quantization;quantization levels neural;learn quantization bitwidth;function learn quantization;discovering quantization levels;exploit uncertainty quantization;discovering quantization"}, "bcd6cd7bdd661bd86c58b7251ae4633a6ba9979e": {"ta_keywords": "automated method recommending;sets papers reviewers;perform recommendation mining;papers reviewers;papers reviewers use;information retrieval;perform recommendation;recommendation mining;combination information retrieval;recommendation mining multiple;actual reviewing preferences;information retrieval database;reviewing preferences;method recommending;reviewing;recommending;reviewers use combination;reviewing preferences given;technology perform recommendation;based actual reviewing;focused sets papers;retrieval database;mining multiple information;method recommending small;actual reviewing;conference paper committee;reviewers use;reviewers;recommending small focused;recommendation", "pdf_keywords": ""}, "705794a57cca12c2e58b2d77ac32bd4f92ed31ab": {"ta_keywords": "gaussian random graph;random graph models;random graph model;crowdsourcing models;use crowdsourcing models;easy use crowdsourcing;graph models;crowdsourcing models based;random graph;crowdsourcing;use crowdsourcing;graph model generalization;graph models called;graph model;based gaussian random;graph model xmath2;generation gaussian random;gaussian random;known gaussian random;models based gaussian;new generation gaussian;generalization known gaussian;graph;known gaussian;based gaussian;generation gaussian;gaussian;random;models;models based", "pdf_keywords": ""}, "fd306df2809c7acc19dd1994e8ecb11caa33290d": {"ta_keywords": "entangled states light;generation entangled states;coherent state laser;state light laser;generate state light;generation coherent state;generation entangled;entangled states;state laser;state laser beam;coherent state;method generation entangled;state light method;light laser;states light method;laser used generate;entangled;state light;frequency laser;beam laser;generation coherent;light laser applied;states light;laser beam laser;laser beam;laser;fixed frequency laser;frequency laser used;beam laser applied;generate state", "pdf_keywords": ""}, "99e56ebc2f3739dfca93d5a92ebc1e6e2a3050d2": {"ta_keywords": "peer auto grading;grading massive online;online courses peer;courses peer auto;auto grading;auto grading independent;auto grading massive;massive online courses;grading setting automated;grading;courses peer;auto grading setting;grading massive;grading independent;grading setting;scaling peer auto;combines peer auto;online courses;peer auto;scaling peer;combines peer;grading independent hybrid;peer;_dimensionality reduction;approach combines peer;problem scaling peer;machine learning;statistics machine learning;massive online;courses", "pdf_keywords": ""}, "65b226f71faaac9b8a4d63445c85601a16635464": {"ta_keywords": "stochastic gradient descent;gradient descent perturbed;gradient descent stochastic;gradient descent polynomial;descent stochastic gradient;nonconvex optimization;nonconvex optimization problems;stochastic gradient;algorithms polynomial convergence;versions stochastic gradient;descent stochastic;gradient descent;polynomial convergence rate;convergence rate perturbed;convergence rate algorithms;stationary points algorithms;efficient converge second;versions nonconvex optimization;analyze stochastic gradient;algorithms efficient converge;perturbed versions stochastic;efficient converge;perturbed versions nonconvex;descent perturbed;points algorithms polylogarithmic;rate algorithms polynomial;polynomial convergence;algorithms polylogarithmic;descent polynomial;descent perturbed versions", "pdf_keywords": "stochastic gradient descent;nonconvex optimization large;perturbed gradient descent;approximation gradient descent;processing gradient descent;gradient descent;gradient descent hessian;nonconvex optimization;points nonconvex optimization;class nonconvex optimization;optimum nonconvex optimization;faster non convex;nonconvex optimization problems;non convex optimization;classical gradient descent;descent gradient descent;approach nonconvex optimization;stochastic approximation gradient;gradient descent algorithm;optimization non convex;gradient descent gradient;descent hessian vector;perturbed stochastic gradient;gradient function nonconvex;gradient descent method;complexity saddle points;stochastic gradients existing;nonconvex optimization problem;convex optimization methods;iteration complexity saddle"}, "c6bb9e4a9eaa0f0f8309597af2cefe03bd3f1bb5": {"ta_keywords": "benchmark chaotic forecasting;dataset benchmark chaotic;chaotic forecasting models;chaotic forecasting;dataset 131 chaotic;benchmark chaotic;chaotic systems known;chaotic systems;200 chaotic systems;131 chaotic systems;chaotic;chaotic systems spanning;dynamics present dataset;chaos;contains 200 chaotic;chaos present;200 chaotic;datasets correlate forecasting;131 chaotic;degree chaos present;chaos present perform;degree chaos;broadly categorize dynamics;categorize dynamics;forecasting;forecasting models variety;categorize dynamics present;performance degree chaos;forecasting models;datasets", "pdf_keywords": "chaotic attractor forecasting;chaotic forecasting;series chaotic forecasting;reproducible chaotic dynamics;predicting nonlinear dynamics;known chaotic dynamical;predict timescale chaotic;chaotic forecasting method;known chaotic;chaotic interaction forecaster;available chaotic systems;series attractor dataset;chaotic systems known;measured chaotic dynamics;reproducible chaotic;prediction nonlinear dynamics;forecasting nonlinear dynamical;learning nonlinear dynamics;forecasting method chaotic;describing chaotic systems;embedding chaotic;database chaotic dynamical;properties reproducible chaotic;database chaotic dynamics;attractor forecasting;computing dynamics chaotic;chaotic dynamical;learning nonlinear time;model chaotic systems;embedding chaotic attractor"}, "ede108538033ae00d1667685afbd488380020613": {"ta_keywords": "coronavirus omicron subvariant;ss coronavirus omicron;coronavirus omicron;syndrome ss coronavirus;ss coronavirus;viral epitope oc;ss subvariant acute;respiratory syndrome ss;subvariant acute respiratory;respiratory syndrome oc;coronavirus;oc subvariant ss;acute respiratory syndrome;spread acute respiratory;viral population ss;epitope oc subvariant;variants ss immune;acute respiratory;ss immune;syndrome oc commonly;respiratory syndrome;viral epitope;oc subvariant;subvariant acute;syndrome oc;ss subvariant;epitope oc;spread acute;significant factor viral;omicron subvariant significant", "pdf_keywords": ""}, "f9f862f48599526147bbb110ba986ff6872ef4b0": {"ta_keywords": "uncertainty indoor trajectories;model uncertainty indoor;uncertainty indoor;indoor trajectories;indoor trajectories based;uncertainty trajectories random;uncertainty trajectories;uncertainty trajectories related;process uncertainty trajectories;random process uncertainty;trajectories random;account uncertainty trajectories;trajectories related random;assumption trajectories random;process uncertainty;trajectories random nature;model uncertainty;trajectories random model;uncertainty;trajectories based assumption;based assumption trajectories;trajectories based;indoor;random process;random model based;assumption trajectories;trajectories;random model;random nature process;random model able", "pdf_keywords": ""}, "be0c64252a2c3071236d88feeab47d06ef6e0fb7": {"ta_keywords": "recommend recipients email;email message based;assigning recipient email;method recommend recipients;recipient email message;recipient email;recommend recipients;assigning recipient;recipients email message;recipients email;email header;information contained email;email message given;method assigning recipient;message based information;contained email header;email header use;recipient;email message;recipients;email;message based;contained email;based information contained;header;header use;based information;header use method;novel method assigning;assigning", "pdf_keywords": ""}, "b661520bf0061b7d96ccf12016e351dd3a6ee780": {"ta_keywords": "networks optimized sloan;deep linear networks;linear networks optimized;networks optimized;importance weighting surprisingly;efficacy deep linear;importance weighting;sdss importance weighting;deep linear;sloan digital sky;effect importance weighting;sky survey sdss;optimized sloan digital;sky survey;digital sky survey;network significantly;importance weighting subtle;linear networks;optimized sloan;networks;digital sky;survey sdss importance;importance weighting efficacy;weighting efficacy deep;sky;effect importance;importance;network;optimized;sdss importance", "pdf_keywords": "deep learning importance;deep networks importance;weighting deep learning;deep learning weighting;importance weighting deep;regularization deep nets;neural networks importance;overparameterized deep networks;regularization deep;improvement regularization deep;performance deep;learning importance weighting;performance deep networks;improve performance deep;importance weights;deep network significantly;useful deep networks;deep neural;importance weighting parameterized;deep nets;overparameterized deep;deep learning;typical deep networks;importance weighting diminishes;reduced importance weighting;deep networks linear;deep networks;deep neural networks;deep learning models;importance weighting diminishing"}, "2cd2df06d488565063e0600ff840d293be2eaf31": {"ta_keywords": "game adaptive procedure;game gretre matching;game studied adaptive;equilibria game adaptive;game adaptive;adaptive procedure playing;correlated equilibria game;asymptotic behaviour game;equilibria game;game resulting asymptotic;game studied;simple model game;game gretre;playing game gretre;model game;empirical distribution play;procedure playing game;behaviour game studied;model game resulting;distribution play;studied adaptive;adaptive;studied adaptive procedure;gretre matching;game resulting;simple adaptive;play shown converge;playing game;adaptive procedure shown;adaptive procedure", "pdf_keywords": "game regret matching;stochastic game;stochastic games;stochastic players generalizes;stochastic players;payoffs random game;randomized strategies;players strategy theorems;probability stochastic game;behavior payoffs games;strategy statistically optimal;game optimal payoffs;optimal strategy statistically;payoffs strategic games;optimal strategy game;probabilities players strategy;symmetry stochastic games;correlated equilibria games;stochastic game theorem;randomized strategies applicable;randomizes strategies;game optimal;adaptive procedure playing;strategy game optimal;equilibria games;stochastic games theorem;equilibria games known;randomizes strategies exhibit;strategies player game;payoffs games covariance"}, "372657f609f5a95b378a1aad7b08deb9b9b510c0": {"ta_keywords": "unsupervised model adaptation;unsupervised adaptation;framework unsupervised adaptation;model adaptation minimize;model adaptation;adaptation minimize;framework wasserstein distance;unsupervised adaptation amp;adaptation minimize integral;learning framework unsupervised;based framework wasserstein;reinforcement learning framework;model based reinforcement;introduces unsupervised model;unsupervised model;reinforcement learning;adaptation;wasserstein distance;based reinforcement learning;feature distributions real;framework wasserstein;feature distributions;integral probability metric;wasserstein;learning;inspires bound maximization;distributions real simulated;learning framework;unsupervised;based reinforcement", "pdf_keywords": "reinforcement learning extended;reinforcement learning rtls;model based reinforcement;policy optimization;extended reinforcement learning;reinforcement learning model;deep reinforcement learning;reinforcement learning;policy optimization introducing;policy optimization solving;based policy optimization;model based policy;policy gradient;benchmark continuous control;learning behavioral policy;existing model adaptation;policy gradient updates;model adaptation domains;model adaptation framework;model adaptation improved;framework deep reinforcement;model learning;framework extended reinforcement;function reinforcement learning;continuous control benchmark;model learned;model adaptation;model learning key;introducing model adaptation;based reinforcement learning"}, "a0511f02a867bf19e2fa01e6cbd3663f4bd1b953": {"ta_keywords": "harmonic trap interaction;harmonic trap dynamics;harmonic trap particles;particles harmonic trap;confined harmonic trap;interacting particles harmonic;harmonic trap;trap dynamics interacting;particles confined harmonic;harmonic oscillator dynamics;trap dynamics;trap interaction described;particles harmonic;trap interaction;dimensional harmonic oscillator;confined harmonic;dynamics interacting particles;trap particles confined;harmonic oscillator;trap particles;interacting particles;dynamics interacting;described dimensional harmonic;dimensional harmonic;oscillator dynamics studied;case interacting particles;harmonic;oscillator dynamics;particles confined;interacting", "pdf_keywords": ""}, "1ae1850bcfa3c31d7bc828cc33f7dd3926cee26f": {"ta_keywords": "linking linked entities;linked entities based;linked entities;link entity associations;novel linking linked;discover link entity;link entity;links link entity;common knowledge base;linking linked;able link entity;knowledge base;novel linking;linking;entity associations;entities based random;associations presence links;linked;entities based;entity associations presence;approach common knowledge;knowledge base able;present novel linking;entities;entity;discover link;common knowledge;associations;able discover link;presence links", "pdf_keywords": ""}, "30cf652bd33049aaf111a5f84eb262a87c045bdb": {"ta_keywords": "provides real timecheckers;information check features;real timecheckers necessary;real timecheckers;check features document;real worldcheckers check;check features;document able detect;claim verified checking;checking previously fact;timecheckers necessary information;performance real worldcheckers;real worldcheckers;fact checked claims;verified checking;features document able;checked claims present;detect real world;able detect real;detect real;worldcheckers check;timecheckers;timecheckers necessary;verified checking previously;contain claim verified;detect subtle features;worldcheckers check demonstrate;features document;features real;previously fact checked", "pdf_keywords": "textually annotated claims;annotated sentence checking;claims debate accuracy;sentence checking claim;determine claims debates;claims debates actually;debate accuracy statements;fact checkers;claims debates;debate verified assertions;fact checkers journalists;annotated claims;checked claims journalists;identify textually annotated;accurately identify textually;debates actually verified;determine claims speeches;debate accuracy;detect sentences;politifact dataset fact;politifact determine claims;politicians verify claims;accuracy claim checking;sentences contain claim;assist fact checkers;accuracy claims debate;referenced claims speeches;journalists focuses claims;sentences verified using;manually annotated sentence"}, "31412f9b23511e212895305927d9ccddb445bcbc": {"ta_keywords": "voice timbre control;control voice timbre;annotation voice timbre;use voice timbre;parameters using voice;voice timbre words;vectors voice timbre;timbre control parameters;intuitively control voice;voice voice timbre;voice timbre;control voice;voice timbre pre;timbre words voice;speakers voice timbre;annotation voice;using voice;timbre control;vectors voice;using voice voice;use voice;words voice voice;speakers voice;words use voice;target speakers voice;voice voice;method annotation voice;acoustic basis vectors;controlled choice voice;voice voice voice", "pdf_keywords": ""}, "633ee881c594cface387557359ef13613d8eaef0": {"ta_keywords": "assignment agents finite;random assignment agents;agents finite objects;agents unrestricted cardinal;random assignment;problem random assignment;achievable value egalitarian;agents finite;assignment agents;cardinal utilities objects;egalitarian value;unrestricted cardinal utilities;value egalitarian;egalitarian value investigate;finite objects consider;egalitarian value finally;value egalitarian value;optimal;value value egalitarian;finite objects;consider problem random;properties like ordinality;experiments analyzing tradeoffs;ordinality truthfulness accuracy;agents unrestricted;ordinality truthfulness;define bounds optimal;egalitarian;unrestricted cardinal;cardinal utilities", "pdf_keywords": ""}, "1ccd031f28dccfb226f6c0c588c93a97a50bf95f": {"ta_keywords": "apps machine learning;automatic code generation;benchmark automatic code;code generation;code models;write code models;code generation introduce;learning benchmark automatic;code models arbitrary;automatic code;language specification generate;learning benchmark;generate satisfactory python;machine learning benchmark;arbitrary natural language;code use benchmark;natural language specification;parallel machine learning;write code;benchmark automatic;generation introduce apps;introduce apps xcite;models write code;natural language;apps xcite;apps;specification generate;python;satisfactory python code;python code use", "pdf_keywords": "code generation apps;benchmark code generation;code generation;code generation ability;evaluating code generation;specifically code generation;_apps automated programming;assess code generation;natural language apps;evaluation code generation;code generation outperform;programming language apps;code generation difficult;code generation performance;syntax errors benchmark;automated programming;natural language specifications;generated code model;descriptions programming;code model generated;code benchmark;apps benchmark code;able generate code;syntactically correct programs;code benchmark designed;automated programming progress;code model;language apps designed;descriptions programming exercises;benchmark code"}, "ac5e7f9bbc5d46bebc4ec5616aba9d014a6d237f": {"ta_keywords": "fiction student discovery;science fiction student;curriculum introductory computer;computer science courses;writing science fiction;introductory computer science;introductory computer;fiction student;science fiction tool;science fiction;computer science;use science fiction;subject artificial intelligence;student guides;science courses focuses;fiction tool learning;student guide reading;curriculum student guide;science courses;student discovery;student discovery strategy;student guide;curriculum introductory;reading writing science;student guides preparing;fiction tool;writing science;guides preparing curriculum;artificial intelligence set;curriculum", "pdf_keywords": ""}, "615b823d1fc9548ce384f1bb4f544445175e8537": {"ta_keywords": "univalent polycrystalline xmath0he;polycrystalline xmath0he;polycrystalline xmath0he xmath1ca;layers polycrystalline xmath0he;xmath4 layered crystal;xmath3 xmath4 layered;xmath4 layered;polycrystalline;univalent polycrystalline;layers polycrystalline;polycrystalline layers polycrystalline;xmath2o xmath3 xmath4;monovalent polycrystalline layers;consisting monovalent polycrystalline;xmath3 xmath4 obtained;monovalent polycrystalline;xmath1ca xmath2o xmath3;polycrystalline layers;xmath0he xmath1ca xmath2o;xmath2o xmath3;xmath1ca xmath2o;xmath3 xmath4;applying univalent polycrystalline;layered crystal consisting;xmath4 obtained;layered crystal;xmath4 obtained applying;xmath0he xmath1ca;xmath2o;xmath4", "pdf_keywords": ""}, "556a4a0b5fcda4d9f9fad637f2655aeb1b1a00b2": {"ta_keywords": "translation sensitive paralinguistic;sensitive paralinguistic information;paralinguistic information paralinguistic;paralinguistic information;information paralinguistic information;information paralinguistic;paralinguistic information appears;speech translation sensitive;sensitive paralinguistic;paralinguistic;speech speech translation;speech translation;speech information;input speech information;speech information used;detect emphasis;translation sensitive;input speech;target language speakers;method speech speech;method speech;speech speech;language speakers detect;target language;speakers detect emphasis;appears input speech;propose method speech;speech;language speakers;emphasis", "pdf_keywords": ""}, "d6fc0fcf0764065f6e58c57ca850abfdd918504b": {"ta_keywords": "error rate training;sentences large corpus;corpus high quality;large corpus;corpus;log linear scoring;rate training mst;training mst context;intersentence features model;documents central engine;sentences;intersentence features;rate training;process sentences large;engine consists log;large corpus high;process sentences;quality documents;quality documents central;linear scoring;linear scoring model;training mst;high quality documents;sentences large;corpus high;intra intersentence features;context central engine;scoring model;designed process sentences;minimum error rate", "pdf_keywords": ""}, "7de4a82edf68b69a9c007fe8e840edf4ade1171c": {"ta_keywords": "kinetics enzyme arginine;enzyme ficin arginine;arginine based enzyme;enzyme arginine derivatives;ficin arginine derivatives;kinetics arginine;kinetics arginine based;enzyme arginine;based enzyme ficin;arginine derivatives studied;ficin arginine;arginine derivatives characterized;enzyme kinetics;enzyme ficin;arginine derivatives;enzyme kinetics enzyme;sulphydryl enzyme kinetics;kinetics enzyme;arginine based;acetylation free arnino;enzyme sulphydryl enzyme;compared enzyme sulphydryl;based enzyme;arginine;enzyme sulphydryl;results compared enzyme;sulphydryl enzyme;enzyme;effect acetylation;compared enzyme", "pdf_keywords": ""}, "2cae732250b59f9e2238626d8d7e0064b97de3c9": {"ta_keywords": "wavelet transform;wavelet transform domain;representation wavelet transform;representation wavelet;new representation wavelet;signal reconstruction;wavelet;signal reconstruction extend;algorithm signal reconstruction;reconstruction image stabilized;reconstruction image;representation applied reconstruction;applicability reconstruction image;reconstruction extend representation;zero crossing representation;image stabilized dimensional;dimensional zero crossing;applied reconstruction;iterative algorithm signal;stabilized zero crossing;transform domain;transform;demonstrate applicability reconstruction;reconstruction extend;image stabilized;stabilized dimensional zero;crossing representation;applicability reconstruction;crossing representation representation;zero crossing", "pdf_keywords": ""}, "616c15dd765c36c21efc75c7ed52e5af81c21053": {"ta_keywords": "universal quantum computing;architecture universal quantum;universal quantum based;based universal quantum;use universal quantum;quantum computing architecture;universal quantum;universal universal quantum;quantum computing;combination universal quantum;quantum based;computing architecture universal;quantum based use;quantum;architecture based universal;architecture combination universal;architecture universal;generation universal universal;computing architecture proposed;computing architecture;universal universal;based universal;universal universal universal;computing architecture architecture;computing architecture combination;generation universal;universal;based use universal;architecture combination;architecture proposed", "pdf_keywords": ""}, "bf9b069242f0af129c2aad8430a52454b008c327": {"ta_keywords": "stochastic gradient descent;rate stochastic gradient;learning rate stochastic;stochastic gradient;gradient descent sgd;formulation stochastic gradient;learning rate dependent;rate stochastic;rate dependent stochastic;learning rate;gradient descent;descent sgd analysis;stochastic differential;use learning rate;dependent stochastic differential;rate convergence;formulation stochastic;time formulation stochastic;stochastic differential equation;rate convergence continuous;descent sgd;linear rate convergence;sgd analysis;dependent stochastic;sgd analysis based;effect learning rate;descent sgd broad;stochastic;surrogate continuous time;rate dependent", "pdf_keywords": "stochastic gradient descent;learning rate stochastic;rate stochastic gradient;stochastic learning rate;learning rate sgrg;gradient learning rate;gradient descent sgd;stochastic gradient;stochastic gradient model;gradient descent sde;learning rate converges;formulation stochastic gradient;learning rate performance;arbitrary learning rate;stochastic gradient algorithm;dependent stochastic gradient;stochastic gradient dynamics;present stochastic gradient;bounded learning rate;learning rate sufficiently;stochastic optimization;learning rate dependent;optimization stochastic optimization;learning rate exponential;learning rates convex;arbitrary learning rates;descent sde stochastic;stochastic learning;function stochastic learning;learning rates"}, "250f8f71f7cff972a70482229ca9053b356217cd": {"ta_keywords": "voice activity detection;unsupervised voice activity;online unsupervised voice;voice activity;unsupervised voice;activity detection vad;activity detection;voice;bayes model comparison;online unsupervised;vad using bayes;model comparison proposed;paper presents statistical;phenomenological model comparison;bayes model;detection vad using;model comparison;using bayes model;model comparison framework;detection vad;unsupervised;statistical methods;comparison proposed method;using bayes;competitive conventional statistical;statistical;detection;presents statistical;based phenomenological model;statistical scheme online", "pdf_keywords": ""}, "3d2ceea5dea234ae9a20f8e1c9e558735757e90e": {"ta_keywords": "language dependent phoneme;support multilingual acoustic;multilingual acoustic;multilingual acoustic texts;phoneme distributions;phoneme distributions able;dependent phoneme distributions;allophone recognizers;accuracy allophone recognizers;acoustic texts;allophone recognizers 17;accuracy allophone;increase accuracy allophone;acoustic texts increase;dependent phoneme;able support multilingual;allophone;language language dependent;multilingual;support multilingual;language dependent;phoneme;model language language;language language;acoustic;recognizers;language;texts increase accuracy;texts;model language", "pdf_keywords": "multilingual acoustic classification;multilingual acoustic recognition;recognition allophones multilingual;phonemes multilingual acoustic;multilingual acoustic modeling;multilingual acoustic models;acoustic phone recognition;multilingual acoustic;allophones multilingual acoustic;approach multilingual acoustic;combines multilingual acoustic;accuracy multilingual acoustic;language specific phonemes;phonemes phonemes multilingual;language dependent phoneme;specific phonemes allophone;phonemes multilingual;recognition spoken languages;acoustic modeling phoneme;language specificity phonemes;phonemes allophone layer;phoneme model language;allophones multilingual;phoneme assignment languages;phoneme inventory languages;phonemes allophone;phoneme inventory corpus;method multilingual acoustic;phone recognition;acoustic classification combines"}, "8234049255a0e03fc745457de456634d1aab214b": {"ta_keywords": "discovering structure web;structure web pages;discover structure web;structure web page;automatically discover structure;extracting information web;understand structure web;structure web;automatically learning discovering;discovering structure;learning discovering structure;automatically learning;web pages method;use wrapper learning;method automatically learning;discover understand structure;information web page;learning discovering;automatically discover understand;wrapper learning;web pages;discover structure;wrapper learning allows;automatically discover;extracting information;web page;pages method based;understand structure;web page present;page present simple", "pdf_keywords": ""}, "e11b4750e288785134f042c144f057a11dc0180a": {"ta_keywords": "dynamic representation elections;elected representatives analyze;representation elections;representations allow voters;representation elections performed;set elected representatives;voters model;performed voters model;voters model uses;representatives analyze;elected representatives;elections;representative systems;representatives analyze performance;elections performed voters;voters alter issue;direct dynamic representations;dynamic representations;representatives;dynamic representation;voters;voters alter;dynamic representations allow;weights set elected;elections performed;direct dynamic representation;allow voters alter;representative;allow voters;representative systems yield", "pdf_keywords": "flexible representative democracy;democracy binary voting;direct democracy binary;direct democracy flexible;democracy optimal voting;direct democratic voting;direct democracy optimal;voting direct democracy;democracy flexible delegations;optimal finite voting;voters choices optimal;flexibility election process;finite voting equivalent;optimal voting;voters guarantee representation;voting equivalent direct;interactive democracy;direct democracy election;election process optimal;model interactive democracy;process finite voting;direct representative democracy;finite voting;existing interactive democracy;systems local election;linear voting direct;democracy binary;election process suitable;interactive democracy smoothly;voting systems consider"}, "b8b813111c411ae61881ab9cd25707d9de6444ec": {"ta_keywords": "compositional attention outperforms;attention variety tasks;compositional attention;multi head attention;compositional attention replaces;called compositional attention;attention mechanism;pairing composes attention;head attention variety;novel attention mechanism;attention mechanism called;attention replaces standard;head structure attention;head attention;attention variety;demonstrate compositional attention;composes attention manner;attention manner independent;attention replaces;attention outperforms standard;structure attention;structure attention blocks;composes attention;attention outperforms;attention blocks;attention manner;attention blocks used;attention;novel attention;propose novel attention", "pdf_keywords": "reasoning robotics attention;tasks compositional attention;decompose attention search;attention composition;efficient way attention;attention search;attention mechanism search;robotics attention;attention search redundancy;composition attention;compositional attention;composition attention decompose;attention decompose multi;compositional attention powerful;attention model flexible;head attention search;decompose attention;multi head attention;head attention composition;attention search retrieval;attention model generalizes;attention retrievals;composition attention uses;attention context computation;attention retrievals proposed;compositional attention able;robotics attention lagging;attention fundamental component;combinations compositional attention;use decompose attention"}, "a469f2ec3ab15f20f06d95aea1839b1263d3385e": {"ta_keywords": "random assignment mechanisms;ordinality envy freeness;welfare aspects random;assignment mechanisms;assignment mechanisms agents;assignment mechanisms approximate;random assignment;different random assignment;ordinality envy;agents unrestricted cardinal;egalitarian welfare;egalitarian welfare aspects;like ordinality envy;consider egalitarian welfare;unrestricted cardinal utilities;mechanisms approximate optimal;aspects random assignment;envy freeness;envy freeness truthfulness;cardinal utilities objects;freeness truthfulness achievable;cardinal utilities;unrestricted cardinal;consider egalitarian;properties like ordinality;egalitarian;utilities objects bounds;agents unrestricted;achievable value;truthfulness achievable value", "pdf_keywords": "guaranteed allocation envy;allocation envy;constraint allocation envy;random assignment mechanisms;allocation envy free;efficiency envy freeness;fair allocation indivisible;symmetric utility mechanism;agents utility allocations;utility allocations fair;optimal allocation agents;arbitrary allocations;allocations fair;envy free mechanisms;preserving envy free;allocation indivisible goods;envy freeness achievable;optimal allocation;guaranteed allocation;assignment maximizes utility;random assignment objects;random allocation;ordinality envy freeness;allocation indivisible;fair allocation;consider randomized mechanisms;tradeoffs efficiency envy;utility allocations robust;allocations fair exponential;allocations items optimal"}, "18f4ec53a4221a97e1482f091f41a23f3d873cf2": {"ta_keywords": "supervised weak supervision;predict evidence annotations;strong supervision task;semi strong supervision;strong supervision tasks;weak supervision semi;supervised semi strong;supervision semi strong;predicting supporting evidence;supervised weak;supervision task predicting;weak supervision;evidence annotations;evidence annotations demonstrate;able predict evidence;supervised semi;supervised;strong supervision;approach supervised weak;robustness supervised semi;predict evidence;supervision semi;new approach supervised;robustness supervised;supervision task;supervision tasks;examining robustness supervised;approach supervised;document able predict;annotations demonstrate", "pdf_keywords": "evidence extraction predicted;evidence sequence labeling;classification evidence extraction;evidence extraction;extraction evidence text;text classification evidence;evidence extraction combines;text sequence classification;rapid evidence extraction;classification evidence sequence;abundant evidence annotations;generating evidence label;evidence annotations;sequence labeling tasks;text classification tasks;evidence extraction apply;supervision prediction tasks;extract evidence;sequence classification evidence;extract evidence using;evidence text documents;evidence text;extraction predicted label;evidence annotations paper;evidence annotations facilitate;model text classification;conditioning evidence extraction;text classification;extraction evidence;extraction classification"}, "c4536a5c7f47bfc48df202ba882002531248f955": {"ta_keywords": "sounds generated electrolarynx;control fundamental frequency;patterns acoustic excitation;electrolarynx real time;frequency patterns acoustic;acoustic excitation sounds;acoustic excitation;excitation sounds generated;generated electrolarynx performance;generated electrolarynx method;patterns produced electrolarynx;electrolarynx performance proposed;patterns acoustic;physical electrolarynx;generated electrolarynx;fundamental frequency patterns;automatically control fundamental;sounds generated;excitation sounds;frequency patterns produced;electrolarynx performance;actual physical electrolarynx;electrolarynx method;predicts fundamental frequency;electrolarynx method relies;electrolarynx;frequency patterns;produced electrolarynx;fundamental frequency;control fundamental", "pdf_keywords": ""}, "f43ae70242aea3dbb80b7c3b5474356e9ee9079b": {"ta_keywords": "complexity natural language;natural language reduced;complexity natural;evaluation complexity natural;natural language;evaluate complexity natural;random variables complexity;complexity;variables complexity natural;evaluation complexity;natural language use;evaluate complexity;variables complexity;method evaluation complexity;natural language presence;method evaluate complexity;language reduced;language;language reduced small;language presence large;large number random;language use method;small number random;language use;number random variables;new method evaluation;language presence;random variables;random;natural", "pdf_keywords": ""}, "72c9663494827b2e87ad5a65a6ff7e769eb15a57": {"ta_keywords": "story loop generative;story written criteria;story loop;evaluation criteria story;criteria story written;criteria story;experiments story loop;story enable;story enable new;story written;novel evaluation;reinforcement learning models;new reinforcement learning;loop generative;novel evaluation criteria;propose novel evaluation;loop generative data;generative;reinforcement learning;capture essence story;generative data;learning models capture;capture experiments story;new reinforcement;essence story enable;reinforcement;story;generative data set;model achieves better;written criteria", "pdf_keywords": "visual story generation;generate good story;story generation quality;optimizes story generation;story generation directly;story generation;generated stories;visual storytelling task;model visual storytelling;quality text generation;stories generated;generation visual storytelling;generated stories using;story photo stream;generate relevant stories;designing good story;text generation visual;text stories adapting;crowdsourcing stories generated;evaluate generated stories;text generation task;text generation;approach visual story;visual storytelling;generated sentences model;text stories;optimizes story;text stories use;unstructured text stories;iteratively generated sentences"}, "15bb07d0996ece844de8cae24d3dc15972e6841a": {"ta_keywords": "summarization datasets reliability;summarization datasets;popular summarization datasets;summarization systems;summarization systems analyse;state art summarization;samples popular summarization;popular summarization;art summarization systems;summarization;reliability automatic metrics;automatic metrics;automatic metrics framework;datasets reliability;datasets reliability models;study reliability automatic;art summarization;metrics framework;quality datasets quantify;quality datasets;impact datasets reliability;reliability automatic;metrics;metrics framework state;dependent quality datasets;reliability models strongly;datasets quantify;reliability models;datasets performance provide;datasets", "pdf_keywords": "describes summarization news;summarization metrics;summarization news;summarization datasets;automatic summarization metrics;summarization metrics like;summarization datasets based;popular summarization datasets;summarization news articles;article describes summarization;summarization documents;describes summarization;evaluation popular summarization;typology summarization datasets;news articles summarization;reliability automatic summarization;summarization documents presented;summarization datasets gigaword;automatic summarization;text summarization documents;text summarization;summarization varies;summarization performed;present summarization news;text summarization process;news articles summaries;consistency text summarization;popular summarization;summarization set heterogeneous;summarization"}, "674833d48a77ef009f751a66988590592dd5d996": {"ta_keywords": "zeeman field spin;spin orbit coupling;orbit coupling electron;coupling electron effect;coupling electron;coupling strength spin;effect zeeman field;significant spin orbit;strength spin orbit;large effect zeeman;field spin orbit;effect zeeman;increase spin orbit;spin orbit;orbit coupling strength;effect significant spin;electron effect;orbit coupling;field spin;zeeman field;electron;significant spin;zeeman;electron effect significant;increase spin;coupling strength arbitrarily;study effect zeeman;coupling strength;strength increase spin;coupling strength increase", "pdf_keywords": ""}, "49ee2270f3265ee27b36e05e130be79e05d5ba29": {"ta_keywords": "parallelization learning problem;text parallelization;learning problem parallelization;learning textbooks approach;parallelization learning;parallelization text;text parallelization achieved;explain text parallelization;learning textbooks;textbooks approach;problem learning textbooks;parallelization text used;allows parallelization text;textbooks approach based;allows parallelization learning;learning problem;textbooks;approach machine learning;machine learning;machine learning problem;problem parallelization;problem learning;problem parallelization achieved;parallelization;learning problem learning;learning;parallel algorithm;use parallel algorithm;use parallel;parallel", "pdf_keywords": ""}, "68ea2572584068befd441dccf461f3444ff14f4a": {"ta_keywords": "student agent simulates;simulates human learning;physical student agent;agent simulates human;agent simulates;creating physical student;student agent;simulates human;toe game demonstrate;human learning process;student learn skill;human learning;game demonstrate ability;game demonstrate;skill knowledge interacting;physical student;learning process;tac toe game;demonstrate ability student;ability student learn;learn skill knowledge;student learn;simulates;creating physical;learn skill;skill knowledge;ability student;interacting users physical;demonstrate ability;toe game", "pdf_keywords": ""}, "4cfc7d3c6a61f6db48b1f3c75235592c1609a54f": {"ta_keywords": "transcription interface use;coding transcription interface;transcription interface;iterative coding transcription;iterative interfaces;coding transcription;variations iterative interfaces;iterative coding;interfaces non iterative;iterative interfaces non;transcription;non iterative coding;iterative;variations iterative;interface;gains variations iterative;interface use mixed;interface use;interfaces;coding;interface design;mixed effects;use mixed effects;non iterative;mixed effects models;choice interface;interfaces non;effects models;choice interface design;effects models quantify", "pdf_keywords": ""}, "e2ac96254d7e9ec0dde882e3a09797d00f26220f": {"ta_keywords": "valve spin orbit;dynamics spin valve;control spin dynamics;spin valve spin;dynamics spin orbit;spin dynamics used;dynamics spin;spin dynamics spin;spin dynamics;spin orbit interaction;dynamics spin dynamics;orbit interaction spin;control spin;spin orbit;interaction spin dynamics;effect spin orbit;spin valve;valve spin;interaction spin;effect spin;used control spin;spin;study effect spin;orbit interaction used;orbit interaction;dynamics used control;dynamics used;dynamics;valve;orbit", "pdf_keywords": ""}, "757acf616a38422c7186952e1075a28fed1a07c0": {"ta_keywords": "neurotransmitters nitric oxide;amino acid neurotransmitters;ferromagnetic component anesthetic;gmp cgmp signaling;acid neurotransmitters nitric;anesthetic xylazole concentrations;anesthetic xylazole;component anesthetic xylazole;neurotransmitters nitric;fetal rat nerve;optimization cgmp signaling;cgmp signaling;neurotransmitters maximum likelihood;acid neurotransmitters;cgmp signaling pathway;acid neurotransmitters maximum;neurotransmitters;neurotransmitters maximum;nerve cells optimization;xylazole concentrations amino;cyclic gmp cgmp;cells optimization cgmp;optimization concentrations amino;nitric oxide cyclic;nitric oxide;pathway fetal rat;rat nerve;rat nerve cells;amino acid;signaling pathway fetal", "pdf_keywords": ""}, "fc912e9af47bf10428396b687b2bfb1e5832fcb1": {"ta_keywords": "connectionist temporal classification;speech recognition asr;speech recognition;asr based connectionist;temporal classification gc;automatic speech recognition;based connectionist temporal;function automatic speech;efficient auxiliary loss;automatic speech;connectionist temporal;auxiliary loss function;recognition asr;temporal classification;recognition asr based;auxiliary loss;gc encoder network;connectionist;gc encoder;layer gc encoder;encoder network;encoder network intermediate;loss function automatic;encoder;simple efficient auxiliary;classification gc;based connectionist;efficient auxiliary;loss function;word error rate", "pdf_keywords": "layer network learning;connectionist temporal classification;network learning performance;end speech recognition;speech recognition asr;speech recognition;network learning;improves network learning;automatic speech recognition;loss stochastic depthwe;temporal classification ctc;nonlinear speech recognition;learning performance network;non autoregressive training;efficient auxiliary loss;network learning key;autoregressive training;performance network learning;network regularizes training;ctc encoder network;automatic speech;asr based connectionist;ctc loss stochastic;depth train network;stochastic depth train;function automatic speech;speech recognition problem;layer ctc encoder;autoregressive training objective;encoder network"}, "f3132572bb3870dbe99b2d1c01ce17fa38783a2f": {"ta_keywords": "power spectrum generated;estimating power spectrum;generation power spectrum;power spectrum;spectrum power spectrum;power spectrum power;pulse power spectrum;power spectrum method;spectrum power;spectrum power pulse;spectrum generated generation;spectrum generated;spectrum method based;spectrum;spectrum method;generating estimating power;generated generation power;power pulse power;power pulse;pulse power;estimating power;generation power;based generation power;generated generation;method generating estimating;method generating;generating;generating estimating;method based generation;new method generating", "pdf_keywords": ""}, "213e471bacff5c0852943988fcb955797f1e591f": {"ta_keywords": "automatic reference translation;automatic reference;references human evaluation;reference translation compares;methodology automatic reference;reference translation;effective reference formulation;provides effective reference;references human;value references human;compares value references;reference formulation;translation compares value;value references;translation compares;effective reference;human evaluation methodology;systems metrics provides;human evaluation;systems metrics;references;evaluation methodology;translation;metrics provides effective;metrics provides;evaluation;evaluation methodology works;present methodology automatic;methodology automatic;metrics", "pdf_keywords": "evaluation machine translation;machine translation evaluation;quality reference translations;machine translation research;human reference translations;translation evaluation accuracy;quality machine translation;translation based evaluation;reference translations bias;machine translation relies;ratings machine translation;translation research automated;machine translation presence;paraphrased reference automatic;translation evaluation;accuracy machine translation;reference translations generally;accuracy reference translations;accuracy human translation;reference translations outputs;unbiased reference translation;machine translation;translation model evaluation;quality paraphrased reference;machine translation observe;automated evaluations stronger;automated evaluations broadly;translationese references;reference translations investigate;references using paraphrasing"}, "77568c594470f9aa029f92774e2c12ab0451d9bb": {"ta_keywords": "robust language models;trained mixture topics;distributionally robust language;topic conditional value;topic conditional;trained mixture reviews;language models;mixture reviews news;robust language;mixture topics;language models approach;training distributionally robust;mixture topics proposed;idea topic conditional;risk topic cvar;model trained mixture;topic cvar;topics;mixture reviews;training distributionally;trained mixture;topics proposed approach;factor trained mixture;approach training distributionally;loss model trained;topic cvar novel;reviews news;distributionally robust;risk topic;topics proposed", "pdf_keywords": "robust language models;robust languages train;robust language modeling;test distribution robust;distributionally robust learning;learn robust languages;robust language;significantly robust learning;language modeling train;likelihood training remedy;learning mle terms;learning mle corpora;likelihood training;mismatch robust language;maximum likelihood training;conditional distribution sentences;learning language model;languages train test;training distribution proposed;robust learning model;language models;distributionally robust optimization;knowledge test distribution;learning mle;training distribution;training text;models learn robust;robust learning;arbitrary test distributions;framework robust language"}, "89c64fd60ca58f4753a818cd0923f5041b51a807": {"ta_keywords": "sensor sink interconnection;sensor routing;connected sensor sink;sink interconnection goal;connected sensor point;sensor routing sensor;routing sensor;place connected sensor;connected sensor;routing sensor available;connect sensor sink;sink interconnection;location sensor routing;sensor available paths;sensor sink;relay network cost;network cost objective;line connect sensor;connected relay network;connect sensor;interconnection goal;sensor point line;interconnection goal place;solutions connected relay;relay network;routing;problem connected sensor;connected relay;network cost;optimal solutions connected", "pdf_keywords": ""}, "612d577534dbbf546405d4036d912666523a8164": {"ta_keywords": "polynomial learnability restricted;description logics tractable;logics tractable learning;learnability restricted;polynomial learnability;learnability restricted subset;examples learnable sublanguage;examples learnable;study polynomial learnability;description logics;logics tractable;learnable sublanguage;learnability;learnable sublanguage appears;used examples learnable;subset description logics;learnable;previously known learnable;known learnable;incomparable expressive;incomparable expressive power;tractable learning;appears incomparable expressive;expressive power subset;tractable learning positive;expressive power;learning positive examples;logic previously known;expressive;restricted subset description", "pdf_keywords": ""}, "0d516b476559485e04290e859ca59101c0a91ae1": {"ta_keywords": "relay millimeter wave;uncertainty quality relay;millimeter wave;ues relay millimeter;relay millimeter;millimeter wave mm;optimal threshold policy;observable decision process;mm wave wave;wave mm wave;partial observable decision;observable decision;derive optimal threshold;optimum distance user;mm wave;wave mm;stationary threshold policy;decision process pdm;optimal threshold;threshold policy;quality relay;optimum distance;wave proposed approach;determine optimum distance;uncertainty quality;model uncertainty;model uncertainty quality;threshold policy maps;link derive optimal;ues relay", "pdf_keywords": "obstacles millimeter wave;relay selection;exploring new relay;learning relaying;relay links vicinity;modeled relay selection;learning relaying link;millimeter wave networks;relay minimizes packet;problem learning relaying;networks d2d relay;relay link learning;communication obstacle dynamic;selecting relay;distributed quantum networks;propose novel relay;relay region proposed;relaying;selection wireless networks;relay link selection;selecting relay minimizes;relay selection problem;discovering relay;relay minimizes;wireless links ubiquitous;relay links;d2d communication obstacle;relay;relay link design;selection wireless"}, "99fe5475ab28fa7ad4bce51d7b294b3f40caad4d": {"ta_keywords": "harmonic trap condensate;bose einstein condensate;condensate harmonic trap;einstein condensate harmonic;atom harmonic trap;harmonic trap interaction;dynamics dimensional bose;condensate harmonic;harmonic trap;atom harmonic;level atom harmonic;trap interaction atom;dimensional bose einstein;einstein condensate;trap condensate created;interaction atom trap;atom trap described;trap condensate;dimensional bose;trap described gaussian;atom trap;trap interaction;bose einstein;condensate created;condensate created level;harmonic;created level atom;trap described;interaction atom;field level atom", "pdf_keywords": ""}, "31392ad8722d9c66181b621936e2013199e02edc": {"ta_keywords": "language model knowledge;tuning nlu tasks;words models need;words models;unsupervised language model;model knowledge probing;language model;models need learn;pretraining data;reliably encode syntactic;fine tuning nlu;number words models;nlu tasks;respect pretraining data;pretraining data volume;semantic features 10m;model knowledge;nlu tasks maximum;encode syntactic semantic;require 10m learn;knowledge probing;learn reliably encode;tuning nlu;syntactic semantic features;unsupervised language;10m learn;judgments unsupervised language;knowledge probing fine;model ability;syntactic semantic", "pdf_keywords": "pretrained language models;programming nlulp tasks;linguistic features learnable;nonlinear programming nlulp;learnable 100m words;large pretrained language;programming nlulp;linguistic features pretraining;nlulp tasks;reliably encode syntactic;learn linguistic features;language models acquire;language models;linguistic generalization highly;words learn representations;ability language models;large class linguistic;language models lsps;language approximation;100m words learn;nlulp tasks use;language approximation framework;feature understanding linguistic;features learnable 100m;syntactic semantic featureswe;characterizing linguistic representations;encode syntactic semantic;acquire representations linguistic;representations linguistic features;representation linguistic features"}, "6c82727731955a2332a0cc38ec56b35a971061eb": {"ta_keywords": "nucleon toolkit;nucleon toolkit nucleon;nucleon nucleon toolkit;toolkit nucleon;toolkit nucleon nucleon;design xnmt nucleon;xnmt nucleon nucleon;xnmt nucleon;multitasked machine translation;nucleon nucleon;nucleon nucleon experiment;nucleon;machine translation speech;nucleon experiment;nucleon experiment configuration;machine translation;tasks machine translation;translation speech recognition;recognition multitasked machine;speech recognition multitasked;design xnmt;translation speech;speech recognition;recognition multitasked;xnmt;multitasked machine;tasks machine;machine;toolkit;translation", "pdf_keywords": "neural machine translation;machine translation nmt;translation nmt toolkit;training inference nmt;deep learning xnmt;tasked machine translation;machine translation speech;machine translation benchmark;machine translation;tasks machine translation;learning nmt toolkit;learning nmt;standard machine translation;recognition transcription translation;accelerate training inference;learning xnmt supports;learning xnmt;translation speech recognition;machine translation parsing;presents training inference;training inference;transcription translation framework;deep learning recurrent;modeling framework deep;source toolkit xnmt;corpus languages neural;nmt toolkit goal;transcription translation;translation benchmark achieves;network learning nmt"}, "7e406537f52528527d10872d1807ad974599b13a": {"ta_keywords": "search signatures neural;text similar neural;patterns input text;input text neural;text neural;text neural network;signatures neural network;signatures neural;text pattern input;pattern input text;input text pattern;text pattern;detect patterns input;similar neural network;patterns input;parallel search signatures;detect patterns;input text similar;pattern input;able detect patterns;search signatures;neural network;text similar;network neural;similar neural;neural network form;neural network neural;patterns;parallel search;patterns form patterns", "pdf_keywords": ""}, "5babe5334c6867db13fa7e6943f64059c7cba6ce": {"ta_keywords": "particle moving harmonic;harmonic potential particle;moving harmonic potential;harmonic potential model;particle described wave;wave function particle;moving harmonic;dynamics particle;potential particle described;dynamics particle moving;harmonic potential;model dynamics particle;potential particle;particle moving;wave function model;function particle described;described wave function;particle described;concept particle moving;wave function wave;wave function;wave function generated;generated wave function;function particle;function wave;harmonic;potential model based;function generated wave;potential model;particle", "pdf_keywords": ""}, "c0cce8955bf10b21753161ffaa1978a7c8b78a16": {"ta_keywords": "microphone input statistical;bodyconductive microphone;microphone proposed framework;special bodyconductive microphone;microphone;air conductive microphone;bodyconductive microphone called;microphone proposed;microphone called;microphone input;nam microphone proposed;nam microphone;microphone called nam;statistical voice conversion;called nam microphone;voice conversion;conductive microphone;conductive microphone input;input statistical voice;voice conversion vc;optical network nam;optical network;statistical voice;american optical network;optical;voice;enhancement signal;applied estimation signal;estimation signal special;estimation signal", "pdf_keywords": ""}, "6027ef3b4e5585b45db0b9d333956425d3972351": {"ta_keywords": "commonsense reasoning open;commonsense reasoning benchmarks;commonsense reasoning;commonsense reasoning uses;ended commonsense reasoning;open ended commonsense;reasoning open ended;reasoning knowledge facts;reasoning open;reasoning knowledge;reasoning benchmarks collect;reasoning benchmarks;question crowd sourcing;reasoning uses;commonsense;hop reasoning knowledge;reasoning;ended commonsense;crowd sourcing;crowd sourcing approach;multi hop reasoning;reasoning uses novel;approach open ended;knowledge facts evaluate;test question crowd;question crowd;answers test;new answers test;knowledge facts;hop reasoning", "pdf_keywords": "open commonsense reasoning;commonsense reasoning opencsr;commonsense reasoning challenging;targeting commonsense reasoning;commonsense reasoningwe propose;existing commonsense reasoning;commonsense reasoningwe;reasoning knowledge graph;commonsense reasoning novel;commonsense reasoning research;commonsense reasoning;open ended reasoning;task answering commonsense;open domain reasoning;ended commonsense reasoningwe;commonsense knowledge;reasoning questions open;use commonsense knowledge;commonsense knowledge present;queries require commonsense;open ended commonsense;require commonsense knowledge;differentiable reasoning knowledge;ended commonsense reasoning;reasoning approach open;reasoning knowledge;knowledge graph;common reasoning opencsr;reasoning paths;commonsense knowledge answer"}, "a38e0f993e4805ba8a9beae4c275c91ffcec01df": {"ta_keywords": "program synthesis;program synthesis general;models program synthesis;programming languages shot;language models program;predict output program;programming languages;large language models;purpose programming languages;language models;general purpose programming;programming;output program;synthesis general;models program;large language;predict output;program;synthesis;program given specific;output program given;languages shot;languages;purpose programming;languages shot fine;synthesis general purpose;fine tuning regimes;predict;tuning regimes models;performance unable predict", "pdf_keywords": "synthesis programming languages;program synthesis tasks;synthesis short programs;program synthesis;program synthesis datasets;models program synthesis;program synthesis software;program synthesis general;synthesis simple programs;program synthesis aim;code program synthesis;program synthesiswe large;program synthesiswe;benchmark program synthesiswe;program synthesis ask;synthesis programming;benchmarks program synthesis;language models program;dataset program synthesis;programs natural language;program synthesis shown;language models generate;programming languages long;synthesis software engineering;language model execution;short programming;new programming language;methods program synthesis;programming language;generate natural language"}, "4f4da6fdb9496b0295764b2db11381dd390de02d": {"ta_keywords": "automatically recognized speech;recognized speech transcripts;speech transcripts train;speech transcripts;manual correction automatically;sensitive manual correction;transcripts train;transcripts train baseline;transcripts;manual correction;correction automatically recognized;cost sensitive manual;optimizes segmentation;recognized speech;optimizes segmentation segments;segmentation segments cost;correction automatically;segments cost sensitive;method optimizes segmentation;human supervision efficiency;human supervision;segmentation used;segmentation;sensitive manual;segmentation segments;user modeling;computing segmentation used;supervision efficiency previous;computing segmentation;speech", "pdf_keywords": ""}, "c581686edbd7227e9eb4a0841cce16728ca27369": {"ta_keywords": "instructional recipes comprehension;introduce machine comprehension;build machine comprehension;machine comprehension;comprehension answer questions;machine comprehension named;recipes comprehension process;comprehension named recipeqa;machine comprehension answer;recipes comprehension;comprehension process use;comprehension process;comprehension named;structure instructional recipes;instructional recipes using;instructional recipes possible;linguistic structure instructional;recipes using linguistic;instructional recipes;comprehension;recipeqa integrates linguistic;aspect instructional recipes;modalities instructional recipes;linguistic aspect instructional;comprehension answer;questions modalities instructional;recipes possible understand;structure instructional;using linguistic;understand linguistic", "pdf_keywords": "recipe description multimodal;machine comprehension cooking;multimodal machine comprehension;comprehension cooking recipes;knowledge recipe descriptions;recipe analyzing lexical;comprehension cooking;comprehension task built;recipes multiple descriptions;machine understand recipes;questions knowledge recipe;context recipe description;recipe descriptions;machine understand recipe;understand recipe descriptions;descriptions recipewe;recipe descriptions recipewe;recipe descriptions method;comprehension tasks;descriptions recipewe present;sequence recipe images;synthetic natural language;machine comprehension;comprehension tasks require;based modality comprehension;modality comprehension task;context recipe;ask recipe description;description multimodal;recipe description"}, "ad48174ccbff6259a7d3cb0d0985e5aefa314b84": {"ta_keywords": "machine translation mtm;machine translation systems;translation mtm systems;different machine translation;translation systems using;translation systems;machine translation;level machine translation;translation mtm;translation systems context;language processing;survey machine translation;use domain morphological;domain morphological properties;beam domain morphological;domain morphological;morphological;mtm systems use;mtm systems;morphological properties;character level machine;natural language processing;morphological properties underlying;translation;mtm;performance different character;natural language;machine compare;machine compare performance;performance different machine", "pdf_keywords": "language processing characterlevel;level machine translation;characterlevel natural language;computational translation;processing characterlevel;character processing;perform computational translation;character processing architecture;character level methods;machine translation tasks;computational translation task;character level systems;processing characterlevel mt;machine translation able;machine translation mt;character level neural;current character models;machine translation powerful;wecomputational linguistics;character level subword;machine translation cng;translation neural machine;text characterizing computational;character level models;perform machine translation;language processing;wecomputational linguistics powerful;data wecomputational linguistics;machine translation nnt;character based machine"}, "1e3e2b03e28f48bb4d48154992cd6b62969c643e": {"ta_keywords": "protein characterization near;protein characterization;domain protein characterization;domain protein;protein;face domain protein;domain characterization near;domain characterization;characterization near;characterization;near future challenges;future challenges;characterization near near;face domain characterization;near near future;near future;future challenges face;brief overview challenges;overview challenges;challenges face domain;near future believe;overview challenges face;future believe challenges;future;present brief overview;future believe;domain;face domain;challenges;brief overview", "pdf_keywords": ""}, "7b51209e7d9cbedc18b6ab202e6fcdabaebbb088": {"ta_keywords": "discriminative language model;discriminative language modeling;accuracy discriminative language;discriminative language;approach discriminative language;log linear neural;language modeling;improve accuracy discriminative;language model;accuracy discriminative;language modeling problem;new approach discriminative;language model application;discriminative;linear neural networks;approach discriminative;log linear;use log linear;application log linear;neural networks;linear neural;neural networks used;use log;neural networks advantage;log;model application log;structure word easy;based use log;language;structure word", "pdf_keywords": ""}, "795aca47df94300fa6bfd464e6873aef56c7f3ae": {"ta_keywords": "extraction synsets semantic;multilingual semantic network;synsets semantic relations;synsets semantic;multilingual semantic;synsets form semantic;scale multilingual semantic;word senses synsets;semantic relations large;effective extraction synsets;extraction synsets;large scale multilingual;semantic relations;semantic network tool;multilingual version tool;semantic network;semantic relations established;multilingual;tool babelnet;senses synsets;release multilingual;relations established synsets;synsets architecture output;tool babelnet aimed;semantic;multilingual version;synsets architecture;present release multilingual;babelnet;babelnet aimed effective", "pdf_keywords": ""}, "4cd66273298128dfb5be290e891870085ecfc455": {"ta_keywords": "joint decoding algorithm;decoding algorithm joint;joint decoding;novel joint decoding;decoding algorithm;decoding;algorithm joint np;joint np np;joint np;algorithm joint;algorithm;np np np;np np;propose novel joint;novel joint;joint;np;novel;propose novel;propose", "pdf_keywords": ""}, "7099d5a4b2d4ed47905071fc23aff08580401e42": {"ta_keywords": "echo state network;networks rnns models;recurrent neural networks;neural networks rnns;networks rnns;speech recognition models;rnns models;speech recognition;automatic speech recognition;automatic speech;models inspired echo;inspired echo state;rnns;rnns models randomly;echo state;recurrent neural;propose automatic speech;subset recurrent neural;neural networks;esn subset recurrent;untrained esn based;inspired echo;state network esn;recognition models inspired;esn based models;initialized untrained esn;echo;subset recurrent;efficient training storage;recognition models", "pdf_keywords": "deep echo state;echo state neural;recurrent layer trained;recurrent network efficient;recurrent network;rnn layers encoder;recurrent neural networks;deep encoder;networks rnn;randomizing encoders training;neural network echo;randomizing encoders esn;networks rnn esn;neural networks rnn;encoder deep;deep echo;deep encoder deep;encoder deep decoder;underlying recurrent network;trained output encoder;combines deep encoder;decoder simple recurrent;deep decoder simple;generating deep neural;efficiently randomizing encoders;randomizing encoders;deep decoder;encoders training;network rnn proposed;simple recurrent layer"}, "af460a6b3ecaddd4015b34255564c366ecfef802": {"ta_keywords": "probabilities open problems;evaluation probabilities open;probabilities open;algorithm evaluation probabilities;open problems algorithm;fact probabilities open;evaluation probabilities;random variable algorithm;open problems evaluated;probabilities;closed algorithm;closed algorithm applicable;open problems;represented closed algorithm;problems evaluated random;problems algorithm based;algorithm generalization;evaluated random;problems algorithm;generalization algorithm classical;algorithm classical;algorithm classical computer;simple algorithm evaluation;algorithm generalization algorithm;generalization algorithm;problem represented closed;based fact probabilities;algorithm applicable wide;algorithm evaluation;algorithm", "pdf_keywords": ""}, "5861dbfcb253ca02067dd182d42b7d567433c834": {"ta_keywords": "confounders mediators observed;frontdoor estimators;confounders mediators;scenario confounders mediators;backdoor frontdoor estimators;frontdoor estimators evaluate;identified scenario confounders;confounders;scenario confounders;estimator leverages;estimator leverages observed;estimator dominate;optimal estimator leverages;estimators;estimators demonstrate;demonstrate estimator dominate;estimators valid derive;leverages observed variables;estimators valid;outperform backdoor frontdoor;variance estimators demonstrate;mediators observed;strictly outperform backdoor;outperform backdoor;estimators evaluate methods;estimator dominate unbounded;estimators demonstrate estimator;estimators evaluate;variance estimators;sample variance estimators", "pdf_keywords": "datasets causal inference;causal models backdoor;estimation causal models;estimators combine confounders;observation confounders mediators;observation confounders;detailed observation confounders;datasets causal;confounders mediators observed;combine confounders mediators;confounders mediators;causal inference fundamental;confounders mediators exhibit;estimation causal;backdoor effect causal;confounders mediators paper;confounders mediators introduce;optimal confounders quantified;causal inference;confounders mediators simultaneously;frontdoor estimators incorporating;confounders quantified;identify optimal confounders;backdoor estimators regression;frontdoor estimator cases;confounders optimal confounders;frontdoor estimators;scenario confounders mediators;optimal confounders;identified scenario confounders"}, "0f655f0e1937ad19b038952e2df69e30d447aac8": {"ta_keywords": "missing data clinical;missingness model trained;missingness model;disease time series;clinical time series;missing data;problem missing data;disease given data;missingness;data clinical time;concept missingness model;model recurrent;recurrent neural network;model recurrent neural;binary model recurrent;time series model;data clinical;disease time;recurrent neural;time series;novel statistical model;using concept missingness;concept missingness;statistical model;time series using;model problem missing;neural network;neural network model;clinical time;trained predict", "pdf_keywords": ""}, "4857e0e3d720b87b4523a6435cc166bcb7ae328a": {"ta_keywords": "neural networks generated;generation artificial neural;neural networks method;neural network method;networks resulting neural;neural networks resulting;generated single neural;artificial neural;artificial neural networks;resulting neural networks;neural network;single neural network;neural networks results;neural networks compared;networks generated single;neural networks;analysis neural networks;networks generated;number neural networks;networks generated large;generation artificial;networks method based;method generation artificial;analysis neural;networks method;number networks method;large number neural;single neural;based analysis neural;networks method applied", "pdf_keywords": ""}, "8ff620f704a4151fd7abba1db792463fbd32bfe5": {"ta_keywords": "pretrained abstractive summarization;short documents;short documents approach;compressing long documents;documents short documents;long documents short;documents short;abstractive summarization problem;abstractive summarization;salient sentences;identification salient sentences;salient sentences source;long documents;summarization problem;salience detection baseline;summarization;summarization problem observe;reduction salience detection;compressing long;method compressing long;salience detection;novel method compressing;sentences source achieved;documents approach;scores pretrained abstractive;method compressing;perplexity scores pretrained;salient;reduction salience;sentences", "pdf_keywords": "summarizing long legal;case summarizing sentences;novel summarization model;abstractive summarizers extract;summarizing long;summarizing sentences source;legal case summarizing;abstractive summarizers;summarizing sentences;extract summary sentences;summary sentences datasets;summarizers;summarizers extract;novel summarization;quality extracted sentences;abstracting abstractive summarizers;summarization model;case summarizing;summary document documents;propose novel summarization;summarization model based;summarizers extract summary;sentences datasets larger;setting summarizing long;summarizing;summarization;long legal briefs;extracted sentences;coherent summary sentences;complex legal documents"}, "baf34ac4080a365a7cec30b6877fa1a018eb31cf": {"ta_keywords": "joint passage retrieval;passage retrieval improves;passage retrieval;retrieval;novel autoregressive reranker;retrieval improves;retrieval improves prior;autoregressive reranker;multiple passages efficient;model joint passage;reranker state art;passages efficient;answer coverage datasets;passages efficient manner;autoregressive reranker state;modeling set passages;reranker;joint passage;answer coverage;passages model;set passages model;multiple passages;passages model able;better answer coverage;passage;passages;answer question multiple;combination novel autoregressive;joint modeling;improves prior approaches", "pdf_keywords": "multi answer retrieval;answer retrieval;passage retrieval model;retrieval multi answer;passage retrieval retrieval;answer retrieval underd;passage retrieval multi;joint passage retrieval;answer retrieval underexplored;passage retrieval improves;passage retrieval;passage retrieval using;passage retrieval problems;evaluation passage retrieval;passages answer generation;answer generation model;downstream question answering;passage retrieval number;autonomous passage retrieval;question answering;answer generation;quality answer generation;retrieval model joint;retrieval model;retrieval retrieval;retrieval recall;recall passages task;multi answer accuracy;retrieval number passages;multi answer datasets"}, "f4465442a9b850a2c5b71a63fff0d24396b15f2c": {"ta_keywords": "harmonic trap electron;trap electron gas;trap electron density;electron gas harmonic;electron gas confined;electron density quantized;trap electron;dimensional electron gas;quantized terms electron;dynamics dimensional electron;confined harmonic trap;electron gas;gas harmonic trap;gas confined harmonic;harmonic trap;electron interaction;electron electron interaction;dimensional electron;confined harmonic;density quantized;electron density;gas harmonic;electron interaction electron;quantized;interaction electron density;density quantized terms;electron electron;interaction electron;terms electron electron;electron", "pdf_keywords": ""}, "7fb1262d4484732c8f7295fa5fb5e6ed6eabb6a0": {"ta_keywords": "electricity market model;reserve electricity market;reserve electricity markets;electricity markets detailed;electricity market;electricity markets;electricity markets achieved;energy reserve electricity;reserve electricity;cooptimized energy reserve;optimized energy reserve;cooptimized energy;markets detailed transmission;simulation optimized energy;transmission representation methodology;relativistic cooptimized energy;energy reserve;optimized energy;performing optimized energy;transmission representation;electricity;fledged transmission representation;detailed transmission representation;transmission representation novel;fully relativistic cooptimized;markets achieved combining;market model proposed;relativistic cooptimized;markets detailed;market model", "pdf_keywords": ""}, "9f208842f70503e8b71fd4c34ba682dcd0ea4788": {"ta_keywords": "incentives strategic decision;adaptive design incentives;incentives noisy;incentives strategic;designing incentives change;incentives noisy noisy;design incentives noisy;designing incentives;simultaneously designing incentives;learn agents decisions;agents decisions simultaneously;strategic interactions agents;incentives change response;agents decisions;incentives change;decision makers agents;design incentives strategic;approach design incentives;incentives;design incentives;strategic decision makers;data strategic interactions;adaptive control;agents based data;adaptive control theory;learn agents;adaptive design;data strategic;adaptive;strategic interactions", "pdf_keywords": "incentive game utility;adaptive incentive design;learning incentive design;utility learning incentive;adaptive incentive;learning algorithm incentive;learning incentive;consider adaptive incentive;method adaptive incentive;incentive design game;incentive game;game utility learning;game incentive parameters;equation adaptive incentive;strategy incentive design;agents incentive game;algorithm incentive design;equilibria incentived game;incentive based models;incentive design nonlinear;incentived game game;incentive design;incentive design theorems;incentived game;given incentive design;algorithm incentive;strategy incentive;theorems incentive design;models incentive design;game incentive"}, "f136a0fdc2065485c83396ae41d431395de51af4": {"ta_keywords": "reviewers large conferences;identifying characterising reviewers;selection suitable reviewers;characterising reviewers large;candidate reviewers;characterising reviewers;icml reviewers selected;features candidate reviewers;icml reviewers;reviewers suited reviewed;meta review pipeline;reviewers selected;reviewers;suitable reviewers;large conferences approach;reviewers large;reviewers suited;suitable reviewers suited;number icml reviewers;candidate reviewers increase;large conferences;conferences approach;existing meta review;reviewed meta review;meta review;increase pool reviewers;reviewed meta;review pipeline;pool reviewers demonstrate;conferences approach combines", "pdf_keywords": "reviewers icml conference;recruit novice reviewers;reviewers large conferences;reviewer selection mentoring;reviewers pool qualified;novice reviewer selection;pool qualified reviewers;participants serve reviewers;qualified reviewers;evaluation reviewers recruited;reviewers team members;icml reviewer pool;recruited experiment reviewer;reviewer pool icml;novice reviewers population;icml 2020 reviewers;curated reviewers;novice reviewers;reviewers recruited;selection reviewers;reviewer pool;enhance reviewer pool;reviewer selection process;reviewers pool;curated reviewers present;reviewers having expertise;reviewer pool provide;selection reviewers proceedings;reviewers team;main reviewer pool"}, "b24e2c3983c3207b1c7124c48d691cf459a3197b": {"ta_keywords": "processing discrete models;learning discrete models;discrete models machine;use discrete models;discrete models;machine learning discrete;discrete models discrete;discrete models useful;models discrete equations;models discrete;information processing discrete;set discrete models;learning discrete;processing discrete;use discrete;discrete equations provide;models machine learning;discrete equations discrete;discrete equations;models;processing machine learning;guide use discrete;machine learning information;models machine;discrete;demonstrate use discrete;equations discrete equations;equations discrete;machine learning;learning information processing", "pdf_keywords": ""}, "0c0d9ecde0efead75e15353ac6c179c4fc22bdda": {"ta_keywords": "nonconvex strategy spaces;equilibria continuous games;local equilibria;local equilibria continuous;characterizing local equilibria;strategy spaces;guaranteeing differential equilibria;constitute local equilibria;local equilibria provide;strategy spaces provide;continuous games infinite;continuous games;equilibria structurally stable;nonconvex strategy;dimensional nonconvex strategy;equilibria isolated equilibria;differential equilibria;equilibria continuous;games infinite finite;differential equilibria isolated;strategies constitute local;equilibria provide sufficient;isolated equilibria;equilibria structurally;equilibria isolated;isolated equilibria structurally;finite dimensional nonconvex;equilibria;conditions ensuring strategies;equilibria provide", "pdf_keywords": "equilibria continuous games;continuous games finite;nash equilibria continuous;local nash equilibria;nash equilibria theorems;continuous games dynamics;differential nash equilibria;nash equilibria non_degenerate;equilibrium game characterized;nash equilibria isolated;convex strategy spaces;continuous games;equilibria class games;known continuous games;continuous games infinite;existence differential nash;nondegenerate differential game;dimensional strategy spaces;nash equilibria structurally;nash equilibria provide;strategy player equilibrium;conditions differential nash;games infinite dimensional;games finite;games finite infinite;nash equilibria;nondegenerate differential nash;player equilibrium game;strategy spaces;dynamics game characterized"}, "e8b026b36d8be73ed428f7e4e55c26b27c34a544": {"ta_keywords": "bifunctional electrochemical sensor;electrochemical sensors;measured electrochemical sensor;electrochemical sensor;electrochemical sensor determined;electrochemical sensors widely;target measured electrochemical;electrochemical sensor performance;measured electrochemical;determined electrochemical response;electrochemical response target;electrochemical response;bifunctional electrochemical;performance bifunctional electrochemical;measurement electroactive;determined electrochemical;devices determined electrochemical;electrochemical;simultaneous measurement electroactive;measurement electroactive non;electroactive responses target;crystal periodic potential;electroactive responses;molecule colloidal crystal;sensor performance bifunctional;periodic potential performance;measure strength macromolecule;macromolecule molecule colloidal;periodic potential;molecule colloidal", "pdf_keywords": ""}, "f20d7185c47ce55cdcd9b839ef6fce595baba029": {"ta_keywords": "xmath1 spectra xmath2;xmath0 xmath1 spectra;xmath1 spectra;xmath2 xmath3 states;xmath9 states;xmath3 states xmath4;xmath9 states particularly;xmath3 states;xmath4 state;spectra xmath2 xmath3;xmath8 states xmath9;xmath6 states strongly;xmath7 xmath8 states;xmath5 xmath6 states;xmath6 states;spectra xmath2;xmath8 states;lorentz force xmath0;states xmath4;xmath4 state xmath5;xmath2 xmath3;states xmath4 state;states xmath9;states xmath9 states;state xmath5;xmath2;xmath1;state xmath5 xmath6;xmath4;xmath9", "pdf_keywords": ""}, "3ebed41fa35e5902b692a3e380c7c9a035c04426": {"ta_keywords": "modern artificial intelligence;principles modern artificial;artificial intelligence strategy;modern artificial;artificial intelligence;intelligence strategy teach;teach principles modern;fiction tool teach;use science fiction;science fiction tool;machines slow creep;control systems image;machines;science fiction;advanced control;students use science;intelligence strategy;students use advanced;principles modern;strategy teach students;strategy teach;approach teach;tool teach principles;advanced control systems;use advanced control;using approach teach;techniques drive machines;systems image;fiction tool;control systems", "pdf_keywords": ""}, "6b02fe6e0f6b2120a08e098513511e15a05f9073": {"ta_keywords": "characterizing datasets shift;datasets shift quantifying;approach characterizing datasets;characterizing datasets;contamination data;level contamination data;datasets shift;quantify level contamination;contamination data demonstrate;contamination data able;identify shifts qualitatively;datasets;datasets able accurately;shifts qualitatively quantitatively;assess level contamination;qualitative behavior datasets;shift quantifying;level contamination;behavior datasets;datasets able;shifts qualitatively;data;quantifying shift;quantifying shift approach;data able identify;contamination;identify shifts;behavior datasets able;capture qualitative;shift quantifying shift", "pdf_keywords": "shift malignancy datasets;dimensional datasets identifying;datasets identifying shift;shifts high dimensional;classifiers dimensionality reduction;shift high dimensional;high dimensional datasets;malignancy datasets quantifying;datasets quantifying shift;domain discriminating classifier;malignancy datasets;dimensional datasets;classifiers dimensionality;shift detection multivariate;identifying shift malignancy;trained classifiers dimensionality;shifts dataset;high dimensional data;dimensionality reduction methods;classification thresholding;datasets identifying;data crucial detection;discriminating classifier;classification thresholding robustness;dimensionality reduction;dimensionality reduction performs;data shift detection;domain discriminating approaches;detection multivariate analyses;dimensional data"}, "3483d04a89dd69afd7b1393eadd8e8e4c5376d59": {"ta_keywords": "conceptual clustering algorithm;unsupervised conceptual clustering;conceptual clustering;clustering;clustering algorithm;use unsupervised conceptual;classification presented based;unsupervised conceptual;clustering algorithm algorithm;classification presented;classification;approach classification presented;new approach classification;approach classification;based use unsupervised;conceptual;use unsupervised;trees respectively approach;trees accuracy approach;algorithm designed;algorithm algorithm designed;unsupervised;algorithm;domains accuracy algorithm;algorithm tested comparing;coupled trees;algorithm algorithm;algorithm tested;comparing coupled trees;trees respectively", "pdf_keywords": ""}, "b1a8c6de4fbfe485c8f1c7723404467b72788ff2": {"ta_keywords": "machine learning treatment;machine learning past;machine learning talk;machine learning;learning treatment;drive machine learning;progress machine learning;learning treatment patients;medical treatment paradigm;patients;treatment paradigm highlight;patients context;learning;patients context modern;treatment patients;modern medical treatment;treatment paradigm;medical treatment;treatment patients context;success machine learning;modern medical;context modern medical;treatment;medical;recent progress machine;progress machine;machine;success machine;tremendous success machine;learning past", "pdf_keywords": ""}, "b6b76f529d273a35180d0dc65912db1538539067": {"ta_keywords": "classifying text metadata;text metadata approach;text metadata;framework classifying text;classifying text;embeds text semantic;text semantic space;text semantic;metadata;document synthesize training;metadata approach;metadata approach based;semantic space;document synthesize;semantic;propose framework classifying;semantic space original;framework classifying;classifying;process embeds text;original document synthesize;text;bottleneck label scarcity;embeds text;label scarcity;label;datasets;document;label scarcity experimental;datasets demonstrate", "pdf_keywords": "categorize text metadata;categorization text metadata;text metadata semantic;embed text metadata;metadata minimal supervision;metadata semantic;supervised categorization text;text metadata minimal;metadata semantic space;text metadata;categorize text;text metadata propose;text metadata task;labels various metadata;relationships text metadata;framework categorize text;information text metadata;documents useful metadata;metadata metadata;metadata;text classification;minimally supervised categorization;embeds corpus;metadata metadata metadata;various metadata;metadata minimal;model embedding corpus;embedding corpus;text metadata demonstrate;supervised framework categorize"}, "ac713aebdcc06f15f8ea61e1140bb360341fdf27": {"ta_keywords": "model bert adversary;language processing adversary;bert adversary;language model bert;bert adversary does;model extraction natural;processing adversary query;model bert;processing adversary;model extraction classification;model extraction;adversary victim model;model assuming adversary;pretrained language model;adversary query;extraction natural language;problem model extraction;classification api watermarking;extraction classification api;victim model attempts;language model;adversary query access;adversary does need;model attempts;adversary victim;adversary;access victim model;strategies model extraction;model attempts reconstruct;bert", "pdf_keywords": "extraction adversary using;extraction adversary;language processing adversary;model bert adversary;model extraction bert;extract attacker model;extracted model attacker;defenses extraction membership;detection model stealing;extraction bert;extraction bert bidirectional;simple defenses extraction;processing adversary query;defenses extraction;extract good models;model stealing attacks;good models adversary;queries pretraining attacker;extraction pretrained attacker;attacker makes extraction;bert adversary;models adversary;model output adversary;able extract models;language model bert;attacker use bert;models extract;extract models high;extract models;extraction classification api"}, "3671dabbfd2e854060e1e382bad96b6bb00fcb46": {"ta_keywords": "noise dictionary adaptation;supervised noise dictionary;noise dictionary;semi supervised noise;dictionary adaptation;dictionary adaptation approach;supervised noise;automatically adapts noise;adapts noise exemplars;noise keeping speech;noise exemplars target;noise exemplars;keeping speech exemplars;speech exemplars fixed;robust unknown noise;context speech recognition;novel semi supervised;exemplars target noise;speech recognition;speech exemplars;semi supervised;adaptation approach automatically;adapts noise;target noise keeping;noise keeping;speech recognition shown;target noise;adaptation approach;adaptation;context speech", "pdf_keywords": ""}, "6bea71fa6deb19c67e9586428f8f240e789fb3df": {"ta_keywords": "bandit linear stochastic;armed bandit algorithms;bandit algorithms;bandit algorithms using;stochastic optimality;multi armed bandit;analysis stochastic optimality;stochastic optimality condition;armed bandit linear;stochastic optimality conditions;bandit linear;2003 stochastic optimality;construction stochastic optimality;bandit;stochastic multi armed;modification stochastic optimality;armed bandit;optimality;improve stochastic;auer 2003 stochastic;linear stochastic multi;stochastic multi;optimality conditions;stochastic;optimality condition;improve stochastic multi;linear stochastic;analysis stochastic;optimality condition sfb;construction stochastic", "pdf_keywords": "stochastic bandit problems;stochastic bandit problem;stochastic bandit;study stochastic bandit;various stochastic bandit;bandit problem learning;bandits stochastic;bandit problem linear;bandits stochastic regression;bandit problem context;bandit problem;bandit problems;bandit problems new;number bandits stochastic;armed bandit problem;finite time bandits;bandits bounds;finite number bandits;confidence sets learning;time bandits bounds;bound regret theorems;bandits bounds sharp;bound regret theorems_;regret algorithm;model armed bandit;regret oful algorithm;bandit;multi armed bandit;number bandits;bandits"}, "2f0221142db900e75bd9c54fa153fb770a72f672": {"ta_keywords": "predicting shape filtered;shape filtered filtered;shape filtered;predicting shape;method predicting shape;filtered filtered filtered;filtered filtered;filtered;predicting;method predicting;powerful method predicting;shape;powerful method;simple powerful method;method;simple;powerful;simple powerful;present;present simple powerful;present simple", "pdf_keywords": ""}, "7dce2877758b0103d1f7a454c184dc641e123359": {"ta_keywords": "processing structured queries;document retrieval software;document retrieval;structured queries;retrieval software designed;retrieval software;structured queries context;document processing;context document retrieval;document processing available;queries context document;suite processing structured;scale document processing;processing structured;http solr org;solr org solr;queries;org solr;solr org;solr;software suite processing;retrieval;queries context;large scale document;suite processing;http solr;online http solr;structured;report presents performance;document", "pdf_keywords": ""}, "309b2c75dcdafea19a053876e56cef9747d428fb": {"ta_keywords": "attention model lattice;predicting position lattice;position lattice;lattice positional inputs;lattice positional;position lattice point;handle lattice positional;lattice model task;lattice point source;attention model;lattice point;translation task model;speech translation task;lattice model;lattice inputs;model lattice inputs;new lattice model;model based lattice;lattice structure;self attention model;lattice algorithms;lattice;lattice inputs important;task predicting position;based lattice;underlying lattice;based lattice structure;model lattice;lattice algorithms proposed;handle lattice", "pdf_keywords": "attention models lattice;attentional lattice encoders;lattice attentional encoders;attention lattice model;attention lattice inputs;attentional models lattice;model lattice attentional;lattice attentional;attention lattice;attentional lattice;self attention lattice;self attentional lattice;attentional encoders lattice;attention latticewe;dual attention lattice;neural lattice model;attention latticewe consider;neural lattice;lattice model sequential;sequential lattice;new neural lattice;sequential sequential lattice;attention sequence modeling;words attention latticewe;lattice model improves;lattice inputs;model lattice inputs;lattice model;training inference lattice;models lattice inputs"}, "975551547fef77605fb85a551bbd7523b77746b7": {"ta_keywords": "hierarchical classification structured;supervised hierarchical classification;hierarchical classification framework;hierarchical classification;existing hierarchical classification;hierarchical classification methods;underlying hierarchical classification;supervised hierarchical;topic label modeling;novel supervised hierarchical;framework supervised hierarchical;topic label;classification structured;classification structured data;supervised classification framework;supervised classification;idea topic label;features underlying hierarchical;classification framework;higitclass based idea;underlying supervised classification;called higitclass based;higitclass based;classification framework named;hierarchical;classification framework demonstrate;classification;propose novel supervised;underlying hierarchical;called higitclass", "pdf_keywords": "hierarchical classification github;embedding keyword enrichment;novel repository classification;hierarchical classification unstructured;classification github repositories;repositories unstructured data;information hierarchical classification;repository classification;classification github;classification repository classification;keyword enrichment;keyworddriven hierarchical classification;classification unstructured data;introduce keyword enrichment;repository classification task;hierarchical classification repository;repositories unstructured;repositories encode structured;keyword driven hierarchical;keyword enrichment module;classification structured data;hierarchical classification publicly;information hin embeddings;supervised hierarchical classification;classification unstructured;available repositories unstructured;classifying text;repository data mining;structured information hierarchical"}, "46ed42e4318e1363a0ec3dde195422cdfecf2017": {"ta_keywords": "quality phrase embeddings;phrase embeddings;phrase embeddings based;embeddings based empirical;improve quality phrase;embeddings;embeddings based;quality phrase;empirical;phrase;based empirical;improve quality;empirical value;based empirical value;quality;approach improve quality;novel approach improve;improve;novel approach;approach improve;propose novel approach;based;novel;propose novel;approach;value;propose", "pdf_keywords": "predicting phrase semantics;accurate phrase embeddings;phrase embedding model;powerful phrase embeddings;dense phrase representations;phrase embeddings;paraphrase generation model;phrase embedding;phrase representations;phrase embeddings approach;phrase embeddings derived;paraphrase generation;predicting phrase similarity;semiparametric phrase embeddings;paraphrases automatically generated;describing sentence representations;captures phrase semantics;diverse phrasal paraphrases;semiparametric phrase embedding;facilitate sentence embeddings;using paraphrase generation;phrase based neural;paraphrases automatically;phrasal paraphrases automatically;phrase representations method;predicting phrase;phrase embeddings coherent;sentence embeddings;diverse phrase level;sentence embeddings method"}, "d4af2654f97c09741aba9f0da9ace7bc84b9a63f": {"ta_keywords": "time probabilistic network;probabilistic network;probabilistic network ectbn;model paths poverty;poverty clients citylink;continuous time probabilistic;time probabilistic;paths poverty clients;event driven;event driven continuous;novel event driven;paths poverty;clients citylink;probabilistic;poverty clients;ectbn model paths;network ectbn representation;citylink;clients citylink center;model paths;ectbn representation model;event;model situations;driven continuous time;citylink center;ectbn model;applying model paths;use ectbn model;model situations influenced;social service provider", "pdf_keywords": ""}, "73a6e4574de038878be1bbb5985400998e420a5b": {"ta_keywords": "algorithms strategyproof assignment;strategyproof assignment assignment;assignment quality guarantees;strategyproofness compromise assignment;compromise assignment quality;assignment assignment quality;strategyproof assignment;assignment quality;algorithms strategyproof;assignment quality required;strategyproofness establish polynomial;strategyproofness compromise;analyze price strategyproofness;price strategyproofness;order strategyproofness;strategyproofness;price strategyproofness compromise;required order strategyproofness;assignment assignment;strategyproofness establish;order strategyproofness establish;time algorithms strategyproof;strategyproof;conference peer;conference peer review;polynomial time algorithms;assignment;algorithms;compromise assignment;quality guarantees", "pdf_keywords": ""}, "78438b61afc2c9123c28ca4d6b58e598462ae9be": {"ta_keywords": "source domain adaptation;domain adaptation;domain adaptation method;clustering embedded adversarial;adversarial training;adversarial training process;embedded adversarial training;optimization adversarial classifiers;min optimization adversarial;adversarial classifiers;optimization adversarial;embedded adversarial;adversarial;adversarial classifiers detect;adaptation method mda;dataset sentiment;target representations empirical;multi source domain;adaptation;dataset sentiment analysis;source domain;toy dataset sentiment;adaptation method;refine target representations;multi source;mda clustering embedded;classifiers;target representations;novel multi source;representations empirical results", "pdf_keywords": ""}, "1f0446dddd192e94f3930a3a449bd89796f4200f": {"ta_keywords": "transformation matrices feature;feature transformation matrix;multiple transformation matrices;estimate multiple transformation;linear regression fmllr;transformation matrices general;estimates multiple transformation;transformation matrices;transformation matrix frame;matrices feature space;regression fmllr framework;single feature transformation;transformation matrix;feature transformation;regression fmllr;multiple transformation;matrices feature;linear regression gmm;maximum likelihood linear;likelihood linear regression;regression gmm framework;general linear regression;regression gmm;linear regression;space maximum likelihood;fmllr framework proposed;matrices general linear;feature space maximum;fmllr framework;matrix frame", "pdf_keywords": ""}, "2226560f94c1e90d6900d4674b649cc5522b78cc": {"ta_keywords": "parameters multilingual model;multilingual model;sharing parameters multilingual;multilingual model different;parameters multilingual;translation accuracy adapting;multilingual;model different languages;improved translation accuracy;translation accuracy;different target languages;different languages;target languages;achieving improved translation;different languages propose;languages;improved translation;languages propose;languages propose methods;accuracy adapting model;translation;accuracy adapting;adapting model;sharing parameters;model accommodate different;adapting model accommodate;model different;adapting;accommodate different target;parameters", "pdf_keywords": "multilingual translation tasks;multilingual neural machine;translation models sharing;multilingual translation task;multilingual machine translation;neural machine translation;multilingual neural;parameters multilingual translation;languages sharing attention;separate translation models;translation models;translations task multilingual;attention parameters shareable;multilingual translation;learning translation;multilingual encoders;accuracy multilingual translation;machine translation target;multilingual parameter sharing;mtm multilingual translation;translation tasks model;sharing attention parameters;multilingual translation presence;perform multilingual translation;translation accuracy multilingual;learning translation rare;multilingual encoders decoders;self attentional encoder;translation models goal;attentional encoder decoder"}, "04b91791225a4f86b0715b41c6f56c00c197d810": {"ta_keywords": "encoded distributed storage;data encoded distributed;encoded distributed;storage systems equivalent;converting data encoded;systems equivalent codes;multidimensional code equivalent;conversion multidimensional code;data encoded;converting multidimensional code;distributed storage systems;distributed storage;equivalent kf code;code equivalent kf;equivalent codes method;multidimensional code;equivalent codes;encoded;storage systems based;storage systems;code equivalent;codes method based;codes method;kf code;based conversion multidimensional;systems based converting;kf code present;based converting multidimensional;conversion multidimensional;converting multidimensional", "pdf_keywords": "convertible codes bandwidth;codes conversion bandwidth;codes bandwidth optimally;codes bandwidth optimal;codes convert bandwidth;bandwidth convertible codes;distributed storage codes;codes distributed storage;coding distributed storage;codes bandwidth;storage codes design;bandwidth given code;initial codes bandwidth;code simultaneously bandwidth;conversion bandwidth optimal;optimally convertible code;bandwidth optimally conversion;bound conversion bandwidth;storage bandwidth requirement;bandwidth optimally convertible;optimal convertible codes;storage bandwidth;network storage bandwidth;minimum conversion bandwidth;network coding distributed;network coding scheme;regenerating codes distributed;bandwidth optimal convertible;convert bandwidth optimally;bandwidth used code"}, "43fe2d8781473360eeaae7a3284169a303200846": {"ta_keywords": "detection fake news;fake news challenge;stacked ensemble classifiers;accuracy detection fake;news using stacked;detection fake;using stacked ensemble;ensemble classifiers study;ensemble classifiers;stacked ensemble;ensemble method detection;news challenge;novel ensemble method;classifiers study conducted;classifiers;fake news proposed;ensemble method;news proposed method;news challenge aim;fake news using;classifier;classifiers study;method detection fake;2017 fake news;accuracy detection;use novel ensemble;ensemble;fake news;news using;novel ensemble", "pdf_keywords": ""}, "d9b89de5c2a39479768c6e32f13ac3e816635cc1": {"ta_keywords": "word lattices transcription;learning lexical translation;outperforming translation model;lexical translation parameters;translation model trained;directly word lattices;outperforming translation;representing translation model;transducer representing translation;translation model;additionally outperforming translation;translation model inference;word lattices;lattices transcription;lattices transcription sought;translation parameters directly;translation model consistent;lexical translation;model learning lexical;representing translation;translation parameters;learning lexical;transcription sought model;weighted finitestate transducer;finitestate transducer representing;transcription;finitestate transducer;lattice weighted finitestate;composed finitestate transducer;word error rate", "pdf_keywords": ""}, "44e24aabd05bef8cb45646486f1a24b7caecee45": {"ta_keywords": "multilingual seq model;language model seq2s;multilingual seq;prior multilingual seq;approach multilingual seq;low resource speech;language model;seq model training;improving prior multilingual;network language model;neural network language;seq2seq model;model seq2seq model;resource speech research;model seq2seq;recurrent neural network;resource speech;seq2seq model use;model seq2s;multilingual;novel approach multilingual;approach multilingual;prior multilingual;seq model;speech research;seq2seq;10 different languages;languages;seq2s;different languages", "pdf_keywords": "speech recognition attention;language model rnn;trained attention decoder;attention decoder model;encoder trained attention;attention function encoder;multilingual model encoder;rnn hmm systems;multilingual seq2seq model;joint attention decoder;training speech recognition;encoder ctc attention;ctc attention decoder;attention decoder;attention decoder joint;multilingual seq2seq;seq2seq model recurrent;hybrid rnn hmm;neural network speech;integrating multilingual seq2seq;neural network language;decoder trained seq2seq;recurrent neural network;rnn mmm decoding;language model transfer;multilingual model;performance hybrid rnn;attention based learning;languages multilingual model;speech recognition"}, "8b468872cf915c98ff46a2bea4d2a34112b7b0b0": {"ta_keywords": "entity extraction;named entity extraction;entity extraction parsed;knowledge base inference;relational rules;graph based knowledge;model relational rules;knowledge base;based knowledge base;relational rules class;extraction parsed text;base inference person;person named entity;extraction parsed;inference person named;model relational;base inference;inference person;named entity;graph based;relational;based knowledge;parsed text;rules constants;discover features search;order rules;rules constants use;entity;model discover features;order rules constants", "pdf_keywords": ""}, "d8704a63517868475b3af7ec25eaa2fb2a44362b": {"ta_keywords": "loss shallow segmentation;shallow segmentation;shallow segmentation based;regularized loss shallow;stiff refractive index;index stiff refractive;refractive index stiff;stiff refractive;segmentation;approach regularized loss;known stiff refractive;combination stiff refractive;regularized loss;segmentation based known;refractive index;refractive index model;segmentation based;loss shallow;novel approach regularized;approach regularized;regularized;refractive;index stiff;shallow;stiff;loss;index model approach;based known stiff;model combination stiff;index model", "pdf_keywords": "cnn segmentation training;cnn segmentation alternative;weakly supervised cnn;deep cnn segmentation;regularized segmentation grid;performance regularized segmentation;supervised cnn segmentation;regularized segmentation;cnn segmentation propose;cnn segmentation;regularized losses network;deep cnn;segmentation training quality;compared deep cnn;shallow image segmentation;losses network training;segmentation training;regularization loss;regularized losses shallow;loss dense network;regularization loss combine;regularized loss;regularized loss approach;regularized loss dense;evaluation regularized loss;supervised cnn;regularized loss function;novel regularized loss;losses shallow image;regularized losses"}, "6dd6d4dfc3cf9ff41aad7e903cf1294de2ac5629": {"ta_keywords": "parameterize scenarios extracted;deceleration scenarios extracted;parameterize scenarios;scenarios extracted real;scenarios extracted;world traffic data;foreseeable parameter ranges;traffic data analyze;traffic data collected;real world traffic;method parameterize scenarios;collected japanese highways;parameter ranges contextualization;japanese highways;japanese highways sakura;traffic data;scenarios extracted new;highways sakura initiative;highways sakura;world traffic;parameter ranges;ranges reasonable risk;set traffic data;highways;deceleration scenarios;scenarios;reasonably foreseeable parameter;risk acceptance thresholds;traffic;resulting ranges reasonable", "pdf_keywords": ""}, "1288d6570085a28518a9f3495e77dbb75899421c": {"ta_keywords": "entity recognition biomedical;unknowns entity recognition;entity recognition;named entity recognition;active annotation;human annotators framework;based active annotation;active annotation method;recognition biomedical domain;imperfectly annotate data;estimating unknowns entity;human annotators;corrected human annotators;annotate data;annotators framework;biomedical domain framework;recognition biomedical;unsupervised learning framework;annotators;annotators framework applied;annotation;annotation method;unknowns entity;imperfectly annotate;unsupervised learning;annotate;annotate data errors;introduces unsupervised learning;initially imperfectly annotate;biomedical domain", "pdf_keywords": ""}, "163a67b5b0371035fa6e0f88b36ba97a32e735bc": {"ta_keywords": "data encoded erasure;encoded erasure code;encoded erasure;erasure code data;data encoded nf;code data encoded;data encoded ni;encoded nf;data encoded;conversion data encoded;converting data encoded;erasure code;encoded nf kf;ki code data;kf code maintaining;encoded ni;code conversion process;encoded ni ki;code conversion;code data;ki code process;encoded;nf kf code;storage systems present;storage systems;kf code;efficiency distributed storage;distributed storage;notion code conversion;distributed storage systems", "pdf_keywords": "distributed storage code;codes distributed storage;coded storage systems;erasure coded storage;coded storage;erasure codes distributed;storage code;code conversion storage;storage regenerating codes;optimal erasure codes;storage code method;storage dfs code;recoverable codes coding;codes provide durability;erasure codes provide;erasure codes;storage systems consider;distributed storage regenerating;erasure code dfs;recoverable erasure code;codes coding theory;locally recoverable codes;distributed storage;failure rate storage;storage systems;distributed storage systems;sd digital codes;storage systems present;reduction storage space;use erasure codes"}, "027c5e44164a2ee3543ecdff73cd4d7888a42a90": {"ta_keywords": "representing preferences large;representing preferences;selection networks post;selection networks recently;post selection networks;selection networks;method representing preferences;metric representation preferences;networks post selection;preferences class machine;machine learning systems;preferences large class;representation preferences class;preferences class;class machine learning;preferences;representation preferences;selection;machine learning;preferences large;post selection;learning systems;learning systems called;networks;called post selection;networks recently proposed;networks post;learning systems use;class machine;recently proposed metric", "pdf_keywords": ""}, "c17ccb7f0372ec98b7e070b0f70518f28516ecd5": {"ta_keywords": "stochastic processes theory;introduction stochastic processes;stochastic processes;paper introduction stochastic;introduction stochastic;stochastic;processes theory;processes theory based;khoze protsenko xcite;khoze xcite paper;protsenko xcite paper;processes;khoze xcite;xcite paper protsenko;protsenko xcite;paper khoze protsenko;paper khoze xcite;based paper khoze;paper protsenko xcite;khoze protsenko;xcite paper;paper introduction;xcite paper based;khoze;theory based paper;paper khoze;xcite;paper protsenko;protsenko;introduction", "pdf_keywords": "magneticthe dynamics xmath0he;motion particle magnetic;analysis dynamics particles;dynamics particlesthe problem;magnetic field dynamics;wave dynamics spherically;interacting particles magnetic;particles magnetic field;particlesthe problem dynamical;dynamics magnetic field;medium dynamics particlesthe;classicalthe dynamics magnetic;particles dynamics;dynamics presence magneticthe;field dynamics xmath0he;field studied magnetic;dynamics particle studied;thethe dynamics magnetic;particle magnetic field;dynamics particles dynamics;studied magnetic field;particles medium dynamics;particles magnetic;dynamics dynamics particles;particle problem wave;study dynamics magnetic;medium dynamics particle;solution dynamics magnetic;wave dynamics approachthe;problem interaction magnetic"}, "23e03cd57b5d75993545127f3fecf99d25021583": {"ta_keywords": "predicting multiple cancer;predict multiple cancer;profiled cancer genome;cancer genome atlas;multiple cancer phenotypes;cancer phenotypes based;cancer genome;cancer phenotypes;cancer phenotypes apply;tumors profiled cancer;identify predict multiple;multiple cancer;gene used predict;gene embedding based;profiled cancer;gene embedding;genome atlas;468 tumors profiled;predict multiple;impact somatic genomic;vector gene embedding;somatic genomic alterations;used predict multiple;model predicting multiple;impact associated gene;tumors profiled;somatic genomic;predicting multiple;learns vector gene;model 468 tumors", "pdf_keywords": "learning gene embeddings;gene embeddings;gene embedding;gene embeddings precisely;embeddings tumor embeddings;gene embeddings tumor;vector gene embedding;predicting cancer phenotypes;patient tumor embeddings;embeddings able predict;tumor embeddings mapping;tumor embeddings;gene embedding abstract;tumor encodings based;tumor embeddings reveal;tumor encodings;learning represent bioinformatics;embed genes;loss tumor encodings;gene embeddings able;similar gene embeddings;genes tumors model;tumors model learns;representation genes;represented sequence genes;predicting cancer;based representation genes;genes continuous embedding;learns vector gene;learning gene"}, "1d3539a8d94bd3ab78993d7cc584efc06ed0e460": {"ta_keywords": "benchmarking feature attribution;feature attribution algorithms;feature attribution;attribution algorithms;attribution algorithms context;benchmarking feature;benchmarking popular explainability;powerful synthetic dataset;synthetic dataset library;attribution;datasets demonstrate;synthetic dataset;library benchmarking popular;datasets demonstrate power;machine learning;realworld datasets demonstrate;library benchmarking;synthetic platform benchmarking;benchmarking popular;platform benchmarking feature;datasets;realworld datasets;dataset library;benchmarking;dataset library ability;dataset;power library benchmarking;popular explainability techniques;context machine learning;values realworld datasets", "pdf_keywords": "benchmarking explainability algorithms;benchmarking explainability;explainability algorithms synthetic;explainability methods benchmarked;benchmarking popular explainability;library benchmarking explainability;benchmarking explainers;benchmarking popular explainers;benchmarking explainers quantitative;fair benchmarking explainers;benchmarking feature attribution;realistic synthetic datasets;algorithms synthetic datasets;synthetic datasets;explainability algorithms;explainability algorithms development;synthetic datasets library;datasets synthetic datasets;synthetic dataset library;dataset faithfulness explainers;practitioners explainability algorithms;synthetic datasets allow;library allows explainability;suite synthetic datasets;synthetic dataset;synthetic datasets offer;datasets synthetic;metrics synthetic datasets;library synthetic feature;synthetic datasets release"}, "8a14b3a9e642f4ca7fad4df997fc1941bdcfb935": {"ta_keywords": "entangled states based;entangled states state;entangled states;set entangled states;number entangled states;entangled;large number entangled;set entangled;number entangled;terms set entangled;representation state state;states based concept;representation state;description representation state;states state art;state art expressed;state art mathematical;state art method;state state art;states based;concept state;representation;mathematical description representation;based concept state;description representation;state art;generate;states;states state;concept state art", "pdf_keywords": ""}, "ffecb8b8b415149f5351b64a2dbb1a1fa64219f0": {"ta_keywords": "end speech translation;speech translation;speech translation st;utterances source languages;multilingual end;multilingual end end;translation st speech;languages directly translated;desired target languages;translations model;translations model outperforms;model outperforms multilingual;multilingual ones;target languages;framework multilingual end;multilingual;target languages universal;outperforms multilingual ones;outperforms multilingual;languages universal sequence;source languages directly;end end speech;translations;speech utterances source;source languages;end speech;languages directly;translated desired target;st speech utterances;directly translated desired", "pdf_keywords": ""}, "9c78481004b7dbb601b83cc081ec23c02e6f5270": {"ta_keywords": "stability scalar games;equilibria scalar games;scalar game dynamics;scalar games;model continuous games;game equilibria stable;scalar game;scalar games provide;stability equilibria scalar;continuous games;scalar games context;variations scalar game;corresponding game equilibria;equilibria corresponding game;equilibria stable robust;stable equilibria corresponding;stable equilibria;continuous games provide;stability scalar;game dynamics;equilibria stable;equilibria scalar;game equilibria;exponentially stable equilibria;guarantees stability;study stability scalar;guarantees stability set;stability equilibria;understanding stability equilibria;formal guarantees stability", "pdf_keywords": "continuous games equilibria;equilibria continuous games;equilibria stable nash;stability class games;stability nash optimality;stability nash;nash equilibrium stable;stable non nash;stability game dynamics;equilibria scalar games;stable nash;characterize stability nash;stable nash robust;game dynamics stability;scalar games equilibria;discrete time games;games equilibria stable;game dynamics;game theory class;stability game;nash equilibrium;game theory;learning dynamics;continuous games;player continuous games;scalar games continuity;nash robust;class scalar games;games equilibria;dynamics class games"}, "8b231737e0048a400527d89aa56c712e8b9bc690": {"ta_keywords": "end speech translation;speech translation;utterances source languages;speech translation tt;translation tt speech;high accuracy translations;multilingual end;multilingual end end;languages directly translated;accuracy translations publicly;desired target languages;accuracy translations;speech utterances source;source languages directly;framework multilingual end;tt speech utterances;multilingual;target languages universal;translations publicly;target languages;end end speech;utterances source;end speech;translation tt;languages universal sequence;translations publicly available;source languages;translations;languages directly;speech utterances", "pdf_keywords": "end speech translation;multilingual speech translation;training speech translation;speech translation task;pipeline speech translation;sequence architecture multilingual;model bilingual end;multilingual end end;speech translation based;multilingual end;speech translation universal;speech translation approach;speech translation code;neural machine translation;sequence model bilingual;multilingual training speech;speech translation asr;translation asr endto;multilingual speech;multilingual models trained;bilingual end end;speech translation;speech translation exploits;machine translation st;speech translation e2e;extend multilingual encoder;asr machine translation;speech translation st;approach multilingual speech;multilingual encoder"}, "da20ab7724335eb48bcd0e9be30f0ac4b6a464c6": {"ta_keywords": "novel scenarios vision;scenarios vision based;detection novel scenarios;scenarios vision;real world driving;trained prediction;learned trained prediction;trained prediction model;driving dataset;world driving dataset;vision based autonomous;indoor racing;driving dataset house;prediction model predict;prediction;indoor racing environment;house indoor racing;novel scenarios;predict;model predict;world driving;autonomous systems;prediction model demonstrate;racing;scenarios;racing environment;vision based;ability trained prediction;prediction model;autonomous", "pdf_keywords": "predict features image;training image reconstruction;utilizes convolutional similarity;training image;convolutional similarity;image learned model;trained predict steering;novel scenarios vision;trained predict;trained prediction;classify novel images;new image similarity;training images;train image;network cnn;image similarity metric;classifier learn representation;image learned;real world driving;learned trained prediction;features trained;learning visual;input image learned;convolutional loss;trained neural;image reconstruction networks;features trained neural;model trained;networks trained predict;deep neural"}, "e54e0d9eaa922cefb1c69e105979399fd34497b1": {"ta_keywords": "fairness aware learning;problem fairness aware;fairness aware;applicable fairness aware;readily applicable fairness;abel assignment fair;applicable fairness;problem fairness;assignment fair;assignment fair pg;fairness;consider problem fairness;group labels propose;fair pg strategy;annotated group labels;pseudo labeling strategy;partially annotated group;labeling strategy benchmark;labeling strategy;annotated group;learning partially annotated;group labels;vanilla pseudo labeling;labels propose simple;pseudo labeling;labeling;labels propose;labels;aware learning;learning partially", "pdf_keywords": "fairness aware learning;fairness partially annotated;fairness aware classifiers;labels optimize fairness;algorithmic group fairness;fair training methods;fair training method;group fairness partially;group labels fair;fairness method group;fairness methods fair;fairness constraints group;fairness aware information;achieve group fairness;neural network fairness;group fairness;optimize fairness constraints;propose fairness method;fairness methods;train fairness aware;fair training fair;fairness loss group;optimize fairness;learning process fairness;apply fairness methods;compute fairness;applicable fairness aware;fair representations data;fairness aware;group fairness perform"}, "dcb28c8ba94434eb8a06e81eb55bfdbc343d2340": {"ta_keywords": "extraction ary relations;learning multiple representations;relations high dimensional;method joint extraction;ary relations;ary relations high;labels distant supervision;relations;relations high;joint extraction ary;joint extraction;representations novel approach;high dimensional document;dimensional document approach;multiple representations;learning multiple;multiple representations novel;weak signals document;document integrating weak;distant supervision approach;representations;extraction ary;document approach combines;representations novel;signals document integrating;document approach;dimensional document;labels distant;extraction;document", "pdf_keywords": "relation extraction high;relation extraction;relation extraction combines;ary relation extraction;relation extraction methods;relation extraction expanding;document level extraction;mention level representations;extraction combines mention;representations learned text;learned text spans;relation document;organize subrelations mention;biomedical text corpus;unstructured text;relations noisy data;text spans subrelation;discovery relation document;sub relations;entity level representation;learned subrelation hierarchy;level sub relations;document restricting relation;neural architecture document;text corpus;combines mention level;learning combines representations;sub relations ary;hierarchy text spans;representations entity level"}, "900b785dbbea7ccd5846eafb14c6715f76fe5e00": {"ta_keywords": "devices malicious content;dynamics malicious content;protecting devices malicious;malicious content security;analyzing dynamics malicious;devices malicious;malicious content leverages;malicious content;dynamics malicious;security mobile devices;malware;content security mobile;security mobile;malware present;malware present novel;greater presence malware;malicious;presence malware;presence malware present;content security;devices described backpropagation;protecting devices;risks associated backpropagations;described backpropagation;network traffic;challenge protecting devices;mobile devices described;mobile devices;described backpropagation exchanges;network traffic risks", "pdf_keywords": ""}, "2a81081c987da2bb8184b8e9a884cf6a73712ee8": {"ta_keywords": "zeeman field stability;field stability xmath0he;stability xmath0he ep;xmath0he ep cluster;stability xmath0he;zeeman field strength;strength stability xmath0he;field stability;effect zeeman field;field strength stability;effect field stability;zeeman field;xmath0he ep;ep cluster framework;cluster framework standard;ep cluster determined;ep cluster;ep cluster minimized;cluster determined interplay;cluster determined;cluster framework;choice zeeman field;stability;effect zeeman;cluster;xmath0he;cluster minimized;strength stability;interplay field strength;cluster minimized suitable", "pdf_keywords": ""}, "126be977c03d732fbef2381565a41b957d41a2cc": {"ta_keywords": "narrative modeling;narrative modeling proposed;characterizing discourse language;characterizing discourse;representations text interpretable;problem characterizing discourse;book narrative modeling;natural language processing;discourse language;nlp problems;natural language;capture linguistic structure;discourse language approach;question answering fictional;models capture linguistic;processing nlp;language processing nlp;understanding book narrative;novel representations text;representations text;nlp problems question;discourse;processing nlp problems;answering fictional relationship;methods natural language;build novel representations;question answering;nlp;book narrative;capture linguistic", "pdf_keywords": ""}, "aae8d332c3ff9d081ae36967d3d7b5f394b51bcc": {"ta_keywords": "teacher stackelberg game;classification named entity;supervised;entity recognition;self training differentiable;weakly supervised;entity recognition model;pseudo labels follower;semiand weakly supervised;named entity recognition;stackelberg game follower;weakly supervised classification;method self training;labels follower;labels follower winner;supervised classification named;supervised classification;training differentiable teacher;stackelberg game;teacher stackelberg;classification;student pseudo labels;treats teacher stackelberg;self training;training differentiable;pseudo labels;differentiable teacher;named entity;differentiable teacher treats;classification named", "pdf_keywords": "self training stackelberg;self training framework;self training method;self training stackelbergwe;self training supervised;self training approach;method self training;differentiable selftraining framework;task self training;differentiable self training;self training teacher;training framework follower;selftraining framework;teacher training framework;framework self training;self training promising;training stackelberg game;student model trained;learning method learn;training stackelberg;selftraining framework formulates;training framework;learning framework;training supervised learning;training stackelbergwe propose;self training based;training method;propose differentiable selftraining;perform supervised learning;differentiable selftraining"}, "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f": {"ta_keywords": "distributed neural networks;parallelizing large scale;mesh tensorflow;large scale models;mesh tensorflow implement;distributed neural;mesh based distributed;called mesh tensorflow;parallel version vertex;data parallelizing large;parallelizing large;train large models;tensorflow;large models;based distributed neural;parallelizing;power distributed neural;data parallelizing;tensorflow implement;vertex splitting algorithm;vertex splitting;data parallel;large models use;tensorflow implement data;implement data parallel;version vertex splitting;representation large scale;data parallel version;parallel version;vertex", "pdf_keywords": "distributed tensor computations;tensor based distributed;based distributed tensor;tensor based machine;parallel use tensors;tensorflow;distributed tensor;tensorflow supports;tensorflow framework;tensorflow supports broad;distributed tensor compression;tensorflow framework based;library mesh tensorflow;tensor computations called;mesh tensorflow framework;mesh tensorflow;tensor based implementation;mesh tensorflow supports;tensors implemented python;tensor computations;class distributed tensor;implementation tensor based;tensors implemented;tensor based library;called mesh tensorflow;processing parallelization powerfulwe;model data parallelism;based parallelization batch;parallel computing framework;parallelization allreduces"}, "e2bd274c8dd2a3b2a0a6f5d8a29baee07df34eb9": {"ta_keywords": "t2s machine translation;machine translation systems;translation systems;translation systems investigating;accuracy translation process;machine translation;translation process improved;accuracy translation;tree string t2s;methods accuracy translation;translation process;string t2s machine;translation process basic;accuracy tree string;improved parsing;process improved parsing;string t2s;phrase based systems;affect accuracy translation;parsing;par phrase based;tree string;t2s machine;accuracy tree;investigate accuracy tree;t2s;performs par phrase;tree;translation;par phrase", "pdf_keywords": ""}, "98b7d5611c0a128f45db100cc796b981573adcc5": {"ta_keywords": "harmonic oscillators asymptotic;oscillators asymptotic behaviour;oscillators asymptotic;asymptotic behaviour spectral;asymptotic behavior spectral;coupled harmonic oscillators;harmonic oscillators;closed coupled harmonic;cycles asymptotic;cycles asymptotic behavior;harmonic oscillators determined;spectral function closed;number cycles asymptotic;coupled harmonic;asymptotic behaviour;spectral function;asymptotic behavior;oscillators determined;oscillators;behaviour spectral function;function closed coupled;spectral function determined;problem asymptotic behaviour;behavior spectral function;oscillators determined number;behaviour spectral;spectral;closed coupled;asymptotic;behavior spectral", "pdf_keywords": ""}, "bcd4c46e4d75ddedb6138cfd77600c6d964a9aa8": {"ta_keywords": "code translation;pretrained code models;code models natural;natural language code;language code translation;natural language tocode;bayes risk decoding;risk decoding;language tocode;language tocode tasks;pretrained code;code models;language code;language code based;program selection improves;performance pretrained code;code translation key;translate natural language;risk decoding mbrexec;program selection;code based machine;models natural language;code based;tocode tasks;algorithms aim translate;code;based minimum bayes;natural language;minimum bayes risk;machine learning", "pdf_keywords": "pretrained code models;pretrained language code;code translation challenging;language code models;decoding program generation;code natural language;pretrained language models;generation code pretrained;code translation;language code translation;algorithm pretrained language;code models natural;natural language code;risk decoding program;natural language tocode;language tocode tasks;large pretrained language;language tocode;pretrained large language;pretrained code;improve program generation;program outperforms semantic;performance pretrained code;code models;code pretrained large;code pretrained;code datasets;text code datasets;language code;program generation code"}, "8688169ad5701e726968e293ff7dc53d76dd8007": {"ta_keywords": "electron gas magnetic;2d electron gas;electron density magnetic;gas magnetic field;2d electron;dimensional 2d electron;gas magnetic;electron gas;magnetic properties;magnetic properties dimensional;control electron density;field magnetic properties;density magnetic field;density magnetic;electron density;density presence magnetic;electron density presence;effect magnetic field;magnetic field effect;magnetic field;magnetic field magnetic;field effect magnetic;effect magnetic;control electron;magnetic field used;magnetic field strength;electron;field magnetic;properties dimensional 2d;study effect magnetic", "pdf_keywords": ""}, "4f78624defde3b60551cfeb37e3943b267ea704a": {"ta_keywords": "distributed learning;distributed learning method;method distributed learning;distributed learning problem;novel distributed learning;large machine learning;learning method large;based compression gradient;compression gradient;block quantization;compression gradient differences;analysis block quantization;ell_infty quantization;quantization;ell_2 ell_infty quantization;quantization closes;method strongly convex;machine learning problems;ell_infty quantization closes;propose novel distributed;training large machine;learning models analysis;distributed;novel distributed;method distributed;block quantization differences;quantization differences ell_2;learning method;strongly convex nonconvex;based compression", "pdf_keywords": "distributed learning;distributed optimization;dual distributed optimization;distributed learning problem;distributed optimization large;distributed learning method;learning method distributed;method distributed optimization;novel distributed learning;technique distributed learning;distributed optimization framework;strongly convex minimization;convex learning;learning model regularized;convex learning algorithm;distributed optimal;estimation distributed primal;distributed primal dual;closed convex regularizer;parameters convex learning;convex regularizer;novel distributed primal;propose distributed approximation;distributed approximation;distributed approximation method;convex minimization;convex regularizer diana;model regularized empirical;nonconvex minimization arbitrary;convex regularizer apply"}, "dbdefb498b619912a726fec7c85533594a1c6a1b": {"ta_keywords": "minimax optimization minx;optimization minx maxy;minimax optimization;minx maxy challenging;consider minimax optimization;optimization minx;maximization algorithm guaranteed;consider minimax;adversary max player;player instead maximization;minimax;adversary max;maxy challenging setting;minx maxy;framework min player;maxy challenging;maximization algorithm;play smooth algorithms;instead maximization algorithm;maximization;optimization;min player play;instead maximization;deployed adversary max;nonconvex nonconcave propose;min player;max player;max player instead;nonconcave propose;new framework min", "pdf_keywords": "nonconcave minimax optimization;nonconcave games minimizes;minimax optimization;nonconcave minimax;minimax optimization problems;nonconvex nonconcave minimax;minimax optimization minx;deterministic adversarial algorithms;consider minimax optimization;gans adversarially robust;gans adversarially;adversarial training problems;proximal optimization;nonconvex nonconcave games;considers minimax optimization;networks gans adversarially;deterministic subgradients optimization;stochastic gradient descent;adversarially robust;ascent inner optimization;minimax optimization problem;adversarial algorithms;pseudocode proximal optimization;nonconcave nonconcave maximization;adversarially robust models;gradient descent;gradient descent sgd;finding gans;nonconcave maximization;nonconcave maximization arises"}, "a3ce3004a0eade48a3ae652dbf5c04a60c2416aa": {"ta_keywords": "personalized dialogue generation;traits dialogue generation;traits personalized dialogue;dialogue generation;dialogue generation propose;dialogue generation process;personalized dialogue;personality traits dialogue;personality traits sequence;traits dialogue;dialogue;personality traits personalized;sequence sequence learning;incorporating personality traits;sequence learning framework;sequence learning;incorporate personality traits;problem incorporating personality;personality traits;personality;incorporating personality;explicit personality traits;traits personalized;approach incorporate personality;traits sequence sequence;traits sequence;incorporate personality;incorporates explicit personality;explicit personality;sequence sequence", "pdf_keywords": "personalized dialogue generation;generate personalized dialogue;traits dialogue generation;traits conversation generation;aware dialogue generation;dialogue generation models;personalized dialogue responses;personalized dialogue model;driven personalized dialogue;dialogue generation model;dialogue generation;generate dialogue responses;personalized dialogue;conversation generation;dialogue dataset personal;dialogue generation proposed;traits dialogue conversations;persona aware dialogue;personality traits conversation;propose personalized dialogue;dialogue generation process;conversation generation construct;study personalized dialogue;dataset personal dialogue;generate dialogue;dialogue set traits;dialogue generation end;present personalized dialogue;dialogue models;personal dialogue based"}, "54a13bcc9613dcaa76fb25fbe96572f376cfcca9": {"ta_keywords": "thedesire learning matrix;adesire learning scheme;adesire learning;propose adesire learning;learning matrix scaling;learning matrix;thedesire learning;publisheddesire learning;learning scheme;moments thedesire learning;results publisheddesire learning;publisheddesire learning regime;learning scheme uses;training lnlap model;decay rate scheme;learning;learning regime;gradually increasing decay;matrix scaling sums;lnlap model;increasing decay rate;matrix scaling;decay rate;training lnlap;increasing decay;decay;learning regime training;regime training lnlap;lnlap;adesire", "pdf_keywords": "learning stochastic optimization;novel stochastic optimization;training deep neural;learning stochastic;training deep;deep neural;generalize learning;optimization method training;able generalize learning;method training deep;optimized stochastically;stochastic optimization;gradients machine translation;stochastic learning;deep neural networks;gradients machine;method learning stochastic;square root gradient;squared past gradients;objective function gradient;approximation gradient constructed;adaptive learning;stochastic optimization able;approximation gradient;gradients input data;learning rate sublinear;gradient constructed matrix;stochastically sampled learning;learning rate;stochastic optimization method"}, "957e3ec3c722f5cb382fe8ac54fc846ee772a95f": {"ta_keywords": "adversarial multiarmed bandit;multiarmed bandit problem;multiarmed bandit;peer game adversarial;bandit problem algorithm;bandit problem;game adversarial multiarmed;bandit;regret bounds improving;new regret bounds;adversarial multiarmed;regret bounds;game adversarial;peer peer game;peer game;algorithm online peer;adversarial;online peer;online peer peer;peer peer;bounds improving;novel algorithm online;peer;algorithm online;bounds improving previous;exploration parameter space;new regret;regret;achieves new regret;exploration parameter", "pdf_keywords": "stochastic bandit learning;bound stochastic banditwe;stochastic bandit algorithms;semi bandit feedback;bandit learning;stochastic adversarial bandits;stochastic bandit;stochastic adversarial banditswe;bandit algorithms;stochastic banditwe propose;stochastic bandit problem;problem stochastic bandit;study stochastic bandit;bandit learning particular;adaptive regret bound;class stochastic bandit;bandit feedback learner;stochastic bandits bounded;adversarial bandits;bandit algorithms simultaneously;information feedback bandit;bandit problem partial;adversarial bandits generalizes;adversarial bandits provide;bandit feedback;stochastic banditwe;bandit problem semi;semi bandit problem;adversarial banditswe;combinatorial semi bandit"}, "c4ce6aca9aed41d57d588674484932e0c2cd3547": {"ta_keywords": "natural language scientific;papers approach annotate;annotate content scientific;problem extracting mechanisms;mechanisms natural language;scientific literature knowledge;extracting mechanisms;extracting mechanisms natural;knowledge base search;literature knowledge base;content scientific literature;mechanisms use knowledge;scientific papers approach;scientific literature;language scientific papers;approach annotate content;finds relevant information;knowledge base;use knowledge base;search relevant information;scientific papers;annotate content;natural language;information mechanisms literature;approach annotate;language scientific;relevant information mechanisms;information literature provide;mechanisms literature;search engine finds", "pdf_keywords": "mechanism relations corpus;semantic information extracted;described biomedical ontologies;extract mechanism relations;relations corpus scientific;texts mechanisms extracted;mechanisms unstructured text;scientific texts mechanisms;extract annotations relationships;biomedical ontologies introduce;biomedical ontologies;mechanisms natural language;mechanisms based schema;nrf ontologies knowledge;semantically structured;information extracted scientific;usual nrf ontologies;natural language scientific;extracting mechanisms unstructured;novel semanticwe propose;annotations unstructured;framework extracting mechanisms;relations corpus;schema mechanisms generalizes;entity annotations unstructured;semanticwe propose novel;ontologies knowledge base;mechanisms biomedical sentence;nrf ontologies;novel semanticwe"}, "73b22457a2f52a834d73d73a76b4124c1cb326be": {"ta_keywords": "prediction decision games;performative prediction decision;nash equilibria game;decision games;game theoretic;equilibria game;stable equilibria efficiently;performative prediction;equilibria efficiently;performatively stable equilibria;new game theoretic;game theoretic framework;nash equilibria;equilibria game mild;formulate new game;prediction decision;decision games focus;stable equilibria;game mild assumptions;equilibria ii nash;ii nash equilibria;equilibria efficiently variety;games;prediction;stable equilibria ii;concepts performatively stable;performatively stable;games focus distinct;equilibria;framework performative prediction", "pdf_keywords": "game stochastic gradient;prediction learning games;performative prediction game;performative prediction games;stochastic game;stochastic games;class stochastic games;game stochastic;stochastic game stochastic;prediction games;stochastic games model;lemma stochastic games;stochastic gradient play;stochastic game used;risk stochastic game;games provide stochastic;decision dependent games;stochastic game prove;stochastic games powerful;prediction game;prediction game provide;prediction games players;continuous games decision;prediction game players;methods continuous games;player performative prediction;strategy dependent game;training stochastic;player determined stochastic;continuous games"}, "6eb974721719056ba8dc74a898c64ae1d081e0ae": {"ta_keywords": "usefulness diagnostic curves;propose diagnostic curve;features diagnostic curve;univariate diagnostic curve;diagnostic curve;diagnostic curves;diagnostic curve constructed;diagnostic curve input;univariate diagnostic;diagnostic curve black;diagnostic curves multiple;features diagnostic;finding univariate diagnostic;undesirable features diagnostic;validate qualitative properties;black box model;demonstrate usefulness diagnostic;curve black box;usefulness diagnostic;validate qualitative;black box;propose diagnostic;diagnostic;qualitative properties monotonicity;models;undesirable features;used validate qualitative;qualitative properties;models understanding;model used validate", "pdf_keywords": "automated dependence plots;dependence plots generalizes;dependence plots optimize;partial dependence plots;dependence plots;dependence plots approach;dependence plots pdps;directional dependence plots;unexpectedness dependence plots;models utility plot;model utility plot;plots demonstrate usefulness;approach automated dependence;plots generalizes;automated dependence;plots commonly;pde plots commonly;plots widely;utility plot used;plots optimize utility;utility plot;plots commonly used;automated dependencewe propose;graphical representation predictions;utility plot defined;automated dependencewe;plots widely used;plotting multivariate models;plots pdps including;plots arbitrary"}, "4e2c41466c8246af0a563ea36fbe80c896bbab2c": {"ta_keywords": "translating ambiguous words;distinguish word sense;neural machine translation;ambiguous words based;word sense proposed;word sense context;word sense;translation based word;sense context input;method translating ambiguous;machine translation;translating ambiguous;based word sense;machine translation nmt;ambiguous words;sense context;context input word;performs translation based;sentential context;method translating;translation based;global sentential context;performs translation;sentential context method;sense proposed method;novel method translating;input word;distinguish word;nmt performs translation;translation nmt", "pdf_keywords": "neural machine translation;translation performance neural;words machine translation;machine translation;word sense disambiguation;model improve translation;machine translation nmt;improve translation words;translation ambiguous words;translation performance;machine translation mt;translating ambiguous words;investigate translation performance;translations baseline;present machine translation;accurately translate ambiguous;translations baseline proposed;challenge machine translation;sense disambiguation;machine translation problem;translate ambiguous words;translation words;translation strategy captures;sample translations baseline;machine translation framework;translation words multiple;translation word target;disambiguation wd translation;translation performance nmt;systems translate ambiguous"}, "e9c52a3fac934919eca036909cc18d909db0d467": {"ta_keywords": "flow molecular fluid;flow fluid continuum;directed flow molecular;scaling peridynamic continuum;flow molecular;continuum model directed;scaling directed flow;fluid continuum continuum;fluid continuum;directed flow fluid;peridynamic continuum model;molecular fluid;fluid continuum like;peridynamic continuum;continuum continuum model;fluid based scaling;molecular fluid based;continuum model;based scaling peridynamic;continuum model continuum;scaling peridynamic;continuum model solved;model solved continuum;directed flow;model directed flow;model continuum model;like continuum model;solved continuum continuum;model continuum like;model scaling directed", "pdf_keywords": "data molecular dynamics;trained molecular dynamics;dynamics molecular displacements;molecular displacements layered;molecular dynamics simulations;molecular dynamics simulation;materials homogenized model;graphene peridynamics;graphene model;modeling paradigm molecular;displacements singlelayered graphene;molecular dynamics;graphene problem learned;graphene peridynamics fundamental;molecular displacements;graphene model based;layer graphene peridynamics;molecular dynamics md;learned material model;nonlocal model displacements;learning nonlocal models;dynamics molecular;graphene optimal response;simulations multilayer graphene;applying graphene optimal;graphene optimal;graphene approach;paradigm molecular dynamics;peridynamic solid model;solving dynamics molecular"}, "52ec4713343083e69b87e36a7a12c7b5898e2780": {"ta_keywords": "speech recognition asr;noisy speech features;unprocessed noisy speech;features speech enhanced;automatic speech recognition;speech recognition;automatic speech;recognition asr systems;speech features;end automatic speech;hidden model nonlinear;noisy speech;asr systems based;model nonlinear partial;features speech;speech features speech;differential equations snp;recognition asr;novel approach adaptive;speech enhanced;adaptive generation end;speech enhanced pathway;nonlinear partial;asr systems;nonlinear partial differential;combines unprocessed noisy;model nonlinear;adaptive generation;equations snp proposed;unprocessed noisy", "pdf_keywords": ""}, "4d1a14352ffb526a1fa0e1cd90e2484e188cddc0": {"ta_keywords": "transferable dialogue systems;transferable dialogue;dialogue systems;dialogue systems framework;framework transferable dialogue;natural language agents;dialogue;agents converse;language agents;reinforcement learning structured;converse natural language;learning structured reward;equips agents converse;language agents continue;training agents;user interoperability;training agents collection;agents continue interact;data equips agents;natural language;learning structured;reinforcement learning;user interoperability multi;interoperability;multi user interoperability;agents converse natural;interact;interoperability multi wide;improving behaviors using;pre training agents", "pdf_keywords": "learning dialogue systems;dialogue systems learns;learning dialogue;learning based dialogues;dialogue user simulator;transferable dialogue systems;learning model dialogue;dialogues framework learnable;dialogue model hybrid;joint learning dialogue;model dialogue representation;dialogue model;proposed dialogue model;dialogue systems;transferable dialogue;generate dialogue;dialog agents;train dialogue user;dialogue systems widely;dialogue systems great;dialogues generated interaction;dialogue representation existing;human human dialogues;domain dialogues;dialogue representation;human dialogues;domain dialogues proposed;multi domain dialogues;purpose dialogue representation;dialog systems"}, "6d00b1024298e5b64ee873028385f7bb4396b05d": {"ta_keywords": "recombination compositional generalization;learning algebraic recombination;algebraic recombination compositional;recombination compositional;compositional generalization text;compositional generalization;algebraic recombination;generate explicit representations;realistic comprehensive compositional;comprehensive compositional;assigning semantic operations;semantic operations;recombination;compositional;generate explicit;learning algebraic;able generate explicit;propose neural;generalization text model;assigning semantic;model learning algebraic;semantic operations pre;algorithm assigning semantic;generalization text;xmath0 generate explicit;explicit representations xmath2;representations xmath2 provide;explicit representations xmath1;neural model learning;representations form xmath0", "pdf_keywords": "parsing compositional generalization;neural parser recombine;compositional generalization semantic;parsing compositional;compositional generalization neural;compositional generalization tasks;train neural parser;compositional semantics;recombination learn syntax;neural parser;describing compositional generalization;compositional semantics underlying;constructing compositional generalization;construct compositional generalization;recombination compositional generalization;generalization semantic parsing;comprehensive compositional generalization;learn syntax semantics;parsing tasks lear;compositional generalization propose;compositional generalization;compositional generalization awe;compositional generalization lexical;limited compositional generalization;semantic representations compositional;approach compositional generalization;neural model compositional;generalization lexical recombination"}, "6ccac8a95bc77549b98d045db6d5e0de3d356ba4": {"ta_keywords": "bert based document;document ranking model;document ranking;document ranking used;based document ranking;neural variant bert;variant bert based;document documents model;documents model;variant bert;context bert based;bert based;documents model combination;characterization document documents;document documents;effective context bert;context bert;bert;ranking model trained;document;ranking model;neural model characterization;ranking;documents;based document;neural;novel neural model;ranking used;trained neural;utility novel neural", "pdf_keywords": "lingual document retrieval;english text retrieval;query document embeddings;lexical translation model;language neural model;neural ranking;retrieval task bert;document retrieval tasks;translation network trained;retrieval combines deep;contextualized embedding effectively;model english text;neural models bert;translation network;language neural;neural ranking benefits;text retrieval;utility lexical translation;text retrieval combines;context dependent embeddings;document embeddings;document embeddings discovery;translation model ibm;using language neural;passage retrieval pretrained;document retrieval task;generation document embeddings;based passage retrieval;ranker model contextualize;contextualized embedding"}, "69c515a62403fcc19125d3a6dd8e878aa5cde604": {"ta_keywords": "autoregressive generator utterance;multi turn utterance;turn utterance experiments;utterance multi turn;generator utterance multi;utterance experiments 200k;generator utterance;turn utterance;utterance experiments;utterance multi;utterance;semi autoregressive generator;faster inference speed;autoregressive generator;inference speed;novel semi autoregressive;models faster inference;faster inference;semi autoregressive;autoregressive;multi turn;models faster;novel semi;semi;proposed model outperforms;propose novel semi;turn;inference;state art models;outperforms state art", "pdf_keywords": "autoregression text generation;automatic utterances useful;automatic utterances;incomplete utterance restoration;text generation sequence;generation sequence labeling;utterance restoration;text generation;sarg restoration utterances;method automatic utterances;restoration utterances;automatic utterances sergey;restoration utterances joint;turn dialogue systems;sequence labeling autoregressive;utterances generative model;utterances based deep;utterances generative;sequence labeling text;autoregression text;sequence labeling;utterance restoration eq;unstructured utterances;turn incomplete utterance;recover entire utterance;labeling autoregressive generation;approach sequence labeling;improves quality sentenceswe;spontaneous utterances generative;utterance removing"}, "d2f327736c9b68f68ad64d0b1cefed9b4dd83313": {"ta_keywords": "imitation learning structured;structured prediction learns;learning structured prediction;structured prediction;imitation learning;learning structured;generation natural language;prediction learns incremental;learns incremental;learns incremental model;prediction learns;natural language unaligned;based imitation learning;approach based imitation;based imitation;natural language;imitation;datasets using automatic;language unaligned data;generation natural;structured;incremental model;method generation natural;incremental;learns;automatic measures human;approach datasets;learning;prediction;datasets", "pdf_keywords": ""}, "e37fb85e8869d464bae8eeebf4cd9321ec8c70ad": {"ta_keywords": "interpretability machine learning;learning model interpretability;accuracy interpretability;accuracy interpretability machine;interpretability performing empirical;offs accuracy interpretability;interpretability machine;model interpretability;interpretability;enforcing interpretability;interpretability using known;implications enforcing interpretability;interpretability using;model interpretability performing;enforcing interpretability using;interpretability performing;interpretable hypotheses;interpretable hypotheses model;set interpretable hypotheses;interpretable;empirical risk minimization;machine learning;results statistical learning;machine learning model;statistical learning;set interpretable;empirical risk;risk minimization;trade offs accuracy;minimization set interpretable", "pdf_keywords": "enforcing interpretability learning;classifiers deemed interpretable;interpretable classifiers computationally;interpretable classifiers;tradeoff accuracy interpretability;learning interpretability enforcing;learning interpretable classifiers;cost learning interpretable;accuracy risk interpretability;interpretability learning;learning interpretability;learning interpretable;identifying interpretable classifiers;interpretability learning algorithms;interpretable classifiers model;interpretability learning propose;empirical classifiers tradeoff;interpretability interpretation classifier;interpretation classifiers;interpretability accuracy;constraint learning interpretability;accuracy interpretability;offs accuracy interpretability;accuracy interpretability quantified;interpretability accuracy literature;interpretability performing empirical;interpretability versus accuracy;consider interpretation classifiers"}, "8b0b2b69657076fc1ce7cce75a6d69e3e5ba2d63": {"ta_keywords": "layer memory memorylesscapacitors;memorylesscapacitors memory based;memory memorylesscapacitors;memorylesscapacitors memory;memory memorylesscapacitors memory;memorylesscapacitors;dielectric memory based;dielectric memory;dielectric dielectric memory;double layer memory;layer memory;memory based double;dielectric double layer;layer dielectric double;single layer dielectric;layers dielectric;layers dielectric dielectric;layer dielectric;dielectric double;memory based;split layers dielectric;memory;dielectric;double layer structure;dielectric dielectric;type double layer;double layer;based double layer;layer structure single;structure single layer", "pdf_keywords": ""}, "6a173e22819480b891306eac65fd44be010dfca8": {"ta_keywords": "signaling inflammation virus;inflammation macaque lungs;influenza virus induced;induced inflammation macaque;virus induced inflammation;signaling inflammation;inflammation macaque;influenza virus;cell signaling inflammation;inflammation virus;cycle 2009 influenza;2009 influenza virus;immune cell signaling;myc associated zinc;inflammation virus dyregulatory;influenza;induced inflammation;2009 influenza;inflammation;transcription factor regulating;associated zinc;macaque lungs model;immune cell;suppresses immune cell;cell signaling;regulation host functions;zinc finger transcription;suppresses immune;protein interaction data;protein interaction", "pdf_keywords": ""}, "1d2a2b14ef14eeaf89169f738f7634cdc685c785": {"ta_keywords": "verifying similarity relational;similarity relational database;detect similarity relational;verifying similarity;method verifying similarity;similarity relational;detect similarity;measurement similarity relational;able detect similarity;relational database database;relational database;relational database based;relational database method;database database;similarity terms;similarity;similarity terms method;database based;database;degree similarity;measurement similarity;based degree similarity;based measurement similarity;database method able;degree similarity terms;database method;database database queryd;relational;database based degree;method verifying", "pdf_keywords": ""}, "c968e8dc442102b38b134b1afadc7cc78fc5b5fb": {"ta_keywords": "named entity recognition;entity recognition ner;entity recognition;evaluation named entity;named entity;interpretable evaluation multi;interpretable evaluation named;dataset ner task;interpretable evaluation;datasets interplay identifying;recognition ner task;entity;methodology interpretable evaluation;dataset ner;evaluation multi dataset;multi dataset ner;recognition ner;task proposed evaluation;ner task proposed;models datasets interplay;models datasets;ner task;datasets interplay;evaluation multi;evaluation named;interpret;interpretable;datasets;methodology interpretable;general methodology interpretable", "pdf_keywords": "entity recognition ner;named entity recognition;entity recognition model;description entities evaluate;recognition ner systems;entity recognition;grained description entities;xmath1 mesons bose;description entities;learning model ner;learning ner evaluation;evaluation named entity;metric evaluating ner;evaluating ner systems;xmath0 mesons measured;models different ner;ner models;recognition ner;fully automated interpretable;ner datasets;model ner described;named entity;bec xmath0 mesons;predicting short entities;automated interpretable evaluation;automated interpretable;xmath0 mesons;label ambiguities entities;xmath0 xmath1 mesons;entities"}, "7a56aba1a4d4020c4933319588b9ed2b34d51125": {"ta_keywords": "secure data noisy;generation secure data;data based peer;secure data;peer connectivity data;security data;peer peer connectivity;peer peer model;data measure security;measure security data;security data able;peer connectivity;based peer peer;peer model;peer model framework;peer peer peer;data noisy data;generation secure;noisy data based;peer peer;noisy data;data noisy;data able generate;secure;measure security;peer;connectivity data;characterized peer peer;based peer;security", "pdf_keywords": "secure linear regression;semi malicious algorithms;implementation semi malicious;malicious algorithms provide;malicious algorithms;implementations semi malicious;secure computation;secure computation demonstrate;framework secure computation;method secure linear;gradient descent spz;malicious algorithms context;gradient descent propose;spdz framework secure;secure linear;gradient descent;stochastic gradient descent;semi malicious;method secure;multiparty computation spdz;simple method secure;computation spdz framework;scalable machine learning;computation spdz;problem secure linear;malicious;framework secure;method problem secure;efficient methods;method stochastic gradient"}, "e02f79b710cdcaa9135b835fad964f6f2c78b1a7": {"ta_keywords": "dog neural encoder;encoder named dog;neural encoder;directly character sequences;bert model challenging;neural encoder operates;character sequences;bert model;comparable bert model;encoder;challenging multilingual benchmark;outperforms comparable bert;character sequences operates;dog neural;directly characters;multilingual benchmark;encoder named;comparable bert;multilingual benchmark use;proposed dog neural;resulting encoder;bert;bias resulting encoder;directly character;encoder operates;encoder operates directly;resulting encoder named;model challenging multilingual;named dog outperforms;subwords soft inductive", "pdf_keywords": "tokenization vocabulary outperforms;deep encoder;tokenization free encoder;modeling encoded words;deep encoder uses;universal tokenization vocabulary;pre trained encoder;trained deep encoder;explicit tokenization vocabulary;pre trained encoders;trained encoder language;new neural encoder;neural encoder efficient;tokenization vocabulary;encoded vocabulary;trained encoder;memorization encoded words;encoder representation efficient;inductive bias encoder;neural encoder;trained encoders;entirely tokenization free;bias encoder;encoder efficient;free encoder;encoder efficient efficient;representation encoder;languages encoder;free encoder named;encoding learning characters"}, "4572ded23106285cbd8ebbe6c3b354973ac06ff7": {"ta_keywords": "interacting particles random;dynamics interacting particles;interacting particles;particle interaction represented;particles random environment;interaction represented particle;particles particle interaction;particle interaction;particles random;dynamics interacting;random environment interaction;treated particle interaction;represented particle particle;represented particle;represented set particles;interacting;particles particle;particles;study dynamics interacting;particle particle;random environment;particle;set particles;set particles particle;interaction represented;interaction represented set;particle treated particle;particle particle treated;treated particle;particle treated", "pdf_keywords": ""}, "e214d2a6399925ce60fa5ce90c0374127a32b47e": {"ta_keywords": "sensor networks light;nodes outage probabilities;optimal policies;networks light traffic;sensor networks;quadratic decision process;deploying wireless sensor;wireless sensor networks;optimal;optimal policies pure;networks light;outage probabilities;structures optimal policies;problem quadratic decision;consider natural heuristic;quadratic decision;light traffic regime;outage probabilities links;deploying wireless;natural heuristic;relays;light traffic;relays placed;nodes outage;wireless sensor;heuristic;relays placed unit;decision process trade;decision process;networks", "pdf_keywords": "optimal deployment wireless;deployment strategies wireless;deployment wireless relays;deployment strategy wireless;deployment algorithms wireless;deployment wireless networks;impromptu deployment wireless;deployment network relays;deployment wireless relay;relay deployment wireless;place relays efficiently;algorithms wireless relays;relay network deployment;relay placement decision;deployment based relay;wireless network deployment;relays efficiently;place relay optimal;wireless relays deployed;relay optimal;algorithm deployment wireless;relay deployment;arbitrary network deployment;relay networks deployed;deployment wireless network;method deployment wireless;based relay placement;propagation wireless network;strategy wireless networks;wireless relay networks"}, "7ee55c115470e1b86e552c5594e2e4258b4ccefb": {"ta_keywords": "proton interaction xmath2;proton interaction xmath3;chiral dynamics xmath0;vertex proton proton;vertex proton;important vertex proton;interaction xmath2 vertex;proton interaction studied;vertex important proton;interaction xmath3 vertex;proton interaction;proton proton interaction;model xmath1 vertex;xmath1 vertex important;xmath2 vertex important;xmath3 vertex important;xmath2 vertex;xmath3 vertex;xmath1 vertex;interaction xmath2;dynamics xmath0 model;interaction xmath3;xmath0 model;xmath0 model xmath1;model xmath1;proton proton;important proton proton;proton;dynamics xmath0;important proton", "pdf_keywords": ""}, "a7766d4c41df235764dfaa9971ce861f6120ac27": {"ta_keywords": "protocol group meetings;group meetings;group meetings use;context group meetings;real timelatency interface;real timelatency;performance ourlatency interface;ourlatency interface;ourlatency interface using;demonstrate performance ourlatency;ourlatency threshold measurements;sixlatency sensors;meetings use;use real timelatency;meetings;set sixlatency sensors;sixlatency sensors threshold;protocol group;information thelatency threshold;timelatency interface center;protocol;timelatency interface;novellatency protocol group;performance ourlatency;meetings use real;ourlatency threshold;thelatency threshold demonstrate;results ourlatency threshold;thelatency threshold;velocity information thelatency", "pdf_keywords": ""}, "252ef125a8874fe8face4540f87f2e000275cc96": {"ta_keywords": "terrorist groups algorithm;communities active terrorist;active terrorist groups;terrorist groups;clustering;finding communities;novel clustering algorithm;clustering algorithm;finding communities active;clustering algorithm task;novel clustering;task finding communities;groups algorithm based;present novel clustering;groups algorithm;active terrorist;communities;groups different ideologies;terrorist;similarity measure;communities active;weighting able groups;similarity;dimensional weighting;similarity measure dimensional;groups;based combination similarity;measure dimensional weighting;way unweighted algorithm;combination similarity measure", "pdf_keywords": ""}, "803c7fdd6e01e1ff8cd43297f4e052078409456d": {"ta_keywords": "conventions social networks;social networks;conventions social;networks central computational;social networks central;formation conventions social;networks;efficient referring;propose probabilistic;formulations continual learning;efficient referring expressions;referring expressions repeated;continual learning;communication;probabilistic model formation;computational problem communication;communication simply;central computational;networks central;expressions repeated interaction;probabilistic;repeated interaction;convergence efficient referring;problem communication simply;propose probabilistic model;conventions;referring expressions;problem communication;social;communication simply transmission", "pdf_keywords": "social conventions emerge;conventions emerge naturally;learning convention formation;conventions shared participants;stable social conventions;conventions communicating;conventions communication combines;conventions communication;formation conventions communication;convention formation fundamentally;coordination convention formation;conventions emerge;level conventions communication;conventions shaped communicative;community level conventions;social conventions;formation conventions language;language conventions communicating;conventions communication aims;conventions context communication;conventions linguistic evolution;people communicate hierarchical;emergence conventions characterized;ad hoc conventions;networks emergence conventions;convention formation;conventions shared;coordination group communicating;conventions humans;coordination convention"}, "c6488f0c62ee4a4d48d0fbf8e8185655226294c1": {"ta_keywords": "attack cma reconfigurableconfigurableconfigurable;controller manipulation attack;cma reconfigurableconfigurableconfigurable surface;transmitter receiver attacker;reconfigurableconfigurableconfigurable surface;reconfigurableconfigurableconfigurable;reconfigurableconfigurableconfigurable surface ris;cma reconfigurableconfigurableconfigurable;assisted communication transmitter;attack called controller;manipulation attack cma;receiver attacker;attacker potential manipulate;manipulation attack;controller modify;receiver attacker potential;ris controller modify;communication transmitter receiver;controller manipulation;ris assisted communication;manipulate ris controller;attacker minimize;called controller manipulation;communication transmitter;controller modify phase;attacker minimize data;attack detection;constraint attack detection;transmitter receiver;elements goal attacker", "pdf_keywords": "attack wireless fading;multiple antennas attack;antennas attack exploits;antennas attack;interference pattern attack;optimality attack wireless;attack wireless;transmitter receiver attacker;able detect attacks;receiver attacker;detect attacks relatively;attack detection given;manipulation wireless communications;detect attacks;different attack detection;receiver attacker potential;attack detection;detection given fading;attack reconfigurable intelligent;pattern attack based;pattern wireless communication;controller manipulation attack;wireless communications fading;manipulation attack reconfigurable;interference pattern wireless;block known channel;detects optimality attacker;communications fading;based attack detection;antenna transmitter communicates"}, "2e4cdd36d77b9d814638fc2cd6c703535cb1d2f7": {"ta_keywords": "pretrained language models;frozen pretrained language;language generation nlu;frozen language model;language models adapt;language generation;natural language generation;pretrained language;generation nlu tasks;creating frozen language;language models;language model;prompts unfamiliar inputs;frozen language;language model pd;adapt input representations;unfamiliar inputs;input representations unfamiliar;frequently natural language;input representations;prompts unfamiliar;inputs frozen pretrained;unfamiliar inputs frozen;representations unfamiliar inputs;natural language;generation nlu;adapt continuous prompts;input;unfamiliar inputs encountered;inputs", "pdf_keywords": "tuning machine translation;pretrained language models;tuning pretrained language;machine translation tasks;pretrained language model;fine tuning translation;tuning translation;input machine translation;translation tasks;embeddings improve prompt;frozen pretrained language;machine translation task;language processing adapting;machine translation;resource translation tasks;nlgs fine tuning;tuning translation low;pretrained language;translation tasks observe;pretraining small language;low resource translation;improve prompt tuning;machine translation propose;small language models;prompt tuning pretrained;translation tasks method;translation task;translation task method;input embeddings improve;nlgs efficient generating"}, "4c2d9136c579a0393d4f50bbbbc6f8dab43c38e9": {"ta_keywords": "prediction velocity dispersion;predict velocity dispersion;super gaussian sources;velocity dispersion source;gaussian sources;velocity dispersion;velocity dispersion variety;gaussian sources context;super gaussian;prediction velocity;velocity prediction paradigm;velocity prediction;robustness velocity prediction;velocity prediction ability;gaussian;able predict velocity;dispersion source;dispersion source method;predict velocity;context velocity prediction;variety super gaussian;method prediction velocity;dispersion variety super;dispersion;sources context velocity;ability predict velocity;dispersion variety;robustness velocity;generalized valued prior;combines robustness velocity", "pdf_keywords": ""}, "6f49026ff623c64ce6de81fd04cf6e1ffe7dd6d9": {"ta_keywords": "using binary neurons;binary valued piano;neurons binary;binary neurons binary;neurons binary neuron;binary neuron;binary neurons instead;binary neurons;binary neuron model;network binary neuron;trained output layer;valued piano rolls;neuron model trained;piano rolls;generative adversarial network;trained output;generative adversarial;convolutional generative;convolutional generative adversarial;piano;piano rolls using;valued piano;generative;adversarial network;piano rolls experimental;adversarial network model;propose convolutional generative;adversarial;rolls using binary;output layer network", "pdf_keywords": "rolls generative networks;piano rolls generative;train network generative;gan generate binary;generator trained stochastic;trained stochastic binary;music based generative;generate binaryvalued piano;binary neural;binary neural network;network generative;music transcription neural;network gan generate;generative networks method;layer generator trained;using binary neurons;generative networks;layers binary neural;stochastic binary neurons;rolls generative;binarizewe propose generative;binary stochastic neurons;method generating piano;refiner trained convolutional;generating piano;gan generate;stochastic generator refinement;trained generator;generator trained model;binaryvalued piano rolls"}, "57e7be6b404abfd7a56a73c0ff9bccc5b27ad7ae": {"ta_keywords": "translation models domain;neural machine translation;translation models;machine translation models;auxiliary language modeling;machine translation;language modeling task;language modeling;embeddings learn model;adaptation neural machine;translation improve performance;model auxiliary language;approach adaptation neural;domain aware feature;feature embeddings learn;learn model auxiliary;method translation improve;embeddings learn;models domain specific;adaptation neural;models domain;model uses domain;translation improve;domain aware;feature embeddings;domain specific data;aware feature embeddings;learn model;auxiliary language;approach adaptation", "pdf_keywords": "domain machine translation;domain embedding learners;task domain embedding;learns embeddings domain;adapting translation accuracy;translation data neural;domain translation;task learned embedding;neural machine translation;domain translation results;output domain translation;adapting translation;domain feature embeddings;domain adaptation;approach adapting translation;input domain task;translation simple adaptation;utilized domain adaptation;learns embeddings;unsupervised domain adaptation;learner learns embeddings;embedding learner outputs;domain task;embedding learners;domain task aware;embedding learner learns;adapt translation model;translation model support;machine translation data;domain embedding"}, "2b3ab7e9c66bffc7af9e4413036e7bba686a7734": {"ta_keywords": "reviewers machine learning;peer review;reviewers negatively biased;novice reviewers;submission novice reviewers;novice reviewers machine;analysis reveals reviewers;reviewers master junior;peer review papers;reveals reviewers negatively;conference peer review;peer review pipeline;reviewers machine;reveals reviewers;reviewers master;reviewers;reviewers negatively;bias submission novice;components peer review;investigate bias submission;133 reviewers master;review papers;investigate bias;writing reviews;133 reviewers;bias submission;writing reviews 19;reviews 19 papers;review papers design;bias", "pdf_keywords": "reviewers biases;resubmission bias reviewers;potential bias reviewers;bias reviewers;bias reviewers evaluations;conferences hypothesize reviewers;bias peer review;reviewers biases review;review biases;bias reviewers decisions;bias evaluation reviewers;review biases review;quality review biases;negative bias reviewers;bias reviewers recommendations;biases review primarily;arising information reviewers;hypothesize reviewers;biases review empirical;hypothesize reviewers judge;preferences reviewers biases;competent reviewers growing;biases review;reviewers conferences;reviewers modifies evaluation;resubmission bias peer;burden reviewers conferences;bias evaluation submissions;reviewers evaluations;effect manifests reviewers"}, "536ce077f08886b5834b639da25068d877c98b2c": {"ta_keywords": "gaussian curvature electromagnetic;curvature electromagnetic spectrum;electromagnetic spectrum propagation;spectrum propagation charged;effect gaussian curvature;particle nonlinear medium;curvature electromagnetic;wave function energy;dependence particle wave;particle wave;gaussian curvature;nonlinear medium energy;propagation charged particle;particle wave function;charged particle nonlinear;energy dependence wave;particle nonlinear;electromagnetic spectrum;nonlinear medium;spectrum propagation;effect gaussian;wave function;propagation charged;energy dependence particle;wave function understood;dependence wave;gaussian;dependence wave function;electromagnetic;wave", "pdf_keywords": ""}, "b54efe01969adaa1c623331d5791897a4dd9f886": {"ta_keywords": "fruit fly genomics;fly genomics interactive;fly genomics;information extraction flybase;extraction flybase;articles fruit fly;extraction flybase database;genomics interactive information;fruit fly;genomics interactive;genomics;curation articles fruit;flybase database;articles curators tools;articles fruit;flybase;www flybase;www flybase org;search curation articles;curation articles curators;http www flybase;flybase database includes;flybase org;flybase org slic;curators tools available;curators tools;tools curation articles;tool search curation;information extraction;search curation", "pdf_keywords": ""}, "0e8da301b098f96fed39c5aa8e2194f678690a16": {"ta_keywords": "algorithm sharing secret;deterministic algorithm sharing;sharing secret;network coding problem;network coding;sharing secret parties;coding problem communication;propose distributed deterministic;distributed deterministic algorithm;consider problem sharing;efficient distributed deterministic;distributed deterministic;problem sharing secret;communication efficient distributed;example network coding;sharing secret provide;algorithm sharing;network propose distributed;efficient distributed;distributed;dealer common distribution;problem communication efficient;problem sharing;propose distributed;secret provide example;sharing;coding problem;secret parties;deterministic algorithm;communication link dealer", "pdf_keywords": ""}, "73569460b023f9ac1fe5a1876c3401460d2fc15d": {"ta_keywords": "code pre trained;source code understanding;code clone;classification code clone;solving code problems;pretrained models solving;source code;tasks source code;code understanding;models solving code;introduce curriculum learning;existing code pre;curriculum learning;use semantic preserving;semantic preserving transformation;semantic preserving;code pre;outperforms existing code;existing code;pre trained models;curriculum learning organize;classification code;code;pretrained models;use pretrained models;data diversity introduce;existing pre trained;diversity introduce curriculum;algorithm classification code;solving code", "pdf_keywords": "semantics pre trainedin;exploit semantic preserving;semantic knowledge downstream;tasks exploit semantic;knowledge downstream tasks;code pre trained;exploit semantic;generative pre trained;trained models downstream;tasks pre trained;models learn semantic;trained models code;coderelated downstream tasks;discovering code variety;nodes pre trained;node pre trained;pre trained models;tasks specialized datasets;code related downstream;adapt pretrained models;parameters pre trained;learn semantic;bridging pre trained;models downstream tasks;furtherpre trained models;transformations furtherpre trained;pre trained model;pretrained models;downstream tasks specialized;discovering code"}, "23f1d4b46bc7c8f357a5a89144d5d32af7be13a5": {"ta_keywords": "generation models news;models news summarization;text based generation;news summarization;news summarization focusing;improving quality news;based generation models;generation models;quality news use;quality news;generation models shown;text based text;text based;models news;summarization focusing;summarization;summarization focusing dynamics;based text based;main aspects generation;based text;dynamics text based;based generation;news use fine;analyze dynamics text;generation process;aspects generation;aspects generation process;generation;news use;text", "pdf_keywords": "summarization models trained;summarization models train;models trained summarization;trained summarization;trained summarization datasets;summarization models tailored;summarization models focusing;summarization models;abstractive summarization models;summarization model;summarization dataset considerably;training strategy summarization;summarization model ofthe;train summarization model;generated summaries;generated summaries propose;generated summaries approach;learning dynamics summarization;abstractiveness generated summaries;generated summaries abstractive;set summarization models;strategy summarization models;summarization datasets;summarization datasets extreme;summarization dataset;generated summaries intrinsic;interview summarization dataset;dynamics summarization models;large abstractive summarization;abstractive summarization constraints"}, "9389af659f14239319186dff1cef49e8ece742c8": {"ta_keywords": "expressive graph learning;graph learning tasks;machine learning graphs;graph learning;core graph learning;learning graphs;scaling expressive graph;expressive graph;large scale graph;learning graphs collection;graph regression node;graph regression;graph data provide;scale graph data;graphs collection;core graph;graphs collection datasets;graphs;discovery expressive models;learning tasks graph;discovery expressive;graph data;learning large scale;machine learning large;tasks graph regression;expressive models significantly;scaling expressive;node classification;node classification andlink;regression node classification", "pdf_keywords": "graph machine learning;graph learning pipeline;graph learning gw;graph learning expressive;graph learning benchmark;graph models trained;graph learning ogb;graph learning framework;graph learning;graphs highly predictive;powerful graph learning;graph models predict;scale graph learning;machine learning graphs;graph learning powerfulwe;learning graphs;graph learning orders;approach graph learning;graph models large;knowledge intensive graphs;graph machine;framework graph learning;large scale graph;graph level prediction;learning graphs perform;link prediction graph;prediction tasks graph;large graphs provide;models large graphs;graphs provide large"}, "c68349ba142be731d6f3339d894764921c69b774": {"ta_keywords": "training data individuals;diabetes cancer data;inform early detection;training data;early detection systems;early detection;early detection process;diabetes method based;acquire training data;data individuals risk;cancer data potential;game like quiz;risk type diabetes;dangerous diseases diabetes;data individuals;cancer data;automatically acquire training;diabetes method;advances early detection;diabetes;type diabetes method;data sets;type diabetes;diseases diabetes;like quiz;inform early;data;data potential inform;diabetes cancer;diseases diabetes cancer", "pdf_keywords": "health question twitter;twitter questions designed;tweets community based;like quiz data;quiz data;use predict obesity;quiz automatically;questions participants weight;game like quiz;like quiz;quiz game like;health information survey;twitter questions;weight height anonymized;data social media;like quiz use;survey public health;predicting prevalence obesity;question twitter similar;tweets community;social media generate;social media driven;predict obesity diabetes;identify individuals overweight;predict obesity;tweets;individuals social media;data social;question data;question twitter"}, "6665e03447f989c9bdb3432d93e89b516b9d18a7": {"ta_keywords": "dynamics xmath0 xmath1;study dynamics xmath0;dynamics xmath0;xmath2 xmath3 xmath4;xmath1 xmath2 xmath3;xmath16 xmath17 xmath18;xmath15 xmath16 xmath17;xmath9 xmath10 xmath11;xmath3 xmath4 xmath5;xmath17 xmath18 xmath19;xmath16 xmath17;xmath14 xmath15 xmath16;xmath10 xmath11 xmath12;xmath0 xmath1 xmath2;xmath17 xmath18;xmath4 xmath5 xmath6;xmath17;xmath1 xmath2;xmath8 xmath9 xmath10;xmath13 xmath14 xmath15;xmath12 xmath13 xmath14;xmath7 xmath8 xmath9;xmath3 xmath4;xmath11 xmath12 xmath13;xmath5 xmath6 xmath7;xmath6 xmath7 xmath8;xmath10 xmath11;xmath4 xmath5;xmath2 xmath3;xmath13 xmath14", "pdf_keywords": ""}, "e92de0c4ef62a84201fac284eb66c37330b5fe1c": {"ta_keywords": "neural machine translation;translation code accelerated;accelerated code generation;machine translation nn;nn translation code;code generation;translation code;code generation use;machine translation;generating neural machine;code accelerated code;code accelerated;generating neural;translation nn translation;method generating neural;code query;quality semantic statements;accelerated code;nn translation;high quality semantic;translation nn;semantic statements;statements given code;quality semantic;code;given code query;neural machine;generate;code query evaluate;novel method generating", "pdf_keywords": "software bug fixing;generating bug fix;generate bug fixes;bug fixes based;bug fixing use;bug fixes software;bug fixing;bug fixing statements;non bug fixing;analysis bug fixes;bug fix statements;determined bug fixes;bug fixes;fixes bug fixes;bug fixes bug;bug fixes performance;discovered software bugs;automatically generating bug;software bugs;generate fix code;bug bug queries;bug fixing non;bug queries;fixes software approach;fix non bug;bug fixing leverage;software bugs present;bug fix;bug queries generated;software change mining"}, "90848c88f56fcd421ac3cfd2c87d3e61211103ea": {"ta_keywords": "spin orbit interaction;dynamics spin orbit;field spin orbit;dynamics spin;spin orbit;study dynamics spin;interaction field spin;orbit interaction repulsive;dynamic magnetic field;dynamic magnetic;interaction presence magnetic;field spin;static dynamic magnetic;magnetic field static;orbit interaction attractive;orbit interaction;orbit interaction presence;magnetic field;field magnetic;magnetic field magnetic;repulsive interaction field;presence magnetic;presence magnetic field;static field interaction;magnetic field treated;field magnetic field;spin;field interaction;interaction repulsive interaction;magnetic", "pdf_keywords": ""}, "051a85bd1384767ea5882dcefa98aee5664aa2cf": {"ta_keywords": "networks speech processing;networks speech;model deep neural;layers deep network;deep network;novel networks speech;knowledge deep network;deep neural networks;domain knowledge deep;model deep;deep neural;deep network parametrization;knowledge deep;based model deep;deep network frameworks;approaches problem speech;inference iterations layers;speech processing including;neural networks;speech processing;neural networks reinterpret;problem speech processing;problem domain knowledge;iterations layers deep;result novel networks;problem speech;layers deep;novel networks;network reinterpreting inference;inference iterations", "pdf_keywords": ""}, "6fde1c63b1a353cf539d319341ae9396000660ed": {"ta_keywords": "pronunciation evaluation noisyenvironment;performance automatic pronunciation;technique automatic pronunciation;automatic pronunciation evaluation;automatic pronunciation;pronunciation evaluation proposed;pronunciation evaluation;noise reduction;noise reduction technique;noise improve performance;evaluation noisyenvironment;effect noise reduction;reduce background noise;pronunciation;noise improve;noisyenvironment;background noise improve;evaluation noisyenvironment use;noisyenvironment use;noisyenvironment use model;background noise;evaluate effect noise;partial wave approximation;noise;effect noise;wave approximation gop;wave approximation;wave approximation spice;reduction technique automatic;named partial wave", "pdf_keywords": ""}, "e03d9684a19c8f8e29ee97b347d4f1e280a88e44": {"ta_keywords": "gestures language grounding;spoken language gestures;language gestures;spoken language representations;gestures language;approaches gestures language;language gestures introduce;language grounding;language grounding demonstrate;similarities gesture space;gesture space;gesture space nce;gestures;learn spoken language;model learn spoken;existing approaches gestures;contrastive learning;language representations;approaches gestures;language representations better;learn spoken;crossmodal grounding relationships;gestures introduce novel;novel contrastive learning;gestures introduce;gesture;spoken language;similarities gesture;consistent similarities gesture;contrastive learning approach", "pdf_keywords": ""}, "d31b5b60e1b3af84cd977da8db0ed4faeb79e7f7": {"ta_keywords": "text style transfer;style transfer;style transfer method;text style;emotive regular text;matching style features;emotive defective texts;regular text;features individual text;text sequence closely;matching style;text texts;defective texts demonstrate;individual text texts;defective texts;texts competitive;individual text;style features;regular text sequence;method text style;text;perfect matching style;texts;features collection texts;style features individual;text texts demonstrate;text sequence;texts competitive state;novel method text;collection texts competitive", "pdf_keywords": "inducing text style;text style representations;transfer textual style;supervised style transfer;text style transfer;style transfer languages;style transfer based;style transfer tasks;style transfer learnwe;context style transfer;style transfer arbitrary;style representations;approach style transfer;perform sentiment transfer;style representations approach;style transfer;learns style vector;perform style transfer;extract style text;sentiment transfer;retrieving style text;style transfer kernel;transfer arbitrary styles;textual style need;accurately extract style;learns style;inducing text;textual style;style extractor;style adjacent sentences"}, "5812e30eb4756aeaf0b013a65b98f8f8aa0f8315": {"ta_keywords": "domain standard model;domain adaptation context;domain adaptation;problem domain adaptation;domain standard;public domain standard;adaptation context standard;standard public domain;xmath0 level abstraction;xmath1 level abstraction;level abstraction use;level abstraction;approach problem domain;abstraction;adaptation context;underlying dsm approach;abstraction use;abstraction ability capture;context standard public;standard model;model dsm approach;problem domain;public domain;domain;standard model dsm;underlying dsm;standard public;dsm approach combines;elements underlying dsm;adaptation", "pdf_keywords": ""}, "cf2a953dc82115d34de51737fef46bf3ff4cd5a6": {"ta_keywords": "meson xmath2 decay;decay xmath1 meson;force xmath0 decay;mass xmath5 decay;mass xmath7 decay;xmath2 decay;xmath0 decay;xmath0 decay xmath1;decay xmath1;xmath2 decay rate;xmath7 decay;xmath5 decay;suppressed xmath3 mass;xmath7 decay rate;xmath5 decay rate;lorentz force xmath0;xmath3 mass suppression;suppressed xmath6 mass;xmath1 meson;xmath1 meson xmath2;xmath4 mass;independent xmath4 mass;meson xmath2;xmath3 mass;xmath4 mass xmath5;xmath6 mass xmath7;xmath6 mass;mass xmath7;rate suppressed xmath3;mass xmath5", "pdf_keywords": ""}, "c44addf352f25f28f69ca9f9422c0e463783206f": {"ta_keywords": "similarity measures parsed;similarity measure parsed;parsed text corpus;measures parsed text;measure parsed text;similarity measures;text corpus;similarity measure;specific similarity measure;task specific similarity;parsed text based;text corpus function;corpus;corpus function graph;similarity;parsed text;derivation similarity measures;specific similarity;use graph walks;graph walks framework;graph walks;corpus function;graph walk;text based;graph walk process;parsed;measures parsed;derivation similarity;meaningful edge sequences;walks framework", "pdf_keywords": ""}, "9c16dcbcdfe6991f5d448543e6f4cbdf37149883": {"ta_keywords": "crucial multimodal tasks;multimodal tasks;multimodal tasks visual;interactions crucial multimodal;empirical multimodally additive;multimodally additive function;multimodally additive;multimodal;empirical multimodally;crucial multimodal;tool empirical multimodally;expressive cross modal;cross modal interactions;multimodally;modal interactions improve;cross modal;modal interactions crucial;visual question answering;modal interactions;isolating cross modal;exploiting unimodal;function projection emap;modal;exploiting unimodal signals;tasks visual;interactions improve performance;projection emap;additive function projection;unimodal signals;unimodal signals data", "pdf_keywords": "multimodal classification tasks;models multimodal classification;multimodal machine learning;task multimodal classification;classification tasks multimodally;multimodal classification specifically;multimodal classification;essential features multimodal;models multimodal;features multimodal interactions;proposed multimodal classification;approach multimodal classification;features multimodal;multimodal interactive models;interactive models multimodal;features multimodal corpora;consider task multimodal;multimodally interactive models;model multimodally;underlying multimodal;real multimodal classification;task multimodal;multimodal;identifying multimodal;propose empirical multimodally;multimodal corpora;recently proposed multimodal;new model multimodally;tasks multimodal;empirical multimodally"}, "335bf6f23ccdae43e45a7c12f33bc4f3488e3762": {"ta_keywords": "neural machine translation;multi source translation;translation single source;source translation single;translate incomplete corpora;machine translation learn;multi source neural;source neural machine;machine translation;learn translate incomplete;source translation;source neural;incomplete corpora;translation original source;source translation original;translation single;translating additional auxiliary;translate incomplete;translation learn;incomplete corpora examine;source generated auxiliary;single source;additional auxiliary language;translation original;translation learn translate;corpora;generated auxiliary language;multi source;source provided translating;single source provided", "pdf_keywords": ""}, "82459c972cc1e439c759010acf7ddce1a89b66e0": {"ta_keywords": "fuzzy graph clustering;graph clustering widely;graph clustering;clustering data global;clustering widely;clustering data;clustering;clustering widely applicable;algorithm clustering data;algorithm clustering;algorithm fuzzy graph;new algorithm clustering;meta algorithm fuzzy;fuzzy graph;meta algorithm tset_;meta algorithm;algorithm based meta;based meta algorithm;algorithm fuzzy;analysis meta algorithm;called tset_ algorithm;tset_ algorithm based;graph;algorithm tset_;tset_ algorithm;algorithm;algorithm based;algorithm generic;new algorithm;networks", "pdf_keywords": "disambiguated sense graph;sense aware clustering;clustering linguistic;clustering linguistic structures;lexical clustering graph;constructing sense graph;sense graph nodes;aware graph clusteringwe;fuzzy graph clustering;graph disambiguated sense;sense graph representation;learning sense embeddings;linguistic graphs natural;sense aware graph;sense embeddings based;linguistic graphs;generate sense graph;sense embedding word;node sense disambiguation;clustering synonymy graph;sense embeddings;clustering semantic structures;clustering semantic;called sense graph;structures linguistic graphs;sense graph;graphs natural language;graph clusteringwe propose;method clustering linguistic;lexical clustering"}, "684821e2459c7fc3ef8a2ec8102678af3613a962": {"ta_keywords": "self trained speech;trained speech representations;speech representations results;trained speech;speech representations;pre trained speech;speech tasks;available self supervised;speech tasks limited;speech representations given;self supervised models;network self trained;supervised benchmark sb;self supervised;specialized prediction heads;multiple speech tasks;introduce supervised benchmark;supervised benchmark;specialized prediction;predicting performance network;evaluating specialized prediction;prediction heads task;prediction heads;performance network self;self trained;predicting performance;multiple speech;supervised minimal architecture;abilities multiple speech;tasks limited supervised", "pdf_keywords": ""}, "2467b2daea0398709d7ea57d084cc1f00f9d168f": {"ta_keywords": "conditional speaker network;conditional speaker model;model automatic speech;novel conditional speaker;conditional speaker;automatic speech recognition;automatic speech;speaker network rnnc;speaker model automatic;speech recognition asr;speaker network;speech recognition;framework multi speaker;speaker generated mixture;speaker model;problem assume speaker;speaker problem assume;multi speaker;multi speaker problem;assume speaker generated;speaker generated;speaker problem;speaker;assume speaker;generated mixture speakers;mixture speakers;recognition asr problem;recognition asr;speech;speakers", "pdf_keywords": "speaker recognition conditional;recognition conditional speaker;parametrically augmented speech;conditional speaker chain;end speaker models;speaker models parametrically;multi speaker recognition;speaker speech recognition;augmented speech amplitude;speaker recognition;segmentation nar speaker;speech datasets presented;speaker speech datasets;speech recognition asr;speaker automatic speech;automatic speech;speech datasets;speaker models;augmented speech;automatic speech recognition;combination conditional speaker;speech recognition;conditional speaker;speaker chain based;consists conditional speaker;multi speaker speech;model multi speaker;end speaker;end end speaker;global acoustic dependencies"}, "a1588ac6d582d30742f998464500bb5ead125dc6": {"ta_keywords": "revenue regret protocol;regret er auctions;regret er auction;auction network;er auction network;regret protocol;auctions new network;revenue regret;regret protocol referred;auction;deep learning architecture;auction network investigate;protocol referred regret;er auction;deep learning;auctions;er auctions;new deep learning;er auctions new;auctions new;network;alternative loss;learning architecture revenue;architecture revenue regret;regret er;regret;alternative loss function;referred regret er;referred regret;transregret alternative loss", "pdf_keywords": "auctions based neural;auctions approach regretnet;designing optimal auctions;optimization auction;hyperparameters regret auction;approach optimization auction;revenue auction;maximizing auctions;optimal auctions;revenue auction satisfying;wish price auction;optimization auction network;auction general regretformer;expected revenue auction;optimal auctions important;constructing auction maximizes;price auction;regret auction;revenue maximizing auctions;auction mechanism;problem optimal auctions;auction systems wish;method constructing auction;auction method;auction maximizes;simple auction;simple auction mechanism;auction design;maximizing auctions recently;financial auction"}, "1c682dca13e47e6e1ee3c8db54af631a8e5e5792": {"ta_keywords": "discounted cost markov;cost markov process;cost markov;optimal policy discounted;computing optimal policy;subspace policy iteration;policy discounted discounted;markov process;discounted discounted cost;markov process large;policy discounted;optimal policy;probability transition matrices;discounted cost;method computing optimal;state space method;policy iteration direct;policy iteration;computing optimal;discounted discounted;subspace policy;markov;spectral decomposition;iteration direct spectral;dominant subspace policy;spectral decomposition outer;process large state;discounted;direct spectral decomposition;product probability transition", "pdf_keywords": ""}, "48e32ba9a891f36183a26f35316e8906d14d83c0": {"ta_keywords": "control toolkit crowdsourcing;crowdsourcing based computational;computational quality control;toolkit crowdsourcing;toolkit crowdsourcing based;toolkit purpose crowdsourcing;quality control toolkit;crowdsourcing based;crowdsourcing;based computational quality;computational quality;use computational quality;benchmarking computational quality;novel computational quality;purpose crowdsourcing;quality control;quality control methods;present computational quality;computational sciences toolkit;based computational;providing novel computational;benchmarking computational;toolkit;computational;computational science;purpose benchmarking computational;control toolkit;benchmarking;toolkit designed;novel computational", "pdf_keywords": "aggregating data crowdsourced;crowdsourced datasets;data crowdsourced datasets;crowdsourced datasets approach;data crowdsourced;toolkit crowd sourcing;crowdsourced;crowdsourcing;crowdsourcing including uncertainty;crowdsourcing including;measures crowd consensus;tasks crowdsourcing;toolkit crowd;control tasks crowdsourcing;crowd consensus methods;accurately predict aggregated;quality resulting datasets;crowd kit computational;tasks crowdsourcing including;crowd sourcing provides;crowd consensus;control toolkit crowd;crowd sourcing;toolkit computational quality;datasets provide benchmark;predicting quality subset;predict aggregated;scalable methodology aggregating;quality subset feature;pair aggregation"}, "d4756d1a7b81f53e71f939ab387cad5f0a4a13b7": {"ta_keywords": "events speech recognition;sound events speech;detection sound events;speech recognition;speech recognition systems;sound events;recurrent neural network;short term memory;memory recurrent neural;recurrent neural;memory recurrent;events speech;term memory recurrent;detection sound;hidden semi markov;event segment based;semi markov model;event segment;speech;markov model proposed;semi markov;recurrent;duration controlled modeling;neural network rl;term memory;markov model;bidirectional long short;markov;rl duration controlled;rl duration", "pdf_keywords": ""}, "19418493b1f9c82809fe4584af427b8807b8ae2d": {"ta_keywords": "extract similarity information;information extract similarity;sentences called similarity;similarity information;similarity information propose;extract similarity;similarity information large;similarity information use;called similarity information;method extract similarity;information large corpus;large corpus sentences;similarity;corpus sentences;information extract;corpus sentences called;corpus sentences use;called similarity;corpus;use information extract;large corpus;sentences called;sentences;sentences use method;sentences use;information large;information use information;use information;information use;information", "pdf_keywords": "describing relation extraction;relation extraction;relation classication deep;sentence level relation;relation corpora;relation extraction task;relation corpora sentence;textual corpora;textual corpora existing;corpora sentence level;graph sentence nonlinearity;object relations;framework describing relation;relations project gutenberg;gutenberg corpus;relation classication task;project gutenberg corpus;predict relation object;relation classication natural;problem textual corpora;characterizing structure relations;object relations project;tasks extracting commonsense;sentence describes objects;describing relation;relation object;enriching sentence level;structure relations presence;corpus;extracting commonsense"}, "298ddceada580c46e40e2a0323c0e3b16ed5f3c9": {"ta_keywords": "estimation parameter distribution;estimation distribution values;estimation probability density;estimation distribution;based estimation distribution;parameter based estimation;parameter distribution values;parameter distribution;estimation parameter associated;estimation parameter;distribution values parameter;parameter associated distribution;estimation probability;method estimation probability;applied estimation parameter;estimation;method applied estimation;distribution values;based estimation;applied estimation;given distribution values;method estimation;new method estimation;distribution;given distribution;associated distribution values;values parameter based;density given distribution;probability density;parameter based", "pdf_keywords": ""}, "562fbb5d706d46f3e250429ac48e6acd2bf18cb1": {"ta_keywords": "toolkit integrates speech;speech enhancement separation;datasets integrated speech;integrates speech enhancement;speech recognition module;integrated speech recognition;speech enhancement;downstream speech recognition;development speech enhancement;integrated speech;speech recognition;enhancement separation toolkits;separation toolkits;integrates speech;downstream speech;separation toolkits optional;enhancement separation systems;optional downstream speech;toolkit integrates;functionality toolkit;toolkit;toolkits;enhancement separation;quick development speech;recognition module;end toolkit integrates;functionality toolkit present;toolkit quick;recognition module aim;toolkit quick development", "pdf_keywords": "speech processing toolkit;programmablethe automatic speech;programmable speech models;speech enhancement modelswe;automatic speech enhancement;programmable speech;integrated automatic speech;enhanced separated speech;library programmable speech;speech separation enhanced;speech enhancement models;automatic speech;separated speech evaluation;speech separation model;source speech processing;speech evaluation new;speech enhancement ar;speech recognition pipeline;end speech enhancement;speech enhancement speech;tasks speech enhancement;extension speech processing;enhancement speech separation;speech separation algorithm;speech enhancement;speech models used;speech models corresponding;integrated advanced speech;speech models;speech evaluation"}, "2dbe78aa516cc911a71ff333a35a5ce0b1a49640": {"ta_keywords": "multi mode speech;mode speech recognition;mode speech;speech recognition;streaming baselines;streaming baselines approach;competing streaming baselines;multi mode;speech recognition able;approach multi mode;baselines approach multi;accuracy competing streaming;speech;context achieve higher;achieve higher accuracy;streaming;novel approach multi;baselines;longer future context;higher accuracy;baselines approach;multi;competing streaming;mode;approach multi;recognition;higher accuracy competing;context;future context achieve;context achieve", "pdf_keywords": "sensor network snn;deep learning model;sensor network;deep learning neural;called deep learning;common deep learning;deep learning machine;deep learning;ducer sensor network;network snn model;trained future context;model trained future;model trained stochastic;trained stochastic;future context sensor;neural network snn;snn model;network snn;advances deep learning;snn model constructed;model called deep;learning model multi;used deep learning;model trained;use deep learning;training encoders;neural network;network snn literatures;train multi mode;sensor sensor"}, "0dc379de3a613110c5fdc9c0361372c1114ee18d": {"ta_keywords": "electron beam transport;xmath0 electron beam;beam transport xmath1;electron beam;ion source studied;electron storage;ion source constructed;principle ion source;electron storage ring;energy xmath0 electron;xmath0 electron;stanford electron storage;ion source;transport xmath1;transport xmath1 proof;low energy xmath0;studied stanford electron;stanford electron;high purity xmath2;beam transport;electron;purity xmath2 xmath3;principle ion;energy xmath0;purity xmath2;proof principle ion;ion;ssr ion source;xmath1 proof principle;xmath2 xmath3 xmath4", "pdf_keywords": ""}, "c6c6b4d328381a530e933c208bb43db2a7fa93c8": {"ta_keywords": "spin orbit coupling;orbit coupling spin;coupling spin orbit;spin orbit interaction;coupling spin polarized;polarized atom coupling;interaction spin orbit;orbit interaction spin;spin polarized atom;coupling spin;interaction spin polarized;coupling strength spin;effect coupling spin;atom coupling;interaction spin;spin orbit;spin polarized;strength spin orbit;atom coupling strength;orbit coupling;example interaction spin;orbit coupling strength;varying spin orbit;polarized atom polarized;polarized atom;atom polarized;orbit interaction;atom polarized background;tuned varying spin;spin", "pdf_keywords": ""}, "7225c2a42990f850f692f8d82e7f3bfaf312145c": {"ta_keywords": "performance recurrent neural;order nmt models;leading order nmt;performance recurrent;recurrent neural network;training large leading;time performance recurrent;leading order;large leading order;recurrent neural;nmt models;neural network models;nmt models framework;models transformers achieving;network models transformers;training based estimated;models transformers;neural network;leading;order nmt;models;framework training large;neural;deciding training samples;training samples;improve training time;training time performance;novel framework training;model;large leading", "pdf_keywords": "models curriculum learning;models trained curriculum;curriculum learning;neural machine translation;translation models;machine translation models;propose curriculum learning;curriculum learning approach;curriculum learning algorithm;model captures learner;curriculum learning framework;learning model trained;machine translation nmt;trained curriculum;novel curriculum learning;curriculum learning improves;resulting models curriculum;models trained;demonstrate curriculum learning;training models elementary;automated machine translation;models curriculum;translation models approach;model trained;better learned model;training models;learned model;machine translation;training data pipelines;learned model performs"}, "4a76869cda286efb20eb78cc6adb13daab37a0d1": {"ta_keywords": "models fokker equations;fokker equations method;fokker equations;based models fokker;models fokker;simulation particle based;particle based simulations;accurate simulation particle;simulation particle;particle based models;particle density approximated;fokker;logarithm particle density;particle density;simulations time evolution;time evolution equations;gradient logarithm particle;particle based;accurate particle based;method accurate simulation;evolution equations;logarithm particle;based simulations time;based simulations;accurate simulation;simulations time;simulation;simulations;evolution equations low;based gradient logarithm", "pdf_keywords": "fokker planck equation;fokker equation stochastic;particle approximations fokker;planck equation stochasticity;fokker planck equations;stochastic particle dynamics;stochastically integrating particle;simulations fokker planck;fokker planck solutions;based stochastic particle;stochastic particle based;force particle diffusion;stochastic particle;solutions stochastic particle;solutions fokker planck;stochastic version fokker;stochastic particle systems;stochastically integrating;particle diffusion;estimate stochasticity stochastic;obtained stochastically integrating;method solving stochastic;equation stochasticity computed;langevin equation sde;particle diffusion proportional;stochasticity estimate stochasticity;estimate stochasticity;approximations fokker equation;stochasticity estimate;logarithm particle density"}, "674f892caa52fa400109defa1773a10088918124": {"ta_keywords": "model predictive control;predictive control mpc;horizon optimal control;predictive control problem;predictive control;finite horizon optimal;underlying predictive control;optimal control;input constraints method;state input constraints;optimal control problem;primal dual method;state input constraint;horizon optimal;optimum constraint violation;input constraints;input constraint sets;novel primal dual;input constraint;underlying finite horizon;optimum constraint;constraints method;control mpc underlying;primal dual;model predictive;control mpc;mpc underlying finite;method model predictive;constraint violation converges;based projected gradient", "pdf_keywords": ""}, "cc549a11d277d86f6228443cb16c231c9bda6c96": {"ta_keywords": "emphasis modeling adaptive;continuous emphasis modeling;emphasis modeling;hybrid continuous emphasis;continuous emphasis;clustering adaptive training;state clustering adaptive;adaptive training combines;hybrid state clustering;modeling adaptive training;adaptive training;clustering adaptive;adaptive training demonstrated;training combines systems;state clustering;training combines;emphasis;modeling adaptive;clustering;training;adaptive;propose hybrid continuous;training demonstrated;training demonstrated effectiveness;hybrid continuous;hybrid state;experiments hybrid state;hybrid;continuous;propose hybrid", "pdf_keywords": ""}, "c1546da843be7ea3e0adfb85b69a0b08d41c7159": {"ta_keywords": "spectral function xmath0;asymptotic behaviour spectral;spectral function;behaviour spectral function;xmath2 xmath3 xmath4;xmath8 xmath9 xmath10;xmath7 xmath8 xmath9;xmath0 xmath1;spectral;xmath3 xmath4;xmath1 xmath2;xmath0 xmath1 xmath2;xmath6 xmath7 xmath8;function xmath0 xmath1;xmath3 xmath4 xmath5;xmath1 xmath2 xmath3;xmath8 xmath9;function xmath0;xmath5 xmath6 xmath7;asymptotic behaviour;xmath9 xmath10;xmath4 xmath5 xmath6;xmath1;xmath2 xmath3;xmath9 xmath10 xmath11;xmath7 xmath8;xmath4 xmath5;xmath0;xmath10 xmath11 xmath12;xmath6 xmath7", "pdf_keywords": ""}, "5b3ca06a7673e2bf372d5f89afb15ae1eb714075": {"ta_keywords": "continuous time probabilistic;probabilistic model dynamics;time probabilistic;time probabilistic model;random events dynamics;events dynamics interacting;based discrete time;probabilistic model;probabilistic;discrete time;interacting systems model;novel probabilistic model;interacting systems based;probabilistic model model;discrete time version;interacting systems;dynamics interacting systems;novel probabilistic;model dynamics interacting;random events;events dynamics;propose novel probabilistic;systems based discrete;dynamics interacting;connected discrete network;discrete network;random variables connected;discrete data;interacting;discrete network use", "pdf_keywords": ""}, "594827fdb2047bc7be4ea2f0d2364f46d187247e": {"ta_keywords": "speaker verification based;recognition speaker verification;speaker verification;automatically filter subtitles;automatic speech;speech recognition speaker;method automatic speech;recognition speaker;speech method automatically;filter subtitles;automatic speech recognition;filter subtitles sequence;speech recognition;youtube method based;collected youtube method;data collected youtube;subtitles sequence;youtube method;speech method;subtitles;subtitles sequence relevant;parameters speech method;parameters speech;relevant parameters speech;automatically filter;collected youtube;verification based;speech;youtube;speaker", "pdf_keywords": "japanese speech corpus;speech corpus youtube;speech corpus jtt;subtitles automatic speech;japanese asv corpus;short speech videos;speaker alignment extraction;automatic speaker verification;corpora automatic speech;speech videos;recognition speaker verification;build japanese speech;automatic speech;speech recognition speaker;speech corpus;speech corpus constructionwe;large speech datasets;videos subtitles automatic;videos subtitles;spontaneous speech corpus;youtube videos subtitles;corpora automatic speaker;scalable speech corpus;corpus youtube videos;speech corpus ar;corpus autonomous speech;segmentation large speech;speech datasets;subtitles video method;speaker verification construct"}, "539631a828bf0badd20d2241784b4e06c223250e": {"ta_keywords": "speaker clustering experiments;applied speaker clustering;speaker clustering;multiscale mixture model;gibbs sampling proposed;blocked gibbs sampling;gibbs sampling;mixture model proposed;continuous multiscale mixture;mixture model;sampling based model;proposed sampling based;multiscale mixture;sampling based methods;clustering experiments nonstationary;noise proposed sampling;sampling based;sampling proposed estimating;estimating continuous multiscale;sampling proposed;conventional sampling based;proposed sampling;sampling method based;estimation improved clustering;method applied speaker;novel sampling method;sampling;conventional sampling;clustering experiments;compared conventional sampling", "pdf_keywords": ""}, "d5181375d242ed181bcde0d682a3c7ec4c4c6102": {"ta_keywords": "social communication training;communication training enhance;communication skills social;improved communication skills;communication training;skills social communication;people improved communication;communication skills;communication social communication;communication early communication;social communication;communication early;communication environments training;communication social;early communication;improved communication;communication late communication;late communication social;types communication early;communication;social communication environments;early communication late;late communication;communication late;verbal cognitive skills;skills social;training help people;types communication;method train people;communication environments", "pdf_keywords": ""}, "562fe9b2f5e7ede128dd9a93edc3971c5e0a2394": {"ta_keywords": "artificial dialog acts;generate artificial dialog;artificial dialog;dialog acts;dialog act approach;dialog act;use dialog acts;dialog acts improved;dialog acts guarantees;future dialog act;knowledge following dialog;dialog;future dialog;following dialog act;knowledge future dialog;use dialog;generative model interlocutors;following dialog;model interlocutors;results use dialog;prediction performance generative;approach generate artificial;performance generative model;generative model;performance generative;model interlocutors results;generate artificial;generative;interlocutors;acts guarantees knowledge", "pdf_keywords": ""}, "05710169c48ac1ffe6af514cc10e72d025023343": {"ta_keywords": "spin polarized current;coupling spin polarized;gaussian random walk;spin polarized;effect coupling spin;coupling spin;strong coupling spin;random walk model;polarized current electric;random walk;polarized current;predictions random walk;dimensional gaussian random;gaussian random;walk model;field dimensional gaussian;walk model model;spin;current electric field;dimensional gaussian;gaussian;strong coupling;walk;current electric;electric field dimensional;electric field;electric field results;limit strong coupling;coupling;effect coupling", "pdf_keywords": ""}, "14a6452d6d026a3f384e425add6ab68f8e65037f": {"ta_keywords": "labelling public crowdsourcing;labelling process crowdsourcing;data labelling public;efficient data labelling;crowdsourcing present practical;public crowdsourcing;public crowdsourcing present;data labelling;process crowdsourcing;crowdsourcing present;crowdsourcing;data labelling process;crowdsourcing present practice;crowdsourcing introduce;process crowdsourcing introduce;implementation data labelling;crowdsourcing introduce concept;labelling public;labelling;concept data labelling;labelling process;label collection;label collection tasks;label;real label collection;choose real label;tutorial efficient data;real label;data;efficient data", "pdf_keywords": ""}, "ea1ba6f5e5852e38ace7bbae4e4f60ffeeabe5b1": {"ta_keywords": "formation microresonators shear;microresonators shear stress;microresonators shear;formation microresonators form;formation microresonators;microresonators form pairs;microresonators form;factor formation microresonators;microresonators;ingredient formation microresonators;shear stress;stress shear stress;stress shear;shear stress important;shear stress shear;phenomenon formation pairs;action shear stress;pairs known phenomenon;formation pairs consequence;known phenomenon formation;simultaneous action shear;phenomenon formation;formation pairs;action shear;important factor formation;shear;factor formation;form pairs known;important ingredient formation;stress important factor", "pdf_keywords": ""}, "70a2a554829f2cebb9fa89829994444fa1ec5a7b": {"ta_keywords": "collective decision distance;decision distance collective;distance collective decision;decision distance;decision makers approximated;decision maker distance;distance close gaussian;distance collective;notion distance collective;collective decision maker;collective decision makers;close gaussian norm;maker distance collective;underlying collective decision;close gaussian;notion distance;gaussian norm underlying;norm underlying collective;approximated distance close;collective decision;approximated distance;decision makers computed;makers approximated distance;gaussian norm;features underlying collective;maker distance;gaussian;decision maker;introduce notion distance;distance close", "pdf_keywords": ""}, "39c5740304b5f4072f92e4e012a4b57e7bc2e817": {"ta_keywords": "channel speech separation;speech separation based;speech separation;verification speaker waveform;single channel separation;single channel speech;channel separation proposed;channel separation;speaker waveform method;speaker waveform;based verification speaker;channel speech;evaluation single channel;verification speaker;separation proposed method;separation based verification;separation based;signal noise ratio;noise ratio sr;approaches single channel;noise ratio;single channel;separation proposed;speaker;separation;novel signal noise;signal noise;waveform method evaluated;waveform method;waveform", "pdf_keywords": ""}, "470fd4faf9b8499e8bf21c5d143145305d07fe83": {"ta_keywords": "entity linking tweets;entities linking tweets;linking tweets based;linking tweets;method linking tweets;tweets necessarily linked;collective entity linking;linking tweets close;collective entities linking;tweets close space;tweets based fact;tweets based;entity linking;tweets;use fact tweets;based fact tweets;entities linking;fact tweets necessarily;fact tweets;tweets necessarily;tweets close;linking;useful collective entities;collective entities;linked space proposed;linked space;linked time;collective entity;necessarily linked;fact linked time", "pdf_keywords": ""}, "5667f934c5bc008d0464878729eed34cbf7ec1df": {"ta_keywords": "learning constraints unlabeled;semi supervised learning;modeling semi supervised;supervised learning constraints;constraints unlabeled data;semi supervised;constraints unlabeled;unlabeled data proposed;unlabeled data;learning constraints;supervised learning;supervised learning tasks;graph classifiers capable;supervised;graph classifiers;use graph classifiers;possible outcomes supervised;unlabeled;outcomes supervised learning;outcomes supervised;classifiers capable describing;classifiers capable;classifiers;learning tasks;modeling semi;constraints;approach modeling semi;capable describing set;learning;data proposed", "pdf_keywords": ""}, "2f1743d1a1be46452ab90691ead8bf916ffd912b": {"ta_keywords": "parametric oscillator parametric;parametric oscillator;parametric parametric oscillator;oscillator parametric;parametric oscillator gain;applied parametric oscillator;oscillator parametric gain;state stable parametric;parametric oscillator method;stable parametric noise;stable parametric;stationary state parametric;state parametric parametric;measure stability stationary;stability stationary state;parametric noise;state parametric;parametric noise method;amplitude phase oscillating;stability stationary;measurement amplitude phase;oscillator method based;method measure stability;parametric gain loss;parametric gain;oscillator gain loss;oscillator method;oscillator gain;parametric parametric;phase oscillating", "pdf_keywords": ""}, "402ab2adcf9da95e6aad9884b1ec53271f39cd32": {"ta_keywords": "inverse extended kalman;extended kalman filter;inverse filters;kalman filter;extended kalman;inverse filters class;inverse state space;nonlinearity forward inverse;kalman filter ekf;novel inverse filters;forward inverse state;inverse state;systems bounded nonlinearity;inverse extended;filter ekf;discrete time systems;filter ekf unknown;forward inverse;nonlinearity unknown matrix;extended versions inverse;time systems bounded;bounded nonlinearity;inverse;nonlinearity forward;using bounded nonlinearity;kalman;nonlinearity derive extended;input considering nonlinearity;novel inverse;state space models", "pdf_keywords": "inverse extended kalman;inverse filter ekf;extended kalman filter;ekf inverse kf;kf ekf inverse;function ekf inverse;ekf inverse underlying;ekf inverse;inverse filtering systems;ekf inverse forward;kalman filter;based inverse filter;kalman filter filtering;inverse filtering;kalman filter ekf;kalman filter proposed;inverse filter;inverse dynamical systems;filter inverse filter;inputs inverse kf;forward filter inverse;inverse filter proposed;extended kalman;filter inverse;including inverse filter;inverse filter method;inverse filter case;considering inverse filter;novel extended kalman;inverse filter ias"}, "f82ae0a87cae2f3a43d4c0289d0cdf7ca57461d0": {"ta_keywords": "xmath2 coupling strength;xmath1 coupling strength;xmath3 coupling strength;xmath4 coupling strength;xmath5 coupling strength;xmath2 coupling;larger xmath3 coupling;coupling strength xmath2;xmath0 xxz spin;coupling xmath0 xxz;xmath1 coupling;strength xmath2 coupling;strength xmath4 coupling;orbit coupling xmath0;coupling strength xmath4;xmath3 coupling;xmath4 coupling;xmath5 coupling;spin orbit coupling;comparable xmath1 coupling;comparable xmath5 coupling;zeeman field spin;coupling xmath0;xxz spin chain;xxz spin;chain spin orbit;orbit coupling strength;strength comparable xmath1;strength larger xmath3;field spin orbit", "pdf_keywords": ""}, "217074971e3dfdbfab3a8c3819cd7953ae666da4": {"ta_keywords": "stable text mining;genetic association database;text mining;constructing database genes;database genes;database genes complex;text mining used;build genetic association;genes complex traits;genetic association;association database;association database demonstrate;features genome;genome build genetic;researchers conveniently construct;genes complex;genes;features genome build;researcher enrich knowledge;constructing database;genome;robust stable text;build genetic;traits;complex traits;researchers conveniently;integrate features genome;researcher;genetic;researcher enrich", "pdf_keywords": ""}, "1b759204e7c13f0e4af9fe00b052af4456ac3669": {"ta_keywords": "skipping reinforcement learning;frame skipping reinforcement;reinforcement learning performance;skipping reinforcement;reinforcement learning;learning performance reinforcement;action repetition loss;performance reinforcement learning;reinforcement;performance reinforcement;reinforcement learning algorithm;learning smaller task;action repetition;frame skipping;learning performance;smaller task horizon;repetition loss;repetition loss offset;incurred action repetition;skipping;gain brought learning;task horizon;loss offset gain;learning smaller;effect frame skipping;learning algorithm define;smaller task;offset gain;task dependent quantity;offset gain brought", "pdf_keywords": "skipping reinforcement learning;reinforcement learning game;frame skipping reinforcement;skipping reinforcement;framework reinforcement learning;learning reinforcement;reinforcement learning reinforcement;reinforcement learning;problem reinforcement learning;learning game theory;learning defensive play;framework reinforcement;learning reinforcement learning;problem reinforcement;task horizon learning;policy problem learning;reinforcement;frame skipping task;reinforcement learning decision;performance simple reinforcement;learning policy set;learning game;horizon learning argue;experiments reinforcement learning;task horizon analysis;policy beneficial learning;decision process reinforcement;game frame skipping;learning policy;learning strategy"}, "c1125fa33a239ef4fd3378ccd46b2a0a0cf79a15": {"ta_keywords": "cascaded data storage;data storage hitchhiker;novel storage data;data storage;data storage capable;data modular;storage data;cascaded data;data modular format;cascaded cascaded data;storage data erasure;novel storage;decoding data modular;storage hitchhiker modular;capable storing decoding;asynchronous cascade cascade;storing decoding data;storage capable storing;asynchronous cascade;distribution asynchronous cascade;data erasure coded;present novel storage;storage;storage capable;distribution based cascaded;data erasure;capable storing;storing decoding;cascaded cascaded;cascade cascaded", "pdf_keywords": ""}, "ac8d33e4c0a45e227a47353f3f26fbb231482dc1": {"ta_keywords": "modeling text associated;joint modeling text;modeling text;text associated proc;text associated;method joint modeling;joint modeling;apjstion suite presented;apjstion suite;lond apjstion suite;computer science aps;text;associated proc;apjstion;modeling;novel method joint;associated proc soc;lond apjstion;soc lond apjstion;science aps;aps baltimore;aps;science aps baltimore;aps division computer;aps baltimore maryland;proc soc lond;aps division;proc soc;method joint;meeting aps division", "pdf_keywords": "aware language models;time aware language;aware language model;introduce temporally aware;know language models;modeling text timestamp;temporally aware model;masked language modeling;temporally scoped knowledge;news corpus;temporal knowledge improves;temporally aware;language models know;language models;temporal knowledge;data language models;jointly modeling text;language modeling;facts model memorize;temporal context data;aware language;language modeling objective;modeling text;time aware;facts separate time;models temporal context;temporal spans;language models complete;data introduce temporally;knowledge date text"}, "9b1057e1f6eb17abf3962d6cd2f49468d27b94c6": {"ta_keywords": "speech translation emphasis;translation emphasis speech;speech systems translation;target speech translation;translation target speech;emphasis speech speech;speech translation;emphasis speech;pause prediction;pause prediction model;integrating pause prediction;pauses inserted words;systems translation emphasis;speech proposed model;speech speech systems;translation emphasis proposed;translation emphasis accomplished;speech systems;translation emphasis;words target speech;target speech proposed;target speech;speech speech;pause;pauses;prediction model translation;pauses inserted;systems translation;integrating pause;emphasis proposed model", "pdf_keywords": ""}, "c985600f0aa223ddc76a2ea628f1fa23504dcbcd": {"ta_keywords": "speech extraction module;speech recognition module;target speech recognition;target speech extraction;module speech recognition;extraction module speech;guided target speech;automatic speech recognition;automatic speech;speech recognition asr;speech extraction;speech recognition;improve target speech;target speaker automatically;module speech;speech recognition performance;target speech;end automatic speech;recognition asr training;recognition module;target speaker;identifying target speaker;speaker automatically correcting;target speaker proposed;extraction module;automatically identifying target;recognition module minimum;recognition performance automatically;location guided target;recognition asr", "pdf_keywords": ""}, "0e141942fa265142f41a2a26eb17b6005d3af29e": {"ta_keywords": "languages global dynamics;dynamics languages discuss;characterizing rle languages;rle languages linguistic;languages linguistic world;dynamics languages;global dynamics languages;languages linguistic;languages global;languages discuss;rle languages;representation languages;languages;languages discuss relevance;representation languages focus;linguistic world;languages focus;linguistic;discuss relevance languages;linguistic world highlight;relevance languages global;relevance languages;art representation languages;characterizing rle;languages focus problem;representation;highlight relevance languages;state art representation;problem characterizing rle;global dynamics", "pdf_keywords": "linguistic diversity languages;language taxonomy;language taxonomy best;bridging linguistic resource;languages use taxonomy;language agnostic community;languages language agnostic;digital resources linguistic;recognising language agnostic;linguistic resource disparitieswe;resource rich languages;linguistic resource;language resources exist;resources linguistic;language resource availability;linguistic diversity world;linguistic resource divide;linguistic diversity;resources linguistic resource;availability research languages;language agnostic status;language resource;languages collectively term;diversity languages;language agnostic;language inclusion language;existing languages;classify language taxonomy;models computational linguistics;nlp tools"}, "683e201783bf76ab99791a02e3763fd3ab8dad96": {"ta_keywords": "named entity recognition;entity recognition combining;labeling combining deep;entity recognition;learning named entity;deep learning active;active learning named;active learning outperform;learning active learning;active learning;named entity;learning named;labeling combining;recognition combining deep;labeling;learning active;combining deep learning;used labeling combining;data used labeling;deep learning;combining deep;method combining deep;entity;recognition combining;used labeling;training data;learning outperform;learning outperform classical;training data significantly;deep", "pdf_keywords": "entity recognition ner;deep active learning;models cnn word;word encoder cnn;cnn word encoder;named entity recognition;based deep active;deep active;entity recognition;active learning sequence;use deep active;faster deep models;best deep models;deep models trained;entity use deep;encoder convolutional word;datasets use deep;faster deep;learning sequence tagging;deep models trainedwe;deep models;deep bayesian active;network nerns cnn;deep models performing;deep learning;deep neural;outperforms best deep;nerns cnn cnn;uses deep neural;cnn word"}, "e318e554098224c9475dfc80765cbbb82fa4a409": {"ta_keywords": "equitable efficient peer;efficient peer review;fair equitable efficient;algorithms help conference;fairness design;efficient peer;promote fairness design;fairness design novel;organizers promote fairness;peer review;promote fairness;fair equitable;equitable efficient;conference organizers;peer review present;guarantees evaluated carefully;conference;algorithms supported strong;semirandomized experimental;help conference;designing algorithms supported;peer;help conference organizers;aaai conference;conference organizers promote;algorithms;algorithmic issue aim;algorithmic;designing algorithms;semirandomized experimental procedure", "pdf_keywords": ""}, "8c9033f976e7787dde9af5ba952d7f9ac9c34496": {"ta_keywords": "streaming algorithm outlier;outlier detection feature;feature evolving streams;outlier detection;algorithm outlier detection;data streams;streaming data streams;evolving streams based;evolving streams;streams based density;detection feature evolving;novel streaming algorithm;algorithm outlier;outlier;streaming algorithm;streams based;data streams evaluate;streams;row streaming data;streaming data;static row streaming;row streaming;streams evaluate;streams evaluate proposed;novel streaming;detection feature;propose novel streaming;feature evolving;streaming;real life datasets", "pdf_keywords": ""}, "36cbc3c24429ba3b69def38e6e64b41485b0a023": {"ta_keywords": "detect maximum entropy;maximum entropy data;extract maximum entropy;entropy data set;entropy set measurements;maximum entropy set;entropy data;maximum entropy;entropy set;entropy;measurements generated subset;method extract maximum;extract maximum;use extract maximum;generated subset data;detect maximum;measurements generated;collection measurements generated;ability detect maximum;set measurements method;measurements method based;subset data;subset data method;data set;measurements method;data set applicable;applicable collection measurements;applied collection measurements;data method;data set use", "pdf_keywords": ""}, "119c33321fc0e1db837ce293f1b65cc26c1cc34e": {"ta_keywords": "matching sentences experiments;key sentences matching;sentences matching;novel reranker mtm;novel reranker;propose novel reranker;matching sentences;sentences matching sentences;rank articles feature;captures key sentences;sentences experiments;reranker mtm;information rank articles;rank articles;key sentences;reranker;sentences experiments real;sentences;feature recent fact;articles feature;articles feature recent;pattern information rank;reranker mtm ethyl;sound fusing event;datasets mtm outperforms;recent fact checked;augmented sound fusing;information rank;datasets mtm;fact checked claims", "pdf_keywords": "predict claim relevance;claim sentence vectors;relevance claim search;claim detection;detecting false claims;predict sentence relevance;claim candidate articles;fact checking articles;prediction claim quality;claim relevance;claim detection general;accurately predict claim;capture lexical similarity;extract semantically similar;filtered claim detection;lexical similarity pretrained;claim relevance claim;claim fact checked;claim candidate article;sentences candidate articles;claim sentence;rank tweets based;claims social media;relevance claim candidate;claims common articles;tweets based recent;predict claim;semantics sentence templates;based recent tweets;semantically similar sentences"}, "708dcd8456426cd609c89a86344e0007c04c80bf": {"ta_keywords": "multilingual benchmark cloze;multilingual benchmark;present multilingual benchmark;benchmark languages;multilingual knowledge access;effectiveness benchmark languages;probes 23 languages;algorithm improve multilingual;multilingual knowledge;benchmark cloze type;benchmark cloze;improve multilingual knowledge;multilingual;improve multilingual;23 languages;method improve multilingual;languages;present multilingual;23 languages develop;languages develop novel;cloze type;cloze;cloze type probes;languages develop;propose code switching;develop novel decoding;code switching based;decoding algorithm improve;novel decoding;benchmark", "pdf_keywords": "novel multilingual benchmark;multilingual benchmark cloze;multilingual benchmark;multilingual retrieval performance;multilingual language models;multilingual low token;probes multilingual language;high multilingual retrieval;multilingual multi token;multilingual retrieval;large scale multilingual;investigate performance multilingual;probes multilingual;multilingual natural language;performance multilingual low;prediction cross lingual;predicting cross lingual;performance multilingual;languages results highly;multilingual multi object;lingual representation prediction;multilingual language;style probes multilingual;high resource multilingual;language models;multilingual low;multilingual;multilingual multi;multilingual natural;achieving high multilingual"}, "db0c587111cfed85dcea413e385b17881e6e0cbb": {"ta_keywords": "parameter tuning neural;structure discovery parameter;neural network language;matrix adaptation;matrix adaptation method;hyper parameter optimization;covariance matrix adaptation;tuning neural network;structure discovery;discovery parameter tuning;language models;automated structure discovery;network language models;tuning neural;language models based;parameter optimization;automatically training configuration;parameter tuning;automated structure;parameter optimization problems;automatically training;neural network;parameter optimization problem;robust black box;network language;black box hyper;neural;box hyper parameter;method automated structure;discovery parameter", "pdf_keywords": ""}, "3b30422b372040ad19a713b35006c21808287720": {"ta_keywords": "bandwidth erasure code;erasure code design;propose erasure code;erasure code based;erasure code;minimum storage regeneration;optimality respect storage;bandwidth erasure;based minimum storage;storage regeneration msr;network bandwidth erasure;minimum storage;regeneration msr codes;codes designed optimal;storage regeneration;storage network;os consumed reconstructions;storage network bandwidth;respect storage network;storage;propose erasure;erasure;respect storage;msr codes designed;code based minimum;consumed reconstructions reduction;codes designed;code design;terms storage network;optimal terms storage", "pdf_keywords": ""}, "6fa7de6f3ce3a599de6fab273a0d43939e176e9d": {"ta_keywords": "human assisted navigation;assisted navigation;learning assistance requesting;assisted navigation problem;agent request contextually;agent request;simulated human assisted;learning assistance;enables agent request;assistance requesting policies;requesting policies agent;policies agent demonstrate;policies agent;navigation;enables agent;framework enables agent;request contextually useful;agent demonstrate;agent;navigation problem framework;useful information assistant;assistance requesting;simulated human;assistant;human assisted;framework simulated human;requesting policies;agent demonstrate practicality;request contextually;responses decision process", "pdf_keywords": "intentions learning agent;dialogue agent learns;learning interacting assistant;navigation agent learns;agents learn answer;intentions learning;learning agent;learning agent present;learning request information;assisted navigation agent;human agent communication;learn assistance requesting;interacting assistant;learning agent equipped;assisted navigation task;speakers learning agent;describing agents learn;agents described learning;learning tasks;actions learn assistance;agent learns capability;dialogue agent;learning interacting;learning training agents;agent learns;enables learner request;answer questions agents;agent communication;useful agent trained;interacting assistant environment"}, "48e4ba2d04bd98843d5aab6e227b29584d63f7b6": {"ta_keywords": "crowdsourcing cooperation existent;crowdsourcing cooperation;nonlinear crowdsourcing annotated;survey crowdsourcing cooperation;tool nonlinear crowdsourcing;nonlinear crowdsourcing;crowdsourcing annotated;linguistic cooperation;linguistic cooperation created;cooperation existent linguistic;presents survey crowdsourcing;called linguistic cooperation;crowdsourcing;survey crowdsourcing;linguistic genres survey;linguistic interface;cooperation existent genres;crowdsourcing annotated bibliography;nonlinear linguistic interface;linguistic genres;linguistic interface called;genres linguistic;existent linguistic genres;existent genres linguistic;use nonlinear linguistic;interface called linguistic;genres linguistic resources;linguistic;cooperation created using;linguistic resources performance", "pdf_keywords": "crowdsourcing linguistic;crowdsourcing linguistic resources;consider crowdsourcing linguistic;crowdsourcing taxonomies cooperation;resource created crowdsourcing;crowdsourcing taxonomies;crowdsourcing representations games;crowdsourcing tool;crowdsourcing representations;created crowdsourcing;crowdsourcing genres;presents survey crowdsourcing;notion crowdsourcing tool;notion crowdsourcing;crowdsourcing evidence;survey crowdsourcing;survey crowdsourcing taxonomies;genres crowdsourcing representations;crowdsourcing tool support;crowdsourcing;types crowdsourcing;introduce notion crowdsourcing;types crowdsourcing genres;different types crowdsourcing;crowdsourcing methods;crowdsourcing genres crowdsourcing;crowdsourcing methods high;genres crowdsourcing evidence;advantage crowdsourcing;genres crowdsourcing"}, "43a87867fe6bf4eb920f97fc753be4b727308923": {"ta_keywords": "transfer learning methods;efficient transfer learning;transfer learning;parameter efficient transfer;pretrained models;tuning parameters tasks;states pretrained models;efficient fine tuning;fine tuning methods;fine tuning parameters;efficient transfer;pretrained models define;tuning parameters;learning methods;tuning methods;new parameter efficient;parameter efficient;learning methods frame;state art parameter;parameters tasks;tuning methods achieve;learning;parameter efficient fine;transfer;hidden states pretrained;models;fine tuning;pretrained;states pretrained;parameters", "pdf_keywords": "pretrained language models;trained language models;transformers natural language;language models downstream;efficient transfer learning;large pretrained language;transfer learning;language models;transfer learning methods;tasks perform translation;pretrained language;trained language;scalable method pretraining;pretraining deep bidirectional;machine translation;pre trained language;machine translation general;method pretraining deep;language models method;attention composition;tuning pre trained;representations frozen transformer;deep bidirectional transformers;language processing;grained attention module;frozen transformer models;language understanding effective;parameters transformer network;pretrained models;modifying attention composition"}, "3e24375a1810375183d47ceadc7418e94533ba5f": {"ta_keywords": "online allocation energy;online allocation perishable;power perishable allocation;online scheduling algorithms;allocation perishable resources;perishable allocation resources;novel online scheduling;online scheduling;computational power perishable;performance online allocation;perishable allocation;online allocation;perishable allocation range;scheduling algorithms;allocation perishable;problem perishable allocation;allocation energy;scheduling algorithms compared;allocation energy computational;scheduling;mechanisms online allocation;perishable resources;allocation resources consider;perishable resources ones;allocation resources;power perishable;approach problem perishable;allocation;fairness efficiency;allocation range objectives", "pdf_keywords": ""}, "b63b698d177ba2861fe97d23763d66324bb1236a": {"ta_keywords": "replication m2 transmembrane;glycoprotein ligand mutant;m2 protein transmembrane;ligand mutant replicates;replication mutant undergoes;mutant containing m2;does replication mutant;protein transmembrane;replication mutant;mutant replicates;replaced hemagglutinin glycoprotein;hemagglutinin glycoprotein ligand;mutant replicates does;chimeric mutant containing;generated chimeric mutant;ligand mutant;chimeric mutant;hemagglutinin glycoprotein;m2 protein;m2 transmembrane;containing m2 protein;m2 transmembrane domain;protein transmembrane domain;mutant containing;glycoprotein ligand;mutant undergoes multiple;transmembrane domain replaced;glycoprotein;transmembrane domain responsible;transmembrane", "pdf_keywords": ""}, "dcd39e2eb27d17c369f3bf7a5a7a2a30bb9201c8": {"ta_keywords": "trained language models;transferability pre trained;language models large;pre trained language;pre trained models;language models;trained models downstream;trained language;datasets pre training;pre training data;large scale unlabeled;unlabeled text data;transfer knowledge model;trained models;transferability pre;transfer knowledge;knowledge model trained;semantics control;constructed datasets pre;semantics control characteristics;benchmark gglue learn;trained models benchmark;model trained;models downstream tasks;model trained dataset;datasets pre;tuning pre trained;trained dataset possessing;unlabeled text;trained dataset", "pdf_keywords": "language downstream tasks;natural language downstream;long shortterm memory;shortterm memory;language downstream;trained natural language;training downstream tasks;pre trained language;trained language models;training data downstream;language trained model;shortterm memory l1;deep linguistic;models pre trained;tasks deep learning;models downstream tasks;pre trained models;data downstream tasks;data pre training;training data semantics;downstream tasks pre;trained long shortterm;thandownstream tasks deep;representations natural language;pre training downstream;tasks deep;trained language;human language trained;pre trained model;datasets pre trained"}, "c6af1ad95917badd7bc65b303a40f54950360279": {"ta_keywords": "statistical dialog managers;statistical dialog manager;dialog manager statistical;rule based dialog;dialog manager based;manager statistical dialog;based dialog manager;dialog managers regularization;statistical dialog;dialog managers;dialog manager;conventional statistical dialog;based dialog;dialog;automobile dialog;task automobile dialog;manager based bayes;bayes decision theory;based bayes decision;bayes decision;manager statistical;chosen rule based;decision theory integrated;based bayes;rule based;existing rule based;managers regularization;actions conventional statistical;rule based experiments;bayes", "pdf_keywords": ""}, "595306f993993e44e2c2f674367103f44df03d9b": {"ta_keywords": "supervised translation baselines;resource machine translation;unsupervised machine translation;supervised translation;improve translation quality;compared supervised translation;translation baselines experiments;translation quality;methods improve translation;machine translation;translation quality compared;machine translation framework;improve translation;translation baselines;translation apply low;machine translation apply;low resource words;augmentation low resource;translation framework;data augmentation low;high resource words;low resource datasets;low resource data;data augmentation;translation;resource words transformed;high resource data;low resource machine;augmentation low;translation apply", "pdf_keywords": "supervised translation augmentation;translation augmentation languages;machine translation low;translation augmentation;low resource translation;resource machine translation;high resource translation;neural machine translation;translation high resource;novel supervised translation;translation low resource;supervised translation;unsupervised machine translation;multilingual translation leverages;parallel translation monolingual;augmentation framework multilingual;machine translation systems;machine translation;monolingual translation unsupervised;translation unsupervised machine;unsupervised forward translation;machine translation uses;resource multilingual translation;perform supervised translation;language lrl augmentation;augmentation languages;supervised translation forsupervised;word translation strategy;translation monolingual translation;improve translation accuracy"}, "5d6f87e31d806a77d22e344106d0310be3342259": {"ta_keywords": "optimal assignment reviewers;reviewer assignment probability;review assignments peer;peer review assignments;assignments peer review;assignment reviewers;assignment reviewers certain;reviewer assignment;constraints reviewer assignment;review assignments;reviewers certain papers;peer review;peer review possible;arbitrary constraints reviewer;evaluation peer;evaluation peer review;assignments peer;assignment probability intractable;optimal assignment;achieve optimal assignment;assigned particular paper;assignment probability;constraints reviewer;reviewers;reviewer;peer;reviewers certain;assignments;review possible;small probability assigned", "pdf_keywords": "decentralized assignment reviewers;reviewers assignment randomized;reviewers randomized assignment;assigning reviewers papers;assigning papers reviewers;papers reviewers decentralized;peer review assignment;algorithm assigning reviewers;paper assignment reviewers;assigning reviewers;sparse assignment reviewers;reviewer assignment;assignment reviewers papers;assigning reviewers given;assignment papers reviewers;reviewers papers assignments;reviewers assignment;given reviewers assignment;probabilities assigning reviewers;review paper assignment;assignment pairs reviewers;assignment reviewers based;probabilities reviewers assignment;reviewer assignments;peer review algorithm;assignment reviewers assignment;reviewers decentralized;reviewers assignment pairs;review assignment process;reviewers assignments constraints"}, "68d0b245e9754de9f36cba305e4ce50ff868cb6a": {"ta_keywords": "robust effective grammar;effective grammar description;effective grammar;arbitrary structure grammar;grammar description;structure grammar;grammar description sentences;grammar induced set;grammar induced;grammar;structure grammar induced;commuting elementary words;induction simple robust;description sentences;following principles grammar;elementary words sentences;sentences allowed arbitrary;description sentences given;principles grammar;principles grammar induced;simple algorithm induction;algorithm induction simple;linearly commuting elementary;words sentences;sentences;words sentences allowed;algorithm induction;simple robust;linearly commuting;sentences given context", "pdf_keywords": ""}, "7cc74ffa1215321712d4a830bb9dee19d9f0fb47": {"ta_keywords": "improve generalization composition;generalization composition;compositional generalization;compositional generalization problem;solve compositional generalization;generalization composition form;improve generalization complex;predicting structured graph;composition form graph;significantly improve generalization;improve generalization;structures predicting structured;predicting structured;generalization problem freebase;generalization complex inputs;structured graph;generalization complex;structured graph containing;solve compositional;model solve compositional;method improve generalization;compositional;generalization problem;generalization;graph containing conjunctions;learn permutation;test structures predicting;composition;structures predicting;learn permutation invariant", "pdf_keywords": "decoding compositional generalization;query representations syntactic;compositional generalization language;improve compositional generalization;graph decoding compositional;semantic structure compositional;syntactic semantic compositions;generalization language representations;aware syntactic composition;compositional generalization neural;semantics syntactic compositions;semantic compositions graph;performing compositional generalizations;syntactic composition attention;semantic compositions model;semantic compositions;graphs natural language;compositions graph decoding;structure sentences neural;decoding compositional;sentences neural networks;decoding improve compositional;compositional generalization;decoding natural language;compositional generalizations;query representations;encoder aware syntactic;decoding conjunctive queries;sentences neural;syntactic compositions"}, "ee3ce47b79917974d30b6eeaaeeba99f1b1a5c59": {"ta_keywords": "endformer trained;end endformer trained;models trained kolmogorov;decoding speed end;performance decoding;trained kolmogorov smirnov;performance decoding speed;trained kolmogorov;decoding speed;endformer trained archi;speed end endformer;decoding;models trained;smirnov rpt network;rpt network;network;study performance decoding;speed end;endformer;set models trained;rpt network propose;framework kolmogorov;end endformer;trained archi;kolmogorov smirnov rpt;trained;network propose novel;framework kolmogorov smirnov;tectures kolmogorov;models", "pdf_keywords": "deep learning network;matrix based network;impulse approximation network;learning network;network matrix based;recognition deep learning;networks convolutional neural;neural network;networks convolutional;pattern recognition deep;network matrix;neural networks convolutional;trained impulse approximation;deep learning;networks model trained;based network matrix;learning network pattern;network pattern recognition;network impulse approximation;neural networks;new deep learning;approximation network;approximation network accelerate;recognition framework matrix;approximation network impulse;convolutional neural networks;recognition deep;learning toolkit matrix;neural networks model;approximation convolutional neural"}, "5f96e3e00b36c5eeebff09a1bf4c804bd4ce4620": {"ta_keywords": "estimation quickest attack;attack remote state;quickest attack detection;estimator attack structure;belief estimator attack;attack detection problem;attack detection;remote state estimation;injection attack remote;data injection attack;attack structure optimal;estimator attack;attack remote;state estimation quickest;probability belief estimator;false data injection;injection attack;quickest attack;attack structure;estimation quickest;state estimation;belief estimator;detection false data;quickest detection false;data injection;detection problem posed;considers quickest detection;remote state;posed constrained decision;detection problem", "pdf_keywords": "attack remote estimation;estimation process attacks;attack detection problems;discrete time attack;detection problems attack;detecting attack;quickest attack detection;attack algorithm;attack detection cost;attack detection problem;sensors quickest attack;problem attack detection;state attack algorithm;attack detection;attack detection general;threshold attack detection;sensors assume attack;linear attack scheme;problem detecting attack;sensor attacked assume;attack detection distributed;observation model attack;time attack detection;problems attack based;attacks approach based;secure estimation networked;systems linear attack;model attack detection;attacks approach;detecting attack strategy"}, "64a106b707586345a055aa22c3356c4dc3d01877": {"ta_keywords": "network interacting particles;dynamics complex networks;complex networks;complex networks based;connectivity interaction particles;network interacting;connectivity model;network connectivity model;connectivity interaction;connectivity model applied;applied network interacting;fixed connectivity interaction;networks based concept;particles fixed connectivity;network connectivity;networks;networks based;random variables connected;model applied network;interacting particles represented;connectivity;interacting particles;concept network connectivity;model dynamics complex;connected set random;fixed connectivity;interaction particles;set interacting particles;dynamics complex;network", "pdf_keywords": ""}, "d5a95567e079685322cd485033d334284c4b0a62": {"ta_keywords": "reversible polymerization systems;reversible polymerization polymer;developments reversible polymerization;reversible polymerization;role reversible polymerization;forward polymerized macromolecules;monomers forward polymerized;polymerized macromolecules repolymerized;polymerization systems;forward polymerized;polymerized macromolecules;polymerization systems small;polymerization polymer science;polymerization;polymerization polymer;repolymerized new polymers;polymerized;polymers;new polymers;macromolecules repolymerized;polymer science;polymer;macromolecules repolymerized new;small monomers forward;understanding properties polymer;properties polymer;reversible;polymer materials;monomers forward;properties polymer materials", "pdf_keywords": ""}, "f16cf130ae75d1ea1ad3b926f605adef41af4af1": {"ta_keywords": "propagation uncertainty dynamic;theory uncertainty propagation;uncertainty dynamic systems;uncertainty propagation;uncertainty propagation using;uncertainty propagation valid;propagation uncertainty;considers propagation uncertainty;uncertainty dynamic;generalized theory uncertainty;theory uncertainty;propagation using conservation;uncertainty;systems using conservation;conservation based method;using conservation based;dynamic systems using;dynamic systems;propagation using;using conservation laws;dynamic systems results;conservation based;study considers propagation;considers propagation;propagation;using conservation;propagation valid;conservation laws;group dynamic systems;propagation valid small", "pdf_keywords": ""}, "b13e9d23983273c0c67b91ae70c55d4c3f745b8b": {"ta_keywords": "simultaneous translation languages;framework simultaneous translation;simultaneous translation;translation languages proposed;machine translation;machine translation methods;translation languages;translation methods;translation presence large;desired translation presence;search desired translation;translation presence;art machine translation;translation;languages proposed framework;languages proposed;novel framework simultaneous;languages;desired translation;framework simultaneous;quickly accurately search;nodes scalable flexible;scalable flexible;simultaneous;nodes scalable;framework comparing predictions;accurately search;proposed framework comparing;accurately search desired;framework able quickly", "pdf_keywords": "neural machine translation;translation agent trainable;simultaneous machine translation;simultaneously optimizes translation;milliseconds translation agent;machine translation simultaneous;simultaneous translation task;machine translation optimization;translation simultaneous machine;simultaneous delayed translation;translation based reinforcement;task translation agent;machine translation nmt;translation optimization;optimizes translation;able delay translation;translation nmt capable;machine translation optimal;translation optimization strategy;optimizes translation quality;delay translation;machine translation;translation agent;translation nmt agent;capable performing translation;translation task translation;delay translation process;delayed translation;greedy machine translation;simultaneous speech translation"}, "ed1c17451a23471afde91c109ecadc6aab8b2ba6": {"ta_keywords": "multimodal disinformation detection;disinformation detection;detection disinformation;detection disinformation use;important detection disinformation;disinformation detection effort;multimodal disinformation;use multimodal disinformation;disinformation use;disinformation;disinformation use types;propagation misinformation explore;misinformation explore;information important detection;information textual;misinformation explore use;textual ii network;information textual ii;propagation misinformation;temporal information use;textual;multimodal;use multimodal;information use;survey use multimodal;misinformation;information use types;types information textual;effective detection;temporal information", "pdf_keywords": "multimodal fake news;detection false news;truthful detection deceptive;verifying veracity rumors;false news detection;fake news detection;disinformation detection;learning false news;multimodal disinformation detection;disinformation detection propagation;detect false news;users media credibility;disinformation detection lack;credibility features verifying;truthful detection;credibility ability detect;research multimodal disinformation;media credibility;veracity rumors propose;news based rumorthe;detection deceptive;information credibility;rumor powerful tool;false news generated;fake news research;disinformation detection covering;detection deceptive behavior;veracity rumors;credibility features;rumorthe negative viral"}, "8c838c7631a7408d5ea5801c9360213782665c9c": {"ta_keywords": "spin valve nanopillar;polymer planar spin;dynamics spin;spin dynamics;dynamics spin valve;spin dynamics spin;model spin dynamics;valve nanopillar planar;planar spin orbit;spin orbit interaction;based spin valve;nanopillar planar geometry;spin valve type;valve nanopillar;spin orbit;spin valve;planar spin;model based spin;nanopillar planar;model spin;spin;based spin;present model spin;nanopillar;valve type polymer;polymer planar;type polymer planar;dynamics;orbit interaction;orbit interaction model", "pdf_keywords": ""}, "db1dafd0c356491cbbf53338b9984de324e7239c": {"ta_keywords": "bilingual lexicon induction;method bilingual lexicon;bilingual lexicon;method bilingual;novel method bilingual;lexicon induction improves;inferred isometry embedding;semi supervised adaptation;lexicon induction;bilingual;isometric adding supervision;isometry embedding;novel semi supervised;isometry embedding spaces;semi supervised;embedding spaces isometric;effective embedding;especially effective embedding;effective embedding spaces;embedding spaces method;inferred isometry;supervised adaptation;embedding spaces;supervised adaptation technique;supervision stabilizes learning;inferred isometry especially;accuracy inferred isometry;embedding;spaces isometric adding;spaces isometric", "pdf_keywords": "bilingual lexicon induction;bilingual lexicons distribution;bilingual word embeddings;aligned bilingual lexicons;bilingual dictionary induction;lexicons distribution matching;bilingual lexicon;bilingual lexicons;language embedding spaces;language matching;matching language pairs;language embedding;depended aligned bilingual;distant language pairs;work bilingual lexicon;language matching based;check language embedding;optimal word embeddings;pairs unlabeled language;learning bilingual dictionaries;aligned bilingual;language pairs based;language pairs unlabeled;bilingual dictionaries based;bilingual dictionaries;similarity embeddings;similarity embeddings demonstrate;learn bilingual word;approach bilingual dictionary;language pairs"}, "9bb9b23823b45ba7521d872bb3e970ede4aafb8a": {"ta_keywords": "random walks network;walks network random;generated random walk;random walk network;random walk generated;walk network random;network random walk;walk generated random;random walks;set random walks;number random walks;walks given random;walk generated;random walks given;walks network;walk random walk;given random walk;random walk;walk network;walk random;walk generated set;random walk random;walks given;walks;walk;network random;generated random;given random;generated set random;generate", "pdf_keywords": ""}, "41675d91ad815f64b0df382c0944247811a62cc9": {"ta_keywords": "phrase tables corpus;extracting phrase tables;machine translation systems;machine translation;machine translation experiments;alignment phrase extraction;single machine translation;tables corpus;based machine translation;tables corpus phrase;phrase tables;translation systems;translation systems proposed;inversion transduction grammars;capture phrase table;phrase extraction;nonparametric inversion transduction;grammars novel bayesian;phrase extraction approach;phrase table;translation experiments language;translation experiments;phrase based machine;corpus phrase based;step word alignment;extracting phrase;method extracting phrase;corpus;novel bayesian inference;transduction grammars", "pdf_keywords": ""}, "1fc6290fa3e6784501ca67cbef33a6a8edcbdb9e": {"ta_keywords": "convergence empirical measure;convergence empirical measures;spaces convergence empirical;convergence empirical;empirical measure obtained;empirical measure;empirical measures;measures general compact;asymptotic finite sample;rates convergence empirical;empirical measures radically;sample rates convergence;general compact metric;general measures;metric spaces convergence;compact metric;results general measures;finite sample rates;measures general;finite sample;empirical;prove finite sample;measure obtained;general measures general;compact metric spaces;measures;measure obtained independent;general compact;sharp asymptotic finite;measure", "pdf_keywords": "convergence empirical measures;convergence empirical measure;convergence measure large;convergence measure underlying;underlying measure large;rate convergence measure;convergence general measures;size empirical measures;empirical measures scaling;asymptotic behavior measure;large distance convergence;convergence empirical;size empirical measure;convergence measure;convergence underlying metric;asymptotic size empirical;limit convergence measure;empirical measures;empirical measure approaches;measures compact metric;empirical measure planar;probability measures finite;distributions clusterable converge;empirical measure usual;empirical measures set;empirical measure;rate convergence empirical;empirical measures supported;measure large limit;empirical measure supported"}, "f5b6819e05e087d2bbae3ddb6d58d2ab4e1d7ca2": {"ta_keywords": "inverse reinforcement learning;demonstrations reinforcement learning;reinforcement learning policy;learning policy orchestration;inverse reinforcement;reinforcement learning learn;agents using reinforcement;demonstrations reinforcement;orchestration autonomous cyber;reinforcement learning;uses inverse reinforcement;rewards contextual bandit;orchestration autonomous;constraints demonstrations reinforcement;approach orchestration autonomous;reinforcement;learning learn maximize;cyber physical agents;policy orchestration approach;autonomous cyber;actions reward maximizing;policy orchestration;using reinforcement learning;using reinforcement;learn maximize environmental;contextual bandit technique;contextual bandit;autonomous cyber physical;maximize environmental rewards;learning policy", "pdf_keywords": ""}, "bfb13c6889626e833bf449fdb361d186467919af": {"ta_keywords": "text experiments sentiment;incorporating feature feedback;experiments sentiment analysis;feature feedback;feature feedback delivered;feature feedback analysis;analysis text experiments;feedback analysis text;sentiment analysis;sentiment analysis natural;text experiments;natural language inference;experiments sentiment;domain natural language;feedback;analysis natural language;methods incorporating feature;better domain evaluations;sentiment;feature;natural language;incorporating feature;feedback delivered;feedback analysis;domain evaluations;language inference proposed;language inference hypothesize;language inference;methods significantly outperform;significantly outperform", "pdf_keywords": "incorporating feature feedback;feedback natural language;appeal feature feedback;feature feedback use;feature feedback classification;feature feedback delivered;using feature feedback;feature feedback;feedback classification text;feature feedback models;feature feedback generalize;feature feedback ii;generated feature feedback;trained feature feedback;feature feedback performance;feedback classification;based feature feedback;solicit feature feedback;rely feature feedback;feature feedback natural;feature feedback model;feedback generalize better;appeal incorporating feature;feedback generalize;extract sentiment;extract underlying sentiment;extracting evidence sentiment;processing natural language;method extract sentiment;feedback"}, "df9949abc06cb4f0f4c0ac1eb7ce0bc62ed5ec02": {"ta_keywords": "problem state art;state art;propose pointwise approach;state art respectively;art state art;propose pointwise;pointwise approach;pointwise approach problem;pointwise;end state art;art state;state art state;approach problem state;art;approach;art respectively;approach problem;art respectively end;problem state;end state;problem;state;respectively end state;respectively end;respectively;propose;end", "pdf_keywords": ""}, "5331a846c854c3ecedf9ecf3ea516cb6dcaba4c8": {"ta_keywords": "saliency methods framework;saliency benchmark framework;saliency benchmark;saliency methods based;benchmark framework saliency;saliency methods;methods framework saliency;framework saliency benchmark;validation saliency methods;empirical validation saliency;development saliency methods;framework saliency;validation saliency;saliency;set saliency methods;set saliency;performance set saliency;framework development saliency;development saliency;benchmark;benchmark framework;benchmark performance;framework benchmark;use framework benchmark;empirically existing methods;benchmark performance set;model reasoning;framework benchmark performance;reasoning use framework;model reasoning use", "pdf_keywords": "saliency p_ models;saliency methods;evaluating saliency methods;new saliency methods;saliency method saliency;extracting saliency;saliency methods ground;saliency applications;saliency methods context;saliency methods correctly;saliency applications effective;saliency methods controlling;evaluating saliency;leading saliency methods;extracting saliency information;evaluation saliency methods;saliency;saliency method;method extracting saliency;evaluation leading saliency;new saliency method;evaluate saliency methods;saliency information model;method evaluating saliency;relies saliency methods;saliency information;saliency methods used;performance saliency methods;saliency methods variety;methodology extracting saliency"}, "807600ef43073cd9c59d4208ee710e90cf14efa8": {"ta_keywords": "neural information retrieval;benchmark information retrieval;information retrieval model;information retrieval type;retrieval model;retrieval type based;retrieval type;information retrieval;retrieval;heterogeneous evaluation benchmark;evaluation benchmark information;evaluation benchmark;benchmark information;evaluating type models;type models evaluate;interaction based neural;late interaction based;heterogeneous evaluation;robust heterogeneous evaluation;type models;benchmark;based neural information;neural information;models evaluate;type;type based;neural;evaluating evaluating type;evaluation;based neural", "pdf_keywords": "benchmark information retrieval;retrieval models variety;retrieval models;retrieval bfm benchmark;different retrieval models;neural information retrieval;retrieval models demonstrate;18 retrieval datasets;information retrieval models;information retrieval bfm;text retrieval tasks;text retrieval;diverse text retrieval;approach dense retrieval;retrieval datasets comparison;retrieval systems;information retrieval;retrieval tasks spanning;retrieval datasets;retrieval datasets provide;different retrieval strategies;retrieval systems including;retrieval strategies;examples different retrieval;dense retrieval;retrieval bfm;information retrieval beir;retrieval unstructured;retrieval tasks;retrieval"}, "3a40cdd82f0706cda6c247e586d5054abeab4e1f": {"ta_keywords": "list expansion;list expansion context;list questions;list answers better;new list answers;sde list expansion;list answers;list produced;list questions explore;generate new list;list;extend candidate answers;new list;use set expansion;set expansion;original list produced;expansion sde list;better original list;expansion context list;original list;expansion context;set expansion sde;original answers sde;answers sde;context list questions;expansion;answers produced sde;candidate answers produced;expansion sde;combines original answers", "pdf_keywords": ""}, "2c3d02ce8780cc6648caf4ee996d9628c6388751": {"ta_keywords": "crowdsourced labels;noisy crowdsourced labels;crowdsourced labels derive;truth noisy crowdsourced;crowdsourcing datasets binary;probabilistic labeling;crowdsourcing datasets;probabilistic labeling model;real crowdsourcing datasets;unique probabilistic labeling;minimax conditional entropy;noisy crowdsourced;crowdsourcing;real crowdsourcing;variety real crowdsourcing;crowdsourced;labeling model;labeling model jointly;labeling;conditional entropy;probabilistic;propose minimax conditional;conditional entropy principle;minimax conditional;labels;entropy principle infer;unique probabilistic;entropy;entropy principle;satisfies objective measurement", "pdf_keywords": "crowdsourced multiclass labels;entropy principle crowdsourcing;model crowdsourcing labels;crowdsourcing labels crowds;labels crowdsourcing;principle crowdsourcing labels;crowdsourcing labels;labels crowdsourcing workers;noisy labels crowdsourcing;labels crowds information;crowdsourced multiclass;crowdsourcing labels provided;approach crowdsourced multiclass;labels provided crowdsourcing;compute crowdsourced information;compute crowdsourced multiclass;labels crowds;crowdsourced information noisy;crowdsourcing tasks;model crowdsourcing;conditional entropy labeling;crowdsourcing workers formulation;crowdsourcing based empirical;real crowdsourcing tasks;probabilistic labels;crowdsourced information;entropy observed labels;crowdsourcing datasets binary;data crowdsourcing;entropy labeling"}, "ce5ff42d629e67a84731b3c62b57b47fc7f2b20d": {"ta_keywords": "encoder autonomous speech;autonomous speech recognition;streaming based encoder;encoder trained;based encoder autonomous;encoder autonomous;autonomous speech;speech recognition;based encoder;input sequence network;mechanism encoder trained;attention proposed network;recognition problem encoder;sequence network;encoder trained novel;encoder;speech recognition problem;synchronous decoding;trained novel network;learns structure input;blockwise synchronous decoding;sequence network data;network learning;learns structure;computation self attention;encoder constructed;novel network learning;synchronous decoding process;decoding;novel context aware", "pdf_keywords": "synchronous decoding attention;end speech recognition;synchronous decoding;speech recognition asr;decoding attention based;speech recognition;attention based encoders;decoding attention;synchronous beam search;implemented hybrid decoding;beam search attention;hybrid decoding;search algorithm streaming;contextual block encoder;transformer node neural;node neural transducer;search attention based;algorithm streaming end;neural transducer trained;label synchronous decoding;automatic speech recognition;large scale speech;processing encoder proposed;encoder proposed;batch decoder based;automatic speech;encoder proposed algorithm;neural transducer;architectures rnns proposed;transducer trained"}, "945d4addf8e94487f6199af71dc15a298791c1b4": {"ta_keywords": "channel energy harvesting;wireless channel energy;fading process markovian;channel fading process;energy arrival process;channel energy;information wireless channel;energy harvesting;scenarios energy arrival;energy arrival;process channel fading;arrival process channel;energy harvesting source;wireless channel;remote sensing problem;information wireless;channel fading;ii energy arrival;horizon decision process;fading process;distributed time;fading process independent;process markovian;infinite horizon decision;remote sensing;distributed time ii;sensing problem formulated;markovian;age information wireless;process channel", "pdf_keywords": "energy arrival sampling;harvesting problem fading;fading channel energy;processes fading channel;sampling source optimal;fading channel optimal;harvesting based markovian;optimal estimation packet;energy packet arrival;channel energy harvesting;probability energy arrival;optimal sampling process;energy arrival process;packet based optimal;model energy arrival;channel optimal transmission;channel energy packet;source optimal estimation;multiple processes fading;sampling policy energy;channel optimal;approach optimal channel;harvesting processes proposed;estimation protocol;model energy harvesting;estimation protocol state;processes fading;energy harvesting processes;processes channel energy;energy harvesting consider"}, "27df24c537b2d3c2a769d917adf92a6a059c5917": {"ta_keywords": "predicting response multiscale;neural operator methods;neural operator representation;coupling neural operator;approach coupling neural;response multiscale;response multiscale problem;neural operator;multiscale;using neural operator;multiscale problem;multiscale problem using;particledynamics continuum model;finite element methods;coupling neural;hyperbrowned particledynamics;hyperbrowned particledynamics continuum;particle hyperbrowned particledynamics;using neural;operator methods proposed;continuum model;high fidelity model;model computational;representation particle hyperbrowned;underlying model computational;operator methods;particledynamics continuum;neural;model finite element;particle hyperbrowned", "pdf_keywords": "model multiscale coupling;numerical model network;multiscale problems sparse;multiscale modeling;model multiscale;particle model multiscale;multiscale model;simulating multiscale problems;multiscale model parameters;multiscale coupling framework;multiscale modeling mechanics;framework multiscale modeling;coupling neural operator;net discretized functions;operator approximation neural;predicting multiscale dynamics;neural network approximation;approach coupling neural;interacting neurons model;approach simulating multiscale;numerical model coupling;neural operator representation;simulating multiscale;approximation neural networks;multiscale coupling;interacting neurons method;linear multiscale model;neurons model;continuum model surrogate;pde using neural"}, "46ef61536a01578e79b6d4e35e803a914afeb629": {"ta_keywords": "electron gas resonant;resonant inelastic scattering;2d electron gas;observed 2d electron;2d electron;resonantly excited electron;inelastic scattering ins;resonant emission observed;gas resonant emission;dimensional 2d electron;electron wave excited;inelastic scattering;electron wave;electron gas;excited electron wave;resonant emission;resonant emission induced;emission observed 2d;ins resonant emission;resonant emission suppressed;scattering ins;scattering;resonant inelastic;excited electron;scattering ins dimensional;gas resonant;emission induced resonantly;electron;type resonant inelastic;excited ins resonant", "pdf_keywords": ""}, "65ee083ce61576955d76b36819bf3ac271335597": {"ta_keywords": "regenerating codes distributed;codes distributed storage;exact regenerating codes;regenerating codes;regenerating codes parameters;codes distributed;distributed storage systems;distributed storage;storage systems;storage systems provide;exact regenerating property;regenerating property prove;peer peer systems;codes parameters;introduce exact regenerating;linear code possess;regenerating property;peer systems;codes parameters relevant;peer systems provide;exact regenerating;possess exact regenerating;storage;linear code;regenerating;construction exact regenerating;conditions linear code;parameters relevant peer;code possess;peer peer", "pdf_keywords": "codes distributed storage;codes regenerating nodes;regenerating codes nodes;regenerating codes distributed;nodes regenerating codes;code regenerating nodes;efficient codes regenerating;regenerating codes constructed;systems regenerating codes;regenerating codes minimum;codesregenerating codes nodes;regenerating codes minimal;codes nodes fully;storage repair bandwidth;codes nodes;code regenerating codes;regenerating codes possess;nodes minimum storage;regenerating codes investigated;nodes existence codes;regenerating codes;construction regenerating codes;regenerating code existence;consider regenerating code;regenerating code proved;exact regenerating codes;regenerating codesregenerating codes;codes regenerating;codes distributed;regenerating codes useful"}, "092ee3a32b6cd951da971124a24872c7cccf3a9f": {"ta_keywords": "transductive transfer learning;unsupervised transductive transfer;transfer learning protein;transfer learning data;transfer learning;learning protein extraction;problem transfer learning;transductive transfer;inductive transductive approaches;learning protein;unsupervised transductive;problem unsupervised transductive;inductive transductive;transductive approaches;learning data target;transductive approaches adapt;transductive;iterative feature transformation;art inductive transductive;models problem transfer;learning data;feature transformation;protein extraction;domain available training;feature transformation ift;protein extraction process;iterative feature;data target domain;transfer;technique iterative feature", "pdf_keywords": ""}, "fce10a1a9727cbda33d44b62409e303f1009417a": {"ta_keywords": "parsing natural language;neural network grammars;parsing natural;network grammars rnng;grammars rnng rnng;parsing process natural;grammars rnng;problem parsing natural;model parsing;parsing;model parsing process;approach problem parsing;process natural language;problem parsing;language based recurrent;used model parsing;natural language model;parsing process;recurrent neural network;network grammars;grammars;natural language;based recurrent neural;recurrent neural;language model;natural language based;based recurrent;language model used;better xmath0 rnngs;rnngs used model", "pdf_keywords": "learning syntax composition;neural network grammars;network grammars rnngs;grammars rnngs;learning syntax induction;model predicting syntax;grammars rnngs focus;parsing model trained;learning syntax;syntax language modeling;novel syntactic model;compositions phrase representations;predicting syntax;phrase representations model;phrase structure parsing;learning syntax way;based parsing model;generative model english;predicting syntax use;syntactic model;state art parsing;structure parsing;model phrase language;structure parsing task;syntax induction bias;parsing model;syntax composition;phrase representations;syntactic model model;language based parsing"}, "49af035c598901fbf766da2cfb040cca7336a8ac": {"ta_keywords": "semantic parsers exploration;semantic parsers;noise semantic parsers;parsers exploration;parsers exploration areas;semantic representation text;semantic representation;abstract meaning representation;parsers;represent semantic representation;represent semantic;graph represent semantic;reduction noise semantic;semantic;noise semantic;imitation learning;text utilizes imitation;imitation learning reduce;utilizes imitation learning;representation text;representation text utilizes;learning reduce information;process text;text utilizes;process text method;imitation;text;utilizes imitation;text method;utilizes graph represent", "pdf_keywords": ""}, "d47ad0a606bedf41dcea614bfa7b7494879c7ba0": {"ta_keywords": "tracking state changes;changes text describing;state changes text;dataset tracking state;dataset tracking;tracking state;generate dataset tracking;changes text;arbitrary text;arbitrary text accuracy;state changes;state changes open;state change tuples;crowdsourcing;domains time dataset;method tracking state;text describing;value arbitrary text;changes open;changes open domains;describing open domains;tracking;text describing open;accuracy evaluated crowdsourcing;crowdsourcing use;novel method tracking;evaluated crowdsourcing;method tracking;text;dataset", "pdf_keywords": "tracking state changes;predicting state changes;predicting entities text;task formulation tracking;machine generated language;capable predicting state;predict open vocabulary;predicting state actions;natural language;language processing nlp;tracking state;task predicting state;state changeswe introduce;processing nlp;task formulation track;text entities;language processing;field natural language;entities state changes;natural language processing;unstructured datasets demonstrate;task tracking;predicted open vocabulary;open source feature;predicting entities;learning state machine;unmentioned entities changes;state machine generated;state changes procedural;changes procedural text"}, "188928df74f9ce00bd1b58686db93ac8cdd07275": {"ta_keywords": "utterances line recognition;batch multiple speech;parallel beam search;traverse multiple utterances;multiple speech utterances;beam search algorithm;beam search;multiple speech;speech utterances line;multiple utterances;speech utterances;utterances line;algorithm line recognition;search algorithm line;vectorizing multiple hypotheses;batch multiple;line recognition propose;line recognition use;search process vectorizing;line recognition;utterances;parallel beam;search algorithm;technique batch multiple;batch;speech;propose technique batch;propose parallel beam;accelerates search process;search", "pdf_keywords": ""}, "2f369845ae7191196d65310210db2485feb3aa86": {"ta_keywords": "g2p conversion training;g2p conversion;new g2p conversion;conversion training method;conversion training;propose new g2p;new g2p;g2p;conversion;training method;training method based;method based adaptive;based adaptive adaptive;adaptive adaptive adaptive;adaptive adaptive;adaptive;training;based adaptive;method;method based;propose new;new;propose;based", "pdf_keywords": ""}, "bd2f3822801a7e2f933d06c261b8783764d8ce18": {"ta_keywords": "adversarial text classification;commonalities adversarial text;adversarial text;commonalities adversarial;study commonalities adversarial;adversarial;adversarial adv samples;model adversarial;adversarial adv;model adversarial adv;layer model adversarial;text classification samples;text classification;classification;classification samples;samples emerge deeper;classes samples;classification samples attempt;samples expose aberration;differences classes samples;classes samples example;adv samples emerge;samples emerge;distribution samples expose;samples expose;expose aberration;identify commonalities;classes;text;commonalities differences classes", "pdf_keywords": "attacks natural language;trained text classifiers;word level adversarial;text classifiers;classify text datasets;text based attack;classify text;text classification;samples adversarial;confidence text classification;able classify text;adversarial;classify text general;adversarial attacks natural;ood samples adversarial;text classifiers different;text classification model;adversarial attacks;sentiment classification;based text classification;adversarial attacks proposed;sentiment classification ood;binary sentiment classification;level adversarial;level adversarial attacks;predictions trained text;blocking adversarial;blocking adversarial attacks;classifier propose novel;samples binary sentiment"}, "576860f910ea8fde366deb03c910ab30cd776966": {"ta_keywords": "classical entanglement distribution;entanglement distribution;entanglement distribution method;classical entanglement;entanglement;version classical entanglement;entangled states;entangled states quantum;collection entangled states;states quantum field;quantum field;large number entangled;large collection entangled;number entangled states;quantum field method;entangled;collection entangled;quantum;quantum mechanical version;number entangled;states quantum;states quantum mechanical;quantum mechanical;particles field method;particles field;motion particles field;collective motion particles;particles;separation collective motion;motion particles", "pdf_keywords": ""}, "95a35473fd1936927dd4a53fe0a5d2d6762d99b3": {"ta_keywords": "hierarchical model musical;control musical performance;detailed control musical;control musical;model musical instruments;musical instruments enables;parameters infer musical;musical instruments;expressive performance timbre;infer musical notes;musical notes;musical performance;model musical;infer musical;notes synthesis;instruments enables detailed;performance timbre dynamics;instruments;musical performance starting;given notes synthesis;timbre dynamics articulation;musical;musical notes high;instruments enables;performance timbre;notes synthesis given;expressive performance;performance given notes;notes high level;timbre dynamics", "pdf_keywords": "synthesis music;neural audio synthesis;hierarchical audio synthesis;generative model musical;synthesis music based;approach synthesis music;audio synthesis;audio synthesis detailed;generative model music;music generating generative;audio synthesis module;trained predict synthesis;hierarchical model musical;audio synthesis concatenative;conventional audio synthesis;trained predict synthesizer;music generating;note performance synthesis;novel music modeling;features music generating;music modeling;hierarchical music modeling;performance predict synthesis;predict synthesizer;music modeling enables;model musical performance;predict synthesis;automatic generation music;analysis expressive musical;predicts synthesis"}, "55a3b36fd21dbbe9384ab3ba1bcf901235d95f47": {"ta_keywords": "supersupervised learns answer;learns answer sub;supervised supersupervised learns;questions supersupervised model;learns answer;supervised supersupervised;sub questions supersupervised;supersupervised learns;questions supersupervised;algorithm supervised supersupervised;answer sub questions;supersupervised model combines;supersupervised model;supersupervised;answer sub;supervised;learns;sub questions;algorithm supervised;large improvements hotpot;improvements hotpot;hotpot best known;propose algorithm supervised;improvements hotpot best;answer large improvements;hotpot;hotpot best;dev;dev sets;questions", "pdf_keywords": "predicting hard questions;hard question corpus;predict hard questions;simple question corpus;question corpus;unsupervised decomp learns;question corpus approach;question corpus accuracy;questions using supersupersupervised;heuristic extractive decompositions;unsupervised decomp;questions use supervised;generate accurate questionswe;nsupervised unsupervised decomp;nsupervised simple questions;decomp learns answer;hard questions using;unsupervised learning create;use unsupervised learning;unsupervised learning;unsupervised learning methods;learns answer;learns answer simple;algorithm nsupervised transduction;better decompositions extractive;answer statistics supervised;extractive decompositions;questions hard;questions using nsupervised;unsupervised training simple"}, "e153713b0423b4bae325340b2211e704effd5252": {"ta_keywords": "inductive logic programming;predicting fault density;logic programming ilp;algorithms known inductive;programming ilp methods;known inductive logic;logic programming;predicting fault;fault density classes;inductive logic;programming ilp;learning algorithms known;class learning algorithms;learning algorithms;classes compare ilp;fault density;metrics calling tree;calling tree representation;programming;algorithms known;evaluate class learning;ilp methods;automatic generation numeric;generation numeric metrics;class learning;task predicting fault;ilp methods task;algorithms;clauses automatic generation;tree representation", "pdf_keywords": ""}, "e718ffe247a61a77b45953a7e8a5b86a45ed579f": {"ta_keywords": "training dependency parsers;dependency parsers;dependency parsers partially;parsers partially annotated;partially annotated corpora;trained partially annotated;annotated corpora;partially annotated data;parsers partially;annotated corpora advantage;partially annotated;parsers;annotated data;annotated data approach;training dependency;approach training dependency;dependency;annotated;corpora;corpora advantage approach;corpora advantage;tree mst trained;mst trained partially;trained partially;tree mst;spanning tree mst;spanning tree;tree;train data;easy analyze train", "pdf_keywords": ""}, "84e50df18c284b985d287b462c63c20186cc5da1": {"ta_keywords": "computer assisted models;classes computer assisted;novel classes computer;assisted models;classes computer;programming demonstration pbd;assisted models method;technique programming demonstration;technique programming;programming demonstration;programming;models;based technique programming;models method;novel method teaching;models method based;method teaching;method teaching novel;teaching novel classes;learning means library;library demonstrate utility;computer assisted;library demonstrate;teach novel classes;classes;teaching novel;demonstration pbd use;showing used teach;novel classes;teach novel", "pdf_keywords": ""}, "efada589efdb0adf3aa9dc2b6cb6979a50658276": {"ta_keywords": "news articles data;analysis news articles;determine headline news;news reporting project;predict headline news;novel news reporting;news articles;headline news article;headline news;able predict headline;news reporting;analysis news;predict headline;novel news;articles data;derived novel news;determine headline;analysis determine headline;articles data set;news article;news article agreement;set analysis news;news;headline;articles;hypothesis article;hypothesis article true;statistical prediction;produces statistical prediction;reporting project", "pdf_keywords": ""}, "3c6407554fb4ee599f42501cf5cba8fcefa88783": {"ta_keywords": "training text encoder;automatic speech;automatic speech recognition;speech recognition models;speech recognition;end automatic speech;text encoder model;error encoder reconstruction;text encoder;training text;encoder model;encoder;encoder reconstruction;models using unpaired;encoder model given;reconstruction error encoder;cycle consistency training;model given encoder;given encoder reconstruction;encoder reconstruction error;defined encoder reconstruction;using unpaired data;unpaired data;error encoder;given encoder;unpaired data proposed;recognition models;consistency training achieved;corpus;achieved training text", "pdf_keywords": "training semisupervised speech;train speech network;training speech networks;end speech recognition;speech network;speech recognition asr;speech text networks;speech networks machine;automatic speech;library speech networks;speech model;speech networks;automatic speech recognition;speech recognition;end automatic speech;semisupervised speech proposed;audio data transcriptions;semisupervised speech;training speech;speech text;speech network optimized;network dimensional speech;speech corpus;dimensional speech model;training condition speech;speech networks use;speech text synthesis;approach training speech;states speech text;data transcriptions"}, "b568c562fcfad8d7a943a9ea63aca36c487b6d7d": {"ta_keywords": "estimating relative error;classifier;estimate relative error;classifier trained;given document based;method estimating relative;estimating relative;classifier trained data;relative error;use classifier;given document presence;detect explain relative;given document;explain relative error;relative error given;use classifier trained;document presence background;based use classifier;document based;trained data;estimate relative;document;documents form data;trained data method;error given document;document based use;document presence;documents;detect;used estimate relative", "pdf_keywords": ""}, "afa9364ec48e38d19099cfc22ac9cb679c4baa39": {"ta_keywords": "stereotypically gendered entities;stereotypically gendered scene;context stereotypically gendered;set stereotypically gendered;stereotypically gendered;exhibit gender biases;demonstrate stereotypically gendered;language context stereotypically;stereotype faithfully describing;intraandity dynamics vision;gender biases;gendered entities exhibit;gender biases preferring;gendered scene;entities exhibit gender;gendered scene demonstrate;scene demonstrate stereotypically;exhibit gender;stereotype faithfully;stereotype;set stereotypically;reinforce stereotype;gendered entities;context stereotypically;role intraandity dynamics;faithfully describing visual;reinforce stereotype faithfully;vision language context;preferring reinforce stereotype;demonstrate stereotypically", "pdf_keywords": "gender bias multimodal;stereotypically gendered entities;bias multimodal language;genders reflected language;bias visual linguistic;gender bias visual;gender association objects;gendered entities demonstrate;recognition gender constructed;recognition gender;gender bias genders;demonstrate stereotypically gendered;bias genders reflected;intermodality associations biases;based recognition gender;using stereotypically gendered;analyzing gender bias;model gender bias;bias multimodal;biases learned multimodal;bias genders;gender information bias;stereotypically gendered;gendered entities tend;gendered entities;amplifies gender association;multimodal language models;captioning dominated bias;describing language bias;genders reflected"}, "8b5071a38718194063cf17ca446ba8d9f4907a18": {"ta_keywords": "sentiment analysis factoid;question answering tasks;question answering;natural language processing;features natural language;factoid question answering;models sentiment;models sentiment analysis;existing models sentiment;answering tasks;simple neural;sentiment analysis;language processing tasks;natural language;dropout model outperforms;simple neural network;neural;models deepening network;neural network;dropout model;answering;present simple neural;language processing;neural network captures;factoid;dropout;deepening network;existing models deepening;sentiment;models deepening", "pdf_keywords": ""}, "4240d8e1e5c2ef82d62ba9d7bb323c357c718c1c": {"ta_keywords": "persistent topological structure;based persistent topological;persistent topological;topological layer deep;novel topological layer;topological layer;topological features data;layer general persistent;topological structure data;persistent homology arbitrary;persistent homology;capturing topological features;topological features;general persistent homology;learning based persistent;layer based persistent;persistent data layer;topological structure;capturing topological;capable capturing topological;novel topological;propose novel topological;topological;layer deep learning;persistent data;layer deep;based persistent data;layer classification;layer based;proposed layer classification", "pdf_keywords": ""}, "1678eccf0f3895dbea6dfac44fc9d4f86de15ff6": {"ta_keywords": "predicting eliciting emotion;predict response emotion;predict person emotion;eliciting emotion social;response emotion trigger;emotion trigger ability;eliciting emotion;emotion sufficient predict;affective communication;emotion trigger;emotion trigger sufficient;social affective communication;affective communication ability;response emotion;emotion social affective;emotion social;person emotion;person emotion sufficient;predicting eliciting;emotion;emotion sufficient;communication ability predict;social affective;trigger ability predict;affective;predict person;predict response;ability predict person;problem predicting eliciting;ability predict response", "pdf_keywords": ""}, "751816df0027c0ae6c337ba392a5447bef86ca77": {"ta_keywords": "state conjugated polymers;conjugated polymers approach;conjugated polymers;energies poly hexylthiopnene;state energies poly;excited state conjugated;conjugated oligomers demonstrate;electronic excitations;electronic excitations excited;excited state energies;energies poly;oligomers demonstrate transferability;underlying electronic excitations;conjugated oligomers;polymers approach relies;polymers approach;long conjugated oligomers;excitations excited state;excitations long conjugated;polymers;p3ht single crystal;electronic excitations long;poly hexylthiopnene p3ht;state energies;excitations excited;transfer learning;lying electronic excitations;excitations;transfer learning models;using transfer learning", "pdf_keywords": ""}, "a4577911d247e472772e2101d21aeaf8f46053cc": {"ta_keywords": "parsing ambiguous natural;parsing ambiguous;semantic parsing;ambiguous natural language;semantic parsing specifically;input semantic parsing;problem parsing ambiguous;ambiguous input semantic;parsing;parsing specifically;paraphrase reduces ambiguity;reduce ambiguity;problem parsing;parsing specifically focus;reduce ambiguity adding;reduces ambiguity adding;input semantic;reduces ambiguity;context free grammars;natural language sentence;focus problem parsing;representation reduce ambiguity;explicit paraphrase reduces;additional explicit paraphrase;explicit paraphrase use;ambiguity adding;natural language;ambiguous input;explicit paraphrase;free grammars", "pdf_keywords": ""}, "fdb3969b654ab01be1807bbf84707a80e6283a52": {"ta_keywords": "automatically extracting structured;extracting structures expert;approaches extracting structures;extracting scientific entities;extracting structured;extracting structured representations;expert annotated articles;synthesis procedures texts;unsupervised approaches extracting;extracted scientific entities;extracting structures;structures expert annotated;annotated articles;approaches extracting;models extracting scientific;model procedural text;automatically extracting;annotated articles strong;structured representations synthesis;supervised models extracting;extracting scientific;texts materials science;models extracting;events extracted scientific;expert annotated;representations synthesis procedures;extracted scientific;syntheses inorganic;linked events extracted;experimental syntheses inorganic", "pdf_keywords": "predicting understanding inorganic;nlp predict structure;synthesis text;synthesis text challenge;extracting structure synthesis;represent inorganic syntheses;processing nlp;use nlp predict;nlp library extracted;nlp predict;nlp library;representations inorganic synthesis;materials crucial predicting;data nlp;extracting action graphs;reaction databases accelerated;processing nlp use;extract structured representations;action graphs synthesis;structured representations inorganic;natural language processing;nlp techniques;inorganic syntheses;data nlp library;language processing nlp;processing nlp techniques;text extracting structure;synthesis text access;science synthesis text;inorganic synthesis"}, "19f727b7a42a21bc3f99536e8368029f4b9b8e14": {"ta_keywords": "mapping noun images;noun images annotated;noun images;foreign language lexical;senses foreign language;language processing visual;mapping noun;language lexical;word senses foreign;lexical resource;lexical resource study;study mapping noun;images annotated;word senses;language lexical resource;lexical;collection word senses;processing visual world;images annotated collection;visual world study;senses foreign;context natural language;visual world;natural language;visual;processing visual;foreign language;annotated collection word;natural language processing;senses", "pdf_keywords": ""}, "97ef5081aa4e2984c16ea78b862266e4852c7faf": {"ta_keywords": "search email approach;tocentred search email;search email;approach tocentred search;email approach based;tocentred search;email approach;center activity content;vector space models;relationship center activity;vector space model;search;center activity;email;activity content;predicting;relationship center;predicting future involvement;approach tocentred;activity content domain;space model proposed;novel approach tocentred;proposed model;proposed model evaluated;model proposed model;use vector space;novel problem predicting;model proposed;predicting future;vector space", "pdf_keywords": ""}, "af034b0e893a0a24e41cdb54afb35d4250407f50": {"ta_keywords": "zeeman field spin;spin orbit interaction;orbit interaction xmath0;effect zeeman field;interaction xmath0 limit;xmath1 limit interaction;field spin orbit;effect zeeman;interaction xmath0;xmath0 limit interaction;spin orbit;interaction strength reduced;orbit interaction;field spin;zeeman field;xmath4 limit effect;xmath2 compared;reduced factor xmath2;interaction strength;limit interaction strength;strength field xmath1;factor xmath2 compared;interaction strength comparable;xmath1;factor xmath2;xmath3 xmath4 limit;zeeman;xmath2;xmath3 xmath4;xmath1 limit", "pdf_keywords": ""}, "7657b56d2ac9269b32e8bcbe2a20f99ea17afe09": {"ta_keywords": "controllable perceived age;age control using;converted singing voice;singing voice higher;waveform modification based;perceived age control;waveform modification;method perceived age;quality converted singing;spectrum differential gender;age control;direct waveform modification;singing voice;voice higher compared;modification based spectrum;converted singing;perceived age wider;perceived age;voice higher;gender dependent modeling;using direct waveform;age wider quality;voice;waveform;direct waveform;differential gender dependent;range controllable perceived;singing;based spectrum differential;based spectrum", "pdf_keywords": ""}, "869d53277b0ec5e47a30b874aeb157df88649ea0": {"ta_keywords": "zeeman field stability;zeeman splitting model;cluster framework zeeman;xmath0he ep cluster;field stability xmath0he;stability xmath0he ep;framework zeeman splitting;zeeman splitting;zeeman field strength;stability xmath0he;effect zeeman field;framework zeeman;strength stability xmath0he;zeeman field;field stability;xmath0he ep;effect zeeman;choice zeeman field;cluster determined interplay;zeeman;cluster determined;ep cluster framework;suitable choice zeeman;ep cluster;ep cluster determined;cluster framework;ep cluster minimized;effect field stability;splitting model;field strength stability", "pdf_keywords": ""}, "bff4d630cbea6a90b149b28caff5489c1a4ccaad": {"ta_keywords": "interacting bath bosons;bosons model;particle interacting bath;model particle interacting;bath bosons model;particle interacting;stochastic differential equation;bosons model solved;transition classical quantum;stochastic differential;stochastic process;stochastic;classical quantum state;statistical mechanics model;statistical mechanics;generates stochastic differential;model particle;classical quantum;stochastic process generates;generates stochastic;quantum state;solved stochastic process;process generates stochastic;bosons;solved stochastic;bath bosons;model solved stochastic;quantum;quantum state depending;coupling constant transition", "pdf_keywords": ""}, "7e0342b304ca8ce564a664eb17e85358b07488fe": {"ta_keywords": "speaker adaptation;adaptation noise mixture;speaker adaptation noise;achieves speaker adaptation;noise mixture model;speech noise evaluation;mixture model estimation;using noise mixture;clean speech noise;simultaneously achieves speaker;noise mixture;speech noise;estimates clean speech;adaptation noise;achieves speaker;mixture model;speaker;noise evaluation;clean speech;speech;model estimation proposed;using noise;joint processing method;estimation proposed method;noise evaluation shows;model estimation;mixture;obtained using noise;proposes joint processing;estimation proposed", "pdf_keywords": ""}, "fcdac45272543b4f8b8eaa59d66044d1b7018494": {"ta_keywords": "data pretrained translation;pretrained translation model;pretrained translation;adapts pretrained translation;forward translation model;train forward translation;parallel data pretrained;translation model generates;translation model;forward translation;translation model validation;pseudoparallel data train;translation model method;pseudo parallel data;generate pseudo parallel;generates pseudoparallel data;multilingual translation;data pretrained;meta learning algorithm;pseudoparallel data;multilingual translation setting;14 multilingual translation;meta learning;pseudo parallel;translation setting;algorithm adapts pretrained;data train forward;parallel data;method meta learning;model generates pseudoparallel", "pdf_keywords": "trained backward translation;trained translation model;model iterative translation;forward translation model;backward translation model;train forward translation;pre trained translation;iterative translation;trained translation;translation model pseudoparallel;iterative translation bt;translation model generate;model optimizes translation;forward translation;translation optimization pre;joint translation optimization;translation model;translation optimization;metabt backward translation;optimizing translation translation;method optimizing translation;translation model present;optimizing translation;forward model training;optimizes translation;learns adjust translation;translation model method;forward model multilingual;backward translation;translation model validation"}, "e8f42dd98d7f546036fa4a1109c3fe3dd98f9647": {"ta_keywords": "natural language argument;argument reasoning comprehension;arguments news comments;argument reasoning;comprehend argument;reasoning comprehension aims;comprehend argument claim;authentic arguments news;arguments news;task argument reasoning;news comments experiments;reasoning comprehension;arguments;language argument claim;language argument;attention language models;crowdsourcing process create;crowdsourcing;aims comprehend argument;argument claim;comments experiments systems;crowdsourcing process;argument;argument claim aim;neural attention language;implicit reasoning options;reasoning;scalable crowdsourcing;reasoning options propose;attention language", "pdf_keywords": ""}, "4358335263622fe189cf95c613f4d6fdcb67fbea": {"ta_keywords": "coupling electron spin;electron spin orbit;zeeman field spin;electron spin;spin orbit coupling;orbit coupling electron;coupling electron;magnetic field spin;effect zeeman field;field spin orbit;field effect zeeman;spin orbit;effect zeeman;field spin;orbit coupling strength;orbit coupling;electron;zeeman field;strength field spin;tuned applied magnetic;zeeman;magnetic field effect;magnetic field;study effect zeeman;spin;coupling strength tuned;coupling strength;applied magnetic field;magnetic;coupling", "pdf_keywords": ""}, "03fff40cff6ac531e340f6ffb376e34609770846": {"ta_keywords": "coordination information warfare;uncover coordination networks;coordination networks accounts;spread online crimes;information warfare;online crimes protests;information warfare scenarios;coordination networks;framework uncover coordination;accounts likely coordinated;online crimes;networks accounts context;uncover coordination;networks accounts;coordination information;networks;uncover groups accounts;crimes protests;kinds coordination information;coordinated;coordination;general network based;likely coordinated;general network;warfare scenarios;introduce general network;crimes protests proposed;groups accounts likely;campaign suppress spread;suppress spread online", "pdf_keywords": "suspicious social networks;detect coordinated campaigns;attacks users coordination;tweets memes attacks;social media identify;detect misinformation inauthentic;patterns information warfare;information warfare;accounts share suspicious;identities use twitter;structural identities twitter;platforms detect misinformation;identities twitter;coordinated tweets accounts;social media coordinated;characterize suspicious social;attack viral webthe;online spreading information;accounts tweets;detect misinformation;identify user tweets;attacks users;patterns social media;coordinated tweets;wide web bots;manipulate social networks;detect bots significantly;behavior erratic tweets;detect bots;share suspicious"}, "eb07ff030df4c3dc20e85d89c2e0d0cc730918a0": {"ta_keywords": "unrolled speaker separation;speaker separation;speaker separation performance;unrolled speaker embedding;speaker separation long;conversations unrolled speaker;reverberant conversations unrolled;method unrolled speaker;improve unrolled speaker;speaker embedding proposed;speaker embedding;reduce unrolled speaker;long reverberant conversations;speaker embedding used;separation long reverberant;unrolled speaker;joint speaker inventory;reverberant conversations;joint speaker;unrolled speaker signal;speaker inventory participating;talkers reduce unrolled;speaker inventory;conversations unrolled;leverages joint speaker;speaker;speaker signal;long reverberant;speaker signal assumed;reverberant", "pdf_keywords": "speaker separation clustering;speaker clustering;based separation speaker;speaker separation;speaker clustering method;new speaker clustering;clustering speaker;clustering speaker proposed;speaker extraction clustering;speaker separation given;extraction clustering speaker;clustering speaker simultaneously;separation given speaker;separation speaker;speech separation method;speaker separation presence;continuous speaker inventory;continuous speech separation;speech separation using;speech separation methods;longitudinal speaker separation;speaker inventory construction;target speaker separation;speech separation;based speaker inventory;speaker inventory mixturewe;separation clustering;speaker inventory proposed;separation speaker candidates;biased speech separation"}, "4c66979a31fa4be5b5814fdb5cb8411572d61da8": {"ta_keywords": "normalization historical text;approach normalization historical;rule based normalization;normalization historical;based approach normalization;based normalization;normalization;approach normalization;linearization historical data;historical text approach;historical data using;historical data;based linearization historical;standard rule based;text approach;historical text;approach based linearization;rule based approach;baseline standard rule;based linearization;respect baseline standard;text approach based;linearization;linearization historical;rule based;accuracy respect baseline;baseline standard;standard rule;respect baseline;text", "pdf_keywords": ""}, "729260566c7fdf689bb04eaaecef59d40da93ef7": {"ta_keywords": "adversarial images deep;detection adversarial images;detection adversarial;facilitates detection adversarial;adversarial images;adversarial;images deep neural;deep neural;images deep;deep neural network;perturbation based detection;neural network dnl;image classification;detection proposed adaptive;classification proposed preprocessing;neural network;based image classification;image classification proposed;dnl based image;learnt stochastic;threshold values learnt;based detection;facilitates detection;based doubly threshold;based detection proposed;detection;neural;learnt stochastic approximation;classification;detection proposed", "pdf_keywords": "adversarial perturbation nonlinear;detection adversarial perturbation;detect adversarial perturbation;detect adversarial image;adversarial perturbation;detecting adversarial image;detect adversarial;components detecting adversarial;detecting adversarial;adversarial detection algorithm;adversarial image need;adversarial image make;able detect adversarial;adversarial image;detection adversarial;problem adversarial image;adversarial image algorithm;adversarial;adversarial detection;detection adversarial images;adversarial image rial;adversarial images;detect existence adversarial;problem adversarial;adversarial attacks agnostic;novel adversarial detection;used detect adversarial;adversarial images based;existence adversarial image;searching adversarial image"}, "ede8ba65c4db10d357d9c3bf8e75b092f536fc84": {"ta_keywords": "vision language navigation;speaker model vision;speaker models integration;vision language;language navigation integrates;model vision language;speaker models;panoramic action spaces;integration panoramic action;augmentation implement pragmatic;panoramic action;language navigation;speaker model outperforms;baseline instruction follower;instruction follower;models integration panoramic;novel speaker model;speaker model;new speaker model;speaker instructions data;navigation integrates;standard speaker models;speaker instructions;novel speaker;speaker;instructions data augmentation;panoramic;navigation;model vision;new speaker instructions", "pdf_keywords": "vision language navigation;language vision navigation;inference improves navigation;instruction context train;language navigation tasks;instruction context follower;natural language instruction;vision language;approach vision language;answer language vision;navigation learning;language vision;navigation tasks;vision andlanguage navigation;context train;improves navigation success;accuracy navigation learning;context train follower;navigation tasks follower;based vision language;language navigation;instruction follower;navigation task based;improves navigation;navigational instructions context;navigation task;room navigation dataset;navigation learning augmented;room navigation;language instruction context"}, "d5634a21b3727258822b78f5c5ababf7261a5c79": {"ta_keywords": "speech enhancement separation;methods speech enhancement;speech enhancement;self supervised learning;supervised learning nir;self supervised;nir methods speech;enhancement separation downstream;supervised;enhancement separation;evaluate self supervised;supervised learning;learning nir methods;learning nir;separation downstream tasks;methods speech;filterbank;separation downstream;learning;nir milky rydalis;outperform baseline features;nir nir milky;nir milky;filterbank fbank;enhancement;hooft filterbank;milky rydalis representations;rydalis representations consistently;speech;representations consistently outperform", "pdf_keywords": "gan speech separation;speech separation generative;separation speech enhancement;speech enhancement separation;network gan speech;speech separation challenge;adversarial network speech;comprehensive speech separation;network speech enhancement;networks speech enhancement;task speech enhancement;speech separation combination;generative networks speech;speech separation;speech enhancement;separation generative;speech separation speech;combination speech separation;gan speech;combination speech enhancement;speech enhancement model;focused speech separation;speakers speech enhancement;separation combination speech;increase robustness speech;separation generative contrastive;separation speech;speech prediction challenging;produced noise speech;speech prediction"}, "3813627f7fec57aa4c15b791e36912f470273bb1": {"ta_keywords": "hashtags topical clusters;clusters hashtags topical;topical clusters hashtags;discussion hashtags;clusters hashtags;discussion hashtags use;twitter content discuss;hashtags topical;quality clusters hashtags;twitter content;content twitter;clusters hashtags shift;utilizes content twitter;content twitter content;form discussion hashtags;topical clusters;content discuss topics;hashtags;online social media;hashtags use;twitter;discuss topics;hashtags use novel;social media;social media users;hashtags shift;hashtags shift course;content discuss;topics form discussion;multi view clustering", "pdf_keywords": ""}, "3e8420d1bebb3f93d285da2de801d2e43b290880": {"ta_keywords": "neural sequence taggers;taggers meta learning;sequence taggers meta;sequence taggers;tagging datasets demonstrate;training meta learning;tagging datasets;meta learning;meta learning techniques;slot tagging datasets;datasets slot tagging;taggers meta;self training meta;meta learning method;training meta;shot training neural;training neural sequence;tagging;pseudolabels experiments benchmark;multilingual ner datasets;learning techniques shot;slot tagging;taggers;learns large amounts;learns;noisy pseudolabels;training neural;learning method learns;noisy pseudolabels experiments;pseudolabels", "pdf_keywords": "training meta learning;labels self training;data meta learning;meta learning tasks;trained language models;labeled data training;weighting meta learning;neural sequence taggers;neural sequence labeling;meta learning;trained language model;meta learning improve;sequence labeling models;meta learning helps;meta learning techniques;propose meta learning;training semi supervised;self training framework;meta learning framework;self training meta;performance meta learning;labeling models specifically;labeling models;training meta;training limited labels;self semi supervised;training neural sequence;reweight data deep;trained language;data training"}, "e51bca890c004c43b25c5a5e7aa968fe70ec2668": {"ta_keywords": "describing hyperlinked web;stacked graphical model;stacked graph hyperlinked;graph hyperlinked web;describing hyperlinked;task describing hyperlinked;graphical models structure;similar hyperlinked web;web page stacked;graphical models;thesis stacked graphical;page stacked graphical;hyperlinked web;graphical model defined;graphical model;class graphical models;structure similar hyperlinked;graph hyperlinked;stacked graphical;graphical model task;hyperlinked web page;model defined stacked;features stacked graphical;defined stacked graph;graphical model class;stacked graph;page stacked;similar hyperlinked;hyperlinked;model class graphical", "pdf_keywords": ""}, "622e05f5d3dd430644288d5048f6050f37947de7": {"ta_keywords": "learning named entity;named entity recognition;entity recognition;structure transfer learning;transfer learning named;transfer learning;entity recognition motivated;similar hierarchical priors;natural language data;prior structure transfer;hierarchical prior structure;learning named;hierarchical priors;hierarchical prior;named entity;novel hierarchical prior;task natural language;similar hierarchical;feature spaces task;prior structure;entity;hierarchical priors smoothed;present novel hierarchical;novel hierarchical;natural language;hierarchical;structure transfer;language data;spaces task natural;language data sets", "pdf_keywords": ""}, "146b84bdd9b9078f40a2df9b7ded26416771f740": {"ta_keywords": "inverse reinforcement learning;based inverse reinforcement;approach inverse reinforcement;inverse reinforcement;gradient based inverse;reinforcement learning decision;reinforcement learning algorithm;reinforcement learning;learning decision;approach inverse;based inverse;novel approach inverse;inverse;learning decision processes;minimize loss function;learning algorithm;minimize loss;agent risk;decision making agent;reinforcement;making agent risk;agent risk sensitive;loss function;learning algorithm seeks;loss function defined;seeks minimize loss;risk;decision processes;decision processes decision;based gradient", "pdf_keywords": ""}, "77899bac8f463b7a77c0c282748e989d419386e7": {"ta_keywords": "formulations semi supervised;predicting semi supervised;semi supervised learning;semi supervised;method predicting semi;predicting semi;supervised learning;supervised learning improvements;supervised;predicting ld formulations;supervised learning combines;automatically predict set;formulations advantage predicting;automatically predict;formulations semi;ld formulations semi;predicting set;predicting ld;predict set;advantage predicting set;method predicting;used automatically predict;predicting;advantage predicting ld;state xmath0 benchmarks;predicting set possible;xmath0 benchmarks;xmath0 benchmarks proposed;predict;learning", "pdf_keywords": "semi supervised learning;semi supervised learners;semi supervised;ensembles semi supervised;weakly supervised learning;strategies semi supervised;weakly supervised;supervised learning results;novel supervised;learning labeled;supervised learning;supervised learners;supervised classification;classifier trained hyperlinked;regularization constraints learners;classification hyperlinked text;class machine learning;hyperlinked text classification;supervised information;classification hyperlinked;specifying learning labeled;supervised;novel supervised label;learning accurate supervised;uses novel supervised;examples distantly supervised;text classification hyperlinked;labeling distantly supervised;classifier trained;supervised learning setting"}, "1a53e7446274016f737236bdd48e3ff05d966384": {"ta_keywords": "snippets natural language;code snippets natural;code snippets;data code snippets;nl code pairs;quality aligned data;natural language nl;code using neural;aligned data code;code pairs using;correlation nl code;code pairs;nl code using;snippets natural;natural language;nl code;mined nl code;extracted correspondence features;language nl method;snippets;quality aligned;data code;language nl;high quality aligned;capture correlation nl;aligned data;hand crafted features;correspondence features obtained;correspondence features;code", "pdf_keywords": "predicting code snippets;code natural language;code mining classifier;code mining;mining relevant code;extracting accurate code;code snippets supervised;predict code snippets;extracting code snippets;extract code theoverflow;extracting relevant code;single code mining;extract code fragments;annotators extracting relevant;data natural language;accurate code predictions;extracting code snippet;predicting code;extracting code;annotators extracting;extract code;code snippets structural;code snippets strongly;annotators extracting aligned;snippet natural language;learning annotated;natural language pairs;structured code snippets;code linear intelligence;100000 code snippets"}, "d0e9c5cb669dec908a38eab4315cbf101bc4b0a0": {"ta_keywords": "adaptation machine translation;domain adaptation machine;selection large language;domain adaptation;machine translation;neural language models;language models data;selection use neural;adaptation domain adaptation;language models effective;language models beneficial;language models;use neural language;models data selection;data selection;adaptation domain;data selection use;neural language;large language pairs;use neural;selection use;language pairs use;models effective selection;language pairs;selection;adaptation machine;beneficial adaptation domain;models beneficial adaptation;large language;translation", "pdf_keywords": ""}, "308eb6751a3a1da0f64f291366c8ee27f84b3f16": {"ta_keywords": "bose einstein condensate;einstein condensate presence;condensate robust magnetic;state dimensional bose;ground state condensate;einstein condensate;condensate presence repulsive;dimensional bose einstein;bose einstein;magnetic field interaction;dimensional bose;repulsive attractive interaction;state condensate;condensate presence;state condensate robust;field interaction strength;effect magnetic;effect magnetic field;condensate robust;magnetic field ground;robust magnetic;interaction strength reduced;magnetic;condensate;attractive interaction interaction;interaction attractive wide;robust magnetic field;interaction strength;interaction strength ground;ground state dimensional", "pdf_keywords": ""}, "2550fafc0cbd8bbf7aadd864ac569596d33db038": {"ta_keywords": "grounding context communication;definition grounding context;grounding context;based notion grounding;notion grounding;definition grounding;grounding;notion grounding powerful;present definition grounding;communication based notion;grounding powerful;definition context communication;context communication;communication based;context communication based;communication;grounding powerful tool;connect text data;connect text;text data;use definition context;text data present;text;definition context;data;definition;tool connect text;use definition;examples use;data present", "pdf_keywords": "grounding cognitive science;definitions grounding cognitive;grounding cognitive sciences;grounding cognitive;grounding concepts;knowledge grounding;grounding concepts human;model grounding concepts;research grounding;grounding concepts paper;definition grounding context;knowledge grounding use;definitions grounding;grounding natural language;successful grounding concepts;grounding language;computational model grounding;grounding context network;network science;grounding linguistic;grounding context;data grounding language;research grounding including;grounding interaction underlying;network science variants;definition grounding;data grounding;grounding real sense;grounding language grounding;grounding entities"}, "7ce80c7df1774e4483b32a813d54a8ff35dd0163": {"ta_keywords": "descent stackelberg equilibria;consider hierarchical game;hierarchical game;stackelberg equilibria;hierarchical game played;games consider hierarchical;gradient descent stackelberg;stackelberg equilibria zero;connections nash equilibrium;nash equilibrium;simultaneous gradient descent;zero sum games;nash equilibrium concepts;descent stackelberg;games consider follower;gradient based update;follower employing gradient;leader follower continuous;simultaneous gradient;sum games consider;game played leader;sum games;follower continuous action;consider hierarchical;gradient descent;attracting critical points;played leader follower;hierarchical;points simultaneous gradient;equilibrium concepts", "pdf_keywords": "stackelberg game learning;stackelberg equilibria continuous;nash stackelberg equilibrium;structure stackelberg game;theory continuous games;stackelberg game;stackelberg equilibria basic;differential stackelberg equilibria;descent stackelberg equilibria;stackelberg equilibrium existence;stackelberg equilibria;stackelberg equilibrium concepts;games learning dynamics;stackelberg game introduce;stackelberg equilibria provide;game learning dynamics;stackelberg equilibrium theory;hierarchical game general;learning strategy gradient;stackelberg equilibria consider;games existence differential;stackelberg game setting;differential stackelberg equilibrium;follower hierarchical game;hierarchical games;stackelberg equilibrium known;stackelberg equilibrium;learning dynamics equivalent;hierarchical game;strategy continuous game"}, "203636315f7c9526189d88c541bedf623d63ea7c": {"ta_keywords": "answer questions ambiguous;question answering task;quality answer questions;measure quality answer;quality answer question;questions ambiguous use;questions ambiguous;question answering;answering task;summary called answer;version question answering;called answer questions;questions ambiguous propose;task called answer;answer question form;answering;answering task called;ambiguous propose measure;question form summary;answer question;answer questions;ambiguous use measure;evaluation measure;new evaluation measure;quality answer;evaluation measure quality;evaluation;called answer;ambiguous use;ambiguous propose", "pdf_keywords": "factoid question answering;question answering;summary ambiguous factoid;novel disambiguation task;ambiguous factoid questions;ambiguous factoid;annotators disambiguation task;factoid question task;disambiguation task;task factoid;disambiguation capable accurately;quality annotators disambiguation;measure task factoid;ambiguous factoid question;question answering release;question task identify;factoid questions;assessment quality retrieval;disambiguation capable;task factoid question;improved answer length;disambiguation task method;new disambiguation capable;disambiguation task task;np generated knowledge;annotators disambiguation;factoid;question answering use;factoid question;316 ambiguous factoid"}, "018bf5da2ba1f1901e98f72c7eedbf6b91967192": {"ta_keywords": "privacy _identified text;approach preservation privacy;_identified text structures;preservation privacy;preservation privacy _identified;_identified text structure;text structures;privacy;privacy _identified;_identified text;text structure;text structures approach;local _identified text;new approach preservation;text;approach preservation;structures approach;structure;structures;local _identified;preservation;structures approach based;variant local _identified;variant local;new approach;present new approach;_identified;application variant local;based application variant;new", "pdf_keywords": "privacy adaptive deep;privacy adaptive representations;private text models;pretraining privacy adaptive;learning approach anonymize;privacy adaptive pretraining;representations private text;pretraining privacy;text data privateized;method pretraining privacy;private text corpora;data privateized;characterizing privacy text;encoder privacy adaptive;privacy adaptation;privacy pretraining;data privacy adaptive;data differentially private;privacy adaptation modified;bert encoder privacy;private text data;privacy utility;data privateized using;data private text;adaptive representations private;privacy utility implicationsin;new privacy adaptive;approach private text;privacy preserving;analyze private text"}, "a6a7374c5ddac1446ceab9d7cbe5a3305238d0ee": {"ta_keywords": "conversation corpora capture;corpora capture conversation;capture conversation turns;capture conversation speakers;trigram conversation turn;capture conversation;turn trigram conversation;conversation corpora;use conversation corpora;filter conversations speakers;conversation turn unit;conversation filtering;filter conversations;conversations speakers;trigram conversation;conversation corpora extracted;present conversation corpora;conversation speakers;conversation filtering introduce;conversation turns speakers;allows filter conversations;conversation speakers use;conversation turn;speakers use conversation;scripts capture conversation;corpora capture;conversation turns;types conversation filtering;conversations;use conversation", "pdf_keywords": ""}, "963c4c34f1292f64a6e9fc04428fc7a0893b8ef3": {"ta_keywords": "enhanced residual attention;residual deep attention;super resolution ssrs;image super resolution;single image super;super resolution;spatial channel attention;deep attention mechanism;residual attention;deep attention;novel residual deep;image super;attention processing module;residual deep;channel attention processing;resolution ssrs;attention mechanism adaptive;attention processing;rescales hierarchical features;channel attention;attention;adaptive reconstruction network;resolution ssrs develop;attention mechanism;resolution;reconstruction network raan;raan single image;rescales hierarchical;automatically rescales;reconstruction network", "pdf_keywords": ""}, "53feb3b34425ea95c259e8d0693edd490d6b470f": {"ta_keywords": "words form backward;backward backward scattering;correlation online social;backward scattering backward;processing mistaken words;backward scattering;backward backward backward;backward backward;social network;backward;scattering backward scattering;scattering backward;online social network;mistaken words form;semantic processing mistaken;online social;mistaken words;social network http;backward scattering framework;correlation online;semantic;social;semantic processing;form backward backward;study semantic processing;study semantic;analysis correlation online;processing mistaken;comparative study semantic;form backward", "pdf_keywords": ""}, "a792d5a1e9a6a53edd8cbc00e387bc07c54e423c": {"ta_keywords": "eliciting truthful responses;truthful responses agents;mechanisms eliciting truthful;answers mechanisms;known answers mechanisms;tasks crowdsourcing;responses agents;answers mechanisms simple;eliciting truthful;crowdsourcing;scale tasks crowdsourcing;eliciting;responses agents absence;agents;mechanisms eliciting;truthful responses;absence known answers;evaluations mechanisms;responses;accuracy evaluations mechanisms;new mechanisms eliciting;agents absence known;intuition improve accuracy;evaluations mechanisms suitable;agents absence;intuitive allow possibility;improve accuracy evaluations;answers;mechanisms simple intuitive;known answers", "pdf_keywords": ""}, "10efdde1ae3a9d359ac1aae0bd5ef7bfd68810dd": {"ta_keywords": "multilingual representation learning;dict mlm multilingual;multilingual language modeling;mlm multilingual language;multilingual representation;mlm multilingual;domain multilingual representation;novel multilingual representation;multilingual language;requirement multilingual representation;multilingual;based dict mlm;dict mlm;domain multilingual;novel multilingual;propose novel multilingual;representation learning multiple;language modeling;analyzing domain multilingual;requirement multilingual;language modeling approach;representation learning method;word predicts domain;spanning 30 languages;representation learning;key requirement multilingual;learning multiple;word predicts;30 languages;representation learning demonstrate", "pdf_keywords": "multilingual language models;lingual alignment existing;lingual training deep;cross lingual alignment;high accuracy multilingual;pre trained multilingual;predict cross lingual;lingual representation learning;lingual alignment;trained multilingual language;crosslingual identification prediction;trained multilingual;accuracy multilingual encoders;language embeddings empirically;predicting language embedded;multilingual encoders;bilingual dictionaries;improve cross lingual;cross lingual resource;lingual resource easily;models use bilingual;bilingual dictionaries purpose;accuracy cross lingual;thousands cross lingual;multilingual encoders present;cross lingual representation;facilitate cross lingual;model predicting language;pairs bilingual dictionaries;bilingual dictionaries dict"}, "ef1d93b03c20b2f488b66e8e2c24fceb2105d58f": {"ta_keywords": "metaphoric expressions languages;characterization metaphoric expressions;metaphoric expressions different;identify metaphoric expressions;metaphoric expressions;characterization metaphoric;identify metaphoric;model identify metaphoric;expressions languages;expressions different languages;model characterization metaphoric;expressions languages establish;metaphoric;xmath1 languages;lingual model lemma;xmath0 xmath1 languages;cross lingual model;lingual model;languages model;different languages;different languages model;languages establish;based cross lingual;languages;cross lingual;languages model based;lingual;languages establish new;lemma equivalence principle;equivalence", "pdf_keywords": ""}, "3993788eb252f5eb7fc19e9f98357a72f9f0476d": {"ta_keywords": "membership network model;mixed membership network;membership network;novel mixed membership;mixed membership controlled;mixed membership;slightly mixed membership;mixed membership encourages;network model based;network model;clusters;balanced clusters;based generative models;model based generative;generative models model;degree mixed membership;membership controlled;generative models;encourages balanced clusters;membership;membership encourages balanced;novel regularization;based novel regularization;membership encourages;regularization;based generative;novel regularization method;network;membership controlled demonstrate;generative", "pdf_keywords": ""}, "71c7104eaed93497824cf197949c77e7d6cb36d3": {"ta_keywords": "domain question answering;question answering iterative;answering framework pullnet;question answering framework;question answering;answering iterative retrieval;retrieval knowledge bases;iterative retrieval knowledge;answering framework;retrieval knowledge;knowledge bases text;pullnet open domain;answering iterative;knowledge bases;iterative retrieval;framework pullnet open;framework pullnet;pullnet open;answering;pullnet;retrieval;context open domain;open domain question;text present experiments;open domain;knowledge;iterative;bases text;empirically demonstrate framework;context open", "pdf_keywords": "learning retrieve reasoning;corpus knowledge base;retrieving knowledge;knowledge bases;knowledge bases questions;answer orneural networks;question subgraph extract;retrieving knowledge relationship;incomplete knowledge base;corpus knowledge;methods retrieving knowledge;question similaritywe;retrieve reasoning;expands question subgraph;retrieve reasoning heterogeneous;question guided subgraph;specifically knowledge bases;facts knowledge base;knowledge base;structure knowledge base;classify knowledge sources;knowledge information neural;text corpus knowledge;reasoning heterogeneous information;web questions dataset;queston answering pullnet;reasoning methods retrieving;relevant question similaritywe;reasoning information text;relationship knowledge sources"}, "911536dc3dfbbbf2bb8d71181b31e0aa7920b9f6": {"ta_keywords": "prediction solutions financial;financial market;mathematics ds method;method discrete time;solutions financial market;financial market proposed;ds method discrete;algorithm able predict;discrete time;able predict values;prediction solutions;algorithm based;predict values;discrete mathematics ds;discrete time dd;ds method;predict values values;method proposed algorithm;mathematics ds;method discrete;algorithm prediction solutions;solutions financial;numerical;proposed algorithm based;algorithm prediction;time dd method;algorithm validated numerical;algorithm based dd;algorithm able;experts proposed algorithm", "pdf_keywords": ""}, "798e45ea830884be36c3f526d3b169eaba95f989": {"ta_keywords": "local nash equilibria;equilibria generically hyperbolic;zero sum games;local equilibria zero;nash equilibria generically;local equilibria;sum games local;results local equilibria;equilibria zero sum;local nash;sum games structurally;games local nash;nash equilibria;equilibria zero;equilibria generically;sum games obtain;sum games;games structurally stable;games local;equilibria;games structurally;generically hyperbolic;generically hyperbolic result;games obtain;games obtain stronger;hyperbolic result extends;hyperbolic result;hyperbolic;setting zero sum;zero sum", "pdf_keywords": "nondegenerate differential nash;genericity differential game;generic local nash;local nash equilibria;local differential nash;continuous games genericity;nash equilibria nondegenerate;existence differential game;existence differential nash;notion differential nash;games theorems stability;game genericity differential;continuous zerosum games;differential nash equilibria;games theorems generic;zerosum games generic;nash equilibria exists;game theorems existence;differential game forms;equilibria continuous games;equilibria differential nash;degenerate differential nash;local equilibria generically;continuous games existence;nash equilibria continuouswe;stability differential nash;nash equilibria continuous;game theory existence;nash equilibria differential;nash equilibria zero"}, "8a0a8568acf2b95c9cb471e28ee6b25c5e4fe186": {"ta_keywords": "stochastic process decision;data stochastic process;data stochastic;based stochastic process;analysis data stochastic;stochasticity process stochastic;stochasticity process;based stochastic;stochastic process;stochastic;process stochastic;approach based stochastic;process stochastic process;stochasticity;characterized stochasticity process;features data stochastic;making characterized stochasticity;characterized stochasticity;process decision making;decision making characterized;process decision;decision making approach;decision making used;decision making;decision making able;extract relevant information;data;process;features data;analysis data", "pdf_keywords": ""}, "e75c388b60cf447be7148be25feeee3e10d12cf4": {"ta_keywords": "high dimensional data;extracting high dimensional;extract high dimensional;high dimensional information;information high dimensional;dimensional information high;arbitrary high dimensional;dimensional data;interpolation training data;high dimensional;dimensional information;dimensional 100 dataset;training data;taken high dimensional;dimensional data demonstrate;dimensional data method;based interpolation training;method extracting high;interpolation training;high dimensional 100;data taken high;extracting high;method extract high;data taken;dataset;information high;training data valid;100 dataset;data;dataset provide", "pdf_keywords": "usefulness dimensionality reduction;intrinsic dimension data;dimensionality reduction context;empirically high dimensional;dimension underlying data;dimensionality reduction;dimension data manifold;experiments usefulness dimensionality;dataset interpolation surely;high dimensional data;data high dimensional;dimensional data high;linear dimensionality reduction;underlying data manifold;dimensionality reduction techniques;usefulness dimensionality;dimensional data;dimensional data representation;data function dimension;100 dataset interpolation;data manifold;dimensional data thein;underlying intrinsic dimension;data manifold claim;dataset interpolation;power dimensionality reduction;dimension data;dimensionality;intrinsic dimension;human models extrapolating"}, "42c3c50b8e368ee2e1b52d010b6c53b3d732770c": {"ta_keywords": "learn sentiment information;learn sentiment;approach learn sentiment;speech sentiment approach;sentiment information written;sentiment information;semi supervised training;based semi supervised;speech sentiment;semi supervised;train semi supervised;end speech sentiment;trained language model;unlabeled speech dataset;sentiment approach;unlabeled speech;large unlabeled speech;pre trained language;sentiment approach advantage;speech dataset training;semi supervised combines;label based semi;speech dataset train;language models;trained language;dataset train semi;speech dataset;language models human;sentiment;texts using pre", "pdf_keywords": "supervised speech sentiment;speech sentiment classifier;semi supervised sentiment;speech sentiment analysis;models speech sentiment;sentiment analysis transcripts;domain speech sentiment;training sentiment classifier;training sentiment;speech sentiment approach;transcripts based sentiment;explore speech sentiment;speech sentiment;e2e speech sentiment;perform speech sentiment;supervised sentiment;semi supervised speech;supervised sentiment analysis;classify sentiment;able classify sentiment;end speech sentiment;sentiment classifier;approach training sentiment;asr based sentiment;classify sentiment accuracy;annotation speech;annotation speech dataset;sentiment analysis;human annotation speech;sentiment analysis second"}, "5dcbdb9bf80575953b5d21f378d8139f0a44168b": {"ta_keywords": "recognition user emotion;identification user emotion;identified user emotion;user emotion using;user emotion;user emotion result;filtering emotion;emotion using;support vector machine;vector machine svm;user emotion subsequent;filtering emotion triggers;svm;svms proposed method;machine svm;emotion using support;svms proposed;svm method based;svm method;svm achieve;emotion subsequent filtering;vector machine;svms;automatic recognition user;machine svm method;subsequent filtering emotion;recognition user;emotion result;emotion result shows;emotion triggers", "pdf_keywords": ""}, "bc632f81dab322ac610a8d11463cc1bba6130eda": {"ta_keywords": "historical text normalization;text normalization datasets;text normalization;normalization datasets languages;multi task learning;languages training data;improvements languages training;task learning;zero shot learning;normalization datasets;normalization;sequence based historical;significant improvements languages;task learning configurations;languages training;improvements training data;shot learning outperforms;shot learning;improvements languages;training data abundant;datasets languages;minimal improvements training;multi task;63 multi task;datasets languages observe;sequence based;historical text;training data target;strong identity baseline;training data", "pdf_keywords": "text normaliza tasks;historical text normalization;tasks autoencoding graph;auxiliary tasks autoencoding;text normalization datasets;text normalization;normalization single task;normalization historical text;tasks including normalization;normalization tasks;machine translation normalize;normalization tasks multi;learning effective normalization;autoencoding graph phoneme;learning models normalization;tasks autoencoding;text normalization use;normalization text normaliza;normalization graph phoneme;normalization datasets languages;normalization historical documents;better autoencoding generally;normalization text;approach normalization text;languages using autoencoding;multi task learning;better autoencoding;task learning;translation normalize historical;normalize historical documents"}, "13b674bb3078623608045a18570b47f6e49a8358": {"ta_keywords": "visual question answering;identifying paintings descriptions;art results task;identifying paintings;contains images paintings;paintings descriptions present;paintings descriptions;method identifying paintings;identify artistic;identify artistic work;images paintings;computer identify artistic;art results;artistic work given;paintings;images paintings tapestries;artistic work;paintings tapestries sculptures;paintings tapestries;textual description dataset;question answering;contains images;covering centuries artistic;artistic;art;sculptures;textual description;tapestries sculptures;artistic movements;given textual description", "pdf_keywords": ""}, "4b9795493a937b9034be9c26afab23f6dc751f62": {"ta_keywords": "retrieval based language;text based retrieval;retrieval automaton;retrieval automaton automaton;probability natural language;retrieval based;based retrieval based;construct retrieval automaton;based retrieval;based language models;language models;natural language text;language model;language text based;language model inference;language models method;probabilistic model datastore;natural language;retrieval;model datastore search;standard language model;language text;datastore search;text based;construct retrieval;novel probabilistic model;language text used;modeling probability;automaton;modeling probability natural", "pdf_keywords": "automaton neural language;neural language models;information extraction automaton;neural language model;language models;language modeling nn;simple unstructured data;natural language text;corpus domain automaton;language modeling;search unstructured data;words database;natural language processing;data unstructured;unstructured data sets;unstructured data;extraction automaton neural;automaton neural;unstructured database based;language models challenging;language models essential;words database significantly;words database method;autoregressive language modeling;unstructured database capable;unstructured data unstructured;unstructured database;extract automaton neural;unstructured data set;search unstructured"}, "a7b6802f20c399615dbac161678cd6a6d2df5a97": {"ta_keywords": "quality data learning;data learning rank;learning rank models;rank models;learning rank;quality data;high quality data;machine learning;hybrid machine learning;machine learning data;business impact planck;data learning;learning data;rank models use;quality control online;data mining pipeline;planck strasbourg germany;data mining;learning data mining;models;germany use human;models use human;impact planck strasbourg;planck strasbourg;quality control;rank;quality quality control;use human loop;learning;gather high quality", "pdf_keywords": ""}, "7df6aa19f50c8ec5f12d58e0685ed5c6e9a08bb2": {"ta_keywords": "estimates user preferences;estimate preferences users;estimate preferences;based user preferences;user preferences product;user preferences using;preferences using model;preferences product;preferences product recommended;generate stochastic models;user preferences;preferences using;stochastic models text;preferences users using;preferences users;used estimate preferences;recommended based user;generate stochastic;stochastic models;preferences;method generate stochastic;text generated efficient;models text;algorithm estimates user;information text generated;stochastic;text generated;models text used;text used estimate;product recommended based", "pdf_keywords": ""}, "a3e3a9d878999c7038c275e75f5cd8a232aa4999": {"ta_keywords": "task diversity super;pre trained models;robustness pre trained;semantic generative capabilities;task specific trainable;task diversity;models increasing task;generative capabilities pre;pretrained model;benchmark evaluating semantic;increasing task diversity;pretrained model parameters;task diversity difficulty;capabilities pre trained;semantic generative;diversity super;heads task diversity;trained models;generative capabilities;evaluating semantic generative;trained models increasing;pre trained;trained models shifts;pretrained;trainable heads task;types tasks entails;freezing pretrained model;task specific;types tasks;specific trainable", "pdf_keywords": "benchmark pretrained neural;supervised speech;sg benchmark pretrained;benchmark pretrained;speech enhancement tasks;tasks speech translation;supervised speech language;speech representations;availableself supervised speech;speech representation;tasks include speech;task speech segmentation;new tasks speech;generative capability speech;domain automatic speech;automatic speech;speech representations fundamental;pre trained models;task speech;pretrained models superb;speech representation decoding;speech processing hear;domain speech conversion;capability speech enhancement;pretrained models;speech representations based;tasks speech;approach speech representation;tasks spontaneous speech;investigate task speech"}, "b5002aa334f8d0c0e1a4dedad79580e10a928c30": {"ta_keywords": "spectral feature xmath0;self supervised learning;spectral feature;resource speech signal;combine spectral feature;xmath0 self supervised;supervised learning xmath1;low resource speech;learning xmath1 representations;self supervised;framework combine spectral;speech signal processing;representations low resource;combine spectral;supervised learning;supervised;speech signal;resource speech;feature xmath0 self;spectral;low resource datasets;learnable interpretable framework;feature xmath0;resource datasets;mixture experts based;learning xmath1;xmath1 representations;xmath1 representations low;resource datasets addi;learnable interpretable", "pdf_keywords": "attention based fusion;features speech model;supervised learning speech;hybrid attention model;grained features speech;models quan attention;layered attention model;quan attention model;learning speech representations;learning speech processing;features speech;hybrid attention;attention model recognition;combines attention based;attention based representations;resource speech tasks;attention model;speech processing tasks;attention model attention;features self supervised;model attention model;layered attention;propose hybrid attention;learning speech;fusion based learnable;speech tasks;speech representations;attention based;speech processing;layer layered attention"}, "8809d0732f6147d4ad9218c8f9b20227c837a746": {"ta_keywords": "convolution augmented powerformer;end speech processing;augmented powerformer;augmented powerformer approach;speech processing;powerformer approach combines;powerformer approach;powerformer;speech processing problem;xmath0 power powerformer;data power powerformer;end speech;combines power powerformer;power powerformer;architecture convolution augmented;proposed architecture convolution;end end speech;powerformer power entropic;powerformer demonstrated;powerformer demonstrated comparing;powerformer power;power powerformer power;power powerformer demonstrated;input data power;architecture convolution;convolution augmented;power entropic model;power entropic;convolution;speech", "pdf_keywords": "end speech processing;frequency mask speech;speech processing;model speech based;model speech;mask speech given;speech extend power;mask speech;speech processing applications;learning convolution conformer;convolution conformer trained;speech novel scalable;conformer model speech;speech based elementary;speech given mixture;given speech;convolutional detector conformer;speech processing applicationswe;propose convolutional detector;learning convolution;convolutional learning model;speech extend;predictions conformer convolutional;end speech;speech based;learning convolutional;comprehensive study convolutional;given speech extend;convolutional learning convolution;detector consists convolutional"}, "d8252e24b6036ca895800b547698ab44d09ae350": {"ta_keywords": "personal information management;searches personal information;improve personal information;performance searches personal;searches personal;personal information;improve performance searches;machine learning used;information management tools;machine learning;information management;performance searches;evidence machine learning;improve personal;searches;used improve personal;learning used improve;circumstances machine learning;management tools;learning used;used improve performance;improve performance;management tools use;evidence machine;information;performance;personal;experimental evidence machine;learning;machine", "pdf_keywords": ""}, "f91c24b0dc56a6b377e99e046d7540e5bb7aa46e": {"ta_keywords": "student information management;student information;college student information;information management;design college student;student;college student;presents design college;design college;college;management;information;design;paper presents design;presents design;paper presents;paper;presents", "pdf_keywords": ""}, "a6a7724763d8adba466519489b0b9d209e7f2d15": {"ta_keywords": "text generation;text text generation;evaluate generated text;generated text;text generation problem;novel text evaluation;generated text text;text evaluation;generated text evaluate;description text sequence;text evaluation method;description generated text;text sequence sequence;text sequence;based description generated;description generated;based description text;evaluate resulting text;evaluate generated;generated;text evaluate;description text;resulting text;generation problem method;text text;text;text evaluate resulting;generation problem;metric evaluate generated;learning metric evaluate", "pdf_keywords": "supervised text generation;text generation evaluation;models text generation;text generation;automatically generating text;text generation context;supervised text;text generation based;predict semantics text;generating text;generating text given;methods text generation;text text generation;evaluation generated text;generated text;sentences train models;empirical neural summarization;predicting semantics;predicting semantics natural;sentences train;neural summarization evaluation;generated text text;natural language models;text generation problem;sentences sentences train;captures linguistic similarity;trained sequence sequence;high quality sentences;approach predicting semantics;predict semantics"}, "e3f1a9c3d87e9828cdeb08ba90a260c69e974a75": {"ta_keywords": "dance generation audio;generates basic dance;dance sequence;basic dance generation;dance sequence experimental;target dance sequence;deep recurrent method;basic dance;supervised deep recurrent;dance generation;audio input deep;generation audio;time basic dance;deep recurrent;dance;basic dance steps;dance steps;supervised deep;decodes target dance;weakly supervised deep;recurrent method;input deep;dance steps low;audio;lds process audio;process audio;target dance;deep lds layer;long term memory;generation audio power", "pdf_keywords": "neural networks dance;dnns generating dance;long dance sequences;music trained dance;dance sequences;motion music tasks;input motion beat;music motion control;dance sequences proposed;music input motion;beat retrieval models;precision motion beat;music motion;beat improve encoder;deep encoder;precision generated dance;generate long dance;music beat retrieval;method music motion;pattern motion beat;track music beat;dance piece music;performance motion music;motion music;generated dance evaluations;encoder deep;networks dnns achieving;generated dance;generating dance piece;beat retrieval"}, "78d57a1ecd724c5f8b1534372969d5b35daa6d4b": {"ta_keywords": "models constituency parsing;constituency parsing achieves;constituency parsing;neural models constituency;search generative neural;direct search generative;models constituency;generative neural;parsing achieves state;search generative;generative neural models;parsing;parsing achieves;constituency;generative;neural models;direct search;neural;pt explicit model;algorithm direct search;pt explicit;pt;numbers pt;models;algorithm direct;search;art numbers pt;results pt explicit;explicit model;explicit model combination", "pdf_keywords": "search generative parsers;generative parsers;models constituency parsing;hybrid parsers reranking;2016 generative parsers;parsers reranking;constituency parsing achieve;constituency parsing;parsers using training;performance hybrid parsers;constituent parser increase;base parsers;novel constituent parser;base parsers decoding;parsers reranking procedure;constituent parser;constituent parsing;generative parsers dyer;accuracy constituent parsing;hybrid parsers;beam search generative;search effective generative;outputs base parsers;generative models improves;parsers;general applied parsing;parsers decoding;power constituent parsing;constituent parsing process;search generative"}, "b6c6e06b4bc68349845b30e01e01d7603f468547": {"ta_keywords": "optimal relay location;optimal placement relay;placement relay nodes;optimal relay;formulas optimal relay;placement relay;multi relay channel;relay nodes multi;relay channel;relay nodes;relay nodes straight;relay channel study;relay location;source node relay;multi relay;node relay;relay node;multi hop wireless;node single relay;problem optimal placement;wireless network line;node relay node;relay node provide;formulas multi relay;nodes multi hop;hop wireless network;optimal placement;wireless network;single relay;relay", "pdf_keywords": "relay source optimal;optimal relay location;optimal relay placement;optimal relay locations;relay location optimal;optimal relay;theorems optimal relay;relays theorems optimal;optimum relay location;relay optimal;theorem optimal relay;study optimal relay;relay prove optimal;relay placement wireless;optimal distance relay;formulas optimal relay;optimal placement relay;optimal position relay;optimum relay locations;theorems optimum relay;relay placement distributed;optimum distance relay;placement multi relay;constraints source relay;relay nodes channel;relay placement multi;single relay placement;placement relay nodes;multi relay channel;optimum position relay"}, "7729fbebff327bebb9292dc1c51c51dd55390954": {"ta_keywords": "tensors transitive verb;tensors sentence similarity;rank tensors sentence;tensors transitive;rank tensors transitive;tensors sentence;tensors comparable;comparable tensors;low rank tensors;comparable tensors occasionally;transitive verb construction;verb construction performance;sentence similarity verb;rank tensors comparable;tensors comparable tensors;tensors;sentence similarity;rank tensors;tensors occasionally surpassing;unconstrained rank tensors;tensors occasionally;verb construction;transitive verb;similarity verb disam;similarity verb;verb;transitive;verb disam;sentence;surpassing unconstrained rank", "pdf_keywords": ""}, "f3271e61dc0507183ee399393129d7888c2f82b9": {"ta_keywords": "supervised quality estimation;automatic quality estimation;supervised quality;quality estimation approaches;lightly supervised quality;quality estimation evaluate;quality estimation;fully automatic quality;annotated scores evaluation;automatic quality;framework lightly supervised;annotated scores;estimates quality reliably;annotation effort;framework estimates quality;annotation effort low;estimates quality;keeping annotation effort;using annotated scores;lightly supervised;quality reliably using;supervised;evaluate using annotated;quality reliably;annotation;approaches keeping annotation;annotated;using annotated;scores evaluation;quality", "pdf_keywords": ""}, "68aa7c7b65365c3303d5024b1273408fb435d178": {"ta_keywords": "entrainment dialogue;effect entrainment dialogue;entrainment changes dialogue;entrainment dialogue acts;dialogue acts lexical;dialogue affects human;dialogue affects;dialogue acts choice;users acts entrainment;dialogue;interaction choice dialogue;dialogue acts;dialogue define;dialogue acts given;entrainment;entrainment fundamental;changes dialogue;choice dialogue;entrainment fundamental aspect;choice dialogue acts;entrainment changes;effect entrainment;entrainment measure;changes dialogue define;aspect dialogue affects;measure entrainment;entrainment measure various;investigate effect entrainment;acts entrainment changes;acts entrainment", "pdf_keywords": ""}, "b0ddd849c5ae0004678fa483908c06d87894f3ab": {"ta_keywords": "isolated word recognition;hidden hidden model;hidden model;hidden model ss;model ss hmm;criterion hidden hidden;hidden hidden hidden;speech social model;standard model speech;model speech;model speech social;word recognition;selection criterion hidden;hidden hidden;probabilistic selection criterion;based variational probabilistic;hmm proposed based;variational probabilistic approach;new probabilistic selection;hidden;criterion hidden;variational probabilistic;probabilistic selection;data isolated word;probabilistic approach proposed;new probabilistic;probabilistic approach;ss hmm proposed;speech social;proposed based variational", "pdf_keywords": ""}, "b0d644277933988c00b22d8ae012512fe498ad62": {"ta_keywords": "word sense inventories;trained word embeddings;word embeddings;fasttext word embeddings;fledged word sense;trained fasttext word;perform disambiguation;sense inventories;pre trained fasttext;word embeddings grave;perform disambiguation task;disambiguation task;word sense;trained fasttext;pre trained word;disambiguation;sense inventories 158;fasttext word;languages using original;inventories 158 languages;trained word;languages basis original;158 languages using;embeddings;158 languages basis;languages basis;languages using;fasttext;fully fledged word;languages", "pdf_keywords": "embedding word sense;create sense embeddings;word sense induction;sense disambiguation languages;word sense disambiguation;unsupervised word sense;sense embeddings trained;sense embeddings;disambiguation demonstrate embeddingswe;word sense languages;sense disambiguation fully;sense disambiguation;trained word embeddings;word embeddings based;sense disambiguation demonstrate;word sense inventories;word sense inventory;trained word vectors;sense disambiguation downstream;sense disambiguation performs;introduce word sense;enabling word sense;word embeddings;word embeddings perform;word embedding;trained word sense;fledged word sense;perform word sense;sense languages approach;multilingual word sense"}, "ebc64974e9e0021984a0158b3c04b60327730a88": {"ta_keywords": "answering based retriever;base question answering;question answering based;question answering;knowledge base;flexible scalable retriever;scale knowledge base;scalable retriever like;answering based;based retriever approach;scalable retriever;knowledge base question;retriever approach;large scale knowledge;functionality retriever like;retriever approach used;retriever like framework;retriever items database;retriever like transducers;based retriever;retrieve relevant;query data;relevant retriever;relevant retriever items;approach used retrieve;functionality retriever;approximate data query;integrate functionality retriever;retrieve relevant retriever;database", "pdf_keywords": ""}, "f0cd4de3cdf547dcdcc6995dca9ab3f65955b324": {"ta_keywords": "recurrent highway networks;layer recurrent models;recurrent models limited;recurrent models;lattice recurrent unit;multi layer recurrent;lattice recurrent;layer recurrent;grid recurrent highway;grid recurrent;models neural;learning deep;called lattice recurrent;recurrent unit lru;models neural networks;recurrent unit;comparing grid recurrent;learning deep multi;recurrent highway;deep multi layer;highway networks;deep multi;language models;challenge learning deep;accurate language models;networks called lattice;neural networks;neural;lattice;recurrent", "pdf_keywords": "layer recurrent models;multi layer recurrent;recurrent network;recurrent neural network;recurrent network ability;train recurrent network;layer recurrent;recurrent models limited;trained state recurrent;memory recurrent neural;recurrent models;recurrent neural;sequence recurrent neural;use recurrent neural;state recurrent neural;deep neural;neural networks generalized;short term memory;learning deep;represented sequence learning;neural network sequence;generalized linear neural;deep multi layer;memory recurrent;long term memory;network sequence learning;network learned sequence;deep neural networks;train recurrent;linear neural networks"}, "485b3f77b9913e151e7ca897d99497e70e7f30d1": {"ta_keywords": "aexhaustive vocabulary using;aexhaustive vocabulary;element aexhaustive vocabulary;vocabulary using training;add new vocabulary;improvements rare words;words way embeddings;aexhaustive;smaller general subwords;vocabulary using;subwords adding larger;new vocabulary;vocabulary;subwords;general subwords adding;general subwords;element aexhaustive;granularity element aexhaustive;vocabulary following held;subwords adding;new vocabulary following;embeddings larger;vocabulary following;words;rare words;embeddings larger units;embeddings;way embeddings larger;training pass incrementally;rare words way", "pdf_keywords": "machine translation vocabulary;neural machine translation;perform deep translation;model training pipeline;machine translation;improves translation accuracy;translation vocabulary using;granularity translation accuracy;vocabulary using training;achieving high translation;translation vocabulary;machine translation algorithm;vocabulary synthesis;improves translation;deep translation;approach translation words;generate new vocabulary;high translation accuracy;nonlinear machine translation;new vocabulary quickly;new vocabulary prediction;generation new vocabulary;training pipeline;bpe granularity translation;optimizing segmentation granularity;translation words hybrid;machine translation nmt;granularity segmentation neural;vocabulary prediction new;segmentation neural machine"}, "1e4e2aceed87febcc643f1473507c9535ba5c19a": {"ta_keywords": "volume efficient mean;efficient mean field;mean field model;mean field;efficient mean;field model volume;model volume efficient;volume efficient;perform volume efficient;model volume;field model used;field model;streaming version volume;present blockwise streaming;perform volume;mean;version volume efficient;blockwise streaming;volume;used perform volume;field;blockwise streaming version;streaming;version volume;streaming version;model;model used;efficient;model used perform;paper present blockwise", "pdf_keywords": "translation attention based;simultaneous speech translation;based translation attention;attention based translation;translation attention;speech translation sst;speech translation;feature simultaneous speech;simultaneous translation model;end speech translation;feature simultaneous translation;streaming volume enriched;speech translation lplp;cross lingual encoding;translation sst tasks;joint attention based;lingual encoding;translation model;novel simultaneous speech;simultaneous speech;encoder;encoder improve;improve encoder convergence;simultaneous translation;simultaneous simultaneous speech;speech translation quadratic;improve encoder;translation denoise focus;lingual encoding method;models streaming volume"}, "6e24bcfcdb31afbb313a13c1c84cb779ceb17500": {"ta_keywords": "stochastic algorithm estimating;propose stochastic algorithm;stochastic algorithm;actions person algorithm;easy implement stochastic;implement stochastic;decision maker based;taxi driver;algorithm estimating reference;stochastic processes algorithm;taxi driver data;person algorithm based;algorithm tested taxi;observed actions person;implement stochastic processes;decision making processes;based observed actions;propose stochastic;decision maker;estimating reference;stochastic;stochastic processes basis;person algorithm;tested taxi driver;hidden model stochastic;estimating reference point;point decision maker;algorithm estimating;processes basis decision;model stochastic", "pdf_keywords": ""}, "0d20360c5d533760d97d7ce19b78d4791a5173cb": {"ta_keywords": "predicting terrorist targets;method predicting terrorist;algorithm predicting terrorist;predicting terrorist;dynamics terrorist attacks;terrorist targets results;network terrorist events;terrorist targets based;terrorist attacks;terrorist targets;meta network terrorist;terrorist events;inference algorithm predicting;terrorist events test;terrorist attacks use;network based inference;network terrorist;heterogeneous dynamics terrorist;dynamics terrorist;algorithm predicting;terrorist;method predicting;inference algorithm;predicting;learning models;new method predicting;machine learning models;based inference algorithm;targets results algorithm;based inference", "pdf_keywords": ""}, "5d9fe38b750f59b4ef0a2b58fde0f60d4317c7ad": {"ta_keywords": "multidomain learning data;multidomain learning attribute;approach multidomain learning;multidomain learning;simultaneous multidomain learning;multiple metadata attributes;novel approach multidomain;multiple metadata;data partitioned domains;learning attribute attributes;metadata attributes approach;using multiple metadata;domains using multiple;metadata attributes;learning data data;data classification;multidomain;data classification approach;approach multidomain;learning attribute;learning data;based simultaneous multidomain;data classification used;metadata;partitioned domains using;partitioned domains;simultaneous multidomain;performance data classification;domains using;attribute attributes approach", "pdf_keywords": ""}, "242c35b91fe1d7aedab9d1da7652aad2219d4784": {"ta_keywords": "high dimensional models;training high dimensional;dimensional models;models;high dimensional;approach training high;novel approach training;approach training;training high;training;dimensional;novel approach;present novel approach;high;novel;present novel;approach;present", "pdf_keywords": ""}, "924e43c4de98743d2e7c14c241b03b2109325b90": {"ta_keywords": "distributing collapsed sampling;collapsed sampling multiple;collapsed sampling;sampling multiple processors;sampling multiple;sampling;memory efficient method;efficient method distributing;method distributing collapsed;correct memory efficient;processors method;memory efficient;distributing collapsed;multiple processors method;statistically correct memory;text based variant;processors method applied;efficient method;variant variant variant;multiple processors;variant variant;based variant variant;correct memory;method distributing;processors;distributing;memory;variant;based variant;collapsed", "pdf_keywords": ""}, "030fade3049e0847702393dde3100ecc41a5e86a": {"ta_keywords": "searching locally optimal;locally optimal performance;optimal performance element;local optimum arbitrarily;optimal performance;optimum arbitrarily;performance random test;locally optimal;local optimum;optimal;performance element arbitrarily;optimum arbitrarily high;performance random;performance element discrete;close local optimum;algorithm returns performance;optimum;discrete space performance;performance element;performance elements formulate;performance elements;random test cases;returns performance element;searching locally;consider problem searching;algorithm;searching;basis performance random;algorithm returns;performance", "pdf_keywords": ""}, "bea54062d105b9fe3250ce3569cf817e54772894": {"ta_keywords": "automatic recognition phrases;historical german automatically;recognition phrases historical;german phrases historical;phrases historical german;historical german phrases;probabilistic parser trained;historical corpora german;probabilistic parser;using probabilistic parser;recognition phrases;german phrases;historical corpora;corpora german evaluation;german automatically;unlexicalized parser outperforms;sequence labeling approach;sequence labeling;corpora german;parser trained sequence;phrases historical;outperforms sequence labeling;parser trained;parser outperforms sequence;unlexicalized parser;german automatically recognized;historical german;parser;parser outperforms;85 historical corpora", "pdf_keywords": "annotations phrase recognition;phrase recognition;automatic recognition phrases;evaluation phrase recognition;phrase annotations;phrase recognition historical;phrase recognition output;phrase annotations constituency;annotated corpora;study annotated corpora;phrase annotations paper;phrase annotations phrase;recognition phrases;parser outperforms neural;trained phrase output;trained parser;annotated corpora historical;neural sequence labeling;use phrase annotations;probabilistic parser evaluation;phrase types data;extract phrase types;parser trained;class phrase annotations;historical corpora accuracy;recognition phrases historical;train parser;based sequence labeling;unlexicalized parser outperforms;parser generalizations historical"}, "38705aa9e8ce6412d89c5b2beb9379b1013b33c2": {"ta_keywords": "semiparametric inference deep;semiparametric inference;study semiparametric inference;develop semiparametric inference;semiparametric inference context;semiparametric inference variety;use semiparametric inference;inference deep neural;semiparametric;inference deep;study semiparametric;develop semiparametric;theory develop semiparametric;convergence deep feedforward;inference variety semiparametric;semiparametric situations;semiparametric situations including;bounds deep networks;use semiparametric;deep networks apply;convergence deep;deep networks;deep neural;emphasis use semiparametric;deep neural networks;rates convergence deep;deep feedforward;deep feedforward neural;variety semiparametric;neural networks provide", "pdf_keywords": "asymptotic inference deep;semiparametric inference approximation;bounds nonparametric estimation;nonasymptotic bounds nonparametric;semiparametric inference;estimating nonparametric regression;develop semiparametric inference;study semiparametric inference;estimating nonparametric;sufficient semiparametric inference;nonasymptotic bounds deep;loss functions nonparametric;learning context semiparametric;context semiparametric inference;bound sufficient semiparametric;nonparametric estimation;semiparametric inference focusingwe;semiparametric causal inference;rate convergence deep;semiparametric inference establish;functions nonparametric regression;theorems asymptotic inference;deep networks theorems;asymptotic inference theorems;sufficient semiparametric;bounds deep nets;general nonparametric regression;convergence deep feedforward;networks use semiparametric;inference deep"}, "4dd85ae17a5fd0bce09ffef0455b6e827d7e1e2b": {"ta_keywords": "attack networked cyberphysical;cyberphysical systems attack;polynomially distributed attack;networked cyberphysical systems;distributed attack networked;networked cyberphysical;distributed attack;cyberphysical systems;attack formulated constrained;network noisy model;attack networked;estimate attack performance;systems attack based;underlying network noisy;network noisy;systems attack;estimate attack;attack performance attack;design polynomially distributed;attack performance;cyberphysical;attack formulated;performance attack formulated;optimal design polynomially;used estimate attack;attack based;performance attack;method optimal design;attack based use;optimal design", "pdf_keywords": "attacker estimation process;attacks secure estimation;secure estimation networked;attacker estimation;injected attacker estimation;systems attacks optimal;stochastic approximation attack;linear attack scheme;attack scheme distributed;linear attack attacker;distributed estimation networked;propose linear attack;distributed estimation;attacks optimal;estimation networked control;attack distributed;secure estimation;nodes attack detection;problem distributed estimation;attacks optimal sense;attack distributed cyber;attack based optimization;robustness attack noise;attack algorithm simultaneous;distributed estimationwe;systems attacks;linear attack;attack algorithm;attack extended distributed;attack timescale stochastic"}, "5143ebd23322fe805bed2667fcfb70920c105f7f": {"ta_keywords": "demonstrations cognitive modeling;effective demonstrations cognitive;demonstrations cognitive;characteristics effective demonstrations;effectiveness effective demonstrations;demonstrations ordering training;effective demonstrations determined;demonstrations determine efficiency;demonstrations determine;cognitive modeling;effective demonstrations;cognitive modeling characteristics;generates cognitive model;demonstrations determined level;demonstrations determined;level demonstrations determine;cognitive model;determined level demonstrations;demonstrations ordering;level demonstrations ordering;level demonstrations;demonstrations;efficiency simulated student;simulated student;student generates cognitive;generates cognitive;simulated student generates;sequence level demonstrations;simulated;cognitive", "pdf_keywords": ""}, "cfbe9183f2fe2847f7a3c811f6309a2cab3f85cf": {"ta_keywords": "scientific summarization dataset;extractive summarization scientific;summarization scientific documents;summarization dataset investigate;summarization scientific;released scientific summarization;existing summarization datasets;scientific summarization;summarization datasets report;bert based extractive;summarization datasets;summarization dataset;pretraining bert based;intermediate pretraining bert;pretraining bert;leverages existing summarization;based extractive summarization;extractive summarization;existing summarization;summarization;bert based;contextualized word embeddings;word embeddings trained;word embeddings;bert;scientific documents derive;pretraining interacts contextualized;scientific documents;datasets report;embeddings trained", "pdf_keywords": ""}, "20086d6a9fab6081f300e08d3f952cb9b16e6de8": {"ta_keywords": "search reference database;linear indices search;indices search;indices search reference;database indexed;reference database;database indexed using;reference database implementation;indexed using;search reference;super linear indices;resulting database indexed;indexed using perfect;indexed;known benchmarks implementation;implementation based hash;known benchmarks;search;linear indices;database implementation;database;indices;hash function implementation;perfect hash;performs known benchmarks;database implementation based;hash;benchmarks implementation;perfect hash function;benchmarks", "pdf_keywords": ""}, "693f5d55e0561099944f5e00e301bf26db0b972d": {"ta_keywords": "semantic analysis stochastic;semantic analysis;semantic;models task semantic;task semantic;task semantic analysis;stochastic models trained;training evaluation stochastic;evaluation stochastic models;stochastic model evaluation;evaluation stochastic;corpus;stochastic models task;stochastic models;analysis stochastic models;stochastic model;analysis stochastic;available corpus huygens;corpus huygens;available corpus;stochastic;output stochastic model;describes training evaluation;models trained;models trained input;data underlying informative;publicly available corpus;models task;output stochastic;informative noisy", "pdf_keywords": ""}, "939a149f156425b83e48ea72e9e09a55ea33b8d7": {"ta_keywords": "wavelet transform domain;wavelet transform;crossing representation wavelet;signal reconstruction representation;representation wavelet transform;signal reconstruction based;domain signal reconstruction;signal reconstruction;representation wavelet;algorithm signal reconstruction;wavelet;reconstruction representation based;reconstruction based iterative;reconstruction representation;zero crossing representation;transform domain signal;formulation projections convex;stabilized zero crossing;domain signal;convex sets stabilized;projections convex;proposed representation stabilized;projections convex sets;reconstruction based;novel formulation projections;equation algorithm signal;representation stabilized;transform domain;algorithm signal;formulation projections", "pdf_keywords": ""}, "0fbb90b8fe1d02a4f0f616df9a09ec42eace53bd": {"ta_keywords": "online unsupervised classification;unsupervised classification proposed;online unsupervised learning;unsupervised classification;unsupervised learning approach;unsupervised learning;method online unsupervised;online unsupervised;based online unsupervised;classification proposed;classification proposed proposed;classification;unsupervised;recording condition robust;remote recording condition;remote recording;learning approach expected;approaches remote recording;learning approach;method based online;recording condition;recording;method online;database demonstrate proposed;mean value ess;learning;new method online;based online;database;mean value", "pdf_keywords": ""}, "e21633b0b5e55dce56bc07e919c6d12ecf8cef0c": {"ta_keywords": "logic language determinate;learn logic language;logic programs;learn logic;logic language;classes logic programs;clauses constant depth;used learn logic;restriction logic programs;language determinate clauses;determinate clauses constant;logic programs used;logic systems;logic programs called;determinate clauses useful;locality classes logic;locality pac learnable;clauses constant;learnable constant locality;determinate clauses;clauses useful resource;pac learnable constant;new restriction logic;logic systems used;learnable constant;restriction logic;clauses useful;classes logic;resource logic systems;language determinate", "pdf_keywords": ""}, "c305e3314c0853b14911f704c68b04cfc9ea7aa1": {"ta_keywords": "pnnian dependency corpus;corpus languages;dependency corpus languages;conversion pnnian dependency;conversion baryon;corpus languages based;baryons conversion;principle conversion baryon;baryon conversion;conversion baryon baryon;baryons conversion used;baryon baryon conversion;baryon conversion used;conversion pnnian;corpus;accuracy baryons conversion;dependency corpus;present conversion pnnian;accuracy baryons;build family baryons;pnnian dependency;baryon baryon;baryon;languages;baryons;family baryons;baryons degree;degree accuracy baryons;languages based;baryons degree accuracy", "pdf_keywords": "dependencies treebank;language hindi treebank;dependencies treebank suitable;typed dependencies treebank;hindi treebank;syntactic dependency relations;treebank linguistic;treebanks d_ based;developed syntactic annotation;mapping syntactic dependency;treebank map syntactic;automatic treebanks d_;syntactic annotation scheme;treebank linguistic dataset;automatic treebanks;treebanks;syntactic annotation;syntactic dependency;treebanks d_;novel automatic treebanks;hindi treebank map;syntactic trees structural;dependencies linguistic;treebank suitable;syntactic structures corresponding;treebank;conversion treebank linguistic;treebank suitable use;corresponding syntactic structures;syntactic trees"}, "652579315d767331d8e05ea46489e6bd081ef48a": {"ta_keywords": "evaluation students;evaluation students classroom;approach evaluation students;classroom based pairwise;pairwise comparisons argue;based pairwise comparisons;pairwise comparisons;evaluation;comparisons;comparisons argue approach;comparisons argue;approach evaluation;new approach evaluation;students;students classroom;cardinal approaches compared;pairwise;approaches compared;based pairwise;students classroom based;evaluators;expertise evaluators;argue approach significantly;classroom;cardinal approaches;conventional cardinal approaches;classroom based;approach significantly;compared;approaches compared lack", "pdf_keywords": ""}, "b293e4659e20815bcf0b6d31ce46b8bd9437c1fa": {"ta_keywords": "privacy machine learning;machine learning private;learning based privacy;private machine learning;learning aided privacy;learning private machine;privacy attack corresponding;privacy attack;privacy machine;state art privacy;privacy protection;interactions privacy machine;aided privacy protection;privacy;privacy issues solutions;aided privacy;learning private;privacy protection iii;interactions privacy;categories interactions privacy;based privacy attack;privacy issues;machine learning survey;iii machine learning;based privacy;machine learning ii;private machine;art privacy issues;machine learning aided;private", "pdf_keywords": "privacy machine learning;learning based privacy;learning aided privacy;applications privacy ml;privacy data supervised;machine learning private;privacy deep learning;private machine learning;privacy ml;learning private machine;prediction privacy;predict privacy data;privacy ml investigating;predicting privacy data;prediction privacy managementas;able predict privacy;privacy based deep;privacy datasets;protecting privacy datasets;survey privacy ml;predicting privacy;predict privacy;private deep learning;privacy ml divide;protection machine learning;assessment prediction privacy;learning ml encryption;used predict privacy;machine learning attack;detect privacy data"}, "8d17543c20f23b6a40bec9334d50e9c15a08c1c4": {"ta_keywords": "extracting answers;extracting answers question;model extracting answers;subgraph containing text;question specific subgraph;entity linked text;subgraph containing;specific subgraph containing;containing text entity;text entity linked;linked text model;text entity;specific subgraph;linked text;subgraph;extracting;containing text;entity linked;model extracting;text model competitive;text model;novel model extracting;answers question;answers question specific;entity;text vastly outperforms;tested using text;answers;text;using text vastly", "pdf_keywords": "graphs knowledge base;underlying knowledge base;knowledge representation graph;knowledge extraction embeddings;knowledge bases using;knowledge bases;using knowledge bases;knowledge base;structure knowledge bases;heterogeneous graphs knowledge;knowledge extraction;large text corpus;learning knowledge representation;knowledge bases kbs;text corpus;semantics documents ability;knowledge base represent;open domain knowledge;rich knowledge representation;linguistic structure knowledge;semantics documents predict;knowledge base use;entities graph embedding;domain knowledge extraction;knowledge representation;knowledge base facts;text sentences builtwe;facts text sentences;graph heterogeneous update;semantics documents"}, "1808b64aec21863489f0fe66f250890a3ac2b843": {"ta_keywords": "random noise secure;noise secure malicious;shares random noise;noise secure;generation random noise;random noise generation;noise generation random;verifiable secret sharing;unbiased random coins;distributed random noise;random coins;random bits use;random noise method;random bits;secret sharing;secure malicious participants;distribution shares random;shares random;random noise;unbiased random bits;shares unbiased random;generation shares random;random noise introduces;noise generation;secure malicious;verifiable secret;random coins fewer;generation random;distributing shares unbiased;executions verifiable secret", "pdf_keywords": "privacy preserving statistical;generate privacy preserving;privacy preserving histograms;privacy preserving;random noise secure;privacy generate;implementation privacy preserving;generate privacy;problem privacy generate;privacy mechanism bit;generation privacy preserving;distributed implementation privacy;problem generation privacy;noise secure malicious;new privacy mechanism;used generate privacy;implementation privacy;privacy mechanism;privacy generate shares;present new privacy;publicly available randomized;randomization certain;new privacy;related construction randomizing;randomization;construction randomizing;noise secure;generate noise database;problem privacy;generation privacy"}, "c3a662b864673d8cc7469051419ab8819926d4b0": {"ta_keywords": "necessary bert multilingual;bert multilingual;bert linguistic properties;languages necessary bert;properties bert linguistic;bert linguistic;bert multilingual illustrate;architectural properties bert;properties bert;necessary bert;findings experiments languages;bert;linguistic properties languages;languages;multilingual illustrate findings;linguistic properties;experiments languages;multilingual;multilingual illustrate;linguistic;languages small setup;languages necessary;experiments languages small;languages small;properties languages;properties languages necessary;synthetic natural data;novel pre training;trained mix synthetic;identifies architectural", "pdf_keywords": "multilinguality bert models;stronger multilinguality bert;multilinguality bert;multilingual bert;modifications multilinguality bert;multilinguality bert mbert;multilingual bert crosslingual;algorithm multilingual bert;influence multilinguality bert;efficiently multilinguality model;bert models trained;efficiently multilinguality;multilayer representations multilingual;small bert models;use efficiently multilinguality;bert models;multilinguality machine;multilingual tasks multilayer;multilinguality resulting embedding;multilinguality machine learning;translation extend bert;represent multilinguality machine;multilingual machine learning;models multilingual;lingual representation training;robustness multilingual machine;bert models demonstrate;classical models multilingual;translation models multilingual;training data multilinguality"}, "b1d24e8e08435b7c52335485a0d635abf9bc604c": {"ta_keywords": "textual sources fever;assertions textual sources;assertions textual;verifying assertions textual;sources fever;textual sources;textual;dataset verifying assertions;new dataset verifying;dataset verifying;verifying assertions;fever;assertions;verifying;sources;present new dataset;new dataset;dataset;present new;present;new", "pdf_keywords": "verify claims wikipedia;fact extraction verification;identify evidence wikipedia;extraction verification claims;extraction claims fact;fact extraction;discovering textual evidence;claims fact checking;verification claims generated;evidence claims extracted;based extraction claims;wikipedia claims classified;dataset claim verification;claims wikipedia page;fact checking website;extract true claims;textual evidence unstructured;claims extracted information;claims wikipedia;detection claim method;fact checking;fake news dataset;wikipedia subsequently verified;evidence wikipedia page;evidence wikipedia;generate claims verified;extraction claims;sections wikipedia claims;wikipedia claims;sentence entity claim"}, "4a4bc9f6c5ec76b0d501b641d3092aceb2e083bd": {"ta_keywords": "dynamics bar restaurant;modal user interface;dynamics bar;analyze dynamics bar;user interface;user interface pages;bar restaurant based;modal user;analyzing dynamics bar;bar restaurant;dynamics bar generate;bar generate;person modal;modal modal user;interface user interface;modal modal;bar generate user;modal;person modal modal;bar;restaurant based use;novel person modal;restaurant;restaurant based;interface pages;user interface user;record user interface;interface pages used;interface user;user interface set", "pdf_keywords": ""}, "d5eeaac5c5e524ad05d9b1f3f3f41aece082955a": {"ta_keywords": "variance speech recognition;robust probabilistic predictive;probabilistic predictive classification;speech recognition;variational bayes probabilistic;probabilistic framework speech;based variational bayes;probabilistic predictive;problem speech recognition;speech recognition combines;variational bayes;speech recognition error;framework variational bayes;introduce robust probabilistic;variance speech;framework speech recognition;robust probabilistic;variance variance speech;variational bayes bpc;speech recognition illustrate;predictive classification;robust training;probabilistic framework predictive;training problem speech;novel probabilistic;robust training problem;predictive classification method;propose novel probabilistic;based variational;bayes probabilistic", "pdf_keywords": ""}, "04d96a75b4383240cb15fb729b29f5775219d724": {"ta_keywords": "library parallelized neural;parallelized data mining;decision trees parallelized;parallelized neural automatic;parallelized neural;library parallelized;parallelized library parallelized;parallelized library;parallelized data library;trees parallelized library;parallelized distributed decision;parallelized library used;parallelized data;present parallelized library;encode parallelized data;encode parallelized;optimally encode parallelized;use parallelized data;parallelized;distributed decision trees;use parallelized;parallelized data underlying;parallelized distributed;speech recognition asr;library 448 parallelized;trees parallelized;data mining library;speech recognition;neural automatic speech;automatic speech recognition", "pdf_keywords": "deep learning decoder;deep learning library;np based deep;deep learning machine;machine translation np;parallel machine translation;parallelization based deep;deep learning model;open source deep;source deep learning;speech encoder;deep learning;deep learning form;deep learning accomplished;analyzed deep learning;deep learning deep;deep learning framework;decoder based neural;deep learning devkar;speech encoder decoder;learning decoder;model deep learning;parallel unstructured neural;learning decoder named;supports speech encoder;language model parallelized;deep convolutional parallel;learning deep learning;neural network;machine translation"}, "312b12dd6aa558b92df3ddd9b1057aa80a0ad718": {"ta_keywords": "shot relation classification;relation classification systems;relation classification;relation classification leverages;zero shot relation;zero shot classification;textual entailment models;textual datasets;textual entailment;shot relation;existing textual entailment;textual datasets enhance;entailment models;performance relation classification;available textual datasets;entailment models leverage;shot classification;relation;zero shot;textual;shot classification performance;f1 zero shot;available textual;readily available textual;entailment;approach zero shot;existing textual;leverages existing textual;classification leverages existing;datasets improved", "pdf_keywords": ""}, "5885625fac055f4f8f47b0d6b5c026c8806896f0": {"ta_keywords": "stochastic process;dynamics stochastic process;stochastic processes;stochastic process resulting;dynamics stochastic;compared stochastic process;stochastic process method;stochastic;study dynamics stochastic;works stochastic processes;method works stochastic;compared stochastic;works stochastic;chunk self similarity;results compared stochastic;property given process;given process;concept chunk self;processes;process;process method based;process method;self similarity;self similarity property;chunk self;process method applied;dynamics;given process method;process resulting;concept chunk", "pdf_keywords": ""}, "098076a2c90e42c81b843bf339446427c2ff02ed": {"ta_keywords": "influence functions deep;influence functions neural;accuracy influence functions;influence functions;influence estimates;functions deep learning;influence estimates vary;accuracy influence estimates;network models trained;functions deep;convex loss functions;neural network models;deep learning non;deep learning;failures influence functions;influence;loss functions;learning non convex;functions neural network;accuracy influence;loss functions provide;convex loss;models trained;benchmark accuracy influence;neural network;non convex loss;functions neural;network models;failures influence;neural", "pdf_keywords": "deep learning influence;influence functions deep;learning influence functions;functions deep learning;deep networks mnist;learning models deep;deep learning extensive;deep learning models;learning deep networks;models deep;deep models;learning deep;deep models use;lower deep networks;deep learning fragile;deeper networks;model deep network;networks trained;model deep;deep learning;estimates model deep;functions deep network;deep networks;gradient stochastic estimation;networks trained regularization;deep learning sg;deeper exact hessian;empirical discovery deep;relu networks models;influence function network"}, "446f1eaec22a90574670491073cd5b03bfa1e273": {"ta_keywords": "claims statistical properties;statistical claims properties;statistical properties countries;simple statistical claims;statistical claims;simple claims statistical;claims statistical;annotating data;identifies simple statistical;annotating data property;numerical information text;knowledge base raw;properties countries freebase;statistical properties;knowledge base;extracting numerical information;16 statistical properties;using knowledge base;instead annotating data;information text;statistical properties problem;countries freebase approach;verification simple claims;supervised baseline;supervised baseline approach;annotating;simple statistical;statistical;text experiments;distantly supervised baseline", "pdf_keywords": ""}, "76468db928e18f97dadbd25c04c80ebd491fec9b": {"ta_keywords": "xmath2 transition metal;xmath0 transition metal;xmath1 crystal;order xmath1 crystal;xmath1 crystal field;transition metal pyrochlore;metal pyrochlore splitting;splitting xmath2 transition;transition metal crystal;xmath2 transition;pyrochlore splitting;splitting xmath0 transition;pyrochlore splitting order;field splitting crystal;splitting xmath2;pyrochlore rare earth;field splitting xmath2;metal pyrochlore rare;crystal field splitting;splitting order xmath1;metal pyrochlore;splitting crystal field;splitting crystal;metal crystal field;xmath0 transition;transition metal cations;discussed transition metal;field splitting xmath0;splitting xmath0;xmath2", "pdf_keywords": ""}, "aaf7e94e1a2f8891e5c5f4d11d4f135a1687bb0a": {"ta_keywords": "mechanical model assembly;tabs mechanical oscillator;oscillator mechanical;mechanical oscillator mechanical;spring locked mechanical;assembly macroscopic structure;mechanical oscillator;tabs mechanical;model assembly macroscopic;model mechanical oscillator;capture tabs mechanical;mechanical oscillator model;assembly macroscopic;locked mechanical mechanical;mechanical model mechanical;simple mechanical;oscillator mechanical mode;coupled mechanical;simple mechanical model;mechanical model;model mechanical;locked mechanical;tabs spring locked;mechanical mechanical;present simple mechanical;simple modification mechanical;model assembly;mechanical mode coupled;mode coupled mechanical;modification mechanical model", "pdf_keywords": ""}, "c589c4ec7247980f38a6bd22f215fea8028a0f66": {"ta_keywords": "annotated human systems;decisions annotated human;explaining decisions annotated;accuracy explaining decisions;workers accuracy reasoning;accuracy reasoning;decisions annotated;accuracy reasoning improved;accuracy reasoning highly;annotated human;human systems using;reasoning improved;accuracy explaining;annotations;human systems;accurate decision making;decision making accurate;annotated;workers accuracy;explaining decisions;reasoning;analysis annotations;reasoning highly subject;qualification workers accuracy;making accurate understanding;errors decision making;accurate understanding errors;accurate decision;careful analysis annotations;reasoning improved careful", "pdf_keywords": "crowdsourced annotation;crowdsourced annotation natural;wethe crowdsourced annotation;human annotators trained;annotated human annotators;learning accuracy annotations;amazon mechanical turk;datasets human annotations;annotation dataset human;estimating accuracy annotations;human annotations;obtain accurate annotations;accuracy annotations tasks;annotators trained;accurate annotations interpretable;experiments using crowdsourced;automatically predict annotation;annotation quality;human annotators;human annotations obtained;high quality annotations;annotations quality;annotated learning;annotations obtained crowd;annotate annotations based;accurate annotations;annotations based;annotations tasks;accuracy annotations annotations;datasets annotate annotations"}, "ecc520794da34d2b141235002c70b06c999bda73": {"ta_keywords": "existing sense embeddings;sense embeddings;interpretable sense representations;discovering interpretable sense;coherence evaluation text;sense representations;new coherence evaluation;sense representations coherent;discovering interpretable;evaluation text embeddings;text embeddings;coherence evaluation;optimized discovering interpretable;text embeddings minimal;new coherence;interpretable sense;coherence;present new coherence;embeddings;coherent existing sense;embeddings minimal;representations coherent existing;existing sense;embeddings minimal model;representations coherent;interpretable;representations;evaluation text;optimized discovering;text", "pdf_keywords": ""}, "c12e46d7d0bb9fa062bce0549f2a6a9de00758d5": {"ta_keywords": "sound event detection;sound event;novel sound event;event detection sase;attention mechanism transformer;transformer encoder proposed;transformer encoder;event detection;mechanism transformer encoder;encoder proposed sase;encoder proposed;encoder;novel sound;input feature;propose novel sound;input feature sequence;self attention mechanism;incorporates self attention;context information input;sound;transformer;attention mechanism;detection sase;self attention;information input feature;event;detection sase method;attention;feature sequence;gennes task4 dataset", "pdf_keywords": ""}, "f2fb4a931580c4f1c5bdb47ebc80c801b422cd1a": {"ta_keywords": "logic dynamics neural;logic dynamics network;dynamics separating logic;separating logic dynamics;dynamics neural network;separate logic dynamics;logic dynamics separating;separating logic;produce logic dynamics;dynamics neural;separate logic;use neural network;able separate logic;logic dynamics;segregate logic dynamics;dynamics network;neural network;neural network perform;logic dynamics different;use neural;method separating logic;neural network use;dynamics network able;dynamics separating;segregate logic;network use neural;produce logic;perform logic tasks;perform logic;network perform logic", "pdf_keywords": ""}, "57b972ebe314cfe8e57fd6b9f9239123eb70e979": {"ta_keywords": "encoders la lexicon;cnns encoders la;speech recognition;deep convolutional neural;automatic speech recognition;cnns encoders;speech recognition problem;convolutional neural networks;automatic speech;deep convolutional;free automatic speech;networks cnns encoders;neural networks cnns;networks cnns;la lexicon free;use deep convolutional;lexicon free automatic;convolutional neural;encoders la;neural networks la;cnns;lexicon free;study convolutional neural;neural networks;comparative study convolutional;study convolutional;la lexicon;convolutional;encoders;speech", "pdf_keywords": "speech recognition convolutions;speech using convolutional;talk speech recognition;speech recognition explore;networks rnns;speech recognition;connectionist temporal classification;decoding conversational;decoding conversational telephone;recurrent neural networks;automatic speech;encoders convolutional ctc;neural networks rnns;deep convolutional neural;predict outcome convolutional;conversational telephone speech;speech recognition typically;predict outcome convolutionalwe;automatic speech recognition;convolutional neural networks;outcome convolutional neural;networks rnns explore;telephone speech using;convolutional ctc models;convolutional layers trained;method decoding conversational;telephone speech;convolutional neural network;convolutional neural;approach automatic speech"}, "6f79cbe893ae46a6f97617d14656ab57c26e6faf": {"ta_keywords": "directional automatic speech;speaker data end;multi speaker data;explicit speaker locations;speaker data;models source speaker;speaker locations asr;field multi speaker;automatic speech recognition;source speaker locations;speech recognition;automatic speech;speaker locations;speech recognition asr;speaker locations improves;source speaker;multi speaker;provides explicit speaker;explicit speaker;speaker;e2e neural network;neural network manner;handling far field;far field multi;neural network;recognition asr;directional automatic;far field;end e2e neural;recognition asr explicitly", "pdf_keywords": "end speech recognition;multi speaker data;separating speech sources;speaker data end;speaker data;speech extraction network;speech recognition source;models source speaker;directional automatic speech;speaker locations asr;field speech recognition;field multi speaker;source speaker locations;speech recognition;source speaker;robust speech extraction;speech recognition asr;far field speech;explicit speaker locations;speech recognition objectives;automatic speech recognition;noisy speech data;speech data;advanced speech recognition;multi speaker;speaker locations;speech extraction;speech sources noisy;robust speech;overlap speech extraction"}, "5cb74e269c57263d475734a66d34e4d2d2f9e1ac": {"ta_keywords": "vibration characteristics sample;modal test ffm;element method ffm;evaluate vibration characteristics;test ffm method;finite element method;used evaluate vibration;method ffm compared;ffm method used;vibration characteristics;evaluate vibration;ffm method;using modal test;method ffm;modal test;ffm compared sample;test ffm;element method used;vibration;using finite element;element method;values finite element;constructed using modal;finite element;ffm compared;using modal;experimental values finite;experimental values;modal;agreement experimental values", "pdf_keywords": ""}, "cefd3993db4d065b95ab8f105452fb728c02b60e": {"ta_keywords": "generates scientific reviews;reviews scientific paper;targeted author reviews;automatically generates scientific;scientific reviews scientific;author reviews improve;scientific reviews;reviews improve;peer reviews train;reviews improve quality;peer reviews;pass peer reviews;reviews scientific;author reviews;natural language processing;reviews train targeted;natural language;generates scientific;reviews train;reviews;quality generated text;scientific paper;generated text;improve quality generated;automatically generates;scientific paper employs;quality generated;generated text suffer;language processing;train targeted author", "pdf_keywords": "summarization automatic review;automatic review generation;summarizing summarizing reviews;summarizing reviews;generate detailed reviews;automatic scientific reviews;analysis generated reviews;generate structured reviews;generate reviews scientific;summarizing reviews randomized;automatically generate reviews;evaluation generated reviews;generated reviews extensively;reviews train summarization;constructed automatic review;structured scientific reviews;generate reviews;review generation task;review summary reviews;generated reviews model;review generation automatically;reviews generated automatic;generated reviews;summary reviews paper;automatic summarization scientific;generates review review;generated automatic summarization;automatic review help;model generate reviews;describing scientific review"}, "f491a5f09ee01436d772a6cff25f22d700d8c9c0": {"ta_keywords": "power flow analysis;approach power flow;distributed generator dg;power flow;distributed generator;nodal analysis method;augmented nodal analysis;challenges distributed generator;generator dg characteristics;nodal analysis;nodal analysis mana;dynamic augmented nodal;generator dg;modified augmented nodal;augmented nodal;flow analysis;flow analysis incorporates;dg characteristics demand;generator;demand management voltage;voltage support proposed;analysis mana method;management voltage support;nodal;voltage support;method dynamic augmented;management voltage;analysis method;approach power;dg characteristics", "pdf_keywords": ""}, "72213e24713264da816f43a42d606f115998fe7b": {"ta_keywords": "stacked graphical learning;graphical learning stacked;collective classification stacked;learning stacked graphical;classification stacked information;algorithm stacked learning;stacked learning;classification stacked;learning stacked;graphical learning collective;online stacked graphical;classify instances stacked;learning algorithm stacked;stacked graphical model;stacked graphical;learning collective classification;collective classification;instances stacked graphical;stacked information;stacked information extraction;stacked algorithm;graphical learning algorithm;stacked algorithm study;graphical learning;application stacked graphical;algorithm stacked;version stacked graphical;graphical learning prove;online stacked;learning collective", "pdf_keywords": ""}, "c04c865c8b33ce0251c9f37d0cccf2b3b1e4fd34": {"ta_keywords": "network reinforcement learning;policies network reinforcement;network reinforcement;learn policies network;network nodes predicted;evolutionary status network;efficiently train agents;policies network;agents predict evolutionary;train agents predict;nodes predicted;reinforcement learning;efficiently learn policies;predicted nodes;nodes predicted nodes;predicted nodes used;reinforcement learning problems;network nodes;agents predict;network nodes demonstrate;status network nodes;network wide navigation;nodes;predict evolutionary;predicting evolutionary;nodes demonstrate;train agents;nodes used efficiently;learn policies;predict evolutionary status", "pdf_keywords": "learning causal states;learning partially observable;predictive state representations;partiallyobservable models causal;learning representation causal;states partiallyobservable models;learning approximate causal;prediction causal states;learned causal states;reinforcement learning partially;prediction partial observability;causal states representations;conditional states partiallyobservable;representations psps partiallyobservable;causal states train;causal states learn;observable policy networks;causal state representation;supervised learning causal;psps partiallyobservable models;partial information markov;representations conditional states;partiallyobservable models;partiallyobservable models generalize;learning partial information;generalize predictive state;partially observable policy;learning causal;observation conditional entropy;learn underlying state"}, "8504a5eb4638aeb2f61f8b7f93440b9e495b443b": {"ta_keywords": "centralized tracking process;algorithms centralized tracking;problem centralized tracking;centralized tracking;tracking process;tracking process unknown;unknown parametric distribution;parametric distribution problem;parametric distribution considered;parametric distribution;distribution considered algorithms;process unknown parametric;distribution problem optimal;tracking;optimal algorithms centralized;algorithms centralized;problem centralized;distribution considered;distribution;centralized;distribution problem;unknown parametric;parametric;problem optimal algorithms;optimal algorithms;deals problem centralized;process unknown;process;considered algorithms;algorithms", "pdf_keywords": ""}, "8e7d063c681557c94382ff3da6415d3720fe11a7": {"ta_keywords": "corpus tweets;2016 corpus tweets;scalar signals text;conditional encoding;2016 twitter task;conditional encoding signal;based conditional encoding;detection scalar signals;signals text based;twitter task;text based conditional;signals text;detection scalar;bidirectional encoding;tweets;twitter task approach;results 2016 twitter;2016 twitter;text;2016 corpus;text based;bidirectional encoding signal;twitter;include bidirectional encoding;scalar signals;results 2016 corpus;encoding;corpus;encoding signal;signal encoding", "pdf_keywords": "predicting stance tweet;detection stance tweets;detect stance tweets;encoding stance detection;stance tweets approach;tweets introduce supervised;predicting stance;tweets approach learns;target stance tweets;stance weakly supervised;successful predicting stance;task stance detection;tweet target representations;stance tweets;approach recognising tweets;tweet corpus propose;recognising tweets;stance tweet particular;predict target stance;stance tweets previously;stance tweet;tweet corpus;detection target stance;corpus detection stance;dependent embeddings tweets;neutral class tweets;dependencies target tweets;termed stance detection;tweets approach;classifying attitude expressed"}, "8568f6eda2e4cb7921fe175ab44b2f5ecbb2b870": {"ta_keywords": "errors human readers;misspellings error rates;readers unimpaired comprehension;eye tracking study;report eye tracking;reading difficulty;eye tracking;words cause reading;human readers unimpaired;cause reading difficulty;words contain errors;occurring misspellings;human readers;word based surprisal;reading difficulty correct;difficulty correct words;contain errors human;error words;errors human;misspellings;unimpaired comprehension;naturally occurring misspellings;misspellings error;error words cause;occurring misspellings error;words present computational;errors error words;readers unimpaired;uses character based;word based", "pdf_keywords": ""}, "ae77189921ffade5ee4c4d4a0e93e879d7280b80": {"ta_keywords": "predicting pose multimodal;pose multimodal body;body virtual avatars;virtual avatars accurately;pose multimodal;multimodal body virtual;virtual avatars;predicting pose;virtual avatars using;predict pose;model predicting pose;avatars accurately data;avatars using data;able predict pose;multimodal body;ability predict pose;model predict pose;pose given body;avatars accurately;pose;predict pose given;given body virtual;body virtual;avatars using;body model;multimodal;pose given;body model able;generative model;given body model", "pdf_keywords": ""}, "93e012cbf8e29aacb9654313250a81d53bbcbdf2": {"ta_keywords": "detection email leaks;email leaks based;email leaks proposed;email leaks;associated email leaks;detection email;leak detection task;leak recipients learn;leak recipients;simulated leak recipients;leak detection;based leak detection;redefining leak detection;recipients outliers;method detection email;patterns associated email;unintended recipients outliers;leaks based;leak detection technique;real email examples;leaks based leak;email examples enron;outlier detection;recipients outliers combine;outlier detection task;leaks proposed method;email examples;leaks proposed;leaks;detection task outlier", "pdf_keywords": ""}, "df99459a75328393a9a989498db46ec445335724": {"ta_keywords": "review data privacy;peer review data;release peer review;data privacy preserving;privacy preserving;privacy preserving manner;utility data privacy;data privacy;privacy data;data privacy data;peer review;privacy;review data;release peer;utility data;peer;framework release peer;data;tradeoff utility data;release;review;preserving;results improve utility;utility;results;utility tradeoff utility;results improve;tradeoff utility;preserving manner;utility tradeoff", "pdf_keywords": "consider privacy preserving;privacy preserving;privacy preserving manner;improving privacy utility;data privacy preserving;algorithm improving privacy;output privacy mechanism;privacy mechanism convex;ensuring privacy reviewer;computing privacy;privacy preserving datathis;algorithm computing privacy;privacy algorithm;privacy utility tradeoff;privacy utility;review data privacy;privacy reviewer;differential privacy;output privacy;use privacy preserving;differential privacy algorithm;preserve differential privacy;privacy review propose;privacy review;privacy reviewer identities;privacy preserving laplace;privacy algorithm simple;privacy mechanism;commonly released privacy;ensuring privacy"}, "c4bc2f7e04e02107aa6eaa0c811c3c046efbbc14": {"ta_keywords": "hardness learning simple;hardness learning;result hardness learning;alphabet hard cryptographic;hard cryptographic;learning simple;hard cryptographic assumptions;simple learning;learning simple learning;alphabet hard;simple learning problem;learning problem examples;nontrivial structure learning;problem examples strings;present result hardness;result hardness;learning problem;fixed alphabet hard;cryptographic;problem examples nontrivial;examples strings;hardness;examples strings fixed;structure learning simple;strings;problem examples;cryptographic assumptions;examples nontrivial;strings fixed alphabet;alphabet", "pdf_keywords": ""}, "69b184f62c97513b03deed96a1443f79b34af0d7": {"ta_keywords": "bidding assignment attacks;paper bidding assignment;approach paper bidding;paper bidding;efficacy paper bidding;bidding assignment robust;bidding assignment;assignment attacks jeopardize;assignment attacks;integrity review process;assignment robust attacks;bidding;review process;integrity review;attacks jeopardize integrity;assignment robust;review process develop;jeopardize integrity review;paper;review;approach paper;robust attacks;integrity;assignment;attacks;efficacy paper;robust attacks current;attacks current approaches;attacks jeopardize;novel approach paper", "pdf_keywords": "adversarial bid manipulation;assignments malicious reviewers;allow adversarial reviewers;adversarial reviewers;malicious reviewers paper;adversarial reviewers review;potential adversarial bid;adversarial bid;malicious reviewer manipulate;bidding assign reviewers;malicious reviewers predictions;score malicious reviewers;papers bid reviewer;allows malicious reviewer;defenses bid manipulation;reviewers colluding adversary;bids reviewers;malicious reviewers;attacks reviewers paper;reviewers aware malicious;adversary modifies bids;bid reviewer;bid manipulation attacks;bids received reviewers;features bids reviewers;malicious reviewer;manipulation attacks reviewers;attacks reviewers;manipulation bids papers;learning based bidding"}, "b21fa4f31c4813444e50259dfbe2c56660161174": {"ta_keywords": "statistical distributions xmath0;distributions xmath0 xmath1;distributions xmath0;xmath8 xmath9 components;xmath9 components xmath10;xmath1 components xmath2;xmath9 components;xmath10 xmath11;xmath1 components;xmath3 components xmath4;xmath10 xmath11 components;xmath0 xmath1 components;components xmath8 xmath9;xmath8 xmath9;xmath2 xmath3 components;xmath2 xmath3;xmath12 xmath13;xmath11 components xmath12;components xmath2 xmath3;xmath12 xmath13 components;xmath0 xmath1;xmath11 components;components xmath10 xmath11;components xmath4;components xmath12 xmath13;xmath4 xmath5 components;xmath9;components xmath2;xmath13;xmath12", "pdf_keywords": ""}, "89fd287f7eacc7a40d0216ba3b919812da658b94": {"ta_keywords": "latent dynamics spatiotemporal;spatiotemporal superresolution using;spatiotemporal superresolution;dynamics spatiotemporal superresolution;inference latent dynamics;latent dynamics;empirical pattern formation;dynamics spatiotemporal;adaptation multidimensional pattern;selective backpropagation time;spatiotemporal;backpropagation time;multidimensional pattern recognition;superresolution using selective;selective backpropagation;empirical pattern;backpropagation;based empirical pattern;multidimensional pattern;backpropagation time use;using selective backpropagation;pattern formation approach;accurate inference latent;pattern recognition;pattern formation;method inference latent;lsda adaptation multidimensional;superresolution using;adaptation multidimensional;inference latent", "pdf_keywords": "training neural dynamics;latent dynamics neuralthe;neural dynamics;neural dynamics models;neural dynamics irregularly;observations neural dynamics;backpropagation time;dynamics neurons coupled;encoded dynamics neurons;network coupled neurons;time learned transfer;selective backpropagation time;learned transfer network;dynamics neuralthe;dynamics neurons;coupled brain network;observed data neurons;brain encoded dynamics;coupled neurons;neural dynamics key;dynamics neuralthe sparse;relevant neural dynamics;model dynamics brain;neural activity simulated;data neurons;neural dynamics 80;quadratic time learned;neurons coupled brain;backpropagation;modeling neural"}, "c5950fa3ee124cf2dcb8783db6f582f49170fb45": {"ta_keywords": "empirical validation unlabeled;data guarantee generalization;guarantee generalization valid;generalization gap valid;guarantee generalization;generalization valid;generalization gap;unlabeled data guarantee;valid arbitrary models;models validate empirical;unlabeled data bound;bound valid models;bound generalization gap;validation unlabeled data;arbitrary models valid;leverage unlabeled data;generalization valid train;arbitrary models;generalization;validate empirical validation;validation unlabeled;valid models;empirical validation;validate empirical;train bound generalization;bound generalization;models validate;models valid models;unlabeled data;valid models validate", "pdf_keywords": "learning error deep;bound deep learning;error deep learning;classifier bounded empirical;bounds supervised learning;generalization bound deep;bound early learning;error empirical classifiers;deep learning leveraging;data deep;empirical classifiers;generalization bounds supervised;empirically practical deep;supervised learning directly;guarantees general learning;bound unlabeled data;training bound true;empirical error classifier;empirical classifiers binary;deep learning;data deep neural;guarantees unlabeled data;data stable learning;labeled empirical error;bounds supervised;machine learning theorems;learning leveraging;training bound;error classifier bounded;stochastic model trained"}, "a7822238f5db7d62731eaeabf9725a65f4edf893": {"ta_keywords": "ground state xmath0he;theory dft ground;equilibrium gft model;state xmath0he xmath1he;thermal equilibrium gft;functional theory dft;dft ground state;density functional theory;xmath0he xmath1he calculations;xmath1he calculations;xmath1he calculations performed;state xmath0he;theory dft;dft model numerical;equilibrium gft;dependent density functional;dft model;xmath1he;predictions dft model;density functional;xmath0he xmath1he;gft model;xmath0he;gft model based;dft ground;time dependent density;predictions dft;dependent density;generalized thermal equilibrium;dft", "pdf_keywords": ""}, "9cdf512f273083efa1ea01f7b31daa97a7bbe884": {"ta_keywords": "coded computation multivariate;coded computation schemes;codes computational locality;coded computation;codes computational;locality codes computational;present coded computation;coded computation leverage;design coded computation;model coded computation;results coded computation;studied locality codes;workers coded computation;locality codes design;coded computation lens;locality codes;reed muller codes;codes design coded;codes present coded;computation schemes multivariate;coded;computation schemes recent;muller codes;computation lens locality;computational locality;computation schemes;computational locality equivalent;computation multivariate polynomials;multivariate polynomials adaptively;muller codes present", "pdf_keywords": "coded computation schemes;coded computation multivariate;coded computation applicable;coded computation scheme;coded computation;coded computation best;coded computation admits;polynomials coded computation;approach coded computation;coded computation enables;coded computation proved;coded computation function;needed coded computation;coded computation theorem;high coded computation;construct coded computation;workers coded computation;locality codes design;coded computation class;coded computation minimum;design coded computation;coding computation;locality codes demonstrate;studied locality codes;locality codes;coded computation leverage;coded computation non;scheme coded computation;present coded computation;framework coded computation"}, "6aeb477e5f0882f8363a3a8e5e6f83962d91edc6": {"ta_keywords": "accelerated future learning;future learning instructional;future learning;learning techniques feature;future learning use;demonstrates accelerated future;feature recognition model;learning techniques;learning;feature recognition;learning instructional;recognition model tested;accelerated future;recognition model;learning instructional treatments;demonstrates accelerated;model demonstrates accelerated;recognition;instructional treatments model;use machine learning;machine learning;techniques feature recognition;variety instructional treatments;learning use;feature;machine learning techniques;instructional treatments yield;tested variety instructional;techniques feature;instructional treatments", "pdf_keywords": ""}, "39ab4b9cdeb4a71b3e25fe5339962654c7c9ff8c": {"ta_keywords": "credibility features nodes;false information spreaders;trust credibility features;nodes spread;aggregate trust credibility;trust credibility;path social network;social network;social network model;information spreaders;information spreaders accuracy;action nodes spread;nodes spread path;credibility features;spread path social;credibility;predict false information;spread path;spreaders accuracy;predict action nodes;spreaders neighborhood;features spreaders neighborhood;spreaders neighborhood vary;nodes;data aggregate trust;trust;spread;predict false;nodes neighborhood;explain features spreaders", "pdf_keywords": "false information spreading;networksthe spread rumors;node credibility;misinformation social networks;information spreading network;node credibility features;spreaders social networks;false information spreaders;spreading networks;svm node credibility;graph neural network;spreaders false information;graph neural networks;spread false information;perception neighbor credibility;spreading networks spreaders;trust dynamics neighbor;spread social networks;spreader false information;credibility perception function;neighbor credibility perception;trust social media;credibility trust features;spreaders use trust;advances graph neural;graph neural;networks world spread;information spreaders false;credibility trust;social networksthe spread"}, "00799dceb9e7209bb9d71b38fa5b49483e886978": {"ta_keywords": "inference dynamic neural;training inference dynamic;dynamic neural networks;dynamic neural;training inference dataflows;inference dynamic;inference dataflows arbitrary;inference dataflows;dynamic instance;dynamic;instance specific graph;use dynamic;use dynamic instance;dynamic instance specific;training inference;static vertex function;accurate training inference;dataflows arbitrary;graph static vertex;static vertex;dataflows;graph static;neural networks;dataflows arbitrary size;execution dataflows;neural networks based;batched execution dataflows;dataflows multiple samples;vertex function;specific graph static", "pdf_keywords": ""}, "ab57c70c14b82d07c40c75fecaac98b0e2dc0510": {"ta_keywords": "supervised record linkage;supervised record;semi supervised record;record linkage;unsupervised semi supervised;semi supervised;record linkage problem;record linkage problems;supervised;linkage problem based;framework supervised record;bootstrapping approach unsupervised;problem based hierarchical;linkage;hierarchical graphical model;unsupervised methods;approach unsupervised methods;hierarchical graphical;approach unsupervised;unsupervised methods framework;hierarchical;linkage problems respectively;based hierarchical graphical;linkage problems;methods unsupervised semi;linkage problem;novel methods unsupervised;based hierarchical;framework supervised;methods unsupervised", "pdf_keywords": "supervised record linkage;probabilistic record linkage;unsupervised probabilistic record;record linkage methods;supervised record;fully supervised record;model record linkage;probabilistic record;record linkage;linkage records;probabilistic model record;record linkage records;linkage records method;propose novel supervised;graphical model supervised;linkage database;large record linkage;supervised;supervised classification;existing unsupervised probabilistic;linkage methods framework;supervised classification tasks;learning approach supervised;linkage methods;approach supervised classification;linkage database systems;unsupervised probabilistic;model supervised;database record linkage;perform supervised classification"}, "6413e6a4f68be0ea6aed0082b205147d9f893699": {"ta_keywords": "inflection generation efficiently;inflection generation using;inflection generation;generate inflections;method inflection generation;generate inflections form;used generate inflections;inflection generation problem;lemma inflection generation;based generation inflections;generation inflections form;generation inflections;inflection;inflections;novel method inflection;inflections form;method inflection;generation using encoder;inflections form lemma;lemma inflection;encoder;encoder decoder;encoder decoder model;using encoder;form lemma inflection;using encoder decoder;decoder model;decoder;decoder model method;generation efficiently generated", "pdf_keywords": "decoder inflection generation;sequences inflection generation;inflection generation task;learning model inflection;dataset inflection generation;based inflection generation;model inflection generation;inflection generation corresponding;learn model inflection;inflection generation;inflection generation wiktionary;inflection generation character;inflection generation able;inflection generation model;character sequences inflection;state inflection generation;generates inflectional;able predict inflections;decoder inflection;accuracy inflection generation;predict inflections input;inflection generation process;inflection generation context;generate given inflection;predict inflections;inflections large corpus;prediction accuracy inflection;encoder decoder inflection;paradigm inflection generation;model generates inflectional"}, "8dd85c3a3700d0d282ddbd4dff5e238f24c00676": {"ta_keywords": "strattal spectral envelope;spectral envelope strattal;strattal noise method;envelope strattal noise;strattal noise using;peaks strattal noise;using strattal spectral;strattal noise;mixture modelling histogram;strattal spectral;noise using mixture;modelling spectral envelope;planar distributions formant;mixture dimensional 2d;peaks strattal;mixture modelling;spectral envelope;mixture dimensional;using mixture dimensional;envelope strattal;3d planar distributions;variational framework mixture;planar distributions;formant like peaks;method modelling spectral;modelling spectral;like peaks strattal;distributions formant like;distributions formant;modelling histogram proposed", "pdf_keywords": ""}, "74dccd379776bbb50b352c19b8caf2a7896d58ee": {"ta_keywords": "sensitivity based coherency;coherency analysis method;coherent constrained variables;coherency analysis model;based coherency analysis;coherency analysis;identifying coherent constrained;identifies coherent constrained;constrained variables method;power optimisation problems;coherent constrained;constrained variables;model constrained variables;power optimisation;constrained variables applying;constrained variables mathematical;power optimisation problem;analysis model constrained;based coherency;large power optimisation;simple power optimisation;model constrained;variables mathematical formulations;coherency;variables applying sensitivity;method identifying coherent;optimisation problems described;optimisation problems;constrained;optimisation problem method", "pdf_keywords": ""}, "6a3cc30d5d6342d912851deb4362b8c47fa5ede3": {"ta_keywords": "debiasing nlu models;framework debiasing nlu;debiasing nlu;existing debiasing methods;debiasing methods;nlu models;debiasing methods allows;existing debiasing;utilizing biases;new framework debiasing;utilizing biases knowing;complementary existing debiasing;nlu models prevents;targeting certain biases;biases specifically targeting;biases evaluation;framework debiasing;biases specifically;biases evaluation suggests;debiasing;biases knowing;biases;certain biases specifically;mainly utilizing biases;certain biases evaluation;certain biases;models reliance biases;biases knowing advance;nlu;reliance biases specifically", "pdf_keywords": "debiasing models trained;debiasing neural networks;bias deep neural;debiasing methods nlu;language inference trained;bias deep;automatically identifying biased;debiased model learns;performance existing debiasing;neural networks nlu;biases existing debiasing;tasks natural language;debiasing artificial bias;natural language inference;debiasing neural;inference trained;performance shallow learning;hypothesis bias deep;identifying potentially biased;learns examples;existing debiasing methods;learning characterizing semantics;bias datasets;task present debiasing;identifying biased examples;artificial bias datasets;learns examples given;trained synthetic bias;inference trained large;bias dataset"}, "51f6654b9d5925002ccaa5cd339b4377b96719ce": {"ta_keywords": "discover activities persistent;activities persistent computer;user activities discover;activities users workstations;inferring activities users;activities persistent;clusters emails persistent;discover clusters emails;clustering emails users;user activities;information user activities;emails persistent computer;activities users;persistent computer present;activities discover activities;approach inferring activities;clusters emails;discover activities;clustering emails;activities discover;users workstations;inferring activities;users workstations approach;persistent computer use;information discover clusters;emails persistent;computer use information;discover clusters;persistent computer;persistent", "pdf_keywords": ""}, "f4906089f0720c83e57e4a46ae75283df4d67e5a": {"ta_keywords": "strategyproof reviewers able;strategyproof reviewers;strategyproof strategyproof reviewers;reviewers accurately thinking;ability reviewers accurately;mechanism peer selection;assess ability reviewers;peer selection;reviewers able accurately;ability reviewers;selection problem reviewers;strategyproof mechanism peer;reviewers accurately;mechanism peer;reviewers;peer selection problem;problem reviewers;reviewers able;problem reviewers able;thensf strategyproof strategyproof;thensf strategyproof;peer;thinking argue mechanism;propose strategyproof;used thensf strategyproof;propose strategyproof mechanism;strategyproof mechanism;strategyproof strategyproof;strategyproof;selection", "pdf_keywords": ""}, "05f8dd59d4184d38e240bdea4d58e424b8cd055c": {"ta_keywords": "persuasion persuasive power;influence persuasion persuasive;persuasion persuasive;influence persuasion;persuasion average impact;persuasive power reputation;probability successful persuasion;individuals online debate;investigate influence persuasion;persuasion;persuasion average;persuasive power;reputation heuristic information;online debate;persuasive power individuals;successful persuasion average;reputation moderated characteristics;online debate having;successful persuasion;persuasive;attributes persuasive power;reputation heuristic;impact reputation moderated;attributes persuasive;impact reputation;power reputation heuristic;reputation moderated;argument content manner;reputation;debate having 10", "pdf_keywords": "debate measure reputation;debate reputation;persuasion reputation indicators;reputation persuasion significant;persuasion reputation;persuasion debate outcome;reputation persuasion;reputation debate success;impact debate prediction;effect reputation debate;effect reputation persuasion;persuasion effect reputation;debate prediction;debate reputation serves;opinion online debate;influence debate outcome;reputation debate introduce;reputation debate;persuasion debate;online debate;online argumentation platform;effect persuasion debate;reputation opinion powerful;debate ability convey;debate training data;predicting outcome debates;online debate instrument;compute debate success;engage debate ability;representing opinions large"}, "59f41c5024a238ae8843f3dd67692961ecc63e75": {"ta_keywords": "discovery deep neural;deep neural networks;phoneme recognition;phoneme recognition spoken;recognition spoken digit;experiments phoneme recognition;using evolutionary algorithms;evolutionary algorithms;evolutionary algorithms proposed;spoken digit detection;deep neural;discovery deep;neural networks;recognition spoken;neural networks dms;using evolutionary;dms using evolutionary;optimization strategy discovery;evolution strategy efficiently;vector proposed evolution;strategy discovery deep;efficient optimization;digit detection;efficient optimization strategy;proposed evolution strategy;neural;spoken digit;optimization;parameters experiments phoneme;digit detection effectiveness", "pdf_keywords": ""}, "f9e32b30fd9ad50cce12ffb753c7be88100b6dc2": {"ta_keywords": "crowd labeled data;model crowd labeled;crowd labeled;model crowd;minimax rates permutation;crowd;labeled data;based model crowd;rates permutation based;permutation based model;propose permutation based;permutation based;computationally efficient estimator;rates permutation;labeled;labeled data significant;propose permutation;efficient estimator;data significant generalization;permutation;match minimax;estimators;simpler smectic based;data;significant generalization;minimax rates;match minimax lower;generalization classical smectic;smectic based model;generalization", "pdf_keywords": "crowd labeling models;crowdsourcing resulting estimators;model crowdsourced labeling;estimated crowdsourced data;crowd labeling;estimated crowdsourced;crowdsourced labeling;crowd labeled data;problem crowd labeling;model crowd labeled;crowdsourcing task;assigning labels crowdsourcing;model crowdsourced;crowd labeling context;crowdsourcing data;crowdsourcing task labeling;class crowd labeling;crowdsourcing characterizing sparse;crowdsourced data;labels crowdsourcing;labels workers crowdsourcing;crowdsourced data accuracy;crowd labeled;crowdsourcing users task;crowdsourced labeling use;workers crowdsourcing;problem crowdsourcing task;truth estimated crowdsourced;crowdsourcing;problem crowdsourcing"}, "15ac2d8629ca9241ea558eb2b816272d82447ac7": {"ta_keywords": "models learning people;learning people significantly;models statistically optimal;models learning;class models learning;learning people;statistically optimal;models statistically;parameter based models;estimators automatically adapt;models models statistically;estimators automatically;statistical bias;models models;win statistical bias;based models;based models models;models;class models;learning;richer classical parameter;optimal significantly robust;robust prior;bias variance tradeoff;significantly robust prior;estimators;statistically optimal significantly;models enjoying surprising;statistical bias variance;optimal classical parameter", "pdf_keywords": ""}, "18ddcd250bbbe716a0616412ea329a8343f60542": {"ta_keywords": "crowdsourcing political economic;crowdsourcing political;problem crowdsourcing political;consider problem crowdsourcing;problem crowdsourcing;crowdsourcing;documents theorems theorems;theorems natural social;documents theorems;economic documents theorems;theorems;theorems theorems;proofs theorems theorems;theorems theorems theorems;theorems theorems natural;theorems natural;proofs theorems;valid proofs theorems;setting proofs theorems;political economic documents;economic documents;political economic;natural social nature;valid proofs;necessarily valid proofs;natural social;social nature;social nature necessarily;social;setting proofs", "pdf_keywords": ""}, "c18700ed4ef07dd85ba8bceab3b9584c6e6af49c": {"ta_keywords": "maximum entropy data;entropy set measurements;extract maximum entropy;entropy data set;predict maximum entropy;maximum entropy set;maximum entropy;set maximum entropy;entropy data;maximum entropy proportional;entropy set;entropy;entropy proportional;set measurements extracted;data set maximum;measurements extracted set;entropy proportional square;predict maximum;extract maximum;method extract maximum;use predict maximum;observation set measurements;set measurements method;measurements extracted;data set;measurements method based;applied set measurements;set measurements proportional;measurements method;based observation set", "pdf_keywords": ""}, "af4e11436268cf68505f1caee5d6f7ff0df9c99a": {"ta_keywords": "causal inference improve;effects natural language;estimating causal effects;causal inference estimation;inference estimation causal;challenge estimating causal;causal inference;uses causal inference;estimating causal;estimation causal effects;causal effects encompassing;natural language processing;causal effects natural;causal effects;use causal inference;uses causal;estimation causal;causal;use causal;explores use causal;natural language;text used outcome;fairness interpretability natural;language processing models;inference improve;fairness interpretability;interpretability natural language;language processing introduce;language processing;robustness fairness interpretability", "pdf_keywords": "causality natural language;nonlinear inference causal;causal effects text;causal inference text;causal inference improve;addressed causal inference;causality improve accuracy;predicting outcome text;fairness interpretability nlp;challenge estimating causal;causal conclusions text;models text treatment;causal effects conversational;estimating causal effects;text confounder estimate;inference text data;causal inference powerful;estimating causal;causal inference;interpretability nlp;inference causal inference;causal inference review;causal inference discuss;inference causal;text treatment estimate;statistical causal inference;causality improve;accurately predict linguistic;causal inference traditional;addressed causal"}, "f8b32c2edcd7ef098ce40b7fd2e68448ac818191": {"ta_keywords": "svm novel eye;text using eye;eye tracking features;svm proposed eye;using eye tracking;eye tracking;eye movement features;support vector machines;learning using svm;detect unknown words;using svm proposed;svm proposed;vector machines svm;using svm;machines svm novel;eye movement;native language text;svm;machines svm;vector machines;novel eye movement;words natural reading;svm novel;reading non native;use support vector;support vector;proposed eye movement;using eye;language text using;natural reading", "pdf_keywords": ""}, "11e4346e60ac76ad018231a851fbbdb2112044d2": {"ta_keywords": "claim phrase veracity;claim verification based;claim phrase using;novel claim verification;claim phrase;claim verification;validity claim phrase;claim verification benchmark;approach claim verification;interpretationability claim approach;claim approach based;predict validity claim;claim approach;aggregating claim phrase;validity claim verified;claim verified;validity claim;phrase veracity;claim verified aggregating;based interpretationability claim;verified aggregating claim;aggregating claim;verification based interpretationability;approaches validity claim;interpretationability claim;phrase veracity final;approach claim;novel claim;verification based;claim", "pdf_keywords": "claim phrase detection;verifying textual claim;textual claim trustworthy;predicting claim veracity;claim veracity interpretable;interpretable fact verification;claim retrieval evidence;claim phrase veracity;claim veracity prediction;veracity interpretable fact;verifying veracity claim;verifying veracity;claim phrases predicting;claim verification based;decompose claim verification;evidence claim phrases;phrases predicting claim;claim retrieval;fact verification benchmark;verification claim phrase;claim phrase predictions;generate claim phrase;fact verification called;fact verification decomposes;variable claim veracity;supervise claim veracity;claim claim retrieval;phrase veracity prediction;verifying textual;claim phrases"}, "bcbac71ac64cd6a6aaae41e37ebe960f508ab741": {"ta_keywords": "retrieving information world;knowledge answered questions;storing retrieving information;world information model;retrieving information;symbolic representation language;world information;information model;information model based;information world information;based symbolic representation;model based symbolic;training manipulating symbolic;information possible;knowledge answered;storing retrieving;information world;symbolic representations;symbolic representation;representation language;information possible earlier;knowledge;manipulating symbolic representations;representation language allows;information;answered questions;improves performance knowledge;language;answered questions particular;based symbolic", "pdf_keywords": "learns embeddings knowledge;knowledge enhanced embedding;embeddings knowledge shared;embeddings knowledge;question answering datasets;knowledge shared embedding;embeds knowledge;knowledge context embeddings;learns embeddings;pretraining corpus entity;question answering integrating;experts learns embeddings;question answering;symbolic knowledge representations;encodes knowledge neural;knowledge representations;encode knowledge;encode knowledge context;language knowledge bases;neural knowledge bases;embeds knowledge paragraph;knowledge neural;knowledge stored memory;trained language models;representations trained knowledge;challenging question answering;scenario encode knowledge;knowledge representations demonstrate;knowledge base learned;knowledge graph trained"}, "1ca247158522991ad54cccaac6c6938576a8bd26": {"ta_keywords": "predict transcription;able predict transcription;predict transcription signal;transcription signal model;rna transcription;signal rna transcription;transcription signal rna;transcription region model;rna transcription translation;rna transcription region;transcription signal;evaluation transcription signal;used predict transcription;transcription translation;transcription;signal rna;transcription translation based;accurate evaluation transcription;evaluation transcription;neural model nonlinearity;translation based neural;rna;field rna transcription;transcription region;10 ranking;problem field rna;approach 10 ranking;10 ranking problem;ranking;transcription region used", "pdf_keywords": "document ranking task;rank lexical translation;translation model trained;marco document ranking;lexical translation model;document ranking;low rank lexical;ranking task m1;translation model;retrieval outperforms;ranking task;rank lexical;effective task searching;efficient information retrieval;parallel corpus;parallel corpus implementation;lexical translation;lambda translation model;improving marco document;using parallel corpus;information retrieval;stage retrieval outperforms;translation model method;retrieval help lambda;retrieval;corpus;corpus implementation;lambda translation;information retrieval help;ranking"}, "2eef9173946078c402596b9b080b6878db00b8ac": {"ta_keywords": "detection obesity community;social media results;early detection obesity;diabetes using social;detection obesity;analysis social media;social media data;obesity community method;users social media;using social media;social media content;obesity community;publicly available twitter;social media site;twitter;diabetes using;language social media;social media;obesity;ii diabetes using;diabetes;available twitter;users social;type ii diabetes;twitter archive;available twitter archive;based analysis social;analysis social;using social;ii diabetes", "pdf_keywords": ""}, "38bd034e6a0589bf1132d3e8c79818b271377290": {"ta_keywords": "training based margin;discriminative training based;error discriminative training;discriminative training;discriminative training problem;framework discriminative training;margin based weighted;framework discriminative;based error discriminative;unified framework discriminative;error discriminative;discriminative;margin based error;margin based;term margin based;based margin;training problem theorem;margin based hinge;based margin combines;term margin;observation margin based;hinge loss modeled;based hinge loss;based weighted error;margin;integration margin based;loss modeled using;gin term margin;margin combines fundamental;weighted error", "pdf_keywords": ""}, "3429a6b440fb6f71990bbeda9d097d709634a913": {"ta_keywords": "parser selftraining;tree parser selftraining;parser selftraining method;translation parse tree;syntaxbased machine translation;translation parse;using translation parse;machine translation;tree parser;parser;parse tree parser;parse trees using;parse tree;parse trees;machine translation using;self training data;parse;syntaxbased machine;performing syntaxbased machine;performing syntaxbased;data performing syntaxbased;syntaxbased;variety parse trees;select translation better;select translation;selecting self training;self training improving;translation better using;metrics select translation;translation using", "pdf_keywords": ""}, "9ce09b03f056253252f3e8c0c65d86a27117a0ac": {"ta_keywords": "domain adaptation image;domain adaptation;target domain adaptation;adaptation image classification;predict image classification;image classification nasa;adaptation image;image classification target;able predict image;predict image;classification target unlabeled;image classification;target unlabeled domain;classification target;classification nasa public;classification nasa;classification;adaptation model new;unlabeled domain demonstrate;problem adaptation;domain standard nasa;adaptation;adaptation model;unlabeled domain;problem adaptation model;domain demonstrate feasibility;model able predict;analysis public domain;nasa public domain;model new data", "pdf_keywords": "training data adaptation;classification domain adaptation;tuning domain adaptation;domain adaptation tent;data tent adaptation;domain adaptation;domain adaptation test;data adaptation;data adaptation modified;adaptation framework convolutional;deep learning adapts;novel adaptive classifier;domain adaptation use;adaptation large data;adaptive training data;adaptation modified training;learning adapts target;adaptive classifier;adaptive classifier supervised;adaptation source data;data computation adaptation;adaptation framework;data adapt training;classification corrupted imagenet;training adaptation;adaptive training;learning adapts;training data adapt;modeling adaptive training;adaptation source"}, "2db020e3398c06e3a22f12d8caffe76b0d9d1dda": {"ta_keywords": "answering commonsense tasks;question answering commonsense;shot question answering;question answering benchmarks;commonsense question answering;question answering;commonsense tasks guided;commonsense tasks;answering commonsense;results commonsense;answering benchmarks;language models training;empirical results commonsense;answering;results commonsense question;commonsense;pre training models;zero shot;existing knowledge resources;tasks provide empirical;language models;tasks guided;zero shot question;commonsense question;tasks provide;tasks;models training;pre existing knowledge;neuro symbolic framework;set language models", "pdf_keywords": "answering commonsense tasks;shot question answering;commonsense tasks approach;tasks commonsense tasks;question answering commonsense;commonsense tasks simplest;tasks commonsense;commonsense tasks guided;commonsense tasks;rich contexts tasks;commonsense tasks commonsense;question answering;context structure tasks;extracting distractors knowledge;question generation techniques;sources question generation;challenging questions supervised;contexts tasks;zero shot commonsense;generating challenging questions;question generation;interplay knowledge sources;natural language pretraining;questions supervised learning;questions generation challenging;semantic relations grounded;shot commonsense models;questions supervised;contexts tasks make;simulated questions generation"}, "68dca6ee694f22e2af66b56e60fdfa74041242e6": {"ta_keywords": "valve junction magnetic;spin valve junction;magnetic field model;junction magnetic field;dynamics spin valve;junction magnetic;magnetic field tesla;model applied magnetic;applied magnetic field;magnetic field;spin valve problem;spin valve;large magnetic field;solution spin valve;dynamics spin;valve junction;magnetic field results;description dynamics spin;applied magnetic;limit large magnetic;magnetic;large magnetic;solution spin;exact solution spin;field tesla;variational method;solved variational method;variational method model;solved variational;problem solved variational", "pdf_keywords": ""}, "a0f8733dd84608b3cad97904624f8bfdc2d2fcbf": {"ta_keywords": "xmath3 state magnetic;magnetic properties xmath0;magnetic properties xmath4;xmath6 state magnetic;magnetic properties xmath6;properties xmath4 state;xmath1 xmath2 states;xmath3 state;xmath2 states xmath3;xmath2 states;xmath4 state;properties xmath6 state;states xmath3;xmath4 state xmath5;states xmath3 state;properties xmath0 xmath1;xmath5 state;xmath5 state used;properties xmath4;xmath6 state;state xmath5 state;state xmath5;state magnetic properties;properties xmath6;properties xmath0;xmath1 xmath2;xmath0 xmath1 xmath2;xmath0 xmath1;state magnetic;xmath3", "pdf_keywords": ""}, "890317710697a9e41d0d9961d99986c4d865393f": {"ta_keywords": "semantic aware mobile;semantic aware;semantic aware performance;called semantic aware;model semantic aware;aware mobile apps;aware mobile;use learned representations;learning model semantic;representations use learned;learned representations learn;task called semantic;learned representations use;learned representations;representations learn;mobile apps;semantic;called semantic;model semantic;apps;representations learn novel;novel spatio temporal;units learned representations;learn novel spatio;representation learning model;meta path based;novel representation learning;representation learning;mobile apps combines;spatio temporal", "pdf_keywords": ""}, "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d": {"ta_keywords": "controlled text generation;text generation future;text generation;fudge controlled text;distributions controlled text;poetry topic control;fudge models conditional;language generation;controlled text;control language generation;language generation formality;conditional probabilities text;controlled text uses;data fudge models;text uses conditional;machine translation;text evaluate fudge;topic control language;fudge models;text;fudge tasks;fudge controlled;conditional distributions controlled;fudge tasks couplet;probabilities text;propose fudge controlled;topic control;models conditional distributions;generation formality;text uses", "pdf_keywords": "efficiently generating linguistic;formalcontrolled text generation;text generation model;generating linguistic;text generation;generating text distribution;text generation powerful;model generating text;controlled text generation;generating text;generation controlled text;generating linguistic structures;text generation given;models reproduce sentences;text distribution fudge;text generation fundamental;text generation controlled;reproduce sentences text;text generation follows;natural language models;controlled text model;text model capable;discriminators generation fudge;generation model conditional;model controlled text;reproduce sentences;distribution conditional factorization;preexisting model generating;future word prediction;text model"}, "97e8430fe01394ea9a49fd841d9aecdfc294a796": {"ta_keywords": "optimization fuzzy rules;optimization fuzzy;fuzzy rules using;method optimization fuzzy;classifying myocardial heart;heart disease ultrasonic;fuzzy rules;classifying myocardial;genetic algorithms proposed;problem classifying myocardial;disease ultrasonic image;sets ultrasound images;using genetic algorithms;genetic algorithms;ultrasonic image;ultrasonic image proposed;ultrasound images;applied problem classifying;rules using genetic;disease ultrasonic;applied sets ultrasound;fuzzy;computer aided diagnosis;ultrasound;ultrasound images normal;sets ultrasound;ultrasonic;myocardial heart disease;myocardial heart;algorithms proposed", "pdf_keywords": ""}, "33206a493e3519d27df968e98eb7fe6af14ef985": {"ta_keywords": "memory causal relationships;creating memory causal;create memory causal;memory causal;causal relationships;causal relationships presents;process creating causal;creating causal model;creating causal;causal model process;causal relationships method;causal model;causal;connected events;process governs events;events occur process;identically connected events;connected events resulting;events resulting model;governs events occur;model process governs;events occur;occur process;events;creating memory;events resulting;process governs;relationships presents examples;memory;method creating memory", "pdf_keywords": ""}, "89440a4cb27d17ced5d54bbe0f81f3477bf16404": {"ta_keywords": "particle motion measurement;measurement shape particle;particle motion measured;shape particle motion;extract shape particle;shape particle;particle motion;motion measurement method;motion measurement;particle moving medium;particle moving;confinement particle motion;velocity particle motion;motion measured;velocity particle;particle motion momentum;motion measured velocity;particle;momentum velocity particle;applied particle moving;transverse confinement particle;method applied particle;measured velocity;motion measured sum;confinement particle;measured sum momentum;measurement method;measurement shape;motion momentum method;applied particle", "pdf_keywords": ""}, "27dafb0b6076e050c31ede8d3e0184ef3592b364": {"ta_keywords": "xmath2 magnetic field;xmath1 xmath2 magnetic;xmath2 magnetic;magnetic properties xmath0;xmath1 xmath2 studied;xmath1 xmath2 effect;properties xmath0 xmath1;xmath2 studied;xmath1 xmath2;xmath2 effect field;xmath0 xmath1 xmath2;properties xmath0;xmath2 studied framework;field opposite xmath0;xmath2 effect;field magnetic properties;xmath2;xmath0 xmath1;xmath1;opposite xmath0 xmath1;field magnetic;magnetic properties;magnetic field magnetic;field induces magnetic;magnetic field;opposite xmath0;xmath0;effect magnetic field;effect field magnetic;magnetic field opposite", "pdf_keywords": ""}, "591ebe6dccb388d041623840db29a5e58824b4b0": {"ta_keywords": "preference statements graph;uniformly generating preference;generating preference statements;preference orders uniformly;generating preference;compact preference models;underlying preference orders;preference statements;preference models;preference orders;preference statements polynomial;structure underlying preference;preference variables;uniform generation graph;generation graph statements;underlying preference;length preference statements;preference variables positive;provably generate graph;graph statements provably;compact preference;number preference variables;generation algorithm uniformly;algorithm uniformly generating;generate graph structure;preference;generate graph;usability compact preference;orders uniformly random;graph statements", "pdf_keywords": ""}, "7f1a6c67d03de88b898271d52dd2e51907d5b615": {"ta_keywords": "spanning dependency parsing;dependency parsing semantic;natural language processing;dependency parsing;generalized natural language;natural language analysis;labeling spans relations;parsing semantic role;parsing semantic;natural language;variety natural language;labeling spans;spans relations spans;relations spans proposed;spans relations;parsing;relations spans;consisting labeling spans;language processing tasks;spans;language analysis;spans proposed;language processing;spanning dependency;semantic role;tasks spanning dependency;language analysis based;spans proposed model;dependency;semantic", "pdf_keywords": "tasks parsing;tasks parsing relation;natural language representations;parsing tasks;parsing tasks attention;spanning dependency parsing;natural language processing;semantic role labeling;role labeling semantics;spans single task;parsing natural language;chains parsing tasks;tasks including sentence;consists tasks parsing;predicting semantics sentences;including semantic parsing;tasks including semantic;model natural language;representation tasks brat;semantic parsing natural;structure natural language;language analysis tasks;characterizing natural language;dependency parsing;semantics relation extraction;language processing tasks;semantic parsing;language processing nlp;entities natural language;processing nlp"}, "a9c0ffc760f65ccaa99a08fc66b31653fd4a5bd7": {"ta_keywords": "organ donation review;review organ donation;organ donation improving;donation review organ;organ donation important;organ donation;organ donation aim;importance organ donation;field organ donation;people organ donation;organ donation literature;young people organ;review organ;donation improving quality;people organ;donation review;organ;donation improving;donation aim review;donation important;field organ;donation important issue;donation important aspect;importance organ;donation literature updated;quality life elderly;donation;donation literature;highlight importance organ;donation aim", "pdf_keywords": ""}, "a03675379685d88c727bc985a323cc71d06f2514": {"ta_keywords": "learns discrete syntactic;unsupervised dependency parsing;network structured generative;structured generative;word representations unsupervised;structured generative prior;discrete syntactic structure;jointly learns discrete;induction unsupervised dependency;dependency parsing demonstrate;discrete syntactic;dependency parsing;jointly learns;generative model jointly;novel generative model;generative model;syntactic structure;generative prior invertibility;model jointly learns;neural network structured;word representations;continuous word representations;parsing demonstrate model;syntactic structure continuous;propose novel generative;long prior structured;novel generative;generative prior;prior structured;unsupervised dependency", "pdf_keywords": "learns discrete syntactic;unsupervised dependency parsing;structured generative;network structured generative;embeddings syntax model;structured generative prior;word representations unsupervised;discrete embeddings learns;simple generative process;parameterization generative;based parameterization generative;generative model;empirical parameterization generative;simple generative;speech induction unsupervised;novel generative model;embeddings learns;generative model based;parameterization generative process;generative model jointly;generative process;generative approach learn;case simple generative;generative process parameterization;generative;generative process results;discrete syntactic structure;generative prior;propose generative model;based generative model"}, "7f817600b612aab6039dfba576ae8e8e7460d8f1": {"ta_keywords": "acquire word pronunciations;acoustic textual data;pronunciation dictionary using;initial pronunciation dictionary;sampled acoustic textual;expanding initial pronunciation;dirichlet process learned;acoustic textual;pronunciation dictionary;pronunciations phone text;word pronunciations phone;initial pronunciation;textual data model;word pronunciations;pronunciations phone;process learned supervised;supervised learning model;pronunciations;text wsj data;discrete version dirichlet;learned supervised;pronunciation;independently sampled acoustic;learning model;supervised learning;textual data;phone text wsj;sampled acoustic;learned supervised learning;dirichlet process", "pdf_keywords": ""}, "60a9438c24847a949419e0350a61fc2a330e4a09": {"ta_keywords": "biases kernel clustering;kernel clustering criteria;kernel clustering popular;kernel clustering propose;popular kernel clustering;kernel clustering;clustering popular kernel;artifacts kernel clustering;density biases kernel;bias kernel means;isolation bias kernel;empirical artifacts kernel;biases kernel;clustering criteria density;bias kernel;kernels breiman bias;density biases;kernel means common;kernel means;bias similarity histogram;density biases theoretically;common class kernels;clustering propose principled;criteria density biases;kernels;class kernels;clustering propose;histogram mode isolation;empirically observed;clustering criteria", "pdf_keywords": "kernel clustering powerful;kernel clustering objectives;kernel clustering criteria;kernel clustering popular;known kernel clustering;clustering criterion kernel;popular kernel clustering;bandwidth kernel clustering;kernel clustering;clustering criteria kernel;kernel clustering adaptive;kernel clustering new;clustering adaptive kernels;adaptive kernel clustering;bias clustering elementary;functions bias clustering;prove kernel clustering;bias clustering;gini criterion clustering;bias kernel means;new clustering criterion;kernels theorems bias;pairwise clustering criterion;clustering optimum continuous;clustering optimum;criterion clustering optimum;clustering powerful;new pairwise clustering;criterion clustering;clustering criterion"}, "11db042ed2264f3ea1b8f20151adf725ec3461e8": {"ta_keywords": "convergence properties neural;convergence loss;convergence loss does;apparent convergence loss;neural networks showing;neural networks;initialization partial training;properties neural networks;weights arriving critical;neural;partial training;networks;error surfaces globally;error surfaces;convergence properties;loss does correspond;space error surfaces;nonconvex breaking symmetry;networks showing apparent;movements flat regions;networks showing;apparent convergence;surfaces globally nonconvex;critical points large;symmetry random initialization;points large movements;regions weight space;large movements flat;analyze convergence properties;random initialization partial", "pdf_keywords": "error surfaces neural;neural networks highly;surfaces neural networks;networks highly nonlinear;neural networks;initialization partial training;loss critical points;convergence loss;surfaces neural;near loss critical;symmetry random initialization;error surfaces;neural;apparent convergence loss;convergence loss does;error surfaces locally;convex local minima;partial training;highly nonlinear symmetry;random initialization regions;weights arriving critical;networks;characterizing error surfaces;networks highly;space error surfaces;analysis training paths;loss critical;training paths;space near loss;random initialization partial"}, "201b79be15b6b01e62a82b29ac4d30d3e6a11799": {"ta_keywords": "estimation variance speech;variance speech signal;variance speech;single channel speech;channel speech signal;channel speech;power law noise;speech signal;decoder power law;rnn encoder;speech signal presence;rnn encoder decoder;fully recurrent neural;law noise approach;network rnn encoder;recurrent neural network;law noise;recurrent neural;law reduction variance;encoder decoder power;power law reduction;neural network rnn;decoder power;effective estimation variance;estimation variance;power law dereverberation;network rnn;speech;noise approach;use fully recurrent", "pdf_keywords": "end speech recognition;channel speech recognition;decoder network speech;multi channel speech;speech single channel;speech features encoders;single channel speech;channel speech single;network speech recognition;speech recognition model;rnn based encoder;multi channel speechwe;speech recognition module;channel speech;speech recognition;channel speechwe;channel speechwe consider;encodes input speech;recognition separate speech;speech recognition replace;neural beamforming module;network speech;speech streams model;problem speech recognition;level speech recognition;speakers encoder;network neural beamforming;speech recognition dereverberation;speech streams;speech recognition case"}, "62606fbb3aa3ffb17c5427b3652c18a81425cd65": {"ta_keywords": "training data annotated;annotated data creation;training data;training data use;conversion training data;data using descripion;data annotated data;annotated data;data annotated;data creation;data use descripion;use descripion process;using descripion process;data creation new;descripion process method;amounts training data;descripion process;based conversion training;applying descripion process;new class data;class data;conversion training;descripion process present;use descripion;large amounts training;annotated;using descripion;descripion process demonstrate;novel method generating;data", "pdf_keywords": ""}, "99848c6424556bce427d621e89b6d05dac131910": {"ta_keywords": "crowdsourcing loka microtask;crowdsourcing loka;intelligence task hic;answer aggregation;passed crowdsourcing loka;pairs passed crowdsourcing;answer aggregation mechanism;decentralized answer aggregation;unique subsumption pairs;task hic;subsumption pairs;loka human intelligence;322 unique subsumption;crowdsourcing;task hic aim;pairs aggregated;aggregation;unique subsumption;passed crowdsourcing;platform pairs aggregated;human intelligence task;pairs aggregated using;loka microtask;hic aim project;aggregated;loka microtask platform;subsumption;platform russian loka;subsumption pairs passed;intelligence task", "pdf_keywords": ""}, "7a1bcf3c84607f7aeb0601658845ca2083059f43": {"ta_keywords": "semi supervised learning;multidimensional supervised;semi supervised;approach semi supervised;multidimensional supervised parallel;train multidimensional supervised;parallel supervised learning;supervised learning input;parallel supervised;supervised parallel;parallel supervised supervised;learning model np;supervised supervised parallel;supervised parallel supervised;supervised learning model;supervised learning;supervised supervised;train multidimensional;supervised;learning input data;multidimensional;large scale dataset;discrete objects learning;dataset linear programming;approach train multidimensional;learning input;multidimensional model;linear programming language;learning model;context multidimensional", "pdf_keywords": ""}, "ef09dd6f5615e2b937d3f9dd555c2daafb4c4f4b": {"ta_keywords": "parallel virtual machine;virtual machine parallel;machine parallel virtual;virtual machine library;performance parallel virtual;traditional parallel virtual;parallel virtual;virtual machine;library traditional parallel;performance parallel;compares performance parallel;loops parallel virtual;application performance libraries;machine parallel;performance libraries evaluated;performance libraries;code loops parallel;machine library;machine library effective;application performance;traditional parallel;typical application performance;machine library context;libraries evaluated comparing;comparing performance;parallel;compares performance;machine library traditional;comparing performance terms;virtual", "pdf_keywords": ""}, "aa0d4f7cfa13758a02d248fd607547f045306519": {"ta_keywords": "email performance;improves performance email;improving performance email;email performance measure;performance document recognition;email content document;email function time;performance email function;standard email performance;performance email;email content;document recognition;compared standard email;document recognition presence;email function;improve performance document;standard email;performance document;email;relationship email content;document;improving performance;performance;content document;recognition;performance measure proposed;document question;performance measure;improve performance;method improving performance", "pdf_keywords": ""}, "36db0616e59c8ac5e9ba8ded820ef6c969f068c1": {"ta_keywords": "symmetry optical fiber;stationary optical fiber;generate stationary optical;construct stationary optical;stationary optical spectrum;stationary optical;dimensional optical fiber;optical fiber;optical fiber fiber;symmetry optical;optical spectrum;robust dimensional optical;optical spectrum resulting;optical fiber used;optical elements fiber;degree symmetry optical;dimensional optical;fiber fiber constructed;optical fiber high;optical;optical spectrum high;constructed set optical;fiber constructed;optical elements;fiber used generate;set optical;set optical elements;fiber fiber;fiber constructed set;spectrum used construct", "pdf_keywords": ""}, "78f1eef6d79a129f59b977a5037f5fc9cc7fda90": {"ta_keywords": "peer review algorithms;review algorithms;quality peer review;peer review;review algorithms based;improving quality peer;algorithms improving quality;quality peer;contributions book xcite;improving quality content;quality content process;xcite;quality content;algorithms evaluation contributions;submissions developed;submissions;book xcite construction;book xcite;submissions developed set;new algorithms evaluation;content process submissions;evaluation contributions book;algorithms evaluation;xcite construction;xcite construction new;peer;algorithms;new algorithms;review;collection algorithms improving", "pdf_keywords": ""}, "afdae523d420278670c30f45c015cc5860a0de22": {"ta_keywords": "parameterized convergence adaptive;convergence adaptive gradient;adaptive gradient methods;convergence adaptive methods;stochastic line search;adaptive gradient;method stochastic line;adaptive methods depends;adaptive methods;study convergence adaptive;convergence adaptive;search method stochastic;variants stochastic line;method stochastic;gradient methods;line search methods;step sizes converges;parameterization convergence;stochastic line;parameterization convergence constant;gradient methods context;line search method;adaptive;converges optimal rate;variants stochastic;methods step sizes;constant step size;parameterized convergence;context parameterization convergence;line search", "pdf_keywords": "adaptive gradient descent;adaptive gradient methods;convergence adaptive gradient;stochastic gradient methods;adaptive gradient method;approach adaptive gradient;adaptive gradient;size adaptive gradient;stochastic gradient method;proposed adaptive gradient;stochastic accelerated gradient;convergence stochastic gradient;sizes adaptive gradient;weights convergence adaptive;rate adaptive gradient;gradient descent algorithms;gradient descent;iteration adaptive step;stepsize iteration adaptive;stochastic gradient;class adaptive gradient;large parameterized gradient;convergence adaptive preconditioner;adaptive step;convergence rate adaptive;gradient method convergence;gradient method optimization;optimization stochastically;iteration adaptive;rate stochastic gradient"}, "656aedc681975c3c97b1764466832de537358150": {"ta_keywords": "speech recognition asr;hybrid automatic speech;automatic speech recognition;speech recognition;recognition asr systems;automatic speech;recognition asr;auxiliary feature based;auxiliary feature extraction;feature based adaptation;available recognition tasks;models hybrid automatic;sequence summary network;available recognition;allows auxiliary feature;auxiliary feature;asr systems proposed;asr systems;propose auxiliary feature;improves recognition performance;recognition performance;consistently improves recognition;recognition tasks;improves recognition;proposed adaptation scheme;feature extraction module;based adaptation scheme;e2e models hybrid;end e2e models;publicly available recognition", "pdf_keywords": ""}, "2ccfa631708b78130b1ea1b8ae3c2b688caf3938": {"ta_keywords": "scheduling corpus surgery;optimal timing surgery;heteroscedasticity optimal;surgery isbooked;surgery isbooked opposed;issue heteroscedasticity optimal;optimally scheduling;prescribed surgery parameters;optimally scheduling corpus;achieved surgery isbooked;problem optimally scheduling;timing surgery;surgery parameters;surgery patients;scheduling;optimal trade booking;patients prescribed surgery;scheduling corpus;heteroscedasticity optimal trade;heteroscedasticity;surgery;supposing optimal timing;surgery patients prescribed;trade achieved surgery;prescribed surgery;issue heteroscedasticity;corpus surgery patients;optimal timing;corpus surgery;uncertainty demonstrate optimal", "pdf_keywords": "prediction surgery duration;predicting durations surgery;surgery duration data;estimate duration surgery;surgery duration based;surrogate model outperforms;prediction surgery;surgery duration using;optimal surgery duration;surgery scheduled based;optimal time surgeries;predict length surgery;surgery case durations;surrogate based model;time surgeries optimal;neural regression algorithms;surgery durationwe use;length surgery scheduled;surgery duration;predicting durations;decisions scheduling surgery;surgery durationwe;durations surgery operations;scheduling surgery;error surgery duration;surgery records recorded;learning hour surgery;simulated surgery duration;surgery records;partial learning regression"}, "58777f0af009a225e315b7240db20ba545207702": {"ta_keywords": "deterministic preferences aggregation;preference aggregation uncertainty;preferences aggregation schemes;preference aggregation;preferences represented probability;deterministic preferences;preferences aggregation;aggregation uncertainty ability;aggregation uncertainty;set deterministic preferences;problem preference aggregation;aggregation schemes;aggregation schemes immune;agents preferences represented;individual agents preferences;manipulation adequate aggregation;preferences represented;social choice function;distributions set deterministic;adequate aggregation;manipulate social choice;agents preferences;choice function individual;aggregation;adequate aggregation large;aggregation large;aggregation large numbers;choice function;uncertainty ability manipulate;set deterministic", "pdf_keywords": ""}, "309fb4d4d0946ac746f352c13cd3be4e2cd86dae": {"ta_keywords": "probabilistic model complexity;stochastic model complexity;acoustic modeling speech;posteriori approximation stochastic;method acoustic modeling;acoustic modeling;model complexity control;variational approximation sampling;modeling speech;approximation stochastic model;variational approximation stochastic;control method acoustic;modeling speech related;approximation stochastic;maximum posteriori approximation;complexity control method;posteriori approximation variational;model complexity;approximation sampling based;method acoustic;posteriori approximation;approximation sampling;probabilistic model;speech related applications;complexity control;approximation variational approximation;applications maximum posteriori;variational approximation;stochastic model;approximation variational", "pdf_keywords": ""}, "ce17dab00ddd2c86da508fc0502247f9b18a570f": {"ta_keywords": "winner proportional approval;proportional approval voting;approval voting strategy;approval voting satisfaction;voting satisfaction approval;satisfaction approval voting;approval voting computing;voting computing winner;approval voting np;winner satisfaction approval;reweighted approval voting;voting satisfaction;computing winner satisfaction;approval voting;proportional approval;voting reweighted approval;computing winner proportional;winner proportional;approval voting reweighted;prominent voting rules;winners rules proportional;voting strategy proof;rules proportional approval;use approval ballots;voting computing;voting strategy;approval ballots;winner satisfaction;approval ballots select;voting np hard", "pdf_keywords": "complexity approval voting;voting rules studied;voting computing winner;approval voting computing;voting rule studied;winner voting process;voting computing;proportional approval voting;stochastic voting agents;approval voting method;winner proportional approval;approval voting np;voting rule approval;rule approval voting;prominent voting rules;voting process;voting method based;approval voting rules;voting proportional approval;approval voting proportional;voting np hard;voting rules approvalwe;voting agents;stochastic voting;voting method;approval voting;agent winner voting;elections winner determination;voting rules best;voting process winning"}, "1d1bbed89882ac1001b915ee73199a919aa0d13c": {"ta_keywords": "language modelling prior;language models;effective language models;language modelling;vocabulary language modelling;prior held languages;training languages approximated;robust effective language;sample training languages;open vocabulary language;languages approximated laplace;training languages;held languages task;vocabulary language;universal linguistic knowledge;languages approximated;effective language;languages;held languages;modelling prior obtained;modelling prior;linguistic knowledge;languages task;universal linguistic;language;linguistic knowledge used;level open vocabulary;laplace method prior;imbued universal linguistic;linguistic", "pdf_keywords": "crucial language prediction;language prediction learned;training languages laplace;model language prediction;train language models;language models;training languages prior;underlying language modeling;language models conditioned;language modeling remains;recurrent models universal;language models used;language modeling infer;neural recurrent models;language modeling relies;transfer language models;language prediction;recurrent models;level language modeling;models conditioned language;language modeling;language models cross;models languages large;language models languages;prior neural weights;outperforms baseline models;language modeling capable;languages prior;memory neural recurrent;characters learned prior"}, "7b7f8fb08262fce3da64c09788fd4b595408e4e6": {"ta_keywords": "spectral density particle;particle harmonic oscillator;comb spectral density;density particle harmonic;harmonic oscillator frequency;oscillator frequency comb;frequency comb spectral;calculation spectral density;analysis spectral density;particle harmonic;oscillator harmonic;harmonic oscillator;spectral density;oscillator harmonic oscillator;harmonic oscillator harmonic;applied harmonic oscillator;oscillator frequency;density particle computed;comb spectral;frequency comb method;harmonic oscillator method;calculation spectral;frequency comb;density particle;method calculation spectral;oscillator method;oscillator method applied;particle computed;oscillator method based;analysis spectral", "pdf_keywords": ""}, "d940e0192a6cc1ddd6288239b77b06e50f042114": {"ta_keywords": "pretrained speech representations;eral pretrained speech;speech recognition e2e;pretrained speech;speech representations;automatic speech recognition;overlapped speech representations;automatic speech;speech recognition;speech representations present;end automatic speech;pretrained representations advanced;corpora e2e asr;recognition e2e asr;e2e asr tasks;pretrained representations effective;pretrained representations;results pretrained representations;sev eral pretrained;cross overlapped speech;e2e asr;available corpora e2e;corpora e2e;e2e asr explore;recognition e2e;asr tasks;overlapped speech;scenarios pretrained representations;representations advanced end;eral pretrained", "pdf_keywords": "pretrained speech representation;pretrained speech segmentation;pretrained speech representations;speech representation advanced;end speech processing;automatic speech;speech recognition e2e;speech representation;sslrs advanced speech;using pretrained speech;results pretrained speech;pretrained speech;speech representations;automatic speech recognition;speech segmentation;speech representations resultsin;speech use pretrained;speech segmentation patterns;speech recognition;advanced speech recognition;select pretrained speech;advanced speech processing;speech recognition machine;end automatic speech;speech recognition systems;various speech representations;advances speech recognition;learning e2e asr;speech corpora models;speech representations present"}, "d5924c8cdef6270a955ba82c2b07a8282d869744": {"ta_keywords": "gesture generation;gesture generation using;method gesture generation;gesture models;class gesture models;gesture models called;language acoustic cues;gesture;novel method gesture;novel class gesture;generation using spoken;language acoustic;acoustic cues introduce;acoustic cues;class gesture;wave partial speech;method gesture;relationships language acoustic;partial speech learn;using spoken;using spoken language;spoken language approach;partial acoustic;partial speech;spoken language;acoustic;partial acoustic wave;acoustic wave partial;speech learn;acoustic wave", "pdf_keywords": ""}, "b82e9b84cb639f6bb061c8a43b97986ecfec00ea": {"ta_keywords": "large partite graphs;partite graphs;similarity queries;similarity queries like;partite graphs number;variety similarity queries;representation large partite;queries like set;graphs number dimensions;queries like;queries;similarity;dimensions small representation;large partite;column classification;graphs;variety similarity;used variety similarity;partite;representation large;results comparable precision;column classification demonstrate;results comparable;comparable precision;acquisition column classification;like set expansion;query;number dimensions;classification;like set", "pdf_keywords": ""}, "ee33d61522fd70fa4e6470decbdac6c17f8b4fdb": {"ta_keywords": "attractors speech embedding;attractors multiplied speech;decoder based attractor;attractors speech;number attractors speech;speech embedding sequence;speech embedding;multiplied speech embedding;attractor calculation;neural speaker diarization;end neural speaker;based attractor calculation;attractor calculation ed;attractors multiplied;generated multiple attractors;speaker diarization eend;extracts speech embedding;encoder decoder;attractor;neural speaker;multiple attractors multiplied;based attractor;multiple attractors;encoder decoder based;encoder;attractors;number attractors;speaker diarization;flexible number attractors;sequence multiplied speech", "pdf_keywords": "attractors speech embedding;analyzing attractors speaker;attractors multiplied speech;attractors speaker mixture;attractors speaker;attractors speech;capture attractors speaker;number attractors speech;attractors embedding sequence;decoder based attractor;attractors embedding;speech embedding sequence;number attractors embedding;speech embedding;deep attractor network;end neural speaker;attractors attractors noisy;speaker embeddings;speaker diarization generates;multiplied speech embedding;end speaker diarization;based deep attractor;diarization speech mixtures;attractors noisy;attractors noisy environment;embeddings speakers method;deep attractor;generated multiple attractors;embeddings apply speaker;attractor network"}, "d6d2003d112e3d9d93edd4920436fe2fe879eb87": {"ta_keywords": "fast clustering large;method fast clustering;clustering large scale;fast clustering;clustering large;scale text mining;clustering;similarity matrix data;pairwise similarity method;text mining;pairwise similarity;text datasets based;scale text datasets;similarity matrix;similarity method based;power pairwise similarity;similarity method;text datasets;text mining use;large scale text;similarity;adaptation similarity matrix;linear adaptation similarity;context big data;context large scale;adaptation similarity;big data;datasets based;big data era;large scale", "pdf_keywords": ""}, "3b8b6f27a5df5dc2c231d0fa1e471887e4583466": {"ta_keywords": "genes publication networks;identifying genes publication;publication networks;genes publication;publication networks approach;predict future author;social networking content;social networking;identifying genes;social information;content predict;future author based;use social networking;problem identifying genes;non social information;content predict future;networking content predict;publication;author based knowledge;author based;future author;genes;networks approach;networks;social;use social;networks approach relies;author;knowledge recent work;based knowledge recent", "pdf_keywords": ""}, "4bfc185dcc67b3eddfa059fc5446f4df844a0728": {"ta_keywords": "spin orbit interaction;orbit interaction spin;interaction spin orbit;orbit coupled spin;coupled spin orbit;spin orbit coupled;dynamics spin orbit;enhance spin orbit;enhancement spin orbit;effect spin orbit;interaction dynamics spin;spin orbit;suppression spin orbit;coupled spin;interaction spin;dynamics spin;orbit interaction dynamics;orbit interaction;orbit interaction responsible;orbit interaction used;enhance spin;effect spin;enhancement spin;suppression spin;orbit coupled;responsible enhancement spin;responsible suppression spin;spin;used enhance spin;study effect spin", "pdf_keywords": ""}, "9635e3c008f7bfa80638ade7134a8fb0ef1b37e1": {"ta_keywords": "likelihood language model;pretrained language model;language model significantly;language model;language model especially;likelihood language;marginal likelihood language;perplexity tokeniser uncertainty;tokeniser uncertainty;tokeniser entropy;pretrained language;measured tokeniser entropy;best pretrained language;tokeniser uncertainty measured;uncertainty measured tokeniser;perplexity tokeniser;language;difference perplexity tokeniser;tokeniser;measured tokeniser;marginal likelihood;model especially domain;likelihood;model significantly better;better best pretrained;entropy;best pretrained;marginal;pretrained;especially domain data", "pdf_keywords": "tokenisation language modelling;language models marginal;tokenisations improve language;evaluating language models;likelihood tokenisations;estimating probabilities tokenisation;language models;tokenisation structured model;language model evaluation;marginal likelihood tokenisations;tokenisation structured;modern language models;sampled tokenisations improve;tokenisation language;language models evaluated;language model robustness;tokenisation marginal likelihood;tokenisations improve;best tokenisation metric;best tokenisation substantial;improve language model;language models recognise;likelihood tokenisations taking;likelihood high tokeniser;metric language models;dominant language modelling;substantial sampled tokenisations;text best tokeniser;best tokenisation;canonical tokenisation unsatisfactory"}, "3f2821dd40c12da560c89b9dad7f95cd4ad9354f": {"ta_keywords": "disk adaptive redundancy;adaptive redundancy protection;architecture disk adaptive;attack degrades storage;storage capacity architecture;overload avoided cipher;redundancy protection transition;degrades storage capacity;redundancy protection;disk adaptive;ciphers avoids imminent;degrades storage;storage;novel architecture disk;ciphers;avoided cipher ciphers;avoided cipher;ciphers avoids;protection transition overload;architecture disk;cipher;cipher ciphers avoids;storage capacity;challenges clusters increasing;exponential growth storage;capacity architecture based;clusters increasing size;challenges clusters;growth storage;cipher ciphers", "pdf_keywords": "redundancy reduce storage;disk adaptive redundancy;scalable storage implementations;scalable storage critical;existing scalable storage;scalable storage;reliable storage systems;storage implementations proposed;storage clusters proposed;storage nodes designed;storage designed resilient;resilient failures storage;storage systems challenging;storage implementations;novel storage architecture;emergence reliable storage;scale storage systems;reliability distributed storage;storage clusters;extensible storage supports;scale storage clusters;nodes storage designed;storage cluster;reliable storage;storage nodes;extensible storage;distributed storage systems;performance storage systems;high performance storage;storage critical applications"}, "ff6ddbd7ba59e0fd4a74748942083391d6e9a666": {"ta_keywords": "extracted metastable interacting;metastable interacting particles;metastable interacting;information extracted metastable;metastable state determined;metastable state obtained;state extracted metastable;state interacting particles;interacting particles state;metastable state;analyze state interacting;compared metastable state;use metastable state;extracted metastable process;metastable process;interacting particles;determined state interacting;state interacting;interacting particles approach;metastable process used;particles state extracted;extracted metastable;interacting particles results;use metastable;particles state;analyze state;based use metastable;metastable;compared metastable;results compared metastable", "pdf_keywords": ""}, "9fe09ca520cb7ce106e65b39c455777d18ec6efe": {"ta_keywords": "curates annotated taxonomy;propose arborist taxonomies;arborist taxonomies library;taxonomies learned algorithm;arborist taxonomies;taxonomies learned;annotated taxonomy;taxonomies library curates;reciprocal rank recall;taxonomies library;library curates annotated;curating texts higher;curates annotated;guarantees taxonomies learned;rank recall achieving;annotated taxonomy contributions;taxonomies;rank recall;curating texts;approach curating texts;guarantees taxonomies;taxonomy;library curates;propose arborist;arborist;curating;recall achieving;taxonomy contributions;function guarantees taxonomies;annotated", "pdf_keywords": ""}, "ef4166a7fb2c40ca87b0ebb253e8ba1e80c09fd7": {"ta_keywords": "chime speech recognition;speech recognition challenge;advanced speech recognition;augmented discriminative feature;speech recognition;speech recognition baselines;discriminative feature transformation;features discriminative feature;discriminative feature;augmented feature transformation;features discriminative;chime speech;propose augmented discriminative;2nd chime speech;augmented discriminative;augmented feature;feature transformation highly;arbitrary features discriminative;feature transformation;subtask 2nd chime;feature transformation introduces;advanced speech;proposed advanced speech;recognition baselines;recognition challenge propose;vocabulary subtask 2nd;2nd chime;results augmented feature;recognition challenge;features", "pdf_keywords": ""}, "5db0fa82c322bb7d9f60109294d088ff139eebf3": {"ta_keywords": "denoiseing images;method denoiseing images;denoiseing images based;denoising quality;achieve denoising quality;denoiseing;denoising quality comparable;achieve denoising;denoising;method denoiseing;novel method denoiseing;method achieve denoising;nearest point gan;noisy image image;gan manifold;gan manifold proposed;noisy image;point gan manifold;projection noisy image;point gan;gan;best performing bayesian;projection noisy;bayesian markov chain;method projection noisy;unseen images;performing bayesian markov;images based identification;image image;bayesian markov", "pdf_keywords": "denoising images generative;networks denoising deblurring;denoising images;denoise image;image denoising;vector denoise image;networks denoising;image denoising efficiently;neural networks denoising;carry image denoising;denoising deblurring;method denoising images;denoise corrupted images;vector denoise;denoise image obtaining;latent vector denoise;recovered noisy images;gan manifold recovering;networks noisy images;denoising deblurring develop;networks gan;denoising;propose denoise corrupted;denoising efficiently;networks gans;denoising efficiently reliably;nearest point gan;neural networks gan;noise image;novel method denoising"}, "1d05e91b6d94f06439b2b41291a8dcc3d8064149": {"ta_keywords": "segmentation using kernel;novel approach segmentation;image grid regularization;segmentation;clustering methods image;pairwise feature clustering;fitting techniques segmentation;grid regularization;techniques segmentation;segmentation analyze;segmentation using;grid regularization using;approach segmentation;techniques segmentation analyze;formulation kernel means;kernel bandwidth gini;using kernel means;kernel means energy;approach segmentation using;segmentation analyze extreme;graph cuts;bound formulation kernel;feature clustering methods;graph cuts similarly;formulation kernel;using graph cuts;feature clustering;kernel bandwidth;clustering methods;clustering", "pdf_keywords": ""}, "da1f22dd6d834e031eb733d2b70320f34ef9458f": {"ta_keywords": "comparison items cardinal;quality score pairwise;items cardinal measurement;score pairwise comparison;rates cardinal ordinal;cardinal measurement;error rates cardinal;parametric ordinal models;estimating quality score;cardinal measurement setting;cardinal ordinal settings;pairwise comparison items;ordinal models;cardinal ordinal;quality score;comparison items;items cardinal;pairwise comparison class;estimating quality;rates cardinal;pairwise comparison;score pairwise;compare error rates;ordinal;comparison class parametric;ordinal settings identical;parametric ordinal;ordinal models error;error estimating quality;ordinal settings", "pdf_keywords": "pairwise comparisons crowdsourcing;comparisons crowdsourcing;comparisons crowdsourcing provide;crowdsourcing provide minimax;crowdsourcing;crowdsourcing provide;estimation quality scores;estimating quality score;estimation quality score;quality scores pairwise;scores pairwise comparisons;comparison graph weighted;crowdsourcing platform provide;estimating quality;crowdsourcing platform;commercial crowdsourcing;estimating latent qualities;pair quality vectors;based pairwise comparisons;rank pairwise comparison;score comparison graph;commercial crowdsourcing platform;quality score data;quality vectors bound;estimation quality;pairwise comparison powerful;amazonturk commercial crowdsourcing;quality score comparison;pairwise comparisons;weighted weighted likelihood"}, "b07b124852f897823490db0a04ea6e411bb77f00": {"ta_keywords": "unlabelative optimal threshold;classifier unlabelative optimal;probabilities optimal threshold;threshold maximizing f1;binary classification;consider optimal threshold;binary classification classifier;optimal threshold;optimal threshold maximizing;maximizing f1 measures;f1 measure classifier;threshold half optimal;optimal threshold half;unlabelative optimal;threshold maximizing;conditional probabilities optimal;classification;classification classifier;classifier;context binary classification;probabilities optimal;classification classifier outputs;measure classifier unlabelative;maximizing f1;classifier unlabelative;optimal value f1;classifier outputs;measure classifier;threshold;threshold half", "pdf_keywords": ""}, "b6502b61bf8f0332c6caa30198cff3619a9790aa": {"ta_keywords": "requests distributed storage;redundant requests distributed;performance redundant requests;optimal redundant requestsing;redundant requestsing;redundant requests;redundant requestsing policies;distributed storage systems;latency performance redundant;distributed storage;situations redundant requests;redundant requests help;requests distributed;storage systems;storage systems characterize;latency performance;design optimal redundant;optimal redundant;performance redundant;latency;analytical study latency;study latency performance;storage;distributed;redundant;requestsing policies;study latency;requestsing;requests;requests help design", "pdf_keywords": "latency serving requests;optimizing latency distributed;process redundant requests;distributed arrival service;redundant requests optimal;distributed queueing;requests distributed storage;reduce latency serving;requests servers redundant;serving requests distributed;latency consisting servers;latency distributed arrival;requests reduce latency;instantly redundant requests;distributed arrival;sending redundant requests;memory distributed queueing;latency distributed;latency serving;redundant requests reduce;redundant requests;servers redundant presence;latency certain request;requests distributed;latency memoryless service;concerning latency service;servers redundant;employ redundant requests;optimizing latency consisting;latency service fundamental"}, "b72c5236dacf2b958ebcf427d17a100bc54af504": {"ta_keywords": "denotating context encoder;context encoder denotating;context encoder;encoder denotating;encoder denotating achieved;encoder;denotating context;problem denotating context;context inheritance evaluations;denotating achieved;denotating;shannon hughes dataset;context aware inheritance;proposed approach shannon;context aware;context inheritance;introducing context aware;approach shannon;implement context inheritance;denotating achieved introducing;implement context;technique implement context;problem denotating;hughes dataset;introducing context;achieved introducing context;dataset;context;shannon;mask technique implement", "pdf_keywords": "end speech recognition;e2e automatic speech;context aware encoder;speech feature sequences;speech recognition asr;encoder introducing context;encoder based;encoder introducing;speech recognition;long speech feature;feature sequences encoder;speech feature;automatic speech recognition;automatic speech;encoder trained;novel encoder architecture;sequences encoder based;encoder novel context;encoder propose;aware encoder capable;encoder;encoder capable;aware encoder;vector encoder;encoder architecture;novel approach encoder;novel encoder layer;sequences encoder;encoder capable processing;aware encoder novel"}, "6dfc2ff03534a4325d06c6f88c3144831996629b": {"ta_keywords": "unstructured problems adversarial;problems adversarial reasoning_;generating unstructured problems;adversarial reasoning_ simple;adversarial reasoning_;generate unstructured problems;quality commonsense problems;high quality commonsense;problems adversarial;bias recognition networks_;commonsense problems;problems simple movies;commonsense problems simple;simple movies approach;quality commonsense;unstructured problems;generating unstructured;recognition networks_;low bias recognition;simple movies;bias recognition;unstructured problems low;adversarial;method generating unstructured;commonsense;unstructured unstructured problems;unstructured problems high;algorithm generate unstructured;generating unstructured unstructured;generate unstructured", "pdf_keywords": "visual commonsense reasoning;task visual commonsense;reasoning engine recognition;images natural language;vision representations unstructured;art reasoning engine;predicting scene descriptions;recognition predicting scene;visual commonsense;commonsense reasoning present;question answering tasks;predicting image interestingness;movie description challenge;commonsense reasoning;recognition pipeline;reasoning engine;reasoning natural language;recognition videos;novel challenge vision;visual commonsense generally;discovering patterns videos;images elicitations;annotated images similar;pattern recognition videos;challenge vision systems;dataset unstructured video;state art reasoning;annotated images;movie images elicitations;description challenge"}, "df43f6ff7c66d39240235af3052be55222bef80d": {"ta_keywords": "nested sampling mixture;nested gibbs sampling;sampling mixture mixture;mixture mixture models;sampling method mixture;sampling mixture;mixture models;gibbs sampling proposed;mixture models based;nested nested sampling;applied speaker clustering;gibbs sampling;nested sampling;speaker clustering;nested nested gibbs;speaker clustering problem;nested gibbs;method mixture mixture;mixture mixture;clustering problem nested;method mixture;models based nested;mixture;sampling proposed;nested nested nested;sampling proposed method;sampling;clustering;nested nested;novel sampling method", "pdf_keywords": ""}, "964293c1cb1b619eb9b474381d2ba60cf44fcc2d": {"ta_keywords": "distribution facility mapping;distribution facilities mapping;mapping distribution facility;facilities mapping applied;facility mapping applied;facilities mapping;distribution facility management;facility mapping;mapping distribution;distribution facility;distribution line maps;construction distribution facility;distribution facilities;maintenance distribution facilities;mapping applied planning;planning maintenance distribution;facility management described;distribution line;mapping applied design;mapping applied construction;facility management;maps mapping applied;mapping applied;maps mapping;maintenance distribution;line maps mapping;maps;construction distribution;applied construction distribution;mapping", "pdf_keywords": ""}, "d389d8c2e15f9e9269c17fe6f960f70559eee840": {"ta_keywords": "unsupervised word segmentation;word segmentation;word segmentation based;unsupervised morpheme patterns;segmentation based parsimony;unsupervised morpheme;unsupervised word;novel unsupervised morpheme;embeddings text corpora;method unsupervised word;enrich word embeddings;text corpora;word embeddings;word embeddings text;morpheme patterns experiments;text corpora use;text downstream language;morpheme patterns;corpora;downstream language modeling;language modeling;corpora use;segmentation;novel unsupervised;unsupervised;use novel unsupervised;method enrich word;text downstream;embeddings text;enrich word", "pdf_keywords": "distributed word representations;semantically meaningful morphemes;memoization morpheme segmentation;discovering morpheme patterns;enrich morpheme vocabulary;morphemes text domains;embeddings extracted morphemes;discovering morpheme;morpheme segmentation vocabulary;entropybased morpheme predictability;subword enriched embeddings;morpheme language enriched;enriching word embeddings;learning distributed word;word embeddings;unsupervised morpheme segmentation;morpheme segmentation;human interpretable morphemes;enrich word embeddings;words morphemes;learned word embeddings;meaningful morphemes;morphemes meaningful enrich;morphemes use segmentation;segment words morphemes;word embeddings based;word embeddings high;entropybased morpheme;meaningful morphemes text;high quality morphemes"}, "3d1318bc66d534eefac7c665fd7cc891fba27b87": {"ta_keywords": "xmath2 symmetry breaking;xmath1 symmetry breaking;xmath4 symmetry breaking;xmath3 symmetry breaking;symmetry breaking xmath1;breaking xmath1 symmetry;symmetry breaking zeeman;xmath0 symmetry breaking;xmath2 symmetry;enhance xmath2 symmetry;xmath1 symmetry;xmath3 symmetry;xmath4 symmetry;enhance xmath4 symmetry;xmath0 symmetry;zeeman field xmath0;field xmath0 symmetry;magnitude xmath3 symmetry;symmetry breaking;breaking zeeman field;effect zeeman field;symmetry breaking used;breaking xmath1;symmetry breaking order;xmath2;xmath1;enhance xmath2;xmath3;xmath4;enhance xmath4", "pdf_keywords": ""}, "33972d9e9a102f9388e5850d8aed3d1aefc9d2e5": {"ta_keywords": "fallacious argumentation argumentative;fallacious argumentation;approach fallacious argumentation;argumentation argumentative discourse;argumentative discourse;logical arguments build;arguments logical arguments;argumentation argumentative;general logical arguments;arguments build logical;argumentative discourse approach;arguments logical nature;logical arguments;arguments logical;network logical arguments;argumentation;arguments using logical;using logical arguments;logical arguments using;logical arguments logical;argumentative;arguments;logical general logical;novel approach fallacious;logical network logical;approach fallacious;discourse;discourse approach build;arguments build;fallacious", "pdf_keywords": "argumentation build game;argument game argotario;game argotario analysis;software argotario argumentative;game provide tool;games called ng;game argotario;ng games;fallacious argument game;architecture class games;software argotario;fallacious argumentation build;called ng games;introduce software argotario;argumentation build;argumentation dataset publicly;fallacious argumentation dataset;argument game;games free traditional;argotario offer game;ng games free;argotario fallacious argumentation;build game;argumentation dataset;open source architecture;build game captures;argotario net;game captures essential;argumentative discourse game;available argotario net"}, "04138da3bac26f83a9d57152118d4cd5cc8c717d": {"ta_keywords": "similarity measures entity;walk based similarity;adaptive similarity measure;text adaptive graph;entity relation graphs;adaptive graph walks;adaptive similarity;using adaptive similarity;similarity measure processing;adaptive graph walk;based similarity measures;similarity measures;graph walk based;similarity measure;relation graphs framework;queries using adaptive;structured text adaptive;framework adaptive graph;relation graphs;parsed text graph;adaptive graph;based similarity;graph walks;text graph;similarity;measures entity relation;text adaptive;graph walks applied;graph represents corpora;graph walk", "pdf_keywords": ""}, "395aae6e7a79e5760457ca38e868acc970016230": {"ta_keywords": "sparse attention architecture;sparse attention;large tables architecture;novel sparse attention;attention architecture;table reasoning datasets;attention architecture modeling;tables architecture;contain large tables;large tables;attention;tables current accelerators;handle large tables;tables;table reasoning;reasoning datasets;tables architecture scales;large tables current;modeling documents;table;state art table;novel sparse;modeling documents contain;tabular;present novel sparse;tabular data sets;sparse;tables current;speed memory;art table reasoning", "pdf_keywords": "attention tables;attention tables mate;view attention tables;attention rows;table reasoning transformers;table reasoning tasks;attention rows columns;tables useful predictingcomputational;data constraining attention;focus attention rows;idea table transformers;textual table reasoning;table based neural;view attention;attention flow machine;table reasoning called;attention patterns implement;table transformers;table reasoning;efficient transformer table;large scale tabular;table transformers focus;attention;way extracting attention;extracting attention;transformer attention;multi view attention;constraining attention;transformer attention model;transformer table reasoning"}, "e07c2e66dab7b61091bb8a4ad132bf279c233027": {"ta_keywords": "stochastic topic modeling;modeling text citations;topic modeling model;topic modeling;citations context stochastic;text citations;text citations context;citations;stochastic topic;context stochastic topic;citations context;joint modeling text;modeling text;subset citeseer data;citeseer data;link structure bipartite;models joint modeling;context stochastic;bipartite graph experiments;link structure;novel models joint;experiments subset citeseer;citeseer;models joint;structure bipartite graph;model assumes link;joint modeling;topic;graphical model;single graphical model", "pdf_keywords": ""}, "aeb4478619461ca25592e6d692f3591ec8c4091b": {"ta_keywords": "generating semantic collisions;generate semantic collisions;study semantic collisions;semantic collisions;semantic collisions texts;semantic collisions evade;semantic collisions generate;collisions generate semantic;collisions texts semantically;texts semantically unrelated;semantically unrelated;semantically unrelated judged;generating semantic;generate semantic;collisions texts;semantic;approach generating semantic;perplexity based;texts semantically;collisions evade perplexity;study semantic;perplexity based filtering;evade perplexity based;filtering discuss;semantically;collisions;collisions generate;evade perplexity;unrelated judged similar;perplexity", "pdf_keywords": "adversarial semantic collisions;generating semantic collisions;generate semantic collisions;adversarial semantic;method adversarial semantic;vulnerable semantic collisions;texts vulnerable semantic;semantic collisions texts;study semantic collisions;semantic collisions;adversary use semantic;adversarial attacks nonlinear;semantic collisions deliver;adversarial;collisions texts semantically;generating semantic;semantic collisions evaluate;semantic collisions input;machine learning nlp;semantic collisions demonstrate;use semantic collisions;similarity texts vulnerable;vulnerabilities natural language;semantic collisions based;approaches generating semantic;texts collisions aggressive;applications semantic collisions;learning nlp;texts collisions;study inverse adversarial"}, "3a3fb140890dbba93290e358af700f9a5c8bcc7a": {"ta_keywords": "questions deep learning;meaning representations approach;symbolic meaning representations;domain questions deep;wikipedia bab problem;learning using symbolic;encyclopedia natural language;questions deep;natural language;problem wikipedia bab;gram machine;deep learning;gram able process;wikipedia bab;synthetic text bab;gram able;called gram machine;gram;machine gram;meaning representations;gram machine gram;machine gram able;approach called gram;problem encyclopedia natural;natural language demonstrate;synthetic text;deep learning using;text bab;problem synthetic text;called gram", "pdf_keywords": ""}, "2f780a18d44f4e3c5c4c74d4060b8dfd542a778d": {"ta_keywords": "dynamics sports broadcasting;sports broadcasting using;football public broadcast;sports broadcasting;public broadcast dynamics;broadcasting;american football public;broadcast dynamics;broadcast dynamics broadcast;public broadcast;investigate dynamics sports;broadcast;broadcast including;broadcast dominated narrative;football public;dynamics sports;broadcasting using;players trajectories time;narrative individual players;database american football;broadcast including impact;dynamics broadcast;broadcasting using publicly;broadcast dominated;sports;dynamics broadcast dominated;points broadcast;dynamics broadcast including;players trajectories;different points broadcast", "pdf_keywords": "racial bias football;football broadcasts racial;bias football broadcasts;racial bias sportswe;analysis racial bias;statistics racial bias;bias football;racial bias;bias natural language;broadcast football transcripts;research racial bias;football transcripts;broadcasts racial;transcripts nfl;bias sportswe present;players frequently referred;football transcripts nfl;white players nonwhite;statistics mentions players;broadcasts racial composition;mentions players statistics;confounds statistics racial;unbiased descriptions playerswe;bias sportswe;transcripts nfl ncaa;skewed mentions players;players major offensive;players statistics mentions;marking nonwhite players;racial metadata use"}, "34fc6da7a88433478fd976fd0b9de3cf7134e652": {"ta_keywords": "random variables quantum;set quantum numbers;quantum numbers set;universal set quantum;set quantum;quantum numbers;variables quantum;variables quantum field;set random variables;generate universal set;numbers set random;quantum;random variables field;set random;quantum field;number particles;applied set random;generate universal;method generate universal;fixed number particles;random variables;number particles method;quantum field method;distribution number variables;random variables method;particles;distribution number;particles method;universal set;generate", "pdf_keywords": ""}, "df1d89f4ca9c20e2c6703cdbf26a62f2b50ac71c": {"ta_keywords": "gradient descent equilibria;learning dynamics;convergence learning dynamics;known equilibrium concepts;descent equilibria;model particle physics;known equilibrium;equilibrium concepts;model equilibrium concepts;model equilibrium;particle physics zero;particle physics;descent equilibria standard;learning dynamics context;known equilibria;equilibrium;particle physics establish;zero sum games;particle physics characterize;equilibria standard model;connections known equilibrium;standard model equilibrium;equilibrium concepts standard;equilibria;model particle;attracting critical points;convergence learning;investigate convergence learning;context known equilibria;physics zero sum", "pdf_keywords": ""}, "02980e5ba847282b683a85e7a8862c6c1b6e0d94": {"ta_keywords": "magnetic properties laalo;xmath4o xmath5 magnetic;xmath5 magnetic;field magnetic properties;xmath5 magnetic field;magnetic properties;magnetic anisotropy;anisotropy local magnetic;investigated effect magnetic;induces magnetic anisotropy;magnetic anisotropy local;magnetic field magnetic;effect magnetic;field magnetic;effect magnetic field;strength applied magnetic;properties laalo xmath0cu;local magnetic moments;magnetic moments;yba xmath3cu xmath4o;magnetic field;applied magnetic field;local magnetic;xmath1o xmath2 yba;xmath2 yba xmath3cu;field induces magnetic;applied magnetic;magnetic field strongly;magnetic;laalo xmath0cu xmath1o", "pdf_keywords": ""}, "73aa33fd469b171d50c452c5e3fe0e9e03520520": {"ta_keywords": "domain knowledge exchange;public domain knowledge;public domain proceedings;knowledge exchange method;knowledge exchange;domain proceedings 2012;domain proceedings;knowledge exchange pst;method knowledge exchange;systems public domain;knowledge exchange lds;proceedings 2012 iwisps;domain knowledge;knowledge exchange pdr;public domain;using public domain;2012 iwisps meeting;method public domain;iwisps meeting systems;proceedings;proceedings 2012;exchange lds method;systems public;iwisps meeting;meeting systems evaluated;evaluated using public;systems evaluated using;exchange method snn;domain;meeting systems", "pdf_keywords": ""}, "c330ec2047d019d98233abb59d13b3256c662cc7": {"ta_keywords": "causal model counterfactuals;model counterfactuals arbitrary;counterfactual query arbitrary;target counterfactual probabilities;counterfactual counterfactual queries;counterfactuals arbitrary structural;identifiable counterfactual;identification algorithms counterfactual;counterfactual probabilities provably;counterfactuals arbitrary;counterfactual queries counterfactual;counterfactual distributions;counterfactual queries;represent counterfactual distributions;non identifiable counterfactual;counterfactual probabilities;models represent counterfactual;model counterfactuals;identifiable counterfactual variations;counterfactual queries called;counterfactual distributions indicative;experimental counterfactual distributions;target counterfactual;query arbitrary causal;arbitrary structural causal;counterfactual query;algorithms counterfactual query;counterfactual counterfactual;bounds target counterfactual", "pdf_keywords": "identification counterfactual distributions;counterfactual distributions causal;counterfactual distribution causal;counterfactual distributions arbitrary;counterfactual distribution underlying;based counterfactual identification;counterfactual interventional probabilities;underlying counterfactual distribution;counterfactual identification discrete;inferring counterfactual distributions;identifying counterfactual distributions;infer counterfactual distributions;model based counterfactual;counterfactual identification;counterfactual probability model;based observed counterfactual;counterfactual distributions;represent counterfactual distributions;observed counterfactual distribution;partial identification counterfactual;defined counterfactual distribution;counterfactual probabilities arbitrary;distribution arbitrary counterfactual;counterfactual distributions concerns;empirically determined counterfactual"}, "0a3caecce668731efe7abf37720793eed1fb951a": {"ta_keywords": "tweets accurate classifier;accurate classifier twitter;politically oriented tweets;twitter user political;classifier twitter user;classifier twitter;classifier politically oriented;classifier politically;political affiliation sender;corpus political messages;precision classifier politically;oriented tweets accurate;presidential election classifier;election classifier;political messages collected;oriented tweets;large corpus political;user political affiliation;tweets accurate;political messages;sentiment information;political affiliation use;corpus political;election classifier applied;twitter user;use sentiment information;sentiment information based;tweets;affiliation use sentiment;twitter", "pdf_keywords": ""}, "5bca90a331417402f5018f552e1a62656dd7fc5b": {"ta_keywords": "community noisy observations;observed community noisy;community structure noisy;graph recovered noisy;partially observed community;noisy graph;noisy noisy graph;recovered noisy nodes;observed community;community noisy;noisy graph number;characterize community structure;noisy observations characterize;noisy nodes simple;community structure;recovering features noisy;noisy nodes;nodes graph recovered;graph recovered;noisy observations;noisy partially observed;graph nodes;observations characterize information;features noisy partially;nodes graph nodes;structure noisy noisy;limit characterize community;information theoretic;characterize community;nodes", "pdf_keywords": ""}, "5166c0e04d77ac0f7969c49c0f8f18129a114198": {"ta_keywords": "measurement motion patient;automated measurement motion;measurement motion;applied measurement motion;based measurement motion;measurement motion central;motion patient method;measurement time motion;target measurement motion;motion patient based;motion central patient;motion patient clinical;patient based measurement;motion patient;time motion patient;automated measurement;method automated measurement;motion;method based measurement;time motion;measurement time;method applied measurement;patient vicinity target;based measurement time;motion central;applied measurement;central patient vicinity;patient vicinity;target measurement;vicinity target measurement", "pdf_keywords": ""}, "205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4": {"ta_keywords": "translation systems neural;adapting translation systems;low resource languages;multilingual low resource;translation systems;adapting translation;adapt new language;method adapting translation;multilingual low;massively multilingual low;neural machine new;new language;resource languages lrls;resource languages;languages lrls;new low resource;new language parametrized;neural machine;start massively multilingual;low resource model;language parametrized lrl;multilingual;low resource;massively multilingual;systems neural machine;languages lrls start;languages;machine new low;language;language parametrized", "pdf_keywords": "adapting multilingual;neural machine translation;source training multilingual;method adapting multilingual;training translation;multilingual training coldstart;training translation large;language challenging adaptation;translation systems new;translation systems;translation process efficient;language high resource;training multilingual;language low source;training multilingual training;language translation process;adapting multilingual nmt;machine translation systems;multilingual training effective;multilingual training compared;multilingual seed models;multilingual training;machine translation;high resource language;starting massively multilingual;language high source;low resource languages;low resource language;gains multilingual training;combination multilingual training"}, "488b1849dd81e63aae2cd327564077ae123c0369": {"ta_keywords": "absolute compression convex;models absolute compression;gradient models absolute;compression convex;compression convex problems;sgl absolute compression;generalized gradient models;gradient models sgl;class generalized gradient;class absolute compression;compression error compensation;absolute compression;generalized gradient;absolute compression provide;gradient models;optimal compressors class;compressors class generalized;optimal compressors;absolute compression error;contains optimal compressors;compression;models sgl absolute;compensation class absolute;compensation class generalized;analysis error compensation;compression provide;gradient;compressors class;compression provide new;models absolute", "pdf_keywords": "absolute compressors stochastic;compressors stochastic input;compensation absolute compression;error compensated stochastic;compensated stochastic gradient;compressors stochastic;error compensation absolute;compensated stochastic;stochastic gradients error;response absolute compressors;compressors error linear;stochastic estimators;absolute compression;stochastic gradient descent;error feedback asymptotically;stochastic gradients assumed;stochastic gradients;stochastically decreasing error;stochastic estimators various;solving stochastic gradients;absolute compression compared;feedback asymptotically optimal;stochastic optimization;experiments absolute compressors;estimation inverse convex;descent rate stochastic;analysis error compensation;stochastic gradient;compression compared contractive;rate stochastic gradient"}, "4cd92a56dca741190e453b4229eb9851abf6944c": {"ta_keywords": "convex stochastic optimization;accelerated stochastic gradient;stochastic gradient accelerated;clipping stochastic gradients;sigma clipping stochastic;new accelerated stochastic;stochastic optimization heavy;variant accelerated stochastic;accelerated stochastic;smooth convex stochastic;convex stochastic;stochastic gradients extend;stochastic optimization;gradient accelerated sigma;stochastic gradients;stochastic gradient;accelerated stochastic order;sigma smooth convex;clipping stochastic;method strongly convex;bounds sigma clipping;accelerated sigma clipping;optimization heavy tailed;stochastic order method;distributed noise method;tailed distributed noise;accelerated sigma;clipped sigma sigma;strongly convex;strongly convex case", "pdf_keywords": "convex stochastic optimization;stochastic optimization convex;stochastic convex optimization;clipped stochastic optimization;clipped stochastic gradient;convex optimization stochastic;accelerated stochastic gradients;method stochastic convex;accelerating learning stochastic;known stochastic convex;known stochastic optimization;stochastic optimization known;optimization stochastic gradients;stochastic gradients extend;gradients stochastic optimization;clipping stochastic gradients;stochastic optimization powerful;stochastic gradient method;stochastic optimization heavy;stochastic gradients sigma;propose stochastic gradient;stochastic gradients;stochastic stochastic convex;method stochastic optimization;stochastic gradient heavy;stochastic convex;linear stochastic gradient;stochastic gradients stochastic;stochastic gradient;stochasticity gradients"}, "40848b41ed8c9c255ecd8a920006877691b52d03": {"ta_keywords": "wild distribution shifts;benchmark wild distribution;distribution shifts based;distribution shifts;new benchmark wild;wild distribution;communities benchmark;benchmark wild;communities benchmark provide;shared communities benchmark;machine learning tools;new benchmark;shifts based;shifts based collection;present new benchmark;shifts;benchmark;benchmark provide challenges;datasets;distribution;machine learning;set machine learning;wild;datasets spanning;communities;benchmark provide;evaluated shared communities;based collection datasets;data sets;baseline models", "pdf_keywords": "wild distribution shifts;machine learning wilds;shift benchmark predicting;distribution shifts wilds;distribution shifts datasets;wilds machine learning;distribution shifts naturally;shifts real world;shift models trained;distribution shifts natural;distribution shifts autonomous;generalize distribution shifts;realistic distribution shift;benchmarks distribution shifts;shifts datasets;landscape machine learning;machine learning environmental;distribution shift benchmark;novel machine learning;deep learning natural;distribution shifts;training algorithms distributional;challenges machine learning;shifts datasets used;shifts models;shifts use classify;shift models;learning algorithms novel;shifts wilds dataset;machine learning bioinformatics"}, "2e492af839e971d05592df1c76d4878908e1d4c0": {"ta_keywords": "factor graph grammars;grammars factor graphs;graph grammars factor;graph grammars;replacement graph grammars;graph grammars generalize;graph grammars plate;graph grammars fggs;factor graphs;factor graphs used;sets factor graphs;graphs factor graph;factor graphs amenable;known factor graph;factor graphs factor;fggs factor graphs;graphs factor;factor graph;grammars fggs;grammars fggs used;class factor graph;grammars plate notation;grammars factor;grammars plate;hyperedge replacement graph;replacement graph;graphs;graphs used generate;graphs used;grammars generalize known", "pdf_keywords": "stochastic computation graphs;factor graph grammars;stochastic computation graphsa;factor graph grammar;graphs stochastic computation;graph grammars;random graphs models;models random graphs;graph grammars fgs;graphs including stochastic;factor graphs general;generated factor graphs;graph grammar;replacement graph grammars;graphs stochastic;represent random graphs;random graphs stochastic;graph grammars fggs;graph grammar fgg;graphs factor graph;factor graphs;networks random graphs;factor graphs present;random graphs including;computation graphsa factor;hypergraph models generalize;random graphs used;random graphs;graphs models;graphs factor"}, "c66b59394f99d639b277a54ad357d20de30285bd": {"ta_keywords": "biomedical literature database;database biomedical literature;biomedical literature using;information biomedical literature;searchable database biomedical;biomedical literature;structured document retrieval;biomedical literature useful;search new biomedical;database biomedical;biomedical literature called;extracts information biomedical;new database biomedical;new biomedical literature;document retrieval;information biomedical;document retrieval extracts;literature database;slif biomedical literature;structured document;biomedical;literature database publicly;retrieval extracts information;retrieval extracts;new biomedical;literature called structured;called structured document;text mining;image processing text;retrieval", "pdf_keywords": ""}, "8fc728b71f9e92f91455f957f10c7e496cbe4772": {"ta_keywords": "enhance entity typing;entity typing language;quality entity typing;entity typing;typing language model;language model enhancement;entity typing process;enhancing language model;enhance entity;method enhance entity;typing language;language model significantly;utilizes language model;language model;enhancing language;typing process experiments;entity;model enhancement improve;improve quality entity;capable enhancing language;typing;typing process;model enhancement;quality entity;model enhancement method;method utilizes language;utilizes language;model significantly outperforms;benchmark datasets demonstrate;method enhance", "pdf_keywords": ""}, "208e5c187e81f63024ece8e2003dbaef094703cb": {"ta_keywords": "pulse quantum memory;quantum memory pulse;quantum memory based;quantum memory;memory quantum memory;quantum memory quantum;fidelity quantum memory;quantum memory used;quantum memory device;memory quantum;laser pulse quantum;high fidelity quantum;coding quantum dense;dense coding quantum;pulse quantum;photon laser pulse;quantum dense coding;coding quantum;performs quantum logic;performs quantum;single photon laser;device performs quantum;memory pulse generated;photon laser;teleportation quantum dense;quantum teleportation quantum;generated single photon;memory pulse;fidelity quantum;teleportation quantum", "pdf_keywords": ""}, "7dadf1e4f6f7a6966d5f691c3707fe221038528b": {"ta_keywords": "computing allocations fair;allocations fair maximize;allocations fair;complexity computing allocations;computing allocations;fairness concepts envy;tractable fairness concepts;allocations;tractable fairness;fair maximize social;welfare sum agents;focus tractable fairness;fairness concepts;sum agents utilities;complexity computing;envy freeness item;maximize social welfare;computational complexity computing;envy freeness;computational complexity;fair maximize;concepts envy freeness;social welfare sum;complexity;sum agents;welfare sum;maximize social;study computational complexity;social welfare;fairness", "pdf_keywords": "efficient allocation envy;allocation envy efficient;computing allocations fair;finding allocation envy;allocations fair efficient;computation allocations fair;envy free allocation;fair allocation utility;allocation envy;allocations fair maximize;construct fair allocation;efficient envy free;computing fair utilitarian;allocation utilitarian maximal;allocations fair;function fair allocation;fair allocation knapsack;complexity computing fair;algorithm efficient allocation;efficient envy;allocations maximizingtheorems theorems;fair allocation;allocations maximizingtheorems;efficient allocation;existence optimal allocations;allocation equivalent maximizing;stronger optimal allocation;computing fair;complexity computing allocations;complexity finding allocation"}, "346081161bdc8f18e2a4c4af7f51d35452b5cb01": {"ta_keywords": "question answering benchmark;answering annotated reasoning;strategyqa question answering;answering benchmark;reasoning strategies human;question answering;answer questions reasoning;reasoning use benchmark;question answering annotated;answering benchmark called;annotated reasoning;humans answer questions;questions reasoning strategies;answering annotated;reasoning strategies;annotated reasoning covers;sophisticated multihop reasoning;introduce question answering;reasoning sophisticated;hop reasoning sophisticated;reasoning sophisticated multihop;answer questions;multihop reasoning use;humans answer;multihop reasoning;questions reasoning;answering;benchmark called strategyqa;ability humans answer;reasoning covers wide", "pdf_keywords": "reasoning strategies pipeline;annotates question decompositions;strategy question generation;annotated question decompositions;reasoning steps manageable;eliciting questions;eliciting strategy questions;eliciting questions simple;reasoning strategies;question decompositions;reasoning strategy;annotated strategy questions;reasoning strategy questions;reasoning questions method;question generation method;annotated strategy questionswe;questions enable reasoning;strategy questions annotations;questions based annotated;question generation;collecting strategy questions;natural language inference;reducing required reasoning;reasoning questions resulting;answers strategy questions;question decompositions evidence;questions annotations;reasoningthe search;questions annotations using;question search corpus"}, "76df9c90d5359585f5501a4da1af1078c32be6d7": {"ta_keywords": "unsupervised text model;unsupervised text;model unsupervised text;decipher text documents;accurately decipher text;decipher text;decipher text text;capture linguistic structure;text model;documents accurately decipher;linguistic structure documents;transcription search linguistic;text documents;search linguistic signature;text documents outperforming;text model able;linguistic signature document;generative model unsupervised;structure documents accurately;capture linguistic;linguistic signature;transcription search;text;able capture linguistic;text text;historical transcription search;search linguistic;documents accurately;structure documents;linguistic structure", "pdf_keywords": "generative model text;produced text model;lines text model;text learned typesetting;typesetting historical documents;images text model;learns font structure;generative model typesetting;text produced;text model capable;document predict ink;text produced text;text model inspired;produced text;documents text model;text model;text able accurately;learned typesetting;text based visual;text;predict obscured text;text based;transcribe images text;obscured text based;text learned;typesetting lines;learns font;historical documents text;text recognition;inspired historical printing"}, "88c3f221a6fc8aff014268b0efb5ff119ab40906": {"ta_keywords": "similarity social media;similarity dataset tweets;emojis interesting connections;based use emojis;use emojis;finding similarity social;finding similarity;emojis interesting;use emojis interesting;finding similarity interesting;finding similarity finally;similarity;similarity social;emojis;similarity finally propose;similarity finally;finding similarity process;similarity process;method finding similarity;similarity process finding;similarity dataset;finding similarity dataset;similarity interesting connections;similarity interesting;dataset tweets;process finding similarity;tweets;social media;social media based;social", "pdf_keywords": ""}, "ce2d6de9cec4a6d135c32bb8d2d02bba09928b33": {"ta_keywords": "knowledge text categorization;text categorization able;text categorization;approach text categorization;categorization able accurately;categorization;expert knowledge text;categorization able;expert knowledge outperforms;expert knowledge given;expert knowledge;knowledge given word;represent expert knowledge;accurately represent expert;knowledge text;algorithmic approach text;expert;knowledge outperforms;algorithmic;represent expert;utility expert knowledge;terms accuracy;knowledge outperforms existing;algorithmic approach;terms accuracy efficiency;method terms accuracy;algorithmic approach able;recent algorithmic approach;given word result;approach text", "pdf_keywords": ""}, "393c5c96e73dd3a82c175f9ab1f6c083830d3b82": {"ta_keywords": "analysis inversion market;return portfolio stocks;normalized market capitalization;return portfolio;market capitalization;value normalized market;parameter reconstruction financial;book value normalized;inversion market price;financial output reconstruction;inversion market;market capitalization proof;data analysis inversion;improvement return portfolio;portfolio stocks;normalized market;book value;portfolio stocks compared;portfolio;price history company;reconstruction financial output;parameter reconstruction price;stocks;market price method;stocks compared;structured data analysis;based book value;analysis inversion;price history;market price", "pdf_keywords": "predict future stock;stocks deep neural;stock market prediction;prediction price stocks;future stock;neural networks predict;predict future;directly predict future;future stock price;predict future fundamentals;market prediction;predicting price;clairvoyantly select stocks;predict future fundamental;prediction price;data predict future;future performance stocks;analysis predicting price;clairvoyant model predict;predicting price directly;predict future performance;price stocks deep;deep neural;stocks deep;price directly predict;deep neural networks;neural network;predicting;market prediction based;deep learning"}, "e929c9b53c66d52ae5ea56f0dc2764aef4cc67f6": {"ta_keywords": "separating channel speech;data channel separation;channel separation approach;channel separation;separating channel;method separating channel;robust separation;robust separation able;method robust separation;separation approach based;separation able capture;channel speech arbitrary;separation approach;novel method separating;speech arbitrary environment;channel speech;input data channel;separation;method separating;separation able;separating;signal input data;structure signal input;data channel;signal input;speech arbitrary;robust variations input;capture intrinsic structure;novel method robust;input data able", "pdf_keywords": ""}, "be28821d510a99ffce40cdcf6860302def8533ef": {"ta_keywords": "facility location games;location games model;strategy model optimal;location games;welfare space optimal;strategy optimal;strategy strategy optimal;use facility location;strategy optimal strategy;optimal strategy;optimal social welfare;space optimal strategy;strategy optimal given;optimal given strategy;strategy constrained strategy;constrained strategy strategy;strategy constrained;optimal strategy profile;optimal use facility;strategy model;given strategy optimal;constrained strategy;strategy model strategy;strategy strategy model;strategy strategy;optimal social;propose strategy model;facility location;necessarily optimal social;given strategy strategy", "pdf_keywords": "mediator game social;mediators induce game;social location game;mediator induce games;social location games;strategic players facility;distributed players social;user mediator games;cost mediator game;social cost players;mediator induces game;games mediator induces;social cost games;facility location game;location game theory;players determined mediator;mediator game;facility location games;mediator games;cost games mediator;mediator games mediator;players mediator;players social network;implemented players mediator;recommendation systems strategic;location sharing players;optimal locations social;games mediator;players induces optimal;players social location"}, "3d2dece28f566792b6dd3a190aa345fc30fee1ff": {"ta_keywords": "ground transportation networks;transportation network model;transportation networks model;capacity ground vehicles;air ground transportation;optimal location capacity;linearized equilibria ground;linear program;equilibria ground nodes;integer linear program;transportation networks;transportation network;ground nodes constrained;ground transportation;ground vehicles hybrid;optimal solution mathematical;optimal location;selecting optimal location;location capacity ground;constraints linearized equilibria;anaheim transportation network;mathematical program solving;hybrid air ground;global optimal solution;solution mathematical program;global optimal;based linearized equilibria;linear program demonstrate;optimal solution;compute global optimal", "pdf_keywords": "traffic equilibria hybrid;model transportation networks;model transportation network;model traffic equilibria;transportation network model;transportation networks model;dynamics traffic hybrid;traffic equilibria model;minimum congestion travel;traffic congestion hybrid;capacity minimizing traffic;traffic hybrid air;airground transportation network;network model transportation;static traffic equilibria;ground transportation networks;transportation networks satisfy;capacity air traffic;traffic hybrid;hybrid airground transportation;ground transportation network;minimizing traffic congestion;traffic equilibria;optimization network capacity;air traffic flow;congestion hybrid;transportation networks;congestion network model;minimizing traffic;transportation network"}, "eadd73c3e1c20d16e32ee8656c4f954603b37450": {"ta_keywords": "discovering onset acoustic;based recognition onsets;onsets use convolutional;able detect onsets;detect onsets;clarinetist video convolutional;onset acoustic events;recognition onsets;discovering onset;detect onsets precision;method discovering onset;events clarinetist video;recognition onsets use;onset acoustic;salient points audio;location onset acoustic;acoustic events clarinetist;events music videos;acoustic events music;clarinetist video;acoustic events;music videos;points audio recordings;audio recordings;discover location onset;points audio;video convolutional neural;music videos based;video convolutional;events music", "pdf_keywords": "onsets clarinetist videos;onset detectors trained;acoustic onsets clarinetist;clarinetist videos;onset detection;onset location clarinet;onsets clarinetist;clarinet recording;cnntranscription string recording;onset detectors;visual onset detection;detecting acoustic onsets;world clarinetist videos;onset detection sound;clarinetist videos 36;dynamics clarinetists;hours clarinetist videos;clarinet recording use;clarinetist videos carry;music transcription;dynamics clarinetists strings;location clarinet recording;dynamics music transcription;sound string ensembles;clarinetist videos focus;real world clarinetist;model dynamics clarinetists;learns convolutional;clarinetist;clarinet"}, "a4e937f0b6e0688f7f3c4fcaebbabefa4a36da85": {"ta_keywords": "espnet tts provide;org espnet2 tts;espnet tts;espnet2 tts;org espnet tts;espnet2 tts introduce;www espnet org;espnet tts extend;www espnet;http www espnet;espnet org espnet;espnet org;espnet org espnet2;espnet org es;org espnet;org espnet2;espnet;espnet2;tts provide unified;tts provide;training inference http;tts;tts introduce;tts introduce http;training inference;tts extend http;tts extend;framework training inference;inference http www;inference http", "pdf_keywords": "networks speech text;networks speech;density networks speech;network speech text;network speech;network speech processing;density network speech;high fidelity speech;tegrable speech models;extended text speech;speech models;text speech e2e;speech emergence network;generating speech;speech text prediction;tegrable speech model;speech model;output tegrable speech;networks gans joint;extended speech processing;speech synthesis toolkit;speech output;tegrable speech synthesis;speech models able;speech synthesis;speech model evaluated;scalable speech synthesis;waveform e2e speech;high quality speech;networks gans"}, "2135c44087e06a6d95d04ad0afa400e926d37944": {"ta_keywords": "feature space normalization;estimation feature spaces;normalization feature spaces;feature spaces model;feature spaces;feature space;approach feature space;feature spaces adaptation;normalization feature;applied normalization feature;space normalization structural;spontaneous parity breaking;estimation feature;posteriori linear regression;approach estimation feature;space normalization;spontaneous parity;spontaneous spontaneous parity;maximum posteriori linear;normalization structural;parity breaking;spaces model parameters;feature;consistently applied normalization;normalization structural pes;normalization;applied normalization;posteriori linear;spaces model;model parameters consistently", "pdf_keywords": ""}, "99c80d608ba2aa638333f27bbe3f09cdc580a051": {"ta_keywords": "python library heavy;library heavy heavy;python library;library heavy;version python library;shelf version python;version python;heavy heavy heavy;python;library;heavy heavy;heavy;shelf version;available shelf version;shelf;available shelf;publicly available shelf;publicly available;publicly;version;available;introduce publicly available;introduce;introduce publicly", "pdf_keywords": "torch audio toolkit;torch audio library;torch audio speech;speech domain toolkit;benchmarks torch audio;functionality torch audio;implementation torch audio;audio toolkit;models torch audio;torch audio;speech datasets achieves;audio toolkit provides;speech datasets;audio toolkit present;large speech datasets;torch audio related;learning applications audio;audio speech models;new projects audio;audio library;applications audio speech;speech models library;projects audio speech;audio speech;maintenance torch audio;audio library provide;deployment torch audio;audio speech domain;projects audio;audio related machine"}, "29bc6654abd34b2405f7a01341f790aed2aab9a4": {"ta_keywords": "distributed storage codes;storage codes efficient;designing distributed storage;distributed storage;storage codes;codes efficient data;storage;node repair;node repair illustrate;smallest data read;designing distributed;efficient data read;codes efficient;codes attain smallest;piggybacking design framework;codes meeting constraints;downloaded node repair;piggybacking design;distributed;constructing explicit codes;smallest data;data read;framework designing distributed;read downloaded repair;codes attain;explicit codes;data read downloaded;codes;codes meeting;new piggybacking design", "pdf_keywords": ""}, "b35ad59ce9a3ea01a0980c90bc750273d1f99e7a": {"ta_keywords": "integrate heterogeneous databases;database integration based;heterogeneous databases using;heterogeneous databases;database integration;databases using;databases using logic;databases;logic database integration;database;names integrate heterogeneous;integration based similarity;local names integrate;similarity local names;propose logic database;logic database;names integrate;based similarity local;integrate heterogeneous;similarity local;names mapped global;local names mapped;names mapped;finally integrate heterogeneous;global domain using;global domain;mapped global domain;local names;heterogeneous;based similarity", "pdf_keywords": ""}, "04e0fb8b3bb06e1200288e6d2a17d55773e97504": {"ta_keywords": "mobile users optimal;optimal policy optimal;users optimal policy;policy optimal;users cellular network;optimal policy;dependent opportunistic bandwidth;policy optimal policy;cellular network;mobile users cellular;opportunistic bandwidth sharing;convergence optimal policy;optimal policy propose;cellular network formulate;average reward process;opportunistic bandwidth;location dependent opportunistic;reward process mdp;reward linear;users cellular;bandwidth sharing;reward linear combination;average reward;reward process;step reward linear;bandwidth sharing static;multi timescale stochastic;timescale stochastic approximation;run average reward;dependent opportunistic", "pdf_keywords": "learning allocation bandwidth;policy cellular net;bandwidth allocation mobile;mobile user stochastic;bandwidth allocation stochastic;learning optimal bandwidth;users cellular network;fair bandwidth sharing;bandwidth static mobile;opportunistic dynamic bandwidth;dynamic bandwidth allocation;performance cellular networks;bandwidth sharing mobile;dependent opportunistic bandwidth;policy cellular;cellular net;cellular network propose;allocation mobile network;mobile users cellular;optimal bandwidth allocation;optimal throughput mobile;sharing policy cellular;cellular network static;cellular network;reward mobile users;cellular networks;model cellular network;learning allocation;bandwidth allocation strategy;opportunistic bandwidth sharing"}, "016d83091a60a6de67ba2395c063967686043380": {"ta_keywords": "variational probabilistic estimation;acoustic model adaptation;model adaptation speech;adaptation speech recognition;based variational probabilistic;adaptation speech;variational probabilistic;probabilistic estimation;model based variational;probabilistic estimation method;speech recognition;estimating posterior;acoustic model;model adaptation;method estimating posterior;estimating posterior value;based variational;problem acoustic model;posterior value parameter;variational;estimation method;estimation;acoustic;probabilistic;novel method estimating;parameter model based;estimating;method estimating;parameter model;estimation method illustrated", "pdf_keywords": ""}, "06708348b64e2e7b11a953389556c701bf3298da": {"ta_keywords": "estimation large number;random variables estimation;large estimation;variables large estimation;variables estimation large;estimation large;problem estimation large;large estimation large;random variables large;estimation;random variables reduced;consider problem estimation;problem estimation;variables estimation;number random variables;large number random;random variables;random variables provided;variables large;small number random;variables large consider;large number;reduced small number;provided number random;variables reduced small;number random;variables reduced;random;reduced small;small number", "pdf_keywords": ""}, "0098123efc851b67137c1028f7bac8d8bffbc8fd": {"ta_keywords": "pretrained language models;language models plms;learning pretrained language;pretrained language;language models;learning pretrained;approach learning pretrained;learning;models plms;models plms approach;grounding domain demonstrate;language;pretrained;approach learning;signal domain grounding;plms;models;novel approach learning;domain demonstrate;domain grounding;grounding domain presence;examining grounding domain;signal domain revealed;domain demonstrate approach;signal domain;grounding domain;signal;plms approach;grounding;domain grounding domain", "pdf_keywords": "semantic parsing models;knowledge concept grounding;downstream semantic parsing;semantic parsing;grounding pretrained language;language models understandable;concepts grounding;comprehension semantic parsing;parsing models;concept grounding;pretrained language models;parsing models investigate;concepts grounding quantifies;semantic parsing efforts;practicable learns grounding;concept knowledge representation;learns grounding;reading comprehension semantic;learn latent grounding;machine reading comprehension;pretrained language model;concept representation exploit;grounding learned;concept train groundingin;question concept representation;language models;knowledge extraction tasks;learns grounding module;concept representation;parsing"}, "45f59bd3ef8e1d76474199c08c140675c04a728c": {"ta_keywords": "multiagent learning agents;multiagent learning rules;heterogeneous multiagent learning;learning agents;multiagent learning;learning agents anticipate;heterogeneous multiagent;conjectures learning processes;learning rules;learning processes devise;learning processes;novel heterogeneous multiagent;learning rules synthesis;conjectures apply learning;forming conjectures learning;framework heterogeneous multiagent;multiagent;apply learning rules;conjectures learning;processes devise learning;agents;learning rules using;agents anticipate reactions;agents anticipate;devise learning rules;learning;devise learning;rules using variational;apply learning;novel heterogeneous", "pdf_keywords": ""}, "a13c580250af3644fe368b08a540f4ea65dac919": {"ta_keywords": "compressive phase retrieval;general compressive phase;compressive phase;signals fisher information;efficient measurement fisher;fisher information noisy;measurement fisher information;unconstrained signals fisher;information noisy signal;phase retrieval;general compressive;signals fisher;recovering complex signal;fisher information;compressive;phase retrieval problem;noisy unconstrained signals;noisy signal based;signal noisy intensity;noisy intensity measurements;stochastic measurement scheme;robust noisy unconstrained;called general compressive;robust noise;complex signal noisy;stochastic measurement;fisher information key;stochastic measurement technique;measurement fisher;robust noisy", "pdf_keywords": ""}, "0b2e9e978898b9fb1116ea964c8c470086ceed87": {"ta_keywords": "detection salient objects;net detection salient;detection salient;depth enhanced feature;salient objects presence;salient objects;depth cues channel;informative depth cues;depth cues;depth enhanced backbone;introduce depth enhanced;depth enhanced;cues channel spatial;salient;enhanced feature;channel spatial views;bbs net detection;enhanced backbone;backbone independent feature;net detection;cues channel;detection;spatial views;feature module dem;views;dem introduce depth;enhanced feature module;strategy depth enhanced;feature;feature aggregation", "pdf_keywords": "saliency depth representation;extracting saliency depth;depth guided saliency;saliency depth existing;representation saliency depth;saliency depth;cascaded decoder saliency;object detection salient;object detection saliency;salient object detection;multiscale saliency;salient object depth;detection saliency;saliency detection context;network novel saliency;decoder saliency;network extracting saliency;decoder saliency mappings;salient objects deep;rgb salient object;model purpose saliency;saliency detection;framework proposed saliency;purpose saliency detection;guided saliency model;saliency learning;saliency based feature;salient objects depth;saliency robust;rgb salient"}, "0b8dbc4a899c836fe2b1a213b9dc064cdf62fd63": {"ta_keywords": "explanations sentences procedural;constructs explanations sentences;explanations sentences;process comprehension benchmark;finding good explanations;comprehension benchmark end;recent process comprehension;comprehension benchmark;better explanation accuracy;constructs explanations;sentences procedural text;process comprehension;sentences procedural;explanations;end task benchmark;benchmark end task;explanations come expense;task benchmark surprising;end task performance;explanations come;explanation accuracy compared;procedural text achieving;explanation accuracy;good explanations come;good explanations;task benchmark;present constructs explanations;comprehension;task performance;procedural text", "pdf_keywords": "reading comprehension hotpot;text multitask learning;procedural text multitask;multihop reading comprehension;comprehension natural language;explanations sentences procedural;natural language inference;sentences procedural text;reading comprehension;key achieving comprehension;construct explanations sentences;recent process comprehension;explanation natural language;sentence modeling passage;language inference task;achieving comprehension;passage sentence modeling;achieving comprehension natural;procedural text passage;efficient unstructured explanations;comprehension hotpot;natural language models;language inference;advances understanding linguistic;process comprehension benchmark;process comprehension;comprehension benchmark;language advances understanding;text multitask;procedural text explanation"}, "f07c5c540233b22f0ca154c80c713e2aed3c9606": {"ta_keywords": "optimization gans;optimization gans experiments;gan purpose minimizing;hybrid gan;method music generation;hybrid gan purpose;gans experiments;amenable optimization gans;use hybrid gan;music generation;gans experiments pre;music generation based;gans;gan purpose;bert model discriminator;gan;model discriminator;discriminative metric hybrid;method music;novel method music;novel discriminative metric;trained mesh bert;novel discriminative;discriminative metric;music;bert model;trained mesh;mesh bert model;pre trained mesh;use novel discriminative", "pdf_keywords": ""}, "d15a7d00897f58a94def2a58c0cb0311851f2968": {"ta_keywords": "speech recognition asr;recognition asr chime;speech recognition;asr chime challenge;baseline automatic speech;automatic speech recognition;chime challenge proposed;recognition asr;chime challenge;generalized eigenvalue beamforming;automatic speech;asr chime;eigenvalue beamforming bidirectional;trained augmented microphones;short term memory;eigenvalue beamforming;beamforming bidirectional long;enhanced data beamforming;data beamforming;augmented microphones plus;augmented microphones;time delay neural;beamforming bidirectional;microphones plus;memory lds mask;mask estimation time;delay neural;chime;microphones;beamforming", "pdf_keywords": "deep learning speechwe;deep learning speech;speech recognition present;learning speech recognition;speech recognition;speech recognition framework;deep learning;automatic speech recognition;propose speech recognition;developments speech enhancement;learning deep learning;learning speechwe;learning speechwe present;deep learning deep;present speech recognition;speech recognition asr;deep learning achieved;speech recognition aasr;training deep learning;speech recognition utilizes;speech enhancement sr;baseline automatic speech;speech processing;learning speech;speech separation recognition;deep learning approach;speech enhancement;deep learning networks;learning deep;automatic speech"}, "04b44c518b145be625ff270af56cfd2e37900137": {"ta_keywords": "using distributed microphones;distributed microphones proposed;distributed microphones;end neural diarization;single channel recordings;neural diarization eend;channel recordings;microphones;microphones proposed method;microphones proposed;neural diarization;diarization eend using;temporal llerc encoders;diarization eend;encoders;encoders spatio temporal;llerc encoders;input model adaptation;channel input model;encoders spatio;recordings;multi channel input;channel recordings demonstrate;end neural;eend multi channel;end end neural;encoders process multi;channel input;llerc encoders process;recordings demonstrate proposed", "pdf_keywords": "speaker activity encoder;sensor speech;speaker voice activity;sensor speech processing;distributed speech recognition;sensor sensor speech;voice activity;voice activity detection;detecting speaker activity;encoders speech;speaker activity;activity detection speakers;distributed speech;signals distributed microphones;context distributed speech;distributed microphones replace;distributed microphone;speech recognition;distributed microphones;microphone;microphones replace encoders;speaker diarization capable;based distributed microphones;multi channel encoders;encoders speech problems;channel target speaker;distributed microphones capable;detection speakers layer;speech processing;speaker activity proposed"}, "a56dba9cabfc110df231051d7c9d6e439f6757dd": {"ta_keywords": "speech ssp tagging;tagging highly domain;tagging;pointwise ssp taggers;tagging highly;ssp tagging highly;ssp tagging;active learning proposed;speech ssp;method speech ssp;active learning;ssp taggers;taggers allow active;taggers;sequence based predictors;taggers allow;tags stacking pointwise;tags;learning proposed;learning proposed method;proposes method speech;estimates ssp tags;ssp tags;ssp taggers allow;method speech;ssp tags stacking;tags stacking;highly domain adaptable;allow active learning;trainable pointwise ssp", "pdf_keywords": ""}, "2ab9fd2be2bf82e0bbd558cc64c1c46728fc4f8a": {"ta_keywords": "model changes speech;changes speech characteristics;adaptation acoustic model;changes speech;method adaptation acoustic;adaptation acoustic;speech characteristics induced;acoustic model language;time scale conversation;scale conversation method;speech characteristics;incremental adaptation;acoustic model;based incremental adaptation;conversation method based;adaptation model time;incremental adaptation model;scale conversation;conversation method;language model;changes time scale;adaptation model;method adaptation;language model demonstrate;model language model;model time evolution;speech;model changes;macroscopic time scale;changes time", "pdf_keywords": ""}, "f17e182fcb7fbbff2257824174ed6f7df512a42b": {"ta_keywords": "end speech recognition;speech recognition;multi en resolution;speech recognition problem;resolution framework multidimensional;en resolution framework;cd based multidimensional;en resolution;resolution framework;multidimensional end;end end speech;end speech;multidimensional end end;multidimensional;multidimensional respectively models;word error rate;based multidimensional;resolution;framework multidimensional end;based multidimensional respectively;speech;heterogeneous global cd;multidimensional respectively;framework multidimensional;multi en;novel multi en;global cd based;recognition;relative word error;recognition problem proposed", "pdf_keywords": "speech encoder;attention encoder described;speech encoder trained;attention encoder;speech recognition asr;recognition asr decoder;multi encoder;problem speech encoder;encoder multi decoder;encoder decoder joint;recognition speech;encoder proposed approach;speech recognition;resolution multi encoder;text encoder;automatic speech recognition;text encoder trained;multi encoder multi;encoder proposed;encoder multi;recognition asr;encoder decoder;based attention encoder;encoder proposed model;automatic speech;propose novel encoder;multi decoder;encoder;joint network encoders;recognition speech level"}, "72b4ff7387223cf0398c298c3cc62ee07d9c0043": {"ta_keywords": "sentence completion task;sentence completion;gram language models;language models probability;sentence estimated probability;estimated probability lexicalisation;simple language models;syntactic dependency tree;probability sentence estimated;language models;approach sentence completion;sentence estimated;lexicalisation given syntactic;improves gram language;given syntactic dependency;probability lexicalisation;probability lexicalisation given;syntactic dependency;language models percentage;completion task microsoft;gram language;approach improves gram;models probability sentence;completion task;completion task using;syntactic;given syntactic;lexicalisation;dependency tree;improves gram", "pdf_keywords": ""}, "57fec656119e82b5e70b1a654f6d87d8c1137ef4": {"ta_keywords": "annotated biological figures;probabilistic topic model;structured probabilistic topic;topic model;captioned life science;topic model built;sampling information retrieval;probabilistic topic;figure generation;annotated biological;gibbs sampling;gibbs sampling information;collapsed gibbs sampling;biological figures;realistic figure generation;figure generation scheme;captioned life;ir engines captioned;biological figures derive;captioned captioned life;information retrieval;structured probabilistic;retrieval visualization;retrieval visualization resulting;information retrieval visualization;new structured probabilistic;engines captioned captioned;based collapsed gibbs;captioned captioned captioned;structurally annotated biological", "pdf_keywords": ""}, "2797a36cd15b8c046683247995261546993c289d": {"ta_keywords": "temporal speech noise;speech noise modeling;stationary noise based;speech recognition;spectral temporal speech;speech noise;audible quality speech;noise based spatial;speech recognition presence;non stationary noise;stationary noise;introduce speech recognition;proposed improves audible;temporal speech;noise modeling;noise based;dynamic variance adaptation;noise modeling combined;improves audible;improves audible quality;improves keyword recognition;keyword recognition accuracy;audible;keyword recognition;variance adaptation proposed;speech substantially improves;audible quality;variance adaptation;dynamic variance;based spatial spectral", "pdf_keywords": ""}, "99bb811beb5d061d2b8fac5a1973b49cace93e2f": {"ta_keywords": "tracking timevarying consumer;current purchase logs;timevarying consumer purchase;real purchase logs;track changes interests;purchase logs;purchase behavior interests;purchase logs previously;timevarying consumer;estimated interests trends;consumer purchase behavior;interests trends use;interests trends based;purchase logs demonstrate;interests trends change;tracking timevarying;changes interests trends;purchase behavior;model tracking timevarying;trends change time;adaptively track changes;behavior interests trends;previously estimated interests;interests trends;based current purchase;logs previously estimated;track changes;estimated interests;model adaptively track;consumer purchase", "pdf_keywords": ""}, "291b651654565cd88e4e56de5250219a71882a50": {"ta_keywords": "agents peer selection;selection agents peer;peer selection problem;weighted peer review;peer selection;weighted peer;nodes weighted peer;robust selection agents;peer review;agents peer;algorithm robust selection;selection agents;selection problem algorithm;feedback nodes weighted;robust selection;accurately robustly select;select subset agents;robustly select;peer review uses;peer;nodes weighted;algorithm robust;robustly select subset;subset agents;new algorithm robust;feedback nodes;algorithm robust presence;selection problem;selection implement;accuracy selection implement", "pdf_keywords": "weighted peer selection;weighted peer nomination;peer reviewing algorithm;accurate peer reviewing;outperform peer selection;peer selection alternative;algorithm weighted peer;quality peer selection;peer selection algorithm;nomination accurate peer;peer nomination accurate;weighted peer;strategyproof peer selection;propose peer selection;peer selection;peer reviewing;underlying peer selection;available reviews algorithm;peer selection model;peer nomination;improve quality peer;extend peer nomination;peer review;known outperform peer;accurate peer;given peer based;outperform peer;peer selection literature;peer nomination handle;form peer review"}, "9cf4609178e2739ed35f8d3e3d6efb7d5e2e1a41": {"ta_keywords": "dynamics traffic traffic;learn dynamics traffic;behavior traffic dynamics;dynamics traffic;traffic dynamics approach;traffic dynamics;unstable behavior traffic;detection unstable behavior;eigenvalues learned dynamics;traffic traffic;unstable eigenvalues learned;behavior traffic;learned dynamics anomaly;dynamics anomaly feature;traffic traffic proposed;automated detection unstable;traffic;sequences unstable eigenvalues;detection unstable;eigenvalues learned;traffic proposed method;approach learn dynamics;unstable eigenvalues;dynamics anomaly;traffic proposed;unstable behavior;learn dynamics;queue lengths accident;anomaly feature;operator approach learn", "pdf_keywords": ""}, "74495e9735b601ce9060ac40ac27d196fdbf7462": {"ta_keywords": "regenerating codes distributed;codes distributed storage;distributed storage code;regenerating codes;storage code construction;storage code;class regenerating codes;codes distributed;large repair bandwidth;distributed storage;repair bandwidth optimal;repair bandwidth;flexible class regenerating;storage;class regenerating;regenerating;code construction provided;code construction;codes;regime large repair;code;distributed;large repair;repair;bandwidth optimal;propose flexible class;bandwidth optimal certain;flexible class;bandwidth;propose flexible", "pdf_keywords": ""}, "f75ba81828fd9d8c7fcba89dd98a0ee73d32dce6": {"ta_keywords": "xmath2 compounds magnetic;magnetic properties xmath0;xmath1 xmath2 compounds;xmath2 compounds;properties xmath0 xmath1;xmath1 xmath2;field magnetic properties;xmath0 xmath1 xmath2;properties xmath0;magnetic properties;compounds magnetic field;compounds magnetic;xmath2;xmath0 xmath1;xmath1;field magnetic;magnetic field magnetic;applied field magnetic;xmath0;effect magnetic field;magnetic field;effect magnetic;magnetic field strongly;field magnetic field;magnetic;study effect magnetic;strength applied field;field strongly;strongly dependent strength;field strongly dependent", "pdf_keywords": ""}, "58fd3001c88e9784b0794eef06cb7c0eab0d8747": {"ta_keywords": "images interacting systems;synthesis images interacting;ontology resulting images;interacting systems;representation resulting images;model interacting particles;interacting systems approach;approach synthesis images;interacting particles model;generate representation approach;synthesis images;images interacting;generate representation;model interacting;interacting particles;simple model interacting;generate representation resulting;based ontology resulting;used generate representation;ontology resulting;applied simulation simple;particles model;applied simulation;particles model used;simulation;interacting;images used generate;simulation simple model;systems;ontology", "pdf_keywords": ""}, "c9472731afe5fca98f362f49e26d17a9d5d0cc8e": {"ta_keywords": "importance black box;black box problem;algorithms require interpretability;black box user;black box;learning algorithms require;require interpretability counterexample;interpretability counterexample;interpretability counterexample ways;need black box;machine learning algorithms;machine learning;box problem counterexample;user algorithm does;algorithms require;algorithm does need;interpretability;learning algorithms;existence importance black;ways machine learning;require interpretability;algorithms;user algorithm;box user;algorithm does;learning;importance black;ways user algorithm;box problem;counterexample widespread", "pdf_keywords": "challenge concept interpretability;explanation algorithms argue;interpretation algorithms argued;interpretability crucial;explanations interpretation algorithms;algorithms definition interpretability;interpretability presented demonstrated;concept interpretability;interpretability crucial issue;interpretability;explanationthe definition interpretability;interpretability presented;use interpretability;explanation ability algorithms;interpretability explicability;concept interpretability seeks;interpretability seeks;say machine learning;interpretability appealing;argue notions interpretability;interpretability matters 2there;definition interpretability crucial;interpretation algorithms understood;interpretability central;interpretation algorithms;interpretability appealing external;interpretability thethis;better articulations interpretability;interpretability seeks encourage;interpretability context machine"}, "b103bb1dc05a48796a3ff0804c11909bf68db11b": {"ta_keywords": "sentence instabilities supervised;detecting sentence instabilities;detect sentence instabilities;sentence instabilities biomedical;detect sentence;detecting sentence;approach detecting sentence;approach detect sentence;instabilities biomedical text;syntactic features proposed;sentence instabilities;text based detection;syntactic features;context syntactic features;biomedical text based;biomedical text;instabilities supervised;instabilities supervised manner;cues context syntactic;text based;instabilities biomedical;syntactic;text;supervised;detection cues context;context syntactic;supervised manner achieve;supervised manner;detection cues;sentence", "pdf_keywords": ""}, "83ac0851a8f6fa02f5db251b260f635907d7a01e": {"ta_keywords": "fast navigation unobserved;robust fast navigation;navigation unobserved;navigation unobserved environment;fast navigation;moving object trajectory;approximation trajectory moving;object trajectory;object trajectory relative;trajectory moving object;trajectory relative reference;good approximation trajectory;navigation;trajectory relative;trajectory moving;trajectory;approximation trajectory;controlled robust fast;description trajectory moving;moving object;moving object good;description trajectory;approach controlled robust;based concept parry;object demonstrate parry;concept parry;controlled robust;concept parry combined;combined description trajectory;robust fast", "pdf_keywords": "efficiently navigate world;complete trajectory search;navigate unstructured;successfully navigate unstructured;trajectory search;efficiently navigate;trajectory search space;navigate unstructured world;graph efficiently navigate;planning navigation;autonomous navigation;optimal action decoding;learning planning navigation;model autonomous navigation;trajectory explore;search trajectory;search trajectory simply;partial trajectory explore;representation navigation task;navigation task strategy;action decoding;representation navigation;systems navigation;search systems navigation;based representation navigation;navigation task;navigate world model;strategy able navigate;approach navigation;navigate world"}, "b02acb3d159b06b2319a164378e1e61c3983676f": {"ta_keywords": "generalizability complex queries;generalizability complex query;complex queries provide;complex queries;complex query models;complex queries new;forms complex queries;benchmark combinatorial generalizability;query models benchmark;combinatorial generalizability complex;query models;queries provide detailed;queries;queries provide;complex query;combinatorial generalizability commonly;new benchmark combinatorial;study combinatorial generalizability;benchmark combinatorial;combinatorial generalizability;generalizability complex;dataset benchmark combinatorial;queries new;generalizability commonly used;generalizability commonly;generalizability;models benchmark;models benchmark based;queries new dataset;query", "pdf_keywords": "qo knowledge graphs;queries knowledge graphs;benchmark combinatorial generalizability;generalizability complex query;knowledge graphs queries;knowledge graphs large;describing queries knowledge;benchmark query types;qcd benchmark combination;queries knowledge;qn models thatwe;query answering models;benchmark query typeswe;generalizable benchmark operators;query types benchmark;qn models;scalable generalizable benchmark;qcd benchmark;knowledge graphs;quantum network qn;qn models extend;knowledge graphs extend;complex query answering;natural queries;describing queries;q_ based qwe;answering models efo;combinatorial generalizability learning;complexity query answering;answering models"}, "eaa224ae5c969180503dda4972ab86d3a71c888c": {"ta_keywords": "polyphonic music recognition;optical music recognition;polyphonic file classification;music recognition;music recognition sheet;recognition sheet music;music recognition create;music recognition treating;music recognition problem;based polyphonic file;datasets suitable polyphonic;polyphonic file;end polyphonic music;end optical music;polyphonic music;suitable polyphonic music;end end polyphonic;approach based polyphonic;optical music;end polyphonic;suitable polyphonic;based polyphonic;file classification architecture;polyphonic;sheet music;file classification;sheet music publicly;recognition sheet;recognition;encoder architecture proposed", "pdf_keywords": "polyphonic music recognition;optical music recognition;music recognition omr;music recognition;polyphonic music encoder;music encoder;automatically identify polyphonic;end optical music;resonance labeling music;prediction polyphonic music;polyphonic optical music;music dataset propose;prediction prediction polyphonic;prediction polyphonic;music dataset;identify polyphonic music;architectures polyphonic music;segmentation resonance labeling;based segmentation resonance;music encoder used;new architectures polyphonic;polyphonic optical;optical music;approach polyphonic music;datasets musescore;polyphonic music;optical music engraving;new approach polyphonic;pattern recognition omr;architectures polyphonic"}, "baad427b45ac691763fe3de4ea3ac1bffd3c74e3": {"ta_keywords": "visualizing changes affiliation;crowd data political;visualizing changes;visualize changes;crowd data;data political social;temporal geographic affiliation;available crowd data;temporal geographic party;able visualize changes;person view visualizing;view visualizing changes;analyze temporal geographic;visualization tool;affiliation people crowd;political social events;visualization;present new visualization;new visualization;users analyze temporal;view visualizing;demographic backgrounds tool;geographic affiliation people;new visualization tool;tool able visualize;visualization tool studying;time person crowd;data political;visualizing;affiliation person view", "pdf_keywords": ""}, "64280761641d8f1eb285165160bd96efac0bb5f5": {"ta_keywords": "simultaneous recognition speech;recognition speech environmental;environmental sounds simultaneously;recognize speech environmental;recognition speech;speech environmental sounds;learning simultaneous recognition;recognize speech;simultaneous recognition;neural network dnn;techniques recognize speech;dnn techniques recognize;speech environmental;environmental sounds investigate;learning simultaneous;deep neural network;environmental sounds;dnn techniques;sounds simultaneously;problem simultaneous recognition;task learning simultaneous;using deep neural;network dnn techniques;deep neural;multi task learning;network dnn;tasks respective noisy;sounds simultaneously improve;single multi task;multi task", "pdf_keywords": ""}, "2391e7446d47f681ad705c8e75d9d2ce1b92ad5f": {"ta_keywords": "rem project;dm based knowledge;rem project effort;based knowledge project;knowledge project;resolution dm literature;knowledge project based;based knowledge;rem;reference corpus middle;literature evaluation dm;dm literature evaluation;israel science;israeli israel science;israel science foundation;knowledge;build reference corpus;dm literature;science foundation israeli;reference corpus;funded israel science;foundation israeli project;israeli project;evaluation dm based;based work dm;project based;project funded dm;project based work;dm based;literature evaluation", "pdf_keywords": ""}, "ff7e60b8d336aef5ed974609a63610641085177e": {"ta_keywords": "softmax distribution estimation;matrix model softmax;estimating softmax distribution;model softmax distribution;introduce softmax distribution;softmax distribution;estimating softmax;model softmax;approximately estimating softmax;softmax distribution temperature;softmax;introduce softmax;random matrix model;model random matrix;estimation random matrix;matrix model random;likelihood random matrix;multiclass classification real;image captioning;random matrix;multiclass classification;maximum likelihood random;matrix model;synthetic data multiclass;classification;captioning;classification real data;classification real;data image captioning;data multiclass classification", "pdf_keywords": "softmax distribution training;softmax distribution estimation;learn softmax distribution;estimating softmax distribution;softmax distribution provide;softmax distribution present;softmax distribution;softmax distribution maximum;introduce softmax distribution;estimation based softmax;propose softmax distribution;called softmax distribution;representation softmax distribution;softmax distribution use;softmax approximation empirical;framework softmax distribution;estimating softmax;based softmax approximation;based estimating softmax;terms softmax distribution;specific reward learning;softmax approximation;reward learning;reward learning strategy;learn softmax;lam softmax distribution;framework learn softmax;softmax;reinforcement learning learning;introduce softmax"}, "4d991a83d6044b1aaed2c117b3d097ecd23cf6f4": {"ta_keywords": "end speaker diarization;speaker diarization model;model speaker diarization;speaker diarization;speaker diarization problem;approach speaker diarization;speaker diarization novel;neural model speaker;formulates speaker diarization;end speaker;end end speaker;problem formulates speaker;model speaker;novel approach speaker;formulates speaker;diarization model based;attention based neural;end neural model;approach speaker;speaker;end end neural;end neural;diarization model;diarization novel approach;diarization;directly minimize diarization;minimize diarization errors;diarization errors self;diarization problem;neural network", "pdf_keywords": "speaker diarization clustering;diarization clustering speaker;speaker embeddings clustering;speaker diarization based;clustering speaker embeddings;processing speaker diarization;clustering speaker;speaker diarization structured;speaker diarization method;speaker diarization;speaker recognition;speaker diarization proposed;approach speaker diarization;novel speaker diarization;applied speaker diarization;generic speaker diarization;model speaker recognition;speaker diarization challenge;speaker recognition proposed;speech speaker diarization;speaker diarization eend;clustering loss speaker;speaker diarization problem;speaker diarization speech;clustering method audio;speaker embedding method;axis speaker diarization;recording speaker segments;features corresponding speaker;speaker embedding"}, "9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e": {"ta_keywords": "encoders domain retrieval;large dual encoders;dual encoders domain;domain retrieval tasks;tasks dual encoders;retrieval tasks dual;domain retrieval;dual encoders;performance dual encoders;dual encoder;encoders variety retrieval;retrieval tasks scaling;encoders generalizable outperform;sparse dense retrievers;dual encoders generalizable;encoders domain;dual encoder model;size dual encoder;encoder;encoders;retrieval tasks;retrieval;dual encoders variety;encoders generalizable;encoder model;encoder model able;dense retrievers npnppi;encoders variety;retrievers npnppi;retrievers npnppi database", "pdf_keywords": "generalization dense retrieval;dense retrieval models;retrieval models generalize;generalizable dense retrieval;dense retrieval model;neural retrieval models;retrieval performance dense;retrieval models powerful;retrieval model generalizable;retrieval tasks dual;matching dense retrieval;new dense retrieval;generalize domains retrieval;retrieval models;dense retrieval;domains retrieval tasks;retrieval model single;retrieval tasks scaling;retrieval model improved;vector retrieval model;popular neural retrieval;retrieval model;single vector retrieval;retrieval tasks challenge;neural retrieval;retrieval use neural;dense retrieval problem;large dual encoders;retrieval models using;improves retrieval performance"}, "d18d8d364bb18d66924919feebb2e892ebe6761c": {"ta_keywords": "statistical machine translation;neural machine translation;translation sm neural;machine translation;machine translation nmt;improve translation quality;machine translation sm;translation quality;successfully improve translation;translation quality different;improve translation;phrase based decoding;translation nmt exploiting;translation nmt;translation sm;decoding cost rerank;decoding cost nmt;sm neural machine;model compute phrase;based decoding cost;translation;sm neural;based decoding model;decoding cost;rerank nmt outputs;forced decoding cost;cost rerank nmt;decoding model;existing phrase based;quality different language", "pdf_keywords": "attention alignment decoding;neural machine translation;translation accuracy neural;machine translation powerful;machine translation sm;statistical machine translation;based attention alignment;machine translation;attention alignment;phrase based attention;phrase based decoding;machine translation nmt;machine translation algorithm;improve translation accuracy;thatneural machine translation;improving performance translation;phrasebased decoding;best translation output;translation powerful computational;phrase based neural;method improve translation;sm translation tasks;translation tasks;benchmark accuracy translation;decoding standard phrase;translation tasks nmt;aneural machine translation;accuracy translation model;translation ranking;translation ranking lexicon"}, "1b96b89d5b3ba444126cebefdfc665d3866f14f0": {"ta_keywords": "fair outcomes hiring;online hiring processes;outcomes online hiring;fair outcomes online;hiring processes;hiring processes formulate;outcomes hiring definitions;outcomes hiring;online hiring;hiring definitions incorporated;fair outcomes;hiring definitions;modern hiring pipeline;definitions fair treatment;fair treatment fair;treatment fair outcomes;hiring pipeline;definitions fair;hiring;modern hiring;challenges fair treatment;constitute modern hiring;treatment fair;set challenges fair;fair treatment;outcomes online;challenges fair;formulate definitions fair;outcomes;algorithms processes", "pdf_keywords": ""}, "c888022dec626171d243d2a056709b9b053a0ed9": {"ta_keywords": "speech recognition encoder;end speech recognition;decoder training acoustic;speech recognition;noise suppression speech;noisy speech benchmarks;speech enhancement proposed;approach speech recognition;training acoustic language;encoder decoder training;speech enhancement;acoustic signal processing;suppression speech enhancement;joint encoder decoder;recognition encoder decoder;acoustic language components;evaluated noisy speech;recognition encoder;speech benchmarks;encoder decoder;architecture joint encoder;joint encoder;speech benchmarks compared;processing noise suppression;encoder;decoder training;noisy speech;encoder decoder framework;acoustic language;training acoustic", "pdf_keywords": "speech enhancement attention;end speech recognition;end speech encoder;speech encoder trained;speech encoder;neural beamformer filter;inference speech enhancement;microphone mask estimation;neural beamformer based;speech encoder encoder;based neural beamformer;speech enhancement recognition;multichannel speech recognition;enhance speech recognition;task speech recognition;neural beamformer proposed;speech enhancement acoustic;objectives speech enhancement;speech recognition noisy;speech enhancement;network neural beamformer;trained neural beamformer;standard speech recognition;beamforming neural beamformer;optimized task speech;speech recognition architecture;speech recognition jointly;networks speech mask;microphone estimate mask;neural beamformer"}, "2526c510610c7220ecc56e6b08d09c4cbaf58c3c": {"ta_keywords": "regular expressions japanese;recognition regular expressions;recognition proper words;based anaphora resolution;expressions japanese language;anaphora resolution proposed;recognition regular;language based anaphora;anaphora resolution;approach recognition regular;regular expressions;expressions japanese;recognition proper;effective recognition proper;recognizing recognizing people;japanese language;recognizing recognizing;recognition correct subjects;recognizing people;problem recognizing recognizing;japanese language based;recognizing;effective recognition correct;effective recognition;recognition correct;method effective recognition;anaphora;problem recognizing;recognition;based anaphora", "pdf_keywords": ""}, "2a462e2b748d7e78f3af2621071265c1ad2683ea": {"ta_keywords": "dispatchable wind energy;optimal power flow;power flow efficiency;wind energy evaluation;evaluation power flow;problem dispatchable wind;flow efficiency power;power flow problem;wind energy;dispatchable wind;problem optimal power;efficiency power method;power flow;solving optimal power;wind energy presented;flow problem dispatchable;optimal power;flow efficiency;energy evaluation efficiency;flow problem method;energy evaluation power;evaluation efficiency power;applied problem dispatchable;power method applied;method solving optimal;efficiency power;power method;problem dispatchable;energy presented method;solving optimal", "pdf_keywords": ""}, "b61c9799f10e4de9cd222dfd8e423bbd950a7c44": {"ta_keywords": "extraction information text;extraction information;text information content;event based questions;information text;text information;questions build corpus;study extraction information;information content;corpus uncoated text;uncoated text information;information content proc;corpus;event based;information text focusing;simple event based;study extraction;event;build corpus;comparative study extraction;corpus uncoated;text;simple event;build corpus uncoated;information;extraction;content;uncoated text;based questions;content proc", "pdf_keywords": "event extraction;knowledge extraction;content knowledge extraction;event extraction proc;knowledge extraction process;extracting information natural;scale event extraction;extraction set documents;extracting information;knowledge underlying content;entity apply extraction;natural language data;distribution natural language;information natural language;documents answer particular;predict answer context;question based knowledge;underlying content knowledge;extracting;problem extracting information;answer idk questions;large scale event;knowledge underlying entity;answerable questions;answer context;answer question based;questions answerable questions;extracting large collection;extraction set;tasks based knowledge"}, "4ebe5792fe2890590e7a5bf8ae0a29e0fb147ef9": {"ta_keywords": "rewriting formulated semantic;formulated semantic segmentation;incomplete rewriting formulated;semantic segmentation task;incomplete rewriting;rewriting;rewriting formulated;semantic segmentation;problem incomplete rewriting;segmentation task;segmentation task proposed;segmentation;semantic;formulated semantic;datasets;public datasets;performance public datasets;local global information;incomplete;global information;task proposed;global information proposed;information proposed;task proposed approach;task;problem incomplete;information;information proposed approach;capture local global;capture local", "pdf_keywords": "utterance rewriting semantic;utterance rewriting;utterance rewriting approach;utterance rewriting task;incomplete utterance rewriting;rewrite incomplete utterance;utterances rewritten;utterance model trained;rewriting semantic segmentation;recurrent computation utterances;utterances rewritten natural;computation utterances;incomplete rewriting semantic;predict edit operations;computation utterances based;utterances approach outperforms;word level edit;rewriting semantic;utterance model;evaluation utterances rewritten;quality utterances;utterances based neural;task incomplete utterance;trained large corpus;utterances able capture;utterances;formulate incomplete utterance;style utterances;corpus unconstrained utterances;utterances multi turn"}, "e6fe601c44835d3654131d0312d65227d3523373": {"ta_keywords": "extraction coordinate term;text underlying graph;extract coordinate term;coordinate term text;extraction coordinate;extract coordinate;framework extraction coordinate;used extract coordinate;improve extraction coordinate;text underlying;features text underlying;framework extraction;underlying syntactic relations;features text;structure underlying syntactic;term text;propose framework extraction;underlying syntactic;syntactic relations;essential features text;term text improves;syntactic relations proposed;extract;based random walks;text;syntactic;term text used;improve extraction;graph structure;extraction", "pdf_keywords": ""}, "5a4d4c0824b5e113c39105c71d42b93d3900d87e": {"ta_keywords": "microtask based crowdsourcing;crowdsourcing engine;based crowdsourcing engine;crowdsourcing engine controls;crowdsourcing process including;crowdsourcing process;entire crowdsourcing process;crowdsourcing;controls entire crowdsourcing;based crowdsourcing;entire crowdsourcing;open source microtask;source microtask engine;microtask based;microtask engine;performing microtask based;source microtask;microtask engine performing;performing microtask;microtask;engine performing microtask;open source;details open source;task allocation worker;ranking answer aggregation;answer aggregation;answer aggregation means;task allocation;allocation worker ranking;task", "pdf_keywords": ""}, "e2b6193a24cd6c9f736139aa66618d1b8bf2a60b": {"ta_keywords": "manual transcription correction;transcription correction speech;transcription correction;correction speech transcripts;automatic manual transcription;speech transcripts tool;transcriber correction selection;manual transcription;guides transcriber correction;transcriber correction;transcripts tool;reducing transcription;transcription;speech transcripts;transcripts tool guides;context reducing transcription;tool guides transcriber;correction speech;errors transcriber;transcripts;guides transcriber;errors transcriber specifies;correction selection segments;transcriber;contain errors transcriber;transcriber specifies;correction selection;error reduction guided;transcriber specifies time;error reduction", "pdf_keywords": ""}, "fc848789b557a7581c51c79fd01897dc5aa7e8a8": {"ta_keywords": "multimodal interoperability smart;interoperability smart transformers;knowledge resource multimodal;model multimodal knowledge;multimodal interoperability;multimodal knowledge resource;multimodal knowledge;resource multimodal interoperability;models smart transformers;smart transformers model;multimodal task zematic;smart transformers;transformers model learned;model multimodal;domain multimodal task;novel model multimodal;resource multimodal;domain multimodal;multimodal task;smart transformers class;multimodal;interoperability smart;transformers model;transformers class adaptive;adaptive models smart;set transformers model;transformers;transformers model parameters;generative problem model;open domain multimodal", "pdf_keywords": "knowledge reasoning encoder;answer question knowledge;jointly reasoning knowledge;knowledge answer generation;knowledge sources answering;knowledge based answer;reasoning encoder;knowledge extraction image;visual language prediction;novel reasoning module;visual semantic matching;explicit knowledge extraction;visual semantic;knowledge language;reasoning encoder decoder;implicit knowledge reasoning;learning external knowledge;question knowledge based;answer generation trained;reasoning knowledge sources;model knowledge language;accuracy answer generation;domain multimodal task;reasoning knowledge;multimodal task;explicit knowledge bases;retriever answer generation;retrieve explicit knowledge;question knowledge;knowledge intensive tasks"}, "db729f2f55a92465cf88682ba7917621fd4c000b": {"ta_keywords": "students given linearly;given student strategy;student strategy performance;student strategy;learning tasks students;effective students given;formula effective students;tasks students given;effective students;tasks students;given student;performance learning tasks;algebraic formula students;learning tasks;linearly constrained self;self lemma linear;effect given student;students;linear time proofs;strategy performance learning;linear algebraic;students given;linearly;student;linearly constrained;performance learning;constrained self lemma;linear time;linear;lemma linear time", "pdf_keywords": ""}, "b2da0f022a48ebd10a23572b5310b7d7341b6448": {"ta_keywords": "spin orbit interaction;orbit interaction spin;interaction spin orbit;orbit coupled spin;coupled spin orbit;spin orbit coupled;dynamics spin orbit;enhance spin orbit;enhancement spin orbit;effect spin orbit;interaction dynamics spin;spin orbit;suppression spin orbit;coupled spin;interaction spin;dynamics spin;orbit interaction dynamics;orbit interaction;orbit interaction responsible;orbit interaction used;enhance spin;effect spin;enhancement spin;suppression spin;orbit coupled;responsible enhancement spin;responsible suppression spin;spin;used enhance spin;study effect spin", "pdf_keywords": ""}, "004ddf5a39a735d0f8ec7547629c2bee65eb1f93": {"ta_keywords": "biases peer review;testing biases peer;biases peer;testing biases;peer review;testing biases single;existence biases peer;peer review present;peer review framework;biases;biases single;biases single vs;investigate existence biases;experiment authors investigate;alarm probability bidding;framework testing biases;existence biases;probability bidding performed;probability bidding;peer;testing;experiment authors;authors investigate;results setup experiment;experiment;bidding performed;based recent experiment;bidding;recent experiment authors;recent experiment", "pdf_keywords": "testing biases peer;bias testing reviewers;test biases peer;biases peer review;bias peer review;reviewers induces biases;reviewers bias veracity;bias reviewers;bias reviewers based;testing biases;testing reviewers;testing reviewers based;bias testing;propose bias testing;describing bias testing;testing biases human;reviewers bias;tests detect biases;bias problem reviewers;test biases;quantifying statisticalagreement reviewers;reviewers papers experiment;independent reviewers bias;reviewers tests;biases peer;problem reviewers bias;relative bias reviewers;reviewers bias assignment;bias testing algorithms;testing biases relative"}, "1bd7d16340642948142d7608ef8f085d934d94a3": {"ta_keywords": "stochastic point method;optimization convex strongly;stochastic importance sampling;momentum unconstrained optimization;optimization convex;unconstrained optimization convex;propose stochastic point;strongly convex functions;convex strongly convex;strongly convex;stochastic point;standard stochastic point;convex strongly;efficient standard stochastic;importance sampling;ball momentum unconstrained;non convex functions;non convex;convex functions;method context stochastic;stochastic importance;unconstrained optimization;convex functions analyze;sampling standard stochastic;importance sampling standard;convex;context stochastic importance;point method spt;case non convex;optimization", "pdf_keywords": "method stochastic optimization;stochastic optimization;stochastic convexity optimization;stochastic convex optimization;optimization stochastic;method stochastic convex;optimization stochastic convexity;method momentum stochastic;stochastic momentum methods;stochastic optimization sps;momentum methods stochastic;stochastic optimization icra;conference stochastic optimization;stochastic point method;method converges stochastic;optimization convergence iterates;stochastic search method;convex optimization;free optimization;learning continuous control;stochastic points method;convexity optimization;convex optimization method;descent optimization convergence;including convexity optimization;iterates convex search;convex search;stochastic search;stochastic coordinate descent;consider stochastic search"}, "db8376698c06d6a688a39bff0300780ef0383821": {"ta_keywords": "corrugated board method;properties corrugated board;corrugated boards double;corrugated board;corrugated boards;class corrugated boards;corrugated boards used;face corrugated boards;mechanical properties corrugated;properties corrugated;double face corrugated;corrugated;class corrugated;new class corrugated;face corrugated;simultaneous control structure;board method based;method simultaneous control;board method;simultaneous control;control structure mechanical;board applied production;control structure;boards used manufacture;boards double face;boards double;use local control;boards used;local control unit;boards", "pdf_keywords": ""}, "4b1555368fd2c5f1234eaac5e41296003481754a": {"ta_keywords": "deep eutectic electrolyte;eutectic electrolyte;eutectic electrolyte dee;lithium oxygen battery;performance lithium oxygen;lithium oxygen;high performance lithium;electrolyte dee fulfill;oxygen battery xmath0;electrolyte dee;methylacetamide lithium bis;solid methylacetamide lithium;electrolyte;methylacetamide lithium;lithium bis;performance lithium;oxygen battery;lithium;battery xmath0;battery xmath0 kg;lithium bis trifluoromethaneyl;developed deep eutectic;battery;mixing solid methylacetamide;deep eutectic;solid methylacetamide;eutectic;bis trifluoromethaneyl imide;trifluoromethaneyl imide;oxygen", "pdf_keywords": ""}, "005879e6587eb6e05f56c20d345f784ee84a44c4": {"ta_keywords": "dependency syntax models;syntax models;nonsyntactic language models;syntax models use;models dependency syntax;language models;recurrent neural nets;dependency syntax;language models perform;better nonsyntactic language;improvement nonsyntactic language;generative models dependency;nonsyntactic language;models use recurrent;models different languages;use recurrent neural;models dependency;recurrent neural;use recurrent;neural nets;dependency;syntax;new generative models;generative models;different languages;languages;neural nets avoid;language;arabic japanese models;generative", "pdf_keywords": ""}, "f61862b286c9e4894302faf716eedb0eb60a2f5f": {"ta_keywords": "structure discussion forums;forums discussions mediated;discussion forums discussions;discussion forums;thread structure discussion;discussions mediated multiple;discussions mediated;forums discussions;recovering thread structure;forums;thread structure;inter message similarity;message similarity;recovering thread;discussions;approach recovering thread;post messages approach;temporal relationships messages;discussion;pre post messages;messages approach;messages postulated based;relationships messages;messages results proposed;post messages;messages approach uses;messages postulated;message similarity evaluate;messages connections;representation messages connections", "pdf_keywords": ""}, "20819855b9517c927a1262850146e525c8083fb4": {"ta_keywords": "predicting spoken word;short term memory;predicting spoken;model spoken language;predicts spoken word;recurrent neural networks;capable predicting spoken;spoken word sequences;spoken language understanding;predicts spoken;human dialog data;tags predicts spoken;memory lsd recurrent;term memory;human human dialog;human dialog;recurrent neural;term memory lsd;lsd recurrent neural;sequences context utterance;dialog data annotated;networks rnns model;spoken language;neural networks rnns;rnns model;language understanding;networks rnns;agent intentions hotel;rnns model trained;language understanding using", "pdf_keywords": ""}, "f74d4fa99ccedfea7a2662fe6944d99f34533912": {"ta_keywords": "fairness machine learning;adaptation fairness machine;adaptive adaptation fairness;model optimize fairness;optimize fairness;accuracy fairness method;fairness machine;optimize fairness constraints;adaptation fairness;accuracy fairness;fairness method;fairness constraints;fairness constraints group;classification thresholds demographic;adaptive classification thresholds;fairness;trade accuracy fairness;estimate adaptive classification;classification thresholds;algorithm adaptive adaptation;classification model optimize;adaptive classification;thresholds demographic;classification;thresholds demographic group;machine learning;distribution classification model;probability distribution classification;distribution classification;classification model", "pdf_keywords": "fairness constrained classification;fairness based classifier;fairness metrics supervised;new fairness estimation;unbalanced fairness metric;fairness optimization class;fairness optimization;classifier fair classification;fairness accuracy propose;optimal fairness model;class fairness metrics;dataset unbalanced fairness;accuracy fairness optimization;fairness accuracy optimal;fair classification;fair classification problem;fairness estimation;improve fairness accuracy;flexible fairness regularization;fairness constraints unified;propose fairness constrained;fairness constraints group;groups improve fairness;learning constraint fairness;fairness metrics performance;fairness metrics;fairness constraints;fairness regularization efficient;classification thresholds demographic;fairness model"}, "25fec1e150a273b3bec3655ace0ff6b97c338a96": {"ta_keywords": "erasures distributed storage;storage systems codes;distributed storage systems;errors erasures distributed;distributed storage;class distributed storage;storage systems;codes errors erasures;erasures distributed;storage systems presence;explicit regenerating codes;respect storage;storage;optimal respect storage;respect storage bandwidth;regenerating codes errors;errors erasures;regenerating codes;storage bandwidth;systems codes optimal;storage bandwidth requirements;capacity class distributed;systems codes;codes optimal respect;malicious adversaries;erasures;systems presence malicious;codes optimal;provide explicit regenerating;capacity class", "pdf_keywords": "resilient codes storage;erasures regenerating codes;resilient regenerating codes;adversaries regenerating codes;erasure coding distributed;regenerating codes resilient;regenerating codes networks;erasures codes optimal;resilient errors erasures;resiliency regenerating code;resilient codes;regenerating codes capable;regenerating codes network;error erasure resiliency;coding distributed storage;universally resilient codes;codes theorems storage;erasures optimality codes;errors erasures codes;error erasure capacity;codes resilient errors;resilient codes theorems;fully erasure codes;erasure codes supported;erasures distributed storage;repair erasure coding;regenerating codes consider;consider regenerating codes;erasures codes;erasure coding"}, "2875d6cac905c3dfec4c8a8e1d89a2a7e4d71d40": {"ta_keywords": "replication distributed networks;replication wireless storage;wireless storage replication;storage replication distributed;distributed storage replication;storage replication wireless;replication distributed;replication wireless;distributed networks;problem distributed storage;storage replication;repair bandwidth node;distributed storage;nodes reduces repair;capacity repair bandwidth;reduces repair bandwidth;replication;failed nodes reduces;partially failed nodes;bandwidth partial repair;distributed networks derive;failed nodes based;repair bandwidth;distributed;multiple failed nodes;wireless storage;failed nodes;information flow graphs;storage capacity repair;capacity repair", "pdf_keywords": "distributed storage repair;distributed storage nodes;distributed storage code;storage networks nodes;storage nodes repair;storage repair bandwidth;repair bandwidth distributed;node repair bandwidth;storage repair schemes;node optimal repair;failures distributed storage;codes clustered storage;storage repair scheme;nodes storage;nodesthe repair bandwidth;storage repair network;storage nodes;optimal repair node;minimum repair bandwidth;clustered storage networks;repair node optimal;bandwidth node repair;optimal storage repair;codes minimum storage;bound repair bandwidth;problem distributed storage;consider distributed storage;bandwidth storage nodes;storage networks;nodes clustered storage"}, "190865e2c3d4908ff20bf9a31f5a2773d6fec5cb": {"ta_keywords": "question retrieval;step question retrieval;question retrieval process;translation open domain;questions key feature;retrieval;questions associated;domain questions associated;open domain questions;retrieval process accomplished;translation open;new translation open;information large collection;large collection questions;retrieval process;retrieving relevant information;questions key;questions;retrieving relevant;information large;big data featureless;translation;domain questions;questions associated problem;large collection;collection questions key;present new translation;big data;new translation;relevant information large", "pdf_keywords": "question retrievalbased quantum;retrievalbased quantum information;retrievalbased quantum;question retrievalbased;question retrieval;question answering;question retrieval task;memory generated retrieval;memory efficient iq;corpus question retrievalbased;memory answer questions;step question retrieval;quantum information qed;generated retrieval;quantum information knowledge;generated retrieval scores;retrieval task;answered iq;question answering problem;question generation model;retrieval;retrieval task method;question answered iq;answer retrieved;faster existing iq;answered iq combines;datasets q_ qc;answer retrieved data;information qed;retrievalbased"}, "4dc8ddef938699d0d8a0b685ad9f56d0b735a25d": {"ta_keywords": "learns incrementally;concept learner algorithm;learns incrementally improve;learning learns incrementally;explanation based learning;concept learner;learner algorithm based;learner algorithm;learned object logical;learning learns;based learning learns;learned object;incremental abductive;incremental abductive explanation;incrementally improve;learns;performance concept learner;abductive explanation based;algorithm called incremental;called incremental abductive;incrementally;assumption learned object;incremental;based learning;based assumption learned;learning;logical object algorithm;called incremental;object algorithm based;new algorithm called", "pdf_keywords": ""}, "5677d2b565c8265fef1693a9be861739cb01bf2f": {"ta_keywords": "vocabulary speech recognition;large vocabulary speech;optimization large vocabulary;speech recognition;speech recognition systems;meta parameters optimized;vocabulary speech;parameters optimized optimization;performance optimization;model meta parameters;meta parameters tuned;optimization underlying model;optimized optimization;performance optimization large;simulation meta parameters;parameters optimized;design performance optimization;hidden model meta;model meta;optimized optimization underlying;optimized;optimization;speech;optimization large;large vocabulary;meta parameters;recognition systems proposed;parameters tuned strategy;optimization underlying;simulation meta", "pdf_keywords": ""}, "3d2da57c2de69b02fa0fee5c12ace618718a3926": {"ta_keywords": "cells meta learning;meta learning scheme;meta learning;cells meta;cells cells meta;predict behavior cells;learning scheme biological;present meta learning;cells;behavior cells cells;behavior cells;stacked graphical model;biological used predict;cells cells;learning scheme based;based stacked graphical;learning scheme;stacked graphical;learning;subfigures generated graphs;graphs text stacked;text stacked graphical;graphical model characterized;biological;graphical model;predict behavior;able predict behavior;subfigures;graphical model validated;scheme biological", "pdf_keywords": ""}, "d21a0e01514732f241b9c138eceb76ecaef17a27": {"ta_keywords": "effective machine translation;better translation models;machine translation;translation models;translation models effort;active learning;particularly annotators translate;easy human translators;machine translation mt;annotators translate short;train better translation;active learning framework;translate short phrases;annotators translate;human translators translate;human translators;translators;short phrases instead;translate short;phrases instead sentences;translation;effort particularly annotators;translators translate paper;translators translate;translation mt making;short phrases;efficiently train statistical;better translation;particularly annotators;selecting informative examples", "pdf_keywords": ""}, "b4bc1a98eb79545f8da4385a6dfb643b0c62a07e": {"ta_keywords": "simultaneous translation discrete;translation discrete media;translation discrete;discrete syntax;based discrete syntax;simultaneous translation;method simultaneous translation;discrete syntax underlying;syntax;syntax target sentence;predictions syntax constituents;sentence predictions constituents;syntax target;syntax constituents;combines predictions syntax;syntax underlying;syntax constituents target;target sentence predictions;sentence predictions;predictions syntax;sentence accurate predictions;predictions syntax target;sentence proposed methods;media based discrete;make predictions syntax;discrete media based;discrete media;constituents target sentence;target sentence proposed;target sentence", "pdf_keywords": ""}, "425a4a9c0598e4101ca2f2b930f5c6986ce40a99": {"ta_keywords": "privacy language models;optimization utility privacy;utility privacy language;memorization language models;privacy language;np assisted search;term memorization language;language models methods;assisted search algorithms;language models;term memorization;assisted search;favorable utility privacy;loss term memorization;language models shown;search algorithms;memorization language;privacy trade offs;utility privacy;utility privacy trade;privacy;privacy trade;memorization;search;joint optimization utility;accuracy np assisted;methods joint optimization;optimization utility;methods favorable utility;algorithms", "pdf_keywords": "privacy regularization;privacy regularization methods;introduce privacy regularization;trained differential privacy;machine learning privacy;propose adversarial regularizer;optimization utility privacy;tuning optimal privacy;learning privacy;adversarial regularizer;adversarial training regularization;adversarial regularizer purpose;optimizing privacy;differential privacy train;optimal privacy utility;optimizing privacy large;regularizing deep learning;learning privacy users;privacy train models;technique optimizing privacy;training regularization;privacy use discriminator;approach regularizing deep;privacy extensive evaluation;optimal privacy;training regularization scheme;differential privacy;regularizing deep;privacy large scale;privacy train"}, "4383e714f4535777ffb7b4f618d4ccede4b08bd3": {"ta_keywords": "allophone model transcriptes;languages universal allophone;maps allophones phonemes;allophones phonemes;resource allophone reconstruction;allophone reconstruction allo;maps allophones;models speech transcription;allophones phonemes 14;allophone reconstruction;universal allophone model;new resource allophone;vera maps allophones;resource allophone;universal allophone;speech transcription;allophone model;speech transcription task;allophones;allophone;specific models speech;models speech;phonemes 14 languages;transcription task;transcription;model transcriptes;languages universal;transcriptes outperforms universal;universal model language;language specific models", "pdf_keywords": "phonetic mappings phonemic;allophone representation language;allophones phonemes;languages universal allophone;phonetic mappings;allophone speech recognition;map phoneme language;allophone representation map;allophone representation;based allophone representation;allophones phonemes 14;combining allophone representation;consists phonetic mappings;specific phoneme correspondences;phoneme correspondences;phonemic models language;phonemic broad phonetic;speech corpus allophonic;speech data phonemic;phonetic representations 14;allophone representation takes;phonemes languages;phonetic representations;broad phonetic representations;phoneme correspondences able;accuracy phonemes languages;phoneme language language;specific represented phoneme;speech recognition allovera;phonemic representations resource"}, "c549b3f2d262efc1f68dfdd842174634f37519ed": {"ta_keywords": "adaptation incorporating delay;adaptation framework speech;proposed incremental adaptation;incremental adaptation;incremental adaptation framework;novel incremental adaptation;conventional incremental adaptation;incorporating delay adaptive;incremental adaptation approach;adopts delay adaptive;models delay adaptive;incremental adaptation realizes;delay adaptive;behavior incremental adaptation;speech recognition adopts;framework speech recognition;adaptive behavior incremental;delay adaptive behavior;speech recognition;adaptation framework;proposed adaptation incorporating;recognition adopts delay;adaptation framework models;performance proposed adaptation;proposed adaptation;adaptation approach successively;novel incremental;adaptation realizes predictor;incremental;conventional incremental", "pdf_keywords": ""}, "40f4d7fe800810288a80f84cdb357a8f4c28e880": {"ta_keywords": "vision transformers empirically;vision transformers;vision transformer;transformer based vision;based vision transformers;based vision transformer;transformer architecture;transformers empirically spatial;transformer architecture propose;transformer architecture novel;vision transformer pit;spatial dimension conversion;beneficial transformer architecture;transformer based;transformer;pooling based vision;performance existing convolutional;cnns tasks;cnns tasks image;spatial dimension reduction;original transformer architecture;dimension conversion effectiveness;conversion effectiveness transformer;existing convolutional neural;transformers;beneficial transformer;networks cnns;networks cnns tasks;cnns;convolutional neural", "pdf_keywords": "convolutional architecture proposed;convolutional architecture;alternative convolutional architecture;vision transformer rnet;vision modeling transformer;new architecture convolutional;dimension conversion vision;architectures convolutional neural;based architectures convolutional;transformer rnet convolutional;architectures convolutional;architecture convolutional;architecture convolutional neural;networks called transformer;convolution strides transformer;vision transformer;images convolutional neural;based vision transformer;imagenet network convolutional;convolutional network;vision transformer vit;architecture computer vision;conversion vision transformer;convolutional network structure;network convolutional network;convolutional transformers;images convolutional;convolutional neural network;neural networks transformation;transformer based architectures"}, "8a73eed98873d91086201f41c6e1f613fcdefe18": {"ta_keywords": "speech forwarding asr;attention context synthesized;speech forwarding;self supervised training;novel self supervised;self supervised;self supervised model;synthesized speech forwarding;context synthesized speech;hyperparameter scales attention;supervised self supervised;supervised self;language model hyperparameter;synthesized speech;attention context;supervised training;scales attention context;supervised;language model;forwarding asr;performance gap supervised;gap supervised self;attention;scales attention;forwarding asr direction;supervised model;speech;gap supervised;advantages language model;context synthesized", "pdf_keywords": "speech data improves;unlabeled speech data;self supervised training;speech data;speech text model;supervised training;speech recognition;training improve robustness;automatic speech;approach unlabeled speech;regularization term training;speech recognition ssa;supervised training shown;unlabeled speech;automatic speech recognition;self supervised;training smoother way;outcome training smoother;training smoother;enhanced analytic speech;parameter training improve;supervised;learning algorithm learns;predict outcome training;end learning algorithm;algorithm learns;flexible end learning;predicting outcome training;speech text;machine learning msg"}, "42be8ed9973b3326a6e3d838c4742bc1d7704704": {"ta_keywords": "speech modification method;novel speech modification;speech modification;modify phonemic sounds;manipulating articulatory parameters;manipulating articulatory;input speech signal;estimated input speech;speech signal proposed;articulatory parameters estimated;sounds input speech;phonemic sounds input;input speech;statistical feature mapping;articulatory parameters;intuitively manipulating articulatory;speech signal;compensate unmodified articulatory;mapping technique mixture;possible modify phonemic;articulatory;feature mapping technique;modify phonemic;speech signal intuitively;articulatory movements;correlation articulatory parameters;dimensional correlation articulatory;articulatory movements considering;feature mapping;technique mixture model", "pdf_keywords": ""}, "7c0ada3511b05897fb4d75c5f657b5fbd953caa8": {"ta_keywords": "online voting influenced;effects biases voting;voting influenced social;influence position voting;voting content online;biases voting;biases voting squares;voting influenced;bias majority online;reputation bias;majority online voting;online voting;social influence;voting content;social influence position;underestimates reputation bias;impact social influence;reputation bias overestimates;position voting content;position bias majority;bias majority;content online platforms;voting squares;position voting;online platforms;effects biases;influence position;voting squares method;online platforms use;biases", "pdf_keywords": "biases online voting;voter biases online;quantify voter biases;quantify voter bias;voting quantify biases;voter bias content;biases online platforms;voter biases using;study voter biases;formulate voter bias;voter bias quantification;effects biasing votes;influence bias online;vote voter bias;bias online platforms;voter biases;study voter bias;voter bias;online voting quantify;votingvoter biases;biasing votes;estimation voter bias;voter bias fundamental;votingvoter biases important;voter bias inherent;biasing votes impact;affect votingvoter biases;bias voter bias;biases online behavior;social influence votes"}, "e79be3f9ce409f1a9b7084ef880298665e5212d0": {"ta_keywords": "token aware contrastive;learning video text;contrastive learning video;aware contrastive loss;text alignment multimodal;contrastive learning algorithm;contrastive learning;alignment multimodal representation;novel contrastive learning;improves contrastive learning;aware contrastive;multimodal representation learning;estimation multimodal fusion;loss estimation multimodal;alignment multimodal;multimodal fusion;video text alignment;aware cascade learning;cascade learning;contrastive loss;representation learning proposed;learning video;representation learning;multimodal representation;video text;multimodal fusion layers;token aware cascade;improves contrastive;contrastive loss computed;estimation multimodal", "pdf_keywords": "alignment contrastive learning;cascade contrastive learning;aware cascade contrastive;contrastive loss corpus;learning contrast single;cascade contrastive loss;contrastive learning pipelines;token aware contrastive;contrastive learning pipeline;aware contrastive loss;videotext alignment contrastive;effective contrastive learning;improves contrastive learning;current contrastive learning;contrastive learning combines;learning contrast;learning optimal contrastive;contrastive loss finetuning;learning video text;new contrastive learning;contrastive loss multi;contrastive learning;training leverages videotext;aware contrastive;cascade contrastive;effective contrastive loss;contrastive learning propose;conventional contrastive learning;leverages videotext alignment;contrastive learning achieved"}, "e40a5c25d39d0f9add6a26c82613cf29edbcccf5": {"ta_keywords": "potential erp using;erp using empirical;potential erp;related potential erp;erp using;event related potential;time integrated event;integrated event related;event related time;integrated event;identify user event;empirical validation empirical;user event related;empirical data time;empirical validation;validation empirical;event related;erp;data time integrated;using empirical validation;validation empirical data;user event;time integrated;event;empirical data;related time integrated;empirical;using empirical;data time;approach identify user", "pdf_keywords": ""}, "92bd9e8a83e82dbbcafd8cde4f5a42d7bb4a5859": {"ta_keywords": "transforming individuality using;statistical machine translation;method transforming individuality;express individuality different;transforming individuality;machine translation smt;individuality using statistical;express individuality;machine translation;individuality using;used express individuality;translation smt technique;individuality used express;individuality used;individuality different ways;individuality different;individuality;translation smt;concept individuality used;based concept individuality;concept individuality;transforming;using statistical machine;propose method transforming;statistical machine;translation;using statistical;paraphrasing;propose method paraphrasing;method paraphrasing", "pdf_keywords": ""}, "d1d23675d2e65cd734f2955c10ec1028b1139b5b": {"ta_keywords": "machine translation training;neural machine translation;translations evaluated neural;better translations evaluated;machine translation;translation training;results better translations;semantic similarity evaluate;machine translation systems;translations evaluated;translation training sme;semantic similarity human;metric optimizing neural;translation systems;semantic similarity;sme semantic similarity;translations;similarity human evaluation;better translations;translation systems based;evaluation metric optimizing;translation;new evaluation metric;work semantic similarity;evaluated neural machine;optimizing neural machine;trans lated english;training sme semantic;disparate languages trans;evaluate disparate languages", "pdf_keywords": "neural machine translation;machine translation optimal;machine translation model;sentence machine translation;machine translation nmt;optimize semantic similarity;translation machine translation;quality machine translation;new machine translation;machine translation;sentence similarity prediction;metric sentence similarity;empirical sentence similarity;machine translation machine;machine translation fundamental;machine translation proposed;better translation accuracy;task machine translation;optimal translation accuracy;sentence similarity model;translation models;statistical machine translation;semantic similarity evaluate;different translation models;results better translations;machine translation fairseq;language optimal translation;better translations evaluated;semantic similarity;semantic similarity human"}, "a538a05864a23e2f80f9b003d5ecbdfb8025b954": {"ta_keywords": "stance detection twitter;classify stance tweet;stance tweet target;stance tweet;target stance detection;task classify stance;stance detection;classify stance;twitter task classify;detection twitter task;classify tweet;way classify tweet;classify tweet way;detection twitter;twitter task 2016;tweet target;twitter task;contained tweet challenge;tweet challenge;tweet way target;target stance;task target stance;way target tweet;target tweet;stance;target contained tweet;tweet target build;submission twitter task;tweet challenge way;twitter", "pdf_keywords": ""}, "3376118362db3751cfbd88acd0c090b8a3897733": {"ta_keywords": "generalization new words;bert best generalization;encoding subwords derivational;subwords derivational model;semantic representations derivationally;pretrained language models;subwords derivational;derivationally complex words;morphologically informed vocabulary;generalization capabilities pretrained;best generalization new;derivational model generalization;generalization new;language models;vocabulary input tokens;semantic representations;model generalization capabilities;language models ppm;model generalization;best generalization;pretrained language;words case bert;improved morphologically informed;generalization capabilities;generalization;new words achieved;words achieved;capabilities pretrained language;improved morphologically;words achieved proper", "pdf_keywords": "models complex words;derivational segmentation words;words derivational segmentation;complex words model;models morphology words;pretrained language models;words model;segmentation words;complex words derivational;derivationally complex words;linguistics complex words;language models;language models increasingly;word segmentation predicting;morphologically informed vocabulary;standard word segmentation;words representational;word segmentation;bert morphologically;meaning complex words;words derivational;predicting distributional semantics;words representational quality;distributional semantics words;words model weights;computation complex words;language neural;programming models bert;complex words;section language models"}, "1392df13a80a962057e979a294a850a50b7deb7e": {"ta_keywords": "correcting automatic annotations;automatic annotations natural;automatic annotations;annotations natural language;automatically segmenting corpus;natural language efficient;annotations natural;segmentation optimizes supervision;segmenting corpus;segmenting corpus uncertain;annotations;corpus uncertain labels;pre context annotated;automatically segmenting;context annotated;annotated;context annotated baselines;annotated baselines;method automatically segmenting;automatically correcting;automatically correcting automatic;natural language;correcting automatic;language efficient;chunk human supervision;corpus;altogether segments segmentation;optimizes supervision;segmenting;corpus uncertain", "pdf_keywords": ""}, "8df3f3f72eb239eb212bd3fc929bd754ce2e03d6": {"ta_keywords": "protein interaction networks;topic coherence yeast;proteins network;networks topic coherence;proteins network method;important proteins network;modeling topic coherence;modeling relationship protein;proteins network evaluate;coherence yeast proteins;protein protein interaction;relationship protein protein;predict relationship protein;protein interaction;relationship protein;yeast proteins method;yeast proteins;topic coherence ability;modeling topic;identify important proteins;coherence yeast;interaction networks topic;topic coherence;proteins method based;proteins;proteins method;protein protein;joint modeling topic;important proteins;protein", "pdf_keywords": ""}, "aae3d5e24d02ae538030ef3995a86118c5323ae1": {"ta_keywords": "storage medium ions;store ions high;medium ions stored;store ions;store ions low;probe ions stored;used store ions;ions stored;ions stored high;molecule ion probe;high purity ions;ions high density;ions low density;ion probe;medium ions;ion probe ions;purity ions based;probe ions;ions based;single molecule ion;molecule ion;ions high;low density storage;high density storage;ions based use;purity ions;density storage medium;ions low;ions;stored high density", "pdf_keywords": ""}, "148bca569a0d2833f05df1297788f64bc6686fa8": {"ta_keywords": "nlp temporal misalignment;nlp temporal;processing nlp temporal;model natural language;processing nlp;temporal adaptation continued;temporal misalignment performance;temporal adaptation;language processing nlp;natural language processing;domains temporal adaptation;nlp;effects temporal misalignment;language processing;temporal misalignment suite;gains temporal adaptation;temporal misalignment;temporal misalignment degrade;adaptation continued pretraining;adaptation continued;natural language;trained data time;temporal;adaptation;different domains temporal;performance model trained;effects temporal;help gains temporal;tasks different;performance multi faceted", "pdf_keywords": "pretrained nonlinear programming;models nlps temporal;programming models nlps;adapting nonlinear programming;downstream tasks temporal;predicting future tasks;predict future tasks;nlps temporal;performance pretrained nonlinear;nonlinear programming models;task temporal;temporal misalignment tasks;tasks trained text;pretrained nonlinear;data adapting temporal;tasks temporal;misalignment tasks temporal;nlps temporal misalignment;mitigate temporal;temporally distant training;programming model task;mitigate temporal misalignment;tasks used predict;future tasks based;novel model temporal;nonlinear programming;adapting temporal;tasks degrade performance;models nlps;summarization task temporal"}, "bb80f7d2269308c3e91da8c47b290645e9d3d7d5": {"ta_keywords": "human poses data;poses data;dynamic human poses;dynamic poses;movements human poses;human poses method;poses data static;poses method based;poses method;static dynamic poses;human poses;reconstructing movements human;poses;reconstructing movements;represented rotation invariant;problem reconstructing movements;dimensional learned bases;represented rotation;static dynamic human;discovery set projections;learned bases represented;bases represented rotation;projections original feature;movements human;dynamic human;discovery bases data;reconstructing;dimensional learned;rotation invariant;learned bases", "pdf_keywords": "sparse rotationinvariant poses;motion learns pose;learns pose;learns pose specific;rotationinvariant poses used;poses rotation;representation human poses;rotationinvariant poses;poses sports datasets;poses rotation invariant;action pose;dimensional bases poses;representation pose based;sparse representation pose;representation pose;learned bases pose;human motion learns;representation human motion;pose bases;pose bases intrinsically;poses movements;rotation invariant learned;pose data ability;motion learns;action estimate pose;reconstructing human motion;datasets reconstruct pose;pose theoretic;pose theoretic data;pose dimensions reconstruction"}, "ed1da1abf0f50ca758e422fbd945f891b6cda690": {"ta_keywords": "paraphrase based dialog;dialog retrieval model;dialog retrieval;based dialog retrieval;retrieval model dialog;distributed word representations;example paraphrase based;paraphrase based;utilize recursive neural;word representations;dialog pair database;pooling determine sentences;dialog;recursive neural network;word representations employ;based dialog;sentences arbitrary length;query distributed word;recursive neural;employ recursive autoencoders;example paraphrase;recursive autoencoders;dialog pair;determine sentences arbitrary;recursive autoencoders dynamic;determine sentences;distributed word;model dialog pair;robustness example paraphrase;retrieval model", "pdf_keywords": ""}, "7596030e0ce7a8df2f43e6ba4e9de5fa4a19240f": {"ta_keywords": "stochastic gradient descent;scale stochastic gradients;stochastic gradient;stochastic gradients simple;stochastic gradients;stepsize extragradient algorithm;extragradient algorithm;converge stochastic gradients;gradient descent step;extragradient algorithm able;based stochastic gradient;gradient descent;stepsize extragradient;stochastic gradients derive;double stepsize extragradient;gradients simple bilinear;descent step exploration;method converge stochastic;extragradient;scale stochastic;able scale stochastic;gradients simple;gradient;bilinear models algorithm;gradients;converge stochastic;gradients derive;stochastic;algorithm based stochastic;gradients derive sharp", "pdf_keywords": "stochastic extragradient method;extragradient stochastic gradients;stepsize extragradient stochastic;stochastic extragradient generalization;novel stochastic extragradient;extragradient stochastic;stochastic extragradient;propose stochastic extragradient;extragradient method optimality;vanilla extragradient stochastic;extragradient method converges;extragradient stochastic non;extragradient algorithm variant;present stochastic extragradient;extragradient algorithm;cases stochastic extragradient;extragradient methods;step extragradient methods;extragradient methods overcome;stochastic optimistic gradient;gradient descent ascent;based extragradient algorithm;stochastic gradients jeopardize;extragradient method solving;training stochastic maxwell;stochastic gradients;strategy extragradient method;extragradient method;extragradient methods staple;method based extragradient"}, "0d1cd3baad7d0734c9bbb008a33e2d10846968cd": {"ta_keywords": "supervised learning process;supervised learning;supervised learning powerful;supervised;evaluated supervised learning;learning process;learning powerful technique;evaluated supervised;learning process method;problems physics based;learning;weights method;physics based;learning powerful;weights method applied;solve problems physics;physics;physics based use;random variables weights;technique solve problems;sizes evaluated supervised;applied problems physics;sum weights method;weights variables;variables weights;powerful technique solve;method based;weights variables evaluation;weights;method applied", "pdf_keywords": ""}, "739c2181f7894050a06b53b41ac5debe8ffc4829": {"ta_keywords": "stochastic gradient descent;gradient descent sde;capacity metric stochastic;stochastic gradient;metric stochastic gradient;gradient descent;sde quantifies generalization;generalization error trajectories;descent sde quantifies;gradient noise proposed;capacity metric accurately;tailed gradient noise;gradient noise;deep neural networks;deep neural;descent sde;propose capacity metric;capacity metric;proposed capacity metric;trajectories sde heavy;accurately estimates generalization;metric stochastic;error trajectories sde;heavy tailed gradient;stochastic;estimates generalization;estimates generalization error;trajectories sde;generalization error support;sde quantifies", "pdf_keywords": "stochastic gradient descent;stochastic generalization;stochastic gradient;novel stochastic learning;stochastic learning;principle stochastic gradient;theory stochastic gradient;derive stochastic generalization;gradient descent sde;stochastic optimization;processes stochastic optimization;class stochastic optimization;generalization theory stochastic;paths stochastic learning;develop stochastic differential;stochastic process training;stochastic optimization powerful;stochastic differential;stochastic gradient monte;solutions stochastic optimization;stochastic learning algorithms;generalize class stochastic;stochastic generalization uncertainty;stochastic learning framework;approximated simple stochastic;stochastic optimization problems;convex deep learning;stochastic optimization algorithms;equations stochastic optimization;underlying stochastic"}, "94f8ef5944ecbdd0350d406bf3a16a7f2dff7349": {"ta_keywords": "speech separation recognition;speech separation;speaker speech separation;neural sequence sequence;2mix corpus model;sequence sequence architecture;wsj1 2mix corpus;multi speaker speech;neural sequence;2mix corpus;sequence architecture;channel multi speaker;novel neural sequence;curriculum learning;novel curriculum learning;multi speaker;input multi channel;sequence architecture extends;multi channel input;speaker speech;channel input multi;curriculum learning strategy;seq2seq;separation recognition;corpus model;speaker;model multi channel;curriculum;separation recognition proposed;channel input", "pdf_keywords": "separated speech recognition;speech separation recognition;speech separation deep;multichannel speech recognition;sequence speech recognition;separation predicting speech;channel speech separation;speech separation signals;speech recognition trained;separation input speech;separated speech model;end speech recognition;end multichannel speech;multichannel speech;multi channel speech;separation signals training;channel speech input;neural beamformer sequence;sequence sequence speech;independent speech separation;perform speech separation;end speech separation;architecture neural beamformer;sequence speech;speech recognition;noise neural beamformer;speech recognition module;based neural beamformer;spatially separated speech;speech separation"}, "ab193c05bc447f368565c1ff37064b1c517a750f": {"ta_keywords": "learning agents dialogue;agents dialogue systems;efficiently exploration deep;exploration deep learning;dialogue systems;dialogue systems algorithm;agents dialogue;learning agents;deep learning agents;exploration deep;dialogue;algorithm learns;make exploration deep;algorithm learns faster;greedy bootstrapping replay;learns faster;efficiently exploration;algorithm efficiently exploration;network algorithm learns;backprop neural;bayes backprop neural;exploration strategies greedy;learns;learning feasible;deep learning feasible;deep learning;backprop neural network;improves efficiency exploration;bootstrapping replay;bootstrapping replay buffer", "pdf_keywords": "learning dialogue policies;learning dialogue;learn dialogue policies;dialogue policies based;exploration dialogue agents;deep reinforcement learning;explore deep reinforcement;task oriented dialogue;learn dialogue;dialogue policies combines;method learning dialogue;dialogue systems;dialogue policies;dialogue agents framework;dialogue policies effective;dialogue interaction;deep reinforcement;oriented dialogue systems;dialogue agents;oriented dialogue;dialogue;reinforcement learning task;exploration dialogue;dialogue systems approach;reinforcement learning;efficiently explore deep;novel reinforcement learning;vime learn dialogue;dialogue interaction allows;informed reinforcement learning"}, "2b5d553cb2f298f36aff1a1519f7f2f6be4db5da": {"ta_keywords": "supervised taxonomy expansion;self supervised taxonomy;self supervised;propose self supervised;existing taxonomy expansion;taxonomy expansion model;node attachment prediction;natural self supervision;taxonomy expansion generate;taxonomy expansion;attachment prediction task;natural supervision existing;supervision existing taxonomy;supervised taxonomy;attachment prediction;feature representations query;natural supervision;task learns feature;learns feature;supervised;attachment task learns;self supervision signals;supervision signals steam;leverages natural supervision;learns feature representations;self supervision;prediction task anchor;paths query terms;query terms;expansion generate natural", "pdf_keywords": "supervised taxonomy expansion;taxonomy self supervised;self supervised taxonomy;taxonomy supervised;expansion task supervised;labeled taxonomy expansion;taxonomy supervised learning;self labeled taxonomy;taxonomy expansion task;existing taxonomy expansion;taxonomy creating learning;novel taxonomy expansion;taxonomy expansion generate;taxonomy expansion model;taxonomy expansion;supervised taxonomy;steam2 taxonomy expansion;specified taxonomy supervised;supervision existing taxonomy;knowledge extraction supervised;taxonomy expansion leverages;perform taxonomy expansion;taxonomy expansion method;terms ing taxonomies;taxonomy taxonomy expansion;viewswe propose supervised;supervised unstructured taxonomy;taxonomies new concept;featureswe propose supervised;parent taxonomy supervised"}, "433b24d63146605d25c0c271062e129608462f03": {"ta_keywords": "reducing dimensionality feature;features log linear;dimensionality feature spaces;nonlinear features log;reducing dimensionality;feature spaces method;feature space using;feature space;method reducing dimensionality;feature spaces;dimensionality feature;high dimensionality feature;approximation feature space;feature spaces experimental;uses nonlinear features;nonlinear features;features log;achieve high dimensionality;log linear models;efficient approximation feature;high dimensionality;dimensionality;features;log linear;feature;approximation feature;linear models achieve;linear models;spaces method;spaces method uses", "pdf_keywords": ""}, "a0e0ce316ce0fdca2db61a52fdc7100e24906075": {"ta_keywords": "xstream measures outlierness;featureevolving data streams;ensemble outlier detector;based ensemble outlier;ensemble outlier;feature streaming evolving;evolving data streams;streams important feature;data streams;important feature streaming;outlier detection;data streams propose;feature streaming;outlier detector;data streams important;consider outlier detection;streaming evolving data;streaming evolving;outlierness;consider outlier;outlier detector called;outlier;measures outlierness;streams propose density;outlierness multiple;updates stream;outlier detection problem;streams;streams important;updates stream progresses", "pdf_keywords": ""}, "1a0d8dbd0252193abe9d64f72fc56cc1f05ed3eb": {"ta_keywords": "structured situational graph;improve situational reasoning;reasoning improve situational;situational graphs;generated situational graphs;situational graph;explicitly structured situational;situational graph st;structured situational;situational reasoning;situational reasoning end;natural language queries;generated situational;input generated situational;improve situational;reasoning improve;reasoning end task;natural language;initio reasoning improve;situational;build graph relevant;using natural language;language queries;language model st;language model;reasoning end;iteratively build graph;consequences explicitly structured;language queries finetuned;graph relevant consequences", "pdf_keywords": "situational reasoning datasets;improve situational reasoning;situational reasoning framework;reasoning graphs effective;reasoning graphs;reasoning graph framework;generates reasoning graph;effective situational reasoning;st reasoning graphs;reasoning graph;reasoning datasets;reasoning framework;language models reasoning;situational reasoning;situational reasoning end;generates reasoning;iteratively generates reasoning;reasoning datasets validated;reasoning framework takes;reasoning end task;seq language models;graph generation tasks;graphs using language;improve st reasoning;improve situational;seq approach reasoning;framework improve situational;st reasoning task;approaches st reasoning;models reasoning st"}, "8529af634b443427d87d62d64467d2f1adfc230f": {"ta_keywords": "clustering malware samples;method clustering malware;clustering malware;heterogeneous malware samples;characterizing malware samples;malware samples;heterogeneous malware;techniques heterogeneous malware;malware samples threat;malware samples possible;malware samples different;multi modal clustering;characterizing malware;results characterizing malware;modal clustering techniques;method characterizing malware;modal clustering;malware;clustering techniques heterogeneous;multi modal data;method clustering;samples threat actors;threat actors unsupervised;clustering;modal data perspectives;threat actors results;best method clustering;samples different threat;samples possible threat;clustering techniques", "pdf_keywords": ""}, "8424dd233577e3bd3fbd7ecdfd8b4d442531a20e": {"ta_keywords": "hidden model parameters;phylogenetic tree;regression hidden model;phylogenetic tree structure;parameters considering phylogenetic;considering phylogenetic tree;learning discrete models;hidden model;tree structure model;regression hidden;phylogenetic;considering phylogenetic;linear regression hidden;parameters model;discrete models;model parameters;models;discrete models proposed;regression parameters;unsupervised learning discrete;optimize parameters model;optimize regression parameters;regression parameters hyper;learning discrete;models proposed;model parameters considering;tree structure;able optimize regression;unsupervised learning;model able optimize", "pdf_keywords": ""}, "7b72f79015a0a5e06cc019bae78f268b16a8e659": {"ta_keywords": "rank approximation deep;neural network dnl;sequence discriminative training;approximation deep neural;discriminative training;speech recognition;low rank approximation;deep neural network;approximation sequence discriminative;automatic speech recognition;approximation deep;deep neural;singular value decomposition;discriminative training advantage;dnl model method;automatic speech;sequence discriminative;value decomposition svd;dnl model;network dnl model;problem automatic speech;rank approximation sequence;rank approximation;discriminative;neural network;method low rank;decomposition svd;dnl;network dnl;low rank", "pdf_keywords": ""}, "0ea90d783a76b7c119fb5471fc71b6bc2defa06d": {"ta_keywords": "recovery distributed storage;distributed storage consider;distributed storage;node recovery distributed;storage node recovery;distributed storage derive;storage node;model distributed storage;problem storage node;storage consider minimum;recovery distributed;codes model distributed;node recovery;storage consider;storage;considers problem storage;storage derive lower;stored individual node;storage derive;problem storage;regenerating codes model;regenerating codes;distributed;consider minimum bandwidth;minimum bandwidth;recover data stored;setting regenerating codes;bandwidth setting regenerating;recover data;required recover data", "pdf_keywords": "codes distributed storage;regenerating codes distributed;storage code optimal;optimal storage code;distributed storage nodes;distributed storage practicalthe;minimizing download storage;distributed storage;node distributed storage;distributed storage systems;stored distributed storage;storage nodes;optimal storage data;stored distributed;minimum storage regeneration;nodes minimum download;storage nodes fundamental;efficient recovery data;recovering data node;minimum download storage;storage systems solution;data stored distributed;storage practicalthe problem;storage decoding data;download storage decoding;optimal storage decoding;storage code;download storage considered;optimal storage decodingwe;storage nodes present"}, "2030551b2590fa70eb5132131e6627c93128f0a1": {"ta_keywords": "parametric utility learning;utility learning non;utility learning;estimating utility functions;utility functions players;multiple utility functions;cooperative continuous games;utility functions;estimating utility;learning non cooperative;method estimating utility;utility functions present;games based probabilistic;combining multiple utility;parametric utility;continuous games based;mixture regression models;continuous games;multiple utility;mixture regression;energy efficient behavior;data social game;adaptation mixture regression;method parametric utility;utility;social game experiment;based probabilistic;cooperative continuous;encourage energy efficient;non cooperative continuous", "pdf_keywords": ""}, "b34f254083b012bafd9b5ebd6c27450b4213c984": {"ta_keywords": "classifying captions biomedical;captions biomedical publications;extracting classifying captions;captions biomedical;classifying captions;misunderstanding content biomedical;misunderstanding biomedical publications;captions;content biomedical publications;content biomedical;biomedical publications;extracting classifying;misunderstanding biomedical;tool misunderstanding biomedical;task extracting classifying;biomedical publications evaluate;automated methods;biomedical;learning techniques methods;classifying;existing learning techniques;misunderstanding content;learning techniques;methods task extracting;novel learning techniques;automated methods task;learning techniques best;extracting;learning;automated", "pdf_keywords": ""}, "4a19211e077bc4ada685854245ba9fab381cbb06": {"ta_keywords": "propose novel framework;novel framework named;novel framework;framework;framework named;propose novel;novel;propose;named", "pdf_keywords": "relation extraction based;performs relation extraction;relation extraction;relation extraction extracted;extraction entities relations;extraction extracted relation;information extraction;extraction structured information;topic relation extraction;extracted relation tuples;information extraction extracts;joint extraction entities;relation tuples unstructured;semantic information extraction;information extraction based;knowledge base relation;extraction entities;entity extracted relation;based information extraction;extracted relation compared;relation triples unstructured;method relation extraction;entities relations based;structured information relation;extracted relation;triples unstructured text;existing knowledge base;problem information extraction;tuples unstructured text;results knowledge base"}, "409280796e924bfe71421fe5bf4986bd3591ea72": {"ta_keywords": "databases using similarity;similarity joins word;data heterogeneous databases;similarity joins;joins word based;using similarity joins;heterogeneous databases using;heterogeneous databases;data heterogeneous;joins word;integrates data heterogeneous;data integration;word based information;data mining available;databases;data integration context;used data integration;databases using;data mining;joins;information representation language;integrates data;based information representation;using similarity;similarity;information representation;word based;integration context data;context data mining;designed used data", "pdf_keywords": ""}, "4e3016617e5e254bafebcbd7e96c509f670bdd37": {"ta_keywords": "audio performance synthesis;sound synthesis;sound generative models;sound generative;sound sound generative;synthesize music;sound synthesis power;novel score audio;test synthesize music;power sound synthesis;synthesis power sound;score audio;synthesize music clear;score audio performance;listening test synthesize;audio;performance synthesis;synthesis model;performance synthesis model;synthesis model combines;synthesize;synthesis;music clear polyphony;audio performance;sound;sound sound;conditioning transformer encoder;transformer encoder;encoder;combines power sound", "pdf_keywords": "audio music synthesis;musical score synthesis;music synthesis;music synthesis proposed;speech synthesis deep;music synthesis propose;based music synthesis;speech synthesis;text speech synthesis;music convolutional neural;generation music convolutional;decoder polyphonic inputs;music convolutional;encoder decoder polyphonic;decoder polyphonic;generation music data;synthesis deep neural;alignment scoreto audio;model piano dataset;new violin dataset;scoreto audio recordings;score audio music;piano dataset achieves;score audio;violin dataset;piano dataset;score synthesis;unaligned musical scores;synthesis deep;music data based"}, "2a39a4f2d18e376ef8a6e2f45416e7b87957481e": {"ta_keywords": "multi task learning;task learning;task learning historical;task data sparse;text normalization;text normalization method;historical text normalization;normalization;normalization method;normalization used;non linear dynamics;normalization used tackle;normalization method used;non linear;method normalization;learning historical text;task data;traditional method normalization;multi task;method normalization used;data sparse;target task data;linear dynamics;combines multi task;linear dynamics combines;learning;learning historical;data sparse demonstrate;field non linear;robust models", "pdf_keywords": ""}, "bd17620c6cb5ca97ef773499223d1509d123745f": {"ta_keywords": "called deep learning;learning called deep;games solved parallel;deep learning;self contained interactive;xcite games solved;interactive runnable code;xcite games;solved parallel game;interactive examples;learning called;parallel game;contained interactive examples;interactive runnable;problem learning called;deep learning based;learning based integration;contained interactive runnable;interactive;learning based;source xcite games;interactive examples approach;problem learning;learning;runnable code;approach problem learning;contained interactive;runnable code approach;games;called deep", "pdf_keywords": "isthe connection data;data processing connection;data processing process;data corresponding;data processing;process real data;machine learning related;data computation;data acquisition analysis;training data function;data represented;processing data;data connection used;data used learn;data mathematical;think data corresponding;learning process used;data data;training data;data information;data step isthe;data datathe;data generated data;data usedthe connection;data mining related;data processing achieved;data function training;data discuss connections;analysis artificial intelligence;learning data begin"}, "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896": {"ta_keywords": "code tokens identifiers;aware encoder decoder;aware encoder;encoder aware;novel identifier aware;encoder aware encoder;code tokens;novel encoder aware;distinguish code tokens;identifier aware;code understanding generation;identifier aware pre;code semantics conveyed;encoder decoder;code understanding;encoder;developer assigned identifiers;distinguish code;code semantics;tokens identifiers;encoder decoder model;novel encoder;identifiers recover masked;leverages code semantics;semantics conveyed developer;novel identifier;identifiers recover;code;propose novel identifier;present novel encoder", "pdf_keywords": "code generation tasks;encoded code predicting;code predicting encoded;code tokens identifiers;code predicting;code learn;code embedding task;code learn useful;code understanding generation;pre trained encoder;distinguish code tokens;trained encoder decoder;identifiers decoder propose;code tokens;identifiers decoder;code generation;code embedding;code search network;enables code learn;code search;code generation prone;codet5 code search;rich code semantics;automatically generating code;code syntactic information;capture code syntactic;code semantics conveyed;trained encoder;code semantics pre;code understanding model"}, "70bc4dc0bc72816773006c71b56fa5885c729caa": {"ta_keywords": "generative music;music based generative;generative music streams;form generative music;generate music;generative adversarial networks;networks generate music;generative adversarial;based generative adversarial;gans use generative;networks gans;generative elliptic networks;generation music based;adversarial networks gans;generation music;used generate music;networks gans use;generative;gans;approach generation music;generate music form;data form generative;based generative;use generative;gans use;adversarial networks;use generative elliptic;generative elliptic;form generative;adversarial", "pdf_keywords": ""}, "78c9181abe18575925fbbb6e6d8c72d7bf90d06d": {"ta_keywords": "adaptive hybrid delayed;delayed hybrid adaptive;distributed hybrid adaptive;delay hybrid adaptive;delay hybrid;hybrid delayed hybrid;mean delay hybrid;hybrid adaptive hybrid;distributed hybrid;adaptive hybrid hybrid;hybrid hybrid adaptive;adaptive hybrid;delayed hybrid;hybrid delayed;hybrid adaptive;adaptive hybrid adaptive;problem distributed hybrid;hybrid adaptive low;hybrid hybrid;hybrid;mean delay;low mean delay;adaptive low;delay;adaptive;distributed;adaptive low mean;problem distributed;consider problem distributed;delayed", "pdf_keywords": "delay based distributed;distributed control qmac;distributed control;channel distributed;delay optimality;delay theorem distributed;delay ii distributed;delay optimality exhaustive;wireless channel distributed;packet distributed;stochastically distributed;distributed optimal control;channel distributed scheduler;packet distributed fashion;delay optimal stable;distributed scheduler able;delay optimal;distributed control central;ii distributed control;protocol based random;distributed random access;queueing network distributed;consider delay optimality;channel seek distributed;stochastically distributed optimal;distributed multi hop;propose stochastically distributed;distributed optimal;delay queueing network;delay optimality greedy"}, "9bbdcc03d872987eef9165f4a63c3878a5b05189": {"ta_keywords": "dense encoder architecture;dense encoder;novel dense encoder;dense language model;decoupled dense language;encoder architecture;encoder;encoder architecture based;dense language;massive decoupled dense;language model m_;training architecture massive;language model;decoupled dense;propose novel dense;architecture massive decoupled;pre training architecture;novel dense;architecture massive;dense;training architecture;language;massive decoupled;novel pre training;architecture based novel;model m_ m_;pre training;model m_;massive;architecture", "pdf_keywords": "contrast bi encoders;learned contrastive model;learned contrastive;using learned contrastive;attention structures condensate;architecture deep denoise;dense retrieval;dense retrieval using;encoder trained;text representations trained;proposed encoder trained;pre train encoder;deep denoise forward;performance dense retrieval;encoders using learned;attention structures;training architecture deep;deep denoise;train bi encoders;attention;deep bidirectional;bi encoders;different attention structures;condensate denoise trained;architecture deep;low memory representations;sentences proposed encoder;attention structures paper;encoder trained introduce;memory representations trained"}, "2dd81061e0b11c828446f6a1843741ae51facbd2": {"ta_keywords": "learning cortical microcircuitry;computation networks slow;learning cortical;fast computation networks;neuronal dynamics;models cortical micro;neurons phase advance;biological neurons phase;model neuronal dynamics;neurons phase;model neuronal;neurons;model learning cortical;neuronal dynamics exhibits;realistic models cortical;biological neurons;neuronal;cortical micro;fast computation;models cortical;tractable model neuronal;computation networks;ability biological neurons;cortical microcircuitry;cortical microcircuitry applied;convolutional architectures demonstrates;networks slow components;convolutional architectures;cortical;fully convolutional architectures", "pdf_keywords": "network neurons;networks neurons;dynamics network neurons;neuron learning predict;state neuron;dynamics neurons;synthetic neural;neuron dynamics;neuron state;state neuronal network;model neurons;learning networks slow;neural dynamics;neuron modeled;neuron dynamics time;learning networks neurons;coding neuronal;dynamics neuronal networks;neurons driven;neurons able perform;random network neurons;neuron;model neurons neurons;neuronal network;neural network slow;robust neuronal;neuron learning;biological neurons phase;neurons neurons driven;neural"}, "87f42406de084e60d2365adac8a159ed3e455856": {"ta_keywords": "anomalies streaming graphs;streaming graph detection;streaming graphs fast;streaming graphs;streaming graph;persistent anomalies streaming;introduce streaming graph;graphs fast scalable;anomalies streaming;graph detection;graph detection approach;detect persistent anomalies;graphs fast;compares graphs;compares graphs based;graphs compares graphs;similarity function graphs;graphs compares;implementation introduce streaming;graphs based relative;detection approach scalable;persistent anomalies;graphs based;graphs;introduce streaming;graph;function graphs compares;fast scalable;detect persistent;scalable memory efficient", "pdf_keywords": "streaming based anomaly;graph anomaly detection;scalable graph streaming;graph streams scalable;threats streaming graphs;detect graph anomaly;graph streams;novel graph streaming;graph streams stream;streaming graphs efficiently;patterns graph streams;graph streams including;anomalies data streams;graph streaming;graph anomaly based;graph streams algorithm;streaming graphs;streaming graphs approach;represent streaming graphs;graph streaming framework;streams scalable clustering;streaming graphs called;asynchronous streaming graphs;streaming graphs able;stream graphs;stream graphs containing;graph streams ablewe;clusters stream nodes;anomaly detection scalable;class graph streams"}, "4cc70dd760c2c8cfc0107921bade45fb5efe860e": {"ta_keywords": "latent factorization graphs;graph recommendation method;recommendations using knowledge;factorization graphs;graph recommendation;factorization graphs compare;making recommendations using;making recommendations;graph based latent;recommendation method large;recommendation method;knowledge graphs;latent factorization;methods making recommendations;recommendations using;using knowledge graphs;latent factor model;knowledge graphs simplest;art graph recommendation;factorization;factor model;strengths latent factorization;based latent factor;latent factor;factor model combines;state art graph;graph extend models;links graph;uses links graph;recommendations", "pdf_keywords": ""}, "fa2125641578934de12d7f792b094ffcfdf82ee2": {"ta_keywords": "text topicture synthesis;topicture synthesis based;topicture synthesis;design text topicture;text topicture;topicture;design text;build linguistic representation;representation text;representation text form;linguistic representation text;text;build linguistic;linguistic representation;synthesis based;synthesis based kuramoto;text group;original text group;text form;synthesis;create group pictures;text group symbols;symbols represented;copies original text;based kuramoto;kuramoto technology;kuramoto;based kuramoto technology;present design text;original text", "pdf_keywords": ""}, "c818f9be503a1ed94f991a2949c29e3ee477e8b8": {"ta_keywords": "uncertainty input features;input features uncertainty;features uncertainty input;features uncertainty;uncertainty reduction;uncertainty reduction methods;based uncertainty reduction;dispersion based uncertainty;vectors random interpolation;based expectation training;uncertainty input;input features modeled;methods deep neural;deep neural networks;enhanced feature vectors;representation input features;probabilistic representation input;reduction methods deep;feature vectors random;input features;random interpolation;networks based expectation;integrates multiple recognition;recognition;expectation training;multiple recognition;feature vectors;deep neural;probabilistic representation;efficient dispersion based", "pdf_keywords": ""}, "da1d5dc331c2839dfc3e6a79ee17f3bdf2231a8b": {"ta_keywords": "database complete method;database complete;partial list completion;list completion;objects database complete;list objects complete;list completion task;completion task database;objects complete set;perform partial list;complete set method;partial list;complete set;completion task;objects complete;completion;expansion database;subset objects database;complete method uses;set expansion database;expansion database technique;complete method;database technique expand;database selecting;recall database generate;recall database;database selecting best;objects database;database;technique expand list", "pdf_keywords": ""}, "eb28e82ca0bbc5d83e1cc07807da16874105d2fa": {"ta_keywords": "unsupervised semantic frame;semantic frame induction;unsupervised semantic;perform unsupervised semantic;web scale corpus;approach unsupervised semantic;semantic frame;graph clustering;graph concept crumpled;scale corpus;frame induction;scale corpus approach;concept crumpled graph;structure cluster objects;frame induction using;graph clustering uses;algorithm graph clustering;crumpled graph concept;structure cluster;corpus;cluster objects;clustering uses dependency;frame induction demonstrate;structure structure cluster;represented crumpled graph;corpus approach;crumpled graph represented;graph represented crumpled;corpus approach based;induction using triframes", "pdf_keywords": ""}, "572535aff31c400578fdd75313c896c0650b2d4c": {"ta_keywords": "dereverberation beamforming neural;beamforming neural;beamforming neural network;dereverberation beamforming;integration dereverberation beamforming;architecture denoise resonance;architecture denoise;beamforming;application architecture denoise;simultaneous integration dereverberation;denoise resonance;neural network proposed;neural network;integration dereverberation;denoise resonance np;denoise;speech challenge achieve;novel architecture simultaneous;dereverberation;speech challenge;dr speech challenge;neural;architecture simultaneous integration;neural network demonstrate;architecture simultaneous;speech;dr speech;np dr speech;simultaneous integration;network proposed novel", "pdf_keywords": ""}, "8495c1722e1f5107733c842839c2d298b9116921": {"ta_keywords": "conversational search;conversation history conversational;problem conversational search;conversational question answering;conversation history;conversational search presence;history conversational;question answering;integration conversation history;question answering convqa;solving problem conversational;search presence history;conversation;history conversational question;conversational;conversational question;bert mode;history turns approach;built bert mode;problem conversational;history turns;answering convqa model;built bert;bert;model built bert;bert mode demonstrate;answering convqa;search presence;history turns different;answering", "pdf_keywords": "conversational question answering;conversation history modeling;conversation history selection;conversational search;conversational search bert;context conversational search;model conversation history;conversational search propose;conversation history conversational;integrate conversation history;seeking conversational search;modeling context conversational;conversation history;question answering;conversation history context;conversation history covered;evaluation conversation history;conversation history answer;generator conversation history;portion conversation history;information seeking conversational;module conversation history;modeling module conversation;insights conversation history;question answering convqa;history conversational;amounts conversation history;conversation history general;conversation history different;seeking conversational"}, "14f925098d57b0fa491a100fa73e52dbc764efa6": {"ta_keywords": "neuroscience structure brain;neuroscience structure;structure brain;structure brain determined;study structure brain;brain brain biological;brain biological;structure brain central;topic neuroscience structure;dynamics brain brain;brain determined dynamics;dynamics brain determined;determined dynamics brain;dynamics brain;brain brain;neuroscience;central topic neuroscience;biological able brain;brain determined;brain biological able;brain central topic;topic neuroscience;study structure;behavior brain brain;structure;brain;able brain brain;behavior brain;able behavior brain;brain central", "pdf_keywords": ""}, "32a2a8baf217d29f628d22d793cace95634f51d5": {"ta_keywords": "reducing content emails;content emails features;emails features based;emails features;content emails;content emails proposed;emails proposed features;filtered content discovery;client thunderbird;email client thunderbird;thunderbird;email client;emails;open source email;approach reducing content;client thunderbird present;content discovery technique;emails proposed;reducing content;thunderbird present;proposed filtered content;email user study;content discovery;thunderbird present preliminary;source email client;filtered content;purpose reducing content;features purpose reducing;source email;real email", "pdf_keywords": ""}, "1092da11519fe8427c8113b16a012f34f4a3fb6b": {"ta_keywords": "proposed federated learning;federated learning;federated learning accurate;federated learning model;approach federated learning;privacy federated learning;efficient proposed federated;fairness privacy federated;novel approach federated;privacy federated;approach federated;federated;proposed federated;accuracy fairness privacy;learning model formulated;learning model based;learning model;fairness privacy;learning accurate efficient;measures accuracy fairness;accuracy fairness;learning accurate;learning;privacy;nodes;formulated dimensional graph;model based;graph finite size;graph generated finite;dimensional graph finite", "pdf_keywords": "learning differential privacy;differential privacy classifiers;deep learning fair;federated learning;transparency federated learning;learning privacy;privacy classifiers;performance federated learning;federated learning promising;framework federated learning;fairness differential privacy;federated learning fpfl;privacy classifiers beneficial;framework learning privacy;fair differential privacy;federated learning demonstrate;learning privacy form;fairness privacy;differential privacy;differential privacy fpfl;phases fairness privacy;fair learning fpfl;differential privacy guarantees;differential privacy demonstrate;guarantees fairness accuracy;eos federated learning;privacy fpfl based;privacy guarantees framework;fair learning;learning fair differential"}, "ed913bed529d6bb7beac3b6086a853698abf627d": {"ta_keywords": "automatic speech recognition;automatic speech;field automatic speech;digital home assistants;speech recognition related;speech recognition;speech recognition associated;speech recognition powerful;topic automatic speech;home assistants article;home assistants;recognition related applications;recognition associated applications;recognition;recognition related;digital home;speech;recognition associated;assistants article;introduction topic automatic;automatic;recognition powerful;assistants;development digital home;recognition powerful capabilities;assistants article intended;art field automatic;topic automatic;overview state art;field automatic", "pdf_keywords": ""}, "914626e2e13bd42a5f06c28ff02ba7c428e81ff1": {"ta_keywords": "turbulent fluid particles;particles turbulent fluid;particles turbulent;pair particles turbulent;turbulent fluid;propagation pair particles;turbulent;parallel particles moving;parallel particles;fluid particles;particles moving direction;fluid particles assumed;pair parallel particles;particles moving;field particle particles;perpendicular field particle;particle particles;propagation pair;propagation;particles assumed form;pair particles;particle particles assumed;particles;field particle;particle;study propagation pair;particles assumed;form pair parallel;direction perpendicular field;perpendicular field", "pdf_keywords": ""}, "e829ee7fe48f4b1e451378b6a21470b2f86c0aa6": {"ta_keywords": "based erasure codes;erasure codes;erasure codes proposed;erasure code algorithm;private information retrieval;erasure code;algorithm private information;code algorithm private;explicit erasure code;cases erasure codes;algorithm private;systems based erasure;private information;retrieval pir systems;codes proposed algorithm;provide perfect privacy;perfect privacy;explicit erasure;privacy;retrieval pir;erasure;special cases erasure;presents explicit erasure;information retrieval pir;based erasure;code algorithm;privacy contrast existing;codes proposed;private;cases erasure", "pdf_keywords": ""}, "31603b3339f4da5bdc6b7de4231bd1ddfb32a50a": {"ta_keywords": "time dependent partial;predicting distribution time;distribution time dependent;distributions data model;predicting distribution;model predicting distribution;time dependent;arbitrary distributions data;dependent partial differential;partial differential equations;known controlled differential;controlled differential;differential equations model;distributions data;controlled differential equation;distribution time;partial differential;equations model based;novel model predicting;model based known;arbitrary distributions;predicting;model predicting;data model;distributions;applicable arbitrary distributions;model based;dependent partial;differential;differential equations", "pdf_keywords": "nonlinear dynamics neural;prediction nonlinear dynamics;predict nonlinear dynamics;dynamics neural network;ode rnn neural;dynamics neural;dynamics irregular time;ode models able;time series dynamics;time series models;neural controlled differential;irregular time series;model describing dynamics;ode models;models called neural;ode model based;describing dynamics irregular;regular time series;nonlinear dynamics;nonlinear function neural;models time series;time series model;alternative ode models;rnn ode;neural ode;neural ode consider;ode rnn;prediction nonlinear;dynamics irregular;sampled time series"}, "5f563da2843e005c4b236f7889e7a22631b53210": {"ta_keywords": "conference publications evaluation;quality conference publications;evaluation quality conference;conference review results;publications evaluation;conference review;publications evaluation context;quality conference;new evaluation quality;evaluation process conference;evaluation quality;conference publications;process conference review;results new evaluation;evaluation;new evaluation;review results;review results used;evaluation context 2014;conference held budapest;conference;improvements evaluation;evaluation process;2014 conference held;2014 conference;examines consistency evaluation;process conference;improvements evaluation process;conference held;evaluation context", "pdf_keywords": "conference peer review;examined inconsistency conference;paper conference neurips;papers reviewers;papers reviewers picking;conferences reviewers picking;paper peer review;experiment accepted papers;conferences reviewers;review paper measure;rate papers conference;review papers reviewers;quality reviews statistically;reviews statistically;peer review determine;reviewer quality scores;conference reviewedthe peer;rate conference paper;papers correlation quality;conference review;reviewers scores;scoring reviewers;majority papers consistently;paper rejected conference;conference review process;inconsistency conference peer;paper conference reviewedthe;subjective scoring reviewers;peer review;accepted papers correlation"}, "dd5e54b08b2c1520d179e88cd524e9bd4fe1f6ab": {"ta_keywords": "approximating emphsinkhorn divergences;complexity emphsinkhorn divergences;divergences emph regularized;variant optimal transport;divergences emph ii;emphsinkhorn divergences;divergences emph;optimal transport ot;optimal transport;sample complexity emphsinkhorn;transport ot distance;emphsinkhorn divergences emph;distance approximating emphsinkhorn;complexity emphsinkhorn;approximating emphsinkhorn;ot distance approximating;divergences;bound sample complexity;regularized variant optimal;regularizer varepsilon optimizers;sample complexity;varepsilon optimizers;transport;transport ot;distance approximating;variant optimal;approximating;emph regularized variant;complexity;regularizer varepsilon", "pdf_keywords": "complexity optimal transport;regularized optimal transport;minimal cost quantum;optimal transport ot;optimal transport;sampling complexity optimal;optimizing transport;optimal transport problem;cost quantum;divergence expectation maximization;sinkhorn divergence stochastically;approximating divergence distance;quantum information theory;regularized optimal;problem optimizing transport;convergence sample complexity;transport ot maximum;consider sampling complexity;sampling complexity;sinkhorn divergence expectation;prove optimizers regularized;divergence stochastically driven;optimizers regularized optimal;sample complexity markov;distance finite samples;approximating divergence;divergence stochastically;techniques approximating divergence;regularized ot bounded;importance quantum information"}, "8b192503f119a0b0cc30ef5179a00f231c20fb93": {"ta_keywords": "fake event epochs;nonparametric deep neural;event epochs;novel nonparametric deep;nonparametric deep;fake event;deep neural;event epochs consecutive;network rnn;rnn;introduction fake event;rnn optimally;events data set;events data;network rnn optimally;real world datasets;events;neural network rnn;events introduction fake;negative evidence observable;evidence observable events;existing baselines datasets;deep neural network;baselines datasets studied;epochs;negative evidence;rnn optimally reinforces;observable events;event interval experiments;event", "pdf_keywords": "deep event models;time deep event;deep temporal;intensity deep temporal;deep event;continuous time deep;predict sequence events;modeling event sequences;parametric deep learning;parametric deep neural;fake event epochs;recurrent neural networks;deep temporal spatial;deep learning;deep neural;channel rnn;non parametric deep;time deep;event epochs;rnn;rnn optimally;predicting time;short term memory;recurrent neural;present deep learning;train deep neural;modeling approach recurrent;recurrent neural network;event stream model;event sequences model"}, "0ca2575a1ef73930dc2abe205b44e079eadc426c": {"ta_keywords": "model traffic flow;multiple lanes;lanes multiple;lanes multiple populations;multiple lanes multiple;traffic flow lane;lane changing behavior;model traffic;decision theoretic lane;traffic flow;design tolling schemes;tolling schemes improve;tolling schemes;incorporates multiple lanes;theoretic lane changing;improve traffic flow;lane changing;macroscopic model traffic;lanes;traffic flow incorporates;model design tolling;lane road examples;traffic;tolling;design tolling;flow lane;improve traffic;flow lane road;drivers local decision;theoretic lane", "pdf_keywords": ""}, "b5991b1018bb89b053a2c8229248f97956391bb5": {"ta_keywords": "iterative pronunciation learning;pronunciation learning method;pronunciation learning;automatically estimated pronunciation;estimated pronunciation variations;estimated pronunciation;native speech recognition;speech recognition learns;non native pronunciations;iterative pronunciation;native pronunciations directly;acoustic model adaptation;propose iterative pronunciation;speech using iterative;native pronunciations;learns non native;pronunciation variations automatically;non native speech;speech non native;non native speakers;native speakers various;pronunciations directly speech;adaptation experiments speech;speech recognition;frequency pronunciation variations;pronunciation variations;pronunciations directly;native speech;iterative estimation;pronunciation", "pdf_keywords": ""}, "b19cba7bfe318c69d5e62f8322cb5d75228452f4": {"ta_keywords": "large scale language;scale language models;language models efficient;language models;weight matrices pretrained;matrices pretrained model;scale language;matrices pretrained;optimization parameterized hypercomplex;tuning large scale;specific weight matrices;low rank optimization;rank optimization parameterized;weight matrices;encoding task specific;encoding task;rank optimization novel;parameterized hypercomplex;parameterized hypercomplex multiplication;optimization parameterized;rank optimization;task specific weight;pretrained model;novel method encoding;models efficient;large scale;hypercomplex;hypercomplex multiplication;method encoding task;fine tuning large", "pdf_keywords": "large scale language;scale language models;language models high;pretrained language models;language models downstream;trained language models;soft computing high;pretrained language model;low rank language;rank language models;tuning large scale;language models;language models important;optimization parameterized hypercomplex;computing high performance;adapting pretrained language;language models achieves;trained parameters memory;parameterizing deep;parameterizing deep learning;kernel pretrained language;high performance computing;language model kernel;language model ability;scale language;computing fine tuning;soft computing;language models used;advances parameterized hypercomplex;language model"}, "3ba013b8b56646d66aac8472fb90a5c029ef55a0": {"ta_keywords": "equations learning process;equations learning;underlying equations learning;numerical solvers;numerical solver;traditional numerical solvers;numerical solvers demonstrate;numerical solver equations;faster traditional numerical;solvers;method training faster;solved numerical solver;solver;solver equations;solver equations describing;learning process optimized;solvers demonstrate;solve class equations;optimized training optimal;equations solved numerical;training optimal time;optimized training;training optimal;solvers demonstrate utility;time solve class;solve class;learning process;process optimized training;method training;learning", "pdf_keywords": "regularization neural odes;accelerating learning differential;learning differential equations;approach regularizing dynamics;regularizing dynamics;speed regularization;regularized trajectories;regularized trajectory;propose speed regularization;trajectory conjecture regularization;regularizing dynamics neural;learning differential;runge kutta solver;model performance regularization;results regularized trajectories;neural odes improves;observe regularized trajectories;apply speed regularization;performance regularization;method accelerating learning;speed regularization neural;solver order regularizer;investigate regularization;learning solve ordinary;accelerating learning;regularization term computed;use regularized trajectory;regularization order solution;adaptive solvers;kutta solver"}, "26299d5fdc5137291dc6a091573b3d18aba1d1c2": {"ta_keywords": "task cross lingual;adapting pretrained multilingual;cross lingual transfer;lingual transfer representative;pretrained multilingual model;lingual transfer introduce;multilingual model new;lingual transfer;multilingual model;cross lingual;pretrained multilingual;language proposed adapter;multilingual;languages named entity;model new language;lingual;art cross lingual;entity recognition outperforms;diverse languages named;diverse languages;typologically diverse languages;named entity recognition;new language;entity recognition;multi task cross;new language proposed;task cross;languages named;languages;adapting", "pdf_keywords": "pretrained multilingual models;adapt pretrained multilingual;pretrained multilingual model;multilingual pretrained models;multilingual model pretrained;adapters multilingual pretrained;efficient language adapters;pretrained multilingual;adapt multilingual task;task adapters multilingual;pretrained multi language;multilingual pretrained;language adapters;language model cross;adapters multilingual;adapt multilingual;monolingual model pretrained;multilingual models achieve;multilingual models;multilingual model arbitrary;model cross lingual;new multilingual model;model pretrained language;adapt languages;lingual transfer based;multi language bridge;multilingual model;able adapt multilingual;language adapters architecture;pretrained model languages"}, "cce8cf3d7f45113a4cba984b878802a5b16d5967": {"ta_keywords": "xmath1 model statistical;transition xmath1 model;xmath2 model statistical;phase transition xmath1;xmath0 model statistical;transition xmath0 model;phase transition xmath0;properties xmath2 model;xmath1 model xmath2;properties xmath0 model;xmath1 model;xmath2 model;model xmath2 model;xmath0 model;statistical properties xmath2;model xmath2;xmath0 model similar;transition xmath1;statistical phase transition;different xmath0 model;similar xmath1 model;statistical properties xmath0;xmath2 model different;transition xmath0;model different xmath0;model similar xmath1;model statistical phase;properties xmath2;properties xmath0;properties statistical phase", "pdf_keywords": ""}, "77ce1b8d425b7538c21ce0be976ee24a58e797c1": {"ta_keywords": "negative matrix factorization;matrix factorization nsf;matrix factorization;factorization nsf basis;factorization nsf;basis functions optimization;optimization reconstruction basis;reconstruction basis functions;factorization;nsf basis functions;non negative matrix;nsf basis;analysis reconstruction basis;negative matrix;basis functions incorporate;functions optimization reconstruction;reconstruction basis;optimization reconstruction;basis functions performed;matrix;incorporate filtering;basis functions;incorporate filtering step;snr performance;separate analysis reconstruction;signal noise ratio;functions incorporate filtering;noise ratio snr;method based generalization;snr performance criteria", "pdf_keywords": ""}, "5e8c52ddbd3581320f7e536b7cd10d7263b81eb2": {"ta_keywords": "feature predicates student;generation feature predicates;create feature predicates;constructed feature predicates;feature predicates;unsupervised grammar induction;feature predicates effort;grammar induction algorithm;predicates student sample;predicates student;grammar induction;predicates;unsupervised grammar;using unsupervised grammar;predicates effort required;manually constructed feature;predicates effort;induction algorithm learning;algorithm learning agent;constructed feature;grammar;learning agent;algorithm learning;learning agent perform;create feature;approach generation feature;generation feature;induction algorithm;feature;learning", "pdf_keywords": ""}, "4481244de2cc0c55d91cebbb152eec79a76386f3": {"ta_keywords": "3d tsv integrations;conductive film 3d;tsvs logic device;logic device fabricated;density reliable packaging;film 3d tsv;reliable packaging technology;packaging technology non;packaging technology;device assembled 3d;logic device assembled;3d tsv;laminated substrates logic;substrates logic device;density reliable assembly;tsv integrations;assembled 3d;reliable packaging;non conductive film;assembled 3d structure;logic device wide;tsv integrations target;device fabricated middle;conductive film;device fabricated;film 3d;logic device;packaging;wide laminated substrates;1200 tsvs logic", "pdf_keywords": ""}, "d26254cf3ec537f37708afaaf7f5a76a7922d4a2": {"ta_keywords": "reorderings word pairs;learn reorderings word;models word pairs;reorderings word;models learn reorderings;word pairs distances;reordering sub models;improve translation quality;learn reorderings;translation quality;word pairs;reorderings;useful improve translation;reordering;word pairs different;improve translation;reordering sub;demonstrate reordering sub;demonstrate reordering;sub models word;experiments demonstrate reordering;sub models learn;pairs different distances;models word;sub models;translation;pairs distances;pairs distances specific;separate sub models;models learn", "pdf_keywords": ""}, "10e221c7d4636703c5c97b54f53b1cb57c25f3a6": {"ta_keywords": "unregularized linear networks;networks effect importance;linear networks effect;linear networks;performance unregularized linear;importance weighting diminishes;l2 regularization;unregularized linear;importance weighting performance;importance weighting;effect importance weighting;networks effect;regularization;weighting performance unregularized;networks;weighting diminishes indefinite;weighting diminishes;presence l2 regularization;weighting performance;effect importance;performance unregularized;diminishes indefinite training;linear;investigates effect importance;indefinite training diminishing;importance;training diminishing effect;weighting;training diminishing;indefinite training", "pdf_keywords": ""}, "6dfecb5915e8b10841abe224c5361bbda7100637": {"ta_keywords": "morphological parsers tigrinya;parsers tigrinya oromo;based morphological parsers;morphological parsers;parsers tigrinya;lemmatization tgrinya language;rule based morphological;lemmatization morphological segmentation;based lemmatization tgrinya;lemmatization morphological;parsers;tigrinya oromo languages;parsers context lemmatization;parsers finite state;based morphological;context lemmatization morphological;lemmatization tgrinya;languages lemmatization;use languages lemmatization;tgrinya language;rule based lemmatization;models evaluate parsers;parsers finite;morphological segmentation;morphological segmentation present;tgrinya language suitable;oromo languages based;morphological;paradigm parsers finite;based paradigm parsers", "pdf_keywords": ""}, "73e401dead436aabd0cd9c941f7b13bfdeda9861": {"ta_keywords": "compressors efficient communication;point compressors efficient;communication distributed optimization;distributed optimization;distributed optimization approach;point compressors;class point compressors;efficient communication distributed;compressors efficient;efficient communication;approach allows compressors;communication distributed;communication complexity;compressors evolve training;communication complexity practical;optimization approach;optimization;compressors;optimization approach allows;theoretical communication complexity;allows compressors;compressors evolve;allows compressors evolve;error feedback mechanism;improving theoretical communication;new efficient methods;communication;feedback mechanism theoretical;feedback mechanism;complexity practical efficiency", "pdf_keywords": "compressors communication efficient;compressor communication efficient;communication efficient compression;communication compression;point compressors communication;communication compressed vectors;point compressor communication;iii communication compression;compressor communication;compressors communication;communication compression provide;communication compressed;method communication compressed;compressor new communication;contractive compression operators;distributed learning compression;approaches contractive compression;efficient compression scheme;efficient compression;compression operators;communication efficient training;compressor pc communication;compression mappings communication;compression operators ii;efficient communication;learning compression;efficient communication efficient;compression algorithms;linear communication algorithms;contractive compression"}, "1c2c9e5d0588599516a78adda1fe3935dc5ae5d7": {"ta_keywords": "stochastic gradient play;efficiency stochastic gradient;optimization stochastic gradient;play unconstrained optimization;unconstrained optimization stochastic;optimization stochastic;convex games;stochastic gradient;monotone games method;convex games shown;method stochastic gradient;gradient play unconstrained;strongly monotone games;class convex games;gradient play strongly;efficiency stochastic;gradient play;unconstrained optimization;stochastic derivative free;play strongly monotone;derivative free play;estimating efficiency stochastic;monotone games;unconstrained optimization method;play unconstrained;optimization class convex;stochastic derivative;free play approach;stochastic;competitive classical method", "pdf_keywords": ""}, "311381feeb6346bfcb2ba622bd8f713261a4075d": {"ta_keywords": "comments edits collaborative;user comments edits;comment edit pairs;edits collaborative documents;comments edits;relationship user comments;wikipedia revision histories;edits collaborative;collaborative documents;based wikipedia revision;user comments;wikipedia revision;comment edit;collaborative documents approach;revision histories;million comment edit;histories propose hierarchical;revision histories propose;comments edits encoding;pairs based wikipedia;edit pairs based;hierarchical neural;deep neural;hierarchical neural network;based hierarchical neural;layer deep neural;revision;edit pairs;documents approach;specific edit actions", "pdf_keywords": ""}, "efcdb62b59e4dfb3f51b53850a81d6149ec3dfc8": {"ta_keywords": "interpretable machine learning;workflow interpretable machine;interpretable machine;diagnostic workflow interpretable;machine learning iml;propose diagnostic workflow;workflow interpretable;diagnostic workflow;machine learning;interpretable;diagnostics propose taxonomy;validate iml methods;useful diagnostics propose;validate iml;diagnostics propose;iml methods useful;define validate iml;iml methods;learning iml;propose diagnostic;useful diagnostics;methods useful diagnostics;diagnostics;learning iml connects;workflow;validate;use cases researchers;cases researchers consumers;cases researchers;use cases methods", "pdf_keywords": "machine learning iml;interpretable machine learning;learning methods iml;diagnostics consumers propose;objectives tos iml;iml embrace diagnostic;technical objectives tos;iml diagnostics use;taxonomy iml use;iml diagnostics;diagnostics consumers;iml methods proposing;objective tos identified;iml artificial intelligence;interpretable machine;reconciling technical objectives;taxonomy iml;machine learning aims;existing iml methods;technical objective tos;diagnostics use;field interpretable machine;methods iml;tos iml methods;technical objectives;iml methods;using methods discovering;methods discovering;narrow technical objectives;taxonomy deliver rigorously"}, "null": {"ta_keywords": "hierarchies graphs;model hierarchies graphs;tree sampling;metrics tree sampling;probabilistic model hierarchies;hierarchical clustering efficient;hierarchical clustering;tree based hierarchies;hierarchies graphs obtained;high quality hierarchies;quality hierarchies;parameter tree sampling;tree sampling parameter;theory hierarchical clustering;quality metrics tree;sampling parameter tree;quality hierarchies present;relaxation tree;continuous relaxation tree;relaxation tree based;hierarchies;hierarchies model;hierarchical;hierarchies model draws;metrics tree;based hierarchies model;model hierarchies;graphs including large;parameter tree;large graph", "pdf_keywords": ""}, "42fc352a0db1e742b0248a02b812db4aaf7b2cd3": {"ta_keywords": "reranking neural machine;reranking neural;based reranking neural;energy based reranking;neural machine translation;machine translation nmt;trained nmt energy;machine translation;translation nmt using;reranking;based reranking;nmt energy;translation nmt;nmt energy based;reranking ebr train;based reranking ebr;reranking ebr;neural machine;train energy;train energy based;improving energy;based trained nmt;improving energy based;trained nmt;mle based trained;energy;problem improving energy;energy based model;method energy;energy based", "pdf_keywords": "translation tasks ranking;translation model trained;improve ranking translations;ranking translations translation;ranking translations;neural machine translation;words translation model;translation model translation;translation combining deep;machine translation;improves translation performance;machine translation datasets;translation performance machine;resource machine translation;translation machine;translation model;translations translation tasks;improve translation performance;translation machine performance;performance machine translation;model translation;translation model evaluated;model translation model;machine translation combining;translation tasks;translation performance;deep fusion lingual;translation performance perturbative;improves translation;fusion lingual models"}, "740afdd1619d797145b056877865f941891e6a65": {"ta_keywords": "decision maker iterations;iteration complexity algorithms;gradient oracle iteration;algorithms decision maker;iteration complexity;oracle iteration complexity;complexity algorithms decision;iterations case decision;algorithms decision;iterations geometric domain;geometric domain decision;maker iterations geometric;iterations geometric;complexity algorithms;gradient oracle case;gradient oracle;assumed gradient oracle;maker iterations;iterations domain;der gradient oracle;maker iterations case;algorithms;iterations case;oracle iteration;decision maker;complexity;problem decision maker;iterations;iteration;decision maker der", "pdf_keywords": "stochastically driven decision;decision dependent learning;risk stochastic decision;stochastic pricing parking;stochastic decision;dynamic implementation bandit;known bandit algorithms;bandit algorithms;stochastic decision dependent;bandit algorithms proposed;algorithms known bandit;driven decision trees;risk minimization;approximated stochastic program;prediction function approximated;performative prediction problem;dependent risk minimization;algorithm stochastic gradient;data dynamic pricing;efficient performatively optimal;provide asymptotically optimal;stochastic gradient methods;optimizing expected risk;dynamic pricing;performatively optimal algorithms;learning problem geometrically;performatively optimal;prediction function stochastic;optimum performative prediction;risk minimization given"}, "b9c026ab6e161a0f8c4b4db82ee8ad10792084cc": {"ta_keywords": "naturalness electrollarynx ngectomees;improving naturalness electrollarynx;naturalness electrollarynx;electrollarynx ngectomees;electrollarynx ngectomees presented;electrollarynx;voice conversion experimental;statistical voice conversion;voice conversion;subtraction statistical voice;naturalness compared ngectomees;ngectomees presented proposed;statistical voice;ngectomees presented;compared ngectomees;ngectomees keeping intelligibility;ngectomees;voice;spectral subtraction statistical;compared ngectomees keeping;ngectomees keeping;based spectral subtraction;method based spectral;spectral subtraction;improving naturalness;approach improving naturalness;based spectral;significant improvements naturalness;naturalness;improvements naturalness", "pdf_keywords": ""}, "066f2023b2b5ba5df61dc193c205785fa5e73fed": {"ta_keywords": "kernel clustering spectral;kernel clustering solvers;clustering spectral relaxation;kernel clustering;approach kernel clustering;continuous kernel clustering;standard kernel clustering;kernel clustering new;clustering spectral;spectral relaxation approach;clustering solvers;kernel spectral bounds;spectral relaxation;kernel spectral;linear kernel spectral;clustering solvers demonstrate;clustering;clustering new linear;discrete continuous kernel;clustering new;new approach kernel;approach kernel;relaxation approach;spectral bounds;linear kernel;spectral bounds new;relaxation approach combines;new linear kernel;bound optimization;bound optimization algorithm", "pdf_keywords": ""}, "9578679e028777dd709881f938114aa59fbbf481": {"ta_keywords": "matching entity names;metrics matching entity;comparing string distance;matching entity;string distance metrics;based string distance;string distance scheme;entity names;comparing string;terms similarity;metrics terms similarity;entity names use;empirical string comparator;distance metrics matching;string distance;metrics matching;method comparing string;similarity;tfi based string;string comparator scheme;scheme compare metrics;comparing metrics terms;compare metrics terms;based string;compare metrics;comparing metrics;string comparator;method comparing metrics;entity;empirical string", "pdf_keywords": ""}, "ab627ba77dced941f9f45eeaee17bc6644308d89": {"ta_keywords": "collective classification email;email act classification;classification email acts;classification email;collective classification method;new collective classification;based collective classification;collective classification;applied collective classification;act classification;act classification proposed;act classification compared;classification;email acts;classification proposed;classification proposed method;classification method;classification method proposed;classification compared;classification method based;email act;network based collective;classifier based;classifier;email acts problem;method applied collective;based dependency network;dependency network based;field email act;improvements email act", "pdf_keywords": ""}, "3429d0529d3e77f9e4606f13b2d252d5d964abad": {"ta_keywords": "spin orbit coupling;orbit coupling spin;coupling spin orbit;spin orbit coupled;coupled spin orbit;orbit coupled spin;spin orbit interaction;interaction spin orbit;orbit interaction spin;coupling strength spin;coupling spin;coupled spin;orbit coupling strength;orbit coupling;strength spin orbit;orbit coupling used;control spin orbit;spin orbit;effect spin orbit;value spin orbit;orbit coupled;interaction spin;orbit interaction;control spin;tuned value spin;coupling strength tuned;coupling strength;coupling;effect spin;spin", "pdf_keywords": ""}, "66b83f0801d0c2d4194ff60c5ef9c754b51ce521": {"ta_keywords": "image segmentation models;segmentation models;interpreting image segmentation;ct scans interpretability;image segmentation;pancreas ct scans;segmentation models learning;segmentation;analysis pancreas ct;learning regions images;scans interpretability model;models learning regions;analysis pancreas;ct scans;regions images noise;scans interpretability;learning regions;pancreas ct;method interpreting image;method analysis pancreas;regions images;interpreting image;obscured images;performance obscured images;images noise;pancreas;interpretability model quantitatively;images noise applied;images;interpretability model", "pdf_keywords": "interpretability image segmentation;semantically segment image;interpretability models image;models image segmentation;interpretability image;image segmentation train;image segmentation tasks;image segmentation;semantically segment;method interpretability image;segment image;analyzing images noise;segmentation model;accurate segmentation model;segmentation;describing analyzing images;accurate segmentation;segmentation model demonstrate;images pancreasct validation;segmentation tasks;method segmentation pancreas;analyzing images;model trained interpret;segmentation train;produce accurate segmentation;images pancreasct;uses semantically segment;segmentation pancreas ct;segmentation pancreas;learning interpretability"}, "568efa8d71d8f2a086c8debcdf547e7053269021": {"ta_keywords": "entanglement quantum measurement;measurement entanglement quantum;measure entanglement quantum;measurement entanglement;based measurement entanglement;measure entanglement;method measure entanglement;measurement quantum state;entanglement quantum;quantum based measurement;quantum measurement;entanglement quantum based;measurement quantum;quantum measurement performed;quantum entanglement;state measurement quantum;used measure entanglement;entanglement quantum entanglement;entanglement;quantum entanglement key;entanglement key;quantum information processing;state quantum;quantum information;quantum state;entanglement key resource;quantum state used;state quantum state;state measurement;coherent state quantum", "pdf_keywords": ""}, "4f0d485cbcde840533f23f0c8b0f3fa1ca2d74df": {"ta_keywords": "linear bandit problems;generalizes linear bandits;linear bandits;linear bandits provide;bandit problems generalizes;linear bandit;class linear bandit;bandit problems;bandits provide instance;lower bounds transductive;bandits;bandits provide;bounds transductive setting;bounds transductive;bandit;transductive setting algorithm;dependent lower bounds;lower bounds;problems generalizes linear;transductive setting;transductive;generalizes linear;problems generalizes;algorithm matches logarithmic;bounds;matches logarithmic factors;setting algorithm;new class linear;instance dependent lower;setting algorithm matches", "pdf_keywords": "measurements transductive bandit;transductive bandit problem;transductive linear bandit;complexity linear bandit;transductive bandit setting;linear bandit problem;general transductive bandit;transductive bandit;bandit problem proposed;exploration linear bandit;linear stochastic bandits;bandit problem general;bandit problem given;linear bandits provide;bandit problem;following bandit problem;linear bandits;simulations linear bandit;linear bandit;linear bandit bandit;consider linear bandits;case linear bandits;bandits provide algorithm;linear bandits present;optimal measurements transductive;bandit assumed linear;stochastic bandits;strategy optimal sample;bandits present adaptive;stochastic bandits best"}, "7b19e6540c786b80a3615a8ae2ef706242a1fa5b": {"ta_keywords": "compressive phase retrieval;scheme compressive phase;sparse graph codes;phase retrieval sparse;compressive phase;propose scheme compressive;scheme compressive;retrieval sparse graph;compressive;sparse graph;recovers sample complexitys;recover sample complexitysup;retrieval sparse;phase retrieval;complexity log sublinear;sparse;graph codes;sample complexitysup log;graph codes consider;sublinear scheme recovers;sublinear scheme recover;sample complexitysup;log computational complexity;complexitysup log computational;complexitysup log;sample complexitys;computational complexity log;log sublinear scheme;complexity log;small measurement matrix", "pdf_keywords": "compressive phase retrieval;noisy compressive phase;compressive phase;recovery sparse phase;phase retrieval algorithm;sparse complex signal;sparse phase linear;schemes compressive phase;phase retrieval based;novel phase retrieval;noisy signal recovery;noisy compressive;sparse phase;consider noisy compressive;algorithm phase coding;phase retrieval;phase retrieval problem;phase coding based;recover sparse complex;signal recovery;based phase coding;consider recovery sparse;novel schemes compressive;complexity signal;compressive;phase coding;recovery sparse;computational complexity signal;phase code algorithm;complexity signal dimension"}, "a0cbaf59f563580f68523ab6839a436e38b6db18": {"ta_keywords": "translation multilingual corpus;improves translation efficiency;low resource translation;multilingual corpus;translation efficiency;improves translation;translation multilingual;multilingual corpus relies;resource translation multilingual;strategy improves translation;translation efficiency orders;resource translation;multilingual;sampling source sentence;corpus relies conditionally;source sentence strategy;corpus;corpus relies;translation;sentence strategy improves;conditionally sampling source;sentence strategy;relies conditionally sampling;optimizing low resource;source sentence;low resource;conditionally sampling;optimizing low;sampling source;reduces training overhead", "pdf_keywords": "optimizing training multilingual;machine translation;multilingual data improve;proposed machine translation;data improve translation;machine translation framework;multilingual data;multilingual training based;training multilingual;multilingual translation;training multilingual nmt;multilingual training;natural language model;relevant data multilingual;problem multilingual training;selecting best translation;data multilingual data;data multilingual;gains translation accuracy;method multilingual translation;improve translation accuracy;multilingual translation novel;multilingual nmt;linguistic specificity translation;multilingual nmt method;multilingual translation given;language model;best translation pattern;named multilingual translation;structure natural language"}, "105146a7872835a52c8c5c55a3aae62c5d8852a1": {"ta_keywords": "based machine translation;phrase based translation;machine translation;machine translation approach;machine translation harnesses;approach machine translation;based translation approach;translation approach;phrase based machine;translation harnesses power;based translation;translation approach leads;character based framework;translation harnesses;accuracy phrase based;alignment strategy significantly;improve accuracy phrase;phrase based;alignment strategy;use alignment strategy;translation;accuracy traditional phrase;character based;traditional phrase based;use alignment;alignment;power character based;framework use alignment;accuracy phrase;novel approach machine", "pdf_keywords": ""}, "9d10bbd21f475d500c3a7e24052e02596e052e2e": {"ta_keywords": "speech recognition;speech recognition called;approach speech recognition;recognize speech;jlaser based neural;recognize speech presence;used recognize speech;neural networks;jlaser used recognize;based neural networks;recognition called jlaser;neural networks recently;recognition;based neural;speech presence strong;speech presence;jlaser based;approach speech;gram language;use gram language;speech;recognition called;new approach speech;approach used recognize;called jlaser based;jlaser used;neural;based use gram;approach called jlaser;called jlaser used", "pdf_keywords": ""}, "36d193c7a9523f55f9fe5ffd0730f248c241f5c7": {"ta_keywords": "bias process publishing;publishing work fair;fair way bias;bias process;bias bias;bias;bias bias process;publishing work;open problems fair;way bias bias;way bias;publishing;publishing work present;work fair;work fair way;open problems;problems fair;process publishing;process publishing work;following open problems;problems fair way;work present solutions;following open;work;process;solutions tutorial;problems;open;fair;work present", "pdf_keywords": ""}, "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad": {"ta_keywords": "new commonsense inference;commonsense inference task;commonsense inference;novel adversarial dataset;adversarial dataset;inference task machine;construct novel adversarial;text rich models;adversarial;new dataset hellaswag;dataset construct novel;dataset hellaswag;novel adversarial;commonsense;new commonsense;rich models xmath2;new dataset;present new commonsense;inference task;dataset;dataset construct;construct new dataset;sentence construct;xmath3 use dataset;rich models;use dataset;models xmath2;sentence construct new;really finish sentence;inference", "pdf_keywords": "deep models commonsense;commonsense models trained;trained dataset commonsense;models commonsense learning;dataset commonsense models;inference challenges deep;datasets commonsense models;commonsense learning;dataset commonsense natural;new dataset commonsense;commonsense learning nng;commonsense inference;dataset commonsense reasoning;commonsense natural language;text datasets commonsense;perform commonsense inference;dataset commonsense;commonsense models;datasets commonsense;models commonsense;commonsense inference state;language inference challenges;challenges deep;commonsense models arewe;commonsense reasoning;commonsense natural;commonsense;challenges deep pretrained;natural language inference;deep pretrained models"}, "de7d0c87794c3de6f8ab2c753ecc398c18c26631": {"ta_keywords": "extracts grammatical specification;agreement languages framework;describing agreement languages;agreement languages;grammatical specification raw;pass grammatical specification;grammatical specification nearly;grammatical specification;rules describing agreement;extracting rules describing;specification raw text;expert annotations language;framework extracting rules;describing agreement;annotations language;extracting rules;transfer expert annotations;annotations language framework;framework extracts grammatical;specification raw;languages framework extracts;language framework extracts;universal interoperability project;universal interoperability;language framework;languages included universal;expert annotations;languages framework;specification nearly equivalent;included universal interoperability", "pdf_keywords": "extract rules linguist;extract grammatical agreement;agreement rules languages;linguist predicting agreement;describing agreement languages;rules languages morphological;rule extractions;rules linguist using;agreement rules language;extract rule extractions;rules text languages;predicting agreement languages;rule extraction;automatic evaluation rules;syntactic rules describing;rules linguist;automatically extract linguisto;rule extractions 50;standard syntactic analysis;syntactic analysis compared;syntactic analysis evaluate;morpho syntactic rules;agreement languages framework;agreement languages;extraction rules;rules languages;syntactic analysis;ofagreement rules languages;syntactic linguistic;extract linguisto tract"}, "b709da495f15a4a9c1173192ecd755d1697dedf0": {"ta_keywords": "reading recommendation saccharomyces;recommendation saccharomyces genome;reading recommendation tasks;recommendation saccharomyces;saccharomyces genome database;discover effective recommendation;recommendation strategies;proposed reading recommendation;recommendation strategies represented;reading recommendation;effective recommendation strategies;based reading recommendation;recommendation tasks;effective recommendation;content collaborative filtering;recommendation tasks leveraging;novel reading recommendation;genome database;genome database based;collaborative filtering approaches;collaborative filtering;saccharomyces genome;random walk model;history based reading;citation history based;recommendation;paths graph experiments;outperforms random walk;genome;content collaborative", "pdf_keywords": ""}, "bef4548a43fca8a7410734e4200157d50e257a29": {"ta_keywords": "span learning automatic;adaptive span learning;adaptive span algorithm;span learning;learning automatic speech;use adaptive span;adaptive span;automatic speech recognition;automatic speech;span algorithm proposed;span algorithm;learning automatic;speech recognition;method adaptive span;speech recognition proposed;based network learning;network learning;network learning concept;automatically selecting sequence;new method adaptive;use adaptive;adaptive;span;automatically selecting;learning concept based;automatically selecting appropriate;method adaptive;recognition;recognition proposed method;recognition proposed", "pdf_keywords": ""}, "dad121213a17c6cc977d2298c7a9927639ca58e6": {"ta_keywords": "generated random walk;random walk generated;walk generated random;walks random environment;random walks;number random walks;random walk;application random walk;random walks random;walks random;random walk stationary;field random walk;random walk presence;state random walk;walk generated;walk presence random;random noise field;walk stationary state;noise field random;stationary state random;random environment method;random noise;random environment;walk stationary;walks;generated random;walk;state random;presence random noise;walk presence", "pdf_keywords": ""}, "9f832bdcbc9d9566f7ab07b7455364bee62086fb": {"ta_keywords": "", "pdf_keywords": ""}, "f79361dda56ee755fc56ab83cf0d9f12d42b2d5e": {"ta_keywords": "identification subset generative;subset generative model;subset generative;efficient identification subset;generative model;generative model based;identification subset;efficient identification;method efficient identification;generative;model;identification;model based;method efficient;new method efficient;subset;efficient;method;new method;present new method;new;based;present new;present", "pdf_keywords": "approximate ranking items;recovering approximate ranking;items pairwise ranking;items optimally ranked;approximate ranking;pairwise ranking;noisy pairwise comparisons;ranking upper bound;ranking items;ranked item pairwise;ranking items based;pairwise ranked item;highly ranked items;item pairwise ranked;noisy pairwise comparison;robust optimal counting;ranking items simple;recovering entire ranking;comparisons provably optimal;pairwise ranking given;items ranked large;entire ranking items;algorithm ranks items;ranking recovered;exact recovery ranking;pairwise comparisons items;items optimal subset;ranking items random;pairwise pairwise ranked;ranking recovered exactly"}, "092687dc06b0b264a524c6d4ea151780ba85a02a": {"ta_keywords": "context communication brain;social communication skills;communication brain;communication skills uses;communication brain proposed;social communication;communication context;communication context communication;people social communication;communication skills;communication context method;communication;context communication;social communication problems;tool social communication;training tool social;train people social;communication problems;modalities communication context;modalities communication;brain proposed method;uses modalities communication;people social;skills uses;brain proposed;nucl communication context;social;skills uses modalities;training tool;context method", "pdf_keywords": ""}, "2d8d51d483a50c6fbf16a0cc120465539f4055da": {"ta_keywords": "tweet information theoretic;tweet results languages;quantifying said tweet;microblog text;languages use entropy;information character languages;languages information;character languages information;single microblog text;tweet information;said tweet information;microblog text 26;information character;languages larger;languages larger character;contained single microblog;multilingual;information theoretic;contain information character;single microblog;character languages;tweet results;multilingual study information;different languages;languages;microblog;results languages larger;languages use;character sets languages;information theoretic approach", "pdf_keywords": ""}, "27ad78b72c3fb77a117b15855008b65e838314e8": {"ta_keywords": "coupling dynamics xmath0he;spin orbit coupling;dynamics xmath0he xmath1he;spin orbit;effect spin orbit;orbit coupling dynamics;orbit coupling;dynamics xmath0he;xmath1he xmath2he xmath3he;xmath2he xmath2he xmath2he;xmath2he xmath2he xmath3he;xmath2he xmath3he xmath2he;xmath3he xmath2he xmath2he;xmath2he xmath3he xmath;xmath1he xmath2he;xmath0he xmath1he xmath2he;xmath2he xmath2he;xmath3he xmath2he;xmath2he xmath3he;xmath2he;xmath3he xmath;xmath1he;xmath3he;xmath0he xmath1he;coupling dynamics;spin;effect spin;xmath0he;xmath;orbit", "pdf_keywords": ""}, "51b2dd5cbec02f016c6fa716705ede9b3846a410": {"ta_keywords": "spin dynamics antiferromagnet;antiferromagnet spin dynamics;dynamics antiferromagnet spin;antiferromagnet spin orbit;heisenberg antiferromagnet spin;spin heisenberg antiferromagnet;antiferromagnet sensitive spin;antiferromagnet spin;dynamics antiferromagnet;dynamics spin heisenberg;dynamics antiferromagnet sensitive;spin dynamics spin;heisenberg antiferromagnet;spin orbit interaction;spin dynamics;interaction spin dynamics;dynamics spin;orbit interaction spin;control spin dynamics;sensitive spin orbit;effect spin orbit;interaction spin;antiferromagnet;antiferromagnet sensitive;spin orbit;effect spin;spin heisenberg;control spin;sensitive spin;spin", "pdf_keywords": ""}, "3e86ecbd41ab55b90d3b45601aeb15d2e5c1c8f8": {"ta_keywords": "knockout tournament prove;player knockout tournament;knockout tournament;draw player wins;optimal draw;tournament prove;tournament prove checking;draw given player;finding optimal draw;optimal draw given;given player knockout;tournament;draw player;player knockout;player wins np;wins np complete;exists draw player;draw;draw given;exists draw;wins np;checking exists draw;knockout;given player;player wins;memoization based algorithm;optimal;solved polynomial time;finding optimal;polynomial time", "pdf_keywords": ""}, "7580df14bf01438e7174bbff260508a39a44df84": {"ta_keywords": "erasure codes distributed;codes distributed storage;explicit erasure codes;erasure codes;storage code;underlying storage code;storage code present;storage channels;distributed storage;codes distributed;repair optimal codes;storage channels ii;codes enabling sparsity;distributed storage following;storage repair optimal;fast encoding spars;possible storage channels;storage repair;fast encoding construct;matrix underlying storage;encoding spars;optimal codes;spread possible storage;fast encoding;storage following desirable;storage following;underlying storage;storage;encoding construct codes;explicit erasure", "pdf_keywords": "sparse storage codes;codes optimal storage;storage codes satisfy;codes distributed storage;storage systems codes;sparse codes distributed;storage codes;existence sparse codes;explicit sparse storage;sparse codes;sparse storage;storage regenerating code;optimal regenerating codes;regenerating codes distributed;storage codes called;erasure codes;erasure code construction;erasure codes specifically;erasure codes application;systematic erasure codes;matrix product codes;product matrix codes;minimum storage repair;matrix matrix codes;sparse code;proposed codes optimal;sparse sparse code;matrix codes;code sparse consider;code sparse sparse"}, "92fc770721e95249f8db01c5019d1cc4cf79ff00": {"ta_keywords": "language query proposal;natural language query;query proposal selection;query language implementation;query language;query language sentence;command query language;query proposal;able query language;specification command query;language query;natural language;command query;language implementation;proposal selection;hubble space telescope;specification macro procedure;selection process hubble;proposal selection process;space telescope;designed able query;specification macro;telescope;language sentence variety;space telescope designed;telescope designed;including specification macro;query;specification command;telescope designed able", "pdf_keywords": ""}, "6c5144872c259611dceb32fe4e4486a6865e6c42": {"ta_keywords": "robust speech recognition;speech recognition proposed;method robust speech;speech recognition;speech recognition accuracy;robust speech;speech recognition based;step speech recognition;improves speech recognition;kalman filter;extended kalman filter;kalman filter ed;based extended kalman;bias component residual;bias residual component;stationary noise sequences;residual component decomposition;extended kalman;method improves speech;recognition based bias;noise sequences;based bias residual;noise sequences important;estimates bias component;bias residual;recognition proposed method;component decomposition proposed;algorithm sequentially estimates;improves speech;accuracy compared noise", "pdf_keywords": ""}, "cf6352c789ab51320fa7ca9b1440c685b57fd769": {"ta_keywords": "speaker diarization challenges;speaker diarization;task speaker diarization;xmath0 challenge diarization;challenge diarization challenge;diarization challenges;challenge diarization;diarization challenge new;diarization challenge;diarization challenges lessons;diarization domain processing;diarization upcoming jhu;new approach diarization;jhu xmath0 challenge;diarization;diarization upcoming;xmath0 challenge present;diarization domain;xmath0 challenge;approach diarization;approach diarization upcoming;task speaker;jhu xmath0;round xmath0 challenge;perform task speaker;upcoming jhu xmath0;combination diarization domain;combination diarization;use combination diarization;xmath0", "pdf_keywords": ""}, "2e1a1588955a8a64ec618b3cc04be961ed0cb59c": {"ta_keywords": "state conjugated polymers;conjugated polymers approach;conjugated polymers;energies poly hexylthiopnene;state energies poly;excited state conjugated;conjugated oligomers demonstrate;electronic excitations;electronic excitations excited;excited state energies;energies poly;oligomers demonstrate transferability;underlying electronic excitations;conjugated oligomers;polymers approach relies;polymers approach;long conjugated oligomers;excitations excited state;excitations long conjugated;polymers;p3ht single crystal;electronic excitations long;poly hexylthiopnene p3ht;state energies;excitations excited;transfer learning;lying electronic excitations;excitations;transfer learning models;using transfer learning", "pdf_keywords": ""}, "9237d6efc603465765e80eb5ca1268c2bd7b5c24": {"ta_keywords": "machine translation;machine translation tool;language generation tasks;translation tool;tasks machine translation;language generation;translation tool implements;systems language generation;linguistic labels source;linguistic labels;use linguistic labels;linguistic;use linguistic;generation tasks machine;generation tasks;comparison log likelihoods;features use linguistic;language;translation;tool holistic analysis;results systems language;source data comparison;comparison results systems;log likelihoods probabilistic;holistic analysis comparison;probabilistic models;systems language;mt tool holistic;likelihoods probabilistic models;tool holistic", "pdf_keywords": "comparing phrase neural;translation accuracy linguistic;word translation accuracy;machine translation designed;tool machine translation;machine translation tool;entire corpus compare;tools machine translation;corpus compare;corpus compare mt;describing translation accuracy;evaluation machine translation;gram analysis compare;machine translation;capable describing translation;machine translation mt;machine translation language;comparing sentences based;neural machine translation;machine translation methodology;machine translation output;evaluation translation quality;word sentence accuracy;comparing sentences;machine translation data;results comparing sentences;translation quality results;translation language generation;results machine translation;performance word processing"}, "a6336fa1bcdeb7c84d2c4189728f0c1b2b7d0883": {"ta_keywords": "sequence learning;sequence learning present;networks sequence learning;neural networks sequence;recent advances neural;translation handwriting recognition;neural networks tasks;advances neural networks;translation handwriting;advances neural;image captioning language;language translation handwriting;powerful neural networks;neural networks;captioning language;handwriting recognition;tasks image captioning;captioning language translation;discovery powerful neural;learning present synthesis;handwriting;captioning;image captioning;neural;powerful neural;recognition;networks sequence;learning;networks tasks image;learning present", "pdf_keywords": "recurrent neural networks;computers simple recurrent;recurrent neural network;advances recurrent neural;neural networks rnns;turing machine neural;neural turing machine;machine extends recurrent;networks rnns;neural networks memory;training recurrent neural;extends recurrent neural;accuracy recurrent neural;time machine learningwe;time machine learning;neural turing;advances recurrent;utility recurrent neural;training recurrent;recurrent neural;overview recurrent neural;language using recurrent;train recurrent neural;using recurrent neural;used recurrent neural;non neural machine;neural machine;neural networks highly;simple recurrent neural;recent advances recurrent"}, "7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7": {"ta_keywords": "large text corpus;text corpus single;text corpus;corpus single model;semiparametric representation large;corpus;semiparametric representation;corpus single;representation large text;integrate semiparametric representation;semiparametric;integrate semiparametric;method integrate semiparametric;large text;text;representation large;propose novel method;representation;novel method integrate;single model;novel;model;propose novel;novel method;integrate;large;method integrate;single;method;propose", "pdf_keywords": "entity mentions corpus;describing mentions entities;predicting mentions entities;dimensional mention encoders;mentions corpus;capable predicting mentions;entity mention encoder;mentions entities;predicting mentions documents;entity mentions;mentions corpus uses;encodings entity mentions;describing mentions;predicting mentions;learned mentions;mentions entity;neural language model;mentions entities form;encode knowledge;learned mentions demonstrated;previously learned mentions;entity mention;mentions single neural;learn informative mentionswe;encode knowledge mentioned;unstructured entity vocabulary;remember mentions entity;entity embedding retrieval;retrieve mentions entity;mentions entities arbitraryrecent"}, "8be39979cb2eb1aeaba15b57e1e4bc712eb962cb": {"ta_keywords": "based paragraph reconstruction;paragraph reconstruction;paragraph reconstruction propose;paragraph reconstruction objective;paragraph embedding;paragraph embedding method;paragraph embedding powerful;tool paragraph reconstruction;input paragraph;occurs input paragraph;state art paragraph;art paragraph embedding;paragraph;method based paragraph;powerful tool paragraph;based paragraph;tool paragraph;art paragraph;sentence occurs input;embedding powerful tool;embedding;embedding method proposed;sentence occurs;embedding method;given sentence occurs;embedding method simple;sentence;embedding powerful;reconstruction objective;reconstruction propose novel", "pdf_keywords": "downstream paragraph classification;encoder downstream paragraph;paragraph embeddings remarkably;captured paragraph embedding;paragraph embeddings;paragraph classification tasks;paragraph embedding;paragraph classification;paragraph embedding model;accurately embed sentences;standard paragraph classification;sentence representations;embed sentences;sentence representations constraints;decoder sentence content;sentence content probe;paragraph embedding method;embed sentences dense;embed paragraph;learning representations sentences;embedding sentence structures;paragraph embedding despite;objective embed paragraph;deconvolutional decoder sentence;paragraph sentence content;structure sentence representations;sentence level probe;downstream paragraph;embed paragraph input;contextualize word representations"}, "f9ee690d223beac6d893aedae13c09dbf0fb694e": {"ta_keywords": "maximum entropy clustering;entropy clustering solution;entropy clustering;clustering methods including;clustering methods;estimation variance clustering;clustering solution;clustering solution using;clustering solution results;clustering;maximum entropy estimation;clustering results method;clustering results;variance clustering solution;variance clustering;estimation maximum entropy;clustering evaluated using;clustering evaluated;entropy estimation;variety clustering methods;variety clustering;guiding maximum likelihood;compared variety clustering;entropy estimation variance;condition process clustering;pairwise constraints method;using maximum entropy;maximum entropy;maximum likelihood estimation;discovery pairwise consistency", "pdf_keywords": ""}, "7354b87a1b4c99ccd9cf25b7314927ced8b156f7": {"ta_keywords": "reorder texts generative;intent assisted writing;texts generative;assisted writing assistant;texts generative dataset;assisted writing;writing assistant generates;generates rephrases text;rephrases text pretrained;writing assistant;text pretrained language;reorder texts;author preference generative;generates rephrases;writing;assistant generates rephrases;means reorder texts;text pretrained;rephrases text;driven intent assisted;texts;pretrained language model;text;intent assisted;generative;demonstrates author preference;generative output accurate;generative dataset;generative output;available demonstrate generative", "pdf_keywords": "automated generation text;automated writing;automated writing present;research automated writing;interactive writing assistant;text generation based;interactive writing;grained author specifications;facilitate generation text;controlled text generation;automated controlled text;text generation;generation text;writing present automated;authoring assistant tools;text generation context;generation text text;text generation using;writing assistant generates;accuracy text generation;generates rephrases text;generated text design;present interactive writing;author specifications controlled;automated generation;novel generative text;generative text;language models controlled;generates rephrases;generated text"}, "0f395654e69cd2e063a6ef221fb66fb46e68cefd": {"ta_keywords": "classifying features directly;algorithm classifying features;classify features;classify features algorithm;classifying features;truthful classifiers;effective classifying features;novel algorithm classifying;characterization truthful classifiers;truthful classifiers shown;used classify features;algorithm classifying;classifiers;features algorithm based;classifying;effective classifying;classification;features algorithm;knowledge classifiers;classifiers used classify;classification does require;classify;features directly;available traditional classification;classification does;classifiers used;shown effective classifying;knowledge classifiers used;possible features present;traditional classification", "pdf_keywords": "strategy classifier;classifiers presence strategic;classifiers;optimal classifiers theorems;classes classifiers;optimal classifier;strategy classifier best;optimal classifiers;existence optimal classifiers;classifier existence beneficial;classifier presence strategic;classifier;strategic machine learning;classifier propose;classifiers theorems;beneficial classifier;consider classifier;classifiers stated;identifying classifier beneficial;beneficial classifier existence;classes classifiers theorems;classifier beneficial;classifiers corresponding;classifier beneficial given;classifier proposed;classes classifiers fundamental;given classifier particularly;classifiers theorems existence;based classifier;classifier choice"}, "60dd53fca1f538fabe18e4d6a9326b2f40e358dd": {"ta_keywords": "implementation variational latent;scalable implementation variational;implementation variational;variational latent;variational;present scalable implementation;scalable implementation;present scalable;latent;scalable;implementation;present", "pdf_keywords": ""}, "084ddb77fce5a7f0b6418ef4e38dbb1bedf4ae78": {"ta_keywords": "electrollarynx maintaining listenability;simulation electrollarynx control;electrollarynx control based;evaluation simulation electrollarynx;electrollarynx control;simulation electrollarynx;speaker evaluation simulation;inter speaker evaluation;improvements naturalness electrollarynx;inter speaker;naturalness electrollarynx maintaining;naturalness electrollarynx;listenability intelligibility experimental;electrollarynx maintaining;speaker evaluation;electrollarynx;listenability intelligibility;speaker;maintaining listenability intelligibility;naturalness listenability intelligibility;listenability;control based statistical;statistical f0 prediction;maintaining listenability;intelligibility experimental;f0 prediction presented;naturalness listenability;based statistical f0;intelligibility experimental results;terms naturalness listenability", "pdf_keywords": ""}, "5b2c5eeea9ac8f26908b9dfc8fd0a2d0e7aa5bb1": {"ta_keywords": "multichannel robust speech;memory adaptive beamforming;robust speech recognition;robust speech;adaptive beamforming;adaptive beamforming proposed;acoustic model deep;memory acoustic model;memory acoustic;speech recognition;term memory acoustic;real time beamforming;baseline systems beamforming;memory recurrent neural;novel multichannel robust;long term memory;source microphones;short term memory;systems beamforming;beamforming filter;multichannel robust;speech recognition using;recurrent neural network;beamforming;time beamforming filter;beamforming proposed;term memory adaptive;beamforming filter coefficients;memory recurrent;recurrent neural", "pdf_keywords": "adaptive beamformer deep;beamforming network deep;training adaptive beamforming;learning acoustic model;reinforcement learning acoustic;learning acoustic;acoustic model network;deep lds adaptive;deep lds acoustic;acoustic model snn;speech processing adaptively;speech enhancement module;acoustic model learned;adaptive beamforming network;integration speech enhancement;speech enhancement;acoustic model speech;model speech recognition;model speech;beamformer acoustic model;deep learning;network deep lds;trained using deep;deep network;network deep reinforcement;speech recognition tasks;deep network long;network deep;deep reinforcement learning;adaptive beamforming"}, "d38b686b8b68d0b91b294fd8a55ac7dea191706f": {"ta_keywords": "extensible guided summarization;guided summarization framework;guided summarization;popular summarization datasets;summarization framework effectively;summarization datasets;performance popular summarization;summarization framework;summarization datasets using;popular summarization;sentences guidance guided;highlighted sentences guidance;summarization;sentences guidance;qualitatively different summaries;different summaries;summaries;generate faithful summaries;summaries demonstrate;using highlighted sentences;summaries demonstrate different;highlighted sentences;guidance generate qualitatively;external guidance;kinds external guidance;guidance generate;guidance guided;guidance guided model;external guidance input;guided model generate", "pdf_keywords": "neural summarization guidance;guided neural summarization;summarization neural networks;summarization neural;approaches summarization neural;neural summarization;neural abstractive summarization;summarization based guided;summarization guidance;generating accurate summaries;summarization guidance content;generating coherent summaries;model abstractive summarization;generated summaries;abstractive summarization powerful;summarization saliency powerful;predict best summaries;novel approach summarization;abstractive summarization able;summarization saliency;abstractive text summarization;summarization powerful tool;annotations inabstractive summarization;abstractive summarization framework;summarization framework;inabstractive summarization saliency;abstractive summarization;summarization powerful;sentence guided summary;text summarization"}, "e114618157e025ed17b7e45684d67becd34a14f3": {"ta_keywords": "total variation distance;distance component mixtures;variation distance component;variation distance;bounds total variation;mixtures form xmath0;total variation;component mixtures;component mixtures form;xmath1 xmath2 xmath3;mixtures form;variation;xmath2 xmath3 xmath4;xmath1 xmath2;xmath0 xmath1 xmath2;mixtures;distance component;xmath2 xmath3;xmath3 xmath4 xmath5;xmath3 xmath4;lower bounds total;xmath7 xmath8 xmath9;xmath8 xmath9 xmath10;xmath4 xmath5 xmath6;xmath5 xmath6 xmath7;xmath6 xmath7 xmath8;xmath10 xmath11 xmath12;xmath4 xmath5;xmath2;xmath7 xmath8", "pdf_keywords": "variational distance distributions;variation distance mixtures;dimensional mixtures theorems;distribution dimensional mixtures;mixtures theorems lower;mixtures theoremstheorems theorems;mixtures essential theorems;distribution learning hypothesis;mixtures component distributions;mixtures theorems;distance dimensional mixture;mixtures theoremstheorems;distance mixtures known;variation distance pairs;component distributions equally;component mixtures theoremstheorems;complexity distribution learning;lower bounds mixtures;theorem pair mixtures;dimensional mixtures essential;distributions close variational;high dimensional mixtures;distribution learning;distribution learning analytically;variation distance characteristic;high dimensional distributions;mixtures shared covariance;total variation distance;distance mixtures;close variational distance"}, "3ed91aae1038b8b0130fb3974060a50b10de1345": {"ta_keywords": "speech recognition asr;automatic speech recognition;field automatic speech;speech recognition;speech recognition distance;problem speech recognition;automatic speech;recognition asr;far field automatic;acoustic beamforming robust;recognition asr problem;end automatic speech;dereverberation acoustic beamforming;advanced machine learning;acoustic beamforming;recognition distance approach;asr problem speech;machine learning algorithms;application far field;machine learning;recognition distance;learning algorithms end;far field;learning algorithms;end dereverberation acoustic;dereverberation acoustic;beamforming robust end;application far;beamforming robust;recognition", "pdf_keywords": "speech recognition pipeline;recognition speech advanced;end speech recognition;speech recognition advanced;advanced speech systems;field speech recognition;advanced speech recognition;field speech processing;advanced speech processing;field automatic speech;speech recognizer;speech recognition ar;end speech recognizer;processing advanced speech;speech recognition;speech recognizer optimization;speech processing arpa;recognition ar speech;speech recognition described;speech recognition proposed;speech processing achieved;automatic recognition speech;speech recognition challenge;developments advanced speech;challengesrecent advances speech;automatic speech recognition;beamforming advanced speech;far field speech;developments automatic speech;recognition speech"}, "addd2d86d19c1e7c8854e827fb2656a50c250440": {"ta_keywords": "aspect based summaries;extracting aspect based;extracting aspect;method extracting aspect;based summaries;summaries given article;summaries;aspect based;based summaries given;novel method extracting;method extracting;aspect;summaries given;extracting;article using method;method wikiasp_;given article using;article using;given article;using method wikiasp_;method wikiasp_ use;wikiasp_;propose novel method;wikiasp_ use;article;wikiasp_ use method;method generate;novel method;novel;use method generate", "pdf_keywords": "aspect summarization dataset;aspect discovery summarization;aspect based summarization;aspect summarization;faster aspect summarization;aspect summarization process;aspect annotation propose;aspect annotation;multi aspect summarization;aspect classification;domain aspect classification;aspect dataset learned;aspect discovery;aspect classification method;summarize relevant aspects;aspect dataset;summarization dataset;discover aspects articles;aspects extracted;aspects sentences model;summarization task generating;aspect discovery process;summarization dataset derived;novel aspect discovery;dataset aspect based;feature aspect dataset;describing summarization;summarization process aspect;generating focused summaries;classes aspect discovery"}, "0c3c4c88c7b07596221ac640c7b7102686e3eae3": {"ta_keywords": "statins np search;search new statins;new drugs search;proton interaction np;search new drugs;drugs search;interaction np search;drugs search new;new statins np;introduce np search;proton interaction xmath1;particular proton interaction;interaction xmath1 proton;np search;xmath1 proton targets;proton targets present;statins np;xmath0 proton interaction;proton targets;interaction proton;np search new;interaction proton interaction;proton interaction;based particular proton;xmath0 proton;new statins;np search performed;xmath1 proton;particular proton;statins", "pdf_keywords": "biomedical question answering;inference nlp task;machines comprehension biomedical;comprehension biomedical text;question answering;learning task annotated;natural language inference;task outperforms nlp;nlp task;task deep knowledge;inference nlp;nlp task set;question machine learning;learning task annotators;comprehension biomedical;pqa annotation corpus;machine beneficial comprehension;answer question machine;method pretraining corpus;biomedical np hard;annotation corpus;language inference nlp;deep knowledge extraction;answering qa dataset;questionwe propose supervised;pretraining corpus;knowledge extraction;answer natural language;knowledge extraction knowledge;extraction knowledge"}, "b7731a9b9142a6deb132e99bc55ddbe458a537a6": {"ta_keywords": "estimation functional causal;estimation causal;asymptotic regret able;estimation causal effects;regret able estimate;asymptotic regret;causal effect deciding;models used causal;zero asymptotic regret;causal;functional causal;modern neuroscience models;neuroscience models;functional causal effect;causal effect propose;neuroscience models used;causal effects;causal effect;used causal;used causal effect;estimate functional efficiently;modern neuroscience;neuroscience;causal effects central;problem modern neuroscience;regret able;effect deciding time;regret;estimation functional;effect deciding", "pdf_keywords": "estimating causal models;estimating causal;estimating ate causal;causal models encoded;online moment selection;causal models heterogeneous;problem estimating causal;ate causal models;causal models;moment selection;parameter causal model;driven selection;target parameter causal;graphical models empirical;data driven selection;parameter causal;models empirical;causal models compare;dynamics qmd estimator;semi empirical methods;causal model method;family causal models;efficiently estimate target;driven selection policy;qmd estimator;causal model;qmd estimator data;empirical methods;adaptive selection data;estimator data driven"}, "3ea5468e6d3007a94d4318932d7778693526145c": {"ta_keywords": "grid computing proposed;grid computing using;grid computing;dynamic resource management;area grid computing;delay compensator;using delay compensator;delay compensator called;wide area grid;dynamic resource;resource management mechanism;transient state performance;resource management;area grid;using delay;grid;computing using delay;feature dynamic resource;paper dynamic resource;performance wide area;delay;state transient state;state performance wide;state transient;steady state transient;compensator called;compensator;state performance;transient state;dynamic", "pdf_keywords": ""}, "97906df07855b029b7aae7c2a1c6c5e8df1d531c": {"ta_keywords": "linguistic information network;description linguistic information;linguistic information;disambiguating information higher;disambiguating information;description linguistic;model description linguistic;qualitative information network;linguistic;information higher level;information network;disambiguating;information network model;capture qualitative information;decisions basis disambiguating;basis disambiguating information;quantify accuracy pipeline;qualitative information;information higher;network use quantify;accuracy pipeline model;accuracy pipeline;higher level representations;pipeline;network model;pipeline model;information network use;level representations;features pipeline;information", "pdf_keywords": "language encoder downstream;based bert encoder;model bert network;bert encoder;layers bert network;encoder downstream tasks;tasks encoded syntactic;language encoder relevant;layer bert;characterizing contextual tasks;layers bert;language encoder;semantic coreference encoded;bert base models;encoded syntactic semantic;bert network;coreference tasks encoded;nonlinear linguistic tasks;bert encoder set;network linguistic;linguistic tasks;semantic coreference tasks;linguistic information encoded;sentence network;bert network network;framework based bert;model characterizing contextual;structure language encoder;aspects language encoder;language encoder combination"}, "e31a3f52890dcb68f596020e45f8c9718b700466": {"ta_keywords": "spin orbit interaction;dynamics spin orbit;magnetic field interaction;dynamics spin;interaction presence magnetic;study dynamics spin;spin orbit;orbit interaction;orbit interaction presence;field interaction attractive;interaction attractive repulsive;field interaction;presence magnetic;field strength interaction;presence magnetic field;magnetic field;spin;magnetic;interaction attractive;dynamics;strength interaction attractive;repulsive depending value;strength interaction;attractive repulsive depending;interaction;orbit;repulsive depending;attractive repulsive;interaction presence;repulsive", "pdf_keywords": ""}, "77c98b45c95121fc2a3d2ab4906fc00364cf381c": {"ta_keywords": "separation level systems;systems level separation;single level separation;level separation achieved;level separation;based separation level;separation level generated;separation level;separation achieved application;applied separation level;method based separation;level systems method;method separation level;method applied separation;separation achieved;applied separation;based separation;new method separation;method separation;level systems level;level systems;interaction level single;level generated interaction;separation;level single level;single level;systems level;level single;interaction level;generated interaction level", "pdf_keywords": ""}, "11a28f9e6fb6581d0a01428dd27a3fb649454395": {"ta_keywords": "differences chymotrypsin andtrypsin;chymotrypsin andtrypsin trypsin;following differences chymotrypsin;chymotrypsin andtrypsin;specific substrates chymotrypsin;differences chymotrypsin;substrates chymotrypsin;chymotrypsin activated substrates;substrates chymotrypsin activated;chymotrypsin;trypsin super proton;chymotrypsin activated;andtrypsin trypsin super;andtrypsin trypsin;atrypsin trypsin special;atrypsin trypsin;trypsin special;trypsin special shape;andtrypsin;form atrypsin trypsin;shape called trypsin;trypsin converted;trypsin;trypsin converted active;trypsin super;called trypsin converted;atrypsin;enzyme tryptic hydrolysis;called trypsin;enzyme tryptic", "pdf_keywords": ""}, "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4": {"ta_keywords": "supervised high dimensional;quality image classification;image classification;learned visual concepts;image classification model;model supervised high;learned visual;high quality image;art supervised models;predict image;supervised high;state art supervised;scalable model supervised;classification model trained;ability predict image;visual concepts;art supervised;classification;supervised models;supervised;supervised models important;dimensional high quality;predict image goes;language learned visual;model supervised;quality image;natural language learned;visual concepts demonstrate;model trained;text images", "pdf_keywords": "learns transferable visual;predicting caption;able predict caption;pre trained imagenet;task predicting caption;nlp imagenet;predict caption image;zero shot supervised;trained imagenet;learning zero shot;predict caption;image supervision large;supervised zero shot;image classification learns;shot capable learning;predicting caption goes;learning describing;large scale supervised;shot supervised zero;training deep;training images supervised;images learns representations;learns supervised natural;reference learned visual;training natural language;transfer deep;representation captures;learning visual;nlp imagenet model;stream images learns"}, "de0e1f9980afa7949df64d53b8ae7a2f59c55579": {"ta_keywords": "style sentences fewshot;sentences fewshot methods;sentences fewshot;capture difference paraphrases;fewshot style transfer;target style sentences;method fewshot style;fewshot style;able capture stylistic;capture stylistic;capture style transfer;style sentences;capture stylistic difference;method fewshot;capture style;style transfer capture;style transfer attribute;difference paraphrases model;style transfer;new method fewshot;difference paraphrases;fewshot methods;fewshot methods widely;extract target style;paraphrases model;fewshot;fewshot methods perform;stylistic difference sentences;able capture style;paraphrases", "pdf_keywords": ""}, "1d79d055cf9711944f14e1388a9d054cbe81ddd0": {"ta_keywords": "discriminative language models;learning discriminative language;discriminative language;learning discriminative;semi supervised learning;supervised learning discriminative;semi supervised model;semi supervised;obtained semi supervised;work semi supervised;language models;discriminative;language models case;supervised learning;word text;supervised training examples;supervised model;supervised model gives;case word text;supervised;half supervised;substituting half supervised;examples obtained semi;word text use;half supervised training;supervised training;text;variant perceptron;variant perceptron algorithm;training examples obtained", "pdf_keywords": ""}, "8cfd299be05bf3df91e0bf656a7e2fb973056350": {"ta_keywords": "acoustic cross lingual;lingual transfer pairs;cross lingual transfer;lingual transfer;propose language similarity;language similarity;cross lingual;language family classification;classification speech synthesis;language similarity approach;pairs hundreds languages;speech synthesis tasks;speech synthesis;wide range languages;synthesis speech synthesis;lingual;synthesis speech;speech synthesis speech;hundreds languages approach;range languages;family classification speech;classification speech;hundreds languages;efficiently identify acoustic;language family;acoustic cross;languages used efficiently;identify acoustic cross;approach language family;range languages used", "pdf_keywords": "acoustic cross lingual;lingual speech recognition;cross lingual algorithms;language embeddings speech;acoustic classification language;cross lingual models;cross lingual speech;lingual transfer approach;measuring language similarity;reproduce language embeddings;multimodal speech encoder;lingual transfer;lingual algorithms;language embeddings;cross lingual transfer;languages scaling speech;lingual transfer promising;embeddings speech based;lingual models;lingual speech;features speech language;lingual transfer pairsin;language similarity;lingual algorithms shown;features speech processing;multimodal speech;speech encoder;speech data trained;compare features speech;embeddings speech"}, "3ba26e897d0085ecd8cb695e1728a083f9227447": {"ta_keywords": "statistical description complex;state statistical properties;description complex systems;observation statistical properties;state statistical;determined state statistical;motion particle random;particle random environment;random environment particle;statistical properties;statistical properties determined;statistical description;complex systems;approach statistical description;particle random;characterizing motion particle;complex systems approach;observation statistical;statistical properties compared;particle motion;based observation statistical;statistical;properties determined state;environment particle motion;motion particle;particle motion related;random environment;properties state approach;description complex;particle", "pdf_keywords": ""}, "8f6763b339363216794f48895b9381d1a7caa88c": {"ta_keywords": "multi agent learning;agent learning;agent learning respect;based multi agent;multi agent;robustness gradient;disturbances injected players;study robustness gradient;robustness gradient based;learning respect disturbances;existence subset players;gradient based multi;disturbance number players;subset players;subset players completely;players completely unaffected;injected players actions;robustness;players actions;study robustness;players actions provide;injected players;agent;gradient based;gradient;number players;learning;players;number players completely;injected disturbance subset", "pdf_keywords": "learning dynamics quadratic;dynamics quadratic game;quadratic games directed;learning dynamics player;quadratic games theorems;decoupling games quadratic;agent learning dynamics;multi agent learning;quadratic game equivalent;quadratic game defined;quadratic games;quadratic games game;prove learning dynamics;games quadratic games;quadratic game;players prove learning;class quadratic games;learning dynamics;quadratic game players;learning dynamics obtained;making quadratic games;games quadratic;consider learning dynamics;examples quadratic games;optimal players finite;horizon quadratic games;quadratic games used;examples games quadratic;equilibria continuous games;simultaneous play gradient"}, "51e5e7093e0183feab61b00ca6c3df61cd8c46de": {"ta_keywords": "gram language models;machine translation extracting;language modeling best;language models;language modeling;extracting phrase tables;training gram language;language models using;machine translation;phrase tables;extracting phrasal similar;phrase tables yielded;translation extracting;based extracting phrasal;methods language modeling;methods machine translation;gram language;translation extracting phrase;extracting phrasal;semi fully supervised;modeling best lists;extracting phrase;fully supervised methods;models using perceptron;supervised methods language;training gram;best lists hallucinated;phrasal similar methods;best lists;fully supervised", "pdf_keywords": ""}, "9d03a125a9568af8af3fae5091752017d6abe59e": {"ta_keywords": "transfer learning;used transfer learning;transfer learning setting;exploiting nonconventional regularities;nonconventional regularities data;methods exploiting nonconventional;exploit regularities improve;regularities data;exploit regularities;regularities data used;regularities improve performance;nonconventional regularities;possible exploit regularities;regularities improve;exploiting nonconventional;methods exploiting;existing methods discover;learning;data used transfer;learning setting possible;explore methods exploiting;methods discover new;methods discover;learning setting;transfer;regularities;discover new ones;existing methods;performance existing methods;used transfer", "pdf_keywords": ""}, "7cbb56da008163df09d254f85b7165f11389f298": {"ta_keywords": "asynchronous asynchronous protocol;asynchronous protocol asynchronous;asynchronous protocol;protocol asynchronous;protocol asynchronous protocol;argument comprehension asynchronous;asynchronous asynchronous asynchronous;comprehension asynchronous asynchronous;asynchronous asynchronous;comprehension asynchronous;asynchronous;asynchronous asynchronous regime;asynchronous regime asynchronous;regime asynchronous asynchronous;asynchronous regime;regime asynchronous;protocol;process argument comprehension;argument comprehension;process argument;comprehension;argument;computational;process;computational investigation;results computational investigation;investigation process argument;results computational;present;computational investigation process", "pdf_keywords": ""}, "3f16887a8e5c81aea554c7a266b08ea70dd2aa9a": {"ta_keywords": "distributed reinforcement;distributed reinforcement learning;reinforcement learning cooperative;uniformly distributed reinforcement;learning cooperative environments;learning cooperative;cooperative environments method;cooperative environments;qed solver;cooperative competitive behaviour;cooperative environment;cooperative environment including;reinforcement learning;qed solver able;state art qed;art qed solver;cooperative competitive;reinforcement;cooperative;kuramoto mokshin algorithm;cope cooperative competitive;mokshin algorithm;competing agents;type cooperative environment;mokshin algorithm variation;method uniformly distributed;distributed;applicable type cooperative;cope cooperative;based kuramoto mokshin", "pdf_keywords": "training cooperative decision;agents cooperative learning;reward selfish agents;reinforcement learning cooperation;method selfish agents;cooperative decision;trained combination selfish;training cooperative policies;cooperative decision making;learning cooperation;behaviour agents cooperative;cooperative learning;preferences cooperative environments;selfish agents environment;cooperative learning models;selfish agents cooperate;method training cooperative;algorithm training cooperative;behavior agents cooperative;resulting cooperative decision;selfish preferences cooperative;dilemma selfish agents;selfish agents modified;cooperative agents;selfish agents achieve;cooperative learning environment;exclusive learning reinforcement;preferences cooperative;learning cooperation improves;cooperative environment agents"}, "a6aed0c4e0f39a55edb407f492e41f178a62907f": {"ta_keywords": "text rich knowledge;generation text rich;generate text rich;automatically generate text;generation text;generate text;rich knowledge graph;text rich;human written text;use generate text;approach generation text;robotic approach generation;used generate text;text approach automatically;knowledge based intrinsic;knowledge graph;rich knowledge based;written text approach;written text;knowledge graph tg;text approach;robotic;knowledge graph gc;rich knowledge;text;automatically generate;knowledge based;knowledge;generate;human written", "pdf_keywords": "unstructured knowledge representations;knowledge representations unstructured;background knowledge graph;text representations biomedical;representation knowledge graphs;generate unstructured knowledge;representations biomedical entities;background knowledge graphs;knowledge based representations;attention contextual text;knowledge graphs representation;representations knowledge graphs;unstructured text representations;representations unstructured text;predicted related entities;knowledge graph;generate representations knowledge;text representations;knowledge representations;unstructured knowledge;similarity based entity;unstructured text generated;graph attention contextual;contextual text attention;knowledge form text;generating informative text;contextual text;link prediction enrich;based representation knowledge;entity representation"}, "5695847f8ffbb3da078842c3683ef74175eb59e5": {"ta_keywords": "japanese speech synthesis;speaker individuality synthetic;preserve speaker individuality;speaker individuality phonetic;speech prosody correction;speech synthesis method;synthetic speech prosody;prosody correction method;individuality synthetic speech;speech synthesis;method preserve speaker;synthetic speech;individuality phonetic sounds;preservation speaker individuality;individuality phonetic;read japanese speech;prosody correction;japanese speech;speaker individuality;preserve speaker;spectrum swapping consonants;phonetic sounds evaluated;swapping consonants;method incorrect phonetic;speakers various english;incorrect phonetic sounds;phonetic sounds based;speech prosody;preserves preservation speaker;phonetic sounds", "pdf_keywords": ""}, "37a0f28f6aa41028e64d0440001ff525d67c1305": {"ta_keywords": "multiagent resource allocation;assignment multiagent resource;allocation problems algorithm;assignment multiagent;resource allocation problem;constrained round robin;resource allocation problems;uniform assignment multiagent;multiagent resource;allocation problem;allocation problems;allocation original algorithm;resource allocation;allocation;multiagent;illustrated resource allocation;round robin crr;round robin;efficient uniform assignment;algorithm flexible efficient;flexible algorithm;algorithm flexible;flexible efficient algorithm;resulting allocation;robin crr algorithm;flexible algorithm efficient;comparison resulting allocation;present flexible algorithm;problems algorithm;algorithm achieves required", "pdf_keywords": "computing fair allocations;envy freeness allocation;able fair allocations;constraints allocation fair;fair allocations wide;fair allocation items;allocation fair allocation;fair allocations;allocation fair;new fair allocations;fair allocation;fair allocations interpolates;fair allocation utilities;fairness fair allocation;optimally reduce envy;allocation satisfying efficiency;algorithm computing fair;utilitarian maximal allocation;computing fair;fair allocations allwe;optimal allocation items;optimal allocation;reduce envy property;optimal allocation best;welfare fairness algorithm;scores fair allocation;possible allocation weighted;maximal allocation;allocation optimal;algorithms able allocations"}, "7bdb04ba2da682e4c0b19b5d61e999d648826edd": {"ta_keywords": "recovering sparse covariance;sparse covariance matrix;sparse covariance;rnn quadratic measurements;covariance matrix rnn;matrix rnn quadratic;recovering sparse;matrix rnn;problem recovering sparse;rnn quadratic;covariance matrix;quadratic measurements;additive noise simplifying;ln measurement vector;sparse;quadratic measurements yi;additive noise;measurement vector wi;measurement vector;noise simplifying;wi additive noise;ai ln measurement;algorithm recover arbitrarily;noise simplifying assumptions;covariance;matrix;passing algorithm recover;vector wi additive;rnn;zero components ck", "pdf_keywords": ""}, "d8c1eb86cc4546e4355ed368d8400d7640926cee": {"ta_keywords": "novel probabilistic clustering;probabilistic clustering method;probabilistic clustering;deterministic clustering;deterministic clustering algorithm;new deterministic clustering;clustering;clustering method;based nonparametric hierarchical;nonparametric hierarchical;clusters determined underlying;clustering algorithm;clustering method based;propose novel probabilistic;nonparametric hierarchical model;novel probabilistic;clusters determined;clustering algorithm based;probabilistic model data;discrete number clusters;clusters;probabilistic;results probabilistic;number clusters determined;probabilistic model;number clusters;combines probabilistic;probabilistic model results;asymptotics probabilistic model;asymptotics probabilistic", "pdf_keywords": "novel probabilistic clustering;clustering information;clustering information represented;clustering information concept;unsupervised clustering;probabilistic clustering;combines clustering information;advantages probabilistic clustering;probabilistic clustering discrete;multi view learning;multivariate clustering;novel multivariate clustering;unsupervised clustering model;clustering discrete learning;standard unsupervised clustering;clustering;empirical clustering;general clustering;probabilistic clustering algorithm;propose novel clustering;clustering information algorithm;novel clustering;concept clustering information;view learning combines;clustering structures;novel approach clustering;combines clustering;problem clustering information;clustering combines;clustering discrete"}, "fe0ec764813fbcb6b6fd77d82188e81826088103": {"ta_keywords": "discriminative objective functions;discriminative objective;view discriminative objective;discriminative;view discriminative;unified view discriminative;observations strings approach;probabilities observations strings;strings presented analysis;difference measure strings;measure strings presented;objective functions based;observations strings;measure strings;strings approach;objective functions;minimum error corresponding;weighting term sum;approach weighting term;strings approach illustrated;modified joint probabilities;common approaches minimum;strings presented;exponential difference measure;strings;joint probabilities observations;functions based negative;minimum error;approaches minimum;approaches minimum minimum", "pdf_keywords": ""}, "659b476a10b0e676a031b1b17ebfe405c1904227": {"ta_keywords": "speech processing toolkit;2020 speech processing;end speech processing;speech processing;advanced data augmentation;data augmentation;data augmentation conformer;processing toolkit incorporates;novel applications toolkit;processing toolkit;new applications toolkit;novel feature toolkit;processing toolkit end;feature toolkit;toolkit incorporates;development 2020 speech;toolkit;applications toolkit;toolkit state art;toolkit ability combine;applications toolkit designed;feature toolkit ability;toolkit ability;augmentation;toolkit designed used;end speech;toolkit designed;toolkit end;end end speech;augmentation conformer", "pdf_keywords": "automatic speech separation;end speech recognition;speech networks;translation speech enhancement;speech separation neural;end speech processing;speech separation based;speech processing toolkit;speech separation tasks;enhancement text speech;speech enhancement text;automatic speech;speech processing;speech networks recent;speech recognition;speech separation new;enhancement speech separation;end speech systems;machine learning speech;training end speech;network covers speech;speech processing tasks;approach speech separation;text speech project;beamforming speech separation;scale speech networks;end speech applications;text speech;speech enhancement recognition;range speech processing"}, "f02948f2976991bb76419775f303c27fc8afb7b5": {"ta_keywords": "rule learning algorithm;traxiition rule learning;rule learning;xmath0 rule learning;xmath1 rule learning;learning sets rules;rules based traxiition;learning algorithm based;learning algorithm;learning algorithm method;sets rules based;rules based;learning algorithm shown;method learning sets;based traxiition rule;learning sets;known traxiition rule;based xmath0 rule;based xmath1 rule;algorithm based;traxiition rule;sets rules;xmath0 rule;xmath1 rule;new method learning;algorithm based xmath0;method learning;algorithm method generalization;algorithm;generalization known traxiition", "pdf_keywords": ""}, "62a5b47def8d21825d06f7407a505ff0b64ecb1a": {"ta_keywords": "semantic parsing ambiguous;parsing ambiguous ungrammatical;semantic parsing;existing semantic parsing;parsing ambiguous;semantic parsing framework;parsing;parsing framework;method semantic parsing;ambiguous ungrammatical input;context free grammars;grammar takes ambiguous;free grammars scfg;parsing framework uses;takes ambiguous input;free grammars;grammars scfg jointly;ambiguous input;ambiguous ungrammatical;ambiguous input string;input sentence output;existing semantic;ungrammatical input;grammars scfg;build existing semantic;semantic;construct grammar;grammar;formalism construct grammar;input string jointly", "pdf_keywords": ""}, "05b0c768ecd4a82e486923e83250ddd53bacbf67": {"ta_keywords": "pruning algorithms provide;linear search tree;tree based pruning;cost pruning algorithms;pruning algorithms;tree based approximation;based pruning algorithm;computational cost pruning;pruning algorithm;pruning algorithm publicly;linear search;search tree based;non linear search;search tree;based pruning;pruning;cost pruning;tree based approaches;algorithms;algorithm;algorithms provide;algorithm publicly available;algorithm publicly;hybrid tree based;reducing computational cost;based approximation;search;based approximation piecewise;effective reducing computational;approximation evaluate approaches", "pdf_keywords": "distances compare pruning;distances demonstrate pruning;metric pruning;non metric pruning;approximation metric pruning;metric pruning rule;metric nearest neighbor;metric pruning problem;nearest neighbor search;distance computation search;pruning low dimensional;data effective pruning;effective pruning data;pruning algorithms;pruning data effective;generic pruning algorithms;nearest neighbor;pruning algorithms low;algorithm effective pruning;approximation pruning;reduced finding similarity;pruning rules provide;compare pruning rules;pruning data;spaces similarity search;prune low dimensional;compare pruning;nearest neighbors;pruning rules explicitly;metric nn search"}, "1d938731dfad31c09b2f58c365f630c640f2ca1a": {"ta_keywords": "training acoustic models;active learning al;active learning;active learning self;strongest active learning;acoustic models natural;self training acoustic;combines active learning;training acoustic;acoustic models;models natural language;learning al;momentum based memory;language processing np;learning;acoustic;framework self training;natural language processing;learning self training;self training baselines;learning self;natural language;language processing;novel dynamic momentum;learning al novel;training baselines 51;training baselines;models natural;dynamic momentum based;dynamic momentum", "pdf_keywords": "language models training;query annotation high;pretrained language models;labeling training;labeling training propose;annotation high uncertainty;trained language models;query annotation;accuracy pretrained language;trained language modelswe;label labeling training;supervised active learning;accuracy trained language;supervised text;weakly supervised text;annotation high;learning label;performance natural language;semi supervised active;sampling active learning;active learning accurately;learning semi supervised;pre trained language;active learning semi;region aware querying;demanding labeled data;querying labels;querying labels combat;improve label efficiency;improve label efficiencywe"}, "ae82f831bda5681edfe40ec15de4e9d2096ea92f": {"ta_keywords": "clustering words senses;clustering words;words senses;words senses proposes;lexical entailment approach;lexical entailment;results clustering words;infer entailment word;word results clustering;effects clustering words;words senses improved;problem lexical entailment;entailment word results;entailment word;lexical;vectors infer entailment;approach problem lexical;problem lexical;entailment approach combines;senses improved using;infer entailment;senses;entailment approach;algorithms problem lexical;senses proposes;clustering;entailment;senses proposes new;senses improved;context vectors approach", "pdf_keywords": "entailment clustering word;lexical entailment clustering;clustering word senses;clustering words;clustering words senses;word sense clusters;clustering word;words clusters;clusters occurrences words;clustering problem lexical;entailment clustering;words clusters compare;word senses clusters;occurrences words clusters;traditional entailment clustering;clusters probability words;entailment clustering able;effects clustering words;context tiered clustering;sense clusters occurrences;classify words context;infer lexical entailment;lexical entailment;infer lexical;lexical entailment using;classify words;lexical entailment context;word relatedness;clustering methods tiered;words method supervised"}, "8c9069641876d025c66ab6800939c278b07f60a3": {"ta_keywords": "document hierarchies based;hierarchies based document;document hierarchies inferred;document hierarchies;document document graph;hierarchy document hierarchies;model document hierarchies;hierarchies documents;document hierarchies resulting;document graph;document hierarchy;explicit hierarchies documents;document graph data;model document graph;hierarchies documents decomposing;generate hierarchy document;groups document hierarchies;document hierarchy groups;decomposing document hierarchy;hierarchy document;generate explicit hierarchies;hierarchies based;generate hierarchy;hierarchies resulting hierarchy;hierarchies inferred;hierarchy groups document;explicit hierarchies;hierarchies;based document document;used generate hierarchy", "pdf_keywords": ""}, "423044220d9642a2d5839cfb19e32171e8a16a83": {"ta_keywords": "bandit model stochastic;dynamics multiarmed bandit;bandit model;bandit setting proposed;reward models satiation;estimating reward models;estimating reward;algorithm estimates rewards;multiarmed bandit;multiarmed bandit setting;arm bandit model;bandit setting;reward models;greedy policy optimal;bandit;problem estimating reward;stochastic governed greedy;estimates rewards;payoffs arm bandit;policy optimal;estimates rewards arm;arm bandit;optimal policy cyclic;optimal policy;order greedy policy;policy optimal policy;greedy policy;governed greedy policy;models satiation dynamics;greedy policy propose", "pdf_keywords": "rebounding bandits framework;bandits model dynamics;bandits based stochastic;rebounding bandits;stochastic bandits;stochastic bandits model;present stochastic bandits;introduce rebounding bandits;rewards deterministic dynamics;networked bandits based;networked networked bandits;estimation rewards bandits;networked bandits;bandits framework;bandits model;propose bandits framework;bandits framework models;bandits biased feedback;bandits rewards arm;stochastic agent rewards;optimal policy pulls;bandits framework problem;dynamics optimal policy;rewards restless bandits;deterministic dynamics optimal;rewards bandits;rewards deterministic;bandits motivate notion;optimal strategy learning;exploration strategy theorems"}, "be0ad0710bfb09f6c875dd6cd834ac643713c93d": {"ta_keywords": "spin polarized fermions;polarized fermions spin;fermions spin orbit;fermions spin;spin orbit coupling;coupling spin polarized;zeeman field spin;orbit coupling spin;coupling spin;coupling increase spin;polarized fermions;spin polarized;enhance spin orbit;increase spin orbit;spin orbit;field spin orbit;effect zeeman field;enhance spin;increase spin;substantial increase spin;field spin;effect zeeman;orbit coupling increase;strength effect zeeman;spin;orbit coupling strength;fermions;orbit coupling;orbit coupling used;orbit coupling leading", "pdf_keywords": ""}, "4a0f96bb17836b4c4d6e19627f176fba8fe05127": {"ta_keywords": "circuit breaker;circuit circuit breaker;circuit breaker circuit;breaker circuit;circuit breaker combination;breaker circuit breaks;breaker combination limiting;breaker combination;breaker;limiting circuit circuit;limiting circuit relay;limiting circuit;limiting circuit combination;circuit breaks;circuit relay circuit;circuit relay;relay circuit circuit;circuit circuit;circuit combination limiting;circuit;relay circuit;circuit breaks energy;combination limiting circuit;value limiting circuit;circuit combination;breaks;relay;breaks energy incoming;limiting;breaks energy", "pdf_keywords": ""}, "1d5d170670889bd82364fbcc594dadcb5481e9e4": {"ta_keywords": "neural machine translation;machine translation nmt;translation nmt decoding;machine translation;translation examples incorporating;translation examples;source sentences translation;retrieved target sentences;translation nmt;engine retrieve sentence;retrieve sentence pairs;seen translation examples;examples incorporating neural;match source sentences;retrieve sentence;nmt decoding;previously seen translation;sentence pairs source;source sentences;sentence use search;sentences translation;specifically input sentence;nmt decoding process;target sentences aligned;words match source;target sentences;sentences aligned words;translation;incorporating neural;neural machine", "pdf_keywords": "neural machine translation;machine translation model;translation memory translation;machine translation nmt;machine translation;efficient translation translation;machine translation powerful;machine translation used;efficient translation;additionalneural machine translation;retrieved source sentences;method efficient translation;sentences translation pieces;memory translation;translation memory;machines parallel translation;machine translation novel;translation nmt decoding;translation systems translation;improves translation accuracy;translation model;translation process efficient;retrieved target sentences;improves significantly translation;translation systems;memory translation memory;method improves translation;retrieves translation;translation optimization;translation pieces search"}, "f32c67daa6a93281bd8645fc2fa423dca67aea00": {"ta_keywords": "assignment papers reviewers;reviewers conference peer;conference peer review;automated assignment papers;papers reviewers conference;papers reviewers;accuracy peer process;peer review;reviewers conference;automated assignment;accuracy peer;optimally fair statistical;near optimally fair;optimally fair;conference peer;assignment papers;peer process;analysis accuracy peer;fair statistical accuracy;peer review focus;fairness statistical accuracy;problem automated assignment;reviewers;review focus fairness;automated;minimax analysis accuracy;fairness statistical;peer process popular;consider problem automated;fair statistical", "pdf_keywords": "algorithm assigning reviewers;arbitrary assignment reviewers;assigning reviewers papers;papers assigning reviewers;assigning reviewers paper;assign reviewers papers;reviewers papers algorithm;assigns paper reviewers;reviewers papers assigning;reviewers paper algorithm;assignment papers reviewers;assignment reviewers papers;reviewers papers fair;automatically assign reviewers;assignment assign reviewers;peer review algorithm;review algorithm peer;iteratively assign reviewers;method assigning reviewers;assignment reviewers theorems;assigning reviewers;assigning reviewers given;fair assignments reviewers;reviewers papers peer;assign reviewers highest;automated assignment papers;fairness assignment papers;assigns reviewers based;assigns reviewers;assigns strong reviewers"}, "827e0def212f6834d615e4f3f25b55fe27f6460d": {"ta_keywords": "crowdsourcing annotation large;crowdsourcing annotation;annotation large datasets;approach crowdsourcing annotation;annotation large;annotations;annotation;scaling annotations;novel approach crowdsourcing;annotations multi step;annotations multi;relies scaling annotations;crowdsourcing;approach crowdsourcing;scaling annotations multi;sensitive verb relations;datasets context sensitive;large datasets context;context sensitive verb;verb relations approach;context sensitive;large datasets;verb relations;datasets context;datasets;context;verb;sensitive verb;scaling data accuracy;data accuracy measure", "pdf_keywords": ""}, "d8ad713ffde54d0a837e6a9cab4e70739d649d41": {"ta_keywords": "dialog retrieval approach;based dialog retrieval;dialog retrieval;example based dialog;based dialog;retrieval approach relies;dialog;retrieval approach;repeat search answer;distributed word representation;answer query database;relies distributed word;use recursive neural;repeat search;recursive neural network;word representation recursive;retrieval;representation recursive autoencoders;recursive autoencoders;search answer;recursive autoencoders use;allowed repeat search;recursive neural;distributed word;autoencoders use recursive;answer query;neural network identification;search;search answer evaluate;neural network", "pdf_keywords": ""}, "f45c777b29e0a00f7b1e1f33daa751853015724a": {"ta_keywords": "workshop artificial intelligence;machine learning held;artificial intelligence machine;intelligence machine learning;artificial intelligence;intelligence machine;machine learning;report workshop artificial;technology economics beijing;economics beijing;economics beijing china;activity report workshop;machine;china july 2016;learning;beijing;beijing china;beijing china july;report workshop;workshop artificial;june 2017 report;workshop;activity report;artificial;activity;china july;intelligence;2017 report;royal institute technology;economics", "pdf_keywords": ""}, "1f7fb2c16e51f207eb1816f4f3fc3e1649c364f0": {"ta_keywords": "robust speech recognition;simulation robust speech;proposed speech recognition;speech recognition presence;robust speech;speech recognition;recognition presence noise;speech recognition section;noise simulated noise;noise simulation;noise simulated;simulated noise levels;simulated noise;noise performance proposed;noise enhancement noise;noise simulation shows;noise enhancement;noise enhanced;noise performance;noise enhanced presence;presence noise enhancement;enhancement noise;performance proposed speech;presence noise simulated;enhancement noise discuss;enhanced presence noise;compared noise generated;generated noise simulation;simulation robust;simulation shows noise", "pdf_keywords": ""}, "83f648f01d858d02b20f9327bebb1d5e91d0b6a9": {"ta_keywords": "decoding network optimization;decoding network;maximum mutual information;mutual information;network optimization;conditional random fields;method decoding network;mutual information mmi;network optimization uses;random fields crfs;decoding;optimization uses conditional;random fields;accelerate convergence network;convergence network proposed;novel method decoding;maximum mutual;uses conditional random;linear classification process;conditional random;convergence network;training technique linear;network;information mmi training;method decoding;crfs accelerate convergence;optimization;linear classification;extends maximum mutual;network proposed", "pdf_keywords": ""}, "099346e2837c53ded931d98135edbb261039764a": {"ta_keywords": "compute social decision;computational social choice;computational social;problem computational social;ability compute social;decision makers social;social decision making;social decision;compute social;social choice combines;social choice;social choice proposed;social choice resulting;choice resulting algorithm;makers social choice;social;group decision makers;group decision;algorithm able compute;machine learning;computational;learning ability compute;decision makers;decision making simple;algorithm able;makers social;machine learning ability;algorithm;linear algebra;level linear algebra", "pdf_keywords": ""}, "cbdccaa4a5bceaae190f78b1ac0a0cf47391968d": {"ta_keywords": "media micro data;micro data created;development micro data;new media micro;micro data platform;creation new media;micro data;media micro;called micro data;media called micro;create new media;micro data demonstration;data platform creation;new media;development micro;data created development;data ability create;demonstrated development micro;new media called;meta data ability;generation meta data;new media demonstrated;meta data;media demonstrated development;platform creation new;data platform;new platform creation;data created;platform creation;micro", "pdf_keywords": ""}, "09fcc7ed1f867bcf9133ab12065ee7366cfaa652": {"ta_keywords": "recommendation models dlrms;learning based recommendation;tolerance deep learning;recommendation models;fault tolerance deep;erasure coding improve;erasure coding;based recommendation models;erasure coding enable;incorporates erasure coding;uses erasure coding;deep learning;demonstrate erasure coding;erasure coding used;fault tolerance;dlrms uses erasure;coding improve accuracy;fault tolerance architecture;deep learning based;based recommendation;present fault tolerance;homogeneous fault tolerance;uses erasure;coding improve;erasure;learning demonstrate erasure;recommendation;coding;coding used improve;open source fault", "pdf_keywords": "fault tolerance deep;replicate deep learning;storage checkpointing;checkpointing recovering stored;loss performance checkpointing;storage checkpointing introduce;fault tolerant machine;erasure coded checkpointing;tolerance deep learning;given storage checkpointing;fault tolerance asynchronous;checkpointing large scale;constant fault tolerance;fault tolerance training;fault tolerant;fault tolerance large;optimized checkpointing;optimized checkpointing recovering;ensure deep learning;challenges fault tolerance;systems checkpointing;techniques fault tolerance;approach fault tolerant;tolerant machine learning;fault tolerant training;fault tolerance framework;coded checkpointing based;replicating underlying storage;performance checkpointing;network optimized checkpointing"}, "8d939637b3a5ecf681130619cd35f295dbb9db03": {"ta_keywords": "risk venous thrombosis;rs710446 risk venous;nucleotide polymorphism;venous thrombosis vvt;venous thrombosis;single nucleotide polymorphism;nucleotide polymorphism rs2731672;thrombosis vvt sample;thrombosis vvt;risk venous;patients rs710446 allele;healthy patients rs710446;thrombosis;patients rs710446;effects single nucleotide;risk vt rs710446;polymorphism rs2731672 9898;rs710446 risk;9898 rs710446 risk;rs710446 allele associated;polymorphism rs2731672;allele associated increased;1110 healthy patients;venous;vt rs710446 allele;rs710446 allele unstable;allele associated;increased risk vt;polymorphism;sample 1110 healthy", "pdf_keywords": ""}, "0bcd8210e9b90b33ab8467b94fd9b9511aad0f86": {"ta_keywords": "processing distributed networking;ubiquitous multiprocessor distributed;distributed networking dmp;network ubiquitous parallelism;multiprocessor distributed processing;distributed processing architecture;distributed networking;speed distributed processing;multiprocessor distributed;distributed processing architectures;distributed processing distributed;distributed network;performance network architecture;scalable distributed network;distributed processing;ubiquitous multiprocessor;processing distributed;distributed network development;hybrid ubiquitous multiprocessor;architecture pervasive applications;networking dmp technology;network ubiquitous;applications based distributed;ubiquitous parallelism enable;efficient scalable distributed;network architecture;based distributed processing;best performance network;network architecture experience;networking dmp", "pdf_keywords": ""}, "cece2d2f7cc38a512325122401f8aa658121b80e": {"ta_keywords": "automatically detecting deception;detecting potentially deceptive;detecting deception;detecting deception computer;deceptive partner dialog;deceptive partner intelligent;discover deceptive partner;deception computer effective;potentially deceptive partners;potentially deceptive partner;deceptive partners;deceptive partner;discover deceptive;deceptive partners proposed;deception computer;deception;potentially deceptive;deceptive;used discover deceptive;partner dialog;automatically detecting;partner dialog proposed;effectively detecting potentially;partner intelligent way;dialog proposed method;detecting potentially;detecting;partner intelligent;dialog proposed;method automatically detecting", "pdf_keywords": ""}, "4bc9d6596069c9277b57a7ee1e1127d231f28663": {"ta_keywords": "unsupervised parsing improves;unsupervised parsing;unsupervised financial market;algorithm unsupervised parsing;improves thesupervised parsing;unsupervised financial;thesupervised parsing performance;thesupervised parsing;supervised autoencoder diora;inside supervised autoencoder;supervised autoencoder;deep inside supervised;parsing improves;financial market new;parsing improves state;financial market;autoencoder;parsing;autoencoder diora soft;art unsupervised financial;autoencoder diora;parsing performance;financial;parsing performance f1;new algorithm unsupervised;state art unsupervised;unsupervised;algorithm improves thesupervised;algorithm unsupervised;inside supervised", "pdf_keywords": ""}, "dbb4035111c12f4bce971bd4c8086e9d62c9eb97": {"ta_keywords": "multi view networks;view networks;learning multi view;graph based convolutional;convolutional network learning;network learning multi;view networks method;network learning;semi supervised learning;semi supervised;existing semi supervised;networks;convolutional network;learning multi;based convolutional network;mobile phone datasets;networks method;multi view;phone datasets;supervised learning;study global poverty;supervised;graph based;networks method outperforms;global poverty;supervised learning algorithms;develop graph;learning algorithms;learning;view", "pdf_keywords": "semi supervised graph;supervised graph learning;graph learning multi;graph learning;supervised learning graphs;supervised learning graph;learning graphs;graph predicting;learning graph;semi supervised learning;learning graphs combines;supervised graph;semi supervised;graph convolutional networks;graph predicting neighbors;graph convolutional network;approaches semi supervised;learning graphs enhanced;ranking graph convolutional;view graph convolutional;graph convolutional;learning graph improved;transductive semi supervised;gvec graph convolutional;approach semi supervised;dle graph predicting;classification graph;graph subspace analysis;embedding data graph;demonstrate semi supervised"}, "8376b18a4dd228ea4c33d606b32b081cee9bf80a": {"ta_keywords": "stochastic optimization;solving stochastic optimization;stochastic accelerated derivative;stochastic optimization problems;derivative free algorithm;propose stochastic accelerated;algorithm solving stochastic;stochastic gradient;stochastic gradient based;stochastic accelerated;based stochasticity complexity;accelerated derivative free;stochasticity complexity;algorithm based stochasticity;stochasticity complexity bound;free algorithm complexity;similar stochastic gradient;free algorithm;free algorithm solving;solving stochastic;derivative free;non accelerated derivative;problems based stochasticity;complexity bound xmath0;accelerated derivative;complexity bound xmath2;stochastic;optimization;gradient based algorithm;propose stochastic", "pdf_keywords": "optimization stochastic oracle;stochastic optimization convex;stochastic convex optimization;algorithm stochastic convex;free stochastic optimization;convex optimization stochastic;oracle stochastic optimization;function stochastic oracle;stochastic optimization powerful;stochastic optimization stochastically;stochastic optimization;stochastic stochastic convex;consider stochastic optimization;optimization stochastically;stochastic stochastic optimization;stochastic convex;function stochastic convex;optimization stochastic;resulting stochastic optimization;stochastic optimization problems;stochastic oracle;stochastically approximating gradient;stochastic oracle stochasticwe;algorithms stochastic smooth;convex function stochastic;consider stochastic convex;free convex optimization;optimization stochastically generated;solution stochastic optimization;problem stochastic optimization"}, "1e5b826ddf0754f6e93234ba1260bd939c255e7f": {"ta_keywords": "autoregressive machine translation;models knowledge distillation;knowledge distillation performance;translation quality based;machine translation;knowledge distillation reduce;translation quality;translation tnp models;knowledge distillation;data best translation;best translation quality;machine translation tnp;effect knowledge distillation;distillation reduce complexity;distillation performance non;distillation performance;non autoregressive machine;performance non autoregressive;autoregressive machine;complexity distilled data;distillation;models knowledge;tnp models knowledge;distilled data;distilled data best;optimal complexity distilled;translation tnp;complexity distilled;distillation reduce;model optimal complexity", "pdf_keywords": "autoregressive translation models;autoregressive machine translation;translation adversarial defense;translation translation adversarial;translation adversarial;machine translation models;models translation quality;machine translation model;performance adaptive translation;machine translation train;translation models;translation train models;autoregressive models trained;translation model strongly;adaptive translation;non autoregressive translation;neural machine translation;models translation;translation model improve;autoregressive translation;adaptive translation ar;automatic machine translation;generation models translation;translation model;translation models fixed;quality translation model;machine translation tasks;training using translation;machine translation;sequence generation models"}, "41d4763792db8ea420efcfbd112a55deec971fee": {"ta_keywords": "commonsense knowledge related;commonsense knowledge;related commonsense knowledge;knowledge commonsense knowledge;knowledge related commonsense;commonsense knowledge commonsense;commonsense knowledge simple;commonsense knowledge based;knowledge commonsense;individuals commonsense knowledge;commonsense knowledge large;relevance commonsense knowledge;relationship commonsense knowledge;related commonsense;commonsense;observation commonsense knowledge;individuals commonsense;define relationship commonsense;relationship commonsense;data relevance commonsense;relevance commonsense;based observation commonsense;knowledge simple empirical;observation commonsense;number individuals commonsense;knowledge related;knowledge;knowledge simple;knowledge large;knowledge large number", "pdf_keywords": ""}, "4cc97c3858b558b4fa80ad73a894fcc7df841114": {"ta_keywords": "characterization proc proc;characterization proc;diagnostic characterization proc;characterization proc acad;agnostic characterization proc;proc acad sci;proc proc acad;proc proc;novel diagnostic characterization;proc acad;diagnostic characterization;proc;diagnostic based known;diagnostic based;proposed diagnostic based;propose novel diagnostic;novel diagnostic;2011 proposed diagnostic;model agnostic characterization;proposed diagnostic;diagnostic;acad sci;characterization;known model agnostic;agnostic characterization;known model;acad sci usa;model agnostic;based known model;sci", "pdf_keywords": "group fairness definitions;group fairness notions;fairness notions performance;performance fairness fairness;classifiers based fairness;group fairness class;group fairness incompatibility;method characterizing fairness;fairness based representation;proposed fairness criterion;characterization group fairness;incompatibility based fairness;group fairness;models multiple fairness;defining fairness;identify fairness notions;fairness constraints model;group fairness notionswe;fairness incompatibility based;incompatibility group fairness;fairness optimality;fairness fairness notions;fairness definitions;fairness class fairness;extended group fairness;fairness definitions literature;fairness definitions quantify;defining fairness context;characterizing fairness;fairness notions constructed"}, "72d89aa7cd77c3f22a667f2b0707758eb8d52a7a": {"ta_keywords": "improve accuracy translation;new translation model;accuracy translation model;ambiguous words model;translation model;resolve ambiguous words;translation model explicitly;accuracy translation;disambiguate word;attention model context;introduce new translation;align attention model;attention model;words model;translation;disambiguate word uses;ambiguous words;context used disambiguate;disambiguate;align attention;words model uses;context resolve ambiguous;word uses guided;guided attention;used disambiguate word;used disambiguate;attention;guided attention strategy;uses guided attention;new translation", "pdf_keywords": "context aware translation;attention disambiguating context;human context translation;regularize attention disambiguating;translation context aware;aware context translation;translators model attention;attention disambiguating;context translation good;context professional translators;aware machine translation;aware translation models;translation context;aware translation accuracy;translation metrics targeted;conventional translation context;improves translation;context translation;machine translation models;translation extended context;improves accuracy translation;improves translation quality;machine translation;aware translation use;translation contrastive translations;context model attention;disambiguating context models;accuracy machine translation;translation models;aware translation"}, "d170bd486e4c0fe82601e322b0e9e0dde63ab299": {"ta_keywords": "neural language models;modeling extend softmax;neural language modeling;representations neural language;softmax;adaptive representations neural;extend softmax;extend softmax known;softmax known work;softmax known;train neural language;adaptive representations;representations neural;language models;class adaptive representations;neural language;language models high;representations benchmark;capacity proposed representations;language modeling extend;popular representations benchmark;language modeling;representations used train;train neural;representations;proposed representations;performance popular representations;proposed representations used;representations used;popular representations", "pdf_keywords": "adaptive word representations;neural language modeling;words adaptive softmax;word models adaptive;embedding words adaptive;adaptive input representations;attentional architecture models;train word models;words adaptive;model words;representations neural language;lattice model;word representations learning;layers model words;models adaptive input;adaptive softmax;formalism random matrix;adaptive word;model words characters;ground state entropy;extend adaptive softmax;neural language;input representations neural;self attentional architecture;softmax output words;language modeling;language modeling extend;word models;representations learning capacity;attentional architecture"}, "3d3f01feee0dd3eea22e390c80deaadc6f11eb9a": {"ta_keywords": "speech recognition chime;recognition chime challenge;chime challenge trained;chime corpus;recognition chime;results chime corpus;end speech recognition;chime challenge;chime corpus word;cnn based multichannel;chime;speech recognition;results chime;channel encoder;single channel encoder;convolutional neural;convolutional neural network;experimental results chime;network cnn;channel encoder uses;neural network cnn;presents convolutional neural;trained single channel;network cnn based;study presents convolutional;channel end end;cnn based;encoder;multichannel end;multichannel end end", "pdf_keywords": "end speech recognition;end multichannel speech;multichannel speech recognition;speech recognition fifth;speech recognition tasks;multichannel speech;end encoder attention;speech recognition task;speech recognition model;encoder attention model;cellular automaton speech;speech recognition;automaton speech recognition;attention model joint;recognition fifth chime;recognize speech signals;encoder attention;cnn accepts multichannel;attention model;speech signals;encoder multichannel;fifth chime task;attention model constructed;learning end end;track speech recognition;encoder multichannel input;end end speech;end speech;robustly recognize speech;rn2c based speech"}, "b2fd7297f7681f9e3ea860cecf1ec97b2cc8ccc3": {"ta_keywords": "oscillator harmonic trap;harmonic trap oscillator;harmonic trap results;harmonic trap;dimensional harmonic oscillator;harmonic oscillator;harmonic oscillator harmonic;oscillator harmonic;harmonic oscillator model;trap oscillator;oscillator excited harmonic;trap oscillator excited;predictions harmonic oscillator;dynamics dimensional harmonic;harmonic field frequency;amplitude harmonic;amplitude harmonic field;determined amplitude harmonic;excited harmonic field;frequency oscillation;excited harmonic;harmonic field results;harmonic field;dimensional harmonic;field frequency oscillation;oscillation determined amplitude;frequency oscillation determined;harmonic;oscillator model;compared dimensional harmonic", "pdf_keywords": ""}, "ff783c4709a095cc581534fec58ef9515613ebc9": {"ta_keywords": "paradigm called forgetting;forgetting right reason;forgetting;model explanation paradigm;explanation paradigm;explanation paradigm called;called forgetting;called forgetting right;forgetting right;powerful model explanation;model explanation;paradigm called;paradigm;explanation;simple powerful model;powerful model;model;propose simple powerful;right reason;simple powerful;reason;propose simple;simple;powerful;called;right;propose", "pdf_keywords": "catastrophic forgetting saliency;prevent catastrophic forgetting;catastrophic forgetting;loss continual learning;forgetting saliency based;phenomenon catastrophic forgetting;catastrophic forgetting phenomenon;continual learning;allows continual learning;forgetting;continual learner;continual learner based;postulate catastrophic forgetting;forgetting phenomenon;catastrophic forgetting postulate;continual learning sequence;hypothesize forgetting;continual learning evaluate;hypothesize forgetting mitigated;called forgetting;evaluation continual learner;paradigm called forgetting;forgetting mitigated;continual learner sequence;forgetting saliency;forgetting mitigated model;forgetting phenomenon able;continual learning process;forgetting right;observation hypothesize forgetting"}, "f2e7598464a0b9376771ffc4ba243233ee12c677": {"ta_keywords": "words based sememes;predicting meaning words;words experiments hownet;hownet sememe knowledge;words external context;predicting meaning;words experiment hownet;framework predicting meaning;context information words;sememe knowledge base;hownet sememe;information words experiments;based sememes;words based;character information words;words external;experiments hownet sememe;information words external;sememe knowledge;meaning words based;external context information;based sememes built;words experiments;sememes;context information;sememes built internal;knowledge base;information words;external context;low frequency words", "pdf_keywords": "lexical sememe prediction;predicting sememes word;predict meaning words;information predicting word;prediction character sememe;effective predicting word;predicting word structures;characters sememe prediction;prediction framework lexical;known subword prediction;subword prediction;sememe prediction character;subword prediction framework;predicting word;predict sememes context;predicting word occurrences;automatically predict meaning;sememes word embeddings;predict sememes content;sememes arbitrary word;prediction character;lexical sememe;sememe prediction generalization;manually annotated sememes;framework predicting sememes;learned sememes prediction;framework lexical sememe;words language sememes;sememes prediction accuracy;predicting sememes"}, "48b18bf5c9cad0e4c36b2d885f380c5c637e1a09": {"ta_keywords": "learning disentangled representations;disentangled representations based;learning disentangled;variational autoencoders;model learning disentangled;disentangled representations;variational autoencoders sudakov;based variational autoencoders;representations based variational;autoencoders sudakov generative;generative models constraints;generative models;autoencoders;based variational;models constraints prior;optimal conditional prior;regularization latent space;autoencoders sudakov;prior serves regularization;probabilistic model learning;generative proposed model;generative;disentangled;variational;generative proposed;constraints prior;regularization latent;experiments generative models;sudakov generative proposed;compact optimal conditional", "pdf_keywords": "unsupervised disentanglement learning;disentangle deep generative;disentangled representations generative;learning disentangled representations;learns disentangled representation;disentangled representations deep;disentanglement learning;learning disentangled;learn disentangled representations;disentanglement learning based;learns disentangled;supervised disentanglement;disentangled representations using;approach learning disentangled;disentangled representations based;deep generative;deep generative models;disentangled representations;method learning disentangled;variational autoencoders;unsupervised disentanglement;variational autoencoders identifiabile;learn disentangled;representations deep generative;generative models unsupervised;disentangled representation using;supervised disentanglement presence;learn underlying generative;disentangling latent representations;method supervised disentanglement"}, "ee5dc631a682696a4704b742ea087e8abb5df897": {"ta_keywords": "multimodal autonomous speech;self supervised multimodal;autonomous speech recognition;supervised multimodal autonomous;autonomous speech;supervised multimodal;speech recognition;self supervised;multimodal autonomous;multimodal;framework self supervised;speech recognition using;supervised unsupervised proposed;supervised unsupervised;supervised;recognition;gap supervised unsupervised;gap supervised;image processing pipeline;performance gap supervised;signal processing pipeline;recognition using combination;recognition using;speech;unsupervised proposed framework;pipeline state art;pipeline proposed framework;pipeline;image processing;processing pipeline", "pdf_keywords": ""}, "162c3cf78af48ddf826ec76a1a3767a88a730170": {"ta_keywords": "spin orbit interaction;orbit interaction spin;interaction spin orbit;orbit coupled spin;coupled spin orbit;spin orbit coupled;dynamics spin orbit;enhance spin orbit;enhancement spin orbit;effect spin orbit;interaction dynamics spin;spin orbit;suppression spin orbit;coupled spin;interaction spin;dynamics spin;orbit interaction dynamics;orbit interaction;orbit interaction responsible;orbit interaction used;enhance spin;effect spin;enhancement spin;suppression spin;orbit coupled;responsible enhancement spin;responsible suppression spin;spin;used enhance spin;study effect spin", "pdf_keywords": ""}, "c8a5d05cb741b3448ec4106d2006ae24a7a401b4": {"ta_keywords": "streaming automatic speech;speech recognition asr;sequence emission regularization;streaming asr models;streaming asr;automatic speech recognition;automatic speech;regularization directly sequence;speech recognition;recognition asr;latency regularization directly;latency regularization;regularization sequence level;recognition asr combining;applies latency regularization;variety streaming asr;rnn transducer;emission regularization sequence;regularization sequence;sequence probability prediction;latency streaming automatic;including rnn transducer;streaming automatic;sequence emission;latency streaming;asr combining sequence;combining sequence emission;asr models;directly sequence probability;regularization directly", "pdf_keywords": "transducer regularization;algorithm transducer regularization;regularized transducer loss;sequence based transducer;sequence transducer models;transducer regularization method;sequence transducer;fastemit sensor acoustic;regularized transducer;probability sequence transducer;streaming automatic speech;sequence level regularization;streaming based speech;new regularized transducer;speech recognition asr;words emitting fast;sensor acoustic;automatic speech;speech recognition;sensor acoustic acoustic;transducer loss structure;automatic speech recognition;transducer forward probabilities;transducer loss;algorithm transducer;acoustic sensors;transducer models;acoustic sensors sensors;transducers used predict;speech recognition model"}, "9918ea4b68e90e1257953b6f2665b2ce29f2bc8b": {"ta_keywords": "nonlinearity nonlinear beamforming;nonlinear beamforming nls;nonlinear beamforming;beamforming enhance nonlinear;ability nonlinear beamforming;end nonlinear beamforming;nonlinear beamforming enhance;nonlinear beamforming eds;beamforming nls challenge;nonlinear signal nonlinear;signal nonlinear systems;beamforming nls;signal nonlinear;nonlinear signal;enhance nonlinear signal;challenge presented nonlinearity;nonlinearity nonlinear;nonlinear;presented nonlinearity nonlinearity;enhance nonlinear;challenges end nonlinear;nonlinearity nonlinearity nonlinear;presented nonlinearity;beamforming;nonlinearity nonlinearity;nonlinearity;beamforming eds challenge;nonlinear systems;end nonlinear;demonstrate ability nonlinear", "pdf_keywords": "neural beamforming enhancement;channel neural beamforming;data nonlinear beamforming;beamforming nonlinear;based nonlinear beamforming;nonlinear signal processing;nonlinear beamforming;nonlinear beamforming nonlinear;nonlinear beamformer;use nonlinear beamforming;neural beamforming;nonlinear beamformer challenge;nonlinear beamforming approach;beamforming enhancement;beamforming enhancement iibu;beamforming dynamical nonlinearity;beamforming nonlinear complex;end nonlinear beamformer;pre filtering beamforming;nonlinear signal;parametric beamforming;parametric beamforming methods;non parametric beamforming;filtering beamforming;3d speech enhancement;conference nonlinear signal;beamforming approach based;msu configuration beamformer;beamforming approach;signal misalignment nonlinear"}, "30109a213aa10765486c676ecfa511db227ab543": {"ta_keywords": "neural machine translation;machine translation nmt;translation nmt models;mini batch;mini batch creation;batch creation efficiency;machine translation;batch creation crucial;translation nmt;choice mini batch;batch creation large;effect mini batch;batch;batch creation;training neural machine;nmt models;training process length;neural machine;training neural;nmt;step training neural;nmt models paper;efficiency training process;process choice mini;compared simple shuffling;efficiency training;translation;creation efficiency training;neural;sorting strategies", "pdf_keywords": "neural machine translation;sentence mini batching;machine translation nmt;machine translation machine;translation machine learning;machine translation;translation machine;mini batching;mini batch;sorting training corpus;translation nmt models;mini batch creation;various mini batch;mini batching sorting;mini batch size;batching sorting training;training corpus;toolkit mini batch;translation tasks;mini batching standard;sorting sentences corpus;machine knowledge translation;additionally mini batch;translation tasks length;batch size word;translation predict;knowledge translation predict;sentence mini;sorting sentences;neural networks nmt"}, "5e6acc5c73f22c2dbbb4910f656a03cf40a2fe15": {"ta_keywords": "dynamics interacting particles;interacting particles interaction;interactions dynamics interacting;interaction strength interaction;interactions dynamics;local interactions dynamics;dynamics interacting;interacting particles;interaction strength;particles interaction;particles interaction used;strength interaction interaction;local interactions;non local interactions;interactions;control strength interaction;strength interaction controlled;interacting;interaction interaction;strength interaction;interaction controlled;strength interaction strength;interaction;interaction interaction used;interaction controlled choice;interaction used;interaction used control;dynamics;particles;controlled choice interaction", "pdf_keywords": ""}, "03e62d5f0265608c6ebdebba0870131b056b79a6": {"ta_keywords": "severity harmful online;harmful online content;harmful online;severity harm;severity harm understood;severity severity harm;harm understood perspectives;severity harmful;harm understood;harm;harm physical;harm physical relational;severity;severity severity;types severity severity;theoretical framework severity;harmful;online content;vulnerability;types harm;types severity;identified types harm;framework severity harmful;scale urgency vulnerability;framework severity;online content based;financial types severity;types harm physical;urgency vulnerability;experience scale", "pdf_keywords": "harmful content online;harmful online content;harmful content interactions;harm online social;dealing harm online;harm online communities;severity harmful online;online harm use;managing harm online;experiences harm online;mitigating harm content;harmful online;online harms;harm online platforms;harm content moderation;severity online harms;online harm;online severity harmful;harmful content attempts;content mitigate harm;upsetting content harm;severity harm online;severity online harm;content harm;mitigating harmful content;harmful content;harm content;harm occur online;harmful content viewers;harm associated content"}, "33ce3cd897a3473973f338c154f3fe5c1175643c": {"ta_keywords": "training virtual knowledge;virtual knowledge bases;virtual knowledge;knowledge bases vkbs;domain question answering;question answering;question answering tasks;knowledge bases;virtual vbf trained;method training virtual;trained entirely text;training virtual;vbf trained entirely;vbf trained;language open domain;icatedicate virtual vbf;virtual vbf;answering tasks;outperforms prior vbf;language model opicatedicate;domain icatedicate virtual;answering tasks used;vbf methods open;based openicatedicate language;prior vbf;bases vkbs open;vkbs open domain;prior vbf methods;open domain icatedicate;language model", "pdf_keywords": "encode relational knowledge;virtual large knowledge;virtual knowledge base;pretrained relation encoder;open virtual knowledge;automatically learning relations;pretraining relation encoder;large knowledge base;encode relational;vl learn relations;knowledge representation extracted;knowledge embeddings;knowledge base vl;knowledge embeddings method;learning relations text;based knowledge embeddings;entity relation encoder;knowledge base vkb;virtual knowledge;structured knowledge representation;reasoning trained structured;relation encoder;representation knowledge entities;learning relations;entity natural language;method encode relational;query language;knowledge base;semi structured queries;relation embedding encodes"}, "b00bc4dcce60e7c631a23d60894e51001de1c630": {"ta_keywords": "vesicular stomatitis virus;stomatitis virus vv;stomatitis virus;vesicular stomatitis;identify vesicular stomatitis;virus vv protein;vv protein anti;pseudotype virus neutralizing;pseudotype virus;anti protein candidate;vv protein protein;suggests vv protein;protein anti protein;protein pair vv;protein suggests vv;anti protein protein;vv protein;pair vv protein;form vv protein;protein anti;anti protein;candidate identify vesicular;used pseudotype virus;virus neutralizing anti;identify vesicular;neutralizing anti protein;stomatitis;protein candidate identify;virus neutralizing;protein protein pair", "pdf_keywords": ""}, "2583e7e279e2969493c3290c8f300ab32da40bf9": {"ta_keywords": "lingual entity linking;entity linking xel;entity linking;cross lingual entity;lingual entity;linking xel languages;entity mentions;entity mentions knowledge;mentions knowledge base;linking xel;generation cross lingual;entity;knowledge base entries;linking;cross lingual;xel languages;xel languages relatively;disconnect entity mentions;knowledge base;mentions knowledge;lingual;model low resource;languages relatively easy;low resource;low resource scenarios;languages;resource;reduce disconnect entity;xel;base entries", "pdf_keywords": "lingual entity linking;crosslingual entity linking;crosslingual entity disambiguation;entity linking powerful;lingual entity disambiguation;disambiguation entity linking;entity linking;entity linking xel;generation crosslingual entity;entity linking low;lingual knowledge bases;entity linking used;entity linking essential;perform entity linking;entity disambiguation based;cross lingual entity;linking entities;entity disambiguation;linking entities low;disambiguation entity mentions;link entities;disambiguation entity;lingual entity;demonstrate entity linking;link linking entities;disambiguation significantly improved;entity disambiguation entity;entity perform disambiguation;linking entity source;entity source language"}, "0bb30ed3340d2c34fe9f37c5002929bc5f458c23": {"ta_keywords": "protein relation extraction;relation extraction tasks;relation extraction;ary relation biomedical;extracting ary relation;extracting attention neighbor;relation biomedical;relation biomedical literature;mechanism bert architecture;attention neighbor tokens;bert architecture improves;method extracting attention;extracting attention;protein relation;mechanism bert;bert architecture;novel method extracting;chemical protein relation;extracting ary;attenuatement mechanism bert;ary chemical protein;method extracting ary;extraction tasks;based extraction mechanism;biomedical literature combines;based extraction;ary relation;extraction tasks proposed;biomedical literature;bert", "pdf_keywords": "biomedical relation extraction;biomedical text mining;relation extraction tasks;predicting relation biomedical;predicting relation extraction;relation extraction task;graph bertthe attention;relation extraction feasible;relation extraction;common biomedical text;focus relation extraction;entities biological text;biomedical text;relation biomedical;relation biomedical object;relation extraction diseases;task relation extraction;information biomedical literature;genes biomedical literature;encoder neighbor attention;biomedical relation;attention mechanism bert;biomedical text studied;tasks relation extraction;search novel genomic;extract relations entities;biomedical knowledge manually;consider relation extraction;text mining powerful;extract relations"}, "9bbc8ca94810e8a21e4a6a55a5913c5b0b6c787f": {"ta_keywords": "speech transcription speaking;speech transcription;transcription speaking;line speech transcription;transcription speaking proposed;automatic transcript confidence;efficient line speech;transcript confidence filtering;automatic transcript;transcription;smaller utterances using;segmented smaller utterances;initial automatic transcript;transcript confidence;smaller utterances;utterances using;line speech;transcript;utterances;utterances using initial;speech;outperforms typing;speaking proposed method;confidence filtering helps;method outperforms typing;outperforms typing terms;correction efficiency demanding;typing;typing terms correction;confidence filtering", "pdf_keywords": ""}, "7e0570f498a5de4f2a861546d4e67ba208f71d12": {"ta_keywords": "speaker recognition benchmark;speaker recognition;introduce speaker recognition;recognition benchmark derived;recognition benchmark;introduce speaker;speaker;benchmark derived;benchmark;benchmark derived publicly;recognition;derived publicly available;publicly available;derived publicly;publicly;available;derived;introduce", "pdf_keywords": ""}, "1410f7d9470a24fb4055c6685c2dda758b9d995f": {"ta_keywords": "games evolve adversarial;evolve adversarial;agents games evolve;coevolving network game;dynamics agents games;games evolve;agents games described;behavior coevolving network;adversarial fashion dynamics;adversarial;evolve adversarial fashion;agents games;network game;games described conservation;coevolving network;flavor agents games;games described;algorithm predicts;algorithm predicts time;laws information theoretic;behavior coevolving;adversarial fashion provide;adversarial fashion;information theoretic flavor;time algorithm predicts;settings agents games;dynamics agents;average behavior coevolving;polynomial time;information theoretic", "pdf_keywords": "evolutionary dynamics games;dynamics game theory;evolutionary game theory;sum game evolution;evolutionary dynamics game;games prove dynamics;dynamics games recurrent;analysis evolutionary games;time evolving games;theory agents games;games equivalent dynamics;polymatrix games theorems;game evolution;games dynamics;dynamics polymatrix games;evolving games generalize;evolutionary games;game theory;sum games dynamics;polymatrix gamestheorems theorems;desirable game theoretic;symmetry evolutionary games;agents evolutionary dynamics;games theorems;time evolving game;time evolution game;game theoretic;game dynamics;learning agents time;games defined dynamics"}, "e8e62a80c7355bcf5dbc9fabafff4025e00cf540": {"ta_keywords": "induction classical linguistic;plausible lexicons model;statistically plausible lexicons;classical linguistic structures;linguistic structures;nonparametric model induction;linguistic structure;linguistic structure model;linguistic structures model;describing linguistic structure;lexicons model;plausible lexicons propose;classical linguistic;plausible lexicons;lexicons model used;equations describing linguistic;describing linguistic;linguistic;novel nonparametric model;introduce novel nonparametric;propose novel nonparametric;model induction classical;novel nonparametric;lexicons propose novel;lexicons propose;nonparametric model;lexicons;nonparametric;model induction;induction classical", "pdf_keywords": ""}, "4ef46d5daf6a7a9536e2ebe3c7aa2296bffcf43e": {"ta_keywords": "speech pos tagging;hmm unsupervised speech;unsupervised speech pos;unsupervised speech;infinite hmm;model hmm;model hmm unsupervised;process infinite hmm;chain model hmm;extension stochastic markov;pos tagging;hmm model;stochastic markov;markov;tagging;stochastic markov chain;infinite hmm ihmm;tagging use infinite;hmm unsupervised;speech pos;pos tagging use;hmm model pos;markov chain model;markov chain;infinite version hmm;version hmm model;clustering properties pos;tagging use;clustering;model intrinsic clustering", "pdf_keywords": ""}, "467b14cc8337dd7efe1d374f9a7feb90ae9d2c12": {"ta_keywords": "probabilities distribution;probabilities distribution distributions;probability distribution probabilities;sum probabilities distribution;distribution probabilities;distribution sum probabilities;distribution probabilities given;probability distribution method;calculation probability distribution;given probability distribution;probability distribution;probability distribution probability;probability distribution sum;distribution probability distribution;calculating probability distribution;distribution probability;distribution distributions method;probability distribution given;distribution given probability;sum probabilities;distribution distributions;method calculating probability;distribution method based;probabilities given probability;distribution method;based calculation probability;distributions method;distribution method applied;distribution given;distributions method applied", "pdf_keywords": ""}, "63a604942f1238e9678aebd697a2379981e9a20a": {"ta_keywords": "student feedback performance;use student feedback;student feedback significant;student feedback improve;student feedback;feedback improve student;student learning performance;effect student learning;finds student feedback;impact student feedback;inhibit student learning;inhibits student learning;student performance;student learning results;improve student performance;improve student;student learning;studies effect student;student performance finds;student learning quantify;inhibits student;performance finds student;factors inhibit student;effect student;inhibit student;use student;feedback improve;feedback performance;learning results study;feedback performance simple", "pdf_keywords": ""}, "db392858262b17aa9c8ff8659738f68fbf832ebe": {"ta_keywords": "arbitrary code programming;code programming language;generating arbitrary code;generate arbitrary code;abstract syntax tree;arbitrary code;program abstract syntax;code programming;programming language;programming language model;syntax tree;syntax tree decomposing;probability program abstract;program abstract;programming;abstract syntax;generate arbitrary;syntax;generating arbitrary;probability program;approach generating arbitrary;program;code;generate;language model;model generate arbitrary;new approach generating;conditional probabilities nodes;estimates probability program;generating", "pdf_keywords": ""}, "040a1abdbef2a0e087a586d719259c32c95bfc78": {"ta_keywords": "linear distributed computing;distributed computing;distributed computing approach;log linear distributed;based belief propagation;belief propagation;distributed;designing log linear;designing log;belief propagation particle;computing;linear distributed;approach based belief;log;log linear;problem designing log;reward model;computing approach;computing approach based;expected reward model;based belief;generate set actions;expected reward;reward;belief;generate set rules;actions yield desired;high expected reward;able choose actions;novel approach problem", "pdf_keywords": ""}, "e1a20480e4168d58deec743035b7ff02720672d7": {"ta_keywords": "speech recognition asr;dispersion models nls;speech recognition;automatic speech recognition;nonlinear dispersion models;automatic speech;hybrid attention connectionist;end automatic speech;connectionist temporal classification;attention connectionist temporal;vocabulary end end;dispersion models;recognition asr;recognition asr using;attention connectionist;using nonlinear dispersion;nl hybrid attention;vocabulary end;open vocabulary end;models nls;models nls proposed;nonlinear dispersion;character based nl;temporal classification ctcvd;asr using nonlinear;hybrid attention;temporal classification;known words score;dispersion;score known words", "pdf_keywords": ""}, "a13d9c8e5a2fc028ad597e2bd46a9c60aca0ede4": {"ta_keywords": "speech synthesis proposed;based speech synthesis;speech synthesis;synthetic speech;quality synthetic speech;expressive speech synthesis;prosody synthetic speech;synthetic speech target;speech synthesis order;synthetic speech means;guide prosody synthetic;user speech inputs;prosody synthetic;speech inputs hmm;hmm based speech;prosody modification;speech inputs;medium based hmm;prosody modification high;based speech;user speech;uses user speech;quality expressive speech;speech target speaker;synthesis;speech means prosody;based hmm based;users guide prosody;synthesis proposed method;synthesis proposed", "pdf_keywords": ""}, "d26a7a86013b3be57acc0f5df73393cab7c302d9": {"ta_keywords": "stochastic particle flows;stochastic particle methods;solving stochastic particle;solving stochastic;stochastic particle;shot stochastic particle;framework solving stochastic;solving fokker planck;particle flows computes;particle flows;particle methods solving;interventions shot stochastic;fokker planck equation;particle methods;stochastic;flows computes optimal;shot stochastic;solution diffusive systems;methods solving fokker;solving fokker;fokker planck;computes optimal interventions;diffusive systems framework;flows computes;optimal interventions shot;diffusive systems;optimal interventions;planck equation employed;planck equation;flows", "pdf_keywords": "stochastic controls markovian;markov chain smoothing;stochastic control framework;controls markovian stochastic;state stochastic control;stochastic markov chain;stochastic control;stochastic control diffusive;framework stochastic control;robust stochastic control;smoothing based stochastic;stochastic optimal control;controlling dynamics stochastic;stochastic control problem;efficient stochastic controls;stochastic dynamics flows;based stochastic markov;stochastic stochastic markov;stochastic markov processes;stochastic controls;markovian stochastic;stochastic flows approach;stochastic flows;dynamics stochastic flows;stochastic differential;stochastic markov;markovian models stochastic;markovian stochastic systems;markov chain approximation;stochastic optimal transport"}, "73c401e29cb83157bc6dfb33d5ce4364a7d2731b": {"ta_keywords": "approach shot generation;domain generative models;cross domain generative;shot generation;shot generation problem;domain generative;generative models train;shot problem generate;generative models;shot model;models train shot;domain transfer learned;train shot model;generative;features shot;shot model large;novel approach shot;features shot problem;generate realistic images;essential features shot;approach shot;model able capture;train shot;shot problem;transfer learned;realistic images previous;models train;model large domain;source domain transfer;generate realistic", "pdf_keywords": "shot image generation;learning adapt images;shot image distributions;transferring shot image;learns source model;domain learning adapt;target domain learning;trained source model;generation realistic images;shot image;approach shot image;learns correspondence source;source target adaptation;images diverse training;generating corresponding images;robust approach shot;model captures;networks gans;generated images diverse;images relate source;approach learns source;adapt adversarial;learns source;domain learning;data sets gans;corresponding images source;arbitrary image domains;adversarial networks generative;images underlying source;learns correspondence"}, "4702bfd200ceb6de126a60afb4db9da5c413476e": {"ta_keywords": "uncertainty decoding acoustic;decoding acoustic models;acoustic model decoding;decoding acoustic;accuracy acoustic model;uncertainty decoding;accuracy acoustic;uncertainties layers deep;acoustic models;improve accuracy acoustic;acoustic models framework;framework uncertainty decoding;acoustic model;acoustic score distribution;uncertainties layers;propagation observation uncertainties;model decoding;layers deep neural;observation uncertainties layers;deep neural network;knowledge acoustic;model decoding framework;deep neural;propagation unscented transform;knowledge acoustic score;acoustic score;simultaneous propagation unscented;acoustic;decoding;neural network", "pdf_keywords": ""}, "b946ce2c3405969bf615bedc623845b0d3d9b010": {"ta_keywords": "novel chunkwise attention;chunkwise attention;speech recognition asr;recognition asr attention;chunkwise attention mocha;speech recognition;asr attention attention;automatic speech;asr attention;recognition asr;automatic speech recognition;end automatic speech;chunk input sequence;chunk input;single chunk input;attention restricted single;restricted single chunk;attention mocha training;chunkwise;attention attention restricted;training inference;mocha training inference;attention attention;attention;novel chunkwise;decoder;class novel chunkwise;attention mocha;attentions;attention restricted", "pdf_keywords": "encoder short speech;end speech recognition;transformer decoder attentions;attentions decoder;decoder attentions;random access encoder;models attentions decoder;attentions decoder monotonic;encoders described novel;quantum information processing;speech sequence based;chunkwise attention;online decoding;encoders;encoder;access encoder;encoders described;decoding;access encoder short;decoder able achieve;encoder short;speech recognition systems;introducing online decoding;decoding method encoders;propose online decoding;online decoder end;speech recognition;novel online decoder;oscillators pnas encoders;oscillators nedos encoders"}, "3ad287cf3b17cb109bf991731d2c0dcf8b7db2b1": {"ta_keywords": "lingual morphological tagging;morphological tagging;morphological tagging improves;morphological tagging powerful;cross lingual morphological;lingual morphological;technique predicting syntactic;predicting syntactic;predicting syntactic traits;tagging improves information;tagging powerful technique;morphological;tagging;accurately generate tag;tagging improves;overlap languages;generate tag sets;overlap languages proposed;exactly overlap languages;tag sets;syntactic traits words;method cross lingual;syntactic traits;cross lingual;words language paper;random fields neural;tagging powerful;generate tag;language paper proposes;words language", "pdf_keywords": "lingual morphological tagging;predicting morphological tags;model morphological tagging;morphological tagging efficient;morphological tagging crucial;morphological tagging significantly;morphological tagging combines;morphological tagging competitive;morphological tagging;generating morphological tags;morphological tagging relied;morphological tagging aims;perform morphological tagging;tag morphologically rich;morphological tags;tag morphologically;tags language;work morphological tagging;tagging combines neural;morphological tags given;cross lingual morphological;morphological tags low;tags neural;lingual morphological;predicting morphological;predicting syntactic;languages morphologically rich;languages morphologically;predicting syntactic traits;morphologicalmorphological tagging"}, "27e1dbe9f7c71cd6cc1b0357f49aef497e572d09": {"ta_keywords": "generate pseudo code;pseudo code generated;pseudo code automatically;pseudo code;statistical machine translation;code automatically given;given source code;code using statistical;code automatically;machine translation sm;source code using;code generated;illustrated pseudo code;source code;generate pseudo;automatically given source;machine translation;method generate pseudo;translation sm framework;code;code using;statistical machine;translation sm;using statistical machine;generate;automatically given;method generate;pseudo;method illustrated pseudo;using statistical", "pdf_keywords": ""}, "1abd1efae8c3849e28de926e52d166b7800965a1": {"ta_keywords": "aggregate rank lists;incomplete rank lists;aggregation incomplete rank;rank lists;rank lists accuracy;rank lists method;incomplete rank;handle incomplete rank;aggregate rank;rank lists ties;aggregate aggregate rank;aggregation;rank;aggregation incomplete;new method aggregation;method aggregation;aggregate;method aggregation incomplete;aggregate aggregate;lists accuracy better;unsupervised deep learning;lists accuracy;aggregate aggregate aggregate;method able aggregate;able aggregate aggregate;based unsupervised deep;lists ties;lists;unsupervised deep;able aggregate", "pdf_keywords": ""}, "d513a3583bd168ee341ce3b26d54a4e4096da471": {"ta_keywords": "performance data storage;data storage systems;data storage;storage systems;erasure code minimum;minimum xmath0 erasure;latency performance data;latency performance;storage systems based;case data storage;copies accessed minimum;xmath0 erasure code;maximum xmath0 erasure;storage;erasure code;erasure code case;xmath0 erasure;study latency performance;latency;performance data;minimum number copies;coding characterized minimum;copies accessed denoted;study latency;based minimum xmath0;erasure;number copies accessed;minimum xmath0;copies accessed;code case data", "pdf_keywords": ""}, "1d634d645bfe0b289fd6a2a0d9210b2a04c9237b": {"ta_keywords": "differentially private learning;private learning large;private learning;learning large nonlinear;approach differentially private;differentially private;large nonlinear models;parallel learning;non private baselines;private baselines;learning large;large nonlinear;model modest memory;accuracy parallel learning;large pretrained models;private baselines depth;private;parallel learning proposed;strong non private;learning proposed;nonlinear models;non private;differentially optimal;pretrained models;learning;based differentially optimal;novel approach differentially;pretrained models achieve;models;nonlinear", "pdf_keywords": "learning differentially private;private learning neural;differentially private learning;text differentially private;accelerating private learning;models differential neural;differential neural models;private differentially trained;neural models differential;models text differentially;neural language models;generative models private;differential privacy;differentially private significantly;differentially trained model;private learning models;learning model deep;models based deep;differentially private;model deep;models called deep;natural language models;differentially trained;differential neural model;novel differential neural;learning differentially;neural models depth;large language models;differentially private generators;private differentially"}, "5ee580fba44c6efb2a9b06f4c62de6b053db7784": {"ta_keywords": "lp losses vectors;network review reviewers;aggregate mapping loss;reviewers characterize hyperparameters;review reviewers;losses vectors social;social network review;losses vectors;mapping loss function;loss function matrix;lp losses;learning aggregate mapping;learning aggregate;mapping loss;reviewers;review;review reviewers characterize;class lp losses;reviewers characterize;loss function;hyperparameters;losses;framework learning aggregate;aggregate mapping;approach reviews;reviews;characterize hyperparameters choice;choice hyperparameters satisfies;choice hyperparameters;approach reviews ijcai", "pdf_keywords": "aggregation reviews based;aggregation reviews;reviews aggregated aggregation;aggregate recommendations reviewers;reviews aggregated;method aggregation reviews;captures reviewers opinions;captures opinions reviewers;opinions reviewers criteria;given reviewers reviews;criteria peer review;reviews aggregated demonstrate;scores given reviewers;reviewers opinions criteria;peer review;aggregating individual opinions;aggregation opinion social;recommendations capturing opinion;peer review method;recommendations reviewers;aggregate recommendations;reviewers criteria;dataset peer review;peer review provide;opinions reviewers;reviewers reviews;scores recommendations capturing;dataset individual reviewers;number reviews aggregated;opinions consensus mapping"}, "45dcccef42ed09cfd2babb630c117e95136b35d1": {"ta_keywords": "dialogue semantics schema;example dialogue semantics;dialogue semantics;labeled example dialogue;example dialogue;type universal dialogue;universal dialogue;universal dialogue dont;dialogue;semantics schema elements;dialogue dont;dialogue dont tell;semantics schema;schema elements proposed;state tracking benchmarks;semantics;schema elements;state tracking;schema;better existing systems;popular state tracking;existing systems;systems popular state;short labeled example;uses short labeled;existing systems popular;labeled example;short labeled;systems;tracking benchmarks", "pdf_keywords": "dialogue semantics schema;conveying semantics service;example dialogue semantics;dialogue semantics;dialogue use representation;descriptions data mining;semantics service;schema guided dialogue;dialogue state tracking;labeled example dialogue;dialogue based utterances;semantics input data;semantics data propose;prompt descriptions data;seq decodes dialogue;characterizing semantics data;semantics data;model conveying semantics;natural language systems;seq outperforms description;decodes dialogue based;generating natural language;process dialogue;natural language representation;conveying semantics;dialogue use;track dialogue agents;example dialogue;semantics schema elements;dialogue state"}, "6b9c3f82a0c0fd62f8ae527126b118890cfd452d": {"ta_keywords": "spin based xmath0;based xmath0 symmetry;xmath2 symmetry xmath3;generate quantum spin;xmath0 symmetry;xmath3 symmetry;symmetry xmath3;xmath0 symmetry xmath0;xmath2 symmetry;symmetry xmath3 symmetry;symmetry xmath0 symmetry;xmath1 symmetry xmath1;xmath1 symmetry;broken xmath2 symmetry;symmetry xmath1;symmetry xmath1 symmetry;xmath3 symmetry broken;xmath0 symmetry broken;broken xmath1 symmetry;symmetry broken xmath2;xmath1 symmetry broken;symmetry xmath0;symmetry broken xmath1;quantum spin;generate quantum;quantum spin based;method generate quantum;broken xmath2;xmath3;xmath2", "pdf_keywords": ""}, "1b06fe6ca5f4404e68b066cdea1a74a36e3e0e13": {"ta_keywords": "classify objects;robotic model adding;classify objects based;unseen objects scene;robotic model;objects scene improvement;object data improves;unseen objects;objects scene improve;objects scene;objects based shape;ability classify objects;objects based;object data model;objects;robotic;presence unseen objects;object data;object;static object data;static object;model representation;success robotic model;adding static object;classify;improving success robotic;success robotic;scene improvement;model adding static;based shape adding", "pdf_keywords": "benchmarking robot manipulation;objects consider robotic;robot grasping objects;objects robot;objects robot environments;grasping objects using;grasping objects;robot trained grasping;robot grasping;task robot;succeeds grasping object;relationships objects robot;predict robot;benchmarking robot;trained grasping;robot manipulation;robot manipulation use;consider robotic grasping;grasping object;robotic grasping;grasping objects used;task robot classification;robot execution;egocentric manipulation data;robot action outcomes;robot place object;predict accuracy robot;grasping object contained;robot succeeds grasping;robot action"}, "4cb3275ec95f4ad407f153aa9dc2d527bc2744e5": {"ta_keywords": "synthetic speech;speech synthesis;synthetic speech module;synthesized speech;synthesized speech proposed;proposes speech synthesis;synthetic speech target;prosody synthetic speech;functionality synthesized speech;reflected synthetic speech;speech synthesis allows;speech reflected synthetic;modification module speech;speech parameter generation;guide prosody synthetic;speech preserving original;speech module generates;speech module;using speech preserving;prosody synthetic;speech preserving;module speech;speaker using speech;input speech;module speech parameter;speech target speaker;input speech reflected;speech parameter;using speech;duration input speech", "pdf_keywords": ""}, "34c9e3152c9a14af711994230d8a3909daeaa7cf": {"ta_keywords": "reviewers characterize hyperparameters;empirical risk minimization;mapping empirical risk;scores final recommendations;minimization erm scores;risk minimization;aggregate mapping empirical;characterize hyperparameters choice;risk minimization erm;choice hyperparameters;reviewers;recommendations different reviewers;hyperparameters;choice hyperparameters satisfies;reviewers characterize;empirical risk;mapping empirical;approach reviews;learning aggregate mapping;hyperparameters choice hyperparameters;learning aggregate;characterize hyperparameters;hyperparameters choice;hyperparameters satisfies;approach reviews ijcai;final recommendations different;reviews;final recommendations;implement approach reviews;hyperparameters satisfies natural", "pdf_keywords": ""}, "46f88a062df05673ae0731aa17f9f9cc9d3e87bf": {"ta_keywords": "harmonic oscillator flow;flow driven harmonic;harmonic generated flow;flow dimensional harmonic;fluid dimensional harmonic;dimensional harmonic oscillator;harmonic oscillator;second harmonic generated;driven harmonic oscillator;driven second harmonic;driven harmonic;oscillator flow non;harmonic generated;second harmonic;dimensional harmonic;oscillator flow;non linear flow;oscillator flow driven;flow non linear;harmonic;linear flow;linear flow flow;flow flow non;flow fluid dimensional;flow dimensional;linear flow study;flow non;oscillator;non linear;generated flow", "pdf_keywords": ""}, "c4e83bfddb38642debb31097501aec8768f9020e": {"ta_keywords": "constructing artificial intelligence;generate artificial intelligence;artificial intelligence models;intelligence models based;physical systems best;method constructing artificial;physical systems;features underlying physical;artificial intelligence;constructing artificial;generate artificial;underlying physical systems;intelligence models;models best features;models based;features models;features models obtained;physical systems used;intelligence models apply;features models best;best features models;models;models obtained;models best;systems best features;models obtained combining;combining best features;systems used generate;combination best features;used generate artificial", "pdf_keywords": ""}, "f5813bb0b398007cae10ffdddeab221d4b9b0dc7": {"ta_keywords": "simulation dynamics fluid;dynamic simulation technique;dynamic simulation;dynamics fluid technique;simulation dynamics;dynamics fluid;fluid represented stationary;type dynamic simulation;technique simulation dynamics;simulation technique;simulation technique simulation;fluid technique based;dependent interaction fluid;simulation;interaction fluid surface;fluid technique;surface fluid represented;fluid surface fluid;technique simulation;fluid surface;fluid represented;interaction fluid;surface fluid;time dependent interaction;dynamics;dynamic;fluid;represented stationary state;stationary state represented;stationary state", "pdf_keywords": ""}, "27de5fb45af9799ed0020c978fe3a3080c60401e": {"ta_keywords": "networks trained morphokinetic;trained morphokinetic patterns;trained morphokinetic;morphokinetic annotations;fluid dynamics video;morphokinetic annotations simple;video present morphokinetic;present morphokinetic annotations;morphokinetic patterns;neural networks annotations;morphokinetic patterns large;morphokinetic;convolutional neural networks;scale convolutional neural;large scale convolutional;dynamics video;convolutional neural;fluid dynamics;dynamics video present;neural networks trained;scale convolutional;present morphokinetic;neural networks;use convolutional neural;networks neural;networks trained;neural networks neural;networks annotations obtained;networks annotations;convolutional", "pdf_keywords": ""}, "bab35e88a510938d22cb28f2ecc6f6e189c3d8ea": {"ta_keywords": "harmonic trap electron;trap electron dynamics;electron dynamics harmonic;2d electron gas;gas electron dynamics;trap electron;electron dynamics;dynamics harmonic trap;electron dynamics controlled;2d electron;electron gas;dimensional 2d electron;electron dynamics strongly;harmonic trap;presence 2d electron;electron gas electron;electron density controlled;gas electron;dynamics harmonic;controlled variation electron;electron density;electron density electron;electron;density electron;density electron density;variation electron;variation electron density;effect dimensional 2d;harmonic;trap", "pdf_keywords": "end speech recognition;speech recognition hsr;transformer modular hmm;speech recognition humans;continuous speech recognition;human speech recognition;advances speech recognition;frequency speech recognition;speech recognition;speech recognition hbr;high frequency speech;modular hmm dnn;automatic speech recognition;human speech data;architecture modular hmm;speech recognition consider;speech recognition lps;dnn human speech;speech recognition based;recognition spoken signal;speech data;proposed modular hmm;frequency machine hmm;modular hmm;feature human speech;dnn high frequency;automatic speech;recognition hsr systems;neural network hmm;frequency speech"}, "0132cb4384c3a6402353d8f349f8dd450d8ea4a2": {"ta_keywords": "normalizing spelling historical;normalizing spelling;method normalizing spelling;spelling historical documents;normalizing;normalization task network;normalization task;novel method normalizing;historical documents model;normalization;perform normalization task;perform normalization;method normalizing;spelling historical;documents model;network perform normalization;historical documents;documents model uses;documents;spelling;deep bi directional;model uses deep;task network;deep bi;uses deep bi;task network achieve;uses deep;deep;historical;bi directional network", "pdf_keywords": "short term memory;annotated text sequences;term memory networks;normalization historical words;normalization historical text;text predict;german text model;normalization task text;memory model normalization;learning additional normalization;text predict final;labeling text predict;term memory;character sequence labeling;annotated text;text sequences;term memory model;normalization modern german;bi lstm;linguistic annotation;linguistic annotation text;text based embeddings;characterizing text;bi lstm model;sequence labeling text;text accuracy annotations;text model;annotation text based;describing historical text;standard bi lstm"}, "39e734da43eb8c72e9549b42e96760545036f8e5": {"ta_keywords": "dialogs crowd workers;dialogs crowd;dialogs;analysis dialogs crowd;reading comprehension architecture;extract dialogs;quantitative model dialogs;approach extract dialogs;extract dialogs publicly;quantitative analysis dialogs;model dialogs;comprehension architecture;analysis dialogs;comprehension architecture extended;dialogs publicly;dialogs report;extended reading comprehension;dialogs publicly available;questions use information;14k information seeking;reading comprehension;crowd workers interested;dialogs report results;information seeking;model dialogs report;understanding content;information seeking questions;workers interested understanding;interested understanding content;understanding content hidden", "pdf_keywords": "reading comprehension dialog;dataset answering questions;comprehension dialog;machine comprehension dataset;reading comprehension dataset;dialogs training;frequent dialogs training;wikipedia pages dialogs;comprehension dialog type;dialoging people;dialogs students;learns ask question;answering questions context;content demonstrate dialogs;dialogs collect annotations;comprehension dataset;dialogs involve;brief answer model;dialogs involve crowd;task dialoging people;understood dialogs effective;dialogs effective;dialogs able achieve;dialogs provide student;dialoging;dialogs hidden wikipedia;comprehension dataset task;dialog easily;based dialogs teacher;dataset answering"}, "68b3905c2f82814294631f2ce29d5be4165e6b1f": {"ta_keywords": "wireless relay nodes;establishment wireless relay;relay nodes;deployment wireless relay;relay nodes person;wireless relay;sequential deployment wireless;wireless relay network;relay network packet;relay network;relay;optimal sequential deployment;optimal sequential;objectives establishment wireless;nodes provide optimal;establishment wireless;communication link nodes;problem optimal sequential;nodes person walks;walks line random;sequential deployment;deployment wireless;optimal policy structures;optimal policy;provide optimal policy;random length;random length known;provide optimal;optimal;network packet", "pdf_keywords": "optimal policy relay;optimal placement relay;optimal place relay;network optimal policy;relay placement distributed;wireless network stochastic;stochastic deployment relays;sequential relay placement;impromptu relay placement;wireless relay networks;relay placement wireless;network optimal;relay placement network;relay performance distributed;relay networks;wireless relay nodes;placement wireless networks;nodes wireless relay;propagation wireless networks;relay arbitrary propagation;placement relay nodes;policy relay placement;relay selection;relay nodes wireless;relay networks powerful;optimize deployment relays;sequential relay;wireless networks model;relay deployment wireless;relay nodes"}, "dc8ebb6d9908542ae474dc2b21bfb6a14216f678": {"ta_keywords": "parallel translation models;large multilingual translation;multilingual translation models;large parallel translation;translation models;evaluation large multilingual;multilingual translation;translation models using;parallel translation;translation models evaluate;large multilingual;multilingual;performance single graphics;available performance single;single high performance;translation;performance single node;performance single;high performance single;single graphics card;best available performance;single graphics;models evaluate parallel;available performance;graphics card;speed single input;input device performance;parallel using single;high performance;graphics card train", "pdf_keywords": ""}, "010df54445ab5f47582eb668dc3488a3e46b55d3": {"ta_keywords": "unsupervised learning;large amplitude entangled;unsupervised generation;approach unsupervised learning;amplitude entangled states;amplitude entangled;unsupervised learning based;based unsupervised generation;unsupervised generation large;learning based unsupervised;novel approach unsupervised;entangled states;generative model outperforms;generative model;entangled states method;unsupervised;approach unsupervised;generative;applied generative model;applied generative;entangled;generation large amplitude;method applied generative;based unsupervised;large amplitude;learning;models;states method;amplitude;learning based", "pdf_keywords": "learning unsupervised;unsupervised hidden models;unsupervised learning;learning unsupervised hidden;approach unsupervised learning;unsupervised learning neural;unsupervised learning unsupervised;induction speech tags;novel approach unsupervised;tag induction robustness;existing generative models;hidden models;unsupervised tasks;generative models;probabilistic learning deep;machine learning nonlinear;learning nonlinear;neuralization hmm;supervised;approach unsupervised;techniques neuralization hmm;neuralization hmm effective;words predicting;neural language modeling;generative information;tag induction;learning deep;hidden models approach;predict linguistic structure;learning nonlinear systems"}, "90b9d19af75c86f42865052c21305c70f884b5fe": {"ta_keywords": "selfish routing game;selfish routing;behavior selfish routing;congestion costs multiplicative;user underestimates congestion;routing game uncertain;equilibrium behavior selfish;underestimates congestion costs;cost congestion;congestion costs;estimates cost congestion;routing game;underestimates congestion;congestion;transportation networks;urban transportation networks;game uncertain users;equilibrium behavior;transportation networks drivers;impacts equilibria;equilibria;routing;distinct impacts equilibria;urban transportation;equilibrium;uncertain users;results urban transportation;transportation;impacts equilibria apply;equilibria apply", "pdf_keywords": ""}, "124385efee78010a4408329dffea4798f5a1ad47": {"ta_keywords": "translation unit segmentation;simultaneous speech translation;phrasebased translation systems;speech translation;probabilities phrasebased translation;translation systems;phrasebased translation;perform translation unit;translation uses lexicalized;begin translation delay;speech translation uses;translation delay;translation unit;perform translation;translation systems decide;english translation parameter;information perform translation;delay japanese english;translation delay experimental;translation parameter;unit segmentation;translation parameter introduced;lexicalized information perform;unit segmentation considering;segmentation;lexicalized information;begin translation;delay japanese;translation uses;reduces delay japanese", "pdf_keywords": ""}, "0eac6cbd150b1a7e9d757ccc871eea2bf0d89e42": {"ta_keywords": "learning soft labels;supervised learning soft;soft labels based;soft labels;labels based deep;learning soft;relationships labels;supervised learning;labels based;labels;labels encoding probability;constrains relationships labels;supervised;deep neural;learning complex relationships;relationships labels encoding;machine learning tasks;deep neural networks;novel method supervised;method supervised learning;machine learning;standard machine learning;method supervised;labels encoding;neural networks;soft;distributions allows learning;based deep neural;learning tasks;learning", "pdf_keywords": "regression based deep;deep neural;ordinal labels deep;label representation deep;ordinal regression encodes;network cnn;learning depth;labels deep learning;representation deep;ordinal classification;learning depth single;able predict ordinal;deep learning;deep neural network;cnn global;soft ordinal label;category ordinal regression;regression encodes rank;ordinal regression;neural network cnn;predict ordinal relationships;learning predicting classes;cnn;ordinal classification approach;based deep neural;labels deep;image method learns;representation deep learning;cnn global recognition;soft encoding rank"}, "59f3e3cad309eb4965d67773d68bc2f91b2e376f": {"ta_keywords": "speech translation corpus;endangered language documents;translation corpus;translation corpus highland;central mexico corpus;mexico corpus;nahuatl endangered language;endangered language spoken;documentation endangered language;endangered language;speech translation;mexico corpus open;corpus;corpus highland puebla;puebla nahuatl endangered;language documents;corpus open;present speech translation;highland puebla nahuatl;corpus highland;spoken central mexico;puebla nahuatl;documentation endangered;corpus open publicly;improve documentation endangered;highland puebla;nahuatl endangered;translation;language spoken central;language spoken", "pdf_keywords": ""}, "2ac98a28fdae4c01a89f09393c736e72445a4c4e": {"ta_keywords": "distributed quantum networks;distributed quantum network;networks distributed quantum;quantum networks distributed;network distributed quantum;quantum network distributed;based distributed quantum;distributed quantum;architecture distributed quantum;distributed quantum computation;performing distributed quantum;communication distributed quantum;processing distributed quantum;described distributed quantum;quantum networks based;concept distributed quantum;distributed quantum information;quantum networks capable;quantum networks;quantum network;quantum computation communication;quantum network described;quantum computation;quantum information processing;quantum information communication;quantum;networks based distributed;networks distributed;scalable architecture distributed;network distributed", "pdf_keywords": ""}, "da660ca9e6fedefe815e305efd0dcd3bf9b4bb60": {"ta_keywords": "features relation extraction;relation extraction;relation extraction process;learned entity filters;approach relation extraction;entity recognizers;entity filters proposed;entity filters;named entity recognizers;jointly learned entity;entity recognizers evaluate;entity recognizers order;performance entity recognizers;learned entity;named entity;entity;features relation;consistency named entity;relation;based jointly learned;improves performance entity;jointly learned;extraction process preciseness;salient features relation;extraction process based;performance entity;recognizers order;filters proposed approach;filters;extraction", "pdf_keywords": ""}, "2078d466766b6876d73ac1981392fa8bd2b9520d": {"ta_keywords": "density states quantum;probability density states;atoms quantum state;states quantum method;quantum level atom;atom quantum level;atom quantum;states quantum;quantum state;level atom quantum;quantum level atoms;atoms quantum;applied quantum level;level atoms quantum;applied quantum;quantum method;quantum state interacting;density states;quantum level;method applied quantum;quantum;schrdinger equation;quantum method based;standard schrdinger equation;schrdinger equation method;state interacting particles;calculation probability density;solution schrdinger equation;schrdinger;probability density", "pdf_keywords": ""}, "dcac1abd2ae5af180e51994a9c8334a6de915765": {"ta_keywords": "distributed stochastic differential;distributed stochastic;stochastic differential;stochastic differential equations;framework distributed stochastic;sde arbitrary compressions;sde arbitrary sampling;learning rate convex;equations sde arbitrary;stochastic;compression quantization gradient;sde arbitrary;biased compression quantization;differential equations sde;expectation constant learning;variants sde arbitrary;complexity variants sde;feedback biased compression;quantization gradient;equations sde;rate convex;rate convex strongly;strongly convex objectives;constant learning rate;optimum asymptotically expectation;biased compression;compression quantization;constant learning;convex objectives;sde", "pdf_keywords": "distributed stochastic methods;stochastic gradient descent;distributed stochastic optimization;complexity stochastic gradients;gradient descent stochastically;distributed stochastic method;distributed stochastic differential;algorithm distributed learning;distributed learning algorithms;descent stochastically;stochastic gradients workers;distributed learning;estimators distributed training;implementation distributed learning;distributed learning linear;gradient estimators distributed;problem distributed learning;algorithms stochastic gradient;estimating stochastic gradients;descent stochastically bounded;consider distributed learning;distributed stochastic;stochastic methods;estimator stochastic gradient;stochastic gradients;distributed learning problem;gradient based stochastic;variants distributed stochastic;based distributed stochastic;converges stochastic gradient"}, "6a9394e5d49c1251c0fb6d7fb0c0813d26c6a907": {"ta_keywords": "gaps semantic parsing;semantic parsing;lexical semantic gaps;semantic parsing model;semantic gaps semantic;semantic gaps;gaps semantic;bridge lexical semantic;lexical semantic;semantic benchmarks scholar;semantic;parsing;semantic benchmarks;parsing model achieves;parsing model;bridge lexical;benchmarks scholar s__intro;lexical;strong performance semantic;method bridge lexical;performance semantic;performance semantic benchmarks;scholar s__intro s__gp;scholar s__intro;benchmarks scholar;s__gp zero labeled;zero labeled data;s__intro s__gp;s__intro s__gp zero;zero labeled", "pdf_keywords": "semantic parser trained;generation semantic parsers;shot semantic parsers;parser trained deep;deep semantic parser;parser trained;semantic parser favorable;semantic parser high;semantic parsers;train semantic parser;learning semantic parsers;semantic parsers optimally;semantic parser;semantic parser finetune;zero shot parsers;given semantic parser;generalization semantic parsers;semantic parsers freebase;parsers optimally linguistic;shot parser trained;parser high level;gap language synthesis;programs grammar paraphrasing;zero shot parser;semantic parsers map;shot parsers;semantic parsers method;semantic parser method;language synthesis;relies semantic parser"}, "5446a8bbadc2ba2c575353b257f26abae27b3b2a": {"ta_keywords": "comparing items embedding;items embedding;learning embedding;items embedding present;learns embedding;set items embedding;approach learns embedding;learning embedding representation;learns embedding representation;embedding;embedding representation;items embedding transformed;embedding representation representing;representation representing items;representing items;embedding representation set;embedding present range;method learning embedding;embedding present;lifted method learning;representing items point;representation set items;embedding transformed;comparing items;item comparing items;item comparing;items point lowdimensional;convex lifted method;novel convex lifted;used item comparing", "pdf_keywords": ""}, "f49ccfb32aad8e6893e8cbb037c1282572fe6e21": {"ta_keywords": "reconstruction model graph;confidence partial reconstruction;partial reconstruction model;graph guided models;generates partial reconstruction;model graph guided;partial reconstruction;reconstruction model;mutation partial reconstruction;reconstruction model performed;deep confidence partial;reconstruction model guide;graph guided;model graph;guided models;graph characteristics generates;model guide graph;method detecting deep;guided models method;detecting deep confidence;deep testing;graph characteristics mutation;guide graph characteristics;using deep testing;graph characteristics;detecting deep;confidence partial;deep testing algorithm;performed using deep;guide graph", "pdf_keywords": ""}, "62dc7bdae6700c4409e6d9773d6ecb5c0fab75a4": {"ta_keywords": "approximate dictionary searching;dictionary searching;approximate dictionary;methods approximate dictionary;dictionaries frequent words;dictionary searching introduce;indexing methods approximate;dictionaries frequent;dictionaries dictionaries frequent;russian dictionaries dictionaries;frequent words extracted;russian dictionaries;dictionaries;state art indexing;indexing methods;indexing;english russian dictionaries;dictionaries dictionaries;language datasets;dictionary;natural language datasets;sequence based filtering;searching introduce taxonomy;searching;words extracted;art indexing methods;words extracted clueweb09;frequent words;art indexing;filtering", "pdf_keywords": ""}, "02aebef93baeef3396f3cb4468a7054067f190c6": {"ta_keywords": "extracting database entities;canonical entities matching;entities matching;nonparametric approach extracting;matching entities database;matching entities;entities matching entities;approach extracting database;database entities text;entities text;canonical entities;database accurately extracted;approach extracting;entities text method;extracting database;entities database;database entities;entity propose probabilistic;structure entity;set canonical entities;entities;entities database database;probabilistic approach;accurately extracted set;propose probabilistic approach;model structure entity;entity;extracting;database accurately;identification set canonical", "pdf_keywords": ""}, "f262ef2f50dfcaf07dc6598f22fb9b2470b37cf1": {"ta_keywords": "information extraction;information extraction using;frameworks information extraction;information extraction tasks;span entities;nested span entities;dynamic span graphs;framework information extraction;detect nested span;extraction using dynamic;enumeration span entities;span entities good;span graphs framework;dynamic span;using dynamic span;span graphs;nested span;extraction tasks multiple;extraction tasks;enumeration span;span;extraction using;observe enumeration span;entities;extraction;entities good approach;detect nested;approach detect nested;graphs framework;graphs framework outperforms", "pdf_keywords": "leveraging relation coreference;entity relation detection;relation extraction tasks;relation detection tasks;entity relation extraction;entity extraction tasks;entity recognition relation;extraction entity mentions;overlapping entity extraction;entity recognition;overlapping entity recognition;relation extraction;joint extraction entity;extraction relation propagation;relation entity mentions;relation coreference links;embeddings extract relation;extraction relation extraction;relation extraction overlapping;tasks relation extraction;relation detection;entity extraction relation;extraction relation entity;recognition relation extraction;entity extraction;propagation entity extraction;entity extraction task;entity extraction propose;entity recognition model;information extraction tasks"}, "36906613dcef29263afe711f128da1fc916cbbee": {"ta_keywords": "attention random data;self attention random;attention random;clustering data random;data random sequence;approach self attention;random data;random data proposed;self attention;data random;problem self attention;approach random data;clustering;random sequence;attention;random data shift;known approach random;clustering data;approach random;random sequence obtain;relative sequence experimental;sequence experimental;random;problem clustering;applied problem clustering;problem clustering data;data shift invariant;sequence experimental evaluation;relative sequence;data", "pdf_keywords": "speech recognition attention;self attention algorithm;input features attentions;attention speech recognition;attention architecture based;attention algorithm;frame indexing attention;attention long sequence;self attention architecture;recognition attention generated;attention source based;attention architecture;recognition attention;self attention speech;indexing attention statistically;encoders attention;features proposed attention;recognition encoders attention;self attention source;features attentions;attention algorithm centered;indexing attention;long data attentions;speech recognition;features attentions described;attention speech;attention directed neural;self attention network;attention block;speech recognition encoders"}, "9a334566b79bc6c6906e2b5285d5ea50b9b99479": {"ta_keywords": "learning invariant;adversarial minimax game;learning invariant representations;adversarial minimax;formulated adversarial minimax;bias free classification;invariant representations;adversarial;invariant representations able;formulated adversarial;invariant representation able;representations able generalize;invariant representation;representation able generalize;independent image classification;able generalize task;free classification;image classification;minimax game;framework learning invariant;framework formulated adversarial;representations able;representations;generalize task proposed;invariant;classification;minimax;generalize task;bias free;generalize given task", "pdf_keywords": "features machine translation;adversarial minimax game;formulated adversarial minimax;adversarial minimax;improves vanilla discriminative;invariant representation learning;invariant representation learned;machine translation tasks;machine translation powerful;learning invariant features;learn invariant representation;learning invariant;adversarial game;machine translation;learning problem adversarial;problem adversarial game;vanilla discriminative approaches;translation model predicting;translation tasks;translation tasks proposed;fair representation learning;feature representation invariant;problem adversarial;machine translation model;learned representation;learn representations invariant;adversarial;formulated adversarial;vanilla discriminative;invariant features machine"}, "939dfa4dec88ca167e6572904c4ad2fcbf726f48": {"ta_keywords": "gradient flows graphons;flows graphons;flows graphons based;exchangeable class graphons;graphs nodes exchangeable;weighted graph discrete;gradient flows;graphons large graphs;weights class graphs;class gradient flows;graph discrete time;class graphons;graphons;behavior weighted graph;graphons graphons;weighted graph;class graphons graphons;graphons based;graphons graphons large;nodes exchangeable;graphons large;nodes exchangeable class;graph discrete;called graphons;called graphons large;large graphs nodes;discrete time evolution;large graphs;flows;graphs nodes", "pdf_keywords": "convergence gradient flows;flows large graphs;convergence gradient flow;gradient flow bounded;graphons gradient flow;graphon gradient flow;graphs limiting distributions;gradient flows arise;flow space graphons;convergence probability measures;gradient flows theorems;gradient flow entropy;gradient flow convex;convergence semiconvex;gradient flows large;generalized gradient flow;gradient flows;continuous gradient flows;gradient flow space;thetheorems semiconvex functions;convergence semiconvex function;graph convergence;theorems semiconvex functionswe;flow convex;existence gradient flows;entropy function graphon;graph convergence rate;flow convex function;distribution initial measures;large graphs limiting"}, "c377bf3ae52dee4075c1e807de9c5579d553de22": {"ta_keywords": "european texts dialogue;2012 european texts;text recognition;texts dialogue;european texts;methods text recognition;recognition;text;texts;texts dialogue conference;presented 2012 european;dialogue;dialogue conference held;dialogue conference;2012 european;paris italy;european;held paris italy;paris italy september;papers presented 2012;conference held paris;best methods text;italy;evaluation;italy september;literature;paris;presented 2012;italy september 13;papers presented", "pdf_keywords": ""}, "ea71f5f59727b63b8912c6db097ba811da41bf5b": {"ta_keywords": "stochastic propagation particle;propagation particle nonlinear;particle nonlinear potential;nonlinear potential particle;particle nonlinear;propagation particle;potential particle flow;strength particle flow;stochastic propagation;particle flow governed;study stochastic propagation;particle flow;nonlinear potential;velocity particle density;particle velocity density;particle velocity particle;function particle velocity;strength particle;velocity density particle;particle velocity;particle particle;particle density particle;potential particle;law function particle;particle density;velocity particle;particle interaction strength;density particle particle;stochastic;particle particle interaction", "pdf_keywords": "controlling dynamics stochastic;control dynamics stochastically;stochastic particle dynamics;controlling stochastic systems;implementing stochastic control;dynamics stochastically;particle flow stochastic;optimally controlling stochastic;dynamics stochastic particle;stochastic control;control stochastic systems;controlling stochastic state;stochastic systems manipulating;paths stochastic control;systems stochastic control;control stochastic;controlling stochastic;deterministic particle flow;dynamics stochastic;stochastic control systems;flow stochastic particle;controlled paths stochastic;stochastic optimal control;deterministic particle dynamics;optimal control stochastic;dynamics stochastic based;stochastic control frameworks;dynamics stochastic processes;implementing stochastic;constrained stochastic differential"}, "5a26eeda7c2ca58c2d56f1d580fbbae9eb1a19cd": {"ta_keywords": "neural networks approximating;small neural networks;pde coefficients representable;networks approximating solutions;elliptic pdes dirichlet;neural networks parameters;linear elliptic pdes;elliptic pdes;representable small neural;networks approximating;pdes dirichlet;simulates gradient descent;power neural networks;neural network architecture;neural networks;pdes dirichlet boundary;pde coefficients;growing neural network;coefficients representable small;gradient descent;dirichlet boundary conditions;coefficient networks;pdes;small neural;approximating solutions linear;gradient descent growing;prove pde coefficients;approximate solution scale;approximating solutions;neural network", "pdf_keywords": "neural network approximations;neural networks approximating;approximated neural network;small neural networks;network approximated neural;neural networks numerical;neural networks parameters;approximated neural;networks approximating solutions;networks numerical approximation;intractable neural networks;representable small neural;represent neural network;networks approximating;network approximations;numerical approximation partial;parameters neural network;expressible small neural;neural network bounded;numerically intractable neural;parameters neural;neural networks used;complexity bounds neural;network approximations solving;simulates gradient descent;pde coefficients representable;pdes provide theorems;neural networks;solution network approximated;neural network derive"}, "7647a06965d868a4f6451bef0818994100a142e8": {"ta_keywords": "neural language models;character aware neural;aware neural language;tasks sequence labeling;sequence labeling;language models;language models extract;sequence labeling proposed;neural language;aware neural;models tasks sequence;nonlinear models tasks;models tasks;character aware;benchmark datasets demonstrate;novel nonlinear models;neural;train novel nonlinear;tasks sequence;key knowledge training;labeling;experiments benchmark datasets;models extract;datasets demonstrate;benchmark datasets;labeling proposed framework;nonlinear models;knowledge training;sequence;models", "pdf_keywords": "neural language models;level neural language;language models trained;trained unannotated sequence;neural language forward;train neural language;model sequence labeling;neural language model;entity recognition;trained language model;annotations train;novel sequence labeling;word level embedding;unannotated sequence data;annotations train models;sequence labeling framework;named entity recognition;forward language model;neural language;recurrent network;character level neural;sequence labeling;models trained unannotated;sufficient annotations train;entity recognition recent;language models;learning model word;language model sequence;pretrained language model;word levels forward"}, "e65676b43338e914ad77afd0fd6ce4bef87943a1": {"ta_keywords": "singing voice conversion;voice conversion;convert voice;voice conversion svc;generate converted singing;converted singing voice;convert voice timbre;singing voice waveforms;statistical singing voice;differential convert voice;converted singing;novel statistical singing;singer using vocoder;voice waveforms proposed;statistical singing;voice waveforms;voice timbre source;singing voice;voice timbre;vocoder generate converted;preserving conversion accuracy;conversion accuracy preserving;preserving conversion;waveform modification based;singer using;target singer using;conversion svc technique;accuracy preserving conversion;timbre source singer;generate converted", "pdf_keywords": ""}, "d29d33f3b92b447d6011606be41b64439a1da088": {"ta_keywords": "contextual bandit algorithm;contextual bandit;bandit algorithm;regret contextual bandit;online agent learns;bandit algorithm underlies;agent learns;agent learns set;bandit;novel online agent;making decisions online;online agent;decisions online;decisions online setting;reward feedback;learns set behavioral;reactive reward feedback;bound expected regret;uses learned constraints;learned constraints;learns;behavioral constraints observation;reactive reward;expected regret contextual;learned constraints guide;observation uses learned;behavioral constraints;reward feedback characterize;regret contextual;set behavioral constraints", "pdf_keywords": "contextual bandit algorithm;constraints argue bandit;modified contextual bandit;contextual bandit;extension contextual bandit;bandit problem learning;bandit problems explicitly;contextual bandit setting;behavioral constrained thompson;constrained thompson sampling;based contextual bandits;contextual bandits;bandit problems explicit;bandit algorithm;behavior constrained thompson;bandit problems allowed;bandit algorithm solve;constrained policy learned;bandit problems;learning behavioral constraints;consider bandit problem;constrained behavior learned;bandit problems relatively;bandit problem;agents constraint learning;contextual bandits class;context multiarmed bandit;agent constrained policy;bandit setting ideal;constrained thompson"}, "721d7c82b80f14246d353251837e1711824a9e60": {"ta_keywords": "speech recognition dereverberation;dereverberation integrated speech;frontend speech recognition;recognition dereverberation module;integrated speech recognition;speech recognition;recognition dereverberation used;perform frontend speech;frontend speech;recognition dereverberation;frontend dereverberation integrated;dereverberation module used;dereverberation proposed framework;integrated speech;dereverberation module;dereverberation integrated;corpus reverb performance;frontend dereverberation;corpus reverb;2mix corpus reverb;wsj1 2mix corpus;dereverberation used;dereverberation used perform;separation dereverberation proposed;framework frontend dereverberation;2mix corpus;joint separation dereverberation;separation dereverberation;dereverberation;corpus", "pdf_keywords": "dereverberation beamforming speech;neural beamformer decoder;speech beamforming;speech beamforming achieved;beamforming speech;end speech recognition;desired speech beamforming;speech recognition dereverberation;beamformer decoder proposed;dereverberation beamforming algorithms;dereverberation wireless speech;field speech recognition;dereverberation beamforming architecture;beamformer decoder;separation speech signal;unified convolutional beamformer;neural beamformer;multi channel speech;speech recognition;speech recognition framework;frontend neural beamformer;speech recognition systems;wireless speech;beamforming speech important;speech dereverberation separation;convolutional beamformer simultaneous;speech recognition proposed;wireless speech applications;unified dereverberation beamforming;advanced dereverberation beamforming"}, "a45c3120c077994409093771077a2d16f77674c5": {"ta_keywords": "transfer learning methods;efficient transfer learning;transfer learning;parameter efficient transfer;pretrained models;states pretrained models;efficient transfer;efficient fine tuning;pretrained models define;fine tuning methods;learning methods;new parameter efficient;learning methods frame;tuning methods;state art parameter;transfer;learning;parameter efficient;hidden states pretrained;tuning methods achieve;states pretrained;parameter efficient fine;models;pretrained;fine tuning;art parameter efficient;tuning;new parameter;methods modifications;previous methods tasks", "pdf_keywords": ""}, "62d17b6f6ad77fd71ef9954c7784700d5e316f1f": {"ta_keywords": "privacy language models;privacy language;sanitization differential privacy;notion privacy language;differential privacy privacy;differential privacy;meaningful notion privacy;privacy privacy;privacy social norm;privacy;popular data protection;privacy privacy social;data protection;privacy social;notion privacy;data protection techniques;protection techniques sanitization;sanitization differential;techniques sanitization differential;public use;language models;existing protection methods;language models trained;explicitly produced public;techniques sanitization;protection methods;sanitization;language models conclude;argue existing protection;models trained text", "pdf_keywords": "privacy language models;privacy natural language;privacy language data;defending privacy language;privacy language;language privacy;complexities language privacy;privacy risks language;trained protect privacy;language privacy argue;preservation privacy language;models preserve privacy;problem privacy language;analysis privacy language;privacy language industry;understand privacy language;privacy language social;privacy large class;language models attacks;analyze privacy large;risks language models;defending privacy;meaningful notion privacy;privacy large;protect privacy;privacy risks large;preserve privacy;challenge privacy enhancing;current state privacy;challenge privacy"}, "211a6838b9550d227ce81d0bec542ec5b70e290b": {"ta_keywords": "news text augmented;text augmented approach;text augmented;knowledge base augmented;outperforms news text;news text;approach outperforms news;standard news text;outperforms augmented approach;accuracy knowledge base;augmented approach outperforms;approach outperforms augmented;outperforms augmented;augmented version;propose augmented version;knowledge base;outperforms news;augmented approach;augmented version standard;version standard news;augmented;base augmented approach;propose augmented;accuracy knowledge;augmented approach points;text;points accuracy knowledge;base augmented;standard news;news", "pdf_keywords": ""}, "cf9fa9ebbefab1877aa7a501c888a8a618c31abb": {"ta_keywords": "polarity political blogs;polarity blog post;polarity blog;political blogs method;polarity social media;political blogs;results polarity blog;set political blogs;posts results polarity;topics discussed blog;polarity polarity political;polarity political;blogs;blogs method;blogs method based;discussed blog posts;post related polarity;blogs method applied;content blogs;blog posts;content blogs method;blog post related;identification polarity social;blog;blog post;social media content;discussed blog;polarity social;blog posts results;related polarity social", "pdf_keywords": ""}, "42605dca59a3aafe2e5b33741a98dad9ba117395": {"ta_keywords": "walk based similarity;graph walk based;ranking graph walk;improving graph walk;graph walk performance;improve graph walk;graph walk;walks use framework;random walks use;similarity objects links;information management nodes;management improve graph;management nodes graph;walk performance ranking;random walks;performance ranking graph;ranking graph;similarity measure;walk based;form random walks;personal information management;walks use;similarity context personal;characterize similarity;similarity objects;based similarity;characterizing improving graph;walk performance;improving graph;management nodes", "pdf_keywords": ""}, "2a94fa0de804b5efaae1a66f50c3ea96539c46b8": {"ta_keywords": "dialog based human;dialog based;dialog example based;dialog unstructured;rich dialog unstructured;dialog unstructured dynamic;goal dialog based;human conversation examples;dialog example;best dialog example;human human conversation;dialog;human conversation;non goal dialog;content rich dialog;best dialog;rich dialog;conversation examples drama;input best dialog;user natural language;conversation examples;goal dialog;interact content;users interact content;natural language input;examples drama television;conversation;drama television goal;users interact;natural language", "pdf_keywords": ""}, "d0895dccd61c567034d197eecfa5d7d59332061f": {"ta_keywords": "band regenerating codes;regenerating codes generating;regenerating codes;regenerating codes using;codes generating minimum;regenerating codes values;minimum band regenerating;generating minimum band;codes generating;codes values generating;codes values;codes using new;band regenerating;constructions generating minimum;optimal constructions generating;new product matrix;generating minimum;minimum band;codes using;generating;codes;product matrix framework;product matrix;constructions generating;values generating minimum;presents optimal constructions;optimal constructions;regenerating;paper presents optimal;values generating", "pdf_keywords": "optimal regenerating codes;regenerating codes feasible;regenerating codes distributed;regenerating code feasible;regenerating codes based;codes based matrix;bound network coding;regenerating codes presented;exact regenerating codes;regenerating codes;constructing matrix code;regenerating codes provides;distributed storage codes;notion regenerating codes;regenerating code necessarily;codes distributed storage;network coding sufficient;network coding;storage codes distributed;regenerating storage codes;codes feasible;exact regenerating code;mtm regenerating codes;encoding matrix;codes distributed;minimum storage code;storage codes based;code theorems storage;encoding matrix message;storage code theorem"}, "ffe6d7573bb2c4fbfac0cc474804b5b1734a1179": {"ta_keywords": "movie recommendation agent;online movie recommendation;recommendation agent;recommendation agent able;behavior constraints;behavioral constraints observations;interactive interface online;behavior constraints significantly;making decisions online;decisions online setting;behavioral constraints;movie recommendation;set behavioral constraints;interface online movie;decisions online;interactive;online movie;learns set behavioral;interactive interface;agent able act;building interactive interface;set behavior constraints;agent;agent able;constraints observations;learns;building interactive;online setting demonstrate;constraints observations uses;constraints guide making", "pdf_keywords": ""}, "40fc6e46f2921be346eacff86ce765ff5b28fbdd": {"ta_keywords": "funding rates predictive;model funding rates;garch models predictive;funding rates based;predictive model funding;funding rates;funding rates aforementioned;relationship funding rates;funding rates paper;model funding;garch models;fit garch models;behaviour funding rates;predict behaviour funding;garch;funding;examines relationship funding;best fit garch;fit garch;rates predictive model;models predictive;relationship funding;predictive model used;models predictive model;rates predictive;predictive model;predictive model constructed;develops predictive model;behaviour funding;model used predict", "pdf_keywords": "funding rate heteroskedasticity;saddle stock market;price saddle bitcoin;rate heteroskedasticity;heteroskedasticity process modelled;heteroskedasticity process;rate heteroskedasticity adopted;saddle bitcoin derivative;rates price saddle;futures contracts saddle;hypothesis heteroskedasticity engle;heteroskedasticity engle;shown heteroskedasticity process;saddle bitcoin;financial markets method;hypothesis heteroskedasticity;heteroskedasticity engle 1982;rates financial markets;heteroskedasticity;prices financial;currency quoted spot;price saddle;funding rates price;alternative hypothesis heteroskedasticity;predict prices financial;stock market;traded saddle stock;heteroskedasticity adopted;spot price basis;rates financial"}, "f7c9521dcd80127d6d4a72fb407e81a9c518ae8d": {"ta_keywords": "knowledge intensive inductive;definitions inductive learning;inductive learning;inductive algorithms;intensive inductive algorithms;inductive algorithms relatively;concept definitions inductive;knowledge free algorithms;definitions inductive;extracting relevant knowledge;algorithms relatively knowledge;knowledge free;finding effective concept;defining knowledge;relevant knowledge knowledge;knowledge knowledge rich;defining knowledge intensive;inductive;intensive inductive;knowledge rich literature;knowledge intensive;relatively knowledge free;knowledge knowledge;effective concept definitions;free algorithms;concept definitions;knowledge;free algorithms framework;framework defining knowledge;extracting relevant", "pdf_keywords": ""}, "737aff546a9112127d7a13a5b835e27a6e1e935e": {"ta_keywords": "transformer recognition speech;autoregressive transformer recognition;speech recognition;speech recognition systems;recognition speech recognition;recognition speech;non autoregressive transformer;autoregressive transformer;transformer recognition;tokens input speech;predicting masked tokens;speech variance masked;optimized predicting masked;variance input speech;input speech variance;input speech;predicted missing tokens;speech variance;transformer;predicting masked;recognition systems proposed;recognition systems;network trained;variance masked tokens;iteratively predicted missing;recognition;masked tokens network;autoregressive;non autoregressive;masked tokens", "pdf_keywords": ""}, "39fdea1c34832f9bb1644bff81f53fb8ce6b2679": {"ta_keywords": "spin glass spin;particle spin glass;dynamics spin particle;spin dynamics spin;spin dynamics particle;spin dynamics;glass spin orbit;zeeman field spin;dynamics particle spin;control spin dynamics;dynamics spin;particle spin dynamics;spin orbit interaction;spin particle spin;field spin dynamics;spin particle;spin glass;particle spin;control spin;glass spin;particle spin orbit;spin orbit;spin;field spin;used control spin;effect zeeman field;effect zeeman;zeeman;zeeman field;study effect zeeman", "pdf_keywords": ""}, "b58e80ad8c6e6844c41535080ccbdef06bce3b6e": {"ta_keywords": "3d house simulator;house simulator;navigation manipulation simulator;house simulator support;simulator support navigation;3d house;present 3d house;manipulation simulator based;simulator based;manipulation simulator;simulator;simulator support;navigation manipulation;3d;support navigation manipulation;present 3d;navigation;support navigation;house;manipulation;based;support;present", "pdf_keywords": "house agent learning;layouts agent learning;agent learning environment;agents structured environment;manipulation simulated environments;structured environment reinforcement;learning complex environments;object based navigation;3d house simulator;actions agents structured;navigation manipulation environment;agent easily create;predicting actions agents;house simulator;agent framework;task designing agents;learning environment chalet;agent framework learning;designing agents;novel agent framework;agents structured;3d house;environment reinforcement learning;actions agents;arbitrary actions agents;agent learning;visual manipulation simulated;learning environment;simulated environments;agent actions"}, "b0efb62aa2a435704a3412d592e73faf6be5ecea": {"ta_keywords": "relationships characters models;characters models unsupervised;characters models;unsupervised learning;models based unsupervised;unsupervised capture relationships;models unsupervised;relationships characters way;capture relationships characters;relationships characters;based unsupervised learning;based unsupervised;relationships evolving sequences;models semantically coherent;evolving sequences latent;learning capture relationships;sequences latent states;capture relationships evolving;models unsupervised capture;characters;unsupervised;capture relationships;sequences latent;semantically coherent;characters way consistent;unsupervised learning capture;models semantically;latent states demonstrate;relationships evolving;evolving sequences", "pdf_keywords": ""}, "8b48c55808636a52699b38869df3eba9c8b999d9": {"ta_keywords": "statistical voice conversion;signal voice conversion;voice conversion;voice conversion realized;conversion nonaudible whisper;digital signal voice;voice conversion nonaudible;whisper digital signal;nonaudible whisper digital;statistical voice;signal voice;whisper digital;time statistical voice;nonaudible whisper;real time statistical;conversion nonaudible;distributed signal processing;voice;conversion realized averaging;input signal distributed;digital signal processing;signal processing sdes;distributed form digital;averaging output signal;signal distributed form;multidimensional distributed signal;signal processing rds;digital signal;distributed signal;implement real time", "pdf_keywords": ""}, "a0b47c7162d1a3b04b27e27c9fadd2eabc4dab0e": {"ta_keywords": "machine translation gw;translation gw transition;statistical machine translation;machine translation;gw based language;gw based decoder;translation gw;gw transition gw;perform gw transition;transition gw gw;gw gw transition;transition gw;transition uses gw;gw transition;convert data gw;language resulting translation;gw transition uses;gw transition problem;problem gw transition;resulting translation able;transition problem gw;translation able perform;range data languages;data gw based;gw transition wide;data languages;decoder convert data;based decoder convert;decoder convert;data gw", "pdf_keywords": ""}, "c3f9c1f702d0c3b35b99502674757b3d8e7dd352": {"ta_keywords": "speaker synthesized speech;native speech synthesis;speech synthesis technique;using synthesized speech;speech synthesis method;synthesized speech native;preserving speaker synthesized;synthesized speech;based speech synthesis;speech synthesis;naturalness preserving speaker;synthesized speech experimental;speaker individuality preserving;individuality preserving speaker;based voice conversion;preserves speaker individuality;speaker synthesized;technique preserves speaker;preserving speaker;method preserves speaker;voice conversion;preserves speaker;speaker method based;non native speech;speaker individuality using;speech native;speech native japanese;voice conversion hidden;speaker method;method based voice", "pdf_keywords": ""}, "89b2a1dc68a7232bc3c68eb4b3e597f99755f7fe": {"ta_keywords": "quiz model;learns word phrase;phrase level representations;quiz model outperforms;text based models;learns word;introduce recursive neural;recursive neural network;recursive neural;qanta learns word;neural network rnn;representations combine sentences;rnn model;bowl quiz model;text based;simple text based;word phrase level;phrase level;questions bowl quiz;network rnn model;model qanta learns;network rnn;neural network;text;quiz;rnn model reason;rnn;neural;sentences;bowl quiz", "pdf_keywords": ""}, "0025b963134b1c0b64c1389af19610d038ab7072": {"ta_keywords": "learning preference functions;learning preference;preference functions based;algorithm learning preference;strategy web search;preference functions;web search engine;web search;learning combination search;search experts;search engine;query expansion strategy;search experts domain;pattern oriented algorithm;combination search experts;specific query expansion;functions based pattern;algorithm learning;based pattern oriented;results learning combination;based pattern;expansion strategy web;simple algorithm learning;combination search;query expansion;pattern oriented;strategy web;preference;results learning;search", "pdf_keywords": "spin orbit interaction;spin orbit coupling;xmath2he xmath3 interaction;xmath3 interaction attractive;interaction dynamics xmath0he;xmath3 interaction;dynamics xmath0he xmath1he;orbit coupling strengths;spin orbit;orbit coupling;effect spin orbit;dynamics xmath0he;orbit coupling value;reduce spin orbit;orbit interaction;orbit interaction dynamics;xmath1he xmath2he xmath3;xmath2he xmath3;range spin orbit;xmath1he xmath2he xmath;xmath3;xmath1he xmath2he;xmath1he;xmath2he xmath;xmath2he;xmath0he xmath1he xmath2he;reduce spin;effect spin;xmath0he xmath1he;wide range spin"}, "448406c38e739695b926d112b2b7aebd4e840322": {"ta_keywords": "identification speaker diarization;speaker diarization online;recognition automatic speaker;speaker recognition;automatic speaker recognition;speaker recognition automatic;speaker diarization information;speaker diarization;automatic speaker identification;new speaker diarization;diarization online meeting;online meeting recognizers;speaker identification;automatic speaker;speaker identification speaker;identification speaker;meeting recognizers;speaker input;speaker input improve;present new speaker;enable automatic speaker;meeting recognizers goal;diarization online;noise speaker input;background noise speaker;speaker;noise speaker;new speaker;diarization information;online meeting", "pdf_keywords": ""}, "06431546c21d7c2528aaa170c2e1078e0a82d12e": {"ta_keywords": "transfer languages benchmark;transfer languages;different transfer languages;multilingual zero shot;languages benchmark;target languages;languages benchmark problems;performance pretrained models;pretrained models;target languages diverse;multilingual zero;impact multilingual zero;set target languages;pretrained models fine;tuned different transfer;comparing performance pretrained;models fine tuned;performance pretrained;future benchmark;multilingual;immediate impact multilingual;languages diverse;languages;languages diverse unknown;zero shot systems;future benchmark designs;impact multilingual;benchmark designs;fine tuning crucial;benchmark", "pdf_keywords": "lingual translation tasks;multilingual models pretrained;cross translation task;translation tasks;translation multilingual optimization;machine translation multilingual;translation task cross;multilingual systems trained;fine tuned multilingual;multilingual zero shot;translation pre training;multilingual benchmarks;tuned multilingual models;translation tasks require;translation tasks fine;perform translation tasks;task cross translation;multilingual optimization significantly;neural machine translation;machine translation bridges;multilingual benchmarks identify;zero shot crosslingual;transferable language english;multilingual pre training;improving multilingual;standard multilingual benchmarks;translation multilingual;tuned multilingual;machine translation;multilingual optimization"}, "7570afa31c68e24fce1342b7d67c591787219bc1": {"ta_keywords": "sentences based abstractive;generating english sentences;way generated sentences;generated sentences;mechanism sentences generated;mechanism extractive summarization;abstractive mechanism sentences;sentences generated;sentences generated combination;generating english;abstractive mechanism extractive;documents resulting sentences;extractive summarization;generated combination abstractive;way resulting sentences;model generating english;summarization source documents;mechanism sentences;resulting sentences;extractive summarization source;resulting sentences read;sentences read expressed;english sentences based;based abstractive mechanism;sentences based;abstractive mechanism;combination abstractive;summarization;combination abstractive mechanism;based abstractive", "pdf_keywords": "extractive summarization train;summarization train abstractive;summarization train;extractive summarization structured;abstractive sentence summarization;abstractive text summarization;summarization english wikipedia;extracting extractive summarization;summarization structured;extractive summarization;extractive summarization identify;sentence summarization able;using extractive summarization;text summarization english;generates wikipedia text;summarization accuracy improved;text summarization accuracy;use extractive summarization;summarization able capture;text summarization;wikipedia extract;summarization accuracy;sentence summarization;wikipedia text conditioning;unstructured text summarization;summarization able;investigate extraction text;summarization english;generation wikipedia articles;summarization score"}, "4f9a4afc0ba500d839f7ee245513af9b87add8be": {"ta_keywords": "audio visual representations;learn audio visual;representations video audio;learning cross modal;learns similarity;video contrastive learning;learns similarity structure;visual representations video;contrastive learning cross;learn audio;approach learn audio;audio visual;cross modal discrimination;self supervised learning;method learns similarity;representations video;good representations video;underlying video contrastive;visual representations;modal discrimination optimizing;learn good representations;video audio;self supervised;structure underlying video;present self supervised;representations video vice;video audio group;contrastive learning;similarity feature spaces;underlying video", "pdf_keywords": "learning visual audio;visual audio representations;audio visual representations;learn audio visual;learning video audio;audio representations unconstrained;audio representations;learn visual audio;learning cross modal;similar video audio;representations video audio;representations multiple audios;discrimination video audio;video audio datasets;audio visual tasks;cross modal similarity;contrastive learning cross;visual modality similarity;natural correspondence audio;benchmark audio visual;modal discrimination video;audio visual instance;learn audio;audio representations contrasting;audio visual;visual audio;discrimination contrastive learning;unsupervised learning video;parallelizing video audio;approach learn audio"}, "2fb54dfcb1a62deac6565e82f2a87919d33074da": {"ta_keywords": "spin particle random;coupling spin particle;coupling spin;interaction particle random;effect coupling spin;random variable interaction;spin particle;particle random;particle random variable;random variable generated;spin;random variable described;random variable;variable random;equation random variable;coupling;described equation random;generated interaction particle;random variable random;variable interaction particle;equation random;variable random variable;interaction particle;effect coupling;particle;random;variable generated interaction;generated interaction;variable interaction;study effect coupling", "pdf_keywords": ""}, "5aa3c6ab6cc55c24bab224505e8ad5a4d9863706": {"ta_keywords": "focus attention decoding;attention decoding;learns focus attention;high performance text;performance text processing;encoder decoder training;task learning mtl;attention mechanism learns;using novel attention;attention decoding observe;decoder architecture high;novel attention mechanism;encoder decoder architecture;decoder training;learns focus;task learning;accelerate encoder decoder;multi task learning;novel encoder decoder;learning mtl architecture;focus attention;trained grapheme phoneme;text processing;architecture trained grapheme;novel attention;decoder training process;attention mechanism;novel encoder;decoder architecture;encoder decoder", "pdf_keywords": ""}, "86db47e228167439f15ee320a8a81d386f529a0c": {"ta_keywords": "language environment manipulation;environment manipulation tasks;environment manipulation;manipulation tasks based;manipulation tasks;framework able manipulate;framework language environment;language environment;arbitrary environments using;framework language;tasks based execution;arbitrary environments;pre training framework;environments using;generalize arbitrary environments;execution pre training;propose framework language;able manipulate generalize;manipulate generalize;manipulate generalize arbitrary;environments;training framework;manipulate;training framework able;based execution pre;manipulation;tasks based;based execution;environments using single;single execution pre", "pdf_keywords": "environment manipulation tasks;task sequence generation;tasks sequence generation;manipulation tasks models;manipulation lem tasks;manipulation tasks;language based environment;pretraining synthetic data;pretraining language based;generative language;language model execution;guided pretraining language;based environment manipulation;using generative language;tasks arbitrary environments;environments language;execution guided pretraining;environments using generative;environments language model;pretraining language;manipulation tasks formulate;synthetic data executor;execution guided;model execution guided;environment manipulation lem;generative language model;interaction environment tasks;environment manipulation;based execution guided;environment manipulation experimental"}, "2842c21e879ee581aa50704817454f21b539fc69": {"ta_keywords": "sentiment languages spoken;opinion sentiment languages;sentiment languages;language preference identify;preference identify languages;identifying languages preferred;expression opinion sentiment;classifiers identifying languages;preference expression opinion;identify languages preferred;language preference;use language preference;preferred languages spoken;languages preferred languages;opinion sentiment;languages spoken individual;preferred languages;identifying languages;identify languages;languages spoken languages;neutral languages;languages preferred;expression opinion;speakers use language;negative neutral languages;speakers classify positive;spoken languages;languages;study preference expression;sentiment", "pdf_keywords": ""}, "a010b3aa83d7d80e52c84d5f239f940eb33df904": {"ta_keywords": "speech recognition asr;encoders acoustic input;multimodal data augmentation;encoders acoustic;recognition asr architecture;automatic speech recognition;speech recognition;data augmentation network;acoustic input architecture;automatic speech;separate encoders acoustic;recognition asr;asr architecture trained;data augmentation;acoustic input symbolic;architecture multimodal data;augmentation network;architecture multimodal;end automatic speech;acoustic input;trained using emphsymbolic;augmentation network mmda;mmda support multimodal;support multimodal;multimodal;encoders;multimodal data;parameters architecture multimodal;utilizes separate encoders;asr architecture", "pdf_keywords": "training attention decoder;attention based encoder;augmenting speech languages;pronunciations augmenting data;augmentation improve encoder;encoder decoder networks;languages pretraining augmenting;attention decoder;encoder data augmentation;data augmenting encoder;method augmenting speech;acoustic encoder data;instead acoustic encoder;augmenting data encoder;augmenting encoder;encoder instead acoustic;acoustic encoder;attention decoder networks;pronunciations augmenting;significantly improves encoderwe;improve encoder;perform pronunciations augmenting;augmenting encoder instead;improvement multilingual training;improve encoder decoder;encoder decoder network;augmenting speech;decoder networks;improves encoderwe;acoustic encoder proposed"}, "f784ab218692364b9c8a1f8064809e4524116f3a": {"ta_keywords": "byzantine decentralized training;secure byzantine;secure byzantine decentralized;protocol secure byzantine;byzantine lebwohlert attacks;byzantine attackers;byzantine decentralized;presence byzantine attackers;bounds resistance byzantine;byzantine lebwohlert;decentralized training;resistance byzantine lebwohlert;novel protocol secure;byzantine;protocol secure;decentralized training emphasizes;resistance byzantine;presence byzantine;lebwohlert attacks demonstrate;modeling presence byzantine;attacks demonstrate;rigorously analyze protocol;lebwohlert attacks;attacks;attacks demonstrate practical;novel protocol;protocol;attackers;decentralized;protocol provide theoretical", "pdf_keywords": "attackers distributed training;distributed training zero;attacks federated learning;distributed deep learning;learning zero trust;distributed training;attack distributed networks;distributed deep;learning distributed;limits distributed learning;distributed learning;distributed training furthermore;distributed training allows;approach distributed deep;distributed attacks;byzantine attackers distributed;distributed learning problems;byzantine framework training;training protocol distributed;distributed learning consistency;distributed attacks valid;attacks distributed;distributed learning single;distributed learning underlying;attackers distributed;scale distributed learning;attack distributed;learning distributed systems;scalable distributed byzantine;deep learning zero"}, "2dd1504d54f8d7e01e1323a9f876f35bb86356da": {"ta_keywords": "incentives given transportation;disutility monetary incentives;effect incentive policy;incentive policy local;incentive policy;monetary incentives given;monetary incentives;effect incentive;evaluate effect incentive;incentives given;cities model;transportation authority effectiveness;incentives;cities model learned;incentive;given transportation authority;level disutility monetary;landscape cities model;disutility monetary;transportation;given transportation;evaluate sensitivity agent;data driven;transportation authority;simulation framework evaluate;policy local area;agent level disutility;simulation framework;sensitivity agent level;driven approach used", "pdf_keywords": ""}, "c9d65eee1b5df8ccda87c024b88e1b620099b316": {"ta_keywords": "humans instructions robots;natural language commands;grounded natural language;instructions robots;instructions robots using;robots;humans robots;natural language;gap humans robots;language commands instruction;setting humans instructions;robots instantiate framework;propose neural architectures;unrestricted natural language;language commands;architectures interpreting contextually;commands instruction sequences;robots using;propose neural;neural architectures interpreting;contextually grounded natural;humans robots instantiate;robots instantiate;humans instructions;contextually grounded;commands instruction;communication gap humans;neural architectures;interpreting contextually grounded;language", "pdf_keywords": ""}, "f6db40e1f0477d27a34240b2e11d6893b9e85b7b": {"ta_keywords": "combat air pollution;protecting environment air;air purifiers;environment air pollution;hydrogen filter automatically;air pollution improving;simple hydrogen filter;hydrogen filter;air pollution environment;air pollution;impact air pollution;improving quality air;air purifiers demonstrate;quality air purifiers;pollution environment simple;environment air;pollution improving;pollution environment;device simple hydrogen;removes dangerous contaminants;pollution improving quality;contaminants air simple;dangerous contaminants air;contaminants air;pollution;protecting environment;quality air;environment simple device;filter;air simple", "pdf_keywords": ""}, "9a36d6b76b3b223aa877b4243e5fdfe5c998689e": {"ta_keywords": "repulsive interaction xmath7;xmath9 xmath10 mixing;xmath10 mixing;xmath11 mixing;xmath0 xmath11 mixing;fluid repulsive interaction;xmath6 mixing;xmath6 mixing fluid;xmath10 mixing used;xmath5 xmath6 mixing;xmath11 mixing xmath0;dynamics xmath0 xmath1;mixing xmath0;study dynamics xmath0;dynamics xmath0;strong attraction xmath0;attraction xmath0 xmath11;mixing fluid repulsive;interaction xmath7 xmath8;attraction xmath0;interaction xmath7;repulsive interaction;xmath8 xmath9 xmath10;xmath10;xmath9 xmath10;xmath1 xmath2;xmath1 xmath2 xmath3;xmath9;xmath1;xmath2 xmath3 xmath4", "pdf_keywords": ""}, "3f59122d4cac12f27ad6ae379deefd9f3fa81f29": {"ta_keywords": "language commands robot;commands robot simulated;commands robot;robot simulated scenes;natural language commands;robot;natural language command;execution robot;goals execution robot;execution robot demonstrate;robot demonstrate fidelity;robot simulated;simulated scenes;robot demonstrate;language commands;natural language;learning representations;scenes;commands;sourced natural language;learning representations convert;framework learning representations;convert natural language;representations convert natural;learning;language command;language command sequence;command sequence intermediate;representations;language", "pdf_keywords": "language commands robot;agents deep learning;robot learned;robot learned encoder;plans natural language;visualizing robot actions;deep neural;goals execution robot;state robot learned;commands robot;robot tasks;commands robot simulated;robot based prediction;visualizing goal robot;trained supervised demonstration;goal robot;robot actions;agents generative neural;novel robotic tasks;natural language commands;learning context agents;robot simulated scenes;robotic tasks specified;deep learning context;possible robot tasks;predicting motion robot;robotic tasks;trained compute representations;robot;agents deep"}, "0b2ff02ab23e5c9910b98fb87c4d58045dbe89ce": {"ta_keywords": "speech recognition challenge;quantitative speech recognition;reverb challenge combines;speech recognition;reverb challenge;quantitative speech;field quantitative speech;reverb;publication reverb challenge;recognition challenge perform;recent publication reverb;recognition challenge;publication reverb;recognition;speech;challenge perform quantitative;analysis following features;challenge combines best;challenge combines;features;features following;following features;features following features;best features emerging;best features;features following following;following features following;features emerging field;features emerging;challenge perform", "pdf_keywords": ""}, "d72a1579074a1a2bc500f257474144b1957d5166": {"ta_keywords": "learning based coded;coded computation enables;coded computation framework;coded computation;based coded computation;resilience nonlinear computations;widely deployed neural;deployed neural networks;nonlinear computations;deployed neural;coded;nonlinear computations implement;based coded;neural network inference;neural networks variety;resilience nonlinear;impart resilience nonlinear;open source prediction;neural networks;neural;slowdowns occur neural;computation enables;object localization;computation enables accurate;neural network;computation framework impart;computation framework;networks;results learning;recognition object localization", "pdf_keywords": ""}, "c5141ed9ed785a6a1df61b36883e6dfa19a59ff7": {"ta_keywords": "synthetic overlap datasets;overlap datasets separation;multiple channel speech;datasets separation;datasets separation single;speech evaluate quality;overlap datasets;channel speech;channel speech evaluate;data training robust;quality synthetic overlap;training robust;training robust models;synthetic overlap;speech evaluate;quality resulting datasets;data training;single multiple channel;separation single;mixture kolmogorov;multiple channel;overlap;mixture kolmogorov smirnov;datasets using mixture;separation single multiple;separation;resulting datasets;robust models;using mixture kolmogorov;robust models generalize", "pdf_keywords": "speech separation;speech separation evaluation;speech separation fundamental;standard speech separation;channel speech separation;learning based separation;predict utterances;speech separation technologies;large scale utterances;predict predict utterances;large corpus utterances;large scale utterance;predict utterances given;utterance mixtures corpus;mixtures corpus utterances;utterance mixtures;utterances corpus;speakers based neural;scale utterance mixtures;realistic conversational speech;large corpus;utterances given speaker;scale utterances corpus;speech bulk;corpus utterances;networks learn speaker;source separation;mixtures corpus;single channel speech;scale utterances"}, "77000dba4b0638bb8f4222efcd731e040938c846": {"ta_keywords": "prediction driver interaction;driver cognitive load;reduce driver cognitive;prediction driver intention;driver interaction;prediction driver action;interface based driving;driver action intention;driver cognitive;driver interaction car;interface car human;driver action;driver intention;human machine interface;interaction car human;use prediction driver;interface use prediction;prediction driver;interface car;driving history prediction;presented prediction driver;driver;car human machine;cognitive load augmenting;interaction car;car human;cognitive load;ways reduce driver;interface use;based driving history", "pdf_keywords": ""}, "ab8be9e585e599db99d8451e63a2311d88ff9293": {"ta_keywords": "cache clusters twitter;workloads cache clusters;cache clusters;cache clusters based;memory cache clusters;cache workloads collecting;analysis cache workloads;cache workloads;cache workloads high;observe cache workloads;characterize cache workloads;thought characterize cache;cache;workloads cache;thought observe cache;153 memory cache;analysis cache;distribution observe cache;cache workloads cache;observe cache;characterize cache;memory cache;comprehensive analysis cache;clusters twitter;traffic pattern popularity;clusters based traffic;workloads;clusters;workloads high;workloads collecting", "pdf_keywords": ""}, "c612905cffc5a9aa9f0d8ac7ce1fd17f90413dab": {"ta_keywords": "modeling argumentative dialogue;argumentative dialogue explicitly;argumentative dialogue;modeling argumentative;dialogue explicitly models;predicting argument;dialogue explicitly;dialogue;argument goal predicting;architecture modeling argumentative;predicting argument successfully;goal predicting argument;argumentative;present neural architecture;argument goal;interplay opinion holder;neural architecture;models interplay opinion;neural architecture modeling;challenger argument goal;attention model;neural;detection attention model;attention;present neural;opinion holder;detection attention;explicitly models interplay;attention model identifies;argument successfully changes", "pdf_keywords": "modeling argumentative dialogue;argument modeling interaction;predicting argument;argument modeling;modeling argumentative;argumentation model;predict argument opposing;argumentation methodology detects;view argumentation methodology;argumentative dialogue explicitly;argumentative dialogue;simple argumentation model;view argumentation;argument goal predicting;regions argument modeling;argumentation model shown;predicting argument successfully;argumentation methodology;dialogue explicitly models;architecture modeling argumentative;representation comment interaction;goal predicting argument;arguments text predict;change view argumentation;predict argument;vulnerable regions argumentation;argumentation method based;regions argumentation;dialogue explicitly;argumentation"}, "5547eff5376c56358be56f8bcc3a4b6ce4600bb5": {"ta_keywords": "robust adiabatic speech;speech recognition asr;robust automatic speech;adiabatic speech processing;automatic speech recognition;speech recognition;automatic speech;speech processing;recognition asr toolkits;speech recognition chapter;recognition asr;speech processing covering;asr toolkits;adiabatic speech;introduction robust automatic;asr toolkits purpose;robust automatic;robust adiabatic;art robust adiabatic;state art robust;introduction robust;speech;recognition;recent advances toolkits;robust;asr;field robust automatic;advances toolkits;provides introduction robust;recognition chapter", "pdf_keywords": ""}, "f430c43018f17cabccd3a2e9258aff3da508afe1": {"ta_keywords": "harmonic potential particle;potential particle harmonic;particle harmonic potential;motion particle harmonic;harmonic potential wave;particle assumed harmonic;particle harmonic;harmonic potential corresponding;harmonic potential;assumed harmonic potential;potential wave function;potential corresponding wave;wave function particle;potential wave;potential particle assumed;potential particle;assumed harmonic;wave function consider;wave function;consider motion particle;harmonic;dynamics motion particle;motion particle;corresponding wave function;wave function determined;function particle assumed;function particle;function determined wave;determined wave function;potential corresponding", "pdf_keywords": ""}, "4b34a4cc5bc9defb0f530d61f9b0f843071e227c": {"ta_keywords": "vaginal bleeding delivery;menstrual hygiene excessive;menstrual hygiene;excessive vaginal bleeding;vaginal bleeding related;vaginal bleeding;hygiene excessive vaginal;association menstrual hygiene;bleeding related fact;bleeding delivery higher;relationship menstrual hygiene;incidence excessive vaginal;bleeding delivery;bleeding delivery high;bleeding related;hygiene excessive;excessive vaginal;hygiene;bleeding;investigates relationship menstrual;menstrual;vaginal;relationship menstrual;association menstrual;suggest incidence excessive;india analysis;incidence excessive;family health survey;strong association menstrual;results suggest incidence", "pdf_keywords": ""}, "39025112f6a40d8aae38f2e966bb27cbc35ea25d": {"ta_keywords": "dynamics xmath0 xmath1;study dynamics xmath0;dynamics xmath0;xmath2 xmath3 xmath4;xmath16 xmath17 xmath18;xmath1 xmath2 xmath3;xmath15 xmath16 xmath17;xmath9 xmath10 xmath11;xmath17 xmath18 xmath19;xmath3 xmath4 xmath5;xmath16 xmath17;xmath14 xmath15 xmath16;xmath10 xmath11 xmath12;xmath0 xmath1 xmath2;xmath17 xmath18;xmath4 xmath5 xmath6;xmath17;xmath13 xmath14 xmath15;xmath8 xmath9 xmath10;xmath1 xmath2;xmath12 xmath13 xmath14;xmath7 xmath8 xmath9;xmath3 xmath4;xmath11 xmath12 xmath13;xmath5 xmath6 xmath7;xmath6 xmath7 xmath8;xmath10 xmath11;xmath13 xmath14;xmath4 xmath5;xmath0 xmath1", "pdf_keywords": ""}, "0db557c4315b1e08ef65ff15b96eb7630014bf72": {"ta_keywords": "digressions discussion detecting;detecting unnecessary utterances;discussion detecting unnecessary;discussion detecting;utterances having dialogue;having dialogue;dialogue intervene performance;unnecessary utterances;having dialogue intervene;utterances;dialogue intervene;dialogue;typical automatic summarization;automatic summarization;automatic summarization method;unnecessary utterances having;eliminating digressions discussion;utterances having;summarization method;summarization;detecting unnecessary;digressions discussion;discussion;eliminating digressions;method eliminating digressions;digressions;propose method eliminating;effective typical automatic;evaluation shows proposed;detecting", "pdf_keywords": ""}, "09a169c853e24b3a5196eefeab4c94eaac744cda": {"ta_keywords": "identify political ideology;method identify political;political ideology based;ideology based analysis;political ideology;identify political;linguistic patterns words;model identify political;patterns words dataset;analysis linguistic patterns;model linguistic patterns;linguistic patterns;words dataset;recursive neural network;ideology based;recursive neural;words dataset 100;use recursive neural;words model;analysis patterns words;000 words model;ideology;patterns words;words model outperforms;network model linguistic;neural network;model linguistic;100 000 words;000 words use;000 words", "pdf_keywords": ""}, "3a72f1346f3cd41e14b45c7fba5259bc77357ed4": {"ta_keywords": "determinate clauses learning;clauses model pac;clauses learning linear;model pac learning;clauses learning;clauses learning boolean;pac learning;pac learning able;determinate clauses model;learning boolean polynomial;clauses model;polynomial determinate clauses;quadratic determinate clauses;learning boolean;learning linear quadratic;learning linear;nonrecursive determinate clauses;learning linear nonrecursive;determinate clauses;boolean polynomial determinate;present model pac;learning;pac;model pac;clauses;able support learning;support learning linear;boolean polynomial;learning able;support learning", "pdf_keywords": "learning recursive logic;recursive logic programs;clauses learnable learnable;pac learning recursive;recursive clauses predicting;clauses pac learning;recursive clauses prediction;determinate recursive programs;programs linearly recursively;learning recursive;predicting clause recursive;class clauses learnable;recursive programs;recursive programs containing;ary recursive clauses;clauses learnable;linearly recursive clauses;programs pac learnability;learning logic programs;learning programs bound;recursive clauses class;clauses learning possible;learnability pac learning;recursively closed clauses;nonrecursive clause learning;clauses class programs;clause hard learning;recursive logic;learning linear recursive;program linear recursive"}, "b62d63580b81a2cbb20c3c1593dd62d118e4cb07": {"ta_keywords": "code pre trained;trained language models;pre trained language;trained language model;language models framework;target similarity;language models;using target similarity;generation code pre;example selection;target similarity tst;trained language;uses example selection;language model;languages sql;languages sql queries;generation code;framework generation code;shot examples training;output pre trained;examples training;world languages sql;code pre;real world languages;shot examples;example selection method;discovery shot examples;language model evaluate;similarity tst uses;similarity tst", "pdf_keywords": "tuning similarity semantic;similarity semantic programs;language generation semantic;generation semantic parsing;automatic parser generation;semantic programs;generation semantic;semantic parsing;free semantic;semantic language;parser generation;large language models;stream semantic language;trained language models;natural language generation;free semantic constraints;predicting accuracy language;semantic parsing sentiment;code stream semantic;parser generation process;language model train;automatic parser;semantic language method;syntactic semantic;language models;similarity semantic;context free semantic;syntactic unstructured relational;semantic constraints grammar;semantic processing crucial"}, "af85c67a1f30f8359be1091234118492b511a088": {"ta_keywords": "propagation charged particle;charged particle turbulent;zeeman field propagation;turbulent medium particle;particle turbulent medium;particle turbulent;effect zeeman field;field propagation charged;propagation charged;turbulent medium;stationary medium particle;medium particle assumed;medium particle;zeeman field;charged particle;field propagation;effect zeeman;particle assumed vicinity;turbulent;particle assumed;particle;zeeman;propagation;vicinity stationary medium;study effect zeeman;stationary medium;vicinity stationary;assumed vicinity stationary;stationary;field", "pdf_keywords": ""}, "cd9e1eac4c93a314254cf8a8682ed5f01b6a808f": {"ta_keywords": "reasoning knowledge base;embedding knowledge base;embedding knowledge;reasoning knowledge;novel embedding knowledge;knowledge base;improvement reasoning accuracy;approach reasoning knowledge;knowledge base set;reasoning accuracy;knowledge base kb;reasoning accuracy state;significant improvement reasoning;novel approach reasoning;natural language processing;improvement reasoning;empirically demonstrate embedding;memorization highly specific;embedded triples;relies novel embedding;embedding allows;embedding allows significant;natural language;embedded triples use;memorization highly;reasoning;novel memorization highly;knowledge;demonstrate embedding allows;set embedded triples", "pdf_keywords": "embed knowledge bases;new relational representation;embed knowledge;relational representation set;relational representation;inference systems embed;knowledge bases representations;knowledge representation;knowledge representation representation;systems embed knowledge;query embedding qe;generalization query embedding;query embedding;entailed deductive reasoning;qk knowledge representation;representation entities kb;knowledge representation tasks;novel query embedding;representation set entities;compositional representation entities;method query embedding;knowledge bases;new approach entailment;logical query relational;qe practice embeddings;representation entities;representation generalized knowledge;generalized knowledge base;entailed deductive;logical inference systems"}, "9712ebfbc4f86c68403f64918463edad3e553ac6": {"ta_keywords": "centralized tracking process;centralized tracking;problem centralized tracking;process known distribution;tracking process;tracking process unknown;known distribution algorithms;parametric distribution proposed;unknown parametric distribution;known distribution;process unknown parametric;distribution algorithms able;parametric distribution;kolmogorov smirnov parameter;tracking;distribution algorithms;distribution proposed algorithms;process known;predict kolmogorov smirnov;smirnov kolmogorov;able predict kolmogorov;predict kolmogorov;possible process known;kolmogorov smirnov kolmogorov;problem centralized;kolmogorov;distribution proposed;kolmogorov smirnov;smirnov parameter minimum;centralized", "pdf_keywords": "dynamic sensor subset;observation multiple sensors;set sensors stochastic;sensors stochastic;selection centralized tracking;sensors stochastic version;observation sensor shared;centralized tracking;sensor subset selection;active sensor selection;subset active sensors;algorithms centralized tracking;active sensors computationally;method centralized tracking;sensor subset;approach active sensing;sensor networks;centralized tracking time;dynamic sensor;sensor based generalized;centralized tracking process;active sensing;energy efficient tracking;assumption observation sensor;sensor selection;noisy sensor networks;selecting single sensor;sensor shared sensors;sensors computationally feasible;process active sensing"}, "873dff010c00f0601d6939324929eeabb1ddbd6e": {"ta_keywords": "secret sharing networks;sharing secret sharing;secret sharing;algorithm sharing secret;sharing secret;sharing networks based;distribute shares participants;sharing networks;distributed algorithm sharing;shares network participants;distribute shares network;distribute shares;disseminate shares network;shares participants;shares network;algorithm sharing;transmit shares network;distributed algorithm;transmit shares;able transmit shares;sharing;shares participants proposed;distributed;used distribute shares;able disseminate shares;networks based exchange;distribute;shares;messages participants algorithm;network participants", "pdf_keywords": "network secret sharing;algorithm secret sharing;secret sharing network;secret sharing;share shared secret;algorithm sharing secret;termed secret sharing;shared secret data;shared secret;sharing secret share;threshold secret sharing;sharing secret information;sharing secret defined;sharing secret;secret sharing general;capable sharing secret;sharing secret data;problem secret sharing;required secret sharing;secret sharing setting;sharing secret messages;secret information network;secret share;secret data collective;secret sharing bounded;network secret;solution sharing secret;efficient algorithm secret;sharing network efficient;shares secret participants"}, "35cb2b9febada179689724c78dfe31d9fa3f74c4": {"ta_keywords": "codes erasure coding;erasure coding;local reconstruction codes;erasure coding called;reconstruction codes;codes erasure;reconstruction codes lv;set codes erasure;coding called local;new set codes;coding;set codes;coding called;local reconstruction;codes lv lv;codes lv;erasure;called local reconstruction;lv based parity;codes;lv symmetric parity;lv symmetric;parity symmetric;reconstruction;symmetric parity;parity symmetric parity;based parity new;symmetric parity symmetric;form lv symmetric;parity new form", "pdf_keywords": "erasure coding;erasure coding codes;fundamental erasure coding;erasure coding lemaitrum;erasure coding fragments;standard erasure coding;implementing erasure coding;reconstruction codes;erasure coding code;erasure coding multiple;associated erasure codingwe;local reconstruction codes;erasure coding turing;implementation erasure coding;reconstruction codes lrc;set erasure coding;number erasure coding;erasure codingwe;reconstruction codes lv;theoretically decodable failure;decodable failure patterns;coding code lemaitaria;erasure codingwe paxos;coding equations achieves;able decode information;capable implementing erasure;information theoretically decodable;theoretically decodable;description storage communication;decodable failure"}, "35c6bdab35e8fd4e982302b5270da3c8098c58b1": {"ta_keywords": "modular architecture instruction;architecture instruction;natural language instructions;modular architecture improves;modularization;architecture instruction following;modularization modular architecture;propose modular architecture;modularization modular;architecture improves generalization;modular architecture;instructions power modularization;propose modular;instruction;generalization novel compositions;language instructions power;instruction following combines;modular;compositions environments;natural language;advantages natural language;novel compositions environments;instruction following;improves generalization novel;compositions environments environments;compositions;architecture improves;improves generalization;language instructions;novel compositions", "pdf_keywords": ""}, "32feca141fce06c6588b4014d27953a3fc25f19b": {"ta_keywords": "model natural language;natural language humans;language model natural;natural language summaries;accurate language summaries;humans accurate language;natural language;language summaries;language model;similarly natural language;language humans based;language summaries judged;language model present;physical systems language;language humans;present language model;accurate language;systems language model;language;text;systems language;summaries judged accurate;humans based interaction;text text;text text alternatives;accurately predict happens;text alternatives;text alternatives 10;present language;summaries", "pdf_keywords": "learns physical commonsense;grounding language models;language model learns;learn language model;language grounded objects;knowledge ground language;learning physical commonsense;language models demonstrate;trained language;language model trained;integrating language;pretrained language model;interaction natural language;physical commonsense knowledge;describing natural language;carrying natural language;grounding language;model language;learns world dynamics;interaction transferring language;objects language model;integrating language model;language model;trained language models;language models;language annotations;interface language model;language annotation;knowledge world language;language object grounded"}, "d9c2242e3aa17db649c92d7d4db46509f3d203db": {"ta_keywords": "robust reinforcement learning;learning stochastic decision;reinforcement learning stochastic;stochastic decision problems;decision problems robust;stochastic decision;problems robust reinforcement;robust reinforcement;robust robust reinforcement;algorithm reinforcement learning;keywords stochastic decision;reinforcement learning framework;learning stochastic;reinforcement learning;algorithm reinforcement;confidence robust learning;robust learning;reinforcement learning upper;auxiliary cost constraints;robust learning xmath0;cost constraints algorithm;problems decision maker;decision problems;stochastic;cost constraints;decision maker select;decision problems decision;decision maker;keywords stochastic;learning framework proposed", "pdf_keywords": "bandit problems algorithm;learning policies constrained;reinforcement learning constraints;compute regret learning;linear regret algorithm;algorithm learning policies;bandit problem provide;bandit problems;confidence reinforcement learning;safe reinforcement learning;pseudo regret bound;bandit problem;kernel optimal policy;bound reward constraint;regret bound sufficient;regret algorithm;transition multiarmed bandit;learning policies;reward costs stochastic;optimal policy randomized;selection optimal policy;reward function constraints;analyzed bounding regret;regret performance algorithm;linear regret log;armed bandit problem;reward constraint costs;armed bandit problems;reinforcement learning settings;constraints learning probability"}, "b990517fbbf4499861d7aa00407b0422874ab990": {"ta_keywords": "predicting words simultaneous;predicting words;method predicting words;words simultaneous;words simultaneous leave;supervised sequence tag;untranslated;supervised sequence;simultaneous leave untranslated;untranslated shallow fiel;leave untranslated;use supervised sequence;leave untranslated shallow;untranslated shallow;sequence tag;novel method predicting;supervised;method predicting;predicting;simultaneous;shallow fiel fiel;shallow fiel;words;use supervised;sequence;fiel;fiel fiel;2014 proposed method;based use supervised;fiel fiel environment", "pdf_keywords": "learns untranslated;predicting untranslated terms;predicts untranslated terms;learns untranslated data;predict untranslated terms;predicting untranslated;interpreter predicts untranslated;model learns untranslated;predict untranslated;predicts untranslated;learns totranslated;predict optimal translation;automatically predicting terminology;model predicting untranslated;supervised sequence tagger;supervised sequence taggers;untranslated data annotated;learns totranslated given;translation large corpus;annotated data learns;model learns totranslated;predicting terminology;simultaneous interpreter predicts;untranslated terms;untranslated terms corpora;corpus translators;text model learns;sequence tagging models;stochastic translation translation;term memory"}, "fb38451ff87254ac1ff15e79154ef958b4efb6a6": {"ta_keywords": "computational graph formalism;dynamics simple classification;graph formalism;graph formalism model;computational graph;model computational;powerful computational graph;patterns multilayer perceptrons;nonlinear dynamics;nonlinear dynamics simple;multilayer perceptrons;formalism model computational;computational;nonlinear;problems nonlinear dynamics;problem nonlinear dynamics;predicting;predicting predicting;perceptrons;multilayer perceptrons use;graph;powerful computational;dynamics;dynamics simple;model computational power;nonlinear dynamics cover;problems nonlinear;perceptrons use;classification;problem predicting", "pdf_keywords": ""}, "e2d770b9ab691753a7ec1eb439185303118c8455": {"ta_keywords": "multilingual agent assistant;multilingual agent assistants;multilingual agents;multilingual agent;support multilingual agents;enabled multilingual agent;resulting multilingual agent;multilingual agents resulting;agents resulting multilingual;learning enabled multilingual;able support multilingual;support multilingual;multilingual;new languages;enabled multilingual;agent assistant;translation language;new languages proposed;resulting multilingual;perform translation language;agent assistants;creation new languages;agent assistants designed;languages;languages proposed;agent assistant applied;languages proposed approach;machine learning enabled;machine learning architecture;machine learning tasks", "pdf_keywords": ""}, "c8f78575bfb642b2dab6ed542a683ade9527c17d": {"ta_keywords": "matching organ;matching organs;matching organ important;matching organs important;markets organ donation;process matching organs;mechanism perform matching;organ donation;matching;markets organ;selection organ organs;perform matching;selection organ;currently consideration organ;organs important selection;important selection organ;consideration organ tissue;consideration organ;matching dynamic;organ donation health;organ tissue authority;organ organs;perform matching dynamic;matching dynamic sided;organ;organ organs important;organ tissue;financial markets organ;organ important;organs", "pdf_keywords": ""}, "4cc07c367e4a1f932e159678ef711e1802edf49f": {"ta_keywords": "model decomposable spoken;decomposable spoken language;decomposable spoken;decomposable tasks outperforms;decomposable task hierarchy;decomposable tasks model;decomposable tasks;decomposable task;model decomposable tasks;spoken language tasks;structure decomposable task;spoken speech wireless;speech wireless;task specific utility;task hierarchy designed;task hierarchy;language tasks;tasks model based;speech wireless signal;language tasks present;tasks model;variety decomposable tasks;tasks outperforms existing;novel model decomposable;sub task specific;decomposable;model decomposable;tasks outperforms;robust sub task;tasks present", "pdf_keywords": "spoken intent prediction;speaker utterance subtasks;predict spoken spoken;models spoken intent;speech intent systems;predict spoken;task spoken intent;utterance subtasks;spoken spoken intents;utterance shake models;intent prediction novel;intent prediction task;spoken intents;utterance subtasks splits;intent prediction;splits task spoken;benchmarks decomposable tasks;home voice assistant;spoken intents commands;quality utterances;voice assistant;task spoken;aims predict spoken;spoken intent;speech intent;characterizing spoken;tasks existing benchmarks;quality utterances given;characterizing spoken language;data quality utterances"}, "8fa6b06cb96e5ae98dfff1c50f6940ef43af223f": {"ta_keywords": "novel storage data;storage data minimally;storage data;proton hdfs asynchronous;data proton hdfs;novel storage;hdfs asynchronous;hdfs;hdfs asynchronous asynchronous;data minimally redundant;storage;asynchronous cascade code;present novel storage;proton hdfs;asynchronous cascade;redundant high performance;asynchronous asynchronous cascade;asynchronous cascade cascade;implemented data proton;cascade code available;cascade code;cascade cascade code;data proton;hitchhiker based asynchronous;data minimally;cascade;cascade cascade;cascade cascade cascade;hashi implemented data;implemented data", "pdf_keywords": ""}, "1941f5b053ccc80fa44980d38ac074145591b4ec": {"ta_keywords": "sentence embeddings improves;semantic similarity evaluations;distinct semantic similarity;evaluations semantic similarity;model sentence embeddings;sentence embeddings;semantic similarity;semantic similarity model;embeddings improves;similarity evaluations;separation parallel sentences;embeddings improves state;novel variational probabilistic;parallel sentences;inference networks;distinct semantic;inference networks experiments;parallel sentences uses;distributions inference networks;embeddings;similarity model;sentences;similarity;variational probabilistic;evaluations semantic;set distinct semantic;variational probabilistic framework;similarity model uses;semantic;benchmark datasets demonstrate", "pdf_keywords": "learning sentence embeddings;learn sentence embeddings;sentences semantic embedding;predicting semantic embeddings;semantic sentence embeddings;embedding sentences high;sentence embeddings monolingual;predict semantic embeddings;embeddings monolingual data;semantic embeddings text;embedding sentences;semantic embedding;semantic embeddings;sentence embeddings encodes;embeddings monolingual;learning sentence representations;embedting sentence embeddings;sentence embeddings;sentence embeddings encoded;semantic embedding model;neural machine translation;method embedding sentences;latent semantic vectors;parallel sentences semantic;parallel sentences languages;crosslingual translation baselines;corpus text generative;sentences latent semantic;semantic vectors;text generative"}, "553028f7f7c850371379c621e40d7d00e75303a6": {"ta_keywords": "interference multilingual models;multilingual models negative;negative interference multilingual;multilingual models;interference multilingual;cross lingual transferability;lingual transferability;lingual transferability alleviates;better cross lingual;interference adding language;language specific layers;multilingual;cross lingual;mitigated adding language;lingual;meta learning;present meta learning;models negative interference;adding language specific;meta parameters training;adding language;language specific;language specific parameters;meta learning algorithm;language;specific layers meta;study negative interference;models negative;layers meta;layers present meta", "pdf_keywords": "multilingual models mitigating;interference multilingual models;multilingual models negative;multilingual models surprisingly;sparse multilingual models;multilingual models demonstrate;multilingual models;multilingual models propose;multilingual models present;multilingual models specifically;multilingual models essential;suggest multilingual models;multilingual models exploits;negative interference multilingual;models multilingual benchmark;models multilingual benchmarks;multilingual models gradient;multilingual models used;phenomenon multilingual models;robustness multilingual models;learn sparse multilingual;models multilingual;bilingual models multilingual;multilingual models using;monolingual bilingual models;computational model crosslingual;sparse multilingual;capacity multilingual models;transferability multilingual models;multilingual models degrade"}, "1606dc1e966ad59dd96dc8e74722dca06b1f1a58": {"ta_keywords": "model emergence biologically;emergence biologically active;emergence biologically;emergence population individuals;emergence population;model emergence;based emergence population;model based emergence;emergence;phase evolution population;evolution population model;present model emergence;evolution population individuals;evolution population;based emergence;evolution;active phase evolution;population model constructed;phase evolution;initial state;initial state model;population individuals model;single initial state;individuals single initial;population model;basis evolution population;biologically active phase;individuals model;initial;state model constructed", "pdf_keywords": "predicting longitudinal influences;model predicting longitudinal;model moral interventions;model predict longitudinal;evolutionary causal matrices;longitudinal influences;predicting longitudinal;predict longitudinal;education longitudinal;evolutionary causal;intervention model predicts;approach predicting longitudinal;based evolutionary causal;adolescents voluntary service;longitudinal influences different;predicting longitudinal outcomes;predict longitudinal outcomes;interventions model;interventions simulation model;interventions model based;moral interventions adolescents;education longitudinal outcome;elementary interventions model;intervention model;influences educational interventions;designed predict longitudinal;study longitudinal;predict future intervention;adolescents voluntary;longitudinal outcomes interventions"}, "e3480d9395e692833b722b2e957d51139985f310": {"ta_keywords": "generative question answering;question answering macaw;question answering;question answer questions;open source generative;macaw designed answer;answering macaw designed;answering macaw;answer questions minimal;answer questions;answering;machine learning open;questions able answer;answer variety questions;generative;questions minimal training;machine learning freely;generative question;designed answer general;source generative;learning open source;designed answer;answer variety;source generative question;learning open;answer general question;learning freely available;question answer;learning freely;machine learning", "pdf_keywords": "answer characterization trained;generative question answering;question answering macaw;question answering;answers language;question training;answers language modelwe;insightful answers language;generative answer;naturally generated answers;language learning macaw;generative answer answer;answer answer characterization;question training process;answer characterization;generated answers;pretrained language model;models language learning;question answer set;language model ground;challenges large language;generated answers question;answering;language models;new language model;answering macaw;answers question text;ability answer questions;answer questions text;new generative"}, "6d19d73909ffaa6c94cae6a2535ed52d138cb63b": {"ta_keywords": "domain dialog utilizing;dialog corpora;dialog utilizing;users dialogue agent;multi domain dialog;dialog users dialogue;dialogue agent utilizes;dialog utilizing real;domain dialog;dialogue agent;dialog;scripts twitter conversations;dialog corpora extracted;based interaction dialog;dialog users;appropriate dialog pair;human conversation examples;users dialogue;appropriate dialog;ensure dialog corpora;movie scripts twitter;dialogue;conversation examples movie;interaction dialog users;human human conversation;human conversation;statistical machine translation;twitter conversations;twitter conversations form;dialog pair", "pdf_keywords": ""}, "d1678032a9eee94ec0a9c54fb008e1addc7213d4": {"ta_keywords": "parametric utility learning;utility learning;utility learning method;learning based heteroskedasticity;utility learning based;based heteroskedasticity inference;heteroskedasticity inference;heteroskedasticity inference use;performance utility learning;based heteroskedasticity;heteroskedasticity;parametric utility;energy efficient behavior;encourage energy efficient;social game experiment;framework parametric utility;utility;learning based;estimators;learning method;variance estimators;behavior building occupants;estimators performance utility;designed encourage energy;energy efficient;encourage energy;learning;learning method using;estimate variance estimators;variance estimators performance", "pdf_keywords": ""}, "33cd5965745dc2e8bb8d0400d0b3c18d4e6369d4": {"ta_keywords": "cache clusters;performance cache clusters;cache clusters context;cache workloads based;cache workloads;performance cache;investigate performance cache;characterize cache workloads;approaches characterize cache;cache;workloads based traffic;characterize cache;ttl popularity distribution;clusters;popularity distribution size;popularity distribution;workloads based;clusters context;workloads;traffic pattern time;clusters context business;popularity;live ttl popularity;traffic pattern;ttl popularity;based traffic pattern;investigate performance;performance;traffic;based traffic", "pdf_keywords": ""}, "0b79cb7fe16aa8b99d521989f39e49034394f701": {"ta_keywords": "review human computation;human computation;human computation research;human computation emphasis;field human computation;human computation book;directions human computation;computation research;computation;computation emphasis emerging;computation emphasis;computation book;computation emphasis connections;human;computation book provides;future directions human;comprehensive review human;review human;emerging field human;directions human;field human;areas capturing unique;goal book;capturing unique;goal book provide;capturing;capturing unique perspectives;areas capturing;perspectives goal book;promising future directions", "pdf_keywords": ""}, "61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886": {"ta_keywords": "benchmark logic language;logic language model;novel benchmark logic;logic language;benchmark logic;logic reasoning novel;reasoning novel benchmark;known benchmark logic;logic reasoning;order logic reasoning;language model benchmark;order logic;logic;method order logic;language model;reasoning novel;diagnostic method order;language model proposed;model benchmark;accuracy generalization interpretability;model benchmark used;reasoning;interpretability identify weaknesses;generalization interpretability identify;benchmark used diagnose;generalization interpretability;diagnostic method;diagnose accuracy generalization;interpretability identify;interpretability", "pdf_keywords": "human logic traceability;logic traceability;logic domain knowledge;learning reasoning;logic traceability based;logical language inference;diagnosing reasoning abilities;machine learning reasoning;benchmark named logical;diagnosing reasoning;benchmark logic network;diagnose reasoning ability;learning reasoning ability;reasoning ability nlu;novel benchmark logic;infer logic domain;method diagnosing reasoning;fol reasoning ability;named logic;logics fols natural;logic network;logic domain;task infer logic;validate human logic;reasoning ability;diagnose reasoning;fols natural language;language inference;named logic nwe;infer logic"}, "a829d65de0cc19da49ad6b4a294dd31545aed2bb": {"ta_keywords": "stochastic perturbation expressed;stochastic perturbation response;stochastic perturbation;perturbation response stochastic;response stochastic perturbation;stochastic response stochastic;perturbation amplitude;perturbation amplitude perturbation;amplitude perturbation expressed;amplitude perturbation;perturbation response;stochastic response;amplitude perturbation amplitude;perturbation expressed;response stochastic;terms amplitude perturbation;stochastic;perturbation;perturbation expressed terms;study stochastic response;study stochastic;approach study stochastic;amplitude;terms amplitude;expressed terms amplitude;expressed terms;expressed;terms;response;present", "pdf_keywords": ""}, "bc5e4b9fb3a40057df4994354a403202218d53a6": {"ta_keywords": "transformed program heuristic;search efficient algorithms;semantics preserving transformations;efficient algorithms solving;efficient algorithms;program heuristic search;algorithms solving combinatorial;semantics preserving;accelerating search efficient;substantial improvements search;algorithm substantial improvements;sequence semantics preserving;algorithm substantial;program heuristic;search efficient;heuristic search procedure;heuristic search;algorithms;heuristic search sequence;search sequence semantics;algorithms solving;transformations resulting algorithm;efficiency transformed program;resulting algorithm substantial;improvements search;method accelerating search;transformed program;accelerating search;improvements search time;solving combinatorial problems", "pdf_keywords": "algorithms natural language;parsing algorithms;parsing algorithms used;parsing algorithms based;designed useful parsing;parsers highly efficient;structure sentence algorithms;faster dynamic programming;programming algorithms natural;computational natural language;natural language processing;hierarchical parsers highly;sentence algorithms;useful parsing;parsers highly;hierarchical parsers;sentences algorithms;sentence algorithms based;parsing;class parsing algorithms;sentences algorithms based;parsers;processing nlp;accelerating dynamic programming;language processing nlp;natural language nlp;parsing complex sentences;useful parsing complex;linear time parsing;improving dynamic programming"}, "ab17c315f7ee4fe69fde2f3d8ae0e30e4e2f3a2b": {"ta_keywords": "documents answer complex;structured documents answer;questions iteratively;complex documents answer;long complex documents;answer complex questions;complex documents;questions iteratively attend;structured documents;hop questions iteratively;heirarchically structured documents;documents answer multi;complex questions present;iterative;reading long complex;documents answer;model iterative;iteratively;complex questions;long heirarchically structured;new model iterative;answer complex;answer multi hop;questions proposed model;structured;questions require reading;iterative d_ hop;heirarchically structured;documents;hop questions proposed", "pdf_keywords": "answer queries structured;answering questions structured;hierarchical document answer;queries structured documents;questions structured documents;queries long structured;structured documents empirically;predicting answer question;long structured documents;answer question document;answer queries;task predicting answer;machine learning conversational;long structured document;conversational task;conversational task predicting;document answer question;questions long document;document dochopper iteratively;queries structured;structured documents;outcome conversational task;unstructured document learning;structured document dochopper;hierarchical document;sentences retrieved;ability answer queries;model answering questions;structured documents approach;questions structured"}, "e54ffc76d805c48660bb0fd20019ca82ac94ba0d": {"ta_keywords": "dimensional language models;low dimensional language;intrinsic dimension pretrained;dimensional language;dimension pretrained model;dimension pretrained;intrinsic dimension words;language models;low dimensional;pretrained models low;dimension words exists;high dimensional low;dimension words;dimension reparameterization effective;low dimension reparameterization;dimensional low;descent intrinsic dimension;dimensional low dimensional;low intrinsic dimension;pretrained models;low dimension;high dimensional;pretrained model low;dimension reparameterization;tune high dimensional;language models using;common pretrained models;gradient descent intrinsic;training parameters;pretrained model", "pdf_keywords": "language models intrinsic;intrinsic dimension pretrained;intrinsic dimension training;dimension pretrained models;predicting intrinsic dimension;trained language models;dimension training;dimension pretrained;intrinsic dimension generalization;dimension training accuracy;dimension generalization stronger;intrinsic dimensionality low;models intrinsic dimension;dimensions pre training;pre training generalization;optimize intrinsic dimensionality;intrinsic dimensions pre;intrinsic dimensionality new;trained models intrinsic;intrinsic dimension analyzed;pre trained language;models intrinsic dimensions;intrinsic dimension;characterizing intrinsic dimensionality;pre trained models;analyzing intrinsic dimensionality;intrinsic dimensionality;bounds intrinsic dimension;parameters pre training;intrinsic dimensionality context"}, "2a82a16bdb793dc388391be57d6424f0d5090513": {"ta_keywords": "peer review scores;scores peer review;datasets peer reviews;review scores;quantized scores peer;review scores method;scores peer;peer review;peer reviews;peer reviews snp;integrating quantized scores;ranking information updates;quantized scores;ranking information;updates scores papers;incorporates ranking information;updates scores;information updates scores;arbitrariness peer review;peer review process;scores papers;ranking;scores method updated;incorporates ranking;scores method;scores papers manner;scores;method incorporates ranking;datasets peer;synthetic datasets peer", "pdf_keywords": "quantized scores reviewers;reviewer quantized scores;scores peer reviewers;scores reviewers method;rankings reviewers based;rankings reviewers;rankings scores review;scores reviewers based;scores given reviewer;dequantizing scores reviewers;score review estimating;assigning rankings reviewers;peer review algorithm;dequantized scores reviewers;scores reviewers;review scores reviewers;review scores;quantized scores peer;available review scores;scores provided reviewers;scores estimate ranking;scores reviewers review;rankings scores;quantized scores rank;ranking papers reviewed;scores provided reviewer;papers review scores;score review;scores consistent reviewers;reported rankings algorithm"}, "188fd1373aefdbf564e90a76fed43e1b8b7052dc": {"ta_keywords": "spin orbit interaction;orbit interaction spin;interaction spin orbit;correlated spin orbit;xmath0 model spin;spin polarized xmath0;dynamics spin orbit;model spin orbit;interaction spin polarized;interaction spin;spin orbit;dynamics spin;correlated spin;study dynamics spin;strongly correlated spin;orbit interaction strongly;orbit interaction interaction;polarized xmath0 model;orbit interaction;spin polarized;model spin;polarized xmath0;spin;interaction strongly correlated;xmath0 model;interaction interaction strongly;orbit;strongly correlated;interaction strongly;xmath0", "pdf_keywords": ""}, "69e2d1f5374918111432fae23212c2759b1357c2": {"ta_keywords": "sequential ranking algorithm;sequential ranking;active ranking;active ranking based;ranking based pairwise;active ranking set;comparisons items ranked;ranking algorithm;ranking optimal;ranking set items;prove sequential ranking;items ranked according;recovers ranking optimal;ranking algorithm recovers;problem active ranking;items ranked;ranking set;ranking based;ranked according probability;ranking;item items ranked;algorithm recovers ranking;ranking optimal logarithmic;ranked according;recovers ranking;pairwise comparisons items;ranked;based pairwise comparisons;comparisons items;items based pairwise", "pdf_keywords": "ranking pairwise comparison;accurate ranking pairwise;ranking sample complexity;analyze sequential ranking;ranking pairwise;estimating ranking;sequential ranking;estimating ranking unknown;ranking unknown items;recovering ranking sample;sequential ranking algorithm;estimating ranking set;comparisons items ranked;ranking robust;estimating total ranking;pairwise comparisons stochastic;noisy pairwise comparisons;recovering ranking;determining ranking unknown;item ranking;sample complexity ranking;ranking active pairwise;algorithm active ranking;ranking sample;ranking multidimensional stochastic;ranking algorithm;ranking provided underlying;ranking algorithm counts;problem estimating ranking;partial complete ranking"}, "926d827aef568ed97431a7845c9a8138930c80fd": {"ta_keywords": "social affective responses;social network communicate;social sharing behavior;affective responses people;social affective;social sharing;sharing behavior efficacy;use social network;affective responses study;relationship social affective;relationship social sharing;affective responses;use social;social network;people use social;sharing behavior;efficacy affective reactions;behavior efficacy affective;communicate messages people;affective reactions;relationship social;exploring relationship social;messages people use;people use network;efficacy affective;communicate information people;responses people use;affective;responses study explores;responses study", "pdf_keywords": ""}, "9b534639bcadc9ad232b338e760c523a4d74c8de": {"ta_keywords": "concise descriptions linguistic;descriptions linguistic;descriptions linguistic phenomena;extraction concise descriptions;linguistic phenomena evaluate;evaluate descriptions;phenomena evaluate descriptions;descriptions help language;evaluate descriptions help;concise descriptions;automated evaluation;linguistic;linguistic phenomena;automated evaluation human;method automated evaluation;language experts propose;extraction concise;help language experts;human evaluation;evaluation human evaluation;help language;descriptions;human evaluation infeasible;language experts;automatic framework extraction;evaluation;framework extraction concise;descriptions help;evaluation human;concise", "pdf_keywords": "extracting linguistical descriptions;automatic use linguistic;extracting linguistic rules;extracting linguistic;linguistic exploration description;extract linguistic rules;linguistic questions classification;based extracting linguistic;extract relevant linguistic;extract linguistic;extraction concise descriptions;extracting syntactic semantic;linguistical descriptions unstructured;extracting linguistical;automatic framework linguistic;concise descriptions linguistic;linguistical descriptions;framework extract linguistic;descriptions linguistic;describing understanding linguistic;marking morphological agreement;words languages extracting;linguistic systems identifying;procedure extract linguistic;linguistic exploration;linguistical framework extract;discovering interesting linguistic;discover rules linguistic;extracting syntactic;languages extracting rules"}, "987c5ad75d5092bed03e9f523aec00dc43bc17e4": {"ta_keywords": "urban air quality;impacts road traffic;impact air quality;road network density;air quality quantitatively;parameters urban air;road traffic characteristic;traffic characteristic;traffic characteristic parameters;air quality policy;investigates impacts road;air quality;road traffic;road area occupancy;bus emission reduction;impacts road;area occupancy intersection;characteristic parameters urban;bus network density;aerosol optical thickness;bus emission;planning bus emission;density road area;parameters urban;network density road;including road network;traffic;quantitatively based aerosol;geographical weighted regression;intersections greater impact", "pdf_keywords": ""}, "89c2b3bfcc309ce16c85d2ab0c8cac5295400715": {"ta_keywords": "meta learning algorithm;maximum entropy learners;meta learning;stacked maximum entropy;method meta learning;new meta learning;sequential stacking stacked;learning algorithm called;learning algorithm specifically;learning algorithm;entropy learners;sequential stacking;entropy learners present;known stacked maximum;stacked maximum;stacking stacked;method sequential partitioning;sequential partitioning;stacking stacked version;problems sequential stacking;designed stacked maximum;sequential stacking consistently;called sequential stacking;entropy method meta;base learners;sequential partitioning problems;stacking consistently improves;stacked version;stacking;partitioning problems sequential", "pdf_keywords": ""}, "9b263129548dc09369e8bc34560fe5bb6047fcee": {"ta_keywords": "simulator market electricity;market electricity greek;market electricity;simulation market;simulator market;simulation market including;time simulation market;development simulator market;market choice market;market rules;market microstructure market;electricity greek goal;market rules existing;market choice;market including processes;processes market choice;processes market;market microstructure;electricity greek;market;simulator potential use;effectiveness market rules;microstructure market;microstructure market microstructure;real time simulation;choice market microstructure;choice market;simulation;simulator;including processes market", "pdf_keywords": ""}, "9333d372ad3887e02029d2eab0dbc0c0478582c7": {"ta_keywords": "generative speech tagging;speech tagging method;speech tagging;tagging method;unsupervised learning methods;evaluation unsupervised learning;tagging;natural language processing;unsupervised learning;methods natural language;tagging method shown;evaluation unsupervised;used generative speech;generative speech;language processing;accuracy generative speech;learning methods natural;used generative;accuracy generative;natural language;unsupervised;language processing tasks;study evaluation unsupervised;learning methods;generative;data used generative;evaluate accuracy generative;learning;speech;methods natural", "pdf_keywords": ""}, "35b376ad9e03e5e0b930c53a48817bfb5703108d": {"ta_keywords": "text style transfer;semantic similarity metrics;semantic similarity;style transfer;style transferred;use semantic similarity;using semantic similarity;texts considering style;considering style transferred;similarity metrics assess;style transfer problem;similarity metrics;texts model;style transferred outputs;approach text style;similarity metrics model;text style;test texts model;texts model provides;similarity;texts;test texts;texts propose efficient;approach text;texts considering;semantics texts;considering semantics texts;novel approach text;content considering semantics;considers preservation texts", "pdf_keywords": "transfer sentence generation;neural machine translation;text style transfer;paraphrastic similarity training;style transfer leveraging;preserving language model;machine translation models;style transfer sentence;accuratelyconventional machine translation;train machine translation;existing machine translation;machine translation model;evaluate style transfer;sentence generation method;sentence generation;similarity training;machine translation methods;machine translation;transfer leveraging semantic;style transfer based;translation methods predicting;language model training;style transfer highly;machine translation agent;style transfer accuracy;style transfer unsupervised;leveraging semantic similarity;novel paraphrastic similarity;style transfer powerful;translation models"}, "c96970cfb1c13ae6dccc30de482ce6b0d4414f2b": {"ta_keywords": "predicate invention;predicate invention based;based structured sparsity;version predicate invention;structured sparsity improves;structured sparsity;improves structure learning;sparsity improves structure;predicate;structure learning performance;improves structure;structure learning;new version predicate;large scale algorithms;structured;version predicate;invention based structured;algorithms;based structured;structure;sparsity improves;sparsity;learning performance large;scale algorithms;learning;performance large scale;performance large;learning performance;invention;large scale", "pdf_keywords": ""}, "4fee3d5d476568deb971768f8a5191eb627309d0": {"ta_keywords": "games differential equilibria;differential equilibria stable;game dynamics linearized;differential equilibria player;equilibria player continuous;equilibria stable;stability differential equilibria;games stable points;differential equilibria robustness;equilibria analyze stability;linearized local equilibria;equilibria robustness variation;equilibria robustness;continuous games;stable potential games;game dynamics;differential equilibria;equilibria stable potential;fixed points game;continuous games near;player continuous games;potential games stable;local equilibria;sum games differential;games differential;games stable;equilibria player;local equilibria analyze;spectrum game dynamics;zero sum games", "pdf_keywords": "learning dynamics proved;stability differential nash;learning dynamics theorems;stable differential nash;stability games theorems;equilibria gradient learning;learning dynamics;learning dynamics player;stable equilibria gradient;gradient learning dynamics;learning dynamics near;learning dynamics provide;differential nash equilibria;concerning stability games;stable equilibria gradientwe;instability learning dynamics;game dynamics symmetric;equilibria agents learning;learning player continuous;stability games fundamental;differential equilibria games;equilibria variations learning;stability game;stability games study;stability games;nash equilibrium sde;differential nash equilibrium;stability differential equilibria;differential equilibria gradient;equilibria gradient based"}, "5df0b8b80aecda1efdebac5d1ab7bcf94a88c68f": {"ta_keywords": "feature extractor;fixed feature extractor;feature extractor fixed;probing biomedical literature;extracted information extracted;information extracted;extracted information;extracted information clustered;information extracted information;analysis extracted information;improves performance extracted;extractor;extractor fixed;specific corpora;domain specific corpora;extractor fixed probe;corpora article pre;nearest neighbor analysis;biomedical literature;extracted;probing biomedical;analysis extracted;corpora article;specific corpora article;biomedical literature use;corpora;neighbor analysis extracted;article pre trained;performance extracted;performance extracted information", "pdf_keywords": "deep bidirectional language;contextual word embeddings;sentences based deep;word embeddings domain;word embeddings;embedding words;trained deep bidirectional;trained embeddings bio;deep bidirectional;domain trained embeddings;tasks bio bert;sentence structure embedding;scenario structured embeddings;language inference nlp;embeddings named bioelmo;trained biomedical corpora;natural language inference;inference nlp;structure embedding words;unstructured content sentences;embeddings bio;embeddings bio elongation;bio bert;elongation deep learning;trained embeddings;inference nlp use;bidirectional language models;intrinsically contextual word;structured embeddings;elongation deep learningwe"}, "5930efbf01efa8944258b1c0f7349111702f779e": {"ta_keywords": "nonlinear feature extraction;processing nlp;natural language processing;novel nonlinear linguistic;language processing nlp;nonlinear linguistic structures;feature extraction;nonlinear linguistic;processing nlp combines;feature extraction feature;extraction feature extraction;feature extraction pipeline;novel feature extraction;nlp combines;nlp nlp;nlp;feature extraction modules;feature extraction extensive;nlp combines novel;extraction feature;nlp nlp report;nonlinear feature;field natural language;nlp report;cogcomp nlp nlp;set nonlinear feature;natural language;language processing;nlp report performance;linguistic structures", "pdf_keywords": ""}, "44268b5a208e8f48a5883bb12e3e80a13101e752": {"ta_keywords": "median creatinine level;factors acute kidney;creatinine level;years median creatinine;median creatinine;creatinine concentration;acute kidney injury;kidney injury ci;kidney injury;acute kidney;creatinine level 44;database creatinine concentration;creatinine;creatinine concentration xmath0;creatinine level xmath1;database creatinine;patients median age;risk factors acute;injury ci reduced;mean free energy;kidney;patients median;factors acute;median age 67;public database creatinine;median age;core centered mean;3848 patients median;injury ci;67 years median", "pdf_keywords": ""}, "96ed7a7da69d654668b35b50344debd44e87c1a1": {"ta_keywords": "topic identification low;dependencies topic identification;topic identification process;topic identification;contextual dependencies selective;contextual dependencies topic;modeling translation lexicons;lexicons attention dependent;contextual dependencies;method topic identification;propose contextual model;lexicons attention;leverage contextual dependencies;context independent models;contextual model;translation lexicons attention;acoustic modeling translation;low resource languages;contextual;context independent;propose contextual;exploration contextual dependencies;identification low resource;translation lexicons;outperforms context independent;able leverage contextual;leverage contextual;modeling translation;resource languages;selective exploration contextual", "pdf_keywords": "topic identification spoken;topic identification speech;language topic classification;topic identification methods;topic identification method;topic identification;topic identification technique;topic identification use;topics unstructured audio;topic identification topic;topic classification;developed topic identification;topic classification attention;novel topic identification;classify spoken documents;quality topic identification;identify topics;identify topics unstructured;approach topic identification;method topic identification;analyzing spoken document;framework classify spoken;identification topic id;identification spoken segments;contextual acoustic models;characterize spoken content;identification speech based;analyzing spoken;topic id;speech classification"}, "b9c3e87bc09c4c6167a03a835c30b1c23bef7a40": {"ta_keywords": "answering knowledge bases;generalization knowledge bases;knowledge bases;bases knowledge bases;knowledge bases knowledge;knowledge bases questions;bases questions answering;questions answering knowledge;knowledge base;knowledge base dataset;knowledge bases approach;bases knowledge;answering knowledge;quality knowledge base;trained contextual embedding;contextual embedding;based use bert;contextual embedding demonstrate;questions answering;use bert;generalization knowledge;trained contextual;propose novel bert;high quality knowledge;approach generalization knowledge;pre trained contextual;novel bert;use bert pre;bert;bert pre trained", "pdf_keywords": "questions knowledge bases;constructing knowledge bases;generalization knowledge bases;new knowledge bases;knowledge bases;arbitrary knowledge bases;generalizing natural language;answer structured queries;knowledge bases propose;generative knowledge base;bert generalization knowledge;knowledge bases provide;natural language database;asking knowledge base;generate structured queries;natural language questions;contextual embedding bert;questions knowledge;knowledge base;knowledge bases answer;contextual embedding general;knowledge bases strong;knowledge bases demonstrate;answer questions knowledge;bases answer structured;knowledge base present;trained contextual embedding;learning approach embeds;structured queries;generalization simple knowledge"}, "05b6be9aec266072669f6f287a846637eedf19b5": {"ta_keywords": "warming invasion nematode;canadensis soil nematode;invasion nematode canadensis;soil nematode community;nematode canadensis;nematode community experimental;nematode canadensis canadensis;soil nematode;invasion nematode;experimental warming invasion;effects experimental warming;results experimental warming;nematode community;experimental warming increased;warming invasion;canadensis soil;experimental warming;nematode;canadensis canadensis soil;nematode community 31;indexes nematode community;experimental air warming;warming applied increase;warming increased;warming increased chao;structure indexes nematode;indexes nematode;air warming applied;warming applied;air warming", "pdf_keywords": ""}, "449310e3538b08b43227d660227dfd2875c3c3c1": {"ta_keywords": "trained parallel models;parallel models trained;models trained parallel;trained parallel;trained parallel using;parallel using generative;depth neural network;deep neural;deep neural network;continuous depth neural;depth neural;neural network models;parallel models;models trained;neural network;generative model;accelerate learning;neural network continuous;network models trained;generative model used;neural;family deep neural;used accelerate learning;using generative model;accelerate learning process;generative;residual network compute;residual network;parallel;time residual network", "pdf_keywords": "backpropagation ode nets;ode nets trained;neural ode nets;computing gradients loss;computes gradient loss;backpropagation network adjoint;backpropagate ode solver;backpropagation ode;direct backpropagation ode;backpropagation network;performance ode nets;gradients loss function;continuous backpropagation network;backpropagation;gradient loss using;gradient loss;backpropagation neural;neural network compute;ode nets;solver framework trained;continuous backpropagation;gradients loss;continuous backpropagation neural;direct backpropagation;efficient direct backpropagation;code neural ode;forward pass learning;solver state loss;backpropagation neural network;computation neural networks"}, "ab1e5a3c5521b6204dc7c6f1fa72b88000bc30ee": {"ta_keywords": "server based noise;noise driven user;noise sources pre;noise sources;pre trained noiseless;trained noiseless noise;noise limited user;trained noiseless;noise driven;based noise driven;based noise;noiseless noise limited;performance variety noise;variety noise sources;noiseless noise;noiseless;noise driven respectively;noise limited;noise;variety noise;server based;performance variety;user server based;based user server;driven user server;sources pre trained;server;user server;performance;server based user", "pdf_keywords": "question answering explore;question answering;noise textual responses;queries machine learns;downstream question answering;noise natural language;accuracy natural language;textual responses;answering explore mitigation;machine learns;textual responses approach;text create questions;characterizing natural language;user interaction;create questions;processing natural language;question errors substantial;answer question context;natural language;language processing natural;learning natural language;answering explore;questions process data;language processing tasks;natural language information;question context variety;natural language processing;questions process;natural language combines;language specific response"}, "5f609f252d8815c5fb660d83c0dc71af21ecf65d": {"ta_keywords": "keywords event monitoring;event monitoring twitter;finding noun phrases;automatically finding noun;keywords event;approach keywords event;monitoring twitter;keywords using feature;noun phrases;phrases nps keywords;keywords using;monitoring twitter use;classify extracted keywords;noun phrases nps;twitter use knowledge;extracted keywords;nps keywords event;extracted keywords using;event monitoring;keywords;events classified using;event monitoring data;finding noun;phrases nps;nps keywords;knowledge base based;phrases;knowledge base;twitter;automatically finding", "pdf_keywords": ""}, "33c691ca050e1806d44c08e55e63fcd7e555899a": {"ta_keywords": "positive unlabeled learning;proportion estimation unlabeled;unlabeled learning based;estimation unlabeled classification;unlabeled classification;unlabeled learning;positive unlabeled;proportions positive negative;proportions positive;method positive unlabeled;classifier classifies unlabeled;classifier;estimation unlabeled;estimates proportions positive;classification;neural network classifier;classifies unlabeled sample;positive negative data;unlabeled sample;classifies unlabeled;unlabeled;identified proportions;proportions estimation dedpul;proportion estimation;identified proportions proposed;network classifier;learning based difference;negative data using;proportions proposed framework;negative data", "pdf_keywords": ""}, "a8ea980b63deaf1404cd9f539a575b4e7135466e": {"ta_keywords": "parity models predictive;predictive model paritym;latency parity models;parity models new;erasure coded resilience;parity models trained;paritym median latency;parity models;coded resilience prediction;systems parity models;erasurecoded queries;median latency parity;parity models implement;models implement parity;parity models reduced;serving systems parity;implement parity models;introduce parity models;transform erasurecoded queries;inference parity models;systems parity;parity;latency parity;implement parity;model paritym median;model paritym;resilience prediction serving;erasurecoded queries form;paritym;coded resilience", "pdf_keywords": ""}, "fb7caddac20dca012f48c90b2e1e2383f7185051": {"ta_keywords": "misuse interpretability tools;use interpretability tools;interpretability tools furthermore;interpretability tools;interpretability tools uncover;use interpretability;misuse interpretability;trust misuse interpretability;interpretability;data scientists trust;observe use interpretability;findings social science;models data scientists;evaluating machine learning;scientists trust misuse;contextualize findings social;implications researchers tool;data scientists observe;data scientists;scientists trust;contextualize findings;implications researchers;machine learning models;designers contextualize findings;researchers tool designers;survey data scientists;trust misuse;tools conclude implications;social science;findings social", "pdf_keywords": "interpretability tools participants;interpretability tools machine;interpretability tools deep;interpretability tools intuitive;interpretability tools interpretability;interpretability tools;interpretability data;tools interpretability data;models interpretability tools;existing interpretability tools;using interpretability tools;interpretability tools explain;interpretability tools interpret;interpretability human;mental models interpretability;interpretability human behavior;interpretability tools used;dataset models interpretability;interpretability intelligibility;use interpretability tools;dataset use interpretability;designing interpretability;interpretability systems;ability interpretability tools;models interpretability;interpretability systems work;tools interpretability;interpretability tools form;designing interpretability systems;interpretability tools uncover"}, "b1d8c868e1d6d4980ee2f8c50e6fc5e4e7027ca2": {"ta_keywords": "story continuation hypothesis;story continuation;context story continuation;accuracy story continuation;story continuation strong;dialogue characters relationship;improves accuracy story;dialogue characters;dialogue;model trains dialogue;data context story;trains dialogue characters;multi task model;trains dialogue;evaluation unstructured data;continuation strong baselines;context story;continuation;evaluation unstructured;task model;multi task;unstructured data context;characters relationship;characters relationship proposed;approach evaluation unstructured;task model trains;unstructured data;use multi task;strong baselines improves;relationship characters", "pdf_keywords": "driven storytelling dataset;storytelling dataset;storytelling dataset crc3;character driven storytelling;storytelling use data;story training optimize;story content driven;character dialogue plus;story continuation accuracy;driven storytelling;story content;game concept crowdsourcing;character prediction systems;predict continue story;character prediction context;driven emergent storytelling;dialogue plus character;challenges character driven;emergent storytelling;story training;character dialogue;driven storytelling case;story continuation using;improves accuracy story;emergent storytelling use;based story continuation;accuracy story continuation;continue story training;structure underlying story;character driven"}, "e785441f5ccd6e4e29b3123e61121df5c65b88f7": {"ta_keywords": "variational autoencoder training;variational autoencoder;dynamics variational autoencoder;autoencoder training reduces;autoencoder training;modification variational autoencoder;autoencoder training context;encoding posterior collapse;training reduces inference;latent encoding posterior;autoencoder;training inference;training inference network;reduces inference lag;posterior collapse;context posterior collapse;stages training inference;dynamics variational;posterior collapse occurs;training context posterior;posterior collapse initial;inference network;inference lag;encoding posterior;investigate dynamics variational;variational;latent encoding;reduces inference;inference lag depending;modification variational", "pdf_keywords": "variational autoencoder model;training deep generative;variational autoencoders;variational autoencoder;modeling variational inference;deep generative models;deep generative;learning generative model;parameters generative models;variational inference network;autoencoder model learned;variational autoencoder vae;variational inference;learning generative;variational inference fully;text modeling variational;generative models;specific variational inference;posterior parameters generative;generative models autoregressive;stochastic variational autoencoder;trained convolutional generative;variational autoencoders stable;stable variational autoencoders;combining variational inference;generative model;deep latent variable;paralel inference network;keeps variational inference;autoencoder model"}, "f7a2f2ae829545ee992a2214b3600cf914544e22": {"ta_keywords": "human students performance;accuracy human students;accuracy student data;students performance comparing;student performance;performance human students;students performance human;determine student performance;performance student data;student performance following;students performance;student model predict;students performance better;students evaluate accuracy;correct performance student;study accuracy student;model human students;student model;accuracy student;student data second;student data;use student model;human students evaluate;human students;student data used;performance student;determine student;study accuracy;students evaluate;use student", "pdf_keywords": ""}, "9112be1801598125d463febb96a525227c32acc1": {"ta_keywords": "automatic differentiation weighted;automatic differentiation;neural network learn;deep neural;deep neural network;differentiation weighted;finite state transducers;differentiation weighted finite;learn latent decomposition;network learn latent;dynamically training;framework automatic differentiation;state transducers wfsts;state transducers;network learn;finally propose convolutional;used dynamically training;propose convolutional layer;neural;traditional convolution;replacement traditional convolution;neural network;interior deep neural;latent decomposition words;transducers wfsts;higher level representations;weighted finite state;transducers wfsts allowing;propose convolutional;learn latent", "pdf_keywords": "automatic differentiation weighted;sequencelevel loss functions;compute weights transitions;weight transition matrices;structured loss functions;transitions neural networks;automatic differentiation;finite state automata;state automata wfsts;sequence level loss;framework automatic differentiation;weights transitions neural;automata wfsts;weights transitions;based sequence trained;state automata;automata;sequencelevel loss;sequence trained;weighted finite state;trained sequence words;automata wfsts framework;learning operations;learning operations structure;loss functions finally;transition matrices linear;new structured loss;finite state transducers;structured loss;loss functions"}, "4cf633d0893a1d3af97723ce1f2fae33c2a30043": {"ta_keywords": "similarity relations knowledge;relations knowledge bases;quantify similarity relations;redundant relations extracted;detect redundant relations;relations extracted;similarity relations;knowledge bases;open information extraction;information extraction;information extraction models;distributions entity pairs;entity pairs;knowledge bases approach;redundant relations;entity pairs provide;similarity empirically outputs;quantify similarity;similarity empirically;relations extracted open;exact similarity empirically;relations knowledge;method quantify similarity;extracted open information;exact similarity;relations;similarity;probability distributions entity;detect redundant;approximation exact similarity", "pdf_keywords": "similar relation extraction;similarity relations knowledge;similarity relations leveraging;similarity entities knowledge;similarity relationships knowledge;similarity relations based;quantify similarity relations;similarity relations defined;captures similarity relationships;similarity relations objects;relation similarity;relation extraction scalable;similarity relations propose;capture similarity relationships;relation prediction extraction;relation similarity relations;similarity relations corresponding;similarity similarity relations;similarity entities;predict similarity relations;prediction relation extraction;similarity relations;similarity relationships representation;relation extraction;relations knowledge bases;similarity score entity;evaluate similarity relations;similarity relationships;relation extraction task;identifying relations entities"}, "9f1059006e4ba303f8945114eddadd50d58a9f3e": {"ta_keywords": "large knowledge bases;learning large knowledge;learning soft symbolic;knowledge bases;soft symbolic databases;knowledge bases dms;symbolic databases;parallel learning tasks;parallel learning;learning soft;learning tasks parallel;learning dms efficiently;learning large;efficiently trained using;symbolic databases illustrate;large knowledge;dms efficiently trained;class parallel learning;framework learning soft;gradient based learning;soft symbolic;efficiently trained;databases;framework learning large;train neural;learning dms;learning tasks;train neural network;neural networks gradient;network train neural", "pdf_keywords": "linearly ordered representations;soft symbolic databases;linearly ordered entities;entities linearly ordered;large knowledge bases;knowledge bases;ordered representations;representations linearly ordered;ordered entities linearly;symbolic databases;ordered representations linearly;learning rules entities;ordered entities;language tensor network;sparse linearly ordered;learning method nql;symbolic databases implemented;dataflow language tensor;tensor based;tensor network framework;symbolic databases using;ordered entities using;ordered entities called;tensor network;tensor based learning;deep learning query;knowledge bases bs;soft symbolic;context based nql;generating linearly ordered"}, "02b932416751674dc25353620a1df4b53c3a5f6f": {"ta_keywords": "sequence model nonlinear;predicting transcriptions linguistic;speech recognition asr;predicting transcriptions;transcriptions linguistic annotations;automatic speech recognition;automatic speech;jointly predicting transcriptions;asr based sequence;recognition asr based;speech recognition;sequence sequence model;model nonlinear partial;quality transcriptions linguistic;method automatic speech;transcriptions linguistic;sequence model;model nonlinear;recognition asr;nonlinear partial;linguistic annotations able;based sequence sequence;nonlinear partial differential;sequence sequence;linguistic annotations;transcriptions;quality transcriptions;nonlinear;linguistic annotations proposed;high quality transcriptions", "pdf_keywords": ""}, "811531c959b0543a8e7abe1e827770e36b96f817": {"ta_keywords": "translation emphasis speech;emphasis speech method;emphasis speech;translate emphasis target;word level emphasis;emphasis target language;able translate emphasis;speech able translate;translate emphasis;emphasis method;translation emphasis;level emphasis method;emphasis target;emphasis method applied;estimate word level;speech method;translate word level;level emphasis;emphasis;method applied speech;speech method uses;accurately translate word;applied speech;applied speech able;method translation emphasis;language conditional random;speech;speech able;target language conditional;word level", "pdf_keywords": ""}, "6c34b7b0441bff66cce2418d36acfd9776ad7bd2": {"ta_keywords": "rule learning algorithm;proposed rule learning;rule learning;learning algorithm irep;algorithm irep;learning algorithm;algorithm irep irep;benchmark problems irep;higher rules;higher rules propose;benchmark problems;algorithm;benchmark;rules propose;rules propose num;irep extremely;rates higher rules;proposed rule;irep;learning;irep irep;recently proposed rule;irep extremely cient;rithm style arxiv;rule;rules;irep irep large;problems irep extremely;large collection benchmark;problems irep", "pdf_keywords": ""}, "d723630c585aa0e4084fdd6e71bc6586cfa30e9d": {"ta_keywords": "pause information syntactic;pause prediction dependency;joint pause prediction;pause prediction;pause information;considers pause information;pause prediction measure;prediction dependency parsing;jointly considers pause;dependency parsing model;considers pause;dependency parsing;joint pause;pause;introduces joint pause;parsing model;gains pause prediction;information syntactic information;syntactic information;syntactic information proposed;parsing model jointly;information syntactic;parsing;prediction dependency;syntactic;gains pause;prediction measure;prediction;information proposed model;dependency", "pdf_keywords": ""}, "81bc64ce5553798c058f25fe5bd537d4bed67aed": {"ta_keywords": "quantum dot luminescence;barrier quantum dot;quantum dot grown;quantum dot;xmath0as quantum dot;type quantum dot;quantum dot ga;dot luminescence energy;dot luminescence;shaped barrier quantum;dot grown circularly;dot fabricated organometallic;dot fabricated;barrier quantum;vapour deposition circularly;ga xmath0as quantum;circularly shaped barrier;deposition circularly shaped;dot ga xmath0as;xmath0as quantum;mev dot fabricated;luminescence energy;luminescence energy approximately;quantum;deposition circularly;dot;dot grown;barrier shape barrier;shaped barrier;chemical vapour deposition", "pdf_keywords": ""}, "a8c62c42509c45a708ba477b603ee3fb81c77056": {"ta_keywords": "false news detection;weibo social platform;news detection process;news detection;approaches false news;news detection approaches;data repository weibo;weibo social;weibo;repository weibo;repository weibo social;transmitting false information;real time false;time false news;false information;false news;false information user;false information goal;detection approaches false;transmitting false;propagation false information;social platform;news;automated real time;automated real;information goal competition;social platform used;detection approaches;detection;real time", "pdf_keywords": "detection false news;false news detection;detecting false news;image detection falsenews;news detection validation;detection falsenews;detection falsenews multi;news detection competition;falsenews multi modal;false news posts;news detection critical;falsenews multi;false news text;false news image;detection validation competition;news posts competition;identify false news;news detection;propagation false news;news text detection;text detection false;news image detection;news detection early;truth validation;detecting false;news posts verify;detection validation;introduce false news;subtasks false news;falsenews"}, "44775500a5380be3776e876aedc43921d42d8de9": {"ta_keywords": "urban mobility data;urban dynamics;captures urban dynamics;urban mobility;massive urban mobility;urban dynamics aspects;rich urban dynamics;urban dynamics massive;semantics rich urban;dynamics massive urban;mobility data;urban states city;urban states;urban;mobility data method;massive urban;extracts urban states;sdd captures urban;captures urban;city captures volume;states city captures;rich urban;states city;city;learns semantics rich;state sharing hidden;population flows;mobility;learns semantics;state sharing", "pdf_keywords": ""}, "470bfbde1dc0ed6ca989957dcd551213720657c0": {"ta_keywords": "dependency trees language;syntax inducing dependency;inducing dependency trees;explicit syntax inducing;translates explicit syntax;trees language pair;dependent translation quality;syntax inducing;trees language;pair dependent translation;resulting dependency trees;dependent translation;dependency trees;language pair dependent;dependency trees resulting;translation quality improved;translation quality;inducing dependency;trees resulting dependency;language pair;translates explicit;introduce model translates;explicit syntax;model translates explicit;syntax;model translates;language;translates;translation;number trees", "pdf_keywords": "neural machine translation;attention models translating;machine translation model;translation model learns;machine translation encoder;attention encoder;attention encoder neural;translation models;self attention encoder;attention models compute;attention models;self attention models;machine translation;attention neural networks;decoder coupled attention;model translation translation;attention neural;attention function sattn;translation model;sentence attention;models translating english;supervised trees attention;sentence attention mechanism;models compute attention;translation models model;pred attention models;dependency trees language;self attention neural;encodes syntactic structure;model translation"}, "bc494b9c6d9602a69b76ab9ea0e95d348a2fce19": {"ta_keywords": "summarization summaries assessed;summarization high;high level summarization;highlighted salient content;summarization high level;highlight based evaluation;manual evaluation highlight;evaluation highlight based;level summarization high;summarization summaries;level summarization summaries;summaries assessed;manually highlighted salient;highlight based;evaluation highlight;summarization;level summarization;document manually highlighted;highlighted salient;summaries assessed multiple;salient content;highlight;annotators source document;manually highlighted;salient content validate;annotators source;summaries;assessed multiple annotators;reference evaluation high;multiple annotators source", "pdf_keywords": "highlights summarization evaluation;highlights summarization;evaluation summarizing text;summarization evaluation propose;highlights evaluate summaries;summarization evaluation;evaluation document summarizing;highlights dependent summaries;evaluations summaries generated;generate reference summaries;highlights summaries;use highlights summarization;summarization captured commonly;quality summarization captured;evaluation summarizing;evaluations summaries framework;human evaluation summarizing;highlight based summaries;articles highlights summaries;highlights summaries obtained;highlight annotation evaluation;reference summaries approach;summaries evaluated source;summarization captured;evaluations summaries;describing summarizemarization documents;document summarizing summary;state art summarization;highlights referencewe introduce;document summarizing"}, "e2ffd0ea7aa9cebaafba4afaee3cbe78070c8aa2": {"ta_keywords": "model selection speech;selection speech recognition;triphone model variational;discrete state triphone;state triphone model;variational probabilistic model;state triphone parameters;speech recognition;model variational discrete;triphone parameters proposed;probabilistic model selection;triphone model;selection speech;propose variational probabilistic;speech recognition based;variational probabilistic;triphone parameters;probabilistic model;state triphone;variational discrete state;recognition based discrete;model variational;model selection;variational discrete;proposed model selection;triphone;model discrete state;discrete state model;model discrete;state model discrete", "pdf_keywords": ""}, "3321c947a4a399803592f26879927e58f587fd74": {"ta_keywords": "behavior human people;time baseline tasked;sub arresting rate;hour time baseline;predicting future orarrestings;time baseline;arresting rate;baseline tasked predicting;behavior human;present study behavior;study behavior human;behavior;tasked predicting future;arresting rate likelihood;future orarrestings findings;predicting future;predicting;findings include participants;participants aware;study behavior;participants aware existence;rate likelihood orarresting;tasked predicting;future orarrestings;arresting;sub arresting;24 hour time;likelihood orarresting high;hour time;existence sub arresting", "pdf_keywords": "prediction crowdsourcing experiments;predictions analysis crowdsourcing;prediction crowdsourcing;human prediction crowdsourcing;participants predicted arrests;crowdsourcing experiments introduce;algorithmic risk human;crowdsourcing experiments;analysis crowdsourcing studies;design crowdsourcing experiments;crowdsourcing experiments setting;analysis crowdsourcing;crowdsourcing studies;crowdsourcing studies conduct;predictions human quantitative;human predictions analysis;predictions instruments participants;reliable predictions human;predictions human loop;participants predictions assess;participants tasked predicting;algorithmic risk;accuracy participants predictions;predicting future arrests;crowdsourcing;risk human task;compared predictions offenders;predicted arrests;participants predictions;algorithmic risk assessment"}, "0554246ebb6c53e88d7ac2aabf0c96a91ad500a0": {"ta_keywords": "speech enhancement problem;speech enhancement capability;speech enhancement;approach speech enhancement;strong speech enhancement;simulation real speech;speech recognition preserving;real speech recognition;preserving strong speech;speech recognition;enhancement problem based;enhancement problem;enhancement capability;generalized multi channel;enhancement;multi channel convex;multi channel;novel approach speech;channel convex asymmetric;channel convex;recognition preserving;enhancement capability frontend;real speech;time domain model;approach speech;speech;recognition preserving strong;domain model generalized;domain model;based time domain", "pdf_keywords": "simulated speech enhancement;domain speech enhancement;speech enhancement proposed;speech enhancement capability;speech enhancement multi;channel speech separation;speech enhancement;advanced speech enhancement;speech enhancement based;speech enhancement advanced;speech separation based;framework speech enhancement;enhance speech enhancement;speech separation task;speech enhancement real;noise restore speech;audio separation network;enhancement capability speech;network speech noise;speech separation;capability speech separation;domain audio separation;simulated speech noise;network speech;multi channel speech;reduce speech recognition;enhance speech;speech noise data;speech noise networks;evaluated simulated speech"}, "c2bd176f8f9c84f9ba52ffb8f8bd4e9299c0f0cf": {"ta_keywords": "predicting parking occupancy;quantile regression approach;quantile regression;quantile regression implemented;multiple quantile regression;predicting parking;parking occupancy data;idea multiple quantile;multiple quantile;present quantile regression;quantile;problem predicting parking;results parking occupancy;parking occupancy;results parking;parking;present results parking;occupancy data;occupancy data city;present quantile;regression approach;regression;regression implemented;regression implemented python;regression approach problem;predicting;approach problem predicting;occupancy;problem predicting;data city", "pdf_keywords": ""}, "a9a7058b39768ece13608e31341cfb16c4faf2c3": {"ta_keywords": "fair machine learning;machine learning ideal;fair machine;literature fair machine;political philosophy;ideal approach political;political philosophy recently;approach political philosophy;recent literature fair;approach political;literature fair;machine learning;algorithms reflect broader;political;algorithms;learning ideal approach;shortcomings proposed algorithms;algorithms reflect;learning ideal;proposed algorithms;proposed algorithms reflect;philosophy recently uncovered;learning;philosophy;reflect broader problems;uncovered shortcomings proposed;broader problems;philosophy recently;recently uncovered shortcomings;shortcomings proposed", "pdf_keywords": "addressing algorithmic injustice;algorithmic injustice;algorithmic injustice distinction;fairness algorithmic;fair machine learning;learning fair algorithms;algorithmic fairness argue;algorithmic fairness empirical;fairness approaches;algorithmic fairness;evaluative justice theorizing;algorithmic fairness approaches;fairness approaches argue;human fairness proposed;fairness algorithmic fairness;algorithmic fairness algorithmic;machine learning fair;decisions justice fairness;injustice world theorizing;fairness proposed;problem human fairness;human fairness;injustice distinction ideal;fairness metrics;justice fairness natural;fairness empirical;fairness empirical evidence;fairness proposed principle;empirical evaluation fairness;current algorithmic fairness"}, "156323f4d87af6cf105c97bf29d324c9e3bc8f92": {"ta_keywords": "optimization speech translation;automatic speech translation;speech translation systems;speech translation asr;optimization speech;speech translation;optimization optimization speech;translation asr synthesis;improvement translation quality;translation systems;combines automatic speech;automatic speech;translation quality;translation asr;asr synthesis words;improvement translation;significant improvement translation;performance joint optimization;translation systems use;joint optimization optimization;optimization minimum error;joint optimization minimum;joint optimization;variance training batch;asr synthesis;minimum variance training;training batch margin;optimization optimization;training minimum variance;optimization minimum", "pdf_keywords": ""}, "3cd4797725ca9cf954946ed5309e15ebab80b92a": {"ta_keywords": "multimodal conditional image;conditional image synthesis;multimodal conditional;synthesize multimodal;approach multimodal conditional;diversity multimodal;quality diversity multimodal;diversity multimodal input;able synthesize multimodal;multimodal multiscale projection;multimodal;novel approach multimodal;unimodal conditional image;multimodal multiscale;image synthesis combines;image synthesis;multimodal input;multimodal input unimodal;knowledge multimodal multiscale;synthesize multimodal user;approach multimodal;image synthesis approaches;multimodal user inputs;knowledge multimodal;multimodal user;expert knowledge multimodal;multiscale projection discriminator;conditional image;existing unimodal conditional;synthesis combines expert", "pdf_keywords": "multimodal image synthesis;generative framework multimodal;images single generative;networks gans generative;gans generative;novel conditional generative;multimodal conditional image;conditional image synthesis;generating conditional images;generate multimodal image;conditional generative framework;conditional generative;synthesizes conditional images;multimodal image synthesized;generate multimodal;generative models generative;generative encoder;networks gans;generative adversarial networks;modeling generative;generative adversarial network;generative adversarial;deep convolutional generative;synthesis based generative;generative models;experts generative adversarial;image multimodal;images multimodal modalities;generative structure;generative"}, "365e049ecb7299cc6925483127f2f2123a97c35f": {"ta_keywords": "change point detection;abrupt change points;detect abrupt change;kernel learning framework;kernel learning;novel kernel learning;point detection auxiliary;novel kernel;change point;propose novel kernel;change points;detection auxiliary generative;point detection;detect abrupt;abrupt change;change points real;applied detect abrupt;dynamical phase convolution;detection;detection auxiliary;convolution;generative model;kernel;generative model proposed;auxiliary generative model;real world datasets;detect;generative model framework;approach applied detect;generative", "pdf_keywords": "detection time series;time series learning;detect abrupt change;change detection time;change point detection;kernel learning framework;new kernel learning;kernel sample test;abrupt change points;data driven kernel;model kernel learning;change detection;novel kernel learning;predicting occurrence change;prediction occurrence change;kernel learning cmd;time series change;change detection proposed;propose deep kernel;change detection method;stable change detection;kernel learning;robust change detection;kernel sample;kernel learning approach;kernel kernel sample;probabilistic change point;propose novel kernel;realization kernel learning;novel kernel"}, "464a75c05a5ce709fc515a2577b43acc8e3d45ce": {"ta_keywords": "multi hop inference;hop inference evaluation;hop inference;task multi hop;multi hop;inference evaluation achieved;hop;inference evaluation;2021 shared task;shared task multi;task multi;detailed analysis 2021;analysis 2021 shared;inference;82 baseline methods;2021 shared;analysis 2021;baseline methods reported;present results detailed;improvement 82 baseline;multi;results detailed;shared task;methods reported achieved;performance;results detailed analysis;present results;2021;baseline methods;achieved best performance", "pdf_keywords": ""}, "a6e61164e7b385cec0e12093bc270eafd3ef1dbc": {"ta_keywords": "recognition user activities;unsupervised activity recognition;activity recognition;activity recognition employs;unsupervised activity;method unsupervised activity;user activities;user activities method;recognition user;activity;activities method;activities;user physical characteristics;problem recognition user;activities method effective;recognition;user physical;characteristics height gender;end user physical;unsupervised;problem recognition;recognition employs information;recognition employs;height gender;information end user;method unsupervised;physical characteristics height;propose method unsupervised;physical;participants", "pdf_keywords": ""}, "d7c1bdafb51fe1a757604f9daeaea812f124320f": {"ta_keywords": "forecasting government contracts;technology forecasting based;predicting government contract;predict government contract;technology forecasting systems;technology forecasting;forecasting based government;forecasting government;proposed forecasting government;parameter technology forecasting;forecasting based;forecasting systems;forecasting;proposed forecasting;predicting government;proposed predicting government;predict government;forecasting systems analyze;government contracts proposed;performance proposed forecasting;able predict government;based government contracts;government contracts;government contracts introduce;government contract;government contract influence;contract influence future;contracts proposed;predicting;predict", "pdf_keywords": ""}, "fba7c0a51a6301ca4086a5ce59b1f13af9acad7f": {"ta_keywords": "pointwise approach analysis;pointwise approach;analysis data;pointwise;approach analysis data;data;analysis;present pointwise approach;approach analysis;present pointwise;approach;present", "pdf_keywords": ""}, "5d07db93e6fbd9e10713a2f372131c777077062d": {"ta_keywords": "bitwidths deep networks;nodes deep network;discovery bitwidths deep;discovery deep networks;deep network;deep networks minimal;deep networks encoded;enables discovery deep;deep networks;discovery deep;deep network information;nodes deep;learning reinforcement;learning reinforcement learning;networks minimal accuracy;bitwidths deep;encoded nodes deep;reinforcement learning;reinforcement learning reinforcement;reinforcement learning problem;learning problem enables;networks minimal;approach reinforcement learning;loss minimizing computation;networks;discovery bitwidths;relies discovery bitwidths;learning problem;networks encoded;networks encoded nodes", "pdf_keywords": ""}, "e63e1db25f33162cc6e498f983dc3f6e10c9867e": {"ta_keywords": "overlapped speech separation;speech separation proposed;speech separation;extract speaker information;approach extract speaker;extract speaker;extract speech;fully overlapped speech;extract speech sources;speech recordings infer;noisy speech recordings;overlapped speech;speakers observation based;speaker information;speech recordings;able extract speaker;speaker information noisy;information inferred speakers;speakers conditions extract;information noisy speech;speech sources experiments;speakers observation;conditions extract speech;noisy speech;inferred speakers;speech recordings high;speech sources;inferred speakers conditions;recordings infer;recordings infer identities", "pdf_keywords": "speaker extraction network;model speech separation;separation speaker extraction;speaker modeling separation;speaker extraction proposed;domain speaker extraction;speaker extraction;model speech extraction;speech separation approach;speech separation speaker;speaker extraction multi;capable extracting speech;speech separation proposed;conventional speech separation;speech separation combines;independent speech separation;structure speaker extraction;sound separation networks;speech extraction;speech separation;extracting speech;speaker information observation;speaker detection prediction;speaker model speech;speech extraction based;speakerinferred model speech;extract speech;predicted speaker information;extracting speech sources;speaker independent speech"}, "c3d2c60e70cad17ea37cb116ab30e1239405dbdd": {"ta_keywords": "orbit interaction xmath0;zeeman field spin;xmath1 model interaction;interaction xmath0 model;xmath3 case zeeman;xmath0 model interaction;spin orbit interaction;effect zeeman field;linear zeeman field;xmath1 model;interaction xmath0;field xmath1 model;xmath0 model;field spin orbit;case zeeman field;effect zeeman;xmath2 compared xmath3;orbit interaction;xmath2 compared;zeeman field;compared xmath3;field spin;linear zeeman;compared xmath3 case;reduced factor xmath2;spin orbit;non linear zeeman;zeeman field non;strength field xmath1;field xmath1", "pdf_keywords": ""}, "6a730aff0b3e23423a00cb3407eb04e7f6e83878": {"ta_keywords": "variational bayes improve;machine translation;random matrices bayesian;use variational bayes;matrices bayesian;variational bayes;matrices bayesian approach;bayesian approach improves;bcs algorithm predicting;translation use variational;schrieffer bcs algorithm;machine translation use;moses machine translation;data bayesian;bcs algorithm;data bayesian approach;algorithm predicting;bayes improve performance;set random matrices;algorithm terms likelihood;random matrices;bayesian approach;bayesian;fit data bayesian;use variational;algorithm predicting shape;cooper schrieffer bcs;algorithm;schrieffer bcs;variational", "pdf_keywords": ""}, "c06410ce8f9b1c941115d6d96780794e66b27eac": {"ta_keywords": "utterance update approximating;adaptation japanese broadcast;novel incremental adaptation;incremental adaptation;incremental adaptation framework;adaptation japanese;utterance update;utterance utterance update;continuous speech recognition;adaptation framework based;adaptation framework;speech recognition;line adaptation japanese;speech recognition 700k;vocabulary continuous speech;novel incremental;update approximating posterior;adaptation;japanese broadcast news;incremental;word accuracy improved;news large vocabulary;line adaptation;evolution realizes utterance;continuous speech;proposes novel incremental;japanese broadcast;recognition 700k vocabulary;approximating posterior distributions;perform line adaptation", "pdf_keywords": ""}, "68167af17980a14ed5fa2514e61d76d5a6a9bed7": {"ta_keywords": "misinformation social media;spread misinformation social;media content vaccination;content vaccination;news conspiracy theories;news conspiracy;responses pandemic vaccination;mainstream news conspiracy;spread misinformation;examine spread misinformation;pandemic vaccination;underlying conspiracy narratives;content vaccination process;misinformation social;responses pandemic;conspiracy narratives;conspiracy narratives mainstream;narratives mainstream news;pandemic vaccination process;spread social media;health responses pandemic;conspiracy theories reflected;social media effect;media content narratives;media effect public;vaccination;pandemic;reflected social media;presence underlying conspiracy;vaccination process affected", "pdf_keywords": "domains topic clustering;topic clustering websites;topic cluster similar;topic cluster;conspiracies online;social media posts;articles shared twitter;topic clustering;conspiracies online hindered;web articles organized;twitter analyze;content dubious sources;topics web associated;twitter related websites;discussion content scientific;shared twitter analyze;discourse topics web;clustering public discourse;discussion content;web articles content;web articles;topic content;vac conspiracies online;analyze behavior twitter;topic content discussion;content web articles;twitter analyze different;content social media;topics web;behavior twitter related"}, "4a96b2b33786301d59670fa647f99e3dd807abb8": {"ta_keywords": "multiagent learning algorithms;multiagent learning;agents learn continuous;gradient based multiagent;based multiagent learning;algorithms continuous games;games provide convergence;learning algorithms continuous;equilibrium agents converge;agents converge provide;agents converge;agents learn;games agents learn;learn continuous games;nonuniform learning;continuous games oracle;nonuniform learning rates;continuous games;based multiagent;multiagent;continuous games provide;equilibrium agents;class games agents;learning algorithms;learning rates;learn continuous;effects nonuniform learning;games agents;alter equilibrium agents;guarantees class games", "pdf_keywords": ""}, "d9b458d39e0912524032887aaaf922f0e950f0c1": {"ta_keywords": "parallelized neural network;network represented parallelized;learning problem parallel;parallelized neural;represented parallelized neural;parallelization large scale;parallelization large;parallelized;represented parallelized;large scale network;predict shape network;parallelization;parallel known problem;parallel known;based parallelization large;problem parallel known;neural network;predicting shape large;neural network model;parallel;network represented;scale model network;problem parallel;supervised learning problem;model network represented;based parallelization;learning problem;scale network;estimate size network;model network", "pdf_keywords": ""}, "76f9f4bf8d97de5e95d2fd9dd8b50041524fb1cc": {"ta_keywords": "joint knowledge embeddings;joint entity alignment;entity alignment joint;knowledge graphs unified;alignment joint knowledge;knowledge embeddings;knowledge embeddings method;knowledge graph completion;entity alignment improve;entity alignment;improvements entity alignment;improve knowledge graph;aligned entities;alignment improve knowledge;knowledge graph;joint entity;encodes entities relations;knowledge graphs;entities relations;aligned entities present;approach joint entity;set aligned entities;jointly encodes entities;various knowledge graphs;embeddings method jointly;entities relations various;joint knowledge;entities;entity;relations various knowledge", "pdf_keywords": ""}, "476ff888fe3917f92b221c522ffb7bfaa4e1861b": {"ta_keywords": "answering conversational search;retrieval question answering;conversational search;end conversational search;conversational search featuring;introduce open retrieval;question answering conversational;open retrieval;open retrieval question;collection extracting answers;retrieval;search featuring reranker;building functional search;conversational search setting;extracting answers;answering conversational;functional search systems;question answering;reranker reader;reranker reader retriever;featuring reranker reader;functional search;search systems;retrieval question;learn retrieve evidence;search systems build;extensive experiments retriever;extracting answers step;retriever extensive experiments;retrieve evidence", "pdf_keywords": "retrieval conversational search;open retrieval conversational;retrieval conversational retriever;conversational retriever search;retrieval conversational;conversational search systems;functional conversational search;conversational search;conversational search use;retriever conversation;conversational retriever;open domain conversational;propose open retrieval;retriever conversation trained;introduce open retrieval;open retrieval;database conversations proc;database conversations;seeking conversations framework;open retrieval retriever;domain conversational learning;retriever retriever conversation;conversational search setting;seeking conversations;information seeking conversations;retrieval passage model;building functional conversational;domain conversational;retrieval suitable;retrieval retriever"}, "b169c4b6c23efe8cbd4dc29eb97939cbcfba0f28": {"ta_keywords": "persuasive power salespersons;persuasive dialogue construct;persuasive dialogue;power persuasive dialogue;influence satisfaction salespersons;satisfaction salespersons factors;salespersons analyze factors;salespersons 19 subjects;salespersons factors;satisfaction salespersons;salespersons factors derived;persuasive power persuasive;salespersons analyze;factors derived dialogue;influence persuasive power;persuasive power;factors influence persuasive;dialogue acts professional;predictors power salespersons;influence persuasive;power salespersons analyze;salespersons;dialogue acts particularly;dialogue;dialogue acts;power persuasive;construct corpus dialogue;persuasive;corpus dialogue;acts professional salespersons", "pdf_keywords": ""}, "65d3575b1c380b1bcc14ec69ccf6989c04be9493": {"ta_keywords": "graph algorithms;graph algorithms model;graph algorithms improves;graph algorithms allows;model graph algorithms;signal elimisation programming;convergence graph algorithms;features graph algorithms;programming model graph;graph;signal elimisation;improves convergence graph;algorithms;choice graph algorithms;parallelization problem empirically;algorithms model;model graph;novel signal elimisation;convergence graph;algorithms model allows;features graph;easy parallelization;algorithms improves;easy parallelization problem;optimal choice graph;elimisation programming;parallelization;algorithms allows easy;parallelization problem;algorithms allows", "pdf_keywords": ""}, "cf7e8f47ad1c57738dc586109dcf28a22ab67b72": {"ta_keywords": "ordering papers peer;problem ordering papers;ordering papers;papers peer review;papers peer;ordering;peer review;problem ordering;studies problem ordering;peer review present;paper studies problem;algorithm;papers;novel algorithm;peer;present novel algorithm;review;novel algorithm called;paper studies;algorithm called;review present;paper;studies problem;review present novel;studies;present novel;problem;novel;present;called", "pdf_keywords": "peer review bidding;ordering review optimal;bidding ordering reviewers;ordering reviews optimal;reviewer best bid;peer review competitive;ranking reviewer;reviewer bidding stage;ordering reviewer algorithm;reviewer bidding;optimal reviewer;reviewer evaluation bid;bids quality reviewers;reviewer prove optimal;peer review algorithm;review bidding able;optimal reviewer prove;reviews process bidding;quality peer review;reviewer paper ordering;reviewer receive bids;review optimally;review process bidding;review bidding;globally optimal reviewer;review optimal;peer review;optimal final reviewer;item peer review;quality reviewers bid"}, "adc273bd25ab1e2a66543f23c7a801af0dd80e5b": {"ta_keywords": "simultaneous speaker diarization;speaker diarization single;recognition speaker diarization;performs speaker diarization;speaker diarization;speaker diarization based;speaker embeddings simultaneously;speaker diarization method;simultaneous speech recognition;speech recognition speaker;estimated speaker embeddings;dialogue recordings speaker;estimates speaker embeddings;speaker embeddings proposed;simultaneous speaker;single channel dialogue;propose simultaneous speaker;speaker embeddings;channel dialogue recordings;simultaneous speech;recognition speaker;method simultaneous speech;based estimated speaker;challenging dialogue recordings;recordings speaker overlap;dialogue recordings;speech recognition;dialogue recordings proposed;simultaneously performs speaker;estimated speaker", "pdf_keywords": "recognition speaker diarization;speech recognition speaker;automatic speaker diarization;speaker diarization method;speaker diarization;based speaker diarization;speakerembedding estimation;speaker diarization applied;simultaneous speaker diarization;speaker automatic speech;new method speakerembedding;speaker diarization single;recognition speaker;method speakerembedding estimation;speech recognition asr;channel speech recognition;speech recognition;iterative speaker diarization;diarization applied speaker;speaker diarization ar;problem speaker diarization;method speakerembedding;speakerembedding;speaker diarization context;speaker embedding;speaker information given;automatic speech recognition;simultaneous speaker extraction;asr based speaker;speaker diarization error"}, "b20cadef0c59e80f7dfdf825b07442619d920fd5": {"ta_keywords": "identifying nodes network;nodes network search;identifying nodes;network search nodes;network nodes nodes;nodes network nodes;search nodes network;network nodes;network assume nodes;assume nodes network;network search;nodes nodes network;problem identifying nodes;network nodes network;nodes network;nodes nodes;nodes network assume;use network search;search nodes;nodes;assume nodes;nodes network use;network assume;network use network;network;search;use network;network use;novel approach problem;identifying", "pdf_keywords": ""}, "16326359081a42c0b254ee6be39824fd2db07e48": {"ta_keywords": "pivot phrases data;extract pivot phrases;pivot phrases;pivot language;uses pivot language;pivot language models;extract pivot;used extract pivot;method extract pivot;applying extract pivot;phrases data;uses pivot;phrases data method;phrases data used;pivot;method uses pivot;phrases data demonstrate;data used extract;models input translation;input translation;language models input;language models;translation process;phrases;input translation process;translation process method;novel method extract;method used extract;extract;method applying extract", "pdf_keywords": ""}, "336ee50043b916c9e932338c02fd1abc87a6e849": {"ta_keywords": "interacting elementary particles;compositional generalization interacting;generalization interacting elementary;cooperative neural modules;elementary particles;coupled cooperative neural;known interacting elementary;elementary particles model;generalization interacting;cooperative neural;interacting elementary;approach compositional generalization;particles model;compositional generalization;particles model consists;novel reinforcement learning;particles;trained novel reinforcement;reinforcement learning;neural modules;coupled cooperative;novel approach compositional;known interacting;reinforcement learning algorithm;novel reinforcement;interacting;neural modules trained;approach compositional;consists coupled cooperative;reinforcement", "pdf_keywords": "learning compositional generalization;compositional generalization learning;achieve compositional generalization;novel compositional generalization;compositional generalization using;training generalize sentences;using compositional generalization;generalization using compositional;compositional generalization model;compositional generalization simple;compositional generalization ability;generalization propose compositional;compositional generalization;propose compositional generalization;compositional generalization propose;learning compositional;learning generalize;learning neural compositionality;model generalize sentences;compositionality neural networks;generalization model deep;learning generalize given;generalization learning;neural model generalize;generalization learning expression;model learning compositional;generalizes sentences organizing;assessment compositional generalization"}, "89ba434b30a3f1b61bcbcf917842899fe3d2eea4": {"ta_keywords": "statistical machine translation;machine translation;transforming individuality using;machine translation smt;transforming individuality;method transforming individuality;individuality using statistical;individuality using;corpus text identical;text identical semantic;corpus text;parallel corpus text;words effective transform;translation smt proposed;corpus;parallel corpus;individuality;identical semantic content;translation smt;function words;uses parallel corpus;translation;text perform automatic;set function words;statistical machine;transforming;identical semantic;function words effective;text identical;semantic content different", "pdf_keywords": ""}, "63a35d8822a042f6d6cd919fd5d3c9e94df6ee18": {"ta_keywords": "change point detection;detect change points;unsupervised change point;detect change;labeled change point;change point instances;windows detect change;improve change point;change points;change point;novel change point;true change point;unsupervised change;existing unsupervised change;change points online;sample tests sliding;point detection framework;point instances supervision;tests sliding;point detection performance;point detection;point detection methods;learning ground metric;labeled change;point instances;using labeled change;detection performance existing;sliding windows detect;tests sliding windows;detection framework", "pdf_keywords": "detect change supervised;metric change detection;change detection learn;change point detection;detecting change points;detect change points;change supervised learning;change detection;detecting change;different change detection;change learned metric;change supervised;metric identify changes;change points learned;learned metric shift;detect change;change detection synthetic;changes learned metric;detection abrupt changes;approach change detection;identify change points;identify changes sequences;change points training;change points distributions;detection sliding;present metric learning;metric learning;point detection sliding;change detection goalwe;require detecting change"}, "00717c695e4a33318fe5655e2b69e1ba8b61f981": {"ta_keywords": "npnpnp 2015 speech;2015 speech segmentation;speech segmentation challenge;deep learning language;segmentation challenge npnpnp;speech segmentation;challenge combine deep;recognition deep learning;combine deep learning;deep learning;recognition deep;challenge npnpnp 2015;deep learning systems;recognition heterogeneous;joint recognition deep;recognition heterogeneous data;challenge npnpnp;language models challenge;npnpnp 2015;combine deep;npnpnp 2015 presents;2015 speech;approach recognition heterogeneous;contribution npnpnp 2015;segmentation challenge;speech;npnpnp;approach joint recognition;learning language models;joint recognition", "pdf_keywords": ""}, "ba201da15899e78629ee5471e8d336b6b2eb7279": {"ta_keywords": "sharing mobility services;sharing mobility;mobility services;mobility services adapted;efficacy mobility services;mobility services basis;sustainable sharing mobility;routing models traffic;models traffic congestion;traffic congestion;models traffic;routing models;mobility;effects routing models;traffic;assesses effects routing;efficacy mobility;traffic congestion different;routing;overall efficacy mobility;congestion;carlo simulator framework;network proposed algorithms;effects routing;monte carlo simulator;shared effect riders;riders overall efficacy;efficient sustainable sharing;carlo simulator;monte carlo", "pdf_keywords": "socially optimal routing;sharing mobility services;sharing mobility;optimally routing user;modal route routing;multi modal route;route users popular;routing choices socially;routing propose;optimal routing strategies;selfish routing;routing strategies;multi modal routers;routing game;optimal congestion sharing;modal routers facilitate;modality public transportation;optimally routing;specific routing propose;mobility services propose;user specific routing;routing strategies performance;optimal routing;different routing strategies;routing strategy public;routing games;routing user traffic;mobility services;strategy selfish routing;routing choices"}, "0af2ff552ab0555914dee90ccfae18297b2792c9": {"ta_keywords": "end deep network;learning using speaker;channel audio recordings;deep network model;deep network;end diarization models;speaker identification component;channel audio;single channel audio;using speaker identification;audio recordings;diarization single channel;module transfer learning;attention module transfer;speaker identification;help diarization performance;diarization performance;diarization performance including;performs meeting diarization;audio;diarization models;end diarization;diarization models data;convolutional network;local convolutional network;recordings;meeting diarization;audio recordings proposed;end end diarization;convolutional network followed", "pdf_keywords": "predicting binary speaker;speaker identification network;recursive speaker model;binary speaker activity;entropy speaker identification;approach speaker diarization;speaker diarization;speaker identification model;speaker activity images;speaker detection estimation;speaker activity;speaker detection;speaker representations generated;diarization models trained;model speaker identification;speaker detection allneural;end deep network;propose model speaker;joint speaker detection;speaker representations;speaker diarization dinner;method speaker detection;discrete time speaker;entropy speaker;cross entropy speaker;binary speaker;speaker generators model;deep network model;recursive speaker;speaker losses attention"}, "12f3bc02d649645fa8734977e28b0ac839e56371": {"ta_keywords": "faceted network queues;allowable congestion queues;congestion queues searching;parking crowded environment;congestion queues;network queues;network curbside parking;searching parking crowded;parking optimized;parking crowded;network queues subject;curbside parking optimized;density parking;parking optimized given;parking provide;drivers searching parking;searching parking provide;allowable congestion;correlation density parking;queues;curbside parking;constraints allowable congestion;searching parking;parking;queues searching available;faceted network curbside;queues searching;density parking number;parking provide method;parking number drivers", "pdf_keywords": "parking queue traffic;parking queue;searching parking queue;pricing parking city;queueing network based;parking network;performance network parking;parking demand pricing;demand pricing parking;parking spots nodes;area network queues;cities model parking;queueing network;model parking occupancy;parking demand;allowable congestion queues;parking occupancy based;pricing parking;network parking spots;stochastic model parking;network maximizing occupancy;network parking;queue traffic;parking prices able;able parking occupancy;model parking prices;network queues;congestion queues;queue traffic proportional;parking occupancy"}, "b7637d1148da569d2211b5dd9851bca82c6aac43": {"ta_keywords": "dependency parsing;graph based dependency;based dependency parsing;dependency parsing approach;relies dynamic parsing;dynamic parsing;dynamic parsing paradigm;parsing paradigm features;parsing paradigm;parsing;parsing approach;feature selection graph;parsing approach based;selection graph based;imitation learning relies;based dependency;dependency;graph based;selection graph;imitation learning;based imitation learning;framework feature selection;feature selection;learning relies;learning relies dynamic;features added sequentially;sequentially edges;sequentially edges pruned;relies dynamic;graph", "pdf_keywords": "dependency parsing significantly;dependency parsing algorithm;faster projective parsing;based dependency parsing;dependency parsing;bottleneck parsing algorithms;bottleneck parsing;projective parsing algorithm;significant bottleneck parsing;parser able quickly;novel parsing algorithm;projective parsing;parsing algorithms;parsing algorithms speed;dependency parser;parsing significantly restricted;parsing algorithm possible;parsing significantly;projective parsing resulting;representation dependency parser;perform projective parsing;parsing algorithm designed;parsing language;dependency parser function;dependency parsing shown;parsing algorithm;parsing;different parsing tasks;parsing language families;parsing algorithm based"}, "5f8d2da91a6c4b9dd079ccb2706c31bda14ef320": {"ta_keywords": "modeling automated speech;automated speech;automatic captioning tasks;recognition automatic captioning;automated speech recognition;automatic captioning;speech recognition automatic;captioning tasks;speech recognition;captioning tasks demonstrate;captioning;recognition automatic;speech;modeling automated;joint modeling automated;automated;approaches joint modeling;recognition;joint modeling;xmath0 state art;improvements evaluation evaluation;state art methods;improvements evaluation;modeling;evaluation evaluation;demonstrate improvements evaluation;evaluation evaluation proposed;evaluation proposed methods;automatic;state xmath0", "pdf_keywords": "neural networks speech;neural networks acoustic;model automated speech;speech captioning trained;automated speech;synthetic speech dataset;speech contents trained;rnn based captioning;automated audio captioning;speech contents neural;automated speech recognition;speech dataset;description speech transcription;synthetic speech;networks speech contents;speech transcription;audio captioning;speech recognition;captioning trained;speech captioning;speech multi task;analyzing synthetic speech;speech recognition asr;speech dataset consisting;modeling framework speech;networks speech;speech transcription approach;speech samples;framework speech captioning;contents neural networks"}, "298b72096b8a770b0cdb263dd53cf2463b8a1a1d": {"ta_keywords": "embeddings text predict;text embeddings suffice;embeddings suffice causal;text embeddings;content text embeddings;predict content text;text predict content;embeddings text;text predict;dimensional embeddings text;causal adjustment content;deep language models;predict content;embeddings suffice;language models learn;content text;adjustment content text;embeddings;deep language;text;language models;causal;suffice causal adjustment;suffice causal;causal adjustment;causal adjustment study;uses deep language;low dimensional embeddings;content text method;content", "pdf_keywords": "text suffices causal;learning estimating causal;causal estimation text;datasets improve causal;causal inference text;effects observational text;information causal identification;improve causal estimation;causally sufficient embeddings;observational text data;estimating causal effects;efficient estimation causal;corpus effect supervision;empirically validating causal;estimating causal;data causal inference;inference text based;causal identification;observational text;causal identification allow;unsupervised counterparts causal;suffices estimate causal;improve causal;data causal;causal bert improves;text representation learning;causal estimation;prediction task causal;task causal inference;causal estimation relatedwe"}, "ffa07e4d7c8fade2ded5ffeea7265d22d8a0c0ab": {"ta_keywords": "accurate 3d model;stable accurate 3d;3d model;3d model object;accurate 3d;images generative neural;generative neural network;cut images generative;object based neural;images generative;neural network;moving objects represented;3d;neural network method;generative neural;images resulting model;simulation ground truth;moving objects;truth cut images;model object based;objects represented;object based;based neural network;moving images;model object;ground truth cut;objects;neural;based neural;simulation ground", "pdf_keywords": ""}, "41e49fd3af628f1c8201942a659769f7cc21d812": {"ta_keywords": "", "pdf_keywords": "graph based label;labels graphs community;labels graphs;graph based learning;learning label propagation;skewed labels graphs;label propagation;learning label;graph based efficient;graphs community;based label propagation;graphs;scalable graph;scalable graph based;novel graph based;graph based;graph graph strong;graph based algorithms;novel graph;node large label;graphs community structure;millions nodes edges;ranking labels node;graph based graph;propose novel graph;label propagation method;ranking labels;graph strong;present scalable graph;graph based approaches"}, "3bfa808ce20b2736708c3fc0b9443635e3f133a7": {"ta_keywords": "graph neural networks;graph neural;networks gnns;networks gnns mechanism;problem graph neural;gnns mechanism propagating;neural networks gnns;propagate messages prediction;propagating information neighbors;bottleneck causes graph;messages neighbors bottleneck;networks;neighbors creates bottleneck;neighbors bottleneck;bottleneck node;gnns mechanism;creates bottleneck node;graph fail propagate;neighbors bottleneck causes;information neighbors;information neighbors creates;messages neighbors;novel problem graph;aggregates messages neighbors;breaking bottleneck improves;bottleneck node aggregates;neural networks;bottleneck improves;messages prediction;messages prediction task", "pdf_keywords": "squashing graph neural;limitation training graph;graph learns short;networks gnns prevents;networks gnns bottleneck;training graph neural;graph learns;graph neural networks;squashing information gnns;networks gnns;networks gnns use;graph neural;networks gnns gnns;network nodes trained;gnns bottleneck long;label graph learns;nodes trained predict;training graph;representation graph neural;gnns susceptible bottleneck;gnns bottleneck causes;isgraph neural networks;networks powerful representation;model graph neural;gnns bottleneck;gnns fail propagate;nodes trained;demonstrate graph neural;graph learningwe present;squashing prevents gnns"}, "9ab3622b3a801b90907f3ee399f881764db05d06": {"ta_keywords": "constraint attack detection;injection attack distributed;attack distributed cyber;cyber physical attack;constraint attack;attack distributed;data injection attack;respecting constraint attack;attack detection probability;distributed cyber physical;attack detection;false data injection;formulated constrained optimization;physical attack designed;constrained optimization;distributed cyber;injection attack;attack designed;problem optimal parameter;lagrange multiplier optimization;constrained optimization problem;data injection;design false data;optimal parameter;stochastic approximation based;problem formulated constrained;formulated constrained;update lagrange multiplier;optimal parameter values;multiplier optimization", "pdf_keywords": "distributed quantum network;linear attack distributed;distributed quantum;attack distributed cyber;attack detection constraint;secure estimation networked;attack distributed;detector attack design;consider distributed quantum;attack stochastic detection;noisy detector attack;estimation networked cyber;attack stochastic;probability constrained attack;attack algorithm;distributed kalman;noisy data attack;attack algorithm constructed;distributed estimation problem;attack detection probability;based distributed kalman;distributed estimation optimization;attack optimization;distributed kalman filtering;attack algorithm formulated;optimal linear attack;noisy quantum based;attack design;defending attacks networked;attack optimal"}, "f2e544c5333125ee30c1c34b08936b6ef87c97dd": {"ta_keywords": "spoken language systems;neural network architectures;development spoken language;neural network;systems using neural;spoken language;using neural network;language systems;language systems using;novel networks able;using neural;novel networks;networks able achieve;empirical tuning network;network architecture;network architectures approach;network architectures;network architecture tuning;networks;neural;design network architecture;tuning network;tuning network parameters;performance design network;networks able;design network;network parameters;discovery novel networks;architecture tuning;development spoken", "pdf_keywords": ""}, "ecfac0d377db229d58bc88698ad3bfd4b384ef37": {"ta_keywords": "automatic argument identification;review based arguments;argument identification;conferences annotated arguments;argument identification helpful;publications driven arguments;arguments used peer;automatic argument;arguments automatic argument;annotated arguments;arguments automatic;peer review;annotated arguments demonstrate;process peer review;peer review dataset;peer review process;new peer review;peer review based;arguments argue arguments;argue arguments used;argue arguments;arguments argue;driven arguments automatic;arguments;arguments demonstrate decision;based arguments argue;science conferences annotated;arguments used;based arguments;conferences annotated", "pdf_keywords": "mining arguments peer;argument mining;automatic argument identification;argument mining task;learning argumentative analysis;learning argumentative;nonlinear learning argumentative;argument identification helpful;approach argument mining;argument identification;mining arguments;argumentative extraction;automatic mining arguments;peer review findargumentative;argumentative content scientific;argumentative analysis documents;approach argumentative extraction;argumentative labeling achieved;argumentative content;arguments peer reviews;argumentative text;arguments peer review;argumentative labeling;argumentative text argumentative;annotated arguments investigate;argumentative extraction arguments;argumentative argumentative labeling;argumentative analysis;text argumentative;argumentative process easily"}, "6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424": {"ta_keywords": "task driven machine;tasks central engine;machine driven central;unstructured tasks central;machine driven;task driven;driven machine driven;driven machine;engine environment driven;execution unstructured tasks;central engine driven;environment driven engine;communicate central engine;driven central engine;engine driven;engine driven engine;central engine environment;driven engine;unstructured tasks;navigates central engine;engine environment;level systems communicate;central engine;accelerates execution unstructured;driven engine collection;central engine follower;tasks central;level systems;engine central engine;central engine present", "pdf_keywords": "tasks based dialogue;agent task completion;embodied agent chat;dialogue communicate tasks;human dialog simulates;task driven embodied;task completion dialogue;task driven agents;robot complete tasks;dialogue perform tasks;human dialogs;dialogue actions environment;natural language tasks;behavior agent tasks;human human dialogs;communicate tasks;task completion;agent tasks;agent task;embodied intelligence understanding;tasks use human;human dialog;task completion model;tasks simulated;agent tasks including;tasks visual simulation;human human dialog;human human dialogue;tasks context agent;topological task description"}, "51b0609155e3a63afd1dd7dcc3034a5950f90ee0": {"ta_keywords": "features model predictions;features model outcomes;features model;influence features model;assess influence features;model predictions;influence features;model predictions proposed;detect features;features;able detect features;detect features dataset;model outcomes demonstrate;features dataset;features dataset used;model outcomes;predictions proposed procedure;predictions proposed;predictions;model;procedure assess influence;assess influence;dataset used assess;used assess influence;robust used assess;detect;robust;able detect;outcomes demonstrate proposed;influence", "pdf_keywords": "disentangled influence audits;feature influence audits;indirect influence audits;indirect feature influence;influence audits detect;indirect influence features;feature indirect influence;affects classifier audited;indirect influence audit;influence audits method;indirect influence feature;use disentangled influence;audits predictions classifier;classifier audited;influence predictions classifier;feature influence mechanism;influence audits;disentangled influence perform;influence audit;influence audit shape;disentangled influence;influence features;influence audits theory;captures indirect influence;influence features outcome;indirect influence predictions;feature influence;extract indirect influence;encoding indirect influence;disentangling influence using"}, "1a27c23453d3f718d854ac4b57dcf3e81ac51aa8": {"ta_keywords": "model active learning;transferability datasets acquired;active learning;transferability datasets;active learning better;investigate transferability datasets;datasets acquired acquisition;datasets acquired;model active;acquisition model distinct;transferability;successor model active;acquisition model;investigate transferability;models updated frequently;models updated;acquired acquisition model;updated frequently labeled;model distinct successor;labeled data;frequently labeled data;distinct successor model;datasets;successor model;acquisition;models;learning;learning better;cases models updated;frequently labeled", "pdf_keywords": ""}, "9c403ca58853fbb223f6e9fce446bb638f291692": {"ta_keywords": "annotations 595 material;new annotation approach;high quality annotations;entity mention annotations;mention annotations;annotations;annotation approach;annotation;annotation approach intended;new annotation;corpus entity mention;consistent annotations new;new corpus entity;annotations new annotation;annotations new;provide consistent annotations;mention annotations 595;consistent annotations;corpus entity;annotations wide;annotations 595;annotations wide range;quality annotations;quality annotations wide;entity mention;material science synthesis;synthesis procedural texts;trained data corpus;new corpus;present new corpus", "pdf_keywords": ""}, "33e5e4b079535957d1275497f8870ea57762a03d": {"ta_keywords": "sentence attackability online;analysis sentence attackability;demonstrate sentence attackability;sentence attackability associated;attackability online arguments;information sentence attackability;sentence attackability;attack argumentation identify;attack argumentation;reasons attack argumentation;attackability online;online arguments analyze;argumentation identify;argumentation identify relevant;attackability;online arguments;attackability associated;arguments analyze;attackability associated characteristics;argumentation;arguments;reasons attack;driving reasons attack;relevant characteristics sentences;arguments analyze driving;sentence content;proposition types tone;proposition types;characteristics sentences demonstrate;sentence content proposition", "pdf_keywords": "predicting attackability sentences;detecting attackable sentences;sentence attackability online;demonstrate sentence attackability;sentence attackability associated;analysis sentence attackability;attackable sentences arguments;sentence attackability arguments;attackable sentences;attackability sentences setting;attackable sentences setting;language attackability sentences;sentence attackability;attackability sentences use;attackability sentences;english language attackability;attackability online arguments;attacks argumentation identify;attacks sentences using;language attackability;attacks sentences;attack sentences best;attacks argumentation;attackability online;attacked sentences using;demonstrate inference attackability;attack sentences;attackability arguments collected;inference attackability efficiently;attacked sentences"}, "81dfa45c568d7c1d9771ba2a1f07dad96558cff6": {"ta_keywords": "classification phoneme data;applied classification phoneme;classification phoneme;phoneme data proposed;sequential pattern classifier;kernel methods proposed;based nonlinear classification;classifier based conventional;phoneme data;nonlinear classification derived;classifier based nonlinear;nonlinear classification;pattern classifier based;classification derived kernel;pattern classifier;classifier based;classifier;classification;enhanced kernel methods;kernel methods;method applied classification;conventional hmm;novel sequential pattern;classification derived;model hmm;applied classification;hidden model hmm;derived kernel methods;compared conventional hmm;model hmm applied", "pdf_keywords": ""}, "be12a8d9ddb12c9ed292430c38d50093191dd442": {"ta_keywords": "new algorithm clustering;clustering called;clustering;algorithm clustering called;algorithm clustering;present new algorithm;new algorithm;algorithm;new;present new;called;present", "pdf_keywords": ""}, "90db4ddb08df23a4c587e6136e66cb388311473b": {"ta_keywords": "filters learned collaborative;transfer filters learned;text categorization;generalization performance collaborative;filters learned;collaborative learning methods;learned collaborative;learned collaborative learning;feature selection;improving generalization performance;used text categorization;collaborative learning;learning methods direct;categorization;feature selection method;generalization performance;learning methods;improving generalization;collaborative;variation feature selection;performance learning methods;performance learning;learning methods use;transfer filters;investigate performance learning;correlated features;performance collaborative;features data perform;direct transfer filters;learning", "pdf_keywords": ""}, "d409ff05d70f7b9787baf6431a84a178ad726e8d": {"ta_keywords": "constraints demonstrations enabling;soft constraints demonstrations;constraints demonstrations;constraint learning;constraint learning method;constraints total reward;use constraint learning;demonstrations enabling agents;soft constraints;hard soft constraints;constraints;model human decision;alternative decision field;agent trajectory;enabling agents quickly;constraint;demonstrations enabling;decision field;constraints total;enabling agents;learning implicit hard;use constraint;human decision making;decision field theory;agent trajectory length;learning method implement;architecture combines cognitive;settings use constraint;agents quickly adapt;method learning implicit", "pdf_keywords": "inverse reinforcement learning;inverse reinforcement;novel inverse reinforcement;context inverse reinforcement;reinforcement learning constraints;case inverse reinforcement;reinforcement learning irl;learning constraints demonstrations;reinforcement learning;reinforcement learning framework;simple reinforcement learning;constraints learned;constraints learned action;learning constraints;learning constraints explicitly;learning constraints bounded;problem learning constraints;method learning constraints;alternative decision field;ethylhexyl decision field;model orchestration constraints;models human decision;simple reinforcement;constraint learning;ethylethylhexyl decision field;data reinforcement;reinforcement learning case;data reinforcement learning;consider problem learning;learning constraints non"}, "f16c0699a873b0209a370e8e6301b0189785c614": {"ta_keywords": "constraint selection based;constraint selection;active learning;selection based active;based active learning;approach constraint selection;works constraint selection;active learning use;select best constraint;best constraint;constraints significantly accurate;best constraint given;restrictive random selection;constraints;constraint;accurate random selection;selection based;constraints significantly restrictive;novel approach constraint;based sampling select;constraints used;constraints significantly;uncertainty based sampling;random selection;sampling select best;restrictive set constraints;constraint given;set constraints;approach constraint;constraints used previous", "pdf_keywords": ""}, "363eb288abf76f7ab52d7789b30399b4b909dd5a": {"ta_keywords": "bribery schemes voting;optimal bribery schemes;optimal bribery;finding optimal bribery;election vote optimal;vote optimal procedure;bribery schemes;vote optimal;voting use combinatorial;vote use combinatorial;bribery;schemes voting;schemes voting use;candidate vote;candidate vote use;best candidate vote;best possible election;voting;possible election vote;voting use;selecting best candidate;election vote;vote use;election;vote;possible election;best candidate;combinatorial model;finding optimal;candidate", "pdf_keywords": ""}, "0d6a4e45acde6f47d704ed0752f17f7ab52223af": {"ta_keywords": "reinforcement learning tasks;generate natural language;hierarchical model reinforcement;language properties reinforcement;natural language instructions;reinforcement learning exploits;reinforcement learning;exploits natural language;approach reinforcement learning;model reinforcement learning;learning tasks;novel approach reinforcement;natural language;model reinforcement;properties reinforcement learning;reinforcement;instructions build hierarchical;approach reinforcement;reinforcement learning approach;generalize unseen tasks;natural language properties;tasks quickly generalize;learning exploits natural;learning tasks use;strategy generate natural;strategy generate;generate natural;test tasks quickly;test tasks;novel strategy generate", "pdf_keywords": "task learning crafting;learning crafting task;imitation learning crafting;crafting task expressed;crafting tasks;trained tasks;task crafting;crafting task;demonstrations learn data;tasks crafting;arbitrary tasks;successfully train agentscrafting;generated reinforcement learning;tasks crafting based;tasks learning;demonstrations guide learning;learning rrl tasks;arbitrary tasks approach;variety tasks learning;hierarchical tasks crafting;learning crafting;crafting tasks model;generated reinforcement;learned human demonstrations;model trained tasks;generalize new tasks;learning crafting based;learn tasks;dynamically generated reinforcement;supervised reinforcement"}, "88b66f705a329da8292e7b8aa4bfe26de4759cfa": {"ta_keywords": "machine translation;approach machine translation;machine translation does;translation does rely;translation;translation does;instead relies alignment;alignment underlying;concept words instead;alignment;words instead;words instead relies;relies alignment;concept words;relies alignment underlying;rely concept words;new approach machine;words;approach machine;machine;new approach;present new approach;concept;approach;underlying;does rely concept;new;instead;instead relies;rely concept", "pdf_keywords": ""}, "a901185ee0710770420044cace33003109d478e3": {"ta_keywords": "informative rating systems;designing informative rating;rating optimized empirically;rating systems;rating systems online;informative rating;rating optimized choosing;rating optimized;answer choices rating;choices rating;choices rating significantly;rating;choosing answer labels;rating significantly;online platforms answer;phrasing design rating;design rating optimized;platforms answer choices;answer labels;finally design rating;optimized choosing answer;design rating;designing informative;problem designing informative;answer labels numeric;systems online platforms;online platforms;rating significantly altered;answer choices;optimized empirically", "pdf_keywords": "ratings online platforms;rating systems explicitly;effective rating systems;informative rating systems;rating marketplace;rating scale platforms;rating systems;rating systems responsive;rating scale marketplace;rating systems develop;rating marketplace reflect;rating marketplace dynamic;new rating marketplace;designing effective rating;rating informative;rating significantly informative;scale online ratings;rating behavior sellers;rating scale online;rating behavior;rating systems learn;rating informative used;rating systems field;rely rating systems;ratings informative ratings;online ratings important;informative ratings altering;rating robust;numeric rating systems;ratings ratings informative"}, "ee9f40f1c1e77b0b39b6e4a158208614fb4995c0": {"ta_keywords": "predicting urban anomalies;urban anomalies traffic;urban anomalies overview;urban anomalies;anomalies traffic;anomalies traffic anomaly;types urban anomalies;traffic anomaly;detecting predicting urban;crowds individual anomaly;traffic anomaly unexpected;urban datasets;anomaly unexpected crowds;predicting urban;urban datasets obtained;urban sensors event;types urban datasets;anomalies overview;anomaly summarize various;records urban sensors;individual anomaly;anomalies overview main;anomalies;individual anomaly summarize;anomaly summarize;anomaly;urban sensors;traffic;unexpected crowds;anomaly unexpected", "pdf_keywords": "urban anomaly analytics;urban anomaly analysis;urban anomaly detection;predicting urban anomalies;anomaly analysis urban;predict urban anomalies;data urban anomaly;prediction urban anomalies;detect anomaly urban;urban data anomaly;analysis urban anomaly;anomaly framework urban;discovering urban anomalies;urban anomalies automatically;algorithms urban anomaly;urban anomalies using;urban anomaly;urban anomaly important;new urban anomaly;anomaly urban environments;anomaly urban dynamics;urban anomalies challenge;driven urban anomaly;research urban anomaly;spatial anomaly urban;urban anomalies urban;anomalies urban big;urban anomalies;challenge urban anomaly;understand urban anomalies"}, "5bcbc4554a68b38ff4a22b848fb0817b809608b2": {"ta_keywords": "mode harmonic oscillator;oscillator mode harmonic;harmonic oscillator mode;oscillator harmonic frequency;harmonic oscillator frequency;harmonic laser pulse;harmonic oscillator harmonic;oscillator harmonic;generated harmonic laser;single mode harmonic;harmonic frequency comb;calculation spectral density;harmonic oscillator;oscillator frequency comb;applied harmonic oscillator;harmonic laser;harmonic oscillator method;harmonic frequency;spectral density single;mode harmonic;oscillator mode;oscillator frequency;spectral density;comb generated harmonic;calculation spectral;method calculation spectral;oscillator method based;oscillator method;frequency comb generated;generated harmonic", "pdf_keywords": ""}, "c159725940750adbad262ac946ce161bb68e41b5": {"ta_keywords": "automatic speech;automatic speech recognition;speech recognition;speech recognition asr;end automatic speech;convolution model elementary;lightweight dynamic convolution;dynamic convolution model;recognition asr;convolution model;dynamic convolution;recognition asr apply;convolution;model elementary particle;elementary particle;end automatic;recognition;end end automatic;automatic;speech;particle;process self attenuatement;elementary particle form;asr apply lightweight;model elementary;particle form discrete;state art model;particle form;asr;lightweight dynamic", "pdf_keywords": "end speech recognition;e2e automatic speech;speech recognition asr;speech recognition fully;speech recognition;automatic speech recognition;speech recognition novel;encoder decoder attention;automatic speech;including speech recognition;robust speech recognition;advanced encoder;convolution based end;attention convolutional layers;recognition asr;decoder attention based;decoder attention;advanced encoder decoder;decoder advanced encoder;encoder convolution;encoder convolutional layer;automatic robust speech;effective encoder convolution;encoder decoder advanced;convolutional layer decoder;convolutional layers encoder;variety speech recognition;speech recognition problems;encoder convolution layer;encoder decoder proposed"}, "ec6499842d3e51b7dda94f5d0620d6df5c1a1b6d": {"ta_keywords": "unwritten speech languages;speech unwritten languages;unwritten languages;unwritten languages ii;directly speech unwritten;unwritten languages bypassing;valid unwritten languages;unwritten speech;unwritten representation built;speech unwritten;unwritten multilayers systems;text unwritten representation;speech languages;unwritten multilayers;systems directly speech;investigates unwritten speech;unwritten representation;representation valid unwritten;learned representation regenerate;learned representation valid;representation built;text unwritten;languages aim unwritten;learned representation;sufficiency learned representation;ensure learned representation;representation regenerate;languages bypassing;valid unwritten;speech languages aim", "pdf_keywords": ""}, "55faed1fbb1575ffa2609bdc4490586e30df441a": {"ta_keywords": "lingual question answering;machine translation accuracy;translation accuracy cross;accuracy cross lingual;cross lingual data;perform machine translation;translation accuracy;question answering clqa;machine translation using;manual machine translation;translation accuracy closely;machine translation perform;machine translation;cross lingual question;translation perform machine;question answering;set machine translation;create cross lingual;cross lingual;translation using data;translation using;lingual data;lingual data set;lingual question;relationship machine translation;lingual;answering clqa;translation perform;answering clqa create;translation", "pdf_keywords": ""}, "e52115834ac7a529b1f4a7769dd538f143cf3eea": {"ta_keywords": "distributed storage codes;storage codes;general erasure codes;erasure codes;storage codes characterize;erasure codes classes;distributed storage;repair schemes piggybacking;designing distributed storage;codes equivalent;repair schemes;storage;characterization repair schemes;constructions codes present;piggybacking framework;piggybacking general erasure;constructions codes;codes characterize structure;schemes piggybacking framework;classes codes equivalent;explicit constructions codes;codes classes codes;general characterization repair;codes characterize;codes classes;schemes piggybacking;codes present comparison;piggybacking framework designing;erasure;characterization repair", "pdf_keywords": "codes distributed storage;piggybacking codes structure;distributed storage characterization;structure piggybacking code;analysis piggybacking codes;piggybacking codes theorems;achieved piggybacking code;codes distributed memory;codes structure piggybacking;constructions piggybacking codes;piggybacking codes regime;codes scalar mds;storage regenerating codes;storage characterization;piggybacking code constructed;codes repair schemes;bandwidth piggybacking codes;consider piggybacking code;piggybacking code achieving;piggybacking code fundamental;piggybacking code substripes;piggybacking framework distributed;distributed storage;storage characterization allows;piggybacking codes;piggybacking code design;piggybacking codes explicit;schemes piggybacking codes;bandwidth piggybacking code;distributed storage exist"}, "777d7b4141c9ce163de99b747e94c8d1db12e11e": {"ta_keywords": "fairness prediction improved;hoc fairness prediction;fairness prediction;machine learning experiment;predicting outcome machine;prediction desired outcome;framework predicting outcome;prediction improved;predicting outcome;machine learning;prediction;prediction improved removing;outcome machine learning;compute prediction;predicting;compute prediction desired;outcome machine;novel framework predicting;used compute prediction;prediction desired;learning experiment framework;framework predicting;post hoc fairness;hoc fairness;learning experiment;experiment framework discovers;features;learning;fairness;input", "pdf_keywords": ""}, "89e53f116ef732d0abe81ee2218fa862ddc5ddce": {"ta_keywords": "speech translation toolkit;speech translation systems;speech speech translation;translation systems toolkit;translation toolkit;speech translation;translation toolkit designed;translation systems wide;translation systems;translation systems translation;systems translation systems;versions translation systems;systems translation;present speech translation;extraction training decoding;training decoding pipelines;provide versions translation;easily develop speech;training decoding;versions translation;speech speech;translation;develop speech speech;benchmark datasets provide;benchmark datasets;speech;wide range benchmark;develop speech;range benchmark datasets;toolkit", "pdf_keywords": "cascaded speech translation;speech translation cascade;models speech translation;speech translation toolkit;predicting translation spoken;speech translation goal;speech translation;translation models training;recognition machine translation;corpora deep state;end speech translation;translation biological speech;toolkit describing translation;machine translation models;translation pipeline;predicting translation;deep state corpora;translation pipeline based;machine translation;speech translation e2e;high fidelity translation;translation models;translating speech;present speech translation;automatic speech;translation toolkit;translation cascade;translating speech based;translation pipeline proposed;state corpora deep"}, "6d654bab72d062d91f731331f16ea01d7cac0812": {"ta_keywords": "bias gender bias;gender bias;bias gender;gender bias sample;study bias gender;narrative elements common;sample predominantly narrative;predominantly narrative elements;narrative elements;predominantly narrative;biases;sample biases;biases occur;narrative;bias;sample biases occur;biases occur sample;literature;study bias;literature common;construct sample biases;literature common literature;bias sample;common literature;literature use;present study bias;literature construct sample;elements common literature;common literature construct;literature use set", "pdf_keywords": "genderedness trope dataset;bias trope presented;diverse gendered tropes;predicted gender tropes;bias trope present;gendered tropes occur;gendered tropes;bias trope;gender tropes;social bias trope;leaning tropes television;gender tropes generation;genderedness trope;tropes television;score genderedness trope;female leaning tropes;high rated tropes;bias diverse genres;gender bias;rated tropes tropes;gender bias large;tropes occur media;tropes classic television;tropes television series;rated tropes;tropes identify;television literature;film television literature;television literature automatically;social bias literature"}, "e79d1206292bc5e67ba19737d87d4b2ea4a37105": {"ta_keywords": "learns subword tokenization;subword representations;latent subword representations;bias learns subword;subword representations characters;learns latent subword;learns subword;subword tokenization module;subword tokenization;based subword tokenization;subword tokenization end;gradient based subword;latent subword;charformer deep model;inductive bias learns;charformer deep;introduce charformer deep;characters data driven;bias learns;automatically learns latent;subword;representations characters data;learns latent;based subword;automatically learns;tokenization;gbst automatically learns;representations characters;deep model;additionally introduce charformer", "pdf_keywords": "learns subword tokenization;subword representations;subword tokenization large;subword representations characters;bias learns subword;subword tokenization module;efficient subword tokenization;subword tokenization;latent subword representations;based subword tokenization;subword tokenization bridge;subwords directly characters;learns subword;subword tokenization end;representation subwords;scalable representation subwords;convolutional subword levels;representation subwords able;learns latent subword;gradient based subword;learning latent subwords;stringon subword tokenization;latent subwords directly;subword based models;subwords directly;subword features language;subwords able perform;based convolutional subword;novel subword based;subword based model"}, "4cdd533963d8fb21fbf4bb3487bf6a6d60e14e93": {"ta_keywords": "cell image segmentation;segmentation method based;image segmentation method;image segmentation;segmentation method;segmentation;chinese restaurant process;restaurant process model;restaurant process;cell image;version chinese restaurant;paper cell image;chinese restaurant;stochastic process;stochastic;restaurant;stochastic process random;process random field;based conditional iteration;process model;random field based;mode conditional iteration;conditional iteration mode;conditional iteration;model based conditional;rpm stochastic process;paper cell;modified version chinese;proposed model based;image", "pdf_keywords": ""}, "396e942542904dd32d0d70daa39613e5a27cc059": {"ta_keywords": "stacked graphical learning;learning stacked graphical;stacked learning stacked;stacked learning;learning stacked;learning large streaming;large streaming datasets;pass stacked learning;graphical learning large;stacked graphical;streaming datasets;streaming datasets developed;empirically stacked graphical;cost stacked graphical;graphical learning competitive;graphical learning;present stacked graphical;empirically stacked;large streaming;memory cost stacked;learning large;streaming;stacked;cost empirically stacked;single pass stacked;cost stacked;datasets;pass stacked;datasets developed;time memory", "pdf_keywords": ""}, "d7851e80f6072991bc99e2157f05515564f894f4": {"ta_keywords": "massive text phoneme;text phoneme conversion;structured learning massive;learning massive text;phoneme conversion;phoneme conversion method;massive text;structured learning;learning massive;text phoneme;approach structured learning;massive weighted wave;weighted wave based;novel massive weighted;wave based model;weighted wave;text;wave based;massive weighted;phoneme;structured;described massive weighted;novel approach structured;model described massive;learning;based novel massive;wave;novel massive;conversion method based;approach structured", "pdf_keywords": ""}, "254491f0d981fb5d796c374287d439d8d1967088": {"ta_keywords": "genotype immunities xmath0;immunities xmath0 genotype;genotype immunities;effects genotype immunities;genotype immunities gsp;immunities xmath0;xmath0 genotype parent;child genotype;immunities gsp;immunities gsp associated;class genotype immunities;parent child genotype;xmath0 genotype;genotype parent child;gsp associated genotype;associated genotype parent;genotype parent;immunities;associated genotype;effects genotype;genotype;study effects genotype;health indicator adolescents;new class genotype;class genotype;indicator adolescents conducted;gsp associated;indicator adolescents;adolescents conducted systematic;candidate primary health", "pdf_keywords": ""}, "e68762a32ec91587d9761030fc75a8f5ee71c45b": {"ta_keywords": "topic mixture modeling;dirichlet topic tracking;topic tracking models;topic mixture;topic tracking;approaches topic mixture;modeling latent dirichlet;unsupervised adaptation speech;latent dirichlet topic;mixture modeling latent;adaptation speech recognition;latent dirichlet;mixture modeling;problem unsupervised adaptation;unsupervised adaptation;dirichlet topic;adaptation speech;speech recognition;speech recognition proposed;tracking models;modeling latent;mixture;tracking models problem;unsupervised;recognition proposed model;dirichlet;problem unsupervised;models problem unsupervised;recognition output improvements;observations input incorporates", "pdf_keywords": ""}, "e66ade4e28d9f401277194ed8feea5c6e9f18253": {"ta_keywords": "clusters terrorist groups;detecting clusters terrorist;terrorist groups sharing;clusters terrorist;groups sharing similar;terrorist groups;clustering groups;stability clustering groups;clustering groups increasing;clustering;operational similarity organizations;similarity organizations driven;similarity organizations;behaviors stability clustering;stability clustering;groups sharing;detecting clusters;clusters;operational similarity;groups;sharing similar behaviors;highlight operational similarity;terrorist;similarity;sharing similar;diversity operational repertoires;similar behaviors;groups increasing 2009;framework detecting clusters;similar behaviors stability", "pdf_keywords": "terrorist operations clustering;clustering terrorist groups;clustering terrorist;occurrence clusters terrorist;operational similarity terrorist;similarity terrorist groups;similarity patterns terrorist;clusters terrorist groups;networks clustering terrorist;patterns terrorist organizations;dynamics terrorist organizations;terrorist organizations emergence;clusters terrorist;global terrorism database;heterogeneity terrorist operations;terrorist organizations presence;classifying terrorist groups;dataset terrorist organization;terrorism database;similarity terrorist;terrorist organizations unsupervised;clustering attacks similarity;patterns terrorist behaviors;patterns terrorist;sample terrorist groups;terrorist organization precursors;terrorist groups subsets;terrorist groups shared;network terrorist organizations;similarity 17 terrorist"}, "82c4be27b0803c08c56bba4352669c1230a3ea19": {"ta_keywords": "regenerating codes;repair bandwidth provide;total repair bandwidth;regenerating codes introduced;original regenerating codes;repair bandwidth;nodes repair;data reconstruction repair;reconstruction repair resilient;nodes repair fixed;repair resilient;regenerating;helper nodes repair;original regenerating;bound storage capacity;bound storage;upper bound storage;repair resilient presence;reconstruction repair;extensions original regenerating;repair;storage capacity;storage;total repair;required total repair;codes introduced dimakis;capacity;bandwidth;data reconstruction;bandwidth provide", "pdf_keywords": "distributed regenerating codes;regenerating codes distributed;distributed storage regenerating;regenerating code distributed;storage regenerating code;bandwidth regenerating codes;codes distributed storage;proposed regenerating codes;minimal repair bandwidth;repair coding scheme;distributed regenerating;repair bandwidth distributed;regenerating codes capable;minimum repair bandwidth;codes repair scheme;repair regenerating codes;repair bandwidth proposed;regenerating regenerating codes;regenerating code bounded;trivial regenerating code;regenerating codes single;fully distributed regenerating;regenerating codes second;possibility regenerating codes;regenerating codes;standard regenerating codes;simple repair bandwidth;code distributed storage;regenerating codes case;regenerating codes useful"}, "4c42d6412c080fef23ad95b4469efe9cf321ae5d": {"ta_keywords": "semi supervised loss;supervised loss combining;supervised loss;novel semi supervised;semi supervised;unpaired speech text;speech text data;leverage unpaired speech;loss tts rightarrow;speech text modalities;rightarrow loss provide;data quantity speech;differentiable loss tts;speech text;rightarrow loss;rightarrow rightarrow loss;loss tts;text data outperform;quantity speech text;unpaired speech;end differentiable loss;loss combining end;loss combining;corpora;consistent gains corpora;gains corpora;differentiable loss;data outperform;loss provide extensive;loss", "pdf_keywords": "speech encoder;generating unpaired speech;speech encoder method;sequence neural machine;end speech recognition;speech text data;semi supervised loss;art speech encoder;using unpaired speech;speech unpaired text;new semi supervised;sequence neural;sequence sequence neural;ttsasr neural networks;sequence learning;sequence based neural;speech text;unpaired speech sentences;speech recognition;end differentiable speech;neural network sequence;differentiable speech text;learning approach encoder;based sequence learning;unpaired speech;semi supervised;speech unpaired;supervised loss;unpaired speech unpaired;automatically predicting"}, "a714ca5254fb3cd7b06ead36d026c4eb154a7134": {"ta_keywords": "partitioned learning processes;treat disparate features;processes partitioned learning;disparate features;disparate features used;encoded disparate treatment;partitioned learning;explicitly treat disparate;learning processes partitioned;equivalent disparate treatment;treat disparate;disparate treatment characteristic;reduce disparate;disparate treatment;apply disparate treatment;reduce disparate impact;learning processes explicitly;apply disparate;partially encoded disparate;disparate treatment protocol;exactly apply disparate;learning processes;learning;equivalent disparate;nonsensitive features;disparate;encoded nonsensitive features;encoded disparate;nonsensitive features exactly;functionally equivalent disparate", "pdf_keywords": "partitioned learning discrimination;quantum classification optimal;discrimination aware learning;quantum classification;learning discrimination aware;learning discrimination;partitioned learning;utility partitioned learning;accuracy quantum classification;partitioned learning partitioned;learning partitioned;discrimination mechanisms implemented;learning partitioned learning;physically motivated discrimination;learning able discriminate;discrimination aware;discrimination mechanisms;classification feature partitioned;dynamically protected classification;classification present fairness;discrimination mechanisms demonstrate;optimizing accuracy quantum;classification optimal;classification feature dynamically;motivated discrimination mechanisms;classification optimal decision;dynamic learning processes;classification;treatment discrimination;disparity harnessed"}, "69320030be096e78380a097810554b648e7409c0": {"ta_keywords": "speaker clustering based;speaker clustering;method speaker clustering;problem speaker clustering;statistical properties speaker;speaker proposed method;clustering based statistical;model speaker applicable;speaker contained mathematical;mathematical model speaker;speaker applicable class;clustering;model speaker;speaker contained;speaker provide unified;problem speaker;properties speaker;clustering based;method speaker;information speaker contained;speaker;speaker proposed;properties speaker provide;novel method speaker;information speaker proposed;properties information speaker;information speaker;speaker applicable;speaker provide;approach problem speaker", "pdf_keywords": ""}, "0639cbb07ec3e03de7c8c1d828a90049c92cf5df": {"ta_keywords": "perovskites asno3 perovskites;bonding orthorhombic perovskites;orthorhombic perovskites asno3;perovskite crystal chemistry;asno3 perovskites;perovskites asno3;asno3 perovskites importance;perovskites importance sn4;bond distance perovskite;perovskite crystal;orthorhombic perovskites;distance perovskite crystal;perovskites importance;perovskite discussed importance;distance perovskite discussed;perovskite discussed;perovskites;distance perovskite;perovskite;sn4 xmath0 bond;crystal chemistry discussed;chemical bonding orthorhombic;crystal chemistry;chemistry discussed sn4;bonding orthorhombic;xmath0 bond;xmath0 bond distance;discussed sn4 xmath0;chemical bonding;importance sn4 xmath0", "pdf_keywords": ""}, "62924cef027a66a75b5465ebb7a926c06f95790f": {"ta_keywords": "adaptation domain adversarial;domain adversarial alignment;domain adversarial algorithms;domain adversarial;standard domain adversarial;domain adaptation;domain adaptation domain;adversarial alignment approach;adversarial alignment;domain adaptation algorithm;novel domain adaptation;adversarial algorithms;adaptation domain;adversarial;adversarial algorithms characterize;version domain adaptation;synthetic real datasets;robustness standard domain;adaptation algorithm;adaptation;propose novel domain;novel domain;adaptation algorithm based;real datasets;improvement robustness;significant improvement robustness;standard domain;datasets;domain;robustness", "pdf_keywords": "adversarial distribution distance;domain adversarial;adversarial distribution;propose domain adversarial;domain adversarial networks;use adversarial distribution;domain adversarial learning;based domain adversarial;minimized adversarial training;adversarial training distances;domain adversarial uses;practically minimized adversarial;method domain adversarial;distances optimized adversarial;adversarial training task;adversarial training;adversarial training optimized;adversarial training strategy;minimized adversarial;optimized adversarial training;domain adaptation;alignment domain learning;notion adversarial training;propose domain adaptation;adversarial;optimized adversarial;target domain classification;adversarial networks simultaneously;adversarial learning;training domain generalized"}, "9b5cf607f9cd3eb5ef47d3597bb9360ea6034264": {"ta_keywords": "peer review;performance peer review;peer review aim;peer;structure performance peer;performance peer;review;review aim paper;computational science present;review aim;computational science;computational;field computational science;tutorial structure performance;paper presents tutorial;provide collection experiments;paper provide framework;field computational;presents tutorial structure;collection experiments;tutorial structure;paper provide;tutorial intended;state art;paper;performance;tutorial intended pedagogical;current state art;art field computational;tutorial", "pdf_keywords": ""}, "9a7a4f125d8016e0fad9f6f5e9e0bca4e38b0784": {"ta_keywords": "scalable probabilistic logic;probabilistic logic framework;stochastic logic;relational networks;probabilistic logic;relational networks framework;stochastic logic program;study relational networks;order stochastic logic;scalable probabilistic;parameter learning weight;gradient descent inference;generated parameter learning;learning parameter learning;parameter learning;descent inference queries;stochastic gradient descent;logic framework;present scalable probabilistic;learning parameter;inference queries;parameter learning parameter;descent inference;networks framework;relational;inference queries approximately;networks;probabilistic;stochastic gradient;logic program", "pdf_keywords": ""}, "684e712f59f11d2bdc98be4c210824ab9e6f11f4": {"ta_keywords": "embeddings learned graphs;embeddings graphs trained;learned graphs transfer;unlabeled relational graphs;learning latent relational;latent relational graphs;embeddings graphs;transferable different embeddings;relational graphs captures;different embeddings graphs;graphs generic transferable;transfer different embeddings;embeddings learned;learned graphs;learned graphs generic;graphs trained including;unlabeled relational;different embeddings learned;transfer learning;graphs trained;graphs transfer;relational graphs;framework unlabeled relational;transfer learning framework;graphs uses learned;relational graphs uses;embeddings task;novel transfer learning;embeddings;uses learned graphs", "pdf_keywords": ""}, "22655979df781d222eaf812b0d325fa9adf11594": {"ta_keywords": "existing knowledge bases;knowledge bases;documents questions diverse;supporting documents questions;knowledge bases knowledge;knowledge schemas;knowledge schemas questions;hotpotqa new dataset;bases knowledge schemas;questions diverse constrained;key features questions;question answer pairs;questions diverse;features questions;new dataset 113k;113k question answer;bases knowledge;new dataset;knowledge;questions require finding;dataset 113k;schemas;questions;existing knowledge;documents questions;dataset 113k question;features questions require;sentence level;key features;multiple supporting documents", "pdf_keywords": "question answering dataset;reasoning systems extracting;dataset requires reasoning;answering dataset aims;question answering;reasoning address challenges;machine reasoning;reasoning multiple documents;machine reasoning systems;answering dataset;reasoning systems;reasoning systems test;tested machine reasoning;existing knowledge bases;scale question answering;answered far universe;qa dataset requires;questions test reasoning;existing knowledge base;creating qa dataset;machine reasoning answer;explainable reasoning address;reasoning ability systems;qa dataset;dataset diverse sentence;required reasoning enables;test reasoning ability;requires reasoning multiple;reasoning ability;universe question answered"}, "0f726fcd676baff957574b223b99fd84163ebe6e": {"ta_keywords": "stacked graphical learning;graphical learning meta;graphical learning approach;graphical learning;meta learning scheme;learning approach describing;meta learning;learning meta learning;stacked graphical;approach describing learning;thesis stacked graphical;base learner algorithm;describing learning;learner algorithm;learning approach;learning scheme;tables stacked graphical;learning meta;base learner;describing learning set;learning scheme augments;learning set problems;hyperlinking;learning;stacked;learner algorithm evaluate;thesis stacked;tables stacked;hyperlinking selecting tables;kind base learner", "pdf_keywords": ""}, "4dfa9de9b3b2b222ddbdda934975bf608b8e1fda": {"ta_keywords": "predict constructiveness conversation;annotated dialogues;annotated dialogues demonstrate;constructiveness conversation interacting;dialogue users improved;conversation interacting users;dialogue users dialogue;dialogue users;conversation interacting;50 annotated dialogues;users dialogue users;dialogues demonstrate usefulness;users dialogue;constructiveness conversation;conversation;annotated data training;dialogue;observation dialogue users;dialogues;dialogues demonstrate;predict constructiveness;demonstrate usefulness annotated;novel annotation schema;method predict constructiveness;based observation dialogue;usefulness annotated data;annotated data;usefulness annotated;interacting users;annotated", "pdf_keywords": "conversations group participants;constructive conversations group;group discussions based;group discussions;group discussions common;dialogue structure participants;discussions solving cognitive;constructive conversations;conversations group;quality constructive conversations;discussions common cognitive;collaborative discussions;collaborative discussions solving;collect dialogue data;utterances helping group;conversation schema;group dialogues;dialogue data group;containing collaborative discussions;group discussions unsupervised;conversation schema based;discussions unsupervised manner;discussions solving;group dialogues 14k;deliberation group discussions;conversations;discussions based analysis;project meetings groups;group participants;group collaborations conversations"}, "bdf6ad58338279634d647447751442db8a6e2f77": {"ta_keywords": "convergence properties neural;convergence loss;convergence loss does;apparent convergence loss;neural networks showing;neural networks;initialization partial training;properties neural networks;weights arriving critical;neural;partial training;error surfaces globally;networks;error surfaces;convergence properties;space error surfaces;loss does correspond;nonconvex breaking symmetry;networks showing apparent;surfaces globally nonconvex;apparent convergence;networks showing;movements flat regions;symmetry random initialization;critical points large;points large movements;regions weight space;large movements flat;random initialization partial;analyze convergence properties", "pdf_keywords": ""}, "88051a6dce3b67541d8096647da2f6d31daa9e9a": {"ta_keywords": "words document entities;language models;probability entity spans;knowledge graph relations;joint distribution words;language based models;knowledge graph;document entities occur;distribution words document;entity spans;document entities;entity spans relations;language models parameterize;occur knowledge graph;entities occur knowledge;word language based;annotate posterior probability;words document;posterior probability entity;class language models;probability entity;word language;distribution words;able annotate posterior;improvements word language;annotate posterior;model able annotate;spans relations predict;relations predict joint;graph relations", "pdf_keywords": "knowledge language models;conditional language models;conditional language modeling;language models relational;model wikipedia articles;knowledge graph context;knowledge graph information;entities knowledge graph;knowledge graph model;structured knowledge graphs;knowledge language;incorporates knowledge graph;language modeling structured;knowledge graph;vocabulary articles structured;modeling structured knowledge;predicting relations entities;predict word relation;knowledge graph relations;language models;knowledge graphs;probability topic entity;relation probability learning;neural knowledge language;structure natural language;knowledge oriented subgraph;knowledge graphs include;nlg knowledge sources;language modeling;structure knowledge graphs"}, "bc33c151a375d30d85a99d4e269185bad360b7bf": {"ta_keywords": "random medium particle;particle random medium;stationary random medium;propagation charged particle;charged particle random;medium particle assumed;moving random medium;particle random;random medium assumed;medium particle;propagation charged;random medium random;homogeneous particle assumed;random medium;medium random medium;assumed homogeneous particle;homogeneous particle;study propagation charged;medium assumed homogeneous;stationary random;medium random;charged particle;assumed stationary random;particle assumed stationary;particle assumed moving;particle assumed;propagation;assumed moving random;moving random;particle", "pdf_keywords": ""}, "72ae4bba9aaa30dfba45f6e7e076952a76e2d751": {"ta_keywords": "conversational models presented;conversational model integrates;party conversational models;conversational models;access party conversational;conversational model;memory language model;present conversational model;term memory language;memory language;language model proposed;party conversational;language model measuring;language model;conversational;role participant responses;traditional language model;short term memory;present conversational;term memory;participant role context;participant responses;participant role participant;participant role;context information long;role context information;model measuring participant;role participant;integrates participant role;model integrates participant", "pdf_keywords": "conversational language model;conversation use recurrent;conversation model;vector conversation model;conversation dataset proposed;conversational language;conversation model model;conversation dataset;conversational structure;capable characterizing conversational;characterizing conversational structure;sentences conversation;words conversation;context conversations incorporating;conversation extend model;conversations incorporating;present conversational language;conversations incorporating topic;short term memory;conversational structure natural;ubuntu conversation dataset;context conversations;characterizing conversational;span context conversations;sentences conversation use;humans quality conversations;conversations;measured language model;role party conversations;topic vector conversation"}, "9b52f250376e07c2caddb5f43b8db8b2f300bb51": {"ta_keywords": "xmath0 supersymmetric standard;dynamics xmath0 supersymmetric;xmath0 supersymmetric;noise dominated xmath1;noise dynamics xmath0;xmath5 component supersymmetric;supersymmetric standard model;supersymmetric standard;gaussian noise dynamics;standard model noise;component supersymmetric standard;gaussian noise;supersymmetric;dynamics xmath0;xmath2 component significantly;xmath4 component xmath5;effect gaussian noise;dominated xmath4;model noise;dominated xmath4 component;model noise significantly;xmath4 component;dominated xmath1;dominated xmath1 component;significantly reduced xmath3;xmath4;xmath1 component;reduced xmath3;xmath1 component xmath2;xmath2 component", "pdf_keywords": ""}, "fd8b33299ce6ca81ce54e7d2de555a1a96ca96f1": {"ta_keywords": "discriminative models speech;structured discriminative models;structured discriminative;models speech recognition;discriminative models;speech recognition asr;classification structured sequences;speech recognition;models speech;recognition asr;classification structured;natural language processing;structured sequences;recognition asr discuss;language processing nlp;nlp including classification;processing nlp;area structured discriminative;structured sequences present;processing nlp including;structured models;discriminative;including classification structured;nlp including;language processing;problems natural language;natural language;application structured models;nlp;structured", "pdf_keywords": ""}, "457e1c9476f08fa2c253982e3effcb364487073e": {"ta_keywords": "quantum state random;observing certain quantum;random process reduced;state random process;random process demonstrate;probability observing certain;random process;quantum state reduced;state random;random process random;probability observing;process probability observing;random processes;modification random process;quantum state;quantum;random processes ubiquitous;demonstrate probability observing;random process probability;process random;certain quantum;process random processes;observing certain state;certain quantum state;process demonstrate probability;technology random process;simple modification random;observing certain;modification random;observing", "pdf_keywords": ""}, "b80ce55fbb4aa427439009985c0ce28a34324dc6": {"ta_keywords": "counterterms generated simultaneous;counterterms required simultaneous;simultaneous generation counterterms;generation counterterms method;designing counterterm given;counterterms generated;generation counterterms counterterms;counterterms method;counterterms counterterms method;counterterms method based;counterterms method applied;designing counterterm;problem designing counterterm;counterterms counterterms;counterterm method;generation counterterms;counterterm method applied;counterterms required;counterterms;number counterterms;number counterterms required;form counterterm method;estimate number counterterms;generated simultaneous;form counterterm;simultaneous generation;required simultaneous generation;generated simultaneous generation;counterterm;counterterm given", "pdf_keywords": ""}, "e23c5dafc718f9e55ccf7729ce2d2834b650540a": {"ta_keywords": "speaker clustering method;effective speaker clustering;speaker clustering systems;speaker clustering;novel speaker clustering;accuracy speaker clustering;utterance oriented dirichlet;dirichlet process mixture;hierarchically structured utterance;dirichlet process;process mixture model;mixture model proposed;mixture model;estimation number speakers;clustering method;clustering systems;structured utterance oriented;efficient effective speaker;oriented dirichlet process;clustering systems case;structured utterance;effective speaker;clustering;clustering method based;speaker;dirichlet;improves accuracy speaker;process mixture;accuracy speaker;utterance oriented", "pdf_keywords": ""}, "773e752ab6dc04b43aaf984bcbdd4895c9ab8c2f": {"ta_keywords": "continuous speech recognition;discriminative training acoustic;speech recognition;speech recognition proposed;linear classifier;training acoustic models;linear classifier employed;based continuous speech;based discriminative training;scale linear classifier;discriminative training;finite state transducer;continuous speech;transducer wft based;training acoustic;state transducer;state transducer wft;distributed perceptron algorithm;acoustic models;transducer wft;error based discriminative;based discriminative;transducer;distributed perceptron;large scale linear;classifier;uses distributed perceptron;weighted finite state;classifier employed framework;perceptron algorithm", "pdf_keywords": ""}, "510aef8370d82c4c4ec50de0f645f34f11e549a7": {"ta_keywords": "protein entity recognition;recall protein entity;entity recognition;entity recognition compare;protein entity;entity recognition using;dictionary hmms semicrfs;methods protein entity;recall protein;high recall protein;hmms semicrfs based;recognition using dictionary;dictionary hmms;methods dictionary hmms;protein;hmms semicrfs;dictionary methods;methods protein;new methods protein;entity;conditional random fields;semicrfs based;using dictionary methods;using dictionary;semicrfs based conditional;effective high recall;dictionary methods dictionary;recognition compare performance;datasets;semicrfs", "pdf_keywords": ""}, "7ddddea393c2cd70fe716e2dfc5d77daf58449c0": {"ta_keywords": "social media influence;person content influence;people influence content;influence content;content influence social;content influence;influence varies content;influence social networks;influence content highly;influence content article;analyze people influence;content article influence;person influence;people influence;person influence varies;given person influence;media influence content;content highly influential;article influence;influence social;influence varies;influence;media influence;article influence power;influential;influence power;social networks use;influence power social;power social media;influential power social", "pdf_keywords": "influencers propagation online;influence social networks;social media influencers;influence social media;person spread misinformation;influencers propagation;person content influence;spread misinformation disinformation;influencers propagation mis;impact social media;predict individuals influence;media influencers;influencers;online mis disinformation;content influence social;content influence;media influencers propagation;spread misinformation;topic influencewe;individuals social media;disinformation online;individuals influence;social network examine;person influence certain;determine influence;person influence;misinformation disinformation online;topic influencewe developed;influence ego poststhe;given topic influencewe"}, "1890775da6ba2627a5d6c17a639e2dca7cdc388d": {"ta_keywords": "deterministic reversible switch;generate switch switch;generate switch;reversible switch switch;reversible switch;switch switch switch;used generate switch;switch used generate;single switch switch;switch switch;switch switch used;single switch;switch connected switch;connected switch switch;switch switch applied;connected switch;switch applied single;switch applied;switch switch connected;applied single switch;simple deterministic reversible;switch;single switch connected;switch connected;switch used;deterministic reversible;reversible;simple deterministic;deterministic;generate", "pdf_keywords": ""}, "ccad27088b9098de4eaca8dc449b18766db4b3ab": {"ta_keywords": "style transfer sentences;style transfer unsupervised;style transfer;dataset style transfer;style transfer setting;world style transfer;style transfer papers;transfer sentences;transfer unsupervised learning;method style transfer;pretrained language models;transfer sentences 11;11 diverse styles;23 style transfer;human automatic evaluations;automatic evaluations;tuning pretrained language;diverse styles;unsupervised learning;styles;dataset style;large dataset style;unsupervised learning based;pretrained language;language models significantly;transfer unsupervised;learning based fine;automatic evaluations survey;style;language models", "pdf_keywords": "style transfer paraphrase;style transfer linguistic;transfer paraphrase generation;transfer linguistic paraphrasing;style specific paraphrasers;style transferred sentence;style specific paraphrases;style transfer tasks;text style transfer;style transfer based;unsupervised style transfer;paraphrase generation;style transfer text;style transfer diverse;reformulate style transfer;perform style transfer;style sentences;style transfer datasets;transfer paraphrase;style transfer;style sentences text;styles text generation;evaluate style transfer;style transfer powerful;tool describing sentences;paraphrase generation problem;styles style transfer;style transfer method;automatically generate style;sentence level paraphrasing"}, "703a8252585948a96f5815025f7f03d68033b8bf": {"ta_keywords": "bots self play;agent user bots;training agent user;example agent trained;agent trained;user bots self;problem training agent;self play learning;training agent;agent user;play learning process;users learning;agent trained set;active users learning;empirical example agent;example agent;bots self;user bots;play learning;users learning process;agent;self play;bots;learning process;learning process surprisingly;investigate problem training;learning;users finally empirical;trained;active users", "pdf_keywords": "reward bots dialog;dialog agent learns;bot learn strategies;learns user strategy;user agent learns;agent learns user;play agent bot;dialog present game;conversational agent dialogue;user bot reward;agent dialogue proposed;agent learns;dialog agent;dialogue training agent;agent dialogue self;place agent bot;bot reward;reward bots;agents use negotiation;user bot learn;bot reward bots;variety conversational agents;learns user problem;train conversational agent;automatic agents;agent bot;agent dialogue;given agent dialogue;automatic agents human;conversational agents"}, "fa6c76d466fef633df51745bad85e991c371622c": {"ta_keywords": "photon laser pulse;laser pulse generated;generated single laser;generation single photon;single photon laser;single laser pulse;laser field photon;photon generated single;parametric oscillator laser;oscillator laser field;photon laser;comb photon generated;frequency comb photon;laser field frequency;laser pulse method;laser pulse;photon generated;field photon generated;pulse laser pulse;oscillator laser;pulse laser;laser pulse laser;single laser;single photon;pulse generated single;laser field;comb photon;applied laser field;field photon;pulse generated", "pdf_keywords": ""}, "41a47363d261459c594525ef330e5fccaa8518a0": {"ta_keywords": "accuracy authorship attribution;features accuracy authorship;authorship attribution highly;authorship attribution;authorship attribution strongly;authorship attribution relationship;approach authorship attribution;accuracy authorship;authorship attribution exploits;attribution relationship features;set accuracy authorship;authorship;approach authorship;attribution highly;new approach authorship;attribution;attribution relationship;attribution strongly;attribution highly dependent;attribution strongly dependent;relationship features accuracy;features data;features accuracy;relationship features data;attribution exploits continuous;features data set;attribution exploits;relationship features;data outperforms;data set accuracy", "pdf_keywords": ""}, "98e6197e21ae530cd33eeff144ee556c5cf91dc8": {"ta_keywords": "cognitive models student;cognitive learning;cognitive learning approach;benchmark cognitive models;generated cognitive model;benchmark cognitive;cognitive model;cognitive models;study cognitive learning;cognitive model use;feasibility cognitive model;cognitive model comparing;demonstrate feasibility cognitive;cognitive model quality;generated cognitive;quality generated cognitive;feasibility cognitive;set benchmark cognitive;solutions cognitive model;cognitive;solutions cognitive;study cognitive;observed solutions cognitive;learning framework;framework study cognitive;learning approach;learning;novel machine learning;human domain expert;learning approach use", "pdf_keywords": ""}, "af679d69fcc1d0fcf0f039aba937853bcb50a8de": {"ta_keywords": "unified nested attention;nested attention;nested attention mechanism;long context sequence;sequence modeling neural;modeling long sequences;context sequence modeling;attention mechanism method;long sequences based;sequence modeling;long sequences;attention mechanism;neural machine translation;attention;including long context;context sequence;machine translation masked;examples including long;long context;masked language modeling;attention mechanism demonstrate;language modeling;sequences based linear;machine translation;modeling long;sequences based;sequences;modeling neural;sequence;method modeling long", "pdf_keywords": "softmax attention nested;nested linear attention;softmax attention;unified nested attention;nested attention operations;attention nested linear;nested attention;attention operations linear;simple linear attention;linear attention;sequence modeling neural;approximates softmax attention;nested attention mechanism;attention nested;linear attention mechanism;attention linear;linear attention functions;function nested attention;attention function nested;regular attention function;attention operations;attention mechanism approximates;novel attention mechanism;attention function;long context sequence;sequence modeling tasks;attention functions;attention mechanism based;perform attention operations;algorithm perform attention"}, "682e69be87f181edcf71800b54083595874d4ec6": {"ta_keywords": "speaker trait prediction;trait prediction;speaker trait;problem speaker trait;trait prediction particular;extract predictions model;model predictions subtasks;extract predictions;error reduction classification;features model predictions;classification;model predictions;predictions subtasks;reduction classification accuracy;predictions model utilized;predictions model;classification accuracy;algorithm relies hierarchical;predictions subtasks subsequently;reduction classification;used extract predictions;applied problem speaker;speaker;extract features model;prediction particular;problem speaker;prediction;prediction particular study;relative error reduction;trait", "pdf_keywords": "persuasive speaker traits;predicting speaker traits;speaker traits supervised;speaker trait prediction;predicting relationship speaker;characterization persuasive speaker;method predicting speaker;predicting speaker;prediction propose hierarchical;persuasive speaker;persuasion passion networks;hierarchical model predicting;classifiers passion credibility;prediction train hierarchical;predicting passion credibility;intermediate classifiers passion;hierarchical models persuasion;models persuasion predicted;intermediate classifiers;nodes trained predict;classifiers passion;speaker traits challenging;communities characterization persuasive;traits supervised learning;relationship speaker traits;persuasion predicted accuracy;speaker traits;trait prediction;trait prediction propose;level persuasion tasks"}, "7c8314e6138ce968f3b9f3bc55d5461ffbbec4aa": {"ta_keywords": "generation quantum information;quantum network;quantum network network;quantum information;global quantum network;nodes connected quantum;quantum bus topological;generated global quantum;generation quantum;quantum information form;connected quantum bus;quantum bus;global quantum;quantum state proposed;quantum state;connected quantum;quantum;applied generation quantum;topological phase network;random variables generated;method generation quantum;nodes topological phase;transformation quantum state;transformation quantum;local transformation quantum;nodes topological;phase network;bus topological phase;random variables;variables random", "pdf_keywords": ""}, "97883f37c62b4b0e52cdc31dea1a375597db3804": {"ta_keywords": "tasks single deep;networks trained task;single deep neural;deep neural;networks trained;trained task;individual networks trained;deep neural network;adding multiple tasks;multiple tasks single;multiple tasks;multiple architectures;learn binary masks;overhead added task;single deep;multiple architectures obtain;tasks single;performance new task;neural;neural network;ability learn binary;networks;method multiple architectures;neural network incurring;binary masks;individual networks;network incurring overhead;comparable individual networks;tasks;task use ability", "pdf_keywords": "trained task imagenet;task imagenet classification;task imagenet;adapting single deep;training convolutional;learning mask tasks;training simpler network;imagenet classification;training convolutional neural;imagenet;trained task;learning mask network;pre trained task;learning tasks large;trained tasks;convolutional network semantic;trained task pre;network learning task;convolutional network;training data tasks;learned tasks building;task pre trained;deep neural;learning tasks;multi task training;tasks pre trained;learned tasks;task masks learned;learning fully convolutional;pre trained network"}, "e4c8447e56fc9cc3867087748acc4b259b9efe19": {"ta_keywords": "explicit memory recurrent;memory recurrent neural;text comprehension tasks;recurrent neural networks;model text comprehension;memory recurrent;recurrent neural;text comprehension;augment memory typed;comprehension tasks;trained sequence typed;memory typed edges;explicit memory;comprehension tasks achieve;edges augment memory;augment memory;memory typed;sequence typed edges;encodes explicit memory;recurrent;trained sequence;neural;memory;model trained sequence;directed acyclic subgraphs;sequence typed;networks model trained;typed edges augment;neural networks;directed acyclic", "pdf_keywords": "recurrent entity network;text recurrent neural;networks rnns;text comprehension models;recurrent neural networks;structured rnn;neural network memory;networks rnns interpret;text comprehension tasks;model text comprehension;recurrent neural;recurrent entity;model recurrent neural;neural networks rnns;tokens text recurrent;nlp text comprehension;linguistic representations sequences;chain structured rnn;generate linguistic representations;text recurrent;structured rnn edges;improve performance recurrent;memory long term;memory acyclic graph;rnns interpret relations;text comprehension;comprehension models;graph based neural;sequence tasks;state art recurrent"}, "be8d6a8d3dfe87a4d9171f25bf9a18d502498756": {"ta_keywords": "clustering data global;clustering data;algorithm clustering data;new algorithm clustering;clustering;algorithm clustering;meta algorithm known;meta algorithm;based meta algorithm;data global context;data global;algorithm known;new algorithm;present new algorithm;global context based;algorithm;context based meta;global context;global;data;context based;based meta;meta;known;context;new;present new;based;present", "pdf_keywords": ""}, "bd1cf4279d834699db871e1451d289c49ff2b6de": {"ta_keywords": "step chart dance;steps step chart;step chart;chart dance;predict steps;chart dance dance;predict steps vicinity;selecting steps;selecting steps step;dance revolution video;dance dance revolution;method selecting steps;dance dance;selecting steps vicinity;networks predict steps;dance revolution;dance;effective selecting steps;steps vicinity chart;steps;revolution video game;steps vicinity;steps step;step;revolution video;video game method;neural networks predict;game method;chart based;neural networks", "pdf_keywords": "choreographing electronic music;dance convolutional neural;learning choreograph dance;dance convolutional;music tasks;choreography musical recordings;recognition music tasks;stages dance convolutional;rhythm based video;representation music starting;choreograph dance;music starting point;trained choreograph;music tasks demonstrate;recognition stages dance;music directed;audio representation music;choreographing large datasets;recognition music;cmd rhythm based;method choreography musical;audio track;choreograph dance dance;learning choreograph;networks trained choreograph;dance revolution cmd;rhythm based;representation music;synchronization music directed;simultaneous recognition music"}, "cf46ecac1cb1bdae153be2b909ff3e313034ac9e": {"ta_keywords": "social skills training;training aid contextual;social skills;existing social skills;skills training aid;aid contextual information;aid contextual;examine modality contextual;modality contextual differences;skills training;context existing social;contextual information helpful;contextual differences;non verbal behaviour;modality contextual;contextual information;verbal behaviour;contextual differences context;infer non verbal;training aid;contextual;examine modality;differences context existing;non verbal;skills;training;differences context;context existing;modality;context", "pdf_keywords": ""}, "ef59f05a30972742a714b8903848e4b5dfc5cdaf": {"ta_keywords": "use cases interpretable;cases interpretable machine;workflow extracting relevant;learning use workflow;interpretable machine learning;workflow extracting;cases essential extracting;cases interpretable;present workflow extracting;machine learning use;use cases provide;extracting relevant information;information use cases;interpretable machine;taxonomy use cases;use cases proposed;extracting relevant;use cases;essential extracting relevant;use cases essential;use workflow;proposed methods workflow;learning use;cases provide description;methods workflow;machine learning;workflow;description process evaluating;cases proposed methods;description process", "pdf_keywords": ""}, "9b9ee9a25fc4d9f8ad22c2923c49b8d5d0b83356": {"ta_keywords": "extracting disambiguated hypernymy;hypernyms sets synonyms;disambiguated hypernymy relationships;synonyms synsets;synonyms synsets establishes;sets synonyms synsets;disambiguated hypernymy;hypernyms sets;hypernymy relationships propagates;relationships propagates hypernyms;propagates hypernyms sets;propagates hypernyms;extracting disambiguated;sets synonyms;synonyms;hypernyms;hypernymy relationships;aware relationships synsets;method extracting disambiguated;relationships synsets;relationships synsets method;hypernymy;sense aware relationships;synsets establishes sense;datasets english russian;disambiguated;synsets establishes;synsets;datasets english;synsets method", "pdf_keywords": "extracting disambiguated hypernymy;aware extraction hypernyms;hypernymy extraction based;approach hypernymy extraction;extraction hypernyms propose;extraction hypernyms;disambiguated hypernyms sense;hypernymy pairs corpus;constructed hypernymy datasets;hypernymy extraction explicit;distributional word representations;distributional semantics;improves hypernymy extraction;word sense disambiguation;disambiguated hypernymy relationships;hypernymy extraction;hypernyms sets synonyms;hypernymy extraction task;disambiguated hypernyms;sense aware hypernymy;russian hypernymy extraction;relevant hypernyms synset;sense disambiguation procedure;informative generated hypernymy;set disambiguated hypernyms;hypernyms given synsets;semantics use synsets;hypernyms extracted literature;hypernyms synset;matching generalizes hypernyms"}, "923ddc71f8a453c7995e97b0681a674224a5fc09": {"ta_keywords": "low quality translations;quality translations experiments;high quality translations;quality translations improves;quality translations identifies;quality translations;translations improves efficiency;translations improves;translations experiments;translations identifies features;translations experiments showed;manual error analysis;translations identifies;translations;error analysis proposed;error analysis;features low quality;manual error;manual;improves efficiency manual;low quality;improve efficiency manual;quality;low high quality;identifies features low;methods improve;efficiency manual error;features low;efficiency manual;models classify low", "pdf_keywords": ""}, "407eacc5ade80b54126c300b57b81f4b4f411487": {"ta_keywords": "parity machine translation;machine translation results;machine translation;machine translation systems;strong machine translation;translation systems;translation systems general;human machine parity;translation results revisiting;translation results;machine parity;machine parity machine;parity machine;parity;recommendations assessing human;translation;general human machine;machine parity offer;human machine;assessing human machine;set recommendations assessing;recommendations assessing;systems general human;machine;assess strong machine;recommendations based empirical;human;set recommendations;assessing human;set recommendations based", "pdf_keywords": "evaluation machine translation;machine translation quality;human machine translation;human translation evaluated;machine translation fluency;human translation evaluate;quality machine translation;human translation quality;neural machine translation;higher machine translation;quality human translation;accuracy machine translation;machine translation research;machine translation human;human translation outputs;human translations quality;evaluate quality translations;translators raters translationese;language machine translation;machine translation test;translation evaluate quality;machine translation;translation systems reported;machine translation falls;ability machine translation;easier translate raters;machine translation systems;machine translation remarkable;machine translations;machine machine translation"}, "4bf1ea102e1eb1246929bb77c11ebbd6b6d27500": {"ta_keywords": "learns emphsparse prototype;model learns emphsparse;learns emphsparse;variational inference emph;emphsparse prototype;emphsparse prototype support;inference emph;generative model learns;emphsparse;strong language modeling;novel generative model;novel generative;language modeling performance;language modeling;propose novel generative;generative;generative model;amortized variational inference;variational inference;inducing prior prototype;model learns;emph;sparsity inducing prior;prototype selection distribution;learns;prior prototype selection;inducing prior;variational;achieves strong language;prototype selection", "pdf_keywords": "learns sparse prototype;learning sparse prototypes;prototypes sparse neural;sparse neural language;baseline sparse prototypes;good sparse prototypes;sparse prototypes sparse;sparse prototypes;samples prototypes sparse;prototypes sparse;sparse prototypes achieve;sparse prototype selection;sparse prototypes possible;sparse prototype posterior;induces sparse prototype;prototypes sparse prototype;sparse prototypes large;infer sparse prototype;inducing prior prototype;prototype driven generative;sparse prototypes method;sparse language;learns sparse;sparse neural editor;sparse language modeling;improve performance generative;prototypes demonstrate sparse;prototypes large corpus;advances sparse language;encourage sparse prototype"}, "93a55f3341aa70bb42c0f76b112e2e8da27b3df2": {"ta_keywords": "entrainment selection dialogue;dialogue digital interoperability;interoperability framework eeg;eeg entrainment used;interaction choice dialogue;eeg entrainment;entrainment selection;framework eeg entrainment;entrainment used;entrainment;dialogue digital;interoperability;select dialogue acts;entrainment process;selection dialogue acts;selection dialogue;european dialogue digital;effect entrainment selection;select dialogue;entrainment used select;used select dialogue;use interoperability;human machine interaction;dialogue;entrainment process affects;dialogue acts suitable;digital interoperability;machine interaction;interoperability framework;dialogue acts context", "pdf_keywords": ""}, "bed0452305633791340f80cb0be02f46e4a34b0d": {"ta_keywords": "voice conversion challenge;seq2seq approach voice;voice conversion;approach voice conversion;convert identity speaker;seq transcription models;seq transcription;transcription models convert;sequence seq2seq approach;power seq transcription;seq2seq sequence sequence;sequence seq2seq;sequence sequence seq2seq;seq2seq sequence;present seq2seq sequence;identity speaker target;seq2seq;seq2seq models;transcription;seq2seq approach;power seq2seq models;transcription models;voice;conversion challenge 2020;power seq2seq;seq2seq models power;identity speaker;seq2seq models context;present seq2seq;conversion challenge", "pdf_keywords": "voice conversion challenge;baseline voice conversion;speech processing toolkit;voice conversion;source speech processing;speech results generation;transcribe input speech;generated speech results;considers voice conversion;speech recognition asr;target text speech;generate voice;feasibility generated speech;transcriptions generate voice;automatic speech;seq2seq baseline voice;input speech automatic;speech network;generate voice target;speech processing;speech tts model;speech automatic speech;speech automatic;generated speech;speech recognition;speech processing developed;text speech;using transcriptions generate;speech model;speech model evaluated"}, "ce97452d031a1a156212f038bab6f47a51575236": {"ta_keywords": "automatic recognition stance;recognition stance taking;detection stance;recognition stance;stance taking behavior;stance fourway recognition;detection stance fourway;stance taking;prosodic features boosting;stance;behavior speech;style prosodic features;behavior speech based;taking behavior speech;prosodic features;behavior spontaneous speech;corpus spontaneous speech;speaking style prosodic;stance taking different;spontaneous speech;spontaneous speech present;style prosodic;new annotated corpus;annotated corpus spontaneous;stance fourway;lexical speaking style;speaking style;annotated corpus;spontaneous speech designed;speech", "pdf_keywords": ""}, "995f4e670c0cdcd5afdef08719c2528a682bff05": {"ta_keywords": "speech translation model;translation model fast;achieves faster decoding;faster decoding;faster decoding speed;end speech translation;speech translation;decoding speed;translation model;decoding speed naveveve;decoding speed nave;decoding;corpora fast md;end end speech;model fast md;end speech;corpora fast;md achieves faster;model fast;fast md achieves;evaluations corpora fast;speed naveveve model;speech;speed naveveve;memory;model computer memory;fast md;memory resources;speed nave model;speed nave", "pdf_keywords": "faster decoding speed;faster decoding;achieves faster decoding;adaptive radar short;fast decoder;recognition asr decoder;decoding speed;speech recognition asr;radar short term;nar decoding based;input fast decoder;autoregressive nar decoding;state adaptive radar;adaptive radar;nar decoding;class adaptive radar;recognition asr;decoding based connectionist;asr decoder;decoding speed thatwe;fast state adaptive;radar short;fast decoder decomposable;speech recognition;decoding;short term memory;multi decoder md;decoding based;multi decoder;temporal classification ctc"}, "e2198b039ee5bfa233cf06e65f26a9f3233ada9f": {"ta_keywords": "dialogue acts entrainment;entrainment dialogue acts;entrainment dialogue using;entrainment dialogue act;xmath2 entrainment dialogue;entrainment dialogue;xmath0 dialogue survey;dialogue act word;dialogue acts xmath2;corpus dialogue acts;acts entrainment dialogue;dialogue acts xmath1;dialogue survey;dialogue using corpus;dialogue act;dialogue acts use;estimate entrainment dialogue;dialogue acts;specific dialogue acts;corpus dialogue;dialogue using;study entrainment dialogue;using corpus dialogue;dialogue survey estimate;dialogue;xmath0 dialogue;choice specific dialogue;specific dialogue;acts xmath2 entrainment;data xmath0 dialogue", "pdf_keywords": ""}, "29da62b3f8aed3fe98b3f02bbfd436dd8e65a532": {"ta_keywords": "aware coding wireless;channel state aware;state aware coding;coding wireless networks;increases channel state;coding wireless;aggressiveness increases channel;aware coding;adaptation aka channel;throughput network message;increases channel;improve throughput network;channel state;collision avoidance aggressiveness;channel state principle;throughput network;traditional channel state;aware coding aka;wireless networks;optimal aggressiveness adaptation;network message passing;channel;wireless networks technique;replace traditional channel;aka channel state;collision avoidance;coding aka collision;traditional channel;aggressiveness adaptation mechanism;avoidance aggressiveness adaptation", "pdf_keywords": ""}, "4264599665522594d9ecb521dd2e1d002e85a961": {"ta_keywords": "computing fair matchings;fair matchings;fair matchings approximately;paper matching directly;paper matching;fairness formulation paper;existing matching algorithms;matching algorithms optimizing;fairness flow computing;directly optimize fairness;matching algorithms;optimize fairness;algorithms fairir fair;matchings approximately optimize;computing fair;fairness flow;local fairness formulation;fair fairness flow;fairness formulation;matching directly;novel local fairness;formulation paper matching;optimize fairness demonstrate;achieve fairness orders;matchings;existing matching;achieve fairness;new algorithms fairir;matching;algorithms fairir", "pdf_keywords": "matchings reviewers papers;assign paper reviewers;matchings reviewers;assigning reviewers;algorithm constructing fair;local fairness constraints;fairness constraints ensure;fairness algorithm constructing;assigning reviewers given;optimize fairness fairness;fairness constraints significantly;simultaneously optimize fairness;reviewer paper affinity;fairness constraints present;fairness algorithm;fairness formulation solves;fairness constraints;fairness constraints applicable;optimize fairness;fairness formulation paper;reviewers assigned paper;reviewers algorithm;algorithm fair minimizes;assigned set reviewers;require reviewer affinity;algorithm fair;local fairness formulation;algorithm fairflow;present fairness algorithm;reviewers assignment factor"}, "1578fba4a2b2ba819986e32c7da6ebbaf9aacf41": {"ta_keywords": "model lexical content;lexical content document;characterizing lexical content;lexical content;neural conditional random;hierarchical neural conditional;model lexical;hierarchical neural;use hierarchical neural;use model lexical;characterizing lexical;syntactic description robust;deep neural;approach characterizing lexical;neural conditional;conditional random field;deep neural models;document context;neural models;neural;random field rcrg;lexical;document based;syntactic description;neural models used;content document based;content document context;document context document;description robust changes;syntactic", "pdf_keywords": "feature independently treebanks;lingual description based;independently treebanks resourced;linguistical encoder;treebanks resourced;linguistic data encoder;lemmatization crosslinguality context;encoder leverages linguistic;linguistical encoder leverages;present linguistical encoder;treebanks;features lingual;independently treebanks;task lemmatization crosslinguality;encode morpho syntactic;crosslinguality context task;crosslingual morphological tagging;syntactic description language;features lingual feature;cross lingual description;language characterizing structure;treebanks resourced thuswe;lemmatization crosslinguality;lingual feature;approach crosslingual morphological;crosslingual morphological;multi lingual description;learning lingual;model interpret linguistic;treebanks approach task"}, "6e2e7df21a5b5457ea4167133a40bc729028250d": {"ta_keywords": "neural ranking stacks;ranking datasets substantially;neural ranking;modern neural ranking;passage ranking datasets;ranking datasets;quality resulting ranking;document passage ranking;ranking stacks;resulting ranking stacks;passage ranking;ranking;ranking stacks observed;resulting ranking;ranking stacks appear;information retrieval;mean reciprocal rank;information retrieval tasks;reciprocal rank;improvements information retrieval;reciprocal rank mrr;judged relevant items;preference judgments;retrieval tasks;rank mrr;rank mrr approach;retrieval tasks including;retrieval;make preference judgments;rank", "pdf_keywords": "neural ranker ranking;leaderboard provides ranking;provides ranking;ranking tasks;ranking task;ranking datasets leaderboards;ranking tasks information;ranker ranking;document passage ranking;ranking;queries produce ranked;possible ranking;passage ranking datasets;neural rankers compared;high possible ranking;ranker passage benchmark;ranking datasets;approach ranking tasks;passage ranking;passage ranking task;use neural ranker;neural ranker rank;ranking publicly available;possible ranking propose;features passage ranking;provides ranking publicly;ranking propose;rankers compared;ranking task approach;new ranking"}, "80257b7d02ad4d6a762ebc0d7f1560e0ef182354": {"ta_keywords": "polite sentences preserving;automatic meaning preservation;sentences preserving meaning;converting non polite;polite sentences;non polite sentences;sentences polite;polite sentences polite;sentences polite sentences;sentences preserving;preserving meaning design;generates sentence target;automatic meaning;generates sentence;sentence target style;polite;subsequently generates sentence;non polite;tag generate pipeline;style preserving;style preserving source;preserving source content;sentence target;tag generate;preserving meaning;pipeline identifies stylistic;target style preserving;task automatic meaning;preservation involves converting;sentences", "pdf_keywords": "polite sentences preserving;polite style transfer;politeness transfer accuracy;linguistic transfer style;generating polite sentences;politeness transfer;transfer generated sentences;generating linguistic style;generating polite;polite sentences model;text style transfer;generate linguistic style;content preservation politeness;style transfer tasks;linguistic style target;2013 generating polite;fluency style transfer;preservation politeness model;style transfer approach;retrieve linguistic style;machine translationstyle transfer;politeness model trained;sentences preserving;linguistic style style;linguistic style domain;preserving style transfer;like sentiment transfer;politeness model;automatically generating linguistic;linguistic transfer"}, "09093e29b1f705bb7a68ea2e9240b3f122efe92b": {"ta_keywords": "quantum state estimation;estimation parameters quantum;separation quantum state;parameters quantum based;quantum state method;quantum based separation;based separation quantum;parameters quantum achieved;separation quantum;parameters quantum;quantum state classical;quantum state;classical state method;state estimation parameters;quantum;state estimation;quantum achieved;quantum based;state classical state;choice quantum state;classical state;quantum achieved suitable;classical state achieved;suitable choice quantum;choice quantum;state method;state method based;state classical;estimation parameters;state method applied", "pdf_keywords": ""}, "3edfccbe6adf18f5263cd2adf3d977bbc5811e0b": {"ta_keywords": "novel data augmentation;neural text encoder;data augmentation;recognition speech;data augmentation method;text encoder;text encoder predicts;text paired speech;augmentation method recognition;e2e asr encoder;recognition speech method;encoder;asr encoder;method recognition speech;speech signals build;pretrained e2e asr;paired speech signals;build neural text;asr encoder sequence;speech signals;neural text;speech method;encoder sequence;encoder predicts;encoder sequence characters;augmentation method;additional training data;encoder predicts sequence;augmentation;paired speech", "pdf_keywords": "training text encoder;training speech encoder;speech encoder;speech encoder deep;neural text encoder;end speech recognition;sequence recognition asr;text encoder network;automatic sequence recognition;text encoder synthesis;text encoder model;text encoder;training text;automatic speech;asr encoder sequence;neural textto encoder;neural machine translation;retrained decoder asr;augmentation method attention;machine translation model;sequence recognition;encoder sequence characters;speech recognition;attention based end;automatic speech recognition;text paired speech;underlying speech encoder;speech dataset;encoder sequence;recognition asr"}, "e6aaac94df717786a467d057cb2157b9d49f0974": {"ta_keywords": "regret arbitrary adversarial;regret algorithms;regret algorithms guarantee;algorithms guarantee regret;guarantee regret optimal;regret optimal response;arbitrary adversarial players;regret optimal;achieve regret arbitrary;arbitrary adversarial opponents;regret arbitrary;response arbitrary adversarial;adversarial opponents;adversarial players;arbitrary adversarial;adversarial opponents iii;response convergent opponents;adversarial players based;guarantee regret;family regret algorithms;convergent opponents;adversarial;achieve regret;optimal response arbitrary;ii achieve regret;regret;converge best response;optimal response;tuning knowledge game;best response convergent", "pdf_keywords": "convex concave games;regret games converge;regret learning algorithms;regret algorithms;strategy regret bounded;algorithms guarantee regret;stable game convergence;game smooth convex;regret algorithms guarantee;regret bound variationally;stable game theorems;concave games results;concave games;class regret algorithms;variationally stable games;games algorithms;learning strategy regret;game convergence linear;regret games;social regret games;equilibria games known;optimal learning game;game convergence;equilibria games theorems;bounded social regret;equilibria games;variationally stable game;payoffs stochastic game;strategy variationally stable;sum game adaptive"}, "efaf07d40b9c5837639bed129794efc00f02e4c3": {"ta_keywords": "authorship attribution using;model authorship attribution;authorship attribution;model authorship;authorship;presents model authorship;attribution using continuous;attribution using;representations gram features;representations features neural;learns continuous representations;attribution;continuous representations features;representations features;gram features model;continuous representations gram;gram features;features neural network;representations gram;features neural;using continuous representations;continuous representations;representations;art datasets producing;features model learns;art datasets;neural network;learns continuous;datasets producing;model learns continuous", "pdf_keywords": ""}, "9d9159026023f21e633f84fd61f3efad2e410214": {"ta_keywords": "logic embeddings;logic embeddings task;logic embeddings apply;order logic embeddings;embeddings task knowledge;knowledge base completion;factorization approach learn;latent continuous representations;embeddings datasets task;embeddings datasets;embeddings task;learning order logic;embeddings;representations order logic;learn latent continuous;knowledge base;learn latent;scalable matrix factorization;approach learn latent;matrix factorization;task knowledge base;matrix factorization approach;embeddings apply approach;comparing embeddings;continuous representations;approach comparing embeddings;comparing embeddings datasets;continuous representations order;embeddings apply;factorization", "pdf_keywords": ""}, "46f66dd37e6366ce102cfd97e718947151d5b1eb": {"ta_keywords": "misinformation social media;detect propagation misinformation;fake news detectors;propagation misinformation social;misinformation social;signals social media;news detectors;propagation misinformation;social media method;detect propagation;method detect propagation;observation social media;social media content;content news;media content news;basic fake news;social media;misinformation;content news post;fake news;social media based;basic fake;news;news post;detect;novel method detect;method detect;fake;performance basic fake;media based observation", "pdf_keywords": "detect fake news;detection fake news;fake news detection;predict fake news;fake news detectors;detection recent news;characterizing fake news;characterize fake news;fake news macro;detection best news;fake news environment;distribution fake news;news exploiting fact;news environment propagates;news detection social;news detectors;news exploiting;news detection;keywords fake news;news detectors keywords;presence fake news;propagates news;fake news design;news detection method;fake news network;propagates news report;fake news exploiting;news detection wewe;news social network;news environments"}, "8122eaeb63098e94416108df918c9669e9105e65": {"ta_keywords": "online erasure coding;cluster cache achieved;accelerated data storage;cluster cache;objects cluster cache;erasure coding objects;erasure coding;erasure coding achieved;relies erasure coding;erasure coding problem;binding erasure coding;data storage processing;approach online erasure;storage processing;coding objects cluster;data storage;online erasure;storage processing approach;cache achieved using;cache achieved;cache;storage;erasure;accelerated data;small memory ii;cluster;coding problem accelerated;binding erasure;erasure splitting late;late binding erasure", "pdf_keywords": ""}, "8d64be0d3bb2650ff99a4c1ae8049eb5fece27a1": {"ta_keywords": "emotional speech recognition;method emotional speech;emotional speech;speech recognition;speech recognition problem;speech recognition use;deep neural network;use deep neural;neural network;extract bottleneck features;recognition problem bottleneck;deep neural;bottleneck features;problem bottleneck features;bottleneck features used;bottleneck features apply;neural network hidden;recognition use deep;new method emotional;neural;speech;network hidden layer;problem bottleneck;recognition;bottleneck;method emotional;extract bottleneck;hidden layer;recognition problem;apply method emotional", "pdf_keywords": ""}, "f8f17f32e651840531276423c7196856d27bcdd0": {"ta_keywords": "discrete time stochastic;time stochastic model;stochastic model;stochastic models method;time stochastic models;stochastic models;time stochastic;stochastic models shown;stochastic;harmonic oscillator based;generating discrete time;class discrete time;mode harmonic oscillator;discrete time;harmonic oscillator single;models method based;single mode harmonic;harmonic oscillator method;harmonic oscillator;oscillator based;oscillator single mode;models method;oscillator based use;models;method generating discrete;mode harmonic;oscillator single;oscillator method;model;generating discrete", "pdf_keywords": ""}, "ee7af49291c030a3e29ad7a9cb5c1975d1b644f4": {"ta_keywords": "counterterms required simultaneous;simultaneous generation counterterms;generation counterterms method;counterterms required generation;generation counterterms obtained;counterterms method;counterterms method based;counterterms method applied;required generation counterterms;counterterms composed;counterterms obtained;model counterterms composed;generation counterterms;counterterms required;simple model counterterms;maximum number counterterms;model counterterms method;counterterms obtained applying;model counterterms;applied model counterterms;counterterms;counterterm maximum number;number counterterms;number counterterms required;counterterm maximum;required simultaneous generation;simultaneous generation;counterterms functional;counterterms functional counterterm;counterterm", "pdf_keywords": ""}, "1cbb43b4d7f79d986a4a78ad3b53368c49e496ee": {"ta_keywords": "feature enhancement reverb;enhancement reverb challenge;reverb challenge proposed;reverb challenge;end speech recognition;speech recognition;enhancement reverb;tandem recognizer;reverb;deep neural network;tandem recognizer end;speech recognition state;art tandem recognizer;neural network feature;recognizer end proposed;recognizer;deep neural;recognizer end;channel end speech;network feature enhancement;feature enhancement;neural network;submission deep neural;recognition state art;recognition;joint submission deep;feature;network feature;recognition state;neural", "pdf_keywords": ""}, "d0fbae81d870bbfb34430654f70fd6a21e8bd1cc": {"ta_keywords": "extract power law;power law relationships;law relationships data;relationships data power;power law;law relationships;neuroscience machine learning;method extract power;extract power;data power;neural;neural network;architecture extract power;neuroscience machine;uses neural network;machine learning proposed;uses neural;relationships data;machine learning;network architecture extract;problems neuroscience machine;method uses neural;law;neural network architecture;data;relationships data method;power;architecture extract;used extract power;novel method extract", "pdf_keywords": "recurrent entity representation;novel recurrent entity;recurrent entity;coreference annotations memory;annotations memory;model recurrent layers;existing recurrent neural;recurrent neural network;recurrent neural;recurrent layers;recurrent layers popular;coreference annotations;memory structure entities;coreference annotations useful;annotations memory choice;encoded coreference annotations;uses coreference annotations;recurrent layer;mentions entity;text existing recurrent;coreference annotations extracted;recurrent layer instead;rnn layers;coreference annotations required;memory entity;model recurrent;network rnn layers;layer uses coreference;novel model recurrent;passage encoded coreference"}, "071216d944bcd2f05deafdb94e657167cce148d9": {"ta_keywords": "method;present method;present", "pdf_keywords": ""}, "1ebf54c0a8b38e8c26ed857cb9d4e565a8f17f17": {"ta_keywords": "similarity objects semi;searching aliased objects;framework characterizing similarity;semi structured data;characterizing similarity objects;similarity objects;structured data set;characterizing similarity;structured data;aliased objects semi;similarity;data set framework;aliased objects;graph walks designed;applicable semi structured;objects semi structured;searching aliased;graph walks;semi structured;random graph walks;based random graph;data set;problem searching aliased;searching;objects semi;framework based random;random graph;set framework based;problem searching;graph", "pdf_keywords": ""}, "72a5c01afe276d06ca9179e24b1c925e206454f3": {"ta_keywords": "review generate explanations;explanations content review;generating explanations content;generate explanations content;review using knowledge;content review generate;generate explanations;generating explanations;review generate;graphs generate explanations;explanations content;knowledge graphs generate;content review approach;content review;content review using;knowledge graphs;using knowledge graphs;explanations content leverage;explanations content illustrate;review using;method generating explanations;leverages knowledge graphs;review approach;leverage knowledge graphs;review;review approach leverages;power knowledge graphs;content illustrate;content leverage knowledge;explanations", "pdf_keywords": "recommendations knowledge graph;explain recommendations knowledge;recommendations knowledge;graph based recommender;produce recommendations explanations;recommender method;method explain recommendations;user decision knowledge;knowledge graph based;personalized algorithm graph;recommender systems;recommender systems especially;based recommender method;knowledge graph;recommendations explanations;personalized algorithm;step recommender systems;items knowledge graph;knowledge graph entities;recommendations explanations explanations;knowledge user decision;running personalized algorithm;recommender method jointly;explain recommendations;knowledge user knowledge;information discovering;step recommender;entities using personalized;knowledge entity interaction;based recommender"}, "f800f60db4427a51e564f1b875ae01d2c642fdce": {"ta_keywords": "functional repair;exact repair code;case functional repair;tradeoff exact repair;functional repair interior;exact repair;achieved exact repair;simple exact repair;repair code;repair;repair pointing existence;repair code case;exact repair pointing;repair interior points;repair interior;storage bandwidth tradeoff;storage;code case functional;repair pointing;interior points storage;functional;case functional;storage bandwidth;bandwidth tradeoff achieved;bandwidth tradeoff;points storage;points storage bandwidth;code case;separate tradeoff exact;existence separate tradeoff", "pdf_keywords": "networks storage repair;bound repair bandwidth;storage networks code;repair code network;network coding;network coding case;bandwidth tradeoff repair;repair bandwidth explicit;repair bandwidth;nodes storage networks;distributed networks storage;storage networks arbitrary;principles network coding;network exact repair;repair bandwidth non;code network sufficient;network coding used;regenerating codes repair;network coding recently;tradeoff storage networks;networks storage;storage networks;storage networks fundamental;repair storage nodes;repair codes large;networks code;networks storage bandwidth;repair codes necessarily;network sufficient storage;code based repair"}, "b2f46145f2a50b609482a69d0581b218a6767cef": {"ta_keywords": "knowledge integration systems;knowledge integration;sources knowledge integration;information integration;traditional knowledge integration;integration information sources;information integration uses;information sources knowledge;novel integration information;integration systems;integration uses ranked;integration systems uses;sources knowledge;retrieval methods approximate;process information integration;approximate logic architecture;integration information;integration systems inferences;approximate logic;ranked retrieval methods;logic architecture;logic architecture general;information sources;methods approximate logic;uses ranked retrieval;integration uses;ranked retrieval;retrieval methods;systems inferences logic;logic accurate efficient", "pdf_keywords": ""}, "80fdacd50ba9ad2e594dd2ddb0b1fa0e591f37ea": {"ta_keywords": "structured prediction shared;performance joint inference;event extraction similarly;prediction shared task;information extraction tasks;structured prediction;event extraction;paradigm event extraction;based structured prediction;joint inference;joint inference using;information extraction;simple domain adaptation;domain adaptation;inference using search;complex information extraction;prediction shared;search based structured;learning paradigm event;learned models;learned models significantly;domain adaptation method;extraction tasks;accuracy learned models;shared task;extraction similarly complex;inference;learning;tasks;prediction", "pdf_keywords": ""}, "d46ecbacf42748ac9ce1fecd9f1b4ed0b9e34980": {"ta_keywords": "email speech classification;toolkit email speech;speech act prediction;email speech act;approach email speech;improve email speech;email speech;speech classification exploits;speech classification;speech classification error;speech act;sequences improve email;messages terms gram;gram sequences improve;gram sequences;contextual information messages;classification exploits contextual;act prediction;messages terms;terms gram sequences;speech;source toolkit email;classification error rate;improve email;toolkit email;approach email;new approach email;classification;information messages;messages use novel", "pdf_keywords": ""}, "b350be3836c3d183464642815b26b061f24e8314": {"ta_keywords": "integer embeddings;mathematical world embeddings;set integer embeddings;integer embeddings capture;numerical methods embeddings;mathematical world;embeddings;embeddings improved;methods embeddings used;methods embeddings;embeddings used;embeddings improved using;embeddings capture properties;world embeddings improved;capture properties mathematical;world embeddings;properties mathematical world;mathematical;numerical;embeddings capture;embeddings used capture;analytical numerical;integer;numerical methods;properties mathematical;present set integer;set integer;combination analytical numerical;analytical numerical methods;analytical", "pdf_keywords": "integer embeddings trained;number embeddings learned;integer embeddings mathematical;usefulness integer embeddings;introduce integer embeddings;integer embeddings;embeddings mathematical knowledge;number embeddings;improve number embeddings;embeddings mathematical;integer value embedding;embedding integer sequences;embeddings learned text;embedding integer;integer representations vocabulary;embeddings learned;use embeddings learned;dimensional embeddings language;embeddings language model;embeddings trained;learned embeddings outperform;learned embeddings;word embeddings;knowledge integer representations;type embedding integer;word embedding model;embeddings language;word embedding;classifiers predict integer;embeddings outperform"}, "e602bde46bca5f424a3d53675c1275386544eb1e": {"ta_keywords": "oscillator harmonic trap;trap harmonic oscillator;generated harmonic trap;harmonic trap coupled;trap coupled harmonic;harmonic trap harmonic;harmonic trap;trap harmonic;coupled harmonic oscillator;harmonic oscillator generated;dimensional harmonic oscillator;oscillator generated harmonic;harmonic oscillator;harmonic oscillator harmonic;oscillator harmonic oscillator;oscillator harmonic;dynamics dimensional harmonic;coupled harmonic;trap coupled;generated harmonic;dimensional harmonic;harmonic;oscillator generated;oscillator;trap;dynamics;dynamics dimensional;study dynamics dimensional;coupled;study dynamics", "pdf_keywords": ""}, "7fcc2cc70498e409168a6c3dfd7c59652b1160c2": {"ta_keywords": "adaptation feature deep;feature deep neural;feature transformation matrix;transformation matrices feature;feature deep;feature transformation;matrices feature space;deep neural;deep neural network;improve adaptation feature;adaptation feature;single feature transformation;neural network dnl;matrices feature;feature space structural;feature space structure;fsma improve adaptation;transformation matrices;transformation matrix frame;posteriori linear regression;neural network;feature space;transformation matrix;multiple transformation matrices;maximum posteriori linear;neural;regression fsma improve;use feature space;improve adaptation;structure dnl naxy", "pdf_keywords": ""}, "dda3f2a2803c80e5b3332868bf86901d6239befc": {"ta_keywords": "stochastic distributed optimization;distributed optimization methods;distributed optimization;distributed methods unbiased;distributed methods;tolerant distributed method;new distributed methods;distributed method reasonable;distributed method;fault tolerant distributed;stochastic distributed;analysis stochastic distributed;distributed non convex;compression distributed;unbiased compression distributed;tolerant distributed;compression distributed non;distributed;scalable decentralized fault;decentralized fault tolerant;centralized local sgd;methods unbiased compression;convex optimization problems;convex optimization;non convex optimization;develop new distributed;new distributed;distributed non;scalable decentralized;decentralized fault", "pdf_keywords": "distributed theorem theorems;theorems describing distributed;describing distributed theorem;distributed distributed theorem;theorems consider distributed;distributed theorem;distributed stochastic problems;stochastic methods distributed;distributed stochastic methods;stochastic learning distributed;learning problem distributed;problem learning distributed;problem distributed learning;distributed stochastic method;consider distributed stochastic;problem distributed stochastic;distributed stochastic;stochastically distributed;learning case distributed;stochastic solverswe;distributed optimization stochastic;case distributed stochastic;stochastic gradients distributed;distributed learning consider;methods applicable distributed;distributed stochastic gradient;stochastically distributed optimization;stochastic solvers;distributed method optimal;learning distributed"}, "9dc4a5284ecfd37ab8bc8990eddf1b39113e004b": {"ta_keywords": "translation self training;combining translation self;source target language;context machine translation;target language;translation self;machine translation;machine translation postulate;alleviated combining translation;target language greatly;language greatly mismatch;translation postulate causes;combining translation;local context machine;local context;source target;effect local context;context machine;translation;translation postulate;source target domain;concept source target;domains source target;causes domains source;language;target domain mismatch;domain mismatch propose;context;mismatch formalize;mismatch formalize concept", "pdf_keywords": "training machine translation;adapting machine translation;domain leveraging backtranslation;domains machine translation;translation self training;translation systems;translation systems domain;train machine translation;neural machine translation;machine translation detect;predicting source monolingual;accuracy machine translation;combining translation self;parallel translation data;target monolingual data;translation data accuracy;data machine translation;machine translation powerful;machine translation different;translating low resource;machine translation new;machine translation limited;machine translation systems;mismatch machine translation;machine translation;translation sensitive data;translation data;translation systems designed;alleviated combining translation;machine translation considered"}, "128610c7df12bff1610949c551b6236cb350dcd9": {"ta_keywords": "language models wav22;acoustic language models;models wav22 bert;speech text representations;trained acoustic language;novel modality conversion;pre trained acoustic;pretrained models;speech text;language models;acoustic language;models wav22;wav22 bert;modality gap speech;obtained pretrained models;modality conversion;gap speech text;representations obtained pretrained;autoregressive nar model;modality conversion mechanism;bert bridge modality;logographic languages inference;wav22 bert bridge;trained acoustic;text representations;pretrained models design;autoregressive nar;languages inference;non autoregressive nar;languages inference employ", "pdf_keywords": "models attention acoustic;attention acoustic based;attention decoding text;speech recognition asr;attention acoustic;combining attention acoustic;attention decoding;acoustic language models;language models wav22;end speech recognition;speech recognition novel;speech recognition model;joint attention decoding;attention architecture fully;attention architecture;existing speech recognition;automatic speech;novel nar attention;speech recognition;models attention;nar attention control;nar attention;attenuatement attention architecture;attention data;model joint attention;based models attention;automatic speech recognition;trained acoustic language;attention data controlled;new attention based"}, "4e1d27c68a60bfd8393462107677469bf286f0f8": {"ta_keywords": "pragmatic program bias;program synthesis;pragmatic program;resulting program synthesis;bias program synthesis;program synthesis algorithm;pragmatic program resulting;program synthesis problem;user pragmatic program;bias user program;bias user pragmatic;specifications communication bias;program bias modeled;program bias;program resulting program;notion pragmatic program;program provided specifications;user pragmatic;inductive bias program;pragmatic;program;synthesis algorithm;output program;resulting program;bias program;specifications communication;program resulting;user program;communication bias user;input output program", "pdf_keywords": "pragmatics program synthesis;pragmatic program synthesis;pragmatic program synthesizer;program synthesis interactive;program synthesis;program synthesis significantly;present program synthesis;program synthesis captures;implementation incremental pragmatic;program synthesis general;recursive pragmatics program;program synthesis demonstrate;program synthesis formulation;pragmatics program;incremental pragmatic;build pragmatic program;examples build pragmatic;pragmatic computation program;program synthesis task;pragmatic program;program synthesis method;computation program synthesis;pragmatic computation;present pragmatic program;build pragmatic;program synthesis drawing;efficiently pragmatic program;recursive pragmatics;calculating program synthesis;communicate efficiently pragmatic"}, "09e4e0eee756da5658c6d572871130d53a89c72b": {"ta_keywords": "optimal bic recommendation;bic recommendation policy;recommendation policy given;recommendation policy;bic recommendation;novel probabilistic predictive;probabilistic predictive;probabilistic predictive model;optimal bic;probabilistic probabilistic predictive;determining optimal bic;policy attractive predictive;novel probabilistic;present novel probabilistic;optimal policy attractive;decision subject optimal;subject optimal policy;bic;probabilistic probabilistic;optimal policy;recommendation;provide probabilistic probabilistic;optimal policy able;baselines optimal policy;features utilized predictive;predictive model;probabilistic;policy given decision;predictive model task;predictive", "pdf_keywords": "persuasion decision maker;using persuasion decision;recommending action decision;game persuasion decision;recommending actions optimal;communicating probabilistic decision;probabilistic persuasion;recommend actions decision;novel probabilistic persuasion;bayesian persuasion framework;probabilistic persuasion model;bayesian persuasion;persuasion decision;action desirable decision;incentivize decision;decision certain action;incentivize decisions based;probabilistic decision;based persuasion robust;probabilistic decision rule;problem game persuasion;incentivize decision maker;revealing strategies utility;action optimal decision;game persuasion;markets bayesian persuasion;persuasion robust manipulation;persuasion framework;able incentivize decision;make arbitrary decisions"}, "933b03a81110676f4c61c449f1926ebd58bc47f7": {"ta_keywords": "blind user interface;touchscreens state art;touchscreens accessible blind;dynamic touchscreens accessible;existing dynamic touchscreens;blind users statelens;dynamic touchscreens;touchscreens accessible;explore open touchscreens;accessible blind users;touchscreens state;touchscreens;open touchscreens state;open touchscreens;user interface;design blind user;users design blind;blind users;interactive;statelens reverse engineering;state diagrams existing;design blind;user interface used;accessible blind;blind user;diagrams existing interfaces;diagrams enable users;state diagrams enable;interfaces using crowd;reverse engineering", "pdf_keywords": "user friendly interfaces;user interfaces;engineering user interfaces;user interfaces arbitrary;user interface;touchscreen interfaces;statelens digital touchpoint;touchscreens accessible statelens;user interfaces statelens;interaction user touchpoint;representation user screen;capturing user interactions;user screen facilitate;user interface touching;interaction user screen;dynamic touchscreen interfaces;graphical touchpoint agent;touchpoint interface;generalized touchpoint interface;models touchscreen interfaces;users explore interface;user interface research;touchscreens hope exploring;user touchpoint;user interaction information;user interaction;touchscreen interfaces able;touchpoint interface variety;detection models touchscreen;investigate user interaction"}, "d462eae8dd5c1415e03651b9fc1c2ca80a69521f": {"ta_keywords": "discover student model;problem solving student;article selection task;method article selection;student model;student model predicts;student model novel;article selection;predicts human student;selection task;human student behavior;amounts background knowledge;solving student;discover student;background knowledge achieved;learning mechanism;student behavior human;human student;novel learning mechanism;background knowledge;selection task case;problem solving;model novel learning;learning mechanism allows;coupling student model;solving student deal;learning;problem coverage method;student behavior;simple problem solving", "pdf_keywords": ""}, "730e5e83586dd5784051f933e7bb82571cec4c94": {"ta_keywords": "architecture speaker separation;speaker counting architecture;speaker separation;architecture speaker counting;diarization speech separation;speaker separation novel;speaker counting;speaker diarization;end speaker diarization;processing architecture speaker;speech separation;speaker diarization novel;speech separation propose;framework speaker diarization;speaker diarization speech;parallel architecture speaker;architecture speaker;corresponding number speakers;speakers proposed framework;number speakers;estimating separation masks;diarization speech;framework speaker;number speakers proposed;end speaker;speaker;estimating separation;architecture estimating separation;novel framework speaker;separation masks corresponding", "pdf_keywords": "speech separation network;speech separation application;diarization speech separation;audio separation network;separation speaker counting;speech separation speaker;speech separation methods;combination speech separation;speech separation eend;approach speech separation;domain audio separation;speech separation;separation speaker;audio separation;audio separation separate;speaker counting framework;end speaker diarization;performs speaker diarization;diarization speaker counting;speech separation key;speaker diarization;separated speech speech;optimizing speaker diarization;speaker diarization used;proposed speaker diarization;refining separated speech;tasks speaker diarization;speaker diarization speech;separated speech;speaker diarization able"}, "1548142a6be92f41e45dcbde9ff8afd71134ac1d": {"ta_keywords": "risk polycyclic aromatic;lung cancer risk;polycyclic aromatic hydrocarbons;cancer risk polycyclic;study lung cancer;aromatic hydrocarbons;aromatic hydrocarbons ps;polycyclic aromatic;lung cancer;cancer risk;risk polycyclic;incremental lung cancer;cancer risk ilcr;hydrocarbons ps;hydrocarbons ps based;aromatic;hydrocarbons;systematic study lung;study lung;values incremental lung;cancer;risk ilcr estimated;concentrations diagnostic ratios;mean concentrations diagnostic;polycyclic;incremental lung;lung;mean concentrations pss;concentrations pss estimated;concentrations diagnostic", "pdf_keywords": ""}, "f48792e8a24e369c80e39a2a2b7451d108f02941": {"ta_keywords": "implementation explainable answering;approaches explainable answering;explainable answering;explainable answering xqa;novel implementation explainable;xqa explainable answering;implementation explainable;approaches explainable;answering xqa explainable;answering xqa proposed;xqa explainable;answering xqa;xqa proposed implementation;answering;implementation;explainable;hybrid approaches explainable;presents novel implementation;proposed implementation;computational model exposes;proposed implementation fully;novel implementation;xqa proposed;implementation fully;interpretation;interface;transparent underlying computational;implementation hybrid;underlying computational;xqa", "pdf_keywords": ""}, "642c85d35b4a3cc9648b269e32fe9d0a18907c98": {"ta_keywords": "continuous speech separation;speech separation;speech separation css;framework continuous speech;multi talk dataset;continuous speech;recorded multi talk;talk dataset online;multi talk;talk dataset;separation css task;online processing framework;dual path modeling;separation css;global modeling framework;task global modeling;recorded multi;separation;global modeling;online processing;dataset online processing;speech;path modeling framework;processing framework proposed;modeling framework proposed;dual path transformer;real recorded multi;processing framework propose;processing framework;modeling framework", "pdf_keywords": "speech separation based;continuous speech separation;speech separation multi;method speech separation;speech separation proposed;task speech separation;speech separation;channel speech separation;monaural speech separation;speech separation crs;speech separation important;speech separation factor;long recording separation;behavior speech separation;variancecontinuous speech separation;speech separation css;recording separation;speech separation thewe;modeling method speech;speech recognition;speech recognition machine;channel speech recognition;multi channel speech;framework continuous speech;single channel speech;speech recognition proposed;separation proposed models;recording separation css;speech based combination;efficiently integrate acoustic"}, "acf0ccc8b67cc441c51d4281c305359073b9c7cc": {"ta_keywords": "approach speech translation;lipatov network decoder;translation uses encoder;speech translation;neural networks decoder;speech translation uses;encoder decoder models;decoder models encoder;encoder decoder;decoder models;networks decoder convolutional;decoder convolutional neural;models encoder;decoder convolutional;encoder;encoder model based;encoder model;networks decoder;models encoder model;uses encoder decoder;decoder;uses encoder;lipatov network;novel approach speech;decoder combination xmath0;decoder combination;kuraev lipatov network;network decoder;xmath1 neural networks;approach speech", "pdf_keywords": ""}, "9cfc4e94e76d8025cd86d6652a641b1440681d28": {"ta_keywords": "linguistic structure universe;linguistic structure;characterization linguistic structure;describing structure universe;structure universe algorithm;universe terms corpora;structure universe terms;algorithm characterization linguistic;structure universe;universe algorithm;characterization linguistic;universe algorithm based;structure universe results;information structure universe;linguistic;corpora;corpora corpora;corpora corpora corpora;universe results algorithm;describing structure;corpora contain information;universe terms;corpora corpora contain;terms corpora corpora;corpora contain;terms corpora;universe;problem describing structure;new algorithm characterization;structure", "pdf_keywords": ""}, "d0a6b70c9dc1942169f48211d47843732c57a3a9": {"ta_keywords": "training navigation;navigation model trained;trained vision navigation;training navigation model;agnostic navigation representations;agnostic navigation;environment agnostic navigation;vision navigation;strategy training navigation;navigation representations;vision navigation related;seamlessly trained vision;environment agnostic learning;navigation representations navigation;navigation policy invariant;representations navigation policy;trained vision;navigation model seamlessly;navigation;navigation model;navigation policy;learn environment agnostic;navigation related tasks;navigation related;model seamlessly trained;environments seen training;seamlessly trained;agnostic learning;experiments navigation model;representations navigation", "pdf_keywords": "navigation natural language;trained functional navigation;novel navigation tasks;vision language navigation;language grounded navigation;agents novel navigation;supervised navigation;trained vision navigation;vision navigation multitask;navigation snn approach;navigation natural;navigation model agent;navigation pre learning;data learned agent;supervised navigation intuitive;tasks learns generalized;approach supervised navigation;navigation tasks;knowledge tasks learns;navigation snn;generalized navigation;generalized navigation model;navigation able predict;navigation fundamental task;tasks learns;learned agent;agnostic agent multitask;vision language;agent trained signal;learns generalized environment"}, "cdf17da4a7638985cb62a5dbf1161239b315eb85": {"ta_keywords": "entity link modeling;jointly modeling links;link modeling jointly;models topic models;topic models;link modeling;topic models improve;modeling links text;modeling links;links text entities;text entities linked;entity link;entities linked;entity entity link;model protein protein;model protein;block models topic;protein protein interaction;modeling jointly modeling;predict protein protein;protein interaction;jointly modeling;text entities;predict protein;apply model protein;models topic;modeling jointly;linked;protein protein;stochastic block models", "pdf_keywords": ""}, "c0484ac1677b942e8b06ea0ac3cad5b01e52ced4": {"ta_keywords": "harmonic trap quantum;quantum harmonic trap;trap quantum state;trap quantum;dynamics quantum harmonic;quantum harmonic;bath quantum state;quantum state bath;quantum state generated;state bath quantum;generated single quantum;bath quantum;harmonic trap;dynamics quantum;quantum state;single quantum;quantum;entangled quantum state;single quantum particle;study dynamics quantum;entangled quantum;particle entangled quantum;quantum particle entangled;quantum particle;particle entangled;state generated single;entangled;state generated;trap;harmonic", "pdf_keywords": ""}, "092b80cc6250f74a2c1e0ba7820c31a8f0153c0a": {"ta_keywords": "literature variance;novel invisible cities;cities estimate variance;literature variance compare;variance literature;empirical representations cities;variance literature variance;article digital literature;digital literature methods;estimate variance literature;digital literature;invisible cities;invisible cities described;discovery new literature;cluster empirical representations;cluster empirical;novel invisible;variance given article;quantitative description variance;literature;representations cities estimate;italo novel invisible;author use unsupervised;methods cluster empirical;literature methods;literature facilitate discovery;representations cities;unsupervised methods cluster;description variance;cities described", "pdf_keywords": "cities literary literature;novel cities literary;cities literary;representations city descriptions;novel cities;word representations reader;discovery novel cities;imaginary city text;novel invisible cities;literary literature;descriptions imaginary cities;word representations;text representation methods;representations analyze novel;text representation;literature;learned clusters literature;identify clusters narrative;clusters narrative;word representations use;word representation highly;linguistic similarity characters;clusters text;vector representations city;representations reader;literary literature approach;description imaginary city;representations city;distribution word representations;word representation"}, "63cd8df0041638b0aa74834a81f99ff136951ff1": {"ta_keywords": "binary neurons gan;gan uses binary;neurons gan objectives;neurons gan;network gan;gan objectives network;adversarial network gan;network gan uses;generative adversarial network;gan objectives;novel generative adversarial;gan;binary neurons output;generative adversarial;adversarial network;generative;gan uses;generate binarized digits;binary neurons;uses binary neurons;binary valued predictions;digits able generate;adversarial;propose novel generative;novel generative;able generate binarized;binarized digits able;output layer generator;able generate binary;types binary neurons", "pdf_keywords": "binary neurons gan;neurons gan;neurons gan objectives;trained generative;networks gans proposed;predictions generative;neurons generate binary;networks gans;network gan;predictions generative adversarial;binary neurons generate;model trained generative;network generative;gan uses binary;valued predictions generative;adversarial network gan;model binary neurons;binary neurons train;gradients binary neurons;adversarial network architecture;gans proposed model;generative adversarial network;case generative adversarial;adversarial network model;binary neurons output;novel generative adversarial;adversarial network;generative adversarial networks;generative adversarial;adversarial networks gans"}, "655b842ae905756b2949758bd7e52e5fd32c3642": {"ta_keywords": "beam search speech;search speech recognition;search speech;beam search;speech recognition task;speech recognition;risk beam search;continuous speech recognition;speech recognition method;speech recognition lps;word speech recognition;minimization search;optimize search error;minimization search error;based minimization search;recognition lps decoder;optimize search;search error risk;method optimize search;search errors;recognition lps;search error;search error using;search errors 200k;search;reduction search errors;lps decoder;200k word speech;lps decoder achieved;new continuous speech", "pdf_keywords": ""}, "28421c7f28adfb9ab8aeb56c196ac3ba326efdbb": {"ta_keywords": "xmath0 model spin;xmath1 parameter spin;xmath2 parameter spin;interaction isotropic xmath0;spin orbit interaction;xmath3 parameter spin;isotropic xmath0 model;model spin orbit;strongly dependent xmath1;evolution spin orbit;strongly dependent xmath2;orbit interaction isotropic;strongly dependent xmath3;parameter spin orbit;isotropic xmath0;xmath0 model;spin orbit;orbit interaction strongly;orbit interaction;dependent xmath2;dependent xmath1;dependent xmath2 parameter;dependent xmath1 parameter;model spin;dependent xmath3;dependent xmath3 parameter;evolution spin;xmath1;xmath1 parameter;xmath2 parameter", "pdf_keywords": ""}, "7c72e63aa112193590861887c5d03b640ce90911": {"ta_keywords": "spin orbit interaction;spin orbit coupling;dynamics spin orbit;spin orbit;value spin orbit;magnetic field interaction;dynamics spin;generate magnetic field;orbit coupling;study dynamics spin;coupling constant magnetic;interaction presence magnetic;generate magnetic;orbit interaction;orbit coupling constant;zero value spin;magnetic field;orbit interaction presence;value spin;spin;constant magnetic field;magnetic field non;constant magnetic;field interaction;used generate magnetic;presence magnetic field;magnetic field used;magnetic;presence magnetic;field interaction used", "pdf_keywords": ""}, "3a6334953cd2775fab7a8e7b72ed63468c71dee7": {"ta_keywords": "human social skills;training presence audiovisual;users social skills;social skills training;perform social skills;audiovisual information proposed;social skills;audiovisual information;audiovisual;presence audiovisual information;presence audiovisual;human social;method perform social;perform social;simulation human social;skills training presence;improve users social;users social;social;training presence;skills training experimental;skills training;simulation human;based simulation human;smiling yaw pitch;skills;ratio smiling;considering ratio smiling;training experimental evaluation;training experimental", "pdf_keywords": ""}, "4e9328b2801e158647dff69606ed47d47045eca8": {"ta_keywords": "datalab;datalab allows;data mining tools;named datalab allows;data lab data;data oriented platform;datalab allows users;named datalab;data lab;platform named datalab;data oriented;lab data oriented;data unified manner;concept data lab;unified data oriented;data unified;data processing;data processing operations;data oriented open;propose unified data;data mining;new data mining;unified data;manipulate data unified;lab data;data analysis tasks;data analysis;different data processing;data;users interactively analyze", "pdf_keywords": "data discovery platform;machine learning bioinformatics;new data discovery;data features;data analysis platform;named data discovery;data machine learning;data discovery new;data discovery data;data discovery;data discovery framework;analysis platform data;large datasets;introduce data discovery;discovery data data;data discovery analysis;large scale data;bioinformatics existing tools;data discovery systemwe;data collaborative platform;bias large datasets;platform data collaborative;datasets;dataset introduce novel;datasets relatively large;data modeling platform;machine learning introduce;learning bioinformatics;large datasets relatively;large datasets used"}, "1e9771a264334c45020421b1c847f6bcd88adc60": {"ta_keywords": "inaccuracies annotations crucially;deep networks trained;potentially inaccurate annotations;inaccuracies annotations;inaccurate annotations;networks trained;annotations crucially;deep learning;relevant deep learning;deep networks;annotations crucially relevant;performance deep;annotations preserving;deforming annotations preserving;annotations preserving topology;annotations;performance deep networks;networks trained potentially;deep learning method;based deforming annotations;deforming annotations;original annotations;crucially relevant deep;original annotations result;annotations result;errors original annotations;trained potentially inaccurate;annotations result approach;jointly train network;boosts performance deep", "pdf_keywords": "annotated neural;images accurate annotations;accurately delineating deep;annotations annotated neural;networks dimensional microscopy;trained inaccurate annotations;annotated neural networks;representation annotations network;delineating deep networks;annotations network;microscopy images accurate;annotations network snakes;accurate annotations crucial;accurate annotations;delineating deep;microscopy images;brain annotated data;annotation neurites brain;brain annotated;analyzing annotations network;deep networks trained;annotations training;networks trained;annotations learn structure;deep networks;vision finding axons;deep network;optimal annotations;performance deep;microscopy"}, "80111013916dae3306316c34e13fe856cb08b87b": {"ta_keywords": "negation failure intuitionistic;failure intuitionistic logic;based intuitionistic logic;default logic provide;intuitionistic logic obtain;explicit default rules;default logic;intuitionistic logic;inheritance hierarchies;default rulebase explicit;inheritance hierarchies encoding;rulebase explicit exceptions;case inheritance hierarchies;logic obtain analogues;defaults default logic;language based intuitionistic;exceptions explicit default;default rulebase;intuitionistic logic use;based intuitionistic;rulebase explicit;default rules;explicit exceptions easy;default rules use;hierarchies encoding explicit;negation failure;explicit exceptions;nonnormal defaults;explicit exceptions explicit;case default rulebase", "pdf_keywords": ""}, "69e8c4327193af4549c06809c821c99deb4022cd": {"ta_keywords": "distributed storage coding;distributed data storage;storage coding method;storage coding;distributed storage;coding method distributed;present distributed storage;nodes suitable storage;data storage method;distributed data;storage method based;nodes capable storing;data storage;suitable storage data;storage method;method distributed data;storage data;storage data present;capable storing subset;suitable storage;coding method;storing subset data;storage;capable storing;distributed;storing;coding;storing subset;method distributed;coupled nodes capable", "pdf_keywords": ""}, "a77643bff6f50ccc4f80ec081e4d078a2e788ae7": {"ta_keywords": "multilingual subword regularization;subword regularization;subword regularization points;subword regularization enforces;multilingual subword;effectiveness multilingual subword;method multilingual subword;xtreme multilingual benchmark;multilingual benchmark;multilingual benchmark method;results xtreme multilingual;standard probabilistic segmentations;regularization;xtreme multilingual;tokenized standard probabilistic;probabilistic segmentations results;improves effectiveness multilingual;probabilistic segmentations;multilingual;novel method multilingual;regularization enforces consistency;standard segmentation;method multilingual;regularization enforces;effectiveness multilingual;subword;standard segmentation algorithms;using standard segmentation;tokenized;segmentations results xtreme", "pdf_keywords": "pretrained multilingual models;multilingual pretrained representations;multilingual models improves;optimal multilingual pretrained;segmentation multilingual;method pretrained multilingual;optimal segmentation multilingual;novel subword segmentation;pretrained language encoder;pretrained multilingual;lingual generalization prediction;multilingual pre training;sub optimal multilingual;multilingual pretrained;pretrained language encoders;segmentation multilingual data;cross lingual generalization;variety pretrained multilingual;training method multilingual;languages machine translation;existing subword segmentation;machine translation accuracy;improves crosslingual transfer;existing subword regularization;language models trained;multilingual representations;views machine translation;multilingual models;improves crosslingual;subword regularization methods"}, "bf50833a46839d3932663b472d6145418f9d0bd6": {"ta_keywords": "represent distributed knowledge;distributed knowledge;distributed knowledge proc;distributed knowledge proposed;characterization distributed knowledge;stream attention based;knowledge proc soc;stream attention;knowledge proc;attention based multi;knowledge proposed framework;knowledge;represent distributed;knowledge proposed;array framework proposed;proposed characterization distributed;tables;tables used represent;attention based;used represent distributed;distributed;datasets proc soc;tables tables;characterization distributed;sequence tables tables;tables used;multi array framework;tables tables used;tables tables tables;sequence tables", "pdf_keywords": "array speech recognition;microphone array speech;attention training decoding;attention based multi;microphone arrays;microphone array;proposed attention networks;attention fusion;attention amplitude;array speech;attention amplitude signal;separation attention amplitude;hierarchical attention fusion;microphone arrays proposed;attention based classifier;end speech recognition;amplitude response attention;decoding novel attention;based hierarchical attention;attention networks;attention networks characterized;coupled microphone arrays;distributed microphone array;hierarchical attention;speech recognition ar;based joint attention;joint attention training;novel attention based;attention based structure;speech recognition"}, "d6e21619df572d04b2b2d97b4c5d1fd604f185fb": {"ta_keywords": "chunked tree tessellation;tree tessellation input;algorithm predicting parse;predicting parse;tree tessellation;predicting parse single;tessellation datasets accuracy;unsupervised generation tessellation;chunked tree;parse single chunked;tessellation datasets;single chunked tree;tessellation proposed algorithm;tessellation input;algorithm en tessellation;autoregressive algorithm predicting;generation tessellation;parse;en tessellation datasets;tessellation input experiments;generation tessellation proposed;novel autoregressive algorithm;tessellation;autoregressive algorithm;autoregressive algorithm demonstrated;autoregressive algorithm en;algorithm predicting;tessellation proposed;parse single;tree", "pdf_keywords": "machine translation predicts;neural machine translation;translation model neural;autoregressive parser trained;machine translation model;translation prediction;syntactically supervised transformer;method translation prediction;translation predicts chunked;translation predicts;translation prediction based;machine translation;statistical translation model;predicting syntactic;parser trained;translation model computational;predicts chunked parse;train parsers;predicting syntactic structure;model autoregressive parser;translation model;bottleneck improve translation;model syntactically supervised;models translation quality;autoregressive parser;novel statistical translation;models translation;machine translation context;chunk sequence trained;existing models translation"}, "a5f42552b2368a587aea0a81175b4a79aa614601": {"ta_keywords": "automatically extracting web;extracting web data;extracting web;automatically extracting;web data;effective collaborative filtering;web data web;collaborative filtering;based concept filtering;extract useful information;concept filtering;filtering;data web approach;useful information web;extracted data proposed;web approach;web approach based;concept filtering uses;web pages construct;problem automatically extracting;extract useful;extracted data;quality extracted data;data web;information web;web pages;information web used;filtering uses;extracting;improve quality extracted", "pdf_keywords": ""}, "0e61536550b7263d67b2928473355171dc37c0ae": {"ta_keywords": "oscillator harmonic trap;trap harmonic oscillator;harmonic trap coupled;generated harmonic trap;harmonic trap harmonic;harmonic trap;dimensional harmonic oscillator;trap harmonic;harmonic oscillator coupled;harmonic oscillator generated;oscillator generated harmonic;harmonic oscillator;harmonic oscillator harmonic;oscillator harmonic oscillator;oscillator harmonic;dynamics dimensional harmonic;coupled dimensional harmonic;generated dimensional harmonic;generated harmonic;oscillator coupled dimensional;dimensional harmonic;oscillator coupled;trap coupled;oscillator generated dimensional;trap coupled dimensional;harmonic;oscillator generated;oscillator;trap;dynamics dimensional", "pdf_keywords": ""}, "7f0dbd30dc839fd95ea953a9229c879396ca11c0": {"ta_keywords": "symbolic knowledge bases;representation symbolic knowledge;knowledge bases;knowledge bases fpfs;symbolic knowledge;original semantics scalable;semantics scalable large;semantics scalable;novel representation symbolic;multihop inferences scalable;entities representation;entities representation enables;inferences scalable;representation symbolic;inferences scalable use;neural modules fully;neural modules;multihop inferences;faithful original semantics;semantics;symbolic;enables neural modules;large fpfs representation;model multihop inferences;expressive model multihop;original semantics;representation enables neural;expressive model;fully differentiable expressive;entities", "pdf_keywords": "reasoning knowledge bases;symbolic knowledge base;symbolic knowledge neural;reasoning symbolic representations;representing symbolic knowledge;sparsematrix reified knowledge;encode knowledge large;large scale semantic;learn semantic;knowledge bases;relation vectors knowledge;resulting knowledge bases;model learn semantic;architecture neural semantic;vectors knowledge base;symbolic knowledge;knowledge bases using;generalized encode knowledge;architecture reasoning large;matrix relations entities;underlying knowledge base;knowledge neural;reified knowledge base;entity matrix relations;learn semantic parsers;encode knowledge;knowledge base;vectors underlying knowledge;reasoning knowledge;neural semantic parsing"}, "c2c6c9947dc9d28bb4fc6f965310be517f4d8c57": {"ta_keywords": "voxel based shape;novel method voxel;method voxel based;method voxel;shape synthesis based;based shape synthesis;shape synthesis;voxel based;voxel;synthesis based;based shape;synthesis;shape;novel method;presents novel method;paper;paper presents novel;method;based;novel;paper presents;presents novel;presents", "pdf_keywords": ""}, "81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85": {"ta_keywords": "querying language models;querying language;language models;knowledge language models;approach querying language;language models lps;language models approach;particular language model;language model;accurate knowledge language;query corpora;lps query corpora;knowledge language;query corpora following;language model profession;corpora following topics;novel approach querying;querying;corpora;lps accurate knowledge;particular language;approach querying;corpora following;profession particular language;knowledge use lps;use lps query;automatically discovering;models lps;lps query;automatically discovering better", "pdf_keywords": "prompts extracting knowledge;language models knowledgeable;effective prompts extracting;knowledge characterizing linguistic;language models crucial;accuracy probing knowledge;level knowledge bases;propose mining paraphrasing;facilitate extraction knowledge;mining paraphrasing;extracting knowledge;probing knowledge;mining paraphrasing based;language models;diverse prompts query;knowledge bases;knowledge retrieval;paraphrased prompts rank;indicates language models;knowledge bases lss;successfully extract knowledge;capture linguistic specificity;extracting knowledge lamas;contained language models;improve promptthe language;results examining knowledge;contextualize knowledge retrieval;language models lms;knowledge contained language;approach prompt generation"}, "2c5a410b781f90c145efac05fea235c5c3e44861": {"ta_keywords": "voice conversion;self supervised speech;source voice conversion;speech representation s3r;supervised speech representation;voice conversion vc;supervised speech;speech representation;open source voice;source voice;voice;evaluation self supervised;representation s3r framework;self supervised;cross lingual a2o;cross lingual;conversion vc framework;intra cross lingual;speech;representation s3r;s3r framework;conversion vc;framework s3r 2020;vc framework s3r;conversion;framework s3r;named s3prl vc;s3prl vc context;s3prl vc;lingual", "pdf_keywords": "voice conversion s3r;source voice conversion;speech s3r based;voice conversion;voice conversion vc;voice conversion challenge;framework voice conversion;supervised speech s3r;speakers speech encoder;encode unseen speaker;standard speech synthesis;speech encoder;simple speaker encoder;new speaker encoder;speaker encoder;speech synthesis rss;resulting speaker encoder;lingual voice conversion;present speaker encoder;speech s3r;speech synthesis;speaker encoder sp3r;speaker encoder able;vc framework voice;open source voice;encoder present speaker;speech encoder reference;combination speech encoder;speaker encoder uses;vocoder speaker encoder"}, "3db9649f2ae986cac13f3e748375f8802f9b07fc": {"ta_keywords": "sparsity pruning low;sparsity pruning;pruning low resource;encoding resource sparse;low resource languages;power sparsity pruning;resource sparse;resource sparse set;pruning low;pruning;sparse;sparsity improves robustness;sparse set nodes;robustness encoding resource;nodes sparsity improves;resource languages;sparse set;sparsity improves;low resource;nodes sparsity;set nodes sparsity;resource languages introduce;training distribution power;especially datasets distinct;datasets distinct training;distinct training distribution;robustness encoding;sparsity;especially datasets;distinct training", "pdf_keywords": "low resource translation;compression memorization low;high performance translation;translation low resource;machine translation parallelized;compression memorization;linguistics sparse;pruning translations english;pruning translations;resource translation languages;learning translation techniques;computational linguistics sparse;learning translation;translation techniques train;machine translation;linguistics sparse models;low resource language;translation low resourcewe;memorization low resource;performance translation quality;low resource languages;high resource corpora;resource corpora languages;low resource deep;impact compression memorization;resource translation;translation parallelized;machine translation introduce;translation tasks;sentences data reduce"}, "12442420adf1c36887fafd108f4b7f4fc822ae60": {"ta_keywords": "neural sequence generation;sequence generation tasks;neural sequence;baseline neural sequence;sequence generation augments;empirically self training;sequence generation;self training empirically;training empirically self;method neural sequence;self training method;prediction pseudo parallel;supervised baseline neural;training method neural;training empirically;self training;generation tasks;version self training;prediction pseudo;improve supervised baseline;supervised baseline;novel self training;original labeled dataset;labeled dataset;model prediction pseudo;supervised;baseline neural;neural;labeled dataset unlabeled;training method", "pdf_keywords": "summarization self training;translation performance supervised;machine translation tasks;machine translation text;machine translation noisy;machine translation;like machine translation;text summarization self;summarization self;perform machine translation;training sequence generation;translation text summarization;sequence generation tasks;neural sequence generation;translation tasks;translation noisy self;method machine translation;experiments machine translation;sequence generation effective;self training deep;summarization demonstrate effectiveness;training neural sequence;self training sequence;self training neural;translation tasks method;sequence generation;sequence generation task;translation performance;context sequence generation;task sequence generation"}, "299ab255f3d940a20891128dfa9e0736d74a936c": {"ta_keywords": "fusion vision model;imitation learning models;imitation learning;fusion vision;vision models;manner imitation learning;early fusion vision;vision model;iterations set vision;set vision models;vision models condition;simulated item retrieval;vision model condition;trained simulated item;learning models;domain vision;models trained;learning models efficient;item retrieval;imitation;satisfied models trained;set vision;models trained simulated;vision;manner imitation;fusion;methods domain vision;end manner imitation;trained simulated;learning", "pdf_keywords": "robot described deep;simulated object retrieval;object retrieval;imitation learning;simple vision;trained imitation learning;simple vision model;goal robot;visual processing pipeline;context simple vision;approaches learns faster;object retrieval task;goal robot provide;vision model;constrained object image;algorithm goal robot;visual processing;object image;goal robot based;robot;robot based early;model learns;trained imitation;commands robot;commands robot described;networks trained imitation;model learns faster;robot provide pattern;robot described;convolutional neural"}, "5c6ff5836639e87e8afeaad47e64d0e2234566e8": {"ta_keywords": "propose", "pdf_keywords": "evidence attention fake;fake news detection;word attention evidence;attention network;word attention layer;evidence attention given;attention network combines;fact check textual;attention evidence attention;head attention network;combines word attention;evidence attention;attention fake news;layers word attention;attention evidence;attention layers;attention layers word;evidence fact checking;news detection task;attention layer;attention layers extract;applying evidence attention;document attention layer;evidence attention applying;evidence aware fake;news detection;words evidence evidence;document attention layers;attention documents;news detection model"}, "963c62b7c4b44ff1fe6aa1f45fa8a7d62b3d5051": {"ta_keywords": "textual entailment trained;domain question answering;support textual queries;question answering qa;question answering;generic textual entailment;textual entailment;textual queries;textual queries open;entailment trained concept;support textual;entailment trained;answering qa questions;answering qa;entailment;characterizing support textual;generic textual;support answer open;qa questions novel;textual;answering;queries open domain;answer open domain;concept support answer;integrates generic textual;trained concept support;questions novel;queries open;qa questions;questions novel methodology", "pdf_keywords": "domain question answering;question answering;question answering combines;domain purpose answering;question answering qa;answer queries based;answer challenges;knowledge conceptnet;answering questions;textual entailment trained;questions answer queries;answering questions large;answer questions efficient;questions answer;purpose answering questions;answer challenges need;conceptnet;questions adapting;answer queries;predicting semantics natural;predicting semantics;answering;purpose answering;knowledge conceptnet framework;database deep knowledge;answering combines best;deep knowledge unstructured;model natural language;knowledge textual;answering combines"}, "e28b9bc26f5f7eb3b0532d823713400202372da2": {"ta_keywords": "critic game model;actor critic game;critic game;interaction actor critic;actor critic approach;novel reinforcement learning;novel reinforcement;known stackelberg game;stackelberg game;propose novel reinforcement;critic approach;reinforcement learning;stackelberg game provide;critic player general;reinforcement;reinforcement learning paradigm;critic player;model interaction actor;standard actor critic;actor critic player;critic;critic approach significantly;general sum game;learning paradigm actor;game model interaction;actor critic;paradigm actor critic;openai gym environments;openai gym;game model", "pdf_keywords": "actor critic algorithms;optimization actor critic;actor critic algorithm;gradients actor critic;critic actor algorithm;actor critic learning;actor critic reinforcement;actor critic functions;policy gradients actor;actor critic framework;function actor critic;critic algorithms;optimizing actor critic;optimization actor;critic reinforcement learning;critic algorithms provide;gradients actor;stackelberg gradient strategy;critic algorithm;critic algorithm provide;critic underlying gradient;gradient demonstrate actor;gradient dynamics actor;stackelberg framework actor;learning problem actor;critic reinforcement;actor critic parameters;critic learning problem;dynamics actor critic;critic algorithms introduce"}, "b63bd17d4bb28ba90cc6ff66b51ba5b0377467bf": {"ta_keywords": "models rnns speech;rnns speech recognition;language models rnns;error training recurrent;recurrent neural network;rnns speech;lstm rnns;training recurrent neural;memory lstm rnns;models rnns;word error training;lstm rnns present;training recurrent;term memory lstm;neural network language;short term memory;speech recognition;rnns advanced model;memory lstm;rnns;rnns present efficient;lstm;elman type rnns;speech recognition apply;recurrent neural;language models;error training best;rnns advanced;error training;type rnns", "pdf_keywords": ""}, "93d3e45395117e21214d404c8753b578c29266d1": {"ta_keywords": "question answering tables;answering tables text;answering tables;open question answering;relevant tabular textual;challenge aggregating evidence;aggregating evidence new;highly relevant tabular;question answering;tabular textual;aggregating evidence;approach aggregating evidence;dataset tabular textual;tabular textual open;relevant tabular;evidence new data;tabular textual units;textual open;tables text;textual open question;answering;approaches capturing evidence;tabular;tables;tables text evaluate;novel approach aggregating;textual;capturing evidence;capturing evidence propose;textual units", "pdf_keywords": "evidence retrieval challenging;making evidence retrieval;evidence retrieval;open question answering;unstructured text evidence;retrieval using hybridqa;open quality retrieval;propose dense retrieval;retrieval challenging propose;novel approach retrieving;retrieval challenging;dense retrieval method;inference data unstructured;dense retriever queries;retrieving aggregating evidence;question answering qa;dense retrieval;question answering;data unstructured text;retrieval;introduce model retrieval;retrieval model;retrieval using;unstructured data;retrieval model single;unstructured text;retriever retrieval;data unstructured;knowledge rich content;retrieval method"}, "a0035379f93e0e95bdadd77a1d8eb27ba89dcf60": {"ta_keywords": "open collaborative storytelling;collaborative storytelling;collaborative storytelling use;collaborative storytelling approach;storytelling use platform;narrative context open;natural language annotations;storytelling use;access natural language;open collaborative;storytelling approach combines;open collaborative ecosystem;long form narrative;storytelling approach;models user experiments;natural language;dynamics open collaborative;dataset open collaborative;storytelling;evaluation models publicly;annotations;narrative;context open collaborative;annotations ability;annotations ability evaluate;language annotations ability;language annotations;narrative context;form narrative;user experiments", "pdf_keywords": "annotations story generation;stories annotated structural;story generator;story generation platform;long stories annotated;story text train;source story generator;annotations interspersed narrative;narrative integrate annotations;generated stories;evaluation generated stories;story generation fully;loop story generation;stories annotated;narrative analytics build;story generation;narrative analytics;story text;text narrative documents;story entry sequence;dataset narrative stories;generated stories goal;field narrative analytics;text narrative;structure text narrative;story generation use;quality story generation;annotations story;story database;library story descriptions"}, "3e4d80e43346b9538504c0a7ee5562f3c6a09178": {"ta_keywords": "class bandit algorithms;bandit algorithms;new class bandit;class bandit;bandit;algorithms;class;new class;propose new class;new;propose new;propose", "pdf_keywords": ""}, "740ecaa7fc1f4fc02116181b1757f03c815c7ea9": {"ta_keywords": "method predicting performance;data mining method;novel method predicting;method predicting;methods predicting performance;predicting performance given;predicting performance;problem predicting performance;data mining;methods predicting;existing methods predicting;trained data predict;mining method based;data predict outcome;techniques data mining;predict outcome problem;neural network;applying problem predicting;mining method;predict outcome;data predict;predicting;performance given clinical;concept neural network;problem predicting;neural network trained;trained data;clinical problem using;based concept neural;given clinical problem", "pdf_keywords": "short term memory;recurrent neural network;architecture present recurrent;recurrent neural;clinical time series;neural network supervised;neural networks;series model trained;supervised phenotyping clinical;neural network;novel feature encoding;predict shape diagnoses;network supervised phenotyping;supervised phenotyping;neural networks ls;neural;architecture linear neural;linear neural networks;length time series;memory lss neural;phenotyping clinical time;feature encoding architecture;able predict shape;present recurrent neural;predicting shape bacterial;linear neural;model predicting;term memory;model predicting shape;model trained"}, "191ef1408406569f0e9a69344add1ae350365431": {"ta_keywords": "characterizing fairness;characterizing fairness properties;framework characterizing fairness;fairness properties;fairness properties set;recidivism prediction task;framework recidivism prediction;recidivism prediction;fairness;credit scoring task;credit scoring;set good models;world credit scoring;prediction task real;framework recidivism;real world credit;illustrate framework recidivism;prediction task;models high stakes;scoring task;recidivism;set models deliver;prediction;models high;world credit;models;models deliver;credit;set models;models deliver similar", "pdf_keywords": "characterizing predictive fairness;predictive fairness;definitions predictive fairness;predictive fairness apply;predictive fairness properties;datasets demonstrate fairness;regression present fairness;models estimate fairness;fairness novel empirical;present fairness framework;estimate fairness;fairness framework;fairness framework fair;fair machine learning;algorithmic fairness novel;algorithmic framework fairness;measure predictive disparities;algorithmic fairness;model fairness;predictive fairs propose;disparities predicted outcomes;demonstrate fairness;demonstrate fairness context;predicted disparities proposed;disparities predictive regression;fairness properties;predictive disparities minimizing;predictive fairs;fair regression;investigate fairness promoting"}, "9eeaeadc1e0e300337b47d867a314caeae5c10a9": {"ta_keywords": "magnetic phase brain;brain structural relaxation;phase brain structural;emergence magnetic phase;structural relaxation model;phase brain;structural relaxation related;brain structural;brain structural components;individual brain structural;magnetic phase;brain components model;magnetic moments individual;observation magnetic moments;model emergence magnetic;emergence magnetic;structural relaxation;magnetic moments;individual brain components;relaxation model based;brain components;relaxation model;relaxation model applied;degree structural relaxation;observation magnetic;moments individual brain;based observation magnetic;relaxation related;magnetic;individual brain", "pdf_keywords": ""}, "9813446d9545b600de9a4972c1382c5e3b22a351": {"ta_keywords": "intelligent student tutoring;use student modeling;tutoring use student;student modeling;student tutoring use;modeling student learning;use student model;student modeling student;student tutoring;student tutoring useful;student model;model student learning;modeling student;student model student;tutoring use;use student;student tutoring evaluate;student learning help;help intelligent student;tutoring;tutoring useful;tutoring useful predicting;students given class;student learning;predicting performance students;model student;students;intelligent student;student;evaluate use student", "pdf_keywords": ""}, "78dadbfb6710ac65f178b5e12bd975184aae62fe": {"ta_keywords": "harmonic oscillator motion;particle harmonic oscillator;particle oscillator motion;oscillator motion particle;motion particle harmonic;oscillator described lorentz;described lorentz force;lorentz force motion;harmonic oscillator;oscillator motion;harmonic oscillator described;lorentz force;particle oscillator;force motion particle;particle harmonic;effect lorentz force;lorentz force force;motion particle;couples particle oscillator;oscillator described;effect lorentz;oscillator;described lorentz;force motion;force couples particle;particle;lorentz;harmonic;study effect lorentz;motion", "pdf_keywords": ""}, "241e890c70f6d013de7fe5e174e061ff824dc5e9": {"ta_keywords": "student perform task;problem learning environment;learning environment;learning environment provided;students learn;teaching simple problem;study student performance;developed learning environment;student performance;simple tasks finding;students;study student;task simple way;perform task simple;list student perform;task simple;simple tasks;simple simple tasks;students carry simple;tasks finding;learning environment students;simple problem learning;particular item student;students learn teaching;tasks finding time;student;results study student;problem learning;student perform;teaching simple", "pdf_keywords": ""}, "ca00ead4e5ddd14cbbbce03d89a57d14b430e320": {"ta_keywords": "smart grid segmentation;segmentation customers smart;customers smart grid;approach segmentation customers;segmentation customers;applied segmentation customers;smart grid smart;smart grid approach;smart grid;grid segmentation process;privacy customers proposed;privacy customers;smart grid applications;grid smart;grid segmentation;privacy valuation process;variety smart grid;use privacy valuation;grid smart devices;smart grid devices;privacy valuation;generation smart grid;breach privacy customers;segmentation process;segmentation process applied;customers smart;construction smart grid;segmentation;approach segmentation;applied segmentation", "pdf_keywords": "privacy metric energy;sensor privacy;privacy data disaggregation;monitor privacy consumers;propose metric privacy;privacy consumers smart;metric privacy;privacy protection smart;protection smart grid;monitor privacy;adversary implementing privacy;privacy metric;novel privacy metric;metric privacy data;measures privacy consumer;sensor privacy breach;measures privacy;implementing privacy;privacy consumer data;consumers smart grid;way monitor privacy;probability successful privacy;consumer choose privacy;present novel privacy;analyze privacy;privacy consumer;privacy endure attack;privacy consumers;consumer offered privacy;analyze privacy high"}, "83c7335904002d2b7c7cb403f3538703c9a69025": {"ta_keywords": "talk given speaker;talk method;listener introducing talk;additional talk method;talk method based;silent speech talk;improving quality talk;talk communicated listener;speaker quiet environment;talk called silent;given speaker quiet;speech talk communicated;quality talk given;introducing talk;speech talk;quality talk;silent speech;speaker quiet;introducing talk called;called silent speech;signal additional talk;speaker able convey;communicated listener introducing;talk communicated;signal form talk;communicated listener;speech;talk given;speaker;hypothesis speaker able", "pdf_keywords": ""}, "9f1a1d2cb6b278b7ee24e67d4c2ac38c1161fa1d": {"ta_keywords": "determine acoustic background;determination acoustic background;determination acoustic;determine acoustic;method determine acoustic;acoustic background form;sound acoustic background;acoustic background;problem determination acoustic;background form acoustic;vowels acoustic acoustic;beings acoustic background;acoustic sound;vowels acoustic;acoustic acoustic sound;acoustic sound acoustic;human beings acoustic;sound acoustic;acoustic acoustic;form vowels acoustic;acoustic acoustic acoustic;form acoustic sound;acoustic sound important;beings acoustic;form acoustic;acoustic sound propose;acoustic;background form vowels;sound propose method;vowels", "pdf_keywords": ""}, "5bf7f468b763f181c31a5e1edc57bce9a6dbd00c": {"ta_keywords": "pde score validate;pde score;differential pde score;pde score method;pde score constructed;consistent pde score;acute respiratory distress;respiratory distress syndrome;respiratory distress;risk critically ill;evaluate risk critically;critically ill patients;patients acute respiratory;acute respiratory;distress syndrome elas;distress syndrome;critically ill;risk critically;method evaluate risk;novel empirical approach;score method based;empirical approach based;evaluate risk;self consistent empirical;ill patients acute;score method;using novel empirical;empirical representation underlying;patients acute;empirical approach", "pdf_keywords": "pneumonia death rnv;viral pneumonia death;viral pneumonia;coronavirus infected pneumonia;score critically ill;cohort critically ill;pneumonia ed eligible;predicting severe acute;critically ill patients;patients critically ill;pneumonia death;ill patients coronavirus;severe acute respiratory;sarscov viral pneumonia;acute respiratory distress;ill patients rnv;risk score critically;bacterial pneumonia ed;infected pneumonia;coronavirus disease;patients critically;patients acute respiratory;pneumonia ed;acute respiratory syndrome;patients coronavirus;critically ill;patients coronavirus infected;pneumonia;coronavirus infected;sample critical care"}, "ea2b138583e587850153f2825fe9e4339aa5f5f9": {"ta_keywords": "xmath4 neurons network;parallelization xmath3 neurons;xmath2 neurons simple;xmath1 xmath2 neurons;xmath2 neurons;xmath3 neurons;xmath4 neurons;xmath3 neurons observed;observed xmath4 neurons;xmath5 neurons;observed xmath5 neurons;neurons observed xmath4;coupled neurons;neurons network coupled;coupled neurons model;neurons observed xmath5;network coupled neurons;coupled neurons observed;dynamics xmath0 xmath1;dynamics xmath0;neurons model;study dynamics xmath0;neurons network;parallelization xmath3;neurons simple model;neurons model compared;neurons model based;based parallelization xmath3;xmath0 xmath1 xmath2;xmath1 xmath2", "pdf_keywords": ""}, "ca1645abedae3b4caa3345aa8720c8b90f7c37db": {"ta_keywords": "voting rules distribution;robust existing voting;rank dependent scoring;existing voting rules;rules distribution votes;voting rules;scoring rules robust;voting rules restricted;distribution votes;distribution votes presence;dependent scoring;dependent scoring restricted;existing voting;votes presence;new scoring rules;votes presence static;family voting rules;rank dependent;scoring rules;votes;variables new scoring;voting;family voting;static random variables;rules robust;concept rank dependent;new family voting;scoring restricted;based concept rank;rank", "pdf_keywords": ""}, "b2c3d660aaefb80085fe72c80ce81c5fa71980e9": {"ta_keywords": "pivot translation approaches;subtrees pivot language;pivot language words;pivot translation method;syntactic subtrees pivot;using pivot translation;pivot translation;distinguishing pivot language;superior pivot translation;new pivot translation;pivot language;pivot language avoid;subtrees pivot;utilizes syntactic subtrees;translation approaches;method parallel corpus;translation method parallel;syntactic subtrees;translation method;translation method utilizes;parallel corpus;language words syntactic;parallel corpus united;words syntactic;distinguishing pivot;pivot;using pivot;method superior pivot;combinations distinguishing pivot;superior pivot", "pdf_keywords": ""}, "b7d6829d9eccdbd3d3a5d6f5321a87158588033b": {"ta_keywords": "characterizing pairs videos;videos continuous traits;identify pairs videos;pairs videos continuous;pairs videos;pairs videos time;videos continuous;videos time order;large scale datasets;videos;videos time;continuous traits;ability identify pairs;datasets;characterizing pairs;scale datasets;identify pairs;method characterizing pairs;datasets shown affordable;datasets shown;scale datasets shown;continuous traits method;preference method scalable;scalable large scale;pairs;traits;scalable large;preference method;order preference;order preference method", "pdf_keywords": ""}, "47adb249ce8f7f5f1e92112ba0f3757f8fbfbfc3": {"ta_keywords": "factoid answerranking word;factoid answerranking;non factoid answerranking;answerranking word syntactic;factoid semantic models;answerranking word;non factoid semantic;answerranking;question answer texts;factoid semantic;answer texts;semantic models chain;answer texts casting;semantic models;construct indirect associations;word syntactic representations;semantic;direct term associations;term associations experimentally;applicable non factoid;allows non factoid;non factoid;indirect associations;term associations;syntactic representations;word syntactic;factoid;task traversal graphs;indirect associations question;syntactic", "pdf_keywords": ""}, "21ac57d41843ac5367e11b8b784aa57f2ef7a1fc": {"ta_keywords": "stochastic convex optimization;smooth stochastic convex;stochastic methods gradient;stochastic convex;stochastic methods;high probability convergence;convex optimization;non smooth stochastic;smooth stochastic;strongly convex problems;convex optimization problems;generalized smooth objectives;strongly convex;rules stochastic methods;continuous gradients methods;non sub gaussian;extension strongly convex;hlder continuous gradients;noise distribution propose;smooth objectives;methods gradient clipping;probability convergence;sub gaussian heavy;sub gaussian;smooth objectives hlder;gradient clipping methods;stochastic;methods gradient;derive high probability;convex problems analysis", "pdf_keywords": "clipped stochastic gradients;stochastic gradient descent;stochastic optimization estimate;convex optimization stochastic;stochastic optimization large;stochastic gradients clipped;stochastic gradient clipped;optimization stochastic approximation;method stochastic optimization;convex optimization complexity;stochastic gradients propose;stochastic optimization;stochastic optimization problems;new stochastic gradient;bounded stochastic gradients;stochastic gradient method;tails stochastic gradients;optimization stochastic;estimate optimal stochastic;approximate optimal stochastic;stochastic gradients;bounds stochastic approximation;solutions stochastic optimization;stochastic gradients stochastic;asymptotically optimal learning;stochastic gradient;gradient learning sg;sgd objective function;stochastic approximation algorithms;stochastic optimization problem"}, "6cdff2505560390b28db5a96c2ae3070712077cf": {"ta_keywords": "gradient based bandits;reinforcement learning gradient;gradient reinforcement learning;competitive gradient based;policy gradient reinforcement;competitive gradient;agents employing gradient;agents employ gradient;gradient reinforcement;policy gradient;online convex optimization;framework competitive gradient;including policy gradient;algorithms potential games;learning gradient;algorithms including policy;reinforcement learning;learning gradient based;learning algorithms avoid;behavior competitive agents;limiting behavior competitive;gradient based learning;learning algorithms including;bandits certain online;based bandits;competitive agents;online convex;based bandits certain;convex optimization algorithms;learning algorithms", "pdf_keywords": "gradient algorithms games;games dynamics learning;policy gradients games;gradients games general;gradient scheme games;gradients games;learning algorithms games;agent policy gradients;game gradient flows;underlying game gradient;games described gradient;games based gradient;agents costs gradient;gradient continuous game;gradients agents costs;generalized learning games;individual gradients agents;behavior competitive gradient;game gradient;competitive stochastic gradient;gradients agents;underlying game learning;game learning algorithm;learning algorithm dynamics;games joint gradient;competitive gradient;learning games prove;dynamics gradient play;dynamics games theorems;gradient play games"}, "7301c7aba3c0824b91f69747e7e50f4db56d7fc1": {"ta_keywords": "paralinguistic translation based;method paralinguistic translation;paralinguistic translation;translation based neural;trained digit translation;method paralinguistic;propose method paralinguistic;paralinguistic;digit translation task;translation based;digit translation;translation task;translation task shown;neural network model;neural network;single neural network;method trained digit;neural network based;single neural;translation;results single neural;based neural;trained digit;based neural network;neural;method trained;model method trained;digit;model;network based model", "pdf_keywords": ""}, "8837530b23a2d51054d8752ae2f0ffef8998da8e": {"ta_keywords": "domain location recommendation;location recommendation based;criterion differential privacy;collective matrix factorization;differential privacy based;differential privacy;user location interaction;matrix factorization;locations user meet;adopt differential privacy;protected user location;recommendation based assumption;location recommendation;locations user;differential privacy share;privacy;matrix factorization cvf;privacy based protection;aware collective matrix;confidence aware collective;real locations user;privacy share;factorization;recommendation based;privacy based;cross domain location;privacy share protected;hide real locations;user location;share protected user", "pdf_keywords": ""}, "cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022": {"ta_keywords": "graph transforming encoder;encoder generate novel;transforming encoder generate;transforming encoder generation;encoder generate;encoder generation;encoder;encoder generation coherent;transforming encoder;novel graph transforming;state art encoder;utilize graph transforming;coder texts;multi coder texts;coder texts output;art encoder;encoder methods;output information extraction;extraction utilize graph;art encoder methods;graph transforming;texts output information;novel graph;document sequences informative;coherent multi coder;quality document sequences;information extraction;document sequences;information extraction utilize;present novel graph", "pdf_keywords": "text knowledgegraphs graph;graph natural language;knowledgegraphs graph structured;texts knowledge graphs;novel text knowledgegraphs;knowledge graph natural;knowledge graphs novel;text knowledgegraphs;produces knowledge graphs;knowledgegraphs graph;encoder novel graph;graph encoder generating;knowledge graph;novel graph encoder;graph encoding relationships;knowledge graphs;graphs representing knowledge;encoding relations graph;encoding relationships graph;knowledgegraphs;encoder generating knowledge;graph encoder;graph structure encoder;graph use encoder;title knowledge graph;graphs novel representations;relations graph decode;graph decode encoder;introduce novel graph;graph transforming encoder"}, "a54019645dd8e9cfd8d71ab60155449307de3d83": {"ta_keywords": "mechanism crowdsourcing low;mechanism crowdsourcing;compatible mechanism crowdsourcing;crowdsourcing low quality;crowdsourcing low;crowdsourcing;propose simple incentive;data mechanism incentivizes;simple incentive compatible;incentive compatible mechanism;mechanism incentive;incentive compatible mechanisms;simple incentive;requirement mechanism incentive;possible incentive compatible;mechanism incentive compatible;incentive;incentive compatible payment;quality data mechanism;incentive compatible;possible possible incentive;data mechanism;possible incentive;low quality data;incentivizes workers answer;workers answer questions;quality data;payment mechanism possible;mechanism incentivizes workers;payment mechanism", "pdf_keywords": "reward mechanism crowdsourcing;mechanism crowdsourcing guarantees;crowdsourcing guarantees;crowdsourcing tasks;mechanism crowdsourcing;crowdsourcing tasks involving;tasks crowdsourcing task;tasks crowdsourcing;crowdsourcing task representation;crowdsourcing task;efficiently crowdsource labeled;method crowdsourcing tasks;crowdsourcing task answers;crowdsourcing tasks based;crowdsourcing tasks method;mechanism crowdsourcing based;crowdsource labeled data;crowdsourcing setups workers;crowdsourcing results;workflow crowdsourcing tasks;problem crowdsourcing tasks;crowdsourcing builds simple;efficiently crowdsource;crowdsource labeled;symmetric inequalities crowdsourcing;optimal workflow crowdsourcing;incentive based theorems;crowdsourcing;scale crowdsourcing tasks;crowdsourcing predicting behavior"}, "2cc7db7b17ee7349800334b3a154f708850c6410": {"ta_keywords": "code distributed distributed;code distributed;job latency centralized;decentralized versions queues;latency centralized;distributed distributed;latency centralized decentralized;queueing;code code distributed;distributed distributed version;queues;queues corroborate analysis;distributed version code;data accessed distributed;distributed;queueing arising;queues corroborate;queueing arising condition;distributed version;accessed distributed;distributed version maximum;job latency;accessed distributed version;average job latency;versions queues;centralized decentralized versions;study queueing arising;study queueing;centralized decentralized;separable mds code", "pdf_keywords": ""}, "0ff5b1e61bbebd2f077a4ef24c3afdb344e5b3d4": {"ta_keywords": "lattice gas harmonic;gas harmonic trap;harmonic trap undergoes;harmonic trap;dimensional lattice gas;lattice gas;dynamics dimensional lattice;transition homogeneous gas;gas harmonic;gas characterized finite;homogeneous gas characterized;trap undergoes order;homogeneous gas;gas homogeneous gas;gas characterized;gas homogeneous;homogeneous gas homogeneous;parameters dynamics;homogeneous gas determined;trap undergoes;trap;dimensional lattice;parameters transition homogeneous;parameters dynamics dimensional;dynamics dimensional;dynamics;state transition homogeneous;lattice;transition homogeneous;harmonic", "pdf_keywords": ""}, "8e992116bbc8afb075577a30672de7a90fbeba78": {"ta_keywords": "stream neurons algorithm;streaming algorithm predicting;stream neurons;given stream neurons;streaming algorithm;novel streaming algorithm;neurons algorithm;neurons algorithm based;feature given stream;approximate batch classifier;encoders realized parallel;beam search encoders;synchronous beam search;predicting arrival feature;batch classifier;streaming approaches;batch classifier performance;algorithm predicting arrival;search encoders;beam search;novel streaming;neurons;search encoders realized;classifier performance algorithm;propose novel streaming;encoders;encoders realized;predicting arrival;algorithm predicting;classifier", "pdf_keywords": ""}, "3163392f56cdffaa009fbc59f299989a1b8baec1": {"ta_keywords": "binary classification unlabeled;classification unlabeled;classification unlabeled data;binary classification;class binary classification;binary classification various;algorithms binary classification;unlabeled data improve;unlabeled oc algorithms;unlabeled data;positive unlabeled;unlabeled;unlabeled oc;unlabeled data available;positive unlabeled oc;unlabeled pu algorithms;classification;leverages unlabeled data;class binary;art positive unlabeled;classification various;algorithms binary;classification various scenarios;binary;unlabeled pu;performance class binary;positive unlabeled pu;oc algorithms robust;pu algorithms binary;approach leverages unlabeled", "pdf_keywords": "learning unreliable unlabeled;positive unlabeled learning;unlabeled learning based;unlabeled learning;learns labeled positive;classification learns labeled;reliable unlabeled;positive unlabeled data;data positive unlabeled;classification learns;unlabeled learning constructing;unlabeled data propose;approaches unlabeled data;unlabeled pu learning;learns labeled;labeled positive data;unreliable unlabeled data;safe predictive robust;algorithms leverage unlabeled;approach positive unlabeled;learning shown robust;robust safe predictive;oc classification learns;unlabeled data convex;unlabeled data best;algorithms unreliable unlabeled;unlabeled data improve;approaches unlabeled;existing positive unlabeled;positive unlabeled"}, "73472692b6090a72e36e03127bb99fc2e6bc8de0": {"ta_keywords": "identify communities empirical;data identify communities;communities empirical data;communities empirical;methodology identify communities;identify communities best;identify communities;communities;communities best fit;communities best;network modularity maximization;network modularity;community assignments methodology;community;cultural cognitive mapping;methodology community assignments;modularity maximization;empirical data identify;modularity maximization nn;community assignments;socio cultural cognitive;unsupervised machine learning;nn network modularity;empirical data;cultural cognitive;socio cultural;cognitive mapping;cognitive mapping scm;empirical data methodology;proposed methodology community", "pdf_keywords": ""}, "2f6843f9345ca56af3fd9df5512daa1e7f80bedf": {"ta_keywords": "accurate prediction structure;prediction structure using;prediction structure;random field classifier;predict shape structure;structure accurately outperforming;field classifier;structure accurately;shape structure accurately;field classifier trained;predict shape;random field baseline;able predict shape;used predict shape;structure using data;conditional random field;random field;method accurate prediction;shape structure;accurate prediction;classifier;structure using;trained data generated;classifier trained;outperforming conditional random;classifier trained data;trained data;structure;prediction;accurately outperforming conditional", "pdf_keywords": ""}, "faa4468f2ad1c7cedaf04bf56ebb20ae4b349952": {"ta_keywords": "short term memory;memory recurrent neural;term memory recurrent;recurrent neural network;memory recurrent;matrix adaptation evolution;recurrent neural;language models sdms;matrix adaptation;term memory;covariance matrix adaptation;language models;neural network language;global optimization long;network language models;recurrent;tuning meta parameters;meta parameters sdms;optimization long;tuning meta;optimization long short;global optimization;adaptation evolution es;method tuning meta;models sdms;neural;adaptation evolution;models sdms method;adaptation;method global optimization", "pdf_keywords": ""}, "872c2d9d8b27ff49367854a7cf67b5dff2010406": {"ta_keywords": "event detection;event detection process;event domain independent;unsupervised learning framework;unsupervised learning;use unsupervised learning;precision event detection;predicting presence event;predict presence event;event domain;method predicting;novel method predicting;based use unsupervised;event;detection process;predicting;use unsupervised;detection;detection process method;method predicting presence;estimate precision event;domain independent way;domain independent;unsupervised;learning framework proposed;presence event domain;predict;applied problem predicting;able predict;framework able predict", "pdf_keywords": ""}, "c6713071291729386955586c6309778b1637b852": {"ta_keywords": "stochastic pricing designs;constructing stochastic pricing;pricing designs adaptive;stochastic pricing;energy management buildings;adaptive energy management;designs adaptive energy;pricing designs;stochastic zone building;constructing stochastic;management buildings method;adaptive energy;energy management;quadratic game agents;method constructing stochastic;applied stochastic;buildings method based;buildings method;linear quadratic game;method applied stochastic;management buildings;stochastic;game applied linear;building model;quadratic game;designs adaptive;applied stochastic zone;stochastic zone;based simplification game;buildings", "pdf_keywords": ""}, "d5084f48212bed80e8c11e1e69669deea3ba2f83": {"ta_keywords": "parallel model pasta;pasta dish model;model pasta dish;model pasta;ancestor pasta dish;pasta dish problem;dish problem pasta;pasta dish;novel parallel model;parallel tasks;parallel tasks easier;problem pasta dish;parallel model;dish model inspired;class parallel tasks;parallel;ancestor pasta;novel parallel;common ancestor pasta;problem pasta;dish model;dish;present novel parallel;pasta;dish problem goal;simplicity problem based;class parallel;dish problem;large class parallel;simplicity problem", "pdf_keywords": ""}, "8451d8e20bb9a94c6a576e52ca1a63470f8d2390": {"ta_keywords": "method distributed optimization;distributed optimization based;distributed optimization;optimization based la;free method distributed;derivative free method;method distributed;optimization;optimization based;derivative free;new derivative free;distributed;free method;propose new derivative;new derivative;derivative;la;based la;free;method;propose new;propose;new;based", "pdf_keywords": ""}, "a6a9c06d138537002aaca79dba359cc320b951df": {"ta_keywords": "probability density closed;distribution number states;density closed method;calculation probability density;calculating probability density;density closed;probability density;states method based;states method;states method applied;method calculating probability;applied calculation probability;closed method based;calculation probability;number states method;calculating probability;closed method applied;distribution;closed method;density;analysis distribution number;distribution number;analysis distribution;probability;number states;based analysis distribution;states;method applied calculation;closed;applied calculation", "pdf_keywords": ""}, "03bbbaa03cb57413c2581cc8dc5cbfa532bbea15": {"ta_keywords": "potential erp using;erp using empirical;potential erp;related potential erp;erp using;event related potential;time integrated event;integrated event related;event related time;integrated event;identify user event;empirical validation empirical;user event related;empirical data time;empirical validation;validation empirical;event related;erp;data time integrated;using empirical validation;validation empirical data;user event;time integrated;event;empirical data;related time integrated;empirical;using empirical;data time;approach identify user", "pdf_keywords": ""}, "73e1dcf5f0f3cf4e645b0bba62d9b1e2ef47b706": {"ta_keywords": "lexical class identification;free grammars parsing;lexical class;implementation lexical class;grammars parsing;grammars parsing methods;class identification based;context free grammars;implementation lexical;parsing;parsing methods;class identification;free grammars;identification based context;lexical;parsing methods main;describes implementation lexical;grammars;identification based;based context free;context free;class;algorithm described;context;implementation;based context;algorithm;main algorithm described;identification;paper describes implementation", "pdf_keywords": ""}, "122b75042daae44f93153dedda15b0fb11b3f279": {"ta_keywords": "question answering datasets;sparse question answering;bert based models;question answering;answering datasets models;answering datasets;task question answering;evaluate bert based;evaluate bert;bert based;bert;answering;datasets models learning;models learning datasets;datasets models;learning datasets single;datasets better;learning datasets;future datasets better;dataset robust experiments;future datasets;datasets better evaluate;models learning;building future datasets;dataset robust;datasets;sparse;dataset;datasets single;single dataset robust", "pdf_keywords": "questions answering models;learning models answering;question answering;answering models;models answering questions;learn questions answering;answering models reached;questions answering;task question answering;models learn questions;models answering;question answering paper;bert based models;popular question answering;models answer questions;question answering qa;answering qa datasets;qa datasets bert;questions contexts answers;learn robust answers;answering questions;machine reading comprehension;answering questions based;powerful approach answering;reading comprehension systems;answer questions robust;questions contexts;datasets bert;learn questions;approach answering questions"}, "e6602786132e040e02df93f729f737f65a116677": {"ta_keywords": "digital home assistants;home assistants commands;speech interaction digital;speech recognition asr;key speech processing;automatic speech recognition;device key speech;home assistants described;automatic speech;speech recognition;home assistants;interaction digital home;assistants commands spoken;speech processing;speech interaction;commands spoken distance;speech processing algorithms;recognition asr;spoken distance;assistants commands;spoken distance sound;key speech;sound capturing;cases automatic speech;sound capturing device;free speech interaction;distance sound capturing;interaction digital;digital home;assistants described", "pdf_keywords": ""}, "d8682a269523a868f2bc9714b00f0519aa0e931f": {"ta_keywords": "retrieval information sources;text integration based;information retrieval;text integration;information retrieval information;retrieval information;fragments text integration;information sources converted;retrieval;information sources;highly structured collection;process information retrieval;collection small fragments;structured collection;fragments text;structured collection small;integration based novel;small fragments text;integration based;integration process;new integration process;integration;converted highly structured;fragments;integration process information;sources converted;text;based novel logic;new integration;sources converted highly", "pdf_keywords": ""}, "8cb74fe4f598699c9c24d88acd4906e2489267af": {"ta_keywords": "interacting particles;interacting particles method;motion single particle;number interacting particles;particles method based;particles method;particles;single particle;particle;particle method based;single particle method;particles method applied;particle method;dimensional motion single;dimensional motion;mapping global dimensional;dimensional dimensional motion;map space large;map space;method demonstrated mapping;adaptive sensor;based mapping global;mapping global;large number interacting;demonstrated mapping large;adaptive sensor method;interacting;method based mapping;demonstrated mapping;mapping large", "pdf_keywords": ""}, "48aced0919e29722d6eed9544353d5507c541cfc": {"ta_keywords": "protein names anaphoric;names anaphoric linking;anaphoric linking protein;linking protein names;protein names based;thephoric protein names;protein names;anaphoric techniques domain;anaphoric linking;names anaphoric;approach thephoric protein;thephoric protein;linking protein;domain specific data;specific data domain;data domain specific;anaphoric techniques;anaphoric;aphoric anaphoric techniques;combination aphoric anaphoric;data domain;protein;domain specific domain;aphoric anaphoric;domain specific;specific domain specific;use domain specific;specific domain;names based use;domain", "pdf_keywords": ""}, "b3d9a0308ba6c4ca583a2b4e5be2b3eed466ccbc": {"ta_keywords": "relay wave d2d;wave d2d communication;relay wave;d2d communication proposed;decide use relay;partially observable decision;relay relay wave;observable decision process;d2d communication;relay;observable decision;use relay;relay relay;decision process pmdp;dynamic obstacles proposed;use relay relay;wave d2d;dynamic obstacles;stochastic;communication proposed;based partially observable;partially observable;obstacles proposed framework;stochastic aspect dynamic;communication proposed framework;obstacles proposed;stochastic aspect;framework incorporates stochastic;incorporates stochastic;pmdp framework", "pdf_keywords": "packet delay relaying;relay optimal;relay selection 5g;relay link optimal;approach relay communications;relay optimal policy;relay communications based;relay communications;relay selection;relay link approach;delay relaying network;relay optimal given;delay relaying;communication policy relay;relay selection presence;choosing relay optimal;relay locally decide;relay finally optimal;algorithm selecting relay;relay exploring possible;relay exploring;source relay optimal;relay choice;relaying network;problem relay selection;best possible relay;relay choice time;relay link communication;novel approach relay;study relay selection"}, "fc8e226c20800c8ccc095bb6a3c0f8dcb637b683": {"ta_keywords": "lung metastases multidisciplinary;managing lung metastases;metastases multidisciplinary treatment;lung cancer metastases;metachronous lung metastases;lung metastases;multidisciplinary therapy lung;therapy lung cancer;lung metastases initial;metastases multidisciplinary;lung cancer;cancer metastases;managing lung;guidelines managing lung;metachronous lung;metastases ep2;therapy lung;metastases ep2 present;form metastases ep2;initial metachronous lung;consensus multidisciplinary therapy;cancer metastases form;multidisciplinary therapy;multidisciplinary treatment;multidisciplinary treatment framework;metastases form metastases;metastases form;metastases initial metachronous;metastases;form metastases", "pdf_keywords": ""}, "6c0a3029afd65c83982b3fb96f623da382344286": {"ta_keywords": "topic models neural;topic models;context topic models;matrix tensor factorization;tensor factorization;tensor factorization theory;natural language processing;processing nlp;tutorial matrix tensor;processing nlp explain;language processing nlp;factorization theory optimization;factorization theory;language processing;tensor;matrix tensor;context natural language;nlp explain methods;natural language;nlp;representations population dynamics;factorization;nlp explain;models neural;topic;accurate representations population;models neural networks;obtain accurate representations;context topic;representations population", "pdf_keywords": ""}, "928f942baf03dd56aae662fa94d85d22b5600f83": {"ta_keywords": "new method;present new method;method;present new;new;present", "pdf_keywords": ""}, "e30b22e692b3c7d2653832bf2901abd8a9375b6e": {"ta_keywords": "content placement algorithm;optimal content placement;content placement nodes;popular content placement;content placement minimizes;content placement;independent content placement;optimal content;content placement independent;content popularity;converging optimal content;develop content placement;placement nodes;placement nodes knowledge;placement independent content;problem optimal content;minimizes rate download;placement minimizes rate;small cell networks;popular content;placement algorithm small;placement algorithm;maximizes cache;equivalently maximizes cache;knowledge content popularity;content popularity demonstrate;ideas gibbs sampling;nodes knowledge content;maximizes cache hit;compared popular content", "pdf_keywords": "placement caches cellular;caches cellular network;networks distributed content;caches cellular;decentralized network content;distributed content;placement heterogeneous networks;network content requests;distributed cache;placement strategy cellular;propose distributed cache;network content;cellular networks;cellular networks approach;estimation cellular networks;cellular network base;cellular networks existing;content placement network;cellular network;strategy cellular networks;cellular network based;stations propose distributed;distributed content delivery;caching strategy;networks distributed;content placement algorithm;content placement caches;popularities estimation cellular;practical cellular network;cellular networks based"}, "2406cf39805c70264c4226b7325a09b506c70921": {"ta_keywords": "learning neural sql;query synthetic corpus;neural sql;neural sql query;sql query synthetic;training table arbitrary;large scale sql;arbitrary tables based;tables based unsupervised;sql;arbitrary tables;sql queries;tables based;query synthetic;table arbitrary tables;tables;ofsql queries program;queries program;queries program written;pre training table;queries;queries high accuracy;ofsql queries;table arbitrary;training table;ofsql;table;sql queries high;synthetic corpus;sql query", "pdf_keywords": "trained sql;trained sql nl;sequence generation tasks;execution neural sql;tasks sequence generation;generated execution neural;train unstructured tables;model trained sql;application trained sql;synthetic database;synthetic database generated;query trained;generating nonlinear semantic;neural sql program;synthetic synthetic database;languages tables achieve;translation machine table;table based computational;semantics model trained;translation generator trained;formal languages tables;machine translation generator;unstructured table source;table pre training;languages tables;tablefv model tasks;execution neural;state art sql;synthetic corpus;pre training corpus"}, "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671": {"ta_keywords": "text generation learns;accurate text generation;text generation;learns demonstrations importance;learns demonstrations weight;approach text generation;text generation state;generation learns demonstrations;learns demonstrations;model learns demonstrations;demonstrations importance weighting;importance weighting demonstrations;demonstrations model generated;reference approach reinforcement;generation learns;weighting demonstrations model;reinforcement learning rl;demonstrations weight weighted;reinforcement learning;weighting demonstrations;approach text;approach reinforcement learning;perform accurate text;demonstrations weight;model learns;generated reference approach;demonstrations model;accurate text;confidence weighted;generated reference", "pdf_keywords": "supervised text generation;text generation tasks;generation context reinforcement;offline reinforcement learning;sentences based reinforcement;stochastic text generation;text generation offline;generation offline reinforcement;text generation;approach text generation;text generation context;text generation based;policy gradient learning;conventional text generation;quality text generation;identify text generation;learning generate;natural language generation;context reinforcement learning;consider text generation;web based reinforcement;generate text;standard text generation;context reinforcement;batch deep reinforcement;language generation training;reinforcement learning structured;policy generate;gradient learning generate;reinforcement learning rl"}, "fdaad09b1a897c0a04b9a9579081d542e2b4546c": {"ta_keywords": "identify wavevector ionization;identification wavevector ionization;electron wavevector identification;identify electron wavevector;electrons cell identification;technique identify electron;wavevector identification process;wavevector ionization time;wavevector ionization;electron wavevector;based identification wavevector;identification wavevector;wavevector identification;2d electron gas;used identify wavevector;dimensional 2d electron;identify wavevector;identify electron;electron gas cell;ionization time determined;2d electron;ionization mode known;cell identification process;identification dimensional 2d;cross identification dimensional;ionization time;electrons cell;determined number electrons;electron gas;ionization mode", "pdf_keywords": ""}, "74276a37bfa50f90dfae37f767b2b67784bd402a": {"ta_keywords": "mt5 multilingual text;mt5 multilingual;present mt5 multilingual;multilingual benchmarks;performance multilingual benchmarks;multilingual text;multilingual text text;performance multilingual;multilingual;crawl based dataset;covering 101 languages;text transfer;text transfer t5;common crawl based;art performance multilingual;text text transfer;101 languages;languages;crawl based;modified training mt5;new common crawl;101 languages design;languages design;common crawl;dataset covering 101;training mt5;languages design modified;transfer t5 model;t5 model pretrained;transfer t5", "pdf_keywords": "pretraining large multilingual;large multilingual generative;multilingual generative models;mt5 languages corpus;massively multilingual model;trained language models;multilingual generative;create massively multilingual;large multilingual;multilingual benchmarks;language mt5 training;present mt5 multilingual;massively multilingual;large languages efficiently;train large languages;multilingual model;mt5 multilingual;trained language model;machine translations;mt5 multilingual variant;multilingual benchmarks simple;machine translations language;language model generative;new language model;transfer language;model cross lingual;performance multilingual benchmarks;unified language model;tuning machine translations;low resource languages"}, "00b2afaf5935b4dea41f134fe11a21a1ed56fa0e": {"ta_keywords": "xmath1 symmetric xmath2;xmath0 symmetric xmath1;symmetric xmath2 xmath3;symmetric xmath1;xmath1 symmetric;xmath0 symmetric;behaviour xmath0 symmetric;symmetric xmath2;xmath2 xmath3 xmath4;symmetric xmath1 symmetric;xmath10 xmath11 xmath12;xmath9 xmath10 xmath11;xmath16 xmath17 xmath18;xmath12 xmath13 xmath14;xmath3 xmath4 xmath5;xmath4 xmath5 xmath6;xmath11 xmath12 xmath13;xmath8 xmath9 xmath10;xmath15 xmath16 xmath17;xmath3 xmath4;xmath17 xmath18 xmath19;xmath14 xmath15 xmath16;xmath2 xmath3;xmath13 xmath14 xmath15;xmath7 xmath8 xmath9;xmath6 xmath7 xmath8;xmath17 xmath18;xmath5 xmath6 xmath7;xmath16 xmath17;xmath11 xmath12", "pdf_keywords": ""}, "1bf36cb3453b51550ebadd904a840c75d59f171b": {"ta_keywords": "robust automatic speech;speech recognition asr;automatic speech recognition;speech recognition;recognition asr systems;developments robust arrndient;automatic speech;recognition asr;robust arrndient;robust arrndient systems;robust automatic;robust arndient;robust systems;robust systems chapter;robust arndient systems;robust;asr systems;deep learning;recent developments robust;field robust systems;field robust arndient;asr systems provides;advances field robust;field robust;recognition;arrndient systems;background robust automatic;arrndient systems including;developments robust;deep learning chapter", "pdf_keywords": ""}, "8b20173b98914f36302389e4c761c334fe867dcd": {"ta_keywords": "robust parsers;morphosyntactic formedness text;directly dependency treebanks;train robust parsers;dependency treebanks tackle;treebanks tackle noisy;dependency treebanks;dependency parse language;treebanks tackle;treebanks;parsers;evaluate morphosyntactic formedness;language evaluate morphosyntactic;parse language;dependency parse;evaluate morphosyntactic;morphosyntactic formedness;morphosyntactic;parse;method evaluate morphosyntactic;relies dependency parse;parse language evaluate;text generation systems;text generation;rules governing morphosyntax;formedness text method;morphosyntax directly dependency;morphosyntax;formedness text present;morphosyntax directly", "pdf_keywords": "corpus robust parser;parsers trained corpora;quality grammatical taggers;machine translation;translation morphologically rich;capture morphological grammatical;robust parsers trained;accuracy natural language;rule entire corpus;parse morphosyntactic rules;treebank capable extracting;standard machine translation;use morphological taggers;corpus resulting treebank;corpus;natural language generation;corpus robust;accuracy machine translation;robust parsers taggers;syntactic rule capturing;robust parsers;language generation systems;machine readable grammatical;based parsers trained;quality grammatical output;natural language processing;sub corpus robust;parsers trained;treebank capable;produced natural language"}, "3dcba175248d0e8d2da44e3731e4adbfb9f00e97": {"ta_keywords": "open information extraction;remine extracting relation;extraction relation tuples;extracting relation tuples;information extraction;extracting relation;extraction relation;information extraction called;novel open information;open information;relation tuples;remine extracting;tuples individual sentences;sentence global structural;relation tuples individual;distant supervision framework;information sentence global;extraction called remine;information sentence;context information sentence;extracting;called remine extracting;signals distant supervision;sentences;quality extraction relation;individual sentences;context information;corpora;relation;real world corpora", "pdf_keywords": "open information extraction;entity phrase extraction;information extraction open;extracted entity phrases;entity phrases extracted;extract entity phrase;phrase extraction;domain information extraction;phrase extractions;supervised entity phrase;extract phrase extractions;single sentence extraction;relation extraction tasks;phrase extractions collection;information extraction;challenges relation extraction;sentence extraction;treats phrase extraction;relation extraction;analyzing relation extraction;sentence level extractions;entity phrase segmentation;entity phrase pairs;phrase extraction task;information extraction proposes;extractions using corpus;extraction open domain;extraction based relation;entity phrases;information extraction based"}, "96dbffb71e4d62a985f826197845623b1415c267": {"ta_keywords": "yielding metacognitive learning;metacognitive learning;metacognitive learning acquiring;free grammar induction;algorithm yielding metacognitive;yielding metacognitive;future learning;grammar induction algorithm;grammar induction;metacognitive;learning;learning acquiring deep;assist future learning;understand human learning;human learning;context free grammar;future learning discuss;acquiring deep;free grammar;learning acquiring;learning discuss;model synthetic student;learning discuss challenges;acquiring deep features;probabilistic context free;synthetic student possible;synthetic student;grammar;deep features;deep features assist", "pdf_keywords": ""}, "ce9919ffb9dab701babd67a945b1590917345789": {"ta_keywords": "explanation based learning;explanations training;explanations training example;possible explanations training;knowledge level explanation;explanation based;learning formalization certain;learning called abductive;based learning formalization;choose possible explanations;learning formalization;abductive explanation based;worse explanation based;inconsistent explanation problem;training example;explanations;level explanation based;multiple inconsistent explanation;learning called;based learning;based learning called;called abductive explanation;learning ebl solves;worse explanation;based learning ebl;inconsistent explanation;training example ebl;learning;explanation problem using;certain type knowledge", "pdf_keywords": ""}, "53e161d4434576355fc5f63fe56afd8e135174b2": {"ta_keywords": "quality text unstructured;text unstructured media;high quality text;text unstructured;trained language models;crowdsourced partially ordered;quality text;generate high quality;crowdsourced partially;quality text wide;language models;structure prediction;language models structure;large 4k crowdsourced;crowdsourced;unstructured media;4k crowdsourced partially;4k crowdsourced;pre trained language;unstructured media combines;structure prediction models;trained language;tasks edge prediction;models structure prediction;text;edge prediction;models trained;unstructured;prediction models trained;dataset finetuned generate", "pdf_keywords": "prediction natural language;prediction script generation;generation task narrative;task narrative chain;events natural language;narrative chain model;task narrative;predict removed narrative;given narrative events;crowdsource scenarios literature;narrative events propose;event given narrative;narrative chain;narrative events;understand narratives providing;narratives providing;natural language generation;understand narratives;narrative event;neural language models;narrative event given;script generation task;narratives providing expectations;generate accurate generative;narratives;scripts crowdsourcing;help understand narratives;prediction script;similar predictions generated;language generation generating"}, "6c59a6ad00d82ca9f76fef92232ff3e2f3c1acc8": {"ta_keywords": "overlap aware diarization;cost tensor matching;tensor matching;aware diarization outputs;aware diarization;combining fusion datasets;tensor matching demonstrate;fusion datasets;diarization outputs majority;combining overlap aware;diarization output weighted;fusion datasets including;cost tensor;overlap aware;global cost tensor;representation diarization output;diarization outputs;weighted partite graph;tensor;combining fusion;output weighted partite;collaborative information networks;representation diarization;principal collaborative information;diarization output;combining overlap;diarization;measurements principal collaborative;outputs majority voting;method combining fusion", "pdf_keywords": "overlapping speaker diarization;diarization overlap aware;speaker diarization systems;speaker diarization detect;diarization detect overlap;speaker diarization;diarization multiple speakers;speaker diarization presence;diarization systems weighted;study speaker diarization;multi speaker conversations;information multi speaker;speech separation recognition;overlapping speech;resulting speaker diarization;diarization algorithms automatically;overlapping speech given;diarization algorithms;diarization systems clustering;weighted set speaker;effective label voting;diarization overlap;usual diarization algorithms;assigning overlapping speakers;problem speaker diarization;overlapping speakers based;speaker classification;automatically combine speakers;speech separation;overlap aware resegmentation"}, "a5881560968963d0c845c468a273261fde0b7248": {"ta_keywords": "perturbations input text;text interpretable robust;generate candidate text;interpretable robust perturbations;text interpretable;candidate text interpretable;text;interpretable robust;perturbations input;robust perturbations experiments;input text;simple perturbations input;robust perturbations;candidate text;input text use;impact simple perturbations;text use;robust;perturbations;perturbations experiments demonstrate;experiments demonstrate generated;simple perturbations;perturbations experiments;text use novel;good quality metrics;generated;demonstrate generated;generated candidates good;generate;demonstrate generated candidates", "pdf_keywords": "fragile interpretations nlp;generating fragile interpretations;generate fragile interpretations;perturbed text examples;deep natural language;word perturbations generate;fragile interpretations deep;induce fragile explanations;words perturbed;10 words perturbed;possible perturbed text;fragile explanations;fragile explanations demonstrate;perturbed text;fragile interpretations natural;generate perturbed text;demonstrate fragile interpretations;search word perturbations;generate text adversarial;fragile interpretations possible;relative error adversarially;natural language processing;fragile interpretations;examples demonstrate fragile;words perturbed average;text adversarial;natural language;word perturbations;text explain predictions;simple word perturbations"}, "3004a3e4d8969dc3c36c9274b0f76ecc874f2e6a": {"ta_keywords": "method separating speech;separating speech based;separation recognition signals;separating speech;separation signal noisy;separation recognition;method separation recognition;based separation signal;method based separation;separation signal;noise ratio based;method separating;signal noisy spectrum;new method separating;signal noise ratio;based separation;recognition signals method;noisy spectrum;noisy spectrum investigate;speech based;use signal noise;noise ratio;method separation;recognition signals;application method separation;separation;separating;signal noisy;signal noise;ratio based objective", "pdf_keywords": ""}, "e29e43d9c0772d44cff53044484970599db30d5f": {"ta_keywords": "domain adaptation neural;domain adaptation;results domain adaptation;neural machine translation;machine translation instead;improvements alternative adaptation;machine translation;framework domain adaptation;use learned domain;adaptation neural machine;learned domain;adaptation neural;learned domain differentials;machine translation demonstrate;alternative adaptation;modeling difference domains;translation instead smoothing;difference domains;translation instead;translation demonstrate effectiveness;alternative adaptation strategies;domain differentials adapt;difference domains using;adaptation;domains using models;domain differentials;consistent improvements alternative;adaptation strategies;translation;consistent improvements", "pdf_keywords": "adaptation machine translation;neural machine translation;machine translation models;translation models;domain adaptation machine;machine translation nmt;domain adaptation;improving accuracy translation;trained domain parallel;machine translation;machine translation analyses;domain adaptation simply;translation models related;adapting neural machine;machine translation experiment;domain adaptation based;adaptation deep;domain adaptation joint;translation nmt model;accuracy translation strategy;given domain adaptation;adapting neural;deep adaptation;beneficial domain adaptation;advantageous domain adaptation;predict domain words;translation strategy optimize;metrics domain adaptation;machine translation use;deep adaptation deep"}, "f6eafb82d2450f28f668443b689c91e896a0d63e": {"ta_keywords": "linear bandit problems;bandit problem linear;linear bandit;bandit problem;bandit problems;features linear bandit;explores bandit problem;linear reinforcement learning;optimism face uncertainty;linear reinforcement;bandit;reinforcement learning ei;problem linear reinforcement;rewards resulting algorithm;variants using optimism;reinforcement learning;using optimism;paper explores bandit;explores bandit;optimism;using optimism face;optimism face;uncertainty ofu principle;uncertainty;confidence set;rewards resulting;uncertainty ofu;function governs rewards;confidence set vector;rewards", "pdf_keywords": ""}, "8225b047e0fe90c2d5f9bb77fd94396a9d0fd21e": {"ta_keywords": "ranking information content;article gene identifier;ranking information;graph based ranking;ranking method graph;named entity recognition;gene identifier method;based ranking method;ranking method;entity recognition systems;entity recognition;ranking;gene identifier;article gene;ranking method used;ranking process;based ranking;method ranking information;systems ranking;systems ranking process;information content article;results named entity;content article based;ranking process graph;relation article gene;ranking method shown;method ranking;individual systems ranking;named entity;content article", "pdf_keywords": ""}, "3873e60de2d20aa33829e2d3d79221e716785546": {"ta_keywords": "semi supervised learning;semi supervised;approach semi supervised;supervised learning nir;supervised learning;supervised;discriminative model trained;features labeled data;labeled data based;telephone speech corpora;data based discriminative;discriminative model;based discriminative model;labeled data;2011 telephone speech;features labeled;telephone speech;based discriminative;speech corpora;discriminative;speech corpora demonstrate;labeled;confidences form features;learning;based perceptron algorithm;learning nir;lds based perceptron;form features labeled;algorithm propagates confidences;algorithm perceptron", "pdf_keywords": ""}, "229c0c13e5c2d8e189efccf77b8179ec16500212": {"ta_keywords": "machine translation engine;translation engine;translation engine uses;aavatar based machine;based machine translation;machine translation;provide aavatar controlled;aavatar controlled;aavatar controlled controlled;trees provide aavatar;aavatar based;controlledavatar interface engine;aavatar;provide aavatar;controlled controlledavatar;controlledavatar interface;controlled controlled controlledavatar;controlledavatar;controlled controlledavatar interface;interface engine;vavatar html interface;interface engine available;engine uses trees;based machine;vavatar;pravatar vavatar;vavatar html;uses trees provide;jp pravatar vavatar;pravatar vavatar html", "pdf_keywords": ""}, "14fce3cfa503894f244fc6ea8a7a00fa0ddfd94e": {"ta_keywords": "quantum fluid driven;quantum fluid;dimensional quantum fluid;zero repulsive force;repulsive force model;repulsive force;dynamics simple;model dimensional quantum;quantum;dimensional quantum;fluid driven single;dynamics;dynamics simple model;non zero repulsive;force model solved;zero repulsive;force model;fluid driven;study dynamics simple;driven single non;force;single non zero;fluid;repulsive;xmath2;driven single;xmath1 xmath2;xmath0 xmath1 xmath2;xmath2 xmath3;xmath2 xmath3 xmath4", "pdf_keywords": ""}, "a54a3a7b02cacd92b3bc633be7ea54e4f365fa65": {"ta_keywords": "characterizing malware communities;malware communities based;malware communities;characterizing characterizing malware;characterizing malware;malware;structure malware;malware binary structure;malware binary;consensus graph representation;structure malware binary;graph representation features;binary structure malware;graph representation;weighted consensus graph;communities based;consensus graph;communities based use;communities;features used characterize;use weighted consensus;cognitive mapping;weighted consensus;cultural cognitive mapping;features modeled;representation features modeled;graph;latent spatial domain;attributes latent spatial;cognitive mapping scm", "pdf_keywords": ""}, "935c275868bec7301f4bd254159978d8ded138b9": {"ta_keywords": "reaction expression xmath3;proton xmath4 reaction;reaction expression xmath1;xmath4 reaction expression;expression xmath3 reaction;expression xmath1 reaction;xmath0 reaction expression;xmath2 reaction expression;section xmath0 reaction;expression xmath2 reaction;proton xmath4;proton proton xmath4;xmath3 reaction;xmath1 reaction;xmath4 reaction;xmath3 reaction literature;xmath0 reaction;xmath1 reaction literature;xmath2 reaction;section expression proton;expression proton;expression xmath3;expression expression proton;expression proton neutron;given expression xmath2;expression xmath2;expression xmath1;expression proton proton;relative contributions proton;contributions proton", "pdf_keywords": ""}, "b9f0c7e99bcc94c2cd75fd8e1cef45188f51270e": {"ta_keywords": "temporal classification;based temporal classification;joint speaker recognition;label transitions neural;speaker recognition;temporal classification gtc;speaker recognition proposed;transitions neural network;graph based temporal;multi speaker speech;speech use gtc;label transitions;labels label transitions;characterizing multi speaker;speaker speech;transitions neural;speaker speech use;model posteriors labels;perform joint speaker;supervised learning tasks;joint speaker;supervised learning;posteriors labels;speech;speaker;supervised;multi speaker;posteriors labels label;classification;speech use", "pdf_keywords": "end speech recognition;temporal classification gtc;predict end utterance;speaker based prediction;speech recognition task;speaker prediction model;speech recognition;automatic speech;transitions label predictions;speech recognition challenging;train linguistic network;automatic speech recognition;classifier conditional speaker;based temporal classification;temporal classification;based speaker prediction;speaker prediction;label predictions neural;networks predict posterior;speaker input classifier;temporal classification method;end automatic speech;representation network;neural network sequence;representation representation network;speech languages;including speech recognition;speech recognition apda;speech languages speaker;probabilities label transitions"}, "dfa34a10e2ba861545549c3188ef245b1e69bcdf": {"ta_keywords": "event extraction bio;event extraction;applied event extraction;extraction bio event;event extraction set;bio event sequences;method event extraction;word embedding features;bio event;word embedding;event sequences based;word embedding method;based word embedding;extraction set biopolymers;sequences based word;event sequences;use bag ofwords;extraction bio;bag ofwords;biopolymers;event;embedding features;bow word embedding;extraction;embedding method based;embedding features method;set biopolymers;bag ofwords bow;biopolymers method;embedding", "pdf_keywords": ""}, "2ea5b0f5e476ddc00ae4450f2888a51fa25dd1d3": {"ta_keywords": "novel task augmentation;improving shot training;task augmentation;data auxiliary task;task augmentation technique;improving shot;trained efficiently;self trained efficiently;task unlabeled texts;approach improving shot;task fine tuning;tuning target task;shot training;fine self trained;target task unlabeled;auxiliary task;relies novel task;shot training presence;auxiliary task fine;self trained;target task;novel task;fine tuning target;training improved;training improved fine;resulting training improved;trained efficiently variety;trained;novel approach improving;improved fine tuning", "pdf_keywords": "training natural language;context shot learning;natural language inference;performance nlp benchmarks;pretrained language models;trained language models;sentence representations learning;language inference nlp;nlp benchmarks;inference nlp propose;approach task augmentation;natural language models;nlp benchmarks generally;inference nlp;inference nlp resulted;performance nlp;downstream performance nlp;based task augmentation;task training natural;data augmentation large;pretrained language representations;task augmentation;language models large;deep bidirectional;shot learning;language inference;training examples tasks;sentence representations;accuracy predicting sentence;intermediate task training"}, "92f93c0014ba4da59180c4cd141ad0dcaad5803f": {"ta_keywords": "multilingual instance based;multilingual instance;tasks involving multilingual;transfer learning;transfer learning method;based transfer learning;present multilingual instance;multilingual data method;multilingual data;involving multilingual data;instance based transfer;multilingual;present multilingual;languages improvement;data target language;auxiliary languages improvement;involving multilingual;target language auxiliary;target languages;auxiliary target languages;target language;transitive vocabulary overlaps;transitive vocabulary;language auxiliary languages;languages improvement explained;auxiliary languages;effect transitive vocabulary;transfer;language auxiliary;based transfer", "pdf_keywords": "multilingual search;vocabulary transfer;tuning transfer sentences;learns embeddings vocabulary;vocabulary transfer multiple;multilingual deeptrieval;indirect vocabulary transfer;multilingual search aim;transfer sentences;workshop multilingual search;cross language instance;learns embeddings;transfer sentences multiple;transfer learning surprisingly;discovered multilingual deeptrieval;large corpus;large corpus candidates;large linguistic domains;corpus improve performance;problem multilingual search;documents large corpus;surprisingly effective multilingual;newly discovered multilingual;multilingual domains associated;transfer learning;embeddings vocabulary items;sentences multiple languages;transfer multiple languages;multilingual domains;multilingual search www"}, "4a9c80e263fd0a88ad8220aa076ede4a3e77fcc1": {"ta_keywords": "graph structures deep;structures deep network;networks predefined structure;discovery deep networks;graph structures;graph structures performed;deep networks predefined;deep network;networks predefined;discovery deep;nodes desired structure;deep network resulting;nodes mutation graph;predefined pattern nodes;deep networks;experiments deep network;network support vector;mutation graph structures;pattern nodes;structures deep;search nodes;deep network support;based mutation graph;networks;nodes;method discovery deep;search nodes desired;network resulting models;mutation graph;nodes desired", "pdf_keywords": "deep learning attack;adversarial deep;detect adversarial;adversarial deep learning;successfully detect adversarial;detecting adversarial;testing adversarial;adversarial samples deep;detect adversarial samples;adversarial examples deep;generated adversarial attack;adversarial attack;detecting adversarial samples;detection adversarial deep;testing adversarial samples;adversarial;discovery adversarial deep;defects deep network;generated adversarial;adversarial sample great;adversarial sample model;adversarial sample;used detect adversarial;adversarial samples generated;adversarial samples pruned;testing method adversarial;adversarial samples;new method adversarial;method adversarial;likely generated adversarial"}, "885fe11ed7ab81c8609ccddb3e10f62577c04ab9": {"ta_keywords": "rich dialogue systems;dialogue systems;dialogue systems algorithm;np rich dialogue;search deep np;agents dialogue;exploration deep np;search deep;dialogue resulting search;dialogue;rich dialogue;agents dialogue resulting;resulting search deep;space agents dialogue;deep np rich;dialogue resulting;efficiency exploration deep;exploration deep;np rich agents;neural networks efficiently;search np rich;bayes backprop neural;backprop neural networks;backprop neural;deep np;networks efficiently explore;efficiently explore sample;search np;sample space agents;resulting search np", "pdf_keywords": "learning dialogue policies;learning dialogue;learn dialogue policies;dialogue policies based;exploration dialogue agents;deep reinforcement learning;explore deep reinforcement;task oriented dialogue;learn dialogue;dialogue policies combines;method learning dialogue;dialogue systems;dialogue policies;dialogue agents framework;dialogue policies effective;dialogue interaction;deep reinforcement;oriented dialogue systems;dialogue agents;oriented dialogue;dialogue;reinforcement learning task;exploration dialogue;dialogue systems approach;reinforcement learning;efficiently explore deep;novel reinforcement learning;vime learn dialogue;dialogue interaction allows;informed reinforcement learning"}, "a1340029d8a5c57bee8a5995ac3beafd3d0ba96c": {"ta_keywords": "sensor subset selection;connected sensor subset;sensor subset;subset information estimation;subset selection;subset selection presented;connected connected sensor;connected sensor;subset information;use subset information;selection presented algorithm;approximation use subset;tools stochastic approximation;estimation partial data;information estimation partial;stochastic approximation use;stochastic approximation;sensor;information estimation;data proposed algorithm;data sets;algorithm connected connected;subset;use subset;novel algorithm connected;accuracy competing algorithms;algorithm connected;selection;experimental data sets;novel algorithm", "pdf_keywords": ""}, "c6854064cb5053e67d23394eee6d1646108f6d56": {"ta_keywords": "textual entailment task;textual entailment;novel textual entailment;setting textual entailment;standard textual entailment;entailment task;textual entailment evaluate;entailment task requires;entailment;multiple premise sentences;trivial lexical inferences;lexical inferences emphasizes;premise sentences;lexical inferences;inferences emphasizes knowledge;premise sentences present;inference multiple premise;inferences emphasizes;entailment evaluate;inference multiple;task requires inference;textual;requires inference multiple;premise task;multiple premise task;inferences;entailment evaluate strong;inference;sentences;challenging setting textual", "pdf_keywords": "textual entailment task;novel entailment task;novel textual entailment;textual entailment;textual entailment propose;proposed entailment task;semantic textual entailment;entailment challenge based;entailment task inspired;entailment task;standard textual entailment;entailment task multiple;task proposed entailment;setting textual entailment;entailment challenge;novel approach entailment;task multiple entailing;entailment task requires;premises multiple entailment;propose novel entailment;textual entailment evaluate;entailment task involves;entailment powerful tool;multiple entailment;approach entailment challenge;novel entailment;entailment unordered premise;challenging realistic entailment;multiple entailing;entailment mpe task"}, "74b05adf1ec74849a4f7963fe3f17fd61b92af4b": {"ta_keywords": "queries employs ranking;users query structures;ranking model weakly;query scenarios fanda;weakly supervised max;users query scenarios;study users query;ranking model;query scenarios;query structures;supervised max learning;weakly supervised;model weakly supervised;query scenarios demonstrate;queries;ranking;supervised max;query structures using;queries employs;follow query scenarios;using ranking model;using ranking;users query;structures using ranking;employs ranking model;max learning;fanda takes account;structures queries employs;fanda multiple baselines;superiority fanda multiple", "pdf_keywords": "learn query languages;learn query;queries contextual;queries contextual information;language follow queries;queries context;natural language interfaces;follow query dataset;queries context sensitive;having learn query;query languages;language interfaces databases;termed follow query;query dataset followup;follow queries;multiple queries contextual;follow query analysis;query intents;query dataset;databases using natural;query analysis propose;queries employs;query analysis;natural language follow;features natural language;anonymizing parsing high;anonymizing parsing;parsing natural language;queries employs ranking;queries"}, "a5f214e23b8cd35a370a182c155ef333d77c5bb2": {"ta_keywords": "acoustic indicators stance;speaker convey topic;stance natural speech;natural speech;ability speaker convey;speaker convey;speech;measure ability speaker;acoustic indicators;ability speaker;experts groups dyads;identify acoustic indicators;convey topic dyads;group experts experts;experts experts groups;speaker;attempt identify acoustic;group experts;experts groups;acoustic;convey topic joint;topic joint work;convey topic;identify acoustic;topic dyads completed;topic joint;topic dyads;experts experts;work group experts;dyads completed tasks", "pdf_keywords": ""}, "b33caf27fe5584b9b773c75fc35ee0e8b1421864": {"ta_keywords": "equilibrium potential game;potential function game;equilibrium suitable optimization;potential game populations;continuous space game;game populations continuous;continuous type game;computing equilibrium potential;equilibrium potential;optimization potential;optimization potential function;function game continuous;potential game;computing equilibrium;suitable optimization potential;equilibrium suitable;problem computing equilibrium;compute equilibrium suitable;game continuous type;populations continuous space;equilibrium;choice potential function;function compute equilibrium;compute equilibrium;game populations;type game populations;populations continuous;game continuous;function game;optimization", "pdf_keywords": ""}, "5ee96dd7e3395d8a53d6d3ceb62593477a4e0fe1": {"ta_keywords": "colorization able predict;conditioned colorization able;language conditioned colorization;conditioned colorization;predict shape images;colorization;colorization able;predict shape objects;predict shape;predict accurately shape;shape images;used predict shape;shape images domain;shape used predict;accurately shape images;shape objects;shape objects domain;images domain arbitrary;images;model language conditioned;arbitrary shape;accurately shape;arbitrary shape demonstrate;language conditioned;domain arbitrary shape;able predict accurately;shape demonstrate model;predict;arbitrary shape used;conditioned", "pdf_keywords": "colorizations model trained;automatic colorization natural;predicting colorizations images;colorization natural language;language conditioned colorization;plausible colorizations language;predicting colorizations;automatic colorization;task automatic colorization;colorizations language;colorizations language agnostic;prediction tasks colorized;method predicting colorizations;accurate plausible colorizations;approach colorization cie;colorization;colorization cie;predict plausibility colorization;novel approach colorization;tasks colorized images;plausible colorizations;novel approach colorize;colorizations;plausibility colorization;based known colorization;known colorization;approach colorization;colorizations images;high quality colorizations;conditioned colorization"}, "53f1fb4dc887540ef134a8d08c152789c313aa5c": {"ta_keywords": "end speech recognition;speech recognition systems;speech recognition;recognition systems embedding;embedding systems trained;attention based encoder;composite embedding systems;using triphone based;triphone based character;embedding systems;novel composite embedding;extraction using triphone;using triphone;triphone based;embedding systems demonstrated;composite embedding;proposed composite embedding;recognition systems;connectionist temporal classification;based encoder;systems embedding systems;encoder decoder connectionist;embedding;embedding systems language;encoder;decoder connectionist temporal;performance feature extraction;systems embedding;triphone;decoder connectionist", "pdf_keywords": ""}, "7262bc3674c4c063526eaf4d2dcf54eecea7bf77": {"ta_keywords": "paraphrastic sentence embeddings;textual similarity competition;sentence embeddings outperform;sentence embeddings;textual similarity;paraphrastic;paraphrastic sentence;train paraphrastic sentence;sentence embeddings use;train paraphrastic;supervised systems textual;systems textual similarity;harnesses power paraphrastic;parallel machine learning;use train paraphrastic;embeddings outperform supervised;textual;power paraphrastic sentence;embeddings;similarity competition;power paraphrastic;embeddings outperform;embeddings use train;systems textual;similarity;embeddings use;sentence;parallel;learning algorithm harnesses;machine learning", "pdf_keywords": "embeddings paraphrastic text;sentence embeddings paraphrastic;paraphrastic sentence embeddings;embeddings paraphrastic;embedding dataset paraphrastic;paraphrastic sentence embedding;embeddings using paraphrastic;larger dataset paraphrastic;sentence embeddings similarity;text learn paraphrastic;paraphrastic text based;sentence embeddings paranmt;sentence embeddings outperform;learn paraphrastic;training paraphrastic;training paraphrastic sentence;learn paraphrastic sentence;sentence embedding dataset;sentence embeddings improves;embeddings semantic similarity;parallel machine translation;paraphrastic text;dataset paraphrastic sentence;sentence embedding tasks;dataset paraphrastic;semantic textual similarity;neural machine translation;translation learns similarity;pretrained sentence embeddings;train paraphrastic sentence"}, "6d2d86cf5e80b58a03360559095ea3603548248f": {"ta_keywords": "estimate matrix rows;projected gradient descent;estimate matrix;permutation columns prior;constraint permutation columns;squares estimator optimal;shape constraint permutation;goal estimate matrix;convex objective;non convex objective;algorithm based projected;statistical seriation problem;estimator optimal logarithmic;estimator optimal;efficient algorithms;based projected gradient;minimum non convex;algorithms computing squares;matrix rows;statistical seriation;columns prior;gradient descent;computing squares estimator;algorithms;constraint permutation;projected gradient;rows satisfy shape;columns prior work;permutation columns;new algorithm", "pdf_keywords": ""}, "2d9769ce319a8acbe97438b45b0d381db2a538d1": {"ta_keywords": "dynamics spin liquid;spin liquid equation;solution spin liquid;spin liquid dimensional;spin liquid;infinite spin solution;spin solution;model dynamics spin;solution spin;dynamics spin;spin model solved;spin solution good;2d ising model;spin model;infinite spin model;infinite spin;liquid dimensional 2d;liquid equation motion;exact solution spin;limit infinite spin;dimensional 2d ising;liquid dimensional;spin;liquid equation;2d ising;ising model model;ising model;liquid;ising;model dynamics", "pdf_keywords": ""}, "c143d2b09bdfc0dff784dce2668fd5657806dbf2": {"ta_keywords": "multi hop inference;hop inference;hop inference characterization;characterization world science;inference characterization world;task multi hop;multi hop;inference characterization;world science team;world science;characterization world;inference;hop;physreva 2010 08;physreva 2010;elsepy org doi;elsepy org;science team;www elsepy org;science team responsible;characterization;www elsepy;1103 physreva 2010;science;teams 2020;team teams 2020;elsepy;teams 2020 shared;http www elsepy;task multi", "pdf_keywords": ""}, "2873053aa18059a61ead5880d449f5bccda2d213": {"ta_keywords": "interdependent scheduling games;scheduling games;scheduling games players;interdependent scheduling;model interdependent scheduling;services schedule independently;scheduling;services schedule;existence equilibria;schedule independently;set services schedule;existence equilibria model;schedule;games players control;existence existence equilibria;players control;equilibria model;equilibria;games;players control set;games players;set services;players;services;interdependent;control set services;independently;independently study existence;propose model interdependent;schedule independently study", "pdf_keywords": "interdependent scheduling games;scheduling games consistency;welfare maximizing schedule;welfare maximizing schedules;schedules existence welfare;optimal schedules games;scheduling games;schedule optimal reward;existence schedule games;schedule maximizes reward;reward schedule optimal;interdependent scheduling;scheduling games players;scheduling model;optimal reward schedule;maximizing schedules proved;maximizes reward schedule;model interdependent scheduling;existence optimal schedules;schedule tasks maximizes;approach interdependent scheduling;schedule services dynamics;transitivity optimal schedule;theorems concerning schedule;scheduling games motivated;consider distributed scheduling;games computing optimality;schedule games provably;maximizing schedule;schedule maximizes"}, "fee62123e1d2ac56065675983475b079e1e9106f": {"ta_keywords": "beam search decoding;output beam search;search decoding;beam search;search decoding procedure;objective beam search;approximation beam search;beam search new;training objective beam;loss metric hamming;decoding;metric hamming loss;named recognition tagging;hamming loss evaluated;beam search focuses;decoding procedure experiments;hamming loss;recognition tagging;training objective yields;decoding procedure;recognition tagging compared;search new training;evaluated output beam;new training objective;optimizing new training;training objective sub;training objective;named recognition;loss metric;output beam", "pdf_keywords": "continuous beam search;approximation beam search;beam search empirically;beam search greedy;approximation beam searchwe;output beam search;discontinuous beam search;loss optimization;beam search continuous;beam search powerful;algorithms beam search;search greedy decod;beam search generator;loss optimization empirically;soft beam search;beam search;beam search defined;beam search composed;greedy decod algorithm;direct loss optimization;presented beam search;loss metric hamming;greedy decod;relaxation beam search;trained optimize sequence;train beam search;trained optimize;beam search able;process beam search;beam search experiments"}, "a96e05353032cc6f3d72eb5eca192295beac065e": {"ta_keywords": "graphical learning stacked;stacked graphical learning;learning stacked graphical;learning stacked;graphical learning meta;features stacked graphical;stacked graphical;graphical learning;graphical learning based;graphical learning shown;graphical learning evaluated;meta learning scheme;terms stacked graphical;meta learning;process stacked graphical;learning meta learning;features stacked;graphical models;learning scheme;known graphical models;graphical models known;learning meta;base learner;stacked;instance features stacked;quadratic terms stacked;learning based;learning shown;process stacked;learning", "pdf_keywords": ""}, "c52ac453e154953abdb06fc041023e327ea609a4": {"ta_keywords": "acoustic model construction;approach acoustic model;acoustic model;approach acoustic;new approach acoustic;based self attention;acoustic;context range proposed;self attention effective;self attention;control context range;context range;attention effective;model construction;model construction based;attention;range proposed model;attention effective previous;model construction allows;model;proposed model effective;explicit control context;control context;proposed model;model effective;construction based self;approaches precise faster;allows explicit control;precise faster precise;model effective popular", "pdf_keywords": "attention acoustic modeling;acoustic variability attention;acoustic sequence prediction;attention acoustic;self attention acoustic;model acoustic sequence;acoustic models;encode acoustic data;acoustic models presented;acoustic data;encode acoustic;acoustic modeling;context acoustic;suited encode acoustic;natural acoustic variability;local context acoustic;acoustic sequence;acoustic modeling proposing;model acoustic;class acoustic models;context acoustic signal;models presented attention;acoustic variability;noise attention bias;attention given utterance;random noise attention;attention model;attention model informative;resulting attention model;context range attention"}, "7ac4227d0b4d38b16da27ed55bd53ce240a32404": {"ta_keywords": "end automatic speech;automatic speech;speech recognition asr;automatic speech recognition;end speech translation;speech recognition;long form utterances;speech translation;end end speech;recognition asr;end speech;recognition asr perform;autoregressive nar models;utterances techniques combined;utterances techniques;form utterances techniques;autoregressive nar;non autoregressive nar;end automatic;utterances;end end automatic;form utterances;nar models end;autoregressive;non autoregressive;study non autoregressive;asr perform;speech;models end end;nar models", "pdf_keywords": "end speech recognition;speech recognition asr;automatic speech recognition;speech recognition;end automatic speech;automatic speech;robust speech recognition;speech networks;speech networks input;speech recognition systems;speech recognition vertex;networks input speech;applied speech networks;recognition asr;performance robust speech;speech compare accuracy;robust speech;recognition asr experiments;estimation attention speech;speech translation models;100 input speech;temporal classification ctc;advanced neural network;unstructured utterances;models automatic scanning;end automatic scanning;unstructured utterances set;threshold nar models;end speech translation;input speech"}, "2f153172b92ea32f242d9cb6b94d162e52ef5f0b": {"ta_keywords": "learning representation d_;dimensional d_ representations;representations d_ dimensional;dimensional d_ representation;learning representation;representation d_ dimensional;d_ representations;d_ representations d_;d_ representation set;problem learning representation;representations d_;representation set d_;representations;representation set;d_ representation;representation d_;d_ representation d_;representation;d_ dimensional d_;dimensional d_ dimensional;dimensional d_;d_ dimensional;set d_ dimensional;dimensional;consider problem learning;problem learning;learning;set d_;d_;consider problem", "pdf_keywords": ""}, "4a348e4725a2bc677e4aa40aa63c1421e8f335c9": {"ta_keywords": "mean precision recall;precision recall;binary classification classifier;binary classification;classification;classifier;f1 score classifier;classification classifier;score classifier;score classifier completely;precision recall f1;classifier parameters optimal;classifier completely;maximizing harmonic mean;harmonic mean precision;classifier produces real;recall f1 score;threshold depends classifier;maximizing harmonic;classification classifier produces;optimal threshold;maximize f1 score;classifier produces;classifier parameters;threshold half optimal;optimal f1 score;mean precision;problem maximizing harmonic;classifier completely uninformative;optimal threshold half", "pdf_keywords": "present classifier maximizes;classifier maximizes;optimal classifiers informwe;optimal classifiers;classifier outputs provide;classifiers;classification theorems optimal;predictions maximize f1;optimal threshold predictions;optimal classifiers informative;forward learning;classifier;label optimal predictions;classification;classifier maximizes macro;choosing optimal classifiers;binary classifier;determined classifier outputs;success binary classifier;forward learning object;classifiers informwe;learning object f1;learningwe present optimal;classifier outputs;maximize f1 score;thresholding maximize f1;prediction macro f1;classifiers informative;optimal prediction macro;predictions maximize"}, "c2a79e2a65b721d4de5f6d4806323174b9f8f393": {"ta_keywords": "zero label learning;label learning natural;fewshot inference synthetic;benchmark label learning;zero label;approach zero label;label learning results;label learning;unsupervised data generation;inference synthetic;inference synthetic data;learning results benchmark;synthetic data train;learning natural;learning natural language;superglue benchmark label;benchmark label;fewshot inference;leverages fewshot inference;models synthetic data;learning results;language processing leverages;natural language processing;novel approach zero;synthetic data;synthetic data achieve;natural language;learning;novel unsupervised data;specific models synthetic", "pdf_keywords": "zero label learning;shot inference supervised;supervised shot inference;label learning natural;pretrained language models;shot inference tasks;shot inference text;inference supervised unsupervised;framework shot inference;semi supervised shot;unsupervised learning tasks;human annotated data;shot generation unsupervised;inference supervised;learning natural language;shot inference extend;explores zero label;annotated data;natural language models;new zero label;learning data unsupervised;supervised shot;semi supervised;supervised unsu pervised;training prediction unstructured;pervised shot inference;zero label;label learning;label learning paradigm;supervised semi"}, "3b00e642de51d0f8378c7c35eca89f2ecb6f3af8": {"ta_keywords": "individuals microduplications 22q11;familial novo microduplications;syndrome region theduplications;microduplications 22q11 21;18 individuals microduplications;microduplications 22q11;novo microduplications 22q11;21 microdeletion syndrome;microdeletion syndrome region;theduplications familial novo;22q11 21 microdeletion;individuals microduplications;microdeletion syndrome;region theduplications familial;theduplications familial;novo microduplications;21 microdeletion;distal 22q11 21;microduplications;distal 22q11;region distal 22q11;microdeletion;theduplications;region theduplications;22q11 21 q11;23 region distal;21 q11 23;syndrome region;22q11 21;21 q11", "pdf_keywords": ""}, "7a6c61b57bac074f7cd85963fd13da8f3321e087": {"ta_keywords": "latent dirichlet allocation;topic discovery;hyperlinking distribution;topic discovery new;dirichlet allocation known;hyperlinks document similar;latent dirichlet;dirichlet allocation;hyperlinking distribution index;effectiveness topic discovery;framework latent dirichlet;hyperlinking;influential blog postings;distribution hyperlinked hyperlinks;generative model hyperlinks;hyperlinking distribution hyperlinked;hyperlinks;model hyperlinks;hyperlinked hyperlinks;hyperlinks document information;study distribution hyperlinked;hyperlinks called;degree hyperlinking distribution;hyperlinks document;highly influential blog;hyperlinked hyperlinks document;model hyperlinks called;influential blog;document degree hyperlinking;blog postings topic", "pdf_keywords": ""}, "8c25e1c223fc70509172a32111c91fe4b9f86a56": {"ta_keywords": "networking sds cognitive;software defined networking;cognitive wireless networking;programmable routers sds;sds cognitive wireless;sdr cognitive radio;software defined radio;defined networking sds;programmable network architecture;defined networking;radio sdr cognitive;programmable routers;programmable network;overview programmable network;networking sds;dds programmable routers;proposed device network;defined radio sdr;programmable routers dds;cw programmable routers;wireless networking;routers dds programmable;network architecture proposed;cognitive radio;cognitive wireless;cognitive radio cw;routers sds;radio sdr;device network;routers sds device", "pdf_keywords": "programmable wireless networking;programmable wireless networks;wireless programmable networks;programmable radio network;programmable wireless network;radio network programmable;radio networks virtualizable;active networking software;defined software wireless;active networking paradigm;software defined wireless;radios wireless virtualization;defined wireless networks;future programmable wireless;overview programmable wireless;cognitive wireless networking;active networking;wireless networks emerging;software wireless networks;programmable networking paradigm;networks cognitive radio;programmable wireless;wireless virtualization;virtualizable wireless networksthis;configurable radio networks;programmable networking;cognitive radio network;virtualizable wireless;programmable networks wsn;inprogrammable wireless networks"}, "4a36a00db217fd98f1bd943aa2f2d6303adbc456": {"ta_keywords": "fair principal component;principal component analysis;characterizing fair principal;optimization underlying manifold;nonconvex optimization manifold;optimization manifold;component analysis;principal component;optimization manifold solve;component analysis pam;form smooth optimization;dimensionality reduced conditional;smooth optimization underlying;smooth optimization;nonconvex optimization;pam nonconvex optimization;method characterizing fair;dimensionality reduced;mhd dimensionality reduced;underlying manifold approach;fair pam nonconvex;manifold approach based;method smoothing pam;characterizing fair;underlying manifold;manifold solve;reduced conditional distributions;fair principal;mhd dimensionality;manifold solve using", "pdf_keywords": "fairness supervised learning;fairness supervised;fairness data supervised;problem fairness supervised;fair dimensional parameterization;new fairness constraint;fairness constraint parameter;fair principal component;fairness parameter free;introduce fairness constraint;fairness constraint;fairness constraint based;fairness parameter;algorithm guarantees fairness;fairness based characterization;definition fairness parameter;fairness bounds proposed;assigning fairness data;assigning fairness;classical fairness bounds;fair representation representations;fair representation machine;bounds proposed fairness;fairness bounds;problem assigning fairness;parameter estimation fair;proposed fair dimensional;fairness data;fair parameter machine;fairness downstream tasks"}, "101d619b5911e9c2fda6f02365c593ae61617cb6": {"ta_keywords": "policy cooperative dialogue;cooperative dialogue using;cooperative cooperative dialogue;cooperative dialogue systems;cooperative dialogue;dialogue using framing;automatically learn cooperative;framing design cooperative;framing learned policy;using framing learned;policy cooperative;cooperative policy cooperative;learn cooperative cooperative;dialogue systems;framing learned;cooperative policy;cooperative cooperative policy;dialogue systems use;learn cooperative;dialogue using;cooperative systems;design cooperative systems;cooperative systems topic;design cooperative;introduction use framing;systems use framing;cooperative cooperative;dialogue;framing introduction;learned policy evaluated", "pdf_keywords": ""}, "d4762619b55c65120307ceebe4a0646984f6045a": {"ta_keywords": "statistical machine translation;conditional translation probabilities;translation probabilities model;translation probabilities framework;joint conditional translation;translation probabilities;machine translation;machine translation smt;conditional translation;translation smt framework;translation smt;tables joint conditional;translation;domains context table;accurate meta tables;modeling behavior joint;table model model;meta tables joint;novel statistical machine;optimal time domain;table model;behavior joint conditional;meta tables;statistical machine;context table model;domains context;domain domains context;joint conditional;domain domain domains;time domain domain", "pdf_keywords": ""}, "50ec3d960ac458573a1e4a1556420c5e96d58609": {"ta_keywords": "large corpus model;large corpus;context large corpus;corpus model bound;corpus model;corpus;supervised learning context;learning context large;accurate evidence iterations;supervised learning;novel approach supervised;approach supervised learning;supervised;approach supervised;evidence iterations;evidence iterations underlying;learning context;learning;distdr able accurate;bound likely evidence;able accurate evidence;context large;benchmarks;distdr;accurate evidence;evidence proposed approach;approach named distdr;evidence proposed;evidence;multi hop benchmarks", "pdf_keywords": "learns evidence large;learns search evidence;learns answer searching;learns evidence;approach learns evidence;learning evidence simple;learning evidence;question weakly supervised;dense evidence search;evidence retrieval;learns answer;corpus distant supervision;answer searching evidence;supervision answer labels;evidence retrieval combines;evidence large corpus;dense retrieval methods;evidence search improved;combines dense retrieval;searching evidence dense;problem evidence retrieval;approach learns answer;corpus use deep;dense retrieval;annotated evidence;weakly supervised;annotated evidence expensive;dense retrieval retriever;search evidence searching;evidence search"}, "d59c7b1c85f8c459863762361f251575785347a8": {"ta_keywords": "permeability hard spheres;regularly porous membranes;porous membranes;membranes consisting hard;porous membranes consisting;dynamics simulations permeability;diffusion mechanism size;moving cylindrical pores;molecular dynamics simulations;simulations permeability hard;cylindrical pores model;pores model;membranes;permeability hard;molecular dynamics;model regularly porous;pores model solved;size cylindrical pores;simulations permeability;diffusion mechanism;hard spheres moving;cylindrical pores;hindered diffusion mechanism;cylindrical pores size;membranes consisting;hard spheres;hard spheres size;consisting hard spheres;driven molecular dynamics;diffusion", "pdf_keywords": ""}, "35750f1908f405bb38b0708972f33fe07b378b64": {"ta_keywords": "interpretability logic zagreb;interpretationability logic modal;interpretability logic;interpretability logic main;interpretability logic version;interpretationability logic;interpretationability logic differential;called interpretability logic;overview interpretability logic;del interpretationability logic;formulas called interpretability;interpretationability;interpretability;del interpretationability;years interpretationability logic;called interpretability;equations called interpretability;overview interpretability;logic zagreb;modal propositional logic;logic modal formulas;logic modal;logic differential;zagreb years interpretationability;called del interpretationability;propositional logic;logic differential equations;paper overview interpretability;logic zagreb years;logic version", "pdf_keywords": ""}, "90766546b29836eb96f54fe8fd70ec51a3e699ba": {"ta_keywords": "3d protein folds;dimensional 3d protein;representation 3d protein;protein folds network;3d protein;protein folds architecture;protein folds;prediction dimensional 3d;data representation 3d;folds network trained;representation 3d;dimensional 3d;cnn architecture prediction;folds network;network cnn;cnn architecture;folds architecture;2d kaplan lattice;network cnn architecture;neural network cnn;folds architecture based;lattice using pooling;3d;convolutional neural network;1d kaplan lattice;folds;prediction dimensional;protein;kaplan lattice;deep convolutional neural", "pdf_keywords": "protein folds network;predict protein structures;predict 3d protein;predict protein folds;predictions protein structures;3d tessellations protein;protein fold recognition;tessellations protein structure;3d protein folds;predicting protein structure;structure protein networks;protein folds 3d;model recognition protein;protein structures network;protein structure prediction;3d tessellation protein;tessellation protein structures;protein structures graph;simulation protein networks;protein networks;recognition protein structure;fold recognition protein;model protein structures;protein folding structure;protein structure graph;tessellations protein;prediction structure protein;characterizing protein structures;structure protein models;protein networks proposed"}, "7181a5139301c8a407da75a105dd457bf03d7057": {"ta_keywords": "stochastic network optimization;emphmarkovian network optimization;network optimization primal;generalize network optimization;network optimization powerful;network optimization;network optimization accommodate;network optimization termed;optimization termed emphmarkovian;method stochastic network;software stochastic network;optimization primal dual;optimization primal;emphmarkovian network;stochastic network;termed emphmarkovian network;flows heterogeneous planning;generalize network;dual method stochastic;optimization powerful;heterogeneous planning time;optimization;pair generalize network;dynamic programming;optimization termed;heterogeneous planning;primal dual method;flow multicommodity flows;software stochastic;technique optimization", "pdf_keywords": ""}, "af553d6121d338fc74dbd5faa43d5383a222198d": {"ta_keywords": "communication skill training;communication skills training;conventional communication skill;communication skill;time communication skill;time communication skills;communication skills;communication skill greater;early time communication;skills training based;skills training;people conventional communication;communication;training non early;skill training;time communication;propose training framework;training help people;conventional communication;skill training help;propose training;training based;training framework;training;training non;training help;training framework training;skills;training based fact;framework training non", "pdf_keywords": ""}, "a8315b5d3ff1b834fb58420397b13b9d169efad1": {"ta_keywords": "proposed disambiguation mechanism;novel disambiguation mechanism;cluster citation records;disambiguation mechanism;cluster citation;proposed disambiguation;disambiguation mechanism uses;propose novel disambiguation;disambiguation mechanism information;disambiguation;novel disambiguation;second cluster citation;citation records;efficiency proposed disambiguation;citation records experimental;uses soft clustering;soft clustering;soft clustering distinguish;hard clustering;hybrid clustering;distinguish different publication;soft clustering second;algorithm hybrid clustering;clustering;hybrid clustering algorithm;uses hard clustering;clustering distinguish;clustering distinguish different;clustering second;clustering second cluster", "pdf_keywords": ""}, "83ddc47f6dd0434c12eff9e4e42b727217a200a8": {"ta_keywords": "learning dynamics convergence;convergence agents stable;convergence guarantees stochastic;equilibrium continuous games;guarantees stochastic;convergence guarantees deterministic;convergence agents;guarantees stochastic case;learning dynamics;uniform learning rates;non uniform learning;stable local equilibrium;rates learning dynamics;agents stable local;local equilibrium continuous;convergence rates provide;rates provide convergence;study convergence agents;dynamics convergence rates;convergence guarantees;continuous games;guarantees deterministic;provide convergence guarantees;learning rates;uniform learning;learning rates provide;deterministic case convergence;equilibrium continuous;continuous games non;local equilibrium", "pdf_keywords": "learning agents uniformly;convergence agents learning;agents nonuniform learning;learning algorithms games;stochastic games agents;agents learning games;learning continuous game;games agents learning;gradient descent agents;game agents learning;agents learning continuous;learning optimal strategies;agents learning rates;class stochastic games;agents learn minimize;games provide convergence;gradient dynamics game;stochastic games;learning dynamics;learning global equilibrium;policy gradients existence;agents learn deterministic;consider stochastic games;convergence guarantees agents;multi agent learning;policy gradients provide;policy gradients;non uniform learning;asymptotic behavior learning;nash equilibrium gradient"}, "48ea80b65f42e9fbb96b856286d12347d1df52d2": {"ta_keywords": "commonsense reasoning dataset;reasoning dataset dense;novel commonsense reasoning;commonsense reasoning;dataset dense annotations;reasoning dataset;dense annotations;dense annotations enables;language understanding tasks;machine reasoning;trained language models;dense annotations shows;pre trained language;language models achieve;tested language understanding;annotations enables evaluation;language models struggle;trained language;language understanding;machine reasoning process;language models;commonsense;annotations enables;evaluation machine reasoning;annotations;dataset dense;reasoning process;annotations shows large;trip novel commonsense;understanding tasks large", "pdf_keywords": "coherent reasoning commonsense;commonsense reasoning dataset;reasoning commonsense language;novel commonsense reasoning;physical commonsense reasoning;reasoning commonsense;dataset quantifying reasoning;commonsense reasoning;commonsense reasoning offers;commonsense language understanding;language understanding reliably;reasoning abilities language;reasoning dataset designed;produce coherent reasoning;evaluation coherent reasoning;prediction physical commonsense;reasoning dataset;verifiable language inference;quantifying reasoning abilities;language understanding powerful;reasoning unified;dataset physical commonsense;coherent reasoning;benchmark physical commonsense;language inference providing;model dense annotations;dense annotations enables;reasoning abilities;commonsense language;able predict linguistic"}, "fa2657c0d66f048dee6b080536abbd1f947e822f": {"ta_keywords": "age estimation demonstrate;aging analysis;age estimation;aging analysis cognitive;model age estimation;life span aging;demonstrate estimated age;brain mris healthy;estimated age life;age significantly accurate;dataset structural brain;span aging analysis;age life span;age life;estimated age significantly;mris healthy population;aging;learning model age;structural brain mris;brain mris;life span evaluation;estimated age;utility estimated age;mris healthy;train deep learning;life span;span aging;model age;age significantly;deep learning", "pdf_keywords": "estimation brain age;neuronalpredicting brain age;estimate age brain;brain age crucial;accuracy brain age;brain aging;neuroimaging adult lifespan;age brain;brain age;brain age using;age brain diverse;brain aging structure;longitudinal neuroimaging data;brain age based;superior age estimation;aging predict age;neuropsychological dataset age;age estimation accuracy;set brain aging;aging predict;age linked neuropsychological;age estimation performance;age estimation heterogeneous;successfully predict age;accuracy age estimation;age brain using;age estimation;age estimation adult;brain activity longitudinal;activity longitudinal neuroimaging"}, "eb1b89751cac821792df36d3a1a2fb01dc4db2d1": {"ta_keywords": "spin orbit interaction;orbit interaction spin;interaction spin orbit;orbit coupled spin;coupled spin orbit;spin orbit coupled;dynamics spin orbit;enhance spin orbit;enhancement spin orbit;effect spin orbit;interaction dynamics spin;spin orbit;suppression spin orbit;coupled spin;interaction spin;dynamics spin;orbit interaction dynamics;orbit interaction;orbit interaction responsible;orbit interaction used;enhance spin;effect spin;enhancement spin;suppression spin;orbit coupled;responsible enhancement spin;responsible suppression spin;spin;used enhance spin;study effect spin", "pdf_keywords": ""}, "120839995e64f8ed734b5249ab681328c4955f5d": {"ta_keywords": "congested stochastic game;stochastic game;stochastic game exists;play congested stochastic;players optimal tolling;algorithms employed game;efficiently game;efficiently game designer;designer players optimal;solved efficiently game;players optimal;players learning algorithms;optimal tolling;congested stochastic;attained players learning;game solved efficiently;designing game;optimal tolling threshold;players learning;designing game solved;attained designer players;game exists threshold;problem designing game;players play congested;game designer players;game designer;threshold tolling;designer players play;stochastic;threshold attained players", "pdf_keywords": ""}, "4d16a47fb6708704b155855045c9e5d2ea380bb0": {"ta_keywords": "sentiment social media;analysis sentiment social;learning methods sentiment;sentiment analysis;methods sentiment analysis;sentiment analysis report;analysis sentiment;sentiment social;sentiment analysis context;methods sentiment;sentiment;domains movie reviews;movie reviews product;supervised machine learning;reviews product reviews;social media users;solutions analysis sentiment;state art supervised;machine learning methods;machine learning;time social media;movie reviews;product reviews;reviews product;machine learning obtain;supervised machine;real time social;social media use;social media;research supervised machine", "pdf_keywords": ""}, "7212cca9be971997434c2b3a27411a163bbd89c3": {"ta_keywords": "ct inference;ct inference new;conditioning ct context;context ct inference;improved conditioning ct;conditioning ct;selfconditioned ct context;selfconditioned ct;original selfconditioned ct;ct context ct;method improved conditioning;ct context;intermediates multipass conditioning;conditioning new method;context ct;multipass conditioning;multipass conditioning new;inference new method;ct;performance original selfconditioned;improved conditioning;conditioning;conditioning new;inference;selfconditioned;inference new;original selfconditioned;new method improved;new method improves;method improved", "pdf_keywords": "connectionist temporal classification;autoregressive encoder decoder;autoregressive encoder;temporal classification ctc;improved connectionist temporal;art autoregressive encoder;prediction tractable conditioning;classification ctc inference;layer conformer encoders;conformer encoders encoder;unlabeled encoders;encoders;temporal classification;representation conditioning inference;conditioned ctc probabilistic;encoders encoder;encoders encoders;conformer encoders;audio feature sequence;encoder;self conditioned ctc;encoder decoder;encoders encoder input;unlabeled encoders consider;connectionist temporal;prediction latent representation;classification ctc;conditioned ctc class;encoders represented;speech recognition"}, "6bb2b856d9a9b873259ba9dc48bc450c96eb3318": {"ta_keywords": "interacting electron gas;strongly interacting electron;electron gas strongly;electron gas vicinity;dimensional electron gas;gas strongly interacting;interacting electron;electron gas;gas electron gas;electron gas electron;dynamics dimensional electron;gas electron;vicinity dimensional electron;strongly interacting;dimensional electron;gas vicinity dimensional;electron;gas strongly;gas vicinity;dynamics dimensional;interacting;study dynamics dimensional;dynamics;vicinity dimensional;gas;dimensional;strongly;study dynamics;vicinity;study", "pdf_keywords": "automatic speech transcript;cost model transcription;speech transcript cost;segmentation transcript annotation;transcript annotation;transcript annotation problem;automatic speech;transcription cost model;speech transcripts;speech transcripts proposed;choosing segments transcriptthis;speech transcript;model transcription cost;predicts transcriber segmentation;model transcription task;accurately predicting transcription;selecting segments transcript;problem transcription segmentation;speech talk transcription;quality speech transcripts;transcript cost sensitive;predicting transcription task;accurately predicts transcriber;transcript cost;predicting transcription;predictive model transcription;train transcription prediction;correction speech transcripts;given transcription cost;transcript needed annotation"}, "79b8ef3905a42b771248719495a2117271906445": {"ta_keywords": "carbon footprint models;efficient carbon footprint;energy use carbon;carbon footprint;use carbon footprint;energy efficient carbon;carbon footprint reduction;carbon footprint large;efficient carbon;networks trained;trained withstand node;designing energy efficient;models evaluating energy;energy usage;networks trained withstand;energy efficient;energy estimates;potential energy efficient;evaluating energy usage;models neural networks;calculation energy use;energy estimates highlight;footprint reduction future;designing energy;neural networks trained;footprint large models;energy use;footprint models;footprint models evaluating;withstand node node", "pdf_keywords": "carbon footprint training;energy consumption carbon;carbon footprint ml;training computationally intensive;energy efficiency carbon;efficiency estimate carbon;model carbon footprint;efficiency carbon footprint;energy use carbon;energydeep learning powerful;energy consumption network;energydeep learning;estimate carbon footprint;carbon footprint model;estimate energy;estimate energy use;energy carbon;model energy;models including energy;energy efficient;improve energy efficiency;energy consumption;footprint 10th energy;energy efficiency;energy usage;including energy usage;reduce energy efficiency;training large nonlinear;carbon footprint;consumption carbon footprint"}, "1c709eef701d933af1383c790c13209f06806b60": {"ta_keywords": "annotated sequential rationales;sequential model predictions;rationales sequential model;sequential rationales sequential;sequential rationales propose;sequential rationales;rationales sequential;study sequential rationales;annotated sequential;sequential model;modeling machine translation;dataset annotated sequential;language modeling machine;language modeling;sequential;problem language modeling;machine translation;estimate faithful rationales;model predictions;rationales propose efficient;model predictions new;study sequential;faithful rationales;rationales propose;algorithm problem language;rationales apply algorithm;algorithm estimate faithful;predictions new dataset;rationales;predictions new", "pdf_keywords": "sequence language greedy;predicting sequence language;language greedy rationalization;prediction greedy rationalization;predictions greedy rationalization;sequential rationales greedy;learning greedy rationalization;sequential rationales language;annotated sequential rationales;rationales language modeling;sequence language;predicting sequence;prediction greedy;language greedy;predictions greedy;greedy rationalization suitable;sequential rationales capture;greedy rationalization greedy;rationalization greedy;greedy rationalization implemented;rationales sequence model;greedy rationalization;predictive computational linguistics;propose greedy rationalization;learning greedy;greedy rationalization new;rationalization greedy algorithm;greedy rationalization used;usual greedy rationalization;rationalize interpretable predictions"}, "0de86afbf91d0cf3e595a23a5b7a4d19deefb891": {"ta_keywords": "model bifurcations;minimal model bifurcations;bifurcation measure gradients;bifurcating parameter regimes;specified bifurcation diagram;model bifurcations match;bifurcation diagram;bifurcation diagram cost;bifurcations;user specified bifurcation;specified bifurcation;optimisers bifurcating parameter;bifurcation;targets bifurcation;specified targets bifurcation;optimisers bifurcating;targets bifurcation measure;bifurcating parameter;bifurcations match specified;bifurcations match;bifurcation measure;push optimisers bifurcating;bifurcating;parameter regimes demonstrate;parameter regimes;inferring parameters differential;parameters differential equations;demonstrate parameter inference;saddle pitchfork diagrams;inferring parameters", "pdf_keywords": "predicting bifurcations mixture;predicting bifurcations;method predicting bifurcations;locating bifurcations parameter;julia package bifurcationinference;bifurcations optimisation;detecting bifurcations;detecting bifurcations path;bifurcations parameter;detecting bifurcations complex;locating bifurcations;locate bifurcations;inferring bifurcations;discovering bifurcations;method detecting bifurcations;bifurcation diagram computational;determining bifurcation;method inferring bifurcations;bifurcations optimisation trajectories;bifurcations mixture;method determining bifurcation;locate bifurcations controlled;method discovering bifurcations;model bifurcations;discovering bifurcations nonlinear;use bifurcation;determining bifurcation structure;specified bifurcation;specified bifurcationin;bifurcationinference"}, "4f02d8775123624088a91fcfff20625463e5239a": {"ta_keywords": "driven collaborative filtering;collaborative filtering algorithm;collaborative filtering;collaborative filtering approach;novel collaborative filtering;data driven collaborative;better logistic regression;large education data;logistic regression;filtering algorithm enhanced;human interpretability based;fully data driven;interpretability based;education data;driven collaborative;enhanced human interpretability;interpretability based new;logistic regression approach;education data set;prediction performance fully;data driven;filtering approach;data set predicts;filtering algorithm;filtering;filtering approach using;evaluate prediction;human interpretability;better logistic;prediction performance", "pdf_keywords": ""}, "614dc4001ad68cac31484887f16542f04693eca4": {"ta_keywords": "model lobbying stochastic;lobbying stochastic;lobbying stochastic environment;model lobbying;propose model lobbying;lobbying;bribery criteria models;bribery criteria;bribery;criteria bribery;evaluation criteria bribery;criteria bribery criteria;propose model;modeled models computationally;models computationally;models computationally complex;stochastic;stochastic environment;stochastic environment provide;criteria models;modeled models;models;provide evaluation criteria;computationally;computationally complex;conclude modeled models;models conclude modeled;criteria models conclude;provide evaluation;models conclude", "pdf_keywords": ""}, "3261728694c0a53a2e8f95326f94147a28e03a83": {"ta_keywords": "sinareq deep quantization;deep quantization;deep quantization neural;quantization neural networks;quantization neural;learn multiple quantization;quantization parameterization;bitwidth assignment quantization;assignment quantization large;quantization;sinareq deep;multiple quantization parameterization;quantization large;regularization called sinareq;multiple quantization;sinusoidal regularization;assignment quantization;novel sinusoidal regularization;called sinareq deep;quantization parameterization conjunction;sinusoidal regularization called;deep networks;sinareq balance compute;quantization large variety;sinareq;sinusoidal properties learn;deep networks dorefa;networks leverage sinusoidal;training process sinareq;sinareq balance", "pdf_keywords": "waveq deep quantization;quantization deep neural;deep quantization;quantization deep;quantization neural network;optimization quantized training;learn layer quantization;quantization neural;wise quantization neural;bitwidth quantization deep;learning quantization;training algorithms quantization;deep quantization leverage;quantization convolutional neural;optimization quantized;quantization propose novel;standard optimization quantized;layer quantization;learn quantization;layer wise quantization;approach learn quantization;waveq deep;learning quantization convolutional;neural network waveq;desired quantization;wave based regularization;promoting desired quantization;method learning quantization;quantization powerful technique;algorithms quantization"}, "80ef8b8a1284790e0d8f7cbf9727c9e0b2a89332": {"ta_keywords": "predictions distribution shift;distribution shift;distribution shift large;distribution correct classifiers;shift estimation;shift estimation demonstrate;predictions distribution;box shift estimation;correct predictions distribution;black box shift;detect correct predictions;classifiers based predictions;shift large class;shift;based predictions;test distribution;box shift;predictions based black;based predictions based;statistical test detect;predictions based;new statistical test;classifiers;estimate test distribution;classifiers based;data sets demonstrate;shift large;correct classifiers based;new statistical;based black box", "pdf_keywords": "label shift supervised;shift supervised learning;label shift diagnosis;shift supervised;label shift detection;domain adaptation;robust label shift;detecting label shift;problem label estimation;detect label shift;label estimation;shift machine learning;learns predict label;consider domain adaptation;domain adaptation dda;predict label sample;predict label;bias label distribution;label estimation context;kernel mean matching;method label shift;target label distribution;label distribution;detect quantify shift;label shift;label shift simple;shift estimation;correcting label shift;learning based kernel;supervised learning"}, "843966d4b567033abff9775c5958f7be4db5c0ad": {"ta_keywords": "gaussian noise dynamics;dynamics level noise;gaussian noise;noise generated gaussian;noise dynamics level;effect gaussian noise;noise dynamics;stationary noise generated;stationary noise;level noise generated;assumed stationary noise;level noise;noise generated;stationary gaussian;stationary gaussian field;assumed stationary gaussian;effect gaussian;generated gaussian;gaussian field gaussian;gaussian;noise;gaussian field;field gaussian;generated gaussian field;gaussian field assumed;field gaussian field;dynamics level;study effect gaussian;dynamics;field assumed stationary", "pdf_keywords": ""}}